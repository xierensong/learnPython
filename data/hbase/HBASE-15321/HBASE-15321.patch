From c9c5fdb7d25fc6e38739182c6fa6a9997ca4347f Mon Sep 17 00:00:00 2001
From: Rahul Gidwani <rgidwani@salesforce.com>
Date: Wed, 24 Feb 2016 20:18:40 -0800
Subject: [PATCH] HBASE-15321 Ability to open a HRegion from hdfs snapshot.

---
 .../apache/hadoop/hbase/regionserver/HRegion.java  | 51 +++++++++++-
 .../hbase/regionserver/HRegionFileSystem.java      |  3 +-
 .../regionserver/TestHdfsSnapshotHRegion.java      | 95 ++++++++++++++++++++++
 3 files changed, 144 insertions(+), 5 deletions(-)
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index b70a4c3..de1fbb0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -387,6 +387,8 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     // whether the reads are enabled. This is different than readOnly, because readOnly is
     // static in the lifetime of the region, while readsEnabled is dynamic
     volatile boolean readsEnabled = true;
+    // whther or not the HRegion is on a read-only filesystem (eg hdfs-snapshots)
+    volatile boolean readOnlyFileSystem = false;
 
     /**
      * Set flags that make this region read-only.
@@ -398,6 +400,14 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       this.readOnly = onOff;
     }
 
+    boolean areWritesEnabled() {
+      return this.writesEnabled && !this.readOnlyFileSystem;
+    }
+
+    void setReadOnlyFilesystem(boolean readOnlyFileSystem) {
+      this.readOnlyFileSystem = readOnlyFileSystem;
+    }
+
     boolean isReadOnly() {
       return this.readOnly;
     }
@@ -798,7 +808,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     status.setStatus("Initializing all the Stores");
     long maxSeqId = initializeStores(reporter, status);
     this.mvcc.advanceTo(maxSeqId);
-    if (ServerRegionReplicaUtil.shouldReplayRecoveredEdits(this)) {
+    if (ServerRegionReplicaUtil.shouldReplayRecoveredEdits(this) && !this.writestate.readOnlyFileSystem) {
       // Recover any edits if available.
       maxSeqId = Math.max(maxSeqId,
         replayRecoveredEditsIfAny(this.fs.getRegionDir(), maxSeqIdInStores, reporter, status));
@@ -811,13 +821,13 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     this.writestate.flushRequested = false;
     this.writestate.compacting.set(0);
 
-    if (this.writestate.writesEnabled) {
+    if (this.writestate.areWritesEnabled()) {
       // Remove temporary data left over from old regions
       status.setStatus("Cleaning up temporary data from old regions");
       fs.cleanupTempDir();
     }
 
-    if (this.writestate.writesEnabled) {
+    if (this.writestate.areWritesEnabled()) {
       status.setStatus("Cleaning up detritus from prior splits");
       // Get rid of any splits or merges that were lost in-progress.  Clean out
       // these directories here on open.  We may be opening a region that was
@@ -844,7 +854,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     // In distributedLogReplay mode, we don't know the last change sequence number because region
     // is opened before recovery completes. So we add a safety bumper to avoid new sequence number
     // overlaps used sequence numbers
-    if (this.writestate.writesEnabled) {
+    if (this.writestate.areWritesEnabled()) {
       nextSeqid = WALSplitter.writeRegionSequenceIdFile(this.fs.getFileSystem(), this.fs
           .getRegionDir(), nextSeqid, (this.recovering ? (this.flushPerChanges + 10000000) : 1));
     } else {
@@ -3646,6 +3656,13 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     this.writestate.setReadsEnabled(readsEnabled);
   }
 
+  public void setReadOnlyFileSystem(boolean readOnlyFilesystem) {
+    if (readOnlyFilesystem && !this.writestate.readOnlyFileSystem) {
+      LOG.info(getRegionInfo().getEncodedName() + " : is on a read only file-system");
+    }
+    this.writestate.setReadOnlyFilesystem(readOnlyFilesystem);
+  }
+
   /**
    * Add updates first to the wal and then add values to memstore.
    * Warning: Assumption is caller has lock on passed in row.
@@ -6319,6 +6336,32 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
     return r.openHRegion(reporter);
   }
 
+  /**
+   * Open a Region on a read-only file-system (like hdfs snapshots)
+   * @param conf The Configuration object to use.
+   * @param fs Filesystem to use
+   * @param rootDir Root directory for HBase instance
+   * @param info Info for region to be opened.
+   * @param htd the table descriptor
+   * @param wal WAL for region to use. This method will call
+   * WAL#setSequenceNumber(long) passing the result of the call to
+   * HRegion#getMinSequenceId() to ensure the wal id is properly kept
+   * up.  HRegionStore does this every time it opens a new region.
+   * @return new HRegion
+   * @throws IOException
+   */
+  public static HRegion openReadOnlyFileSystemHRegion(final Configuration conf, final FileSystem fs,
+      final Path rootDir, final Path tableDir, final HRegionInfo info, final HTableDescriptor htd, final WAL wal)
+          throws IOException {
+    if (info == null) throw new NullPointerException("Passed region info is null");
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Opening region (readOnly filesystem): " + info);
+    }
+    HRegion r = HRegion.newHRegion(tableDir, wal, fs, conf, info, htd, null);
+    r.setReadOnlyFileSystem(true);
+    return r.openHRegion(null);
+  }
+
 
   /**
    * Useful when reopening a closed region (normally for unit tests)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
index 74ff546..a71ab67 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionFileSystem.java
@@ -28,6 +28,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.UUID;
 
+import com.google.common.annotations.VisibleForTesting;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
@@ -72,7 +73,7 @@ public class HRegionFileSystem {
   public static final String REGION_SPLITS_DIR = ".splits";
 
   /** Temporary subdirectory of the region directory used for compaction output. */
-  private static final String REGION_TEMP_DIR = ".tmp";
+  @VisibleForTesting  static final String REGION_TEMP_DIR = ".tmp";
 
   private final HRegionInfo regionInfo;
   //regionInfo for interacting with FS (getting encodedName, etc)
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java
new file mode 100644
index 0000000..0a168b3
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java
@@ -0,0 +1,95 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import com.google.common.collect.Iterables;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.testclassification.MediumTests;
+import org.apache.hadoop.hbase.testclassification.RegionServerTests;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hdfs.DFSClient;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.IOException;
+
+import static org.junit.Assert.assertNotNull;
+
+@Category({RegionServerTests.class, MediumTests.class})
+public class TestHdfsSnapshotHRegion {
+
+    private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+    public static final String SNAPSHOT_NAME = "foo_snapshot";
+    private HTable table;
+    public static final TableName TABLE_NAME = TableName.valueOf("foo");
+    public static final byte[] FAMILY = Bytes.toBytes("f1");
+    private DFSClient client;
+    private String baseDir;
+
+
+    @Before
+    public void setUp() throws Exception {
+        Configuration c = TEST_UTIL.getConfiguration();
+        c.setBoolean("dfs.support.append", true);
+        TEST_UTIL.startMiniCluster(1);
+        table = TEST_UTIL.createMultiRegionTable(TABLE_NAME, FAMILY);
+        TEST_UTIL.loadTable(table, FAMILY);
+
+        // setup the hdfssnapshots
+        client = new DFSClient(TEST_UTIL.getConfiguration());
+        String fullUrIPath = TEST_UTIL.getDefaultRootDirPath().toString();
+        String uriString = TEST_UTIL.getTestFileSystem().getUri().toString();
+        baseDir = StringUtils.removeStart(fullUrIPath, uriString);
+        client.allowSnapshot(baseDir);
+    }
+
+    @After
+    public void tearDown() throws Exception {
+        client.deleteSnapshot(baseDir, SNAPSHOT_NAME);
+        TEST_UTIL.shutdownMiniCluster();
+    }
+
+    @Test
+    public void testOpeningReadOnlyRegionBsic() throws Exception {
+        String snapshotDir = client.createSnapshot(baseDir, SNAPSHOT_NAME);
+        HRegionInfo firstRegion = Iterables.getFirst(table.getRegionLocator().getAllRegionLocations(), null).getRegionInfo();
+        Path tableDir = FSUtils.getTableDir(new Path(snapshotDir), TABLE_NAME);
+        HRegion result = openSnapshottedRegion(firstRegion, tableDir);
+        assertNotNull(result);
+    }
+
+    @Test
+    public void testSnapshottingWithTmpSplitsAndMergeDirectoriesPresent() throws Exception {
+        // lets get a region and create those directories and make sure we ignore them
+        HRegionInfo firstRegion = Iterables.getFirst(table.getRegionLocator().getAllRegionLocations(), null).getRegionInfo();
+        String encodedName = firstRegion.getEncodedName();
+        Path tableDir = FSUtils.getTableDir(TEST_UTIL.getDefaultRootDirPath(), TABLE_NAME);
+        Path regionDirectoryPath = new Path(tableDir, encodedName);
+        TEST_UTIL.getTestFileSystem().create(new Path(regionDirectoryPath, HRegionFileSystem.REGION_TEMP_DIR));
+        TEST_UTIL.getTestFileSystem().create(new Path(regionDirectoryPath, HRegionFileSystem.REGION_SPLITS_DIR));
+        TEST_UTIL.getTestFileSystem().create(new Path(regionDirectoryPath, HRegionFileSystem.REGION_MERGES_DIR));
+        // now snapshot
+        String snapshotDir = client.createSnapshot(baseDir, "foo_snapshot");
+        // everything should still open just fine
+        openSnapshottedRegion(firstRegion, FSUtils.getTableDir(new Path(snapshotDir), TABLE_NAME));
+    }
+
+    private HRegion openSnapshottedRegion(HRegionInfo firstRegion, Path tableDir) throws IOException {
+        return HRegion.openReadOnlyFileSystemHRegion(
+                TEST_UTIL.getConfiguration(),
+                TEST_UTIL.getTestFileSystem(),
+                TEST_UTIL.getDefaultRootDirPath(),
+                tableDir,
+                firstRegion,
+                table.getTableDescriptor(),
+                null
+        );
+    }
+}
-- 
2.5.4 (Apple Git-61)

