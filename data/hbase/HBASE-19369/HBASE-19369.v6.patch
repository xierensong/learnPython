From f8c4ffab225a9e01252b82e5476b356da6699ea2 Mon Sep 17 00:00:00 2001
From: Alex Leblang <alex.leblang@cloudera.com>
Date: Fri, 22 Dec 2017 13:40:33 -0600
Subject: [PATCH] HBASE-19369 Switch to Builder Pattern In WAL

This patch switches to the builder pattern by adding a helper method.
It also checks to ensure that the pattern is available (i.e. that
HBase is running on a hadoop version that supports it).

Amending-Author: Mike Drob <mdrob@apache.org>
---
 .../apache/hadoop/hbase/util/CommonFSUtils.java    | 124 +++++++++++++++++++++
 .../procedure2/store/wal/WALProcedureStore.java    |   2 +-
 .../hbase/io/asyncfs/AsyncFSOutputHelper.java      |   6 +-
 .../hbase/regionserver/wal/ProtobufLogWriter.java  |   3 +-
 .../hbase/regionserver/wal/TestHBaseOnEC.java      |  81 ++++++++++++++
 5 files changed, 208 insertions(+), 8 deletions(-)
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHBaseOnEC.java

diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java
index 5a6625bd4d..ce335a3e6d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/CommonFSUtils.java
@@ -806,6 +806,130 @@ public abstract class CommonFSUtils {
     conf.setIfUnset(dfsKey, Integer.toString(hbaseSize));
   }
 
+  private static class DfsBuilderUtility {
+    static Class<?> dfsClass = null;
+    static Method createMethod;
+    static Method overwriteMethod;
+    static Method bufferSizeMethod;
+    static Method blockSizeMethod;
+    static Method recursiveMethod;
+    static Method replicateMethod;
+    static Method replicationMethod;
+    static Method buildMethod;
+    static boolean allMethodsPresent = false;
+
+    static {
+      String dfsClassName = "org.apache.hadoop.hdfs.DistributedFileSystem";
+      Class<?> builderClass = null;
+
+      try {
+        dfsClass = Class.forName(dfsClassName);
+      } catch (ClassNotFoundException e) {
+        LOG.info("DistributedFileSystem class not available, will not use builder API for file creation.");
+      }
+      try {
+        builderClass = Class.forName(dfsClassName + " .HdfsDataOutputStreamBuilder");
+      } catch (ClassNotFoundException e) {
+        LOG.info("DistributedFileSystem.HdfsDataOutputStreamBuilder not available, will not use builder API for file creation.");
+      }
+
+      if (dfsClass != null && builderClass != null) {
+        try {
+          createMethod = dfsClass.getMethod("createFile", Path.class);
+          overwriteMethod = builderClass.getMethod("overwrite", boolean.class);
+          bufferSizeMethod = builderClass.getMethod("bufferSize", int.class);
+          blockSizeMethod = builderClass.getMethod("blockSize", long.class);
+          recursiveMethod = builderClass.getMethod("recursive");
+          replicateMethod = builderClass.getMethod("replicate");
+          replicationMethod = builderClass.getMethod("replication", short.class);
+          buildMethod = builderClass.getMethod("build");
+
+          allMethodsPresent = true;
+        } catch (NoSuchMethodException e) {
+          LOG.info("Not all builder methods were found, will not use DFS Builder API for file creation.");
+        }
+      }
+    }
+
+    /**
+     * Attempt to use builder API via reflection to create a file with the given parameters and replication enabled.
+     */
+    public static FSDataOutputStream createHelper(FileSystem fs, Path path, boolean overwritable, int bufferSize,
+        short replication, long blockSize, boolean isRecursive) throws IOException {
+      FSDataOutputStream output = null;
+
+      if (allMethodsPresent && dfsClass.isInstance(fs)) {
+        try {
+          Object builder;
+
+          builder = createMethod.invoke(fs, path);
+          builder = overwriteMethod.invoke(builder, overwritable);
+          builder = bufferSizeMethod.invoke(builder, bufferSize);
+          builder = blockSizeMethod.invoke(builder, blockSize);
+          if (isRecursive) {
+            builder = recursiveMethod.invoke(builder);
+          }
+          builder = replicateMethod.invoke(builder);
+          builder = replicationMethod.invoke(builder, replication);
+          output = (FSDataOutputStream) buildMethod.invoke(builder);
+        } catch (IllegalAccessException | InvocationTargetException e) {
+          // Couldn't reflect to build an output stream, probably on an old version of hadoop
+        }
+      }
+
+      if (output == null) {
+        if (isRecursive) {
+          output = fs.create(path, overwritable, bufferSize, replication, blockSize, null);
+        } else {
+          output = fs.createNonRecursive(path, overwritable, bufferSize, replication, blockSize, null);
+        }
+      }
+
+      return output;
+    }
+
+    /**
+     * Attempt to use builder API via reflection to create a file with the given parameters and replication enabled.
+     */
+    public static FSDataOutputStream createHelper(FileSystem fs, Path path, boolean overwritable) throws IOException {
+      FSDataOutputStream output = null;
+
+      if (allMethodsPresent && dfsClass.isInstance(fs)) {
+        try {
+          Object builder;
+
+          builder = createMethod.invoke(fs, path);
+          builder = overwriteMethod.invoke(builder, overwritable);
+          builder = replicateMethod.invoke(builder);
+          output = (FSDataOutputStream) buildMethod.invoke(builder);
+        } catch (IllegalAccessException | InvocationTargetException e) {
+          // Couldn't reflect to build an output stream, probably on an old version of hadoop
+        }
+      }
+
+      if (output == null) {
+        output = fs.create(path, overwritable);
+      }
+
+      return output;
+    }
+  }
+
+  /**
+   * Attempt to use builder API via reflection to create a file with the given parameters and replication enabled.
+   */
+  public static FSDataOutputStream createForWal(FileSystem fs, Path path, boolean overwritable) throws IOException {
+    return DfsBuilderUtility.createHelper(fs, path, overwritable);
+  }
+
+  /**
+   * Attempt to use builder API via reflection to create a file with the given parameters and replication enabled.
+   */
+  public static FSDataOutputStream createForWal(FileSystem fs, Path path, boolean overwritable, int bufferSize,
+      short replication, long blockSize, boolean isRecursive) throws IOException {
+    return DfsBuilderUtility.createHelper(fs, path, overwritable, bufferSize, replication, blockSize, isRecursive);
+  }
+
   // Holder singleton idiom. JVM spec ensures this will be run at most once per Classloader, and
   // not until we attempt to reference it.
   private static class StreamCapabilities {
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
index 521ae7e8ca..b6543c3800 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java
@@ -1020,7 +1020,7 @@ public class WALProcedureStore extends ProcedureStoreBase {
     long startPos = -1;
     newLogFile = getLogFilePath(logId);
     try {
-      newStream = fs.create(newLogFile, false);
+      newStream = CommonFSUtils.createForWal(fs, newLogFile, false);
     } catch (FileAlreadyExistsException e) {
       LOG.error("Log file with id=" + logId + " already exists", e);
       return false;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/AsyncFSOutputHelper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/AsyncFSOutputHelper.java
index 43ddfb06cb..aeff6c955e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/AsyncFSOutputHelper.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/asyncfs/AsyncFSOutputHelper.java
@@ -54,11 +54,7 @@ public final class AsyncFSOutputHelper {
     final FSDataOutputStream out;
     int bufferSize = fs.getConf().getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY,
       CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);
-    if (createParent) {
-      out = fs.create(f, overwrite, bufferSize, replication, blockSize, null);
-    } else {
-      out = fs.createNonRecursive(f, overwrite, bufferSize, replication, blockSize, null);
-    }
+    out = CommonFSUtils.createForWal(fs, f, overwrite, bufferSize, replication, blockSize, createParent);
     // After we create the stream but before we attempt to use it at all
     // ensure that we can provide the level of data safety we're configured
     // to provide.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java
index aeb2c19c25..34b4fc684e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java
@@ -88,8 +88,7 @@ public class ProtobufLogWriter extends AbstractProtobufLogWriter
   @Override
   protected void initOutput(FileSystem fs, Path path, boolean overwritable, int bufferSize,
       short replication, long blockSize) throws IOException, StreamLacksCapabilityException {
-    this.output = fs.createNonRecursive(path, overwritable, bufferSize, replication, blockSize,
-      null);
+    this.output = CommonFSUtils.createForWal(fs, path, overwritable, bufferSize, replication, blockSize, false);
     // TODO Be sure to add a check for hsync if this branch includes HBASE-19024
     if (fs.getConf().getBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE, true) &&
         !(CommonFSUtils.hasCapability(output, "hflush"))) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHBaseOnEC.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHBaseOnEC.java
new file mode 100644
index 0000000000..48a4ab18ce
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHBaseOnEC.java
@@ -0,0 +1,81 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hdfs.DFSTestUtil;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import static org.junit.Assert.assertArrayEquals;
+import static org.junit.Assert.assertEquals;
+
+import java.io.IOException;
+import java.lang.reflect.Method;
+
+// @Category LargeTest
+public class TestHBaseOnEC {
+  private static final HBaseTestingUtility util = new HBaseTestingUtility();
+
+  final byte[] row = Bytes.toBytes("row");
+  final byte[] cf = Bytes.toBytes("cf");
+  final byte[] cq = Bytes.toBytes("cq");
+  final byte[] value = Bytes.toBytes("value");
+
+  @BeforeClass
+  public static void setup() throws Exception {
+    util.startMiniCluster();
+    MiniDFSCluster cluster = util.getDFSCluster();
+
+    try {
+      Method m = DFSTestUtil.class.getMethod("enableAllECPolicies", DistributedFileSystem.class);
+      m.invoke(null, cluster.getFileSystem());
+    } catch (NoSuchMethodException e) {
+      // We are on an older version of hadoop, don't set the policies
+    }
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    util.shutdownMiniCluster();
+  }
+
+  @Test
+  public void testBasic() throws IOException {
+    TableName name = TableName.valueOf(getClass().getSimpleName());
+
+    Table t = util.createTable(name, cf);
+    t.put(new Put(row).addColumn(cf, cq, value));
+
+    util.getAdmin().flush(name);
+
+    assertArrayEquals(value, t.get(new Get(row)).getValue(cf, cq));
+  }
+}
+
-- 
2.15.0

