Index: src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java	(revision 1138121)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/TableSplit.java	(working copy)
@@ -25,74 +25,106 @@
 import java.util.Arrays;
 
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.InputSplit;
+import org.mortbay.log.Log;
 
 /**
- * A table split corresponds to a key range (low, high). All references to row
- * below refer to the key of the row.
+ * A table split corresponds to a key range (low, high) and an optional scanner.
+ * All references to row below refer to the key of the row.
  */
-public class TableSplit extends InputSplit
-implements Writable, Comparable<TableSplit> {
-
-  private byte [] tableName;
-  private byte [] startRow;
-  private byte [] endRow;
+public class TableSplit extends InputSplit implements Writable,
+    Comparable<TableSplit> {
+  private byte[] tableName;
+  private byte[] startRow;
+  private byte[] endRow;
   private String regionLocation;
+  private String scan = "";
 
   /** Default constructor. */
   public TableSplit() {
-    this(HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY,
-      HConstants.EMPTY_BYTE_ARRAY, "");
+    this(HConstants.EMPTY_BYTE_ARRAY, null, HConstants.EMPTY_BYTE_ARRAY,
+        HConstants.EMPTY_BYTE_ARRAY, "");
   }
 
   /**
    * Creates a new instance while assigning all variables.
-   *
-   * @param tableName  The name of the current table.
-   * @param startRow  The start row of the split.
-   * @param endRow  The end row of the split.
-   * @param location  The location of the region.
+   * 
+   * @param tableName The name of the current table.
+   * @param scan A scanner associated with this split .
+   * @param startRow The start row of the split.
+   * @param endRow The end row of the split.
+   * @param location The location of the region.
    */
-  public TableSplit(byte [] tableName, byte [] startRow, byte [] endRow,
-      final String location) {
+  public TableSplit(byte[] tableName, Scan scan, byte[] startRow,
+      byte[] endRow, final String location) {
     this.tableName = tableName;
     this.startRow = startRow;
     this.endRow = endRow;
+    try {
+      this.scan =
+          (null == scan) ? "" : TableMapReduceUtil.convertScanToString(scan);
+    } catch (IOException e) {
+      Log.warn("Failed to convert Scan to String", e);
+    }
     this.regionLocation = location;
   }
 
   /**
+   * Creates a new instance without a scanner.
+   * 
+   * @param tableName The name of the current table.
+   * @param startRow The start row of the split.
+   * @param endRow The end row of the split.
+   * @param location The location of the region.
+   */
+  public TableSplit(byte[] tableName, byte[] startRow, byte[] endRow,
+      final String location) {
+    this(tableName, null, startRow, endRow, location);
+  }
+
+  /**
+   * Returns a Scan object from the stored string representation.
+   * 
+   * @return Returns a Scan object based on the stored scanner.
+   * @throws IOException
+   */
+  public Scan getScan() throws IOException {
+    return TableMapReduceUtil.convertStringToScan(this.scan);
+  }
+
+  /**
    * Returns the table name.
-   *
+   * 
    * @return The table name.
    */
-  public byte [] getTableName() {
-    return tableName;
+  public byte[] getTableName() {
+    return this.tableName;
   }
 
   /**
    * Returns the start row.
-   *
+   * 
    * @return The start row.
    */
-  public byte [] getStartRow() {
+  public byte[] getStartRow() {
     return startRow;
   }
 
   /**
    * Returns the end row.
-   *
+   * 
    * @return The end row.
    */
-  public byte [] getEndRow() {
+  public byte[] getEndRow() {
     return endRow;
   }
 
   /**
    * Returns the region location.
-   *
+   * 
    * @return The region's location.
    */
   public String getRegionLocation() {
@@ -101,18 +133,18 @@
 
   /**
    * Returns the region's location as an array.
-   *
+   * 
    * @return The array containing the region location.
    * @see org.apache.hadoop.mapreduce.InputSplit#getLocations()
    */
   @Override
   public String[] getLocations() {
-    return new String[] {regionLocation};
+    return new String[] { regionLocation };
   }
 
   /**
    * Returns the length of the split.
-   *
+   * 
    * @return The length of the split.
    * @see org.apache.hadoop.mapreduce.InputSplit#getLength()
    */
@@ -124,13 +156,14 @@
 
   /**
    * Reads the values of each field.
-   *
-   * @param in  The input to read from.
+   * 
+   * @param in The input to read from.
    * @throws IOException When reading the input fails.
    */
   @Override
   public void readFields(DataInput in) throws IOException {
     tableName = Bytes.readByteArray(in);
+    scan = Bytes.toString(Bytes.readByteArray(in));
     startRow = Bytes.readByteArray(in);
     endRow = Bytes.readByteArray(in);
     regionLocation = Bytes.toString(Bytes.readByteArray(in));
@@ -138,13 +171,14 @@
 
   /**
    * Writes the field values to the output.
-   *
-   * @param out  The output to write to.
+   * 
+   * @param out The output to write to.
    * @throws IOException When writing the values to the output fails.
    */
   @Override
   public void write(DataOutput out) throws IOException {
     Bytes.writeByteArray(out, tableName);
+    Bytes.writeByteArray(out, Bytes.toBytes(scan));
     Bytes.writeByteArray(out, startRow);
     Bytes.writeByteArray(out, endRow);
     Bytes.writeByteArray(out, Bytes.toBytes(regionLocation));
@@ -152,26 +186,32 @@
 
   /**
    * Returns the details about this instance as a string.
-   *
+   * 
    * @return The values of this instance as a string.
    * @see java.lang.Object#toString()
    */
   @Override
   public String toString() {
-    return regionLocation + ":" +
-      Bytes.toStringBinary(startRow) + "," + Bytes.toStringBinary(endRow);
+    return regionLocation + ":" + Bytes.toStringBinary(startRow) + "," +
+        Bytes.toStringBinary(endRow);
   }
 
   /**
    * Compares this split against the given one.
-   *
-   * @param split  The split to compare to.
+   * 
+   * @param split The split to compare to.
    * @return The result of the comparison.
    * @see java.lang.Comparable#compareTo(java.lang.Object)
    */
   @Override
   public int compareTo(TableSplit split) {
-    return Bytes.compareTo(getStartRow(), split.getStartRow());
+    // If The table name of the two splits is the same then compare based on the
+    // start row
+    // otherwise compare based on table name
+    int tableNameComparison =
+        Bytes.compareTo(getTableName(), split.getTableName());
+    return tableNameComparison != 0 ? tableNameComparison : Bytes.compareTo(
+        getStartRow(), split.getStartRow());
   }
 
   @Override
@@ -179,18 +219,21 @@
     if (o == null || !(o instanceof TableSplit)) {
       return false;
     }
-    return Bytes.equals(tableName, ((TableSplit)o).tableName) &&
-      Bytes.equals(startRow, ((TableSplit)o).startRow) &&
-      Bytes.equals(endRow, ((TableSplit)o).endRow) &&
-      regionLocation.equals(((TableSplit)o).regionLocation);
+    return Bytes.equals(tableName, ((TableSplit) o).tableName) &&
+        scan.equals(((TableSplit) o).scan) &&
+        Bytes.equals(startRow, ((TableSplit) o).startRow) &&
+        Bytes.equals(endRow, ((TableSplit) o).endRow) &&
+        regionLocation.equals(((TableSplit) o).regionLocation);
   }
 
-    @Override
-    public int hashCode() {
-        int result = tableName != null ? Arrays.hashCode(tableName) : 0;
-        result = 31 * result + (startRow != null ? Arrays.hashCode(startRow) : 0);
-        result = 31 * result + (endRow != null ? Arrays.hashCode(endRow) : 0);
-        result = 31 * result + (regionLocation != null ? regionLocation.hashCode() : 0);
-        return result;
-    }
+  @Override
+  public int hashCode() {
+    int result = tableName != null ? Arrays.hashCode(tableName) : 0;
+    result = 31 * result + (scan != null ? scan.hashCode() : 0);
+    result = 31 * result + (startRow != null ? Arrays.hashCode(startRow) : 0);
+    result = 31 * result + (endRow != null ? Arrays.hashCode(endRow) : 0);
+    result =
+        31 * result + (regionLocation != null ? regionLocation.hashCode() : 0);
+    return result;
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java	(revision 1136064)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java	(working copy)
@@ -36,7 +36,6 @@
  */
 public class TableRecordReaderImpl {
 
-
   static final Log LOG = LogFactory.getLog(TableRecordReader.class);
 
   private ResultScanner scanner = null;
@@ -48,8 +47,8 @@
 
   /**
    * Restart from survivable exceptions by creating a new scanner.
-   *
-   * @param firstRow  The first row to start at.
+   * 
+   * @param firstRow The first row to start at.
    * @throws IOException When restarting fails.
    */
   public void restart(byte[] firstRow) throws IOException {
@@ -60,7 +59,7 @@
 
   /**
    * Build the scanner. Not done in constructor to allow for extension.
-   *
+   * 
    * @throws IOException When restarting the scan fails.
    */
   public void init() throws IOException {
@@ -69,8 +68,8 @@
 
   /**
    * Sets the HBase table.
-   *
-   * @param htable  The {@link HTable} to scan.
+   * 
+   * @param htable The {@link HTable} to scan.
    */
   public void setHTable(HTable htable) {
     this.htable = htable;
@@ -78,8 +77,8 @@
 
   /**
    * Sets the scan defining the actual details like columns etc.
-   *
-   * @param scan  The scan to set.
+   * 
+   * @param scan The scan to set.
    */
   public void setScan(Scan scan) {
     this.scan = scan;
@@ -87,16 +86,22 @@
 
   /**
    * Closes the split.
-   *
-   *
+   * 
+   * 
    */
   public void close() {
     this.scanner.close();
+    try {
+      this.htable.close();
+    } catch (IOException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    }
   }
 
   /**
    * Returns the current key.
-   *
+   * 
    * @return The current key.
    * @throws IOException
    * @throws InterruptedException When the job is aborted.
@@ -108,7 +113,7 @@
 
   /**
    * Returns the current value.
-   *
+   * 
    * @return The current value.
    * @throws IOException When the value is faulty.
    * @throws InterruptedException When the job is aborted.
@@ -117,30 +122,31 @@
     return value;
   }
 
-
   /**
    * Positions the record reader to the next record.
-   *
+   * 
    * @return <code>true</code> if there was another record.
    * @throws IOException When reading the record failed.
    * @throws InterruptedException When the job was aborted.
    */
   public boolean nextKeyValue() throws IOException, InterruptedException {
-    if (key == null) key = new ImmutableBytesWritable();
-    if (value == null) value = new Result();
+    if (key == null)
+      key = new ImmutableBytesWritable();
+    if (value == null)
+      value = new Result();
     try {
       value = this.scanner.next();
     } catch (IOException e) {
       LOG.debug("recovered from " + StringUtils.stringifyException(e));
       if (lastRow == null) {
-        LOG.warn("We are restarting the first next() invocation," +
-            " if your mapper's restarted a few other times like this" +
-            " then you should consider killing this job and investigate" +
-            " why it's taking so long.");
+        LOG.warn("We are restarting the first next() invocation,"
+            + " if your mapper's restarted a few other times like this"
+            + " then you should consider killing this job and investigate"
+            + " why it's taking so long.");
         lastRow = scan.getStartRow();
       }
       restart(lastRow);
-      scanner.next();    // skip presumed already mapped row
+      scanner.next(); // skip presumed already mapped row
       value = scanner.next();
     }
     if (value != null && value.size() > 0) {
@@ -153,7 +159,7 @@
 
   /**
    * The current progress of the record reader through its data.
-   *
+   * 
    * @return A number between 0.0 and 1.0, the fraction of the data read.
    */
   public float getProgress() {
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java	(revision 1136064)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java	(working copy)
@@ -26,6 +26,7 @@
 import java.io.IOException;
 import java.net.URL;
 import java.net.URLDecoder;
+import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashSet;
 import java.util.Set;
@@ -35,7 +36,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scan;
@@ -54,115 +54,173 @@
 @SuppressWarnings("unchecked")
 public class TableMapReduceUtil {
   static Log LOG = LogFactory.getLog(TableMapReduceUtil.class);
-  
+
   /**
-   * Use this before submitting a TableMap job. It will appropriately set up
-   * the job.
-   *
-   * @param table  The table name to read from.
-   * @param scan  The scan instance with the columns, time range etc.
-   * @param mapper  The mapper class to use.
-   * @param outputKeyClass  The class of the output key.
-   * @param outputValueClass  The class of the output value.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
+   * Use this before submitting a TableMap job. It will appropriately set up the
+   * job.
+   * 
+   * @param table The table name to read from.
+   * @param scan The scan instance with the columns, time range etc.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
    * @throws IOException When setting up the details fails.
    */
   public static void initTableMapperJob(String table, Scan scan,
       Class<? extends TableMapper> mapper,
       Class<? extends WritableComparable> outputKeyClass,
-      Class<? extends Writable> outputValueClass, Job job)
-  throws IOException {
+      Class<? extends Writable> outputValueClass, Job job) throws IOException {
     initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass,
         job, true);
   }
 
-
   /**
-   * Use this before submitting a TableMap job. It will appropriately set up
-   * the job.
-   *
+   * Use this before submitting a TableMap job. It will appropriately set up the
+   * job.
+   * 
    * @param table Binary representation of the table name to read from.
-   * @param scan  The scan instance with the columns, time range etc.
-   * @param mapper  The mapper class to use.
-   * @param outputKeyClass  The class of the output key.
-   * @param outputValueClass  The class of the output value.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
+   * @param scan The scan instance with the columns, time range etc.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
    * @throws IOException When setting up the details fails.
    */
-   public static void initTableMapperJob(byte[] table, Scan scan,
+  public static void initTableMapperJob(byte[] table, Scan scan,
       Class<? extends TableMapper> mapper,
       Class<? extends WritableComparable> outputKeyClass,
-      Class<? extends Writable> outputValueClass, Job job)
-  throws IOException {
-      initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass,
-              job, true);
+      Class<? extends Writable> outputValueClass, Job job) throws IOException {
+    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass,
+        outputValueClass, job, true);
   }
 
   /**
-   * Use this before submitting a TableMap job. It will appropriately set up
-   * the job.
-   *
-   * @param table  The table name to read from.
-   * @param scan  The scan instance with the columns, time range etc.
-   * @param mapper  The mapper class to use.
-   * @param outputKeyClass  The class of the output key.
-   * @param outputValueClass  The class of the output value.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
-   * @param addDependencyJars upload HBase jars and jars for any of the configured
-   *           job classes via the distributed cache (tmpjars).
+   * Use this before submitting a TableMap job. It will appropriately set up the
+   * job.
+   * 
+   * @param table The table name to read from.
+   * @param scan The scan instance with the columns, time range etc.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @param addDependencyJars upload HBase jars and jars for any of the
+   *          configured job classes via the distributed cache (tmpjars).
    * @throws IOException When setting up the details fails.
    */
   public static void initTableMapperJob(String table, Scan scan,
       Class<? extends TableMapper> mapper,
       Class<? extends WritableComparable> outputKeyClass,
       Class<? extends Writable> outputValueClass, Job job,
-      boolean addDependencyJars)
-  throws IOException {
+      boolean addDependencyJars) throws IOException {
     job.setInputFormatClass(TableInputFormat.class);
-    if (outputValueClass != null) job.setMapOutputValueClass(outputValueClass);
-    if (outputKeyClass != null) job.setMapOutputKeyClass(outputKeyClass);
+    if (outputValueClass != null)
+      job.setMapOutputValueClass(outputValueClass);
+    if (outputKeyClass != null)
+      job.setMapOutputKeyClass(outputKeyClass);
     job.setMapperClass(mapper);
     HBaseConfiguration.addHbaseResources(job.getConfiguration());
     job.getConfiguration().set(TableInputFormat.INPUT_TABLE, table);
-    job.getConfiguration().set(TableInputFormat.SCAN,
-      convertScanToString(scan));
+    job.getConfiguration()
+        .set(TableInputFormat.SCAN, convertScanToString(scan));
     if (addDependencyJars) {
       addDependencyJars(job);
     }
   }
 
   /**
-   * Use this before submitting a TableMap job. It will appropriately set up
-   * the job.
-   *
+   * Use this before submitting a TableMap job. It will appropriately set up the
+   * job.
+   * 
    * @param table Binary representation of the table name to read from.
-   * @param scan  The scan instance with the columns, time range etc.
-   * @param mapper  The mapper class to use.
-   * @param outputKeyClass  The class of the output key.
-   * @param outputValueClass  The class of the output value.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
-   * @param addDependencyJars upload HBase jars and jars for any of the configured
-   *           job classes via the distributed cache (tmpjars).
+   * @param scan The scan instance with the columns, time range etc.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @param addDependencyJars upload HBase jars and jars for any of the
+   *          configured job classes via the distributed cache (tmpjars).
    * @throws IOException When setting up the details fails.
    */
   public static void initTableMapperJob(byte[] table, Scan scan,
       Class<? extends TableMapper> mapper,
       Class<? extends WritableComparable> outputKeyClass,
       Class<? extends Writable> outputValueClass, Job job,
-      boolean addDependencyJars)
-  throws IOException {
-      initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass,
-              outputValueClass, job, addDependencyJars);
+      boolean addDependencyJars) throws IOException {
+    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass,
+        outputValueClass, job, addDependencyJars);
+  }
+
+  /**
+   * Use this before submitting a Multi TableMap job. It will appropriately set
+   * up the job.
+   * 
+   * @param tables The tables collection to read from.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @throws IOException When setting up the details fails.
+   */
+  public static void initTableMapperJob(MultiTableInputCollection tables,
+      Class<? extends TableMapper> mapper,
+      Class<? extends WritableComparable> outputKeyClass,
+      Class<? extends Writable> outputValueClass, Job job) throws IOException {
+    initTableMapperJob(tables, mapper, outputKeyClass, outputValueClass, job,
+        true);
+  }
+
+  /**
+   * Use this before submitting a Multi TableMap job. It will appropriately set
+   * up the job.
+   * 
+   * @param tables The tables collection to read from.
+   * @param mapper The mapper class to use.
+   * @param outputKeyClass The class of the output key.
+   * @param outputValueClass The class of the output value.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @param addDependencyJars upload HBase jars and jars for any of the
+   *          configured job classes via the distributed cache (tmpjars).
+   * @throws IOException When setting up the details fails.
+   */
+  public static void initTableMapperJob(MultiTableInputCollection tables,
+      Class<? extends TableMapper> mapper,
+      Class<? extends WritableComparable> outputKeyClass,
+      Class<? extends Writable> outputValueClass, Job job,
+      boolean addDependencyJars) throws IOException {
+    job.setInputFormatClass(MultiTableInputFormat.class);
+    if (outputValueClass != null)
+      job.setMapOutputValueClass(outputValueClass);
+    if (outputKeyClass != null)
+      job.setMapOutputKeyClass(outputKeyClass);
+    job.setMapperClass(mapper);
+    HBaseConfiguration.addHbaseResources(job.getConfiguration());
+    ArrayList<String> inputTables = new ArrayList<String>();
+
+    for (MultiTableInputCollection.TableInputConf tic : tables) {
+      inputTables.add(tic.getTableName());
+      String scan = convertScanToString(tic.getScan());
+      inputTables.add(scan);
+    }
+    job.getConfiguration().setStrings(MultiTableInputFormat.INPUT_TABLES,
+        inputTables.toArray(new String[inputTables.size()]));
+
+    if (addDependencyJars) {
+      addDependencyJars(job);
+    }
   }
 
   /**
    * Writes the given scan into a Base64 encoded string.
-   *
-   * @param scan  The scan to write out.
+   * 
+   * @param scan The scan to write out.
    * @return The scan saved in a Base64 encoded string.
    * @throws IOException When writing the scan fails.
    */
@@ -175,8 +233,8 @@
 
   /**
    * Converts the given Base64 string back into a Scan instance.
-   *
-   * @param base64  The scan details.
+   * 
+   * @param base64 The scan details.
    * @return The newly created Scan instance.
    * @throws IOException When reading the scan instance fails.
    */
@@ -189,108 +247,112 @@
   }
 
   /**
-   * Use this before submitting a TableReduce job. It will
-   * appropriately set up the JobConf.
-   *
-   * @param table  The output table.
-   * @param reducer  The reducer class to use.
-   * @param job  The current job to adjust.
+   * Use this before submitting a TableReduce job. It will appropriately set up
+   * the JobConf.
+   * 
+   * @param table The output table.
+   * @param reducer The reducer class to use.
+   * @param job The current job to adjust.
    * @throws IOException When determining the region count fails.
    */
   public static void initTableReducerJob(String table,
-    Class<? extends TableReducer> reducer, Job job)
-  throws IOException {
+      Class<? extends TableReducer> reducer, Job job) throws IOException {
     initTableReducerJob(table, reducer, job, null);
   }
 
   /**
-   * Use this before submitting a TableReduce job. It will
-   * appropriately set up the JobConf.
-   *
-   * @param table  The output table.
-   * @param reducer  The reducer class to use.
-   * @param job  The current job to adjust.
-   * @param partitioner  Partitioner to use. Pass <code>null</code> to use
-   * default partitioner.
+   * Use this before submitting a TableReduce job. It will appropriately set up
+   * the JobConf.
+   * 
+   * @param table The output table.
+   * @param reducer The reducer class to use.
+   * @param job The current job to adjust.
+   * @param partitioner Partitioner to use. Pass <code>null</code> to use
+   *          default partitioner.
    * @throws IOException When determining the region count fails.
    */
   public static void initTableReducerJob(String table,
-    Class<? extends TableReducer> reducer, Job job,
-    Class partitioner) throws IOException {
+      Class<? extends TableReducer> reducer, Job job, Class partitioner)
+      throws IOException {
     initTableReducerJob(table, reducer, job, partitioner, null, null, null);
   }
 
   /**
-   * Use this before submitting a TableReduce job. It will
-   * appropriately set up the JobConf.
-   *
-   * @param table  The output table.
-   * @param reducer  The reducer class to use.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
-   * @param partitioner  Partitioner to use. Pass <code>null</code> to use
-   * default partitioner.
+   * Use this before submitting a TableReduce job. It will appropriately set up
+   * the JobConf.
+   * 
+   * @param table The output table.
+   * @param reducer The reducer class to use.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @param partitioner Partitioner to use. Pass <code>null</code> to use
+   *          default partitioner.
    * @param quorumAddress Distant cluster to write to; default is null for
-   * output to the cluster that is designated in <code>hbase-site.xml</code>.
-   * Set this String to the zookeeper ensemble of an alternate remote cluster
-   * when you would have the reduce write a cluster that is other than the
-   * default; e.g. copying tables between clusters, the source would be
-   * designated by <code>hbase-site.xml</code> and this param would have the
-   * ensemble address of the remote cluster.  The format to pass is particular.
-   * Pass <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
+   *          output to the cluster that is designated in
+   *          <code>hbase-site.xml</code>. Set this String to the zookeeper
+   *          ensemble of an alternate remote cluster when you would have the
+   *          reduce write a cluster that is other than the default; e.g.
+   *          copying tables between clusters, the source would be designated by
+   *          <code>hbase-site.xml</code> and this param would have the ensemble
+   *          address of the remote cluster. The format to pass is particular.
+   *          Pass
+   *          <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
    * </code> such as <code>server,server2,server3:2181:/hbase</code>.
    * @param serverClass redefined hbase.regionserver.class
    * @param serverImpl redefined hbase.regionserver.impl
    * @throws IOException When determining the region count fails.
    */
   public static void initTableReducerJob(String table,
-    Class<? extends TableReducer> reducer, Job job,
-    Class partitioner, String quorumAddress, String serverClass,
-    String serverImpl) throws IOException {
+      Class<? extends TableReducer> reducer, Job job, Class partitioner,
+      String quorumAddress, String serverClass, String serverImpl)
+      throws IOException {
     initTableReducerJob(table, reducer, job, partitioner, quorumAddress,
         serverClass, serverImpl, true);
   }
 
   /**
-   * Use this before submitting a TableReduce job. It will
-   * appropriately set up the JobConf.
-   *
-   * @param table  The output table.
-   * @param reducer  The reducer class to use.
-   * @param job  The current job to adjust.  Make sure the passed job is
-   * carrying all necessary HBase configuration.
-   * @param partitioner  Partitioner to use. Pass <code>null</code> to use
-   * default partitioner.
+   * Use this before submitting a TableReduce job. It will appropriately set up
+   * the JobConf.
+   * 
+   * @param table The output table.
+   * @param reducer The reducer class to use.
+   * @param job The current job to adjust. Make sure the passed job is carrying
+   *          all necessary HBase configuration.
+   * @param partitioner Partitioner to use. Pass <code>null</code> to use
+   *          default partitioner.
    * @param quorumAddress Distant cluster to write to; default is null for
-   * output to the cluster that is designated in <code>hbase-site.xml</code>.
-   * Set this String to the zookeeper ensemble of an alternate remote cluster
-   * when you would have the reduce write a cluster that is other than the
-   * default; e.g. copying tables between clusters, the source would be
-   * designated by <code>hbase-site.xml</code> and this param would have the
-   * ensemble address of the remote cluster.  The format to pass is particular.
-   * Pass <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
+   *          output to the cluster that is designated in
+   *          <code>hbase-site.xml</code>. Set this String to the zookeeper
+   *          ensemble of an alternate remote cluster when you would have the
+   *          reduce write a cluster that is other than the default; e.g.
+   *          copying tables between clusters, the source would be designated by
+   *          <code>hbase-site.xml</code> and this param would have the ensemble
+   *          address of the remote cluster. The format to pass is particular.
+   *          Pass
+   *          <code> &lt;hbase.zookeeper.quorum>:&lt;hbase.zookeeper.client.port>:&lt;zookeeper.znode.parent>
    * </code> such as <code>server,server2,server3:2181:/hbase</code>.
    * @param serverClass redefined hbase.regionserver.class
    * @param serverImpl redefined hbase.regionserver.impl
-   * @param addDependencyJars upload HBase jars and jars for any of the configured
-   *           job classes via the distributed cache (tmpjars).
+   * @param addDependencyJars upload HBase jars and jars for any of the
+   *          configured job classes via the distributed cache (tmpjars).
    * @throws IOException When determining the region count fails.
    */
   public static void initTableReducerJob(String table,
-    Class<? extends TableReducer> reducer, Job job,
-    Class partitioner, String quorumAddress, String serverClass,
-    String serverImpl, boolean addDependencyJars) throws IOException {
+      Class<? extends TableReducer> reducer, Job job, Class partitioner,
+      String quorumAddress, String serverClass, String serverImpl,
+      boolean addDependencyJars) throws IOException {
 
     Configuration conf = job.getConfiguration();
     HBaseConfiguration.addHbaseResources(conf);
     job.setOutputFormatClass(TableOutputFormat.class);
-    if (reducer != null) job.setReducerClass(reducer);
+    if (reducer != null)
+      job.setReducerClass(reducer);
     conf.set(TableOutputFormat.OUTPUT_TABLE, table);
     // If passed a quorum/ensemble address, pass it on to TableOutputFormat.
     if (quorumAddress != null) {
       // Calling this will validate the format
       ZKUtil.transformClusterKey(quorumAddress);
-      conf.set(TableOutputFormat.QUORUM_ADDRESS,quorumAddress);
+      conf.set(TableOutputFormat.QUORUM_ADDRESS, quorumAddress);
     }
     if (serverClass != null && serverImpl != null) {
       conf.set(TableOutputFormat.REGION_SERVER_CLASS, serverClass);
@@ -317,13 +379,13 @@
   /**
    * Ensures that the given number of reduce tasks for the given job
    * configuration does not exceed the number of regions for the given table.
-   *
-   * @param table  The table to get the region count for.
-   * @param job  The current job to adjust.
+   * 
+   * @param table The table to get the region count for.
+   * @param job The current job to adjust.
    * @throws IOException When retrieving the table details fails.
    */
   public static void limitNumReduceTasks(String table, Job job)
-  throws IOException {
+      throws IOException {
     HTable outputTable = new HTable(job.getConfiguration(), table);
     int regions = outputTable.getRegionsInfo().size();
     if (job.getNumReduceTasks() > regions)
@@ -333,13 +395,13 @@
   /**
    * Sets the number of reduce tasks for the given job configuration to the
    * number of regions the given table has.
-   *
-   * @param table  The table to get the region count for.
-   * @param job  The current job to adjust.
+   * 
+   * @param table The table to get the region count for.
+   * @param job The current job to adjust.
    * @throws IOException When retrieving the table details fails.
    */
   public static void setNumReduceTasks(String table, Job job)
-  throws IOException {
+      throws IOException {
     HTable outputTable = new HTable(job.getConfiguration(), table);
     int regions = outputTable.getRegionsInfo().size();
     job.setNumReduceTasks(regions);
@@ -349,80 +411,76 @@
    * Sets the number of rows to return and cache with each scanner iteration.
    * Higher caching values will enable faster mapreduce jobs at the expense of
    * requiring more heap to contain the cached rows.
-   *
+   * 
    * @param job The current job to adjust.
    * @param batchSize The number of rows to return in batch with each scanner
-   * iteration.
+   *          iteration.
    */
   public static void setScannerCaching(Job job, int batchSize) {
     job.getConfiguration().setInt("hbase.client.scanner.caching", batchSize);
   }
 
   /**
-   * Add the HBase dependency jars as well as jars for any of the configured
-   * job classes to the job configuration, so that JobClient will ship them
-   * to the cluster and add them to the DistributedCache.
+   * Add the HBase dependency jars as well as jars for any of the configured job
+   * classes to the job configuration, so that JobClient will ship them to the
+   * cluster and add them to the DistributedCache.
    */
   public static void addDependencyJars(Job job) throws IOException {
     try {
       addDependencyJars(job.getConfiguration(),
-          org.apache.zookeeper.ZooKeeper.class,
-          job.getMapOutputKeyClass(),
-          job.getMapOutputValueClass(),
-          job.getInputFormatClass(),
-          job.getOutputKeyClass(),
-          job.getOutputValueClass(),
-          job.getOutputFormatClass(),
-          job.getPartitionerClass(),
+          org.apache.zookeeper.ZooKeeper.class, job.getMapOutputKeyClass(),
+          job.getMapOutputValueClass(), job.getInputFormatClass(),
+          job.getOutputKeyClass(), job.getOutputValueClass(),
+          job.getOutputFormatClass(), job.getPartitionerClass(),
           job.getCombinerClass());
     } catch (ClassNotFoundException e) {
       throw new IOException(e);
-    }    
+    }
   }
-  
+
   /**
-   * Add the jars containing the given classes to the job's configuration
-   * such that JobClient will ship them to the cluster and add them to
-   * the DistributedCache.
+   * Add the jars containing the given classes to the job's configuration such
+   * that JobClient will ship them to the cluster and add them to the
+   * DistributedCache.
    */
-  public static void addDependencyJars(Configuration conf,
-      Class... classes) throws IOException {
+  public static void addDependencyJars(Configuration conf, Class... classes)
+      throws IOException {
 
     FileSystem localFs = FileSystem.getLocal(conf);
 
     Set<String> jars = new HashSet<String>();
 
     // Add jars that are already in the tmpjars variable
-    jars.addAll( conf.getStringCollection("tmpjars") );
+    jars.addAll(conf.getStringCollection("tmpjars"));
 
     // Add jars containing the specified classes
     for (Class clazz : classes) {
-      if (clazz == null) continue;
+      if (clazz == null)
+        continue;
 
       String pathStr = findContainingJar(clazz);
       if (pathStr == null) {
         LOG.warn("Could not find jar for class " + clazz +
-                 " in order to ship it to the cluster.");
+            " in order to ship it to the cluster.");
         continue;
       }
       Path path = new Path(pathStr);
       if (!localFs.exists(path)) {
-        LOG.warn("Could not validate jar file " + path + " for class "
-                 + clazz);
+        LOG.warn("Could not validate jar file " + path + " for class " + clazz);
         continue;
       }
       jars.add(path.makeQualified(localFs).toString());
     }
-    if (jars.isEmpty()) return;
+    if (jars.isEmpty())
+      return;
 
-    conf.set("tmpjars",
-             StringUtils.arrayToString(jars.toArray(new String[0])));
+    conf.set("tmpjars", StringUtils.arrayToString(jars.toArray(new String[0])));
   }
 
-  /** 
-   * Find a jar that contains a class of the same name, if any.
-   * It will return a jar file, even if that is not the first thing
-   * on the class path that has a class with the same name.
+  /**
+   * Find a jar that contains a class of the same name, if any. It will return a
+   * jar file, even if that is not the first thing on the class path that has a
+   * class with the same name.
    * 
    * This is shamelessly copied from JobConf
    * 
@@ -434,8 +492,8 @@
     ClassLoader loader = my_class.getClassLoader();
     String class_file = my_class.getName().replaceAll("\\.", "/") + ".class";
     try {
-      for(Enumeration itr = loader.getResources(class_file);
-          itr.hasMoreElements();) {
+      for (Enumeration itr = loader.getResources(class_file); itr
+          .hasMoreElements();) {
         URL url = (URL) itr.nextElement();
         if ("jar".equals(url.getProtocol())) {
           String toReturn = url.getPath();
@@ -459,5 +517,4 @@
     return null;
   }
 
-
 }
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormat.java	(revision 0)
@@ -0,0 +1,99 @@
+/**
+ * Copyright 2007 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapreduce;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Convert HBase tabular data from multiple input tables and scanners into a
+ * format that is consumable by Map/Reduce.
+ * 
+ * <p>
+ * Usage example
+ * </p>
+ * 
+ * <pre>
+ * Scan scan1 = new Scan();
+ * scan1.setStartRow(firstRow1);
+ * scan1.setStopRow(lastRow1);
+ * 
+ * Scan scan2 = new Scan();
+ * scan2.setStartRow(firstRow2);
+ * scan2.setStopRow(lastRow2);
+ * 
+ * MultiTableInputCollection mtic = new MultiTableInputCollection();
+ * mtic.Add(tableName1, scan1);
+ * mtic.Add(tableName2, scan2);
+ * 
+ * TableMapReduceUtil.initTableMapperJob(mtic, TableMapper.class, Text.class,
+ *     IntWritable.class, job);
+ * </pre>
+ */
+public class MultiTableInputFormat extends MultiTableInputFormatBase implements
+    Configurable {
+
+  private final Log LOG = LogFactory.getLog(MultiTableInputFormat.class);
+
+  /** Job parameter that specifies the input tables list. */
+  public static final String INPUT_TABLES = "hbase.mapreduce.inputtables";
+
+  /** The configuration. */
+  private Configuration conf = null;
+
+  /**
+   * Returns the current configuration.
+   * 
+   * @return The current configuration.
+   * @see org.apache.hadoop.conf.Configurable#getConf()
+   */
+  @Override
+  public Configuration getConf() {
+    return conf;
+  }
+
+  /**
+   * Sets the configuration. This is used to set the details for the table to be
+   * scanned.
+   * 
+   * @param configuration The configuration to set.
+   * @see org.apache.hadoop.conf.Configurable#setConf(org.apache.hadoop.conf.Configuration)
+   */
+  @Override
+  public void setConf(Configuration configuration) {
+    this.conf = configuration;
+    String[] inputTables = conf.getStrings(INPUT_TABLES);
+    MultiTableInputCollection tc = new MultiTableInputCollection();
+
+    for (int i = 0; i < inputTables.length; i = i + 2) {
+      try {
+        tc.Add(inputTables[i],
+            TableMapReduceUtil.convertStringToScan(inputTables[i + 1]));
+      } catch (IOException e) {
+        e.printStackTrace();
+      }
+    }
+    this.setInputTables(tc);
+  }
+}
\ No newline at end of file
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java	(revision 0)
@@ -0,0 +1,206 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+
+public abstract class MultiTableInputFormatBase extends
+    InputFormat<ImmutableBytesWritable, Result> {
+
+  final Log LOG = LogFactory.getLog(MultiTableInputFormatBase.class);
+
+  /** Holds the input table+scanner collection. */
+  private MultiTableInputCollection tables;
+
+  /** The reader scanning the table, can be a custom one. */
+  private TableRecordReader tableRecordReader = null;
+
+  /**
+   * Builds a TableRecordReader. If no TableRecordReader was provided, uses the
+   * default.
+   * 
+   * @param split The split to work with.
+   * @param context The current context.
+   * @return The newly created record reader.
+   * @throws IOException When creating the reader fails.
+   * @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(org.apache.hadoop.mapreduce.InputSplit,
+   *      org.apache.hadoop.mapreduce.TaskAttemptContext)
+   */
+  @Override
+  public RecordReader<ImmutableBytesWritable, Result> createRecordReader(
+      InputSplit split, TaskAttemptContext context) throws IOException {
+    TableSplit tSplit = (TableSplit) split;
+
+    if (tSplit.getTableName() == null) {
+      throw new IOException("Cannot create a record reader because of a"
+          + " previous error. Please look at the previous logs lines from"
+          + " the task's full log for more details.");
+    }
+    HTable table =
+        new HTable(context.getConfiguration(), tSplit.getTableName());
+
+    TableRecordReader trr = this.tableRecordReader;
+    // if no table record reader was provided use default
+    if (trr == null) {
+      trr = new TableRecordReader();
+    }
+    Scan sc = tSplit.getScan();
+    sc.setStartRow(tSplit.getStartRow());
+    sc.setStopRow(tSplit.getEndRow());
+    trr.setScan(sc);
+    trr.setHTable(table);
+    trr.init();
+    return trr;
+  }
+
+  /**
+   * Calculates the splits that will serve as input for the map tasks. The
+   * number of splits matches the number of regions in a table.
+   * 
+   * @param context The current job context.
+   * @return The list of input splits.
+   * @throws IOException When creating the list of splits fails.
+   * @see org.apache.hadoop.mapreduce.InputFormat#getSplits(org.apache.hadoop.mapreduce.JobContext)
+   */
+  @Override
+  public List<InputSplit> getSplits(JobContext context) throws IOException {
+    if (tables.isEmpty()) {
+      throw new IOException("No tables were provided.");
+    }
+    List<InputSplit> splits = new ArrayList<InputSplit>();
+
+    for (MultiTableInputCollection.TableInputConf tic : this.tables) {
+      HTable table = new HTable(context.getConfiguration(), tic.getTableName());
+      Pair<byte[][], byte[][]> keys = table.getStartEndKeys();
+      if (keys == null || keys.getFirst() == null ||
+          keys.getFirst().length == 0) {
+        throw new IOException("Expecting at least one region.");
+      }
+      int count = 0;
+
+      for (int i = 0; i < keys.getFirst().length; i++) {
+        if (!includeRegionInSplit(keys.getFirst()[i], keys.getSecond()[i])) {
+          continue;
+        }
+        String regionLocation =
+            table.getRegionLocation(keys.getFirst()[i]).getServerAddress()
+                .getHostname();
+        byte[] startRow = tic.getScan().getStartRow();
+        byte[] stopRow = tic.getScan().getStopRow();
+        // determine if the given start an stop key fall into the region
+        if ((startRow.length == 0 || keys.getSecond()[i].length == 0 || Bytes
+            .compareTo(startRow, keys.getSecond()[i]) < 0) &&
+            (stopRow.length == 0 || Bytes
+                .compareTo(stopRow, keys.getFirst()[i]) > 0)) {
+          byte[] splitStart =
+              startRow.length == 0 ||
+                  Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ? keys
+                  .getFirst()[i] : startRow;
+          byte[] splitStop =
+              (stopRow.length == 0 || Bytes.compareTo(keys.getSecond()[i],
+                  stopRow) <= 0) && keys.getSecond()[i].length > 0 ? keys
+                  .getSecond()[i] : stopRow;
+          InputSplit split =
+              new TableSplit(table.getTableName(), tic.getScan(), splitStart,
+                  splitStop, regionLocation);
+          splits.add(split);
+          if (LOG.isDebugEnabled())
+            LOG.debug("getSplits: split -> " + (count++) + " -> " + split);
+        }
+      }
+      table.close();
+    }
+    return splits;
+  }
+
+  /**
+   * 
+   * 
+   * Test if the given region is to be included in the InputSplit while
+   * splitting the regions of a table.
+   * <p>
+   * This optimization is effective when there is a specific reasoning to
+   * exclude an entire region from the M-R job, (and hence, not contributing to
+   * the InputSplit), given the start and end keys of the same. <br>
+   * Useful when we need to remember the last-processed top record and revisit
+   * the [last, current) interval for M-R processing, continuously. In addition
+   * to reducing InputSplits, reduces the load on the region server as well, due
+   * to the ordering of the keys. <br>
+   * <br>
+   * Note: It is possible that <code>endKey.length() == 0 </code> , for the last
+   * (recent) region. <br>
+   * Override this method, if you want to bulk exclude regions altogether from
+   * M-R. By default, no region is excluded( i.e. all regions are included).
+   * 
+   * 
+   * @param startKey Start key of the region
+   * @param endKey End key of the region
+   * @return true, if this region needs to be included as part of the input
+   *         (default).
+   * 
+   */
+  protected boolean includeRegionInSplit(final byte[] startKey,
+      final byte[] endKey) {
+    return true;
+  }
+
+  /**
+   * Allows subclasses to get the {@link MultiTableInputCollection}.
+   */
+  protected MultiTableInputCollection getInputTables() {
+    return this.tables;
+  }
+
+  /**
+   * Allows subclasses to set the collection of input tables and scanners to use
+   * 
+   * @param tables The {@link MultiTableInputCollection} containing the input
+   *          tables
+   */
+  protected void setInputTables(MultiTableInputCollection tables) {
+    this.tables = tables;
+  }
+
+  /**
+   * Allows subclasses to set the {@link TableRecordReader}.
+   * 
+   * @param tableRecordReader A different {@link TableRecordReader}
+   *          implementation.
+   */
+  protected void setTableRecordReader(TableRecordReader tableRecordReader) {
+    this.tableRecordReader = tableRecordReader;
+  }
+}
\ No newline at end of file
Index: src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputCollection.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputCollection.java	(revision 0)
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputCollection.java	(revision 0)
@@ -0,0 +1,119 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapreduce;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+
+import org.apache.hadoop.hbase.client.Scan;
+
+/**
+ * A collection of input tables and scanners to be used as the source data for a
+ * mapper
+ */
+public class MultiTableInputCollection implements
+    Iterable<MultiTableInputCollection.TableInputConf> {
+  /**
+   * An internal structure to hold table names + associated scanners
+   */
+  public class TableInputConf {
+    private String tableName = null;
+    private Scan scan = null;
+
+    /**
+     * The empty constructor
+     */
+    public TableInputConf() {
+    }
+
+    /**
+     * 
+     * @param tableName Input table name
+     * @param scan A Scanner associated with this table
+     */
+    public TableInputConf(String tableName, Scan scan) {
+      this.tableName = tableName;
+      this.scan = scan;
+    }
+
+    /**
+     * Returns the table name
+     * 
+     * @return The table name
+     */
+    public String getTableName() {
+      return this.tableName;
+    }
+
+    /**
+     * Sets the table name
+     * 
+     * @param tableName The table name to set
+     */
+    public void setTableName(String tableName) {
+      this.tableName = tableName;
+    }
+
+    /**
+     * Returns the scanner
+     * 
+     * @return The scanner
+     */
+    public Scan getScan() {
+      return this.scan;
+    }
+
+    /**
+     * Sets the scanner
+     * 
+     * @param tableName The scanner to set
+     */
+    public void setScan(Scan scan) {
+      this.scan = scan;
+    }
+  }
+
+  private ArrayList<TableInputConf> tables = new ArrayList<TableInputConf>();
+
+  /**
+   * Add new table and scanner pairs to the collection
+   * 
+   * @param tableName An input table name
+   * @param scan An associated scanner
+   */
+  public void Add(String tableName, Scan scan) {
+    TableInputConf tic = new TableInputConf(tableName, scan);
+    tables.add(tic);
+  }
+
+  @Override
+  public Iterator<TableInputConf> iterator() {
+    return tables.iterator();
+  }
+
+  /**
+   * Checks if the collection is empty
+   * 
+   * @return true if empty false otherwise
+   */
+  public boolean isEmpty() {
+    return tables.isEmpty();
+  }
+}
Index: src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormat.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormat.java	(revision 0)
+++ src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormat.java	(revision 0)
@@ -0,0 +1,371 @@
+/**
+ * Copyright 2007 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapreduce;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Map;
+import java.util.NavigableMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Tests various scan start and stop row scenarios. This is set in a scan and
+ * tested in a MapReduce job to see if that is handed over and done properly
+ * too.
+ */
+public class TestMultiTableInputFormat {
+
+  static final Log LOG = LogFactory.getLog(TestMultiTableInputFormat.class);
+  static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+
+  static final String TABLE_NAME = "scantest";
+  static final byte[] INPUT_FAMILY = Bytes.toBytes("contents");
+  static final String KEY_STARTROW = "startRow";
+  static final String KEY_LASTROW = "stpRow";
+
+  // private static MultiTableInputCollection mtic = new
+  // MultiTableInputCollection();
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    // switch TIF to log at DEBUG level
+    TEST_UTIL.enableDebug(MultiTableInputFormat.class);
+    TEST_UTIL.enableDebug(MultiTableInputFormatBase.class);
+    // start mini hbase cluster
+    TEST_UTIL.startMiniCluster(3);
+    // create and fill table
+    for (int i = 1; i <= 3; i++) {
+      HTable table =
+          TEST_UTIL.createTable(Bytes.toBytes(TABLE_NAME + String.valueOf(i)),
+              INPUT_FAMILY);
+      TEST_UTIL.createMultiRegions(table, INPUT_FAMILY);
+      TEST_UTIL.loadTable(table, INPUT_FAMILY);
+    }
+    // start MR cluster
+    TEST_UTIL.startMiniMapReduceCluster();
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniMapReduceCluster();
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    // nothing
+  }
+
+  /**
+   * @throws java.lang.Exception
+   */
+  @After
+  public void tearDown() throws Exception {
+    Configuration c = TEST_UTIL.getConfiguration();
+    FileUtil.fullyDelete(new File(c.get("hadoop.tmp.dir")));
+  }
+
+  /**
+   * Pass the key and value to reduce.
+   */
+  public static class ScanMapper extends
+      TableMapper<ImmutableBytesWritable, ImmutableBytesWritable> {
+    /**
+     * Pass the key and value to reduce.
+     * 
+     * @param key The key, here "aaa", "aab" etc.
+     * @param value The value is the same as the key.
+     * @param context The task context.
+     * @throws IOException When reading the rows fails.
+     */
+    @Override
+    public void map(ImmutableBytesWritable key, Result value, Context context)
+        throws IOException, InterruptedException {
+      if (value.size() != 1) {
+        throw new IOException("There should only be one input column");
+      }
+      Map<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>> cf =
+          value.getMap();
+      if (!cf.containsKey(INPUT_FAMILY)) {
+        throw new IOException("Wrong input columns. Missing: '" +
+            Bytes.toString(INPUT_FAMILY) + "'.");
+      }
+      String val = Bytes.toStringBinary(value.getValue(INPUT_FAMILY, null));
+      LOG.info("map: key -> " + Bytes.toStringBinary(key.get()) +
+          ", value -> " + val);
+      context.write(key, key);
+    }
+
+  }
+
+  /**
+   * Checks the last and first key seen against the scanner boundaries.
+   */
+  public static class ScanReducer
+      extends
+      Reducer<ImmutableBytesWritable, ImmutableBytesWritable, NullWritable, NullWritable> {
+    private String first = null;
+    private String last = null;
+
+    protected void reduce(ImmutableBytesWritable key,
+        Iterable<ImmutableBytesWritable> values, Context context)
+        throws IOException, InterruptedException {
+      int count = 0;
+      for (ImmutableBytesWritable value : values) {
+        String val = Bytes.toStringBinary(value.get());
+        LOG.info("reduce: key[" + count + "] -> " +
+            Bytes.toStringBinary(key.get()) + ", value -> " + val);
+        if (first == null)
+          first = val;
+        last = val;
+        count++;
+      }
+      assertEquals(4, count);
+    }
+
+    protected void cleanup(Context context) throws IOException,
+        InterruptedException {
+      Configuration c = context.getConfiguration();
+      String startRow = c.get(KEY_STARTROW);
+      String lastRow = c.get(KEY_LASTROW);
+      LOG.info("cleanup: first -> \"" + first + "\", start row -> \"" +
+          startRow + "\"");
+      LOG.info("cleanup: last -> \"" + last + "\", last row -> \"" + lastRow +
+          "\"");
+      if (startRow != null && startRow.length() > 0) {
+        assertEquals(startRow, first);
+      }
+      if (lastRow != null && lastRow.length() > 0) {
+        assertEquals(lastRow, last);
+      }
+    }
+
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanEmptyToEmpty() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan(null, null, null);
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanEmptyToAPP() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan(null, "app", "apo");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanEmptyToBBA() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan(null, "bba", "baz");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanEmptyToBBB() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan(null, "bbb", "bba");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanEmptyToOPP() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan(null, "opp", "opo");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanOBBToOPP() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("obb", "opp", "opo");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanOBBToQPP() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("obb", "qpp", "qpo");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanOPPToEmpty() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("opp", null, "zzz");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanYYXToEmpty() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("yyx", null, "zzz");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanYYYToEmpty() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("yyy", null, "zzz");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  @Test
+  public void testScanYZYToEmpty() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    testScan("yzy", null, "zzz");
+  }
+
+  /**
+   * Tests a MR scan using specific start and stop rows.
+   * 
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   */
+  private void testScan(String start, String stop, String last)
+      throws IOException, InterruptedException, ClassNotFoundException {
+    String jobName =
+        "Scan" + (start != null ? start.toUpperCase() : "Empty") + "To" +
+            (stop != null ? stop.toUpperCase() : "Empty");
+    LOG.info("Before map/reduce startup - job " + jobName);
+    Configuration c = new Configuration(TEST_UTIL.getConfiguration());
+    Scan scan = new Scan();
+    scan.addFamily(INPUT_FAMILY);
+    if (start != null) {
+      scan.setStartRow(Bytes.toBytes(start));
+    }
+    c.set(KEY_STARTROW, start != null ? start : "");
+    if (stop != null) {
+      scan.setStopRow(Bytes.toBytes(stop));
+    }
+    c.set(KEY_LASTROW, last != null ? last : "");
+    LOG.info("scan before: " + scan);
+    Job job = new Job(c, jobName);
+
+    MultiTableInputCollection mtic = new MultiTableInputCollection();
+    mtic.Add(TABLE_NAME + "1", scan);
+    mtic.Add(TABLE_NAME + "1", scan);
+    mtic.Add(TABLE_NAME + "2", scan);
+    mtic.Add(TABLE_NAME + "3", scan);
+
+    TableMapReduceUtil.initTableMapperJob(mtic, ScanMapper.class,
+        ImmutableBytesWritable.class, ImmutableBytesWritable.class, job);
+    job.setReducerClass(ScanReducer.class);
+    job.setNumReduceTasks(1); // one to get final "first" and "last" key
+    FileOutputFormat.setOutputPath(job, new Path(job.getJobName()));
+    LOG.info("Started " + job.getJobName());
+    job.waitForCompletion(true);
+    assertTrue(job.isSuccessful());
+    LOG.info("After map/reduce completion - job " + jobName);
+  }
+}
\ No newline at end of file
