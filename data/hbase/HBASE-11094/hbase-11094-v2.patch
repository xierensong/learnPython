diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/RegionServerConfigMismatchException.java hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/RegionServerConfigMismatchException.java
new file mode 100644
index 0000000..5e1427c
--- /dev/null
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/exceptions/RegionServerConfigMismatchException.java
@@ -0,0 +1,37 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.exceptions;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.NotServingRegionException;
+
+/**
+ * Thrown when current region server has different configuration settings required by caller
+ * This exception happens during rolling restart/upgrade where caller requires a specific 
+ * configuration settings while some region server havn't been restarted/upgraded yet
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class RegionServerConfigMismatchException extends NotServingRegionException {
+  private static final long serialVersionUID = -7232903522610558392L;
+
+  public RegionServerConfigMismatchException(String message) {
+    super(message);
+  }
+}
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index aca8b86..f875e7e 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -1596,7 +1596,7 @@ public final class ProtobufUtil {
   public static void openRegion(final AdminService.BlockingInterface admin,
       ServerName server, final HRegionInfo region) throws IOException {
     OpenRegionRequest request =
-      RequestConverter.buildOpenRegionRequest(server, region, -1, null);
+      RequestConverter.buildOpenRegionRequest(server, region, -1, null, null);
     try {
       admin.openRegion(null, request);
     } catch (ServiceException se) {
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
index a1b548d..c572d78 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
@@ -21,6 +21,8 @@ import java.io.IOException;
 import java.util.List;
 
 import com.google.protobuf.HBaseZeroCopyByteString;
+
+import org.apache.commons.configuration.Configuration;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.CellScannable;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
@@ -702,17 +704,18 @@ public final class RequestConverter {
   * Create a protocol buffer OpenRegionRequest to open a list of regions
   *
   * @param regionOpenInfos info of a list of regions to open
+  * @param isOpenForReplay
   * @return a protocol buffer OpenRegionRequest
   */
  public static OpenRegionRequest
      buildOpenRegionRequest(final List<Triple<HRegionInfo, Integer,
-         List<ServerName>>> regionOpenInfos) {
+         List<ServerName>>> regionOpenInfos, Boolean isOpenForReplay) {
    OpenRegionRequest.Builder builder = OpenRegionRequest.newBuilder();
    for (Triple<HRegionInfo, Integer, List<ServerName>> regionOpenInfo: regionOpenInfos) {
      Integer second = regionOpenInfo.getSecond();
      int versionOfOfflineNode = second == null ? -1 : second.intValue();
-     builder.addOpenInfo(buildRegionOpenInfo(
-       regionOpenInfo.getFirst(), versionOfOfflineNode, regionOpenInfo.getThird()));
+     builder.addOpenInfo(buildRegionOpenInfo(regionOpenInfo.getFirst(), versionOfOfflineNode, 
+       regionOpenInfo.getThird(), isOpenForReplay));
    }
    return builder.build();
  }
@@ -724,12 +727,15 @@ public final class RequestConverter {
   * @param region the region to open
   * @param versionOfOfflineNode that needs to be present in the offline node
   * @param favoredNodes
+  * @param isOpenForReplay
   * @return a protocol buffer OpenRegionRequest
   */
  public static OpenRegionRequest buildOpenRegionRequest(ServerName server,
-     final HRegionInfo region, final int versionOfOfflineNode, List<ServerName> favoredNodes) {
+     final HRegionInfo region, final int versionOfOfflineNode, List<ServerName> favoredNodes,
+     Boolean isOpenForReplay) {
    OpenRegionRequest.Builder builder = OpenRegionRequest.newBuilder();
-   builder.addOpenInfo(buildRegionOpenInfo(region, versionOfOfflineNode, favoredNodes));
+   builder.addOpenInfo(buildRegionOpenInfo(region, versionOfOfflineNode, favoredNodes, 
+     isOpenForReplay));
    if (server != null) {
      builder.setServerStartCode(server.getStartcode());
    }
@@ -1493,7 +1499,7 @@ public final class RequestConverter {
    */
   private static RegionOpenInfo buildRegionOpenInfo(
       final HRegionInfo region, final int versionOfOfflineNode,
-      final List<ServerName> favoredNodes) {
+      final List<ServerName> favoredNodes, Boolean isOpenForReplay) {
     RegionOpenInfo.Builder builder = RegionOpenInfo.newBuilder();
     builder.setRegion(HRegionInfo.convert(region));
     if (versionOfOfflineNode >= 0) {
@@ -1504,6 +1510,9 @@ public final class RequestConverter {
         builder.addFavoredNodes(ProtobufUtil.toServerName(server));
       }
     }
+    if(isOpenForReplay != null) {
+      builder.setIsOpenForDistributedLogReplay(isOpenForReplay);
+    }
     return builder.build();
   }
 }
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
index 0ad10ad..5426586 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
@@ -4032,6 +4032,24 @@ public final class AdminProtos {
        */
       org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getFavoredNodesOrBuilder(
           int index);
+
+      // optional bool isOpenForDistributedLogReplay = 4;
+      /**
+       * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+       *
+       * <pre>
+       * open region for distributedLogReplay
+       * </pre>
+       */
+      boolean hasIsOpenForDistributedLogReplay();
+      /**
+       * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+       *
+       * <pre>
+       * open region for distributedLogReplay
+       * </pre>
+       */
+      boolean getIsOpenForDistributedLogReplay();
     }
     /**
      * Protobuf type {@code OpenRegionRequest.RegionOpenInfo}
@@ -4110,6 +4128,11 @@ public final class AdminProtos {
                 favoredNodes_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.PARSER, extensionRegistry));
                 break;
               }
+              case 32: {
+                bitField0_ |= 0x00000004;
+                isOpenForDistributedLogReplay_ = input.readBool();
+                break;
+              }
             }
           }
         } catch (com.google.protobuf.InvalidProtocolBufferException e) {
@@ -4227,10 +4250,35 @@ public final class AdminProtos {
         return favoredNodes_.get(index);
       }
 
+      // optional bool isOpenForDistributedLogReplay = 4;
+      public static final int ISOPENFORDISTRIBUTEDLOGREPLAY_FIELD_NUMBER = 4;
+      private boolean isOpenForDistributedLogReplay_;
+      /**
+       * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+       *
+       * <pre>
+       * open region for distributedLogReplay
+       * </pre>
+       */
+      public boolean hasIsOpenForDistributedLogReplay() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      /**
+       * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+       *
+       * <pre>
+       * open region for distributedLogReplay
+       * </pre>
+       */
+      public boolean getIsOpenForDistributedLogReplay() {
+        return isOpenForDistributedLogReplay_;
+      }
+
       private void initFields() {
         region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
         versionOfOfflineNode_ = 0;
         favoredNodes_ = java.util.Collections.emptyList();
+        isOpenForDistributedLogReplay_ = false;
       }
       private byte memoizedIsInitialized = -1;
       public final boolean isInitialized() {
@@ -4267,6 +4315,9 @@ public final class AdminProtos {
         for (int i = 0; i < favoredNodes_.size(); i++) {
           output.writeMessage(3, favoredNodes_.get(i));
         }
+        if (((bitField0_ & 0x00000004) == 0x00000004)) {
+          output.writeBool(4, isOpenForDistributedLogReplay_);
+        }
         getUnknownFields().writeTo(output);
       }
 
@@ -4288,6 +4339,10 @@ public final class AdminProtos {
           size += com.google.protobuf.CodedOutputStream
             .computeMessageSize(3, favoredNodes_.get(i));
         }
+        if (((bitField0_ & 0x00000004) == 0x00000004)) {
+          size += com.google.protobuf.CodedOutputStream
+            .computeBoolSize(4, isOpenForDistributedLogReplay_);
+        }
         size += getUnknownFields().getSerializedSize();
         memoizedSerializedSize = size;
         return size;
@@ -4323,6 +4378,11 @@ public final class AdminProtos {
         }
         result = result && getFavoredNodesList()
             .equals(other.getFavoredNodesList());
+        result = result && (hasIsOpenForDistributedLogReplay() == other.hasIsOpenForDistributedLogReplay());
+        if (hasIsOpenForDistributedLogReplay()) {
+          result = result && (getIsOpenForDistributedLogReplay()
+              == other.getIsOpenForDistributedLogReplay());
+        }
         result = result &&
             getUnknownFields().equals(other.getUnknownFields());
         return result;
@@ -4348,6 +4408,10 @@ public final class AdminProtos {
           hash = (37 * hash) + FAVORED_NODES_FIELD_NUMBER;
           hash = (53 * hash) + getFavoredNodesList().hashCode();
         }
+        if (hasIsOpenForDistributedLogReplay()) {
+          hash = (37 * hash) + ISOPENFORDISTRIBUTEDLOGREPLAY_FIELD_NUMBER;
+          hash = (53 * hash) + hashBoolean(getIsOpenForDistributedLogReplay());
+        }
         hash = (29 * hash) + getUnknownFields().hashCode();
         memoizedHashCode = hash;
         return hash;
@@ -4473,6 +4537,8 @@ public final class AdminProtos {
           } else {
             favoredNodesBuilder_.clear();
           }
+          isOpenForDistributedLogReplay_ = false;
+          bitField0_ = (bitField0_ & ~0x00000008);
           return this;
         }
 
@@ -4522,6 +4588,10 @@ public final class AdminProtos {
           } else {
             result.favoredNodes_ = favoredNodesBuilder_.build();
           }
+          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+            to_bitField0_ |= 0x00000004;
+          }
+          result.isOpenForDistributedLogReplay_ = isOpenForDistributedLogReplay_;
           result.bitField0_ = to_bitField0_;
           onBuilt();
           return result;
@@ -4570,6 +4640,9 @@ public final class AdminProtos {
               }
             }
           }
+          if (other.hasIsOpenForDistributedLogReplay()) {
+            setIsOpenForDistributedLogReplay(other.getIsOpenForDistributedLogReplay());
+          }
           this.mergeUnknownFields(other.getUnknownFields());
           return this;
         }
@@ -5001,6 +5074,55 @@ public final class AdminProtos {
           return favoredNodesBuilder_;
         }
 
+        // optional bool isOpenForDistributedLogReplay = 4;
+        private boolean isOpenForDistributedLogReplay_ ;
+        /**
+         * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+         *
+         * <pre>
+         * open region for distributedLogReplay
+         * </pre>
+         */
+        public boolean hasIsOpenForDistributedLogReplay() {
+          return ((bitField0_ & 0x00000008) == 0x00000008);
+        }
+        /**
+         * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+         *
+         * <pre>
+         * open region for distributedLogReplay
+         * </pre>
+         */
+        public boolean getIsOpenForDistributedLogReplay() {
+          return isOpenForDistributedLogReplay_;
+        }
+        /**
+         * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+         *
+         * <pre>
+         * open region for distributedLogReplay
+         * </pre>
+         */
+        public Builder setIsOpenForDistributedLogReplay(boolean value) {
+          bitField0_ |= 0x00000008;
+          isOpenForDistributedLogReplay_ = value;
+          onChanged();
+          return this;
+        }
+        /**
+         * <code>optional bool isOpenForDistributedLogReplay = 4;</code>
+         *
+         * <pre>
+         * open region for distributedLogReplay
+         * </pre>
+         */
+        public Builder clearIsOpenForDistributedLogReplay() {
+          bitField0_ = (bitField0_ & ~0x00000008);
+          isOpenForDistributedLogReplay_ = false;
+          onChanged();
+          return this;
+        }
+
         // @@protoc_insertion_point(builder_scope:OpenRegionRequest.RegionOpenInfo)
       }
 
@@ -21166,12 +21288,13 @@ public final class AdminProtos {
       "FileResponse\022\022\n\nstore_file\030\001 \003(\t\"\030\n\026GetO" +
       "nlineRegionRequest\";\n\027GetOnlineRegionRes" +
       "ponse\022 \n\013region_info\030\001 \003(\0132\013.RegionInfo\"" +
-      "\326\001\n\021OpenRegionRequest\0224\n\topen_info\030\001 \003(\013" +
+      "\376\001\n\021OpenRegionRequest\0224\n\topen_info\030\001 \003(\013" +
       "2!.OpenRegionRequest.RegionOpenInfo\022\027\n\017s" +
-      "erverStartCode\030\002 \001(\004\032r\n\016RegionOpenInfo\022\033" +
-      "\n\006region\030\001 \002(\0132\013.RegionInfo\022\037\n\027version_o" +
-      "f_offline_node\030\002 \001(\r\022\"\n\rfavored_nodes\030\003 " +
-      "\003(\0132\013.ServerName\"\235\001\n\022OpenRegionResponse\022",
+      "erverStartCode\030\002 \001(\004\032\231\001\n\016RegionOpenInfo\022" +
+      "\033\n\006region\030\001 \002(\0132\013.RegionInfo\022\037\n\027version_" +
+      "of_offline_node\030\002 \001(\r\022\"\n\rfavored_nodes\030\003" +
+      " \003(\0132\013.ServerName\022%\n\035isOpenForDistribute",
+      "dLogReplay\030\004 \001(\010\"\235\001\n\022OpenRegionResponse\022" +
       "=\n\ropening_state\030\001 \003(\0162&.OpenRegionRespo" +
       "nse.RegionOpeningState\"H\n\022RegionOpeningS" +
       "tate\022\n\n\006OPENED\020\000\022\022\n\016ALREADY_OPENED\020\001\022\022\n\016" +
@@ -21180,8 +21303,8 @@ public final class AdminProtos {
       "sion_of_closing_node\030\002 \001(\r\022\036\n\020transition" +
       "_in_ZK\030\003 \001(\010:\004true\022\'\n\022destination_server" +
       "\030\004 \001(\0132\013.ServerName\022\027\n\017serverStartCode\030\005" +
-      " \001(\004\"%\n\023CloseRegionResponse\022\016\n\006closed\030\001 " +
-      "\002(\010\"P\n\022FlushRegionRequest\022 \n\006region\030\001 \002(",
+      " \001(\004\"%\n\023CloseRegionResponse\022\016\n\006closed\030\001 ",
+      "\002(\010\"P\n\022FlushRegionRequest\022 \n\006region\030\001 \002(" +
       "\0132\020.RegionSpecifier\022\030\n\020if_older_than_ts\030" +
       "\002 \001(\004\"?\n\023FlushRegionResponse\022\027\n\017last_flu" +
       "sh_time\030\001 \002(\004\022\017\n\007flushed\030\002 \001(\010\"K\n\022SplitR" +
@@ -21190,8 +21313,8 @@ public final class AdminProtos {
       "onResponse\"W\n\024CompactRegionRequest\022 \n\006re" +
       "gion\030\001 \002(\0132\020.RegionSpecifier\022\r\n\005major\030\002 " +
       "\001(\010\022\016\n\006family\030\003 \001(\014\"\027\n\025CompactRegionResp" +
-      "onse\"\262\001\n\031UpdateFavoredNodesRequest\022@\n\013up" +
-      "date_info\030\001 \003(\0132+.UpdateFavoredNodesRequ",
+      "onse\"\262\001\n\031UpdateFavoredNodesRequest\022@\n\013up",
+      "date_info\030\001 \003(\0132+.UpdateFavoredNodesRequ" +
       "est.RegionUpdateInfo\032S\n\020RegionUpdateInfo" +
       "\022\033\n\006region\030\001 \002(\0132\013.RegionInfo\022\"\n\rfavored" +
       "_nodes\030\002 \003(\0132\013.ServerName\".\n\032UpdateFavor" +
@@ -21200,8 +21323,8 @@ public final class AdminProtos {
       "ionSpecifier\022\"\n\010region_b\030\002 \002(\0132\020.RegionS" +
       "pecifier\022\027\n\010forcible\030\003 \001(\010:\005false\"\026\n\024Mer" +
       "geRegionsResponse\"X\n\010WALEntry\022\024\n\003key\030\001 \002" +
-      "(\0132\007.WALKey\022\027\n\017key_value_bytes\030\002 \003(\014\022\035\n\025" +
-      "associated_cell_count\030\003 \001(\005\"4\n\030Replicate",
+      "(\0132\007.WALKey\022\027\n\017key_value_bytes\030\002 \003(\014\022\035\n\025",
+      "associated_cell_count\030\003 \001(\005\"4\n\030Replicate" +
       "WALEntryRequest\022\030\n\005entry\030\001 \003(\0132\t.WALEntr" +
       "y\"\033\n\031ReplicateWALEntryResponse\"\026\n\024RollWA" +
       "LWriterRequest\"0\n\025RollWALWriterResponse\022" +
@@ -21210,8 +21333,8 @@ public final class AdminProtos {
       "nse\"\026\n\024GetServerInfoRequest\"B\n\nServerInf" +
       "o\022 \n\013server_name\030\001 \002(\0132\013.ServerName\022\022\n\nw" +
       "ebui_port\030\002 \001(\r\"9\n\025GetServerInfoResponse" +
-      "\022 \n\013server_info\030\001 \002(\0132\013.ServerInfo2\306\007\n\014A" +
-      "dminService\022>\n\rGetRegionInfo\022\025.GetRegion",
+      "\022 \n\013server_info\030\001 \002(\0132\013.ServerInfo2\306\007\n\014A",
+      "dminService\022>\n\rGetRegionInfo\022\025.GetRegion" +
       "InfoRequest\032\026.GetRegionInfoResponse\022;\n\014G" +
       "etStoreFile\022\024.GetStoreFileRequest\032\025.GetS" +
       "toreFileResponse\022D\n\017GetOnlineRegion\022\027.Ge" +
@@ -21220,8 +21343,8 @@ public final class AdminProtos {
       "\032\023.OpenRegionResponse\0228\n\013CloseRegion\022\023.C" +
       "loseRegionRequest\032\024.CloseRegionResponse\022" +
       "8\n\013FlushRegion\022\023.FlushRegionRequest\032\024.Fl" +
-      "ushRegionResponse\0228\n\013SplitRegion\022\023.Split" +
-      "RegionRequest\032\024.SplitRegionResponse\022>\n\rC",
+      "ushRegionResponse\0228\n\013SplitRegion\022\023.Split",
+      "RegionRequest\032\024.SplitRegionResponse\022>\n\rC" +
       "ompactRegion\022\025.CompactRegionRequest\032\026.Co" +
       "mpactRegionResponse\022;\n\014MergeRegions\022\024.Me" +
       "rgeRegionsRequest\032\025.MergeRegionsResponse" +
@@ -21230,8 +21353,8 @@ public final class AdminProtos {
       "Replay\022\031.ReplicateWALEntryRequest\032\032.Repl" +
       "icateWALEntryResponse\022>\n\rRollWALWriter\022\025" +
       ".RollWALWriterRequest\032\026.RollWALWriterRes" +
-      "ponse\022>\n\rGetServerInfo\022\025.GetServerInfoRe" +
-      "quest\032\026.GetServerInfoResponse\0225\n\nStopSer",
+      "ponse\022>\n\rGetServerInfo\022\025.GetServerInfoRe",
+      "quest\032\026.GetServerInfoResponse\0225\n\nStopSer" +
       "ver\022\022.StopServerRequest\032\023.StopServerResp" +
       "onse\022M\n\022UpdateFavoredNodes\022\032.UpdateFavor" +
       "edNodesRequest\032\033.UpdateFavoredNodesRespo" +
@@ -21290,7 +21413,7 @@ public final class AdminProtos {
           internal_static_OpenRegionRequest_RegionOpenInfo_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_OpenRegionRequest_RegionOpenInfo_descriptor,
-              new java.lang.String[] { "Region", "VersionOfOfflineNode", "FavoredNodes", });
+              new java.lang.String[] { "Region", "VersionOfOfflineNode", "FavoredNodes", "IsOpenForDistributedLogReplay", });
           internal_static_OpenRegionResponse_descriptor =
             getDescriptor().getMessageTypes().get(7);
           internal_static_OpenRegionResponse_fieldAccessorTable = new
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/MapReduceProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/MapReduceProtos.java
index 85e4816..eeab45c 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/MapReduceProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/MapReduceProtos.java
@@ -720,53 +720,53 @@ public final class MapReduceProtos {
   public interface TableSnapshotRegionSplitOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
 
-    // optional .TableSchema table = 1;
+    // repeated string locations = 2;
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>repeated string locations = 2;</code>
      */
-    boolean hasTable();
+    java.util.List<java.lang.String>
+    getLocationsList();
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>repeated string locations = 2;</code>
      */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema getTable();
+    int getLocationsCount();
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>repeated string locations = 2;</code>
      */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder getTableOrBuilder();
-
-    // optional .RegionInfo region = 2;
+    java.lang.String getLocations(int index);
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>repeated string locations = 2;</code>
      */
-    boolean hasRegion();
+    com.google.protobuf.ByteString
+        getLocationsBytes(int index);
+
+    // optional .TableSchema table = 3;
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegion();
+    boolean hasTable();
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionOrBuilder();
-
-    // repeated string locations = 3;
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema getTable();
     /**
-     * <code>repeated string locations = 3;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
-    java.util.List<java.lang.String>
-    getLocationsList();
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder getTableOrBuilder();
+
+    // optional .RegionInfo region = 4;
     /**
-     * <code>repeated string locations = 3;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
-    int getLocationsCount();
+    boolean hasRegion();
     /**
-     * <code>repeated string locations = 3;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
-    java.lang.String getLocations(int index);
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegion();
     /**
-     * <code>repeated string locations = 3;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
-    com.google.protobuf.ByteString
-        getLocationsBytes(int index);
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionOrBuilder();
   }
   /**
    * Protobuf type {@code TableSnapshotRegionSplit}
@@ -819,7 +819,15 @@ public final class MapReduceProtos {
               }
               break;
             }
-            case 10: {
+            case 18: {
+              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
+                locations_ = new com.google.protobuf.LazyStringArrayList();
+                mutable_bitField0_ |= 0x00000001;
+              }
+              locations_.add(input.readBytes());
+              break;
+            }
+            case 26: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.Builder subBuilder = null;
               if (((bitField0_ & 0x00000001) == 0x00000001)) {
                 subBuilder = table_.toBuilder();
@@ -832,7 +840,7 @@ public final class MapReduceProtos {
               bitField0_ |= 0x00000001;
               break;
             }
-            case 18: {
+            case 34: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder subBuilder = null;
               if (((bitField0_ & 0x00000002) == 0x00000002)) {
                 subBuilder = region_.toBuilder();
@@ -845,14 +853,6 @@ public final class MapReduceProtos {
               bitField0_ |= 0x00000002;
               break;
             }
-            case 26: {
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-                locations_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              locations_.add(input.readBytes());
-              break;
-            }
           }
         }
       } catch (com.google.protobuf.InvalidProtocolBufferException e) {
@@ -861,7 +861,7 @@ public final class MapReduceProtos {
         throw new com.google.protobuf.InvalidProtocolBufferException(
             e.getMessage()).setUnfinishedMessage(this);
       } finally {
-        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
+        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
           locations_ = new com.google.protobuf.UnmodifiableLazyStringList(locations_);
         }
         this.unknownFields = unknownFields.build();
@@ -896,84 +896,84 @@ public final class MapReduceProtos {
     }
 
     private int bitField0_;
-    // optional .TableSchema table = 1;
-    public static final int TABLE_FIELD_NUMBER = 1;
+    // repeated string locations = 2;
+    public static final int LOCATIONS_FIELD_NUMBER = 2;
+    private com.google.protobuf.LazyStringList locations_;
+    /**
+     * <code>repeated string locations = 2;</code>
+     */
+    public java.util.List<java.lang.String>
+        getLocationsList() {
+      return locations_;
+    }
+    /**
+     * <code>repeated string locations = 2;</code>
+     */
+    public int getLocationsCount() {
+      return locations_.size();
+    }
+    /**
+     * <code>repeated string locations = 2;</code>
+     */
+    public java.lang.String getLocations(int index) {
+      return locations_.get(index);
+    }
+    /**
+     * <code>repeated string locations = 2;</code>
+     */
+    public com.google.protobuf.ByteString
+        getLocationsBytes(int index) {
+      return locations_.getByteString(index);
+    }
+
+    // optional .TableSchema table = 3;
+    public static final int TABLE_FIELD_NUMBER = 3;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema table_;
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
     public boolean hasTable() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema getTable() {
       return table_;
     }
     /**
-     * <code>optional .TableSchema table = 1;</code>
+     * <code>optional .TableSchema table = 3;</code>
      */
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder getTableOrBuilder() {
       return table_;
     }
 
-    // optional .RegionInfo region = 2;
-    public static final int REGION_FIELD_NUMBER = 2;
+    // optional .RegionInfo region = 4;
+    public static final int REGION_FIELD_NUMBER = 4;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo region_;
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
     public boolean hasRegion() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegion() {
       return region_;
     }
     /**
-     * <code>optional .RegionInfo region = 2;</code>
+     * <code>optional .RegionInfo region = 4;</code>
      */
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionOrBuilder() {
       return region_;
     }
 
-    // repeated string locations = 3;
-    public static final int LOCATIONS_FIELD_NUMBER = 3;
-    private com.google.protobuf.LazyStringList locations_;
-    /**
-     * <code>repeated string locations = 3;</code>
-     */
-    public java.util.List<java.lang.String>
-        getLocationsList() {
-      return locations_;
-    }
-    /**
-     * <code>repeated string locations = 3;</code>
-     */
-    public int getLocationsCount() {
-      return locations_.size();
-    }
-    /**
-     * <code>repeated string locations = 3;</code>
-     */
-    public java.lang.String getLocations(int index) {
-      return locations_.get(index);
-    }
-    /**
-     * <code>repeated string locations = 3;</code>
-     */
-    public com.google.protobuf.ByteString
-        getLocationsBytes(int index) {
-      return locations_.getByteString(index);
-    }
-
     private void initFields() {
+      locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
       table_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.getDefaultInstance();
       region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
-      locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
@@ -999,14 +999,14 @@ public final class MapReduceProtos {
     public void writeTo(com.google.protobuf.CodedOutputStream output)
                         throws java.io.IOException {
       getSerializedSize();
+      for (int i = 0; i < locations_.size(); i++) {
+        output.writeBytes(2, locations_.getByteString(i));
+      }
       if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, table_);
+        output.writeMessage(3, table_);
       }
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, region_);
-      }
-      for (int i = 0; i < locations_.size(); i++) {
-        output.writeBytes(3, locations_.getByteString(i));
+        output.writeMessage(4, region_);
       }
       getUnknownFields().writeTo(output);
     }
@@ -1017,14 +1017,6 @@ public final class MapReduceProtos {
       if (size != -1) return size;
 
       size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, table_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, region_);
-      }
       {
         int dataSize = 0;
         for (int i = 0; i < locations_.size(); i++) {
@@ -1034,6 +1026,14 @@ public final class MapReduceProtos {
         size += dataSize;
         size += 1 * getLocationsList().size();
       }
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(3, table_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(4, region_);
+      }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
       return size;
@@ -1057,6 +1057,8 @@ public final class MapReduceProtos {
       org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit other = (org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit) obj;
 
       boolean result = true;
+      result = result && getLocationsList()
+          .equals(other.getLocationsList());
       result = result && (hasTable() == other.hasTable());
       if (hasTable()) {
         result = result && getTable()
@@ -1067,8 +1069,6 @@ public final class MapReduceProtos {
         result = result && getRegion()
             .equals(other.getRegion());
       }
-      result = result && getLocationsList()
-          .equals(other.getLocationsList());
       result = result &&
           getUnknownFields().equals(other.getUnknownFields());
       return result;
@@ -1082,6 +1082,10 @@ public final class MapReduceProtos {
       }
       int hash = 41;
       hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getLocationsCount() > 0) {
+        hash = (37 * hash) + LOCATIONS_FIELD_NUMBER;
+        hash = (53 * hash) + getLocationsList().hashCode();
+      }
       if (hasTable()) {
         hash = (37 * hash) + TABLE_FIELD_NUMBER;
         hash = (53 * hash) + getTable().hashCode();
@@ -1090,10 +1094,6 @@ public final class MapReduceProtos {
         hash = (37 * hash) + REGION_FIELD_NUMBER;
         hash = (53 * hash) + getRegion().hashCode();
       }
-      if (getLocationsCount() > 0) {
-        hash = (37 * hash) + LOCATIONS_FIELD_NUMBER;
-        hash = (53 * hash) + getLocationsList().hashCode();
-      }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
@@ -1205,19 +1205,19 @@ public final class MapReduceProtos {
 
       public Builder clear() {
         super.clear();
+        locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
         if (tableBuilder_ == null) {
           table_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.getDefaultInstance();
         } else {
           tableBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
+        bitField0_ = (bitField0_ & ~0x00000002);
         if (regionBuilder_ == null) {
           region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
         } else {
           regionBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
         bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
@@ -1247,7 +1247,13 @@ public final class MapReduceProtos {
         org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit result = new org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit(this);
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+        if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          locations_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              locations_);
+          bitField0_ = (bitField0_ & ~0x00000001);
+        }
+        result.locations_ = locations_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
           to_bitField0_ |= 0x00000001;
         }
         if (tableBuilder_ == null) {
@@ -1255,7 +1261,7 @@ public final class MapReduceProtos {
         } else {
           result.table_ = tableBuilder_.build();
         }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
           to_bitField0_ |= 0x00000002;
         }
         if (regionBuilder_ == null) {
@@ -1263,12 +1269,6 @@ public final class MapReduceProtos {
         } else {
           result.region_ = regionBuilder_.build();
         }
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          locations_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              locations_);
-          bitField0_ = (bitField0_ & ~0x00000004);
-        }
-        result.locations_ = locations_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
         return result;
@@ -1285,22 +1285,22 @@ public final class MapReduceProtos {
 
       public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit other) {
         if (other == org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.TableSnapshotRegionSplit.getDefaultInstance()) return this;
-        if (other.hasTable()) {
-          mergeTable(other.getTable());
-        }
-        if (other.hasRegion()) {
-          mergeRegion(other.getRegion());
-        }
         if (!other.locations_.isEmpty()) {
           if (locations_.isEmpty()) {
             locations_ = other.locations_;
-            bitField0_ = (bitField0_ & ~0x00000004);
+            bitField0_ = (bitField0_ & ~0x00000001);
           } else {
             ensureLocationsIsMutable();
             locations_.addAll(other.locations_);
           }
           onChanged();
         }
+        if (other.hasTable()) {
+          mergeTable(other.getTable());
+        }
+        if (other.hasRegion()) {
+          mergeRegion(other.getRegion());
+        }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
@@ -1340,18 +1340,111 @@ public final class MapReduceProtos {
       }
       private int bitField0_;
 
-      // optional .TableSchema table = 1;
+      // repeated string locations = 2;
+      private com.google.protobuf.LazyStringList locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureLocationsIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          locations_ = new com.google.protobuf.LazyStringArrayList(locations_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public java.util.List<java.lang.String>
+          getLocationsList() {
+        return java.util.Collections.unmodifiableList(locations_);
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public int getLocationsCount() {
+        return locations_.size();
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public java.lang.String getLocations(int index) {
+        return locations_.get(index);
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public com.google.protobuf.ByteString
+          getLocationsBytes(int index) {
+        return locations_.getByteString(index);
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public Builder setLocations(
+          int index, java.lang.String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureLocationsIsMutable();
+        locations_.set(index, value);
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public Builder addLocations(
+          java.lang.String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureLocationsIsMutable();
+        locations_.add(value);
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public Builder addAllLocations(
+          java.lang.Iterable<java.lang.String> values) {
+        ensureLocationsIsMutable();
+        super.addAll(values, locations_);
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public Builder clearLocations() {
+        locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>repeated string locations = 2;</code>
+       */
+      public Builder addLocationsBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureLocationsIsMutable();
+        locations_.add(value);
+        onChanged();
+        return this;
+      }
+
+      // optional .TableSchema table = 3;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema table_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder> tableBuilder_;
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public boolean hasTable() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+        return ((bitField0_ & 0x00000002) == 0x00000002);
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema getTable() {
         if (tableBuilder_ == null) {
@@ -1361,7 +1454,7 @@ public final class MapReduceProtos {
         }
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public Builder setTable(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema value) {
         if (tableBuilder_ == null) {
@@ -1373,11 +1466,11 @@ public final class MapReduceProtos {
         } else {
           tableBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000001;
+        bitField0_ |= 0x00000002;
         return this;
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public Builder setTable(
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.Builder builderForValue) {
@@ -1387,15 +1480,15 @@ public final class MapReduceProtos {
         } else {
           tableBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000001;
+        bitField0_ |= 0x00000002;
         return this;
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public Builder mergeTable(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema value) {
         if (tableBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
+          if (((bitField0_ & 0x00000002) == 0x00000002) &&
               table_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.getDefaultInstance()) {
             table_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.newBuilder(table_).mergeFrom(value).buildPartial();
@@ -1406,11 +1499,11 @@ public final class MapReduceProtos {
         } else {
           tableBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000001;
+        bitField0_ |= 0x00000002;
         return this;
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public Builder clearTable() {
         if (tableBuilder_ == null) {
@@ -1419,19 +1512,19 @@ public final class MapReduceProtos {
         } else {
           tableBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000001);
+        bitField0_ = (bitField0_ & ~0x00000002);
         return this;
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.Builder getTableBuilder() {
-        bitField0_ |= 0x00000001;
+        bitField0_ |= 0x00000002;
         onChanged();
         return getTableFieldBuilder().getBuilder();
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder getTableOrBuilder() {
         if (tableBuilder_ != null) {
@@ -1441,7 +1534,7 @@ public final class MapReduceProtos {
         }
       }
       /**
-       * <code>optional .TableSchema table = 1;</code>
+       * <code>optional .TableSchema table = 3;</code>
        */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchema.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TableSchemaOrBuilder> 
@@ -1457,18 +1550,18 @@ public final class MapReduceProtos {
         return tableBuilder_;
       }
 
-      // optional .RegionInfo region = 2;
+      // optional .RegionInfo region = 4;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder> regionBuilder_;
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public boolean hasRegion() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+        return ((bitField0_ & 0x00000004) == 0x00000004);
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo getRegion() {
         if (regionBuilder_ == null) {
@@ -1478,7 +1571,7 @@ public final class MapReduceProtos {
         }
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public Builder setRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo value) {
         if (regionBuilder_ == null) {
@@ -1490,11 +1583,11 @@ public final class MapReduceProtos {
         } else {
           regionBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000002;
+        bitField0_ |= 0x00000004;
         return this;
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public Builder setRegion(
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder builderForValue) {
@@ -1504,15 +1597,15 @@ public final class MapReduceProtos {
         } else {
           regionBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000002;
+        bitField0_ |= 0x00000004;
         return this;
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public Builder mergeRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo value) {
         if (regionBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
+          if (((bitField0_ & 0x00000004) == 0x00000004) &&
               region_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance()) {
             region_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.newBuilder(region_).mergeFrom(value).buildPartial();
@@ -1523,11 +1616,11 @@ public final class MapReduceProtos {
         } else {
           regionBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000002;
+        bitField0_ |= 0x00000004;
         return this;
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public Builder clearRegion() {
         if (regionBuilder_ == null) {
@@ -1536,19 +1629,19 @@ public final class MapReduceProtos {
         } else {
           regionBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000002);
+        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder getRegionBuilder() {
-        bitField0_ |= 0x00000002;
+        bitField0_ |= 0x00000004;
         onChanged();
         return getRegionFieldBuilder().getBuilder();
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder getRegionOrBuilder() {
         if (regionBuilder_ != null) {
@@ -1558,7 +1651,7 @@ public final class MapReduceProtos {
         }
       }
       /**
-       * <code>optional .RegionInfo region = 2;</code>
+       * <code>optional .RegionInfo region = 4;</code>
        */
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfoOrBuilder> 
@@ -1574,99 +1667,6 @@ public final class MapReduceProtos {
         return regionBuilder_;
       }
 
-      // repeated string locations = 3;
-      private com.google.protobuf.LazyStringList locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      private void ensureLocationsIsMutable() {
-        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
-          locations_ = new com.google.protobuf.LazyStringArrayList(locations_);
-          bitField0_ |= 0x00000004;
-         }
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public java.util.List<java.lang.String>
-          getLocationsList() {
-        return java.util.Collections.unmodifiableList(locations_);
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public int getLocationsCount() {
-        return locations_.size();
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public java.lang.String getLocations(int index) {
-        return locations_.get(index);
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public com.google.protobuf.ByteString
-          getLocationsBytes(int index) {
-        return locations_.getByteString(index);
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public Builder setLocations(
-          int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureLocationsIsMutable();
-        locations_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public Builder addLocations(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureLocationsIsMutable();
-        locations_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public Builder addAllLocations(
-          java.lang.Iterable<java.lang.String> values) {
-        ensureLocationsIsMutable();
-        super.addAll(values, locations_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public Builder clearLocations() {
-        locations_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string locations = 3;</code>
-       */
-      public Builder addLocationsBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureLocationsIsMutable();
-        locations_.add(value);
-        onChanged();
-        return this;
-      }
-
       // @@protoc_insertion_point(builder_scope:TableSnapshotRegionSplit)
     }
 
@@ -1699,9 +1699,9 @@ public final class MapReduceProtos {
     java.lang.String[] descriptorData = {
       "\n\017MapReduce.proto\032\013HBase.proto\".\n\013ScanMe" +
       "trics\022\037\n\007metrics\030\001 \003(\0132\016.NameInt64Pair\"g" +
-      "\n\030TableSnapshotRegionSplit\022\033\n\005table\030\001 \001(" +
-      "\0132\014.TableSchema\022\033\n\006region\030\002 \001(\0132\013.Region" +
-      "Info\022\021\n\tlocations\030\003 \003(\tBB\n*org.apache.ha" +
+      "\n\030TableSnapshotRegionSplit\022\021\n\tlocations\030" +
+      "\002 \003(\t\022\033\n\005table\030\003 \001(\0132\014.TableSchema\022\033\n\006re" +
+      "gion\030\004 \001(\0132\013.RegionInfoBB\n*org.apache.ha" +
       "doop.hbase.protobuf.generatedB\017MapReduce" +
       "ProtosH\001\240\001\001"
     };
@@ -1721,7 +1721,7 @@ public final class MapReduceProtos {
           internal_static_TableSnapshotRegionSplit_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_TableSnapshotRegionSplit_descriptor,
-              new java.lang.String[] { "Table", "Region", "Locations", });
+              new java.lang.String[] { "Locations", "Table", "Region", });
           return null;
         }
       };
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ZooKeeperProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ZooKeeperProtos.java
index 0af2a97..9d037f5 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ZooKeeperProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ZooKeeperProtos.java
@@ -3230,6 +3230,16 @@ public final class ZooKeeperProtos {
      * <code>required .ServerName server_name = 2;</code>
      */
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getServerNameOrBuilder();
+
+    // optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];
+    /**
+     * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+     */
+    boolean hasMode();
+    /**
+     * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode getMode();
   }
   /**
    * Protobuf type {@code SplitLogTask}
@@ -3312,6 +3322,17 @@ public final class ZooKeeperProtos {
               bitField0_ |= 0x00000002;
               break;
             }
+            case 24: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode value = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(3, rawValue);
+              } else {
+                bitField0_ |= 0x00000004;
+                mode_ = value;
+              }
+              break;
+            }
           }
         }
       } catch (com.google.protobuf.InvalidProtocolBufferException e) {
@@ -3460,6 +3481,97 @@ public final class ZooKeeperProtos {
       // @@protoc_insertion_point(enum_scope:SplitLogTask.State)
     }
 
+    /**
+     * Protobuf enum {@code SplitLogTask.RecoveryMode}
+     */
+    public enum RecoveryMode
+        implements com.google.protobuf.ProtocolMessageEnum {
+      /**
+       * <code>UNKNOWN = 0;</code>
+       */
+      UNKNOWN(0, 0),
+      /**
+       * <code>LOG_SPLITTING = 1;</code>
+       */
+      LOG_SPLITTING(1, 1),
+      /**
+       * <code>LOG_REPLAY = 2;</code>
+       */
+      LOG_REPLAY(2, 2),
+      ;
+
+      /**
+       * <code>UNKNOWN = 0;</code>
+       */
+      public static final int UNKNOWN_VALUE = 0;
+      /**
+       * <code>LOG_SPLITTING = 1;</code>
+       */
+      public static final int LOG_SPLITTING_VALUE = 1;
+      /**
+       * <code>LOG_REPLAY = 2;</code>
+       */
+      public static final int LOG_REPLAY_VALUE = 2;
+
+
+      public final int getNumber() { return value; }
+
+      public static RecoveryMode valueOf(int value) {
+        switch (value) {
+          case 0: return UNKNOWN;
+          case 1: return LOG_SPLITTING;
+          case 2: return LOG_REPLAY;
+          default: return null;
+        }
+      }
+
+      public static com.google.protobuf.Internal.EnumLiteMap<RecoveryMode>
+          internalGetValueMap() {
+        return internalValueMap;
+      }
+      private static com.google.protobuf.Internal.EnumLiteMap<RecoveryMode>
+          internalValueMap =
+            new com.google.protobuf.Internal.EnumLiteMap<RecoveryMode>() {
+              public RecoveryMode findValueByNumber(int number) {
+                return RecoveryMode.valueOf(number);
+              }
+            };
+
+      public final com.google.protobuf.Descriptors.EnumValueDescriptor
+          getValueDescriptor() {
+        return getDescriptor().getValues().get(index);
+      }
+      public final com.google.protobuf.Descriptors.EnumDescriptor
+          getDescriptorForType() {
+        return getDescriptor();
+      }
+      public static final com.google.protobuf.Descriptors.EnumDescriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.getDescriptor().getEnumTypes().get(1);
+      }
+
+      private static final RecoveryMode[] VALUES = values();
+
+      public static RecoveryMode valueOf(
+          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
+        if (desc.getType() != getDescriptor()) {
+          throw new java.lang.IllegalArgumentException(
+            "EnumValueDescriptor is not for this type.");
+        }
+        return VALUES[desc.getIndex()];
+      }
+
+      private final int index;
+      private final int value;
+
+      private RecoveryMode(int index, int value) {
+        this.index = index;
+        this.value = value;
+      }
+
+      // @@protoc_insertion_point(enum_scope:SplitLogTask.RecoveryMode)
+    }
+
     private int bitField0_;
     // required .SplitLogTask.State state = 1;
     public static final int STATE_FIELD_NUMBER = 1;
@@ -3499,9 +3611,26 @@ public final class ZooKeeperProtos {
       return serverName_;
     }
 
+    // optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];
+    public static final int MODE_FIELD_NUMBER = 3;
+    private org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode mode_;
+    /**
+     * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+     */
+    public boolean hasMode() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    /**
+     * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode getMode() {
+      return mode_;
+    }
+
     private void initFields() {
       state_ = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.State.UNASSIGNED;
       serverName_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance();
+      mode_ = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN;
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
@@ -3533,6 +3662,9 @@ public final class ZooKeeperProtos {
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
         output.writeMessage(2, serverName_);
       }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeEnum(3, mode_.getNumber());
+      }
       getUnknownFields().writeTo(output);
     }
 
@@ -3550,6 +3682,10 @@ public final class ZooKeeperProtos {
         size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(2, serverName_);
       }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeEnumSize(3, mode_.getNumber());
+      }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
       return size;
@@ -3583,6 +3719,11 @@ public final class ZooKeeperProtos {
         result = result && getServerName()
             .equals(other.getServerName());
       }
+      result = result && (hasMode() == other.hasMode());
+      if (hasMode()) {
+        result = result &&
+            (getMode() == other.getMode());
+      }
       result = result &&
           getUnknownFields().equals(other.getUnknownFields());
       return result;
@@ -3604,6 +3745,10 @@ public final class ZooKeeperProtos {
         hash = (37 * hash) + SERVER_NAME_FIELD_NUMBER;
         hash = (53 * hash) + getServerName().hashCode();
       }
+      if (hasMode()) {
+        hash = (37 * hash) + MODE_FIELD_NUMBER;
+        hash = (53 * hash) + hashEnum(getMode());
+      }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
@@ -3728,6 +3873,8 @@ public final class ZooKeeperProtos {
           serverNameBuilder_.clear();
         }
         bitField0_ = (bitField0_ & ~0x00000002);
+        mode_ = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN;
+        bitField0_ = (bitField0_ & ~0x00000004);
         return this;
       }
 
@@ -3768,6 +3915,10 @@ public final class ZooKeeperProtos {
         } else {
           result.serverName_ = serverNameBuilder_.build();
         }
+        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+          to_bitField0_ |= 0x00000004;
+        }
+        result.mode_ = mode_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
         return result;
@@ -3790,6 +3941,9 @@ public final class ZooKeeperProtos {
         if (other.hasServerName()) {
           mergeServerName(other.getServerName());
         }
+        if (other.hasMode()) {
+          setMode(other.getMode());
+        }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
@@ -3982,6 +4136,42 @@ public final class ZooKeeperProtos {
         return serverNameBuilder_;
       }
 
+      // optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];
+      private org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode mode_ = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN;
+      /**
+       * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+       */
+      public boolean hasMode() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      /**
+       * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+       */
+      public org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode getMode() {
+        return mode_;
+      }
+      /**
+       * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+       */
+      public Builder setMode(org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode value) {
+        if (value == null) {
+          throw new NullPointerException();
+        }
+        bitField0_ |= 0x00000004;
+        mode_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional .SplitLogTask.RecoveryMode mode = 3 [default = UNKNOWN];</code>
+       */
+      public Builder clearMode() {
+        bitField0_ = (bitField0_ & ~0x00000004);
+        mode_ = org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN;
+        onChanged();
+        return this;
+      }
+
       // @@protoc_insertion_point(builder_scope:SplitLogTask)
     }
 
@@ -9399,29 +9589,32 @@ public final class ZooKeeperProtos {
       "gionTransition\022\027\n\017event_type_code\030\001 \002(\r\022" +
       "\023\n\013region_name\030\002 \002(\014\022\023\n\013create_time\030\003 \002(" +
       "\004\022 \n\013server_name\030\004 \002(\0132\013.ServerName\022\017\n\007p" +
-      "ayload\030\005 \001(\014\"\231\001\n\014SplitLogTask\022\"\n\005state\030\001" +
+      "ayload\030\005 \001(\014\"\214\002\n\014SplitLogTask\022\"\n\005state\030\001" +
       " \002(\0162\023.SplitLogTask.State\022 \n\013server_name",
-      "\030\002 \002(\0132\013.ServerName\"C\n\005State\022\016\n\nUNASSIGN" +
-      "ED\020\000\022\t\n\005OWNED\020\001\022\014\n\010RESIGNED\020\002\022\010\n\004DONE\020\003\022" +
-      "\007\n\003ERR\020\004\"n\n\005Table\022$\n\005state\030\001 \002(\0162\014.Table" +
-      ".State:\007ENABLED\"?\n\005State\022\013\n\007ENABLED\020\000\022\014\n" +
-      "\010DISABLED\020\001\022\r\n\tDISABLING\020\002\022\014\n\010ENABLING\020\003" +
-      "\"%\n\017ReplicationPeer\022\022\n\nclusterkey\030\001 \002(\t\"" +
-      "^\n\020ReplicationState\022&\n\005state\030\001 \002(\0162\027.Rep" +
-      "licationState.State\"\"\n\005State\022\013\n\007ENABLED\020" +
-      "\000\022\014\n\010DISABLED\020\001\"+\n\027ReplicationHLogPositi" +
-      "on\022\020\n\010position\030\001 \002(\003\"%\n\017ReplicationLock\022",
-      "\022\n\nlock_owner\030\001 \002(\t\"\230\001\n\tTableLock\022\036\n\ntab" +
-      "le_name\030\001 \001(\0132\n.TableName\022\037\n\nlock_owner\030" +
-      "\002 \001(\0132\013.ServerName\022\021\n\tthread_id\030\003 \001(\003\022\021\n" +
-      "\tis_shared\030\004 \001(\010\022\017\n\007purpose\030\005 \001(\t\022\023\n\013cre" +
-      "ate_time\030\006 \001(\003\";\n\017StoreSequenceId\022\023\n\013fam" +
-      "ily_name\030\001 \002(\014\022\023\n\013sequence_id\030\002 \002(\004\"g\n\026R" +
-      "egionStoreSequenceIds\022 \n\030last_flushed_se" +
-      "quence_id\030\001 \002(\004\022+\n\021store_sequence_id\030\002 \003" +
-      "(\0132\020.StoreSequenceIdBE\n*org.apache.hadoo" +
-      "p.hbase.protobuf.generatedB\017ZooKeeperPro",
-      "tosH\001\210\001\001\240\001\001"
+      "\030\002 \002(\0132\013.ServerName\0221\n\004mode\030\003 \001(\0162\032.Spli" +
+      "tLogTask.RecoveryMode:\007UNKNOWN\"C\n\005State\022" +
+      "\016\n\nUNASSIGNED\020\000\022\t\n\005OWNED\020\001\022\014\n\010RESIGNED\020\002" +
+      "\022\010\n\004DONE\020\003\022\007\n\003ERR\020\004\">\n\014RecoveryMode\022\013\n\007U" +
+      "NKNOWN\020\000\022\021\n\rLOG_SPLITTING\020\001\022\016\n\nLOG_REPLA" +
+      "Y\020\002\"n\n\005Table\022$\n\005state\030\001 \002(\0162\014.Table.Stat" +
+      "e:\007ENABLED\"?\n\005State\022\013\n\007ENABLED\020\000\022\014\n\010DISA" +
+      "BLED\020\001\022\r\n\tDISABLING\020\002\022\014\n\010ENABLING\020\003\"%\n\017R" +
+      "eplicationPeer\022\022\n\nclusterkey\030\001 \002(\t\"^\n\020Re" +
+      "plicationState\022&\n\005state\030\001 \002(\0162\027.Replicat",
+      "ionState.State\"\"\n\005State\022\013\n\007ENABLED\020\000\022\014\n\010" +
+      "DISABLED\020\001\"+\n\027ReplicationHLogPosition\022\020\n" +
+      "\010position\030\001 \002(\003\"%\n\017ReplicationLock\022\022\n\nlo" +
+      "ck_owner\030\001 \002(\t\"\230\001\n\tTableLock\022\036\n\ntable_na" +
+      "me\030\001 \001(\0132\n.TableName\022\037\n\nlock_owner\030\002 \001(\013" +
+      "2\013.ServerName\022\021\n\tthread_id\030\003 \001(\003\022\021\n\tis_s" +
+      "hared\030\004 \001(\010\022\017\n\007purpose\030\005 \001(\t\022\023\n\013create_t" +
+      "ime\030\006 \001(\003\";\n\017StoreSequenceId\022\023\n\013family_n" +
+      "ame\030\001 \002(\014\022\023\n\013sequence_id\030\002 \002(\004\"g\n\026Region" +
+      "StoreSequenceIds\022 \n\030last_flushed_sequenc",
+      "e_id\030\001 \002(\004\022+\n\021store_sequence_id\030\002 \003(\0132\020." +
+      "StoreSequenceIdBE\n*org.apache.hadoop.hba" +
+      "se.protobuf.generatedB\017ZooKeeperProtosH\001" +
+      "\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -9457,7 +9650,7 @@ public final class ZooKeeperProtos {
           internal_static_SplitLogTask_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_SplitLogTask_descriptor,
-              new java.lang.String[] { "State", "ServerName", });
+              new java.lang.String[] { "State", "ServerName", "Mode", });
           internal_static_Table_descriptor =
             getDescriptor().getMessageTypes().get(5);
           internal_static_Table_fieldAccessorTable = new
diff --git hbase-protocol/src/main/protobuf/Admin.proto hbase-protocol/src/main/protobuf/Admin.proto
index 5b889cd..2075914 100644
--- hbase-protocol/src/main/protobuf/Admin.proto
+++ hbase-protocol/src/main/protobuf/Admin.proto
@@ -75,6 +75,8 @@ message OpenRegionRequest {
     required RegionInfo region = 1;
     optional uint32 version_of_offline_node = 2;
     repeated ServerName favored_nodes = 3;
+    // open region for distributedLogReplay
+    optional bool isOpenForDistributedLogReplay = 4;
   }
 }
 
diff --git hbase-protocol/src/main/protobuf/ZooKeeper.proto hbase-protocol/src/main/protobuf/ZooKeeper.proto
index 082e1f7..37816da 100644
--- hbase-protocol/src/main/protobuf/ZooKeeper.proto
+++ hbase-protocol/src/main/protobuf/ZooKeeper.proto
@@ -85,8 +85,14 @@ message SplitLogTask {
     DONE = 3;
     ERR = 4;
   }
+  enum RecoveryMode {
+    UNKNOWN = 0;
+    LOG_SPLITTING = 1;
+    LOG_REPLAY = 2;
+  }
   required State state = 1;
   required ServerName server_name = 2;
+  optional RecoveryMode mode = 3 [default = UNKNOWN];
 }
 
 /**
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogTask.java hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogTask.java
index 67a0994..f030950 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogTask.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/SplitLogTask.java
@@ -18,10 +18,12 @@
 package org.apache.hadoop.hbase;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos;
+import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
 import org.apache.hadoop.hbase.util.Bytes;
 
 import com.google.protobuf.InvalidProtocolBufferException;
@@ -36,49 +38,62 @@ import com.google.protobuf.InvalidProtocolBufferException;
 public class SplitLogTask {
   private final ServerName originServer;
   private final ZooKeeperProtos.SplitLogTask.State state;
+  private final ZooKeeperProtos.SplitLogTask.RecoveryMode mode;
 
   public static class Unassigned extends SplitLogTask {
-    public Unassigned(final ServerName originServer) {
-      super(originServer, ZooKeeperProtos.SplitLogTask.State.UNASSIGNED);
+    public Unassigned(final ServerName originServer, final Configuration conf) {
+      super(originServer, ZooKeeperProtos.SplitLogTask.State.UNASSIGNED, conf);
     }
   }
 
   public static class Owned extends SplitLogTask {
     public Owned(final ServerName originServer) {
-      super(originServer, ZooKeeperProtos.SplitLogTask.State.OWNED);
+      super(originServer, ZooKeeperProtos.SplitLogTask.State.OWNED, null);
     }
   }
 
   public static class Resigned extends SplitLogTask {
     public Resigned(final ServerName originServer) {
-      super(originServer, ZooKeeperProtos.SplitLogTask.State.RESIGNED);
+      super(originServer, ZooKeeperProtos.SplitLogTask.State.RESIGNED, null);
     }
   }
 
   public static class Done extends SplitLogTask {
     public Done(final ServerName originServer) {
-      super(originServer, ZooKeeperProtos.SplitLogTask.State.DONE);
+      super(originServer, ZooKeeperProtos.SplitLogTask.State.DONE, null);
     }
   }
 
   public static class Err extends SplitLogTask {
     public Err(final ServerName originServer) {
-      super(originServer, ZooKeeperProtos.SplitLogTask.State.ERR);
+      super(originServer, ZooKeeperProtos.SplitLogTask.State.ERR, null);
     }
   }
 
   SplitLogTask(final ZooKeeperProtos.SplitLogTask slt) {
-    this(ProtobufUtil.toServerName(slt.getServerName()), slt.getState());
+    this.originServer = ProtobufUtil.toServerName(slt.getServerName());
+    this.state = slt.getState();
+    this.mode = (slt.hasMode()) ? slt.getMode() : 
+      ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN;
   }
 
-  SplitLogTask(final ServerName originServer, final ZooKeeperProtos.SplitLogTask.State state) {
+  SplitLogTask(final ServerName originServer, final ZooKeeperProtos.SplitLogTask.State state,
+      final Configuration conf) {
     this.originServer = originServer;
     this.state = state;
+    this.mode = (conf == null) ? ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN :
+      ((HLogSplitter.isDistributedLogReplay(conf)) ? 
+          ZooKeeperProtos.SplitLogTask.RecoveryMode.LOG_REPLAY : 
+          ZooKeeperProtos.SplitLogTask.RecoveryMode.LOG_SPLITTING);
   }
 
   public ServerName getServerName() {
     return this.originServer;
   }
+  
+  public ZooKeeperProtos.SplitLogTask.RecoveryMode getMode() {
+    return this.mode;
+  }
 
   public boolean isUnassigned(final ServerName sn) {
     return this.originServer.equals(sn) && isUnassigned();
@@ -167,7 +182,8 @@ public class SplitLogTask {
     // pbs just created.
     HBaseProtos.ServerName snpb = ProtobufUtil.toServerName(this.originServer);
     ZooKeeperProtos.SplitLogTask slts =
-      ZooKeeperProtos.SplitLogTask.newBuilder().setServerName(snpb).setState(this.state).build();
+      ZooKeeperProtos.SplitLogTask.newBuilder().setServerName(snpb).setState(this.state).
+      setMode(this.mode).build();
     return ProtobufUtil.prependPBMagic(slts.toByteArray());
   }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 9ae4371..a69f6dd 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -56,6 +56,7 @@ import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
+import org.apache.hadoop.hbase.exceptions.RegionServerConfigMismatchException;
 import org.apache.hadoop.hbase.executor.EventHandler;
 import org.apache.hadoop.hbase.executor.EventType;
 import org.apache.hadoop.hbase.executor.ExecutorService;
@@ -1926,6 +1927,10 @@ public class AssignmentManager extends ZooKeeperListener {
                 "try=" + i + " of " + this.maximumAttempts, t);
           } else {
             needNewPlan = true;
+            if(t instanceof RegionServerConfigMismatchException) {
+              // try another server & reset retry count
+              i--;
+            }
             LOG.warn(assignMsg + ", trying to assign elsewhere instead;" +
                 " try=" + i + " of " + this.maximumAttempts, t);
           }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
index 63b48f5..4fb787c 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
@@ -61,6 +61,7 @@ import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.RegionOpeningState;
+import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Triple;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
@@ -693,8 +694,9 @@ public class ServerManager {
         " failed because no RPC connection found to this server");
       return RegionOpeningState.FAILED_OPENING;
     }
-    OpenRegionRequest request =
-      RequestConverter.buildOpenRegionRequest(server, region, versionOfOfflineNode, favoredNodes);
+    OpenRegionRequest request = RequestConverter.buildOpenRegionRequest(server, 
+      region, versionOfOfflineNode, favoredNodes, 
+      HLogSplitter.isDistributedLogReplay(master.getConfiguration()));
     try {
       OpenRegionResponse response = admin.openRegion(null, request);
       return ResponseConverter.getRegionOpeningState(response);
@@ -722,8 +724,8 @@ public class ServerManager {
       return null;
     }
 
-    OpenRegionRequest request =
-      RequestConverter.buildOpenRegionRequest(regionOpenInfos);
+    OpenRegionRequest request = RequestConverter.buildOpenRegionRequest(regionOpenInfos, 
+        HLogSplitter.isDistributedLogReplay(master.getConfiguration()));
     try {
       OpenRegionResponse response = admin.openRegion(null, request);
       return ResponseConverter.getRegionOpeningStateList(response);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
index f975b3e..3b6ab49 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
@@ -493,7 +493,7 @@ public class SplitLogManager extends ZooKeeperListener {
       if (count == 0 && this.master.isInitialized()
           && !this.master.getServerManager().areDeadServersInProgress()) {
         // no splitting work items left
-        deleteRecoveringRegionZNodes(null);
+        deleteRecoveringRegionZNodes(watcher, null);
         // reset lastRecoveringNodeCreationTime because we cleared all recovering znodes at
         // this point.
         lastRecoveringNodeCreationTime = Long.MAX_VALUE;
@@ -549,14 +549,6 @@ public class SplitLogManager extends ZooKeeperListener {
   void removeStaleRecoveringRegionsFromZK(final Set<ServerName> failedServers)
       throws KeeperException, InterruptedIOException {
 
-    if (!this.distributedLogReplay) {
-      // remove any regions in recovery from ZK which could happen when we turn the feature on
-      // and later turn it off
-      ZKUtil.deleteChildrenRecursively(watcher, watcher.recoveringRegionsZNode);
-      // the function is only used in distributedLogReplay mode when master is in initialization
-      return;
-    }
-
     Set<String> knownFailedServers = new HashSet<String>();
     if (failedServers != null) {
       for (ServerName tmpServerName : failedServers) {
@@ -624,7 +616,7 @@ public class SplitLogManager extends ZooKeeperListener {
     }
   }
 
-  private void deleteRecoveringRegionZNodes(List<String> regions) {
+  public static void deleteRecoveringRegionZNodes(ZooKeeperWatcher watcher, List<String> regions) {
     try {
       if (regions == null) {
         // remove all children under /home/recovering-regions
@@ -682,7 +674,7 @@ public class SplitLogManager extends ZooKeeperListener {
   }
 
   private void createNode(String path, Long retry_count) {
-    SplitLogTask slt = new SplitLogTask.Unassigned(serverName);
+    SplitLogTask slt = new SplitLogTask.Unassigned(serverName, this.conf);
     ZKUtil.asyncCreate(this.watcher, path, slt.toByteArray(), new CreateAsyncCallback(), retry_count);
     SplitLogCounters.tot_mgr_node_create_queued.incrementAndGet();
     return;
@@ -858,7 +850,7 @@ public class SplitLogManager extends ZooKeeperListener {
     task.incarnation++;
     try {
       // blocking zk call but this is done from the timeout thread
-      SplitLogTask slt = new SplitLogTask.Unassigned(this.serverName);
+      SplitLogTask slt = new SplitLogTask.Unassigned(this.serverName, this.conf);
       if (ZKUtil.setData(this.watcher, path, slt.toByteArray(), version) == false) {
         LOG.debug("failed to resubmit task " + path +
             " version changed");
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
index 2e7b022..6aaa459 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -66,6 +66,7 @@ import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.exceptions.FailedSanityCheckException;
 import org.apache.hadoop.hbase.exceptions.OperationConflictException;
 import org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException;
+import org.apache.hadoop.hbase.exceptions.RegionServerConfigMismatchException;
 import org.apache.hadoop.hbase.filter.ByteArrayComparable;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.io.hfile.HFile;
@@ -1184,6 +1185,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
             + regionServer.serverName));
       }
     }
+
     OpenRegionResponse.Builder builder = OpenRegionResponse.newBuilder();
     final int regionCount = request.getOpenInfoCount();
     final Map<TableName, HTableDescriptor> htds =
@@ -1198,6 +1200,18 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
       }
       HTableDescriptor htd;
       try {
+        // check if current region open is for distributedLogReplay. This check is to support rolling 
+        // restart/upgrade where we want to Master/RS see same configuration
+        if (regionOpenInfo.hasIsOpenForDistributedLogReplay() && 
+              regionOpenInfo.getIsOpenForDistributedLogReplay()) {
+          // check if current RS has distributedLogReplay on
+          if (!regionServer.distributedLogReplay) {
+            throw new RegionServerConfigMismatchException("This OpenRegion request is opening "
+                + "region for recovering while this server " + regionServer.serverName
+                + " hasn't turn on distributedLogReplay yet.");
+          }
+        }
+      
         final HRegion onlineRegion = regionServer.getFromOnlineRegions(region.getEncodedName());
         if (onlineRegion != null) {
           //Check if the region can actually be opened.
@@ -1260,10 +1274,17 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
 
         if (previous == null) {
           // check if the region to be opened is marked in recovering state in ZK
-          if (regionServer.distributedLogReplay
-              && SplitLogManager.isRegionMarkedRecoveringInZK(regionServer.getZooKeeper(),
-            region.getEncodedName())) {
-            regionServer.recoveringRegions.put(region.getEncodedName(), null);
+          if (SplitLogManager.isRegionMarkedRecoveringInZK(regionServer.getZooKeeper(),
+              region.getEncodedName())) {
+            if(regionServer.distributedLogReplay) {
+              regionServer.recoveringRegions.put(region.getEncodedName(), null);
+            } else {
+              // remove stale recovery region from ZK when we open region not for recovering which
+              // could happen when turn distributedLogReplay off from on.
+              List<String> tmpRegions = new ArrayList<String>();
+              tmpRegions.add(region.getEncodedName());
+              SplitLogManager.deleteRecoveringRegionZNodes(regionServer.getZooKeeper(), tmpRegions);
+            }
           }
           // If there is no action in progress, we can submit a specific handler.
           // Need to pass the expected version in the constructor.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
index e01bb00..45dae54 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java
@@ -45,6 +45,7 @@ import org.apache.hadoop.hbase.client.RetriesExhaustedException;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.master.SplitLogManager;
+import org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos;
 import org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler;
 import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtil;
@@ -302,7 +303,6 @@ public class SplitLogWorker extends ZooKeeperListener implements Runnable {
    */
   private void grabTask(String path) {
     Stat stat = new Stat();
-    long t = -1;
     byte[] data;
     synchronized (grabTaskLock) {
       currentTask = path;
@@ -335,6 +335,16 @@ public class SplitLogWorker extends ZooKeeperListener implements Runnable {
         return;
       }
 
+      // RS & Master have to see the same configuration values
+      if((slt.getMode() == ZooKeeperProtos.SplitLogTask.RecoveryMode.UNKNOWN)
+         || (slt.getMode() == ZooKeeperProtos.SplitLogTask.RecoveryMode.LOG_REPLAY && 
+            !HLogSplitter.isDistributedLogReplay(conf)) 
+         || (slt.getMode() == ZooKeeperProtos.SplitLogTask.RecoveryMode.LOG_SPLITTING &&
+            HLogSplitter.isDistributedLogReplay(conf))) {
+        LOG.debug("Didn't grab Task=" + path + " because recovery mode isn't expected. Current " +
+        		"task has recovery mode=" + slt.getMode());
+        return;
+      }
       currentVersion = attemptToOwnTask(true, watcher, serverName, path, stat.getVersion());
       if (currentVersion < 0) {
         SplitLogCounters.tot_wkr_failed_to_grab_task_lost_race.incrementAndGet();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
index d7082ed..3835d44 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
@@ -120,7 +120,7 @@ public class TestSerialization {
 
   @Test
   public void testSplitLogTask() throws DeserializationException {
-    SplitLogTask slt = new SplitLogTask.Unassigned(ServerName.valueOf("mgr,1,1"));
+    SplitLogTask slt = new SplitLogTask.Unassigned(ServerName.valueOf("mgr,1,1"), null);
     byte [] bytes = slt.toByteArray();
     SplitLogTask sltDeserialized = SplitLogTask.parseFrom(bytes);
     assertTrue(slt.equals(sltDeserialized));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
index 82b8673..df780f8 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestSplitLogManager.java
@@ -238,7 +238,7 @@ public class TestSplitLogManager {
         " startup");
     String tasknode = ZKSplitLog.getEncodedNodeName(zkw, "orphan/test/slash");
     //create an unassigned orphan task
-    SplitLogTask slt = new SplitLogTask.Unassigned(DUMMY_MASTER);
+    SplitLogTask slt = new SplitLogTask.Unassigned(DUMMY_MASTER, conf);
     zkw.getRecoverableZooKeeper().create(tasknode, slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,
         CreateMode.PERSISTENT);
     int version = ZKUtil.checkExists(zkw, tasknode);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
index b7cc51a..b86834d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
@@ -129,7 +129,7 @@ public class TestRegionServerNoMaster {
     ZKAssign.createNodeOffline(HTU.getZooKeeperWatcher(), hri, getRS().getServerName());
     // first version is '0'
     AdminProtos.OpenRegionRequest orr =
-      RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
+      RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null, null);
     AdminProtos.OpenRegionResponse responseOpen = getRS().rpcServices.openRegion(null, orr);
     Assert.assertTrue(responseOpen.getOpeningStateCount() == 1);
     Assert.assertTrue(responseOpen.getOpeningState(0).
@@ -248,7 +248,8 @@ public class TestRegionServerNoMaster {
 
     // We're sending multiple requests in a row. The region server must handle this nicely.
     for (int i = 0; i < 10; i++) {
-      AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
+      AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(
+        getRS().getServerName(), hri, 0, null, null);
       AdminProtos.OpenRegionResponse responseOpen = getRS().rpcServices.openRegion(null, orr);
       Assert.assertTrue(responseOpen.getOpeningStateCount() == 1);
 
@@ -274,7 +275,7 @@ public class TestRegionServerNoMaster {
       // fake region to be closing now, need to clear state afterwards
       getRS().regionsInTransitionInRS.put(hri.getEncodedNameAsBytes(), Boolean.FALSE);
       AdminProtos.OpenRegionRequest orr =
-        RequestConverter.buildOpenRegionRequest(sn, hri, 0, null);
+        RequestConverter.buildOpenRegionRequest(sn, hri, 0, null, null);
       getRS().rpcServices.openRegion(null, orr);
       Assert.fail("The closing region should not be opened");
     } catch (ServiceException se) {
@@ -429,7 +430,8 @@ public class TestRegionServerNoMaster {
     //actual close
     closeNoZK();
     try {
-      AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(earlierServerName, hri, 0, null);
+      AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(
+        earlierServerName, hri, 0, null, null);
       getRS().getRSRpcServices().openRegion(null, orr);
       Assert.fail("The openRegion should have been rejected");
     } catch (ServiceException se) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
index eaf5547..a9011f8 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitLogWorker.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
@@ -31,6 +32,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.SplitLogCounters;
@@ -38,6 +40,7 @@ import org.apache.hadoop.hbase.SplitLogTask;
 import org.apache.hadoop.hbase.Waiter;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.executor.ExecutorType;
+import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
 import org.apache.hadoop.hbase.util.CancelableProgressable;
 import org.apache.hadoop.hbase.zookeeper.ZKSplitLog;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
@@ -149,7 +152,9 @@ public class TestSplitLogWorker {
     final ServerName RS = ServerName.valueOf("rs,1,1");
     RegionServerServices mockedRS = getRegionServer(RS);
     zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS),
-      new SplitLogTask.Unassigned(ServerName.valueOf("mgr,1,1")).toByteArray(), Ids.OPEN_ACL_UNSAFE,
+      new SplitLogTask.Unassigned(
+        ServerName.valueOf("mgr,1,1"), TEST_UTIL.getConfiguration()).toByteArray(), 
+        Ids.OPEN_ACL_UNSAFE,
         CreateMode.PERSISTENT);
 
     SplitLogWorker slw =
@@ -184,8 +189,8 @@ public class TestSplitLogWorker {
     final ServerName SVR1 = ServerName.valueOf("svr1,1,1");
     final ServerName SVR2 = ServerName.valueOf("svr2,1,1");
     zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TRFT),
-      new SplitLogTask.Unassigned(MANAGER).toByteArray(), Ids.OPEN_ACL_UNSAFE,
-        CreateMode.PERSISTENT);
+      new SplitLogTask.Unassigned(MANAGER, TEST_UTIL.getConfiguration()).toByteArray(), 
+        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
     RegionServerServices mockedRS1 = getRegionServer(SVR1);
     RegionServerServices mockedRS2 = getRegionServer(SVR2);
     SplitLogWorker slw1 =
@@ -227,8 +232,8 @@ public class TestSplitLogWorker {
 
       // this time create a task node after starting the splitLogWorker
       zkw.getRecoverableZooKeeper().create(PATH,
-        new SplitLogTask.Unassigned(MANAGER).toByteArray(), Ids.OPEN_ACL_UNSAFE,
-        CreateMode.PERSISTENT);
+        new SplitLogTask.Unassigned(MANAGER, TEST_UTIL.getConfiguration()).toByteArray(), 
+        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
 
       waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 1, WAIT_TIME);
       assertEquals(1, slw.taskReadySeq);
@@ -258,7 +263,8 @@ public class TestSplitLogWorker {
       Thread.sleep(100);
       waitForCounter(SplitLogCounters.tot_wkr_task_grabing, 0, 1, WAIT_TIME);
 
-      SplitLogTask unassignedManager = new SplitLogTask.Unassigned(MANAGER);
+      SplitLogTask unassignedManager = 
+        new SplitLogTask.Unassigned(MANAGER, TEST_UTIL.getConfiguration());
       zkw.getRecoverableZooKeeper().create(PATH1, unassignedManager.toByteArray(),
         Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
 
@@ -298,7 +304,7 @@ public class TestSplitLogWorker {
     Thread.sleep(100);
 
     String task = ZKSplitLog.getEncodedNodeName(zkw, "task");
-    SplitLogTask slt = new SplitLogTask.Unassigned(MANAGER);
+    SplitLogTask slt = new SplitLogTask.Unassigned(MANAGER, TEST_UTIL.getConfiguration());
     zkw.getRecoverableZooKeeper().create(task,slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,
       CreateMode.PERSISTENT);
 
@@ -351,8 +357,9 @@ public class TestSplitLogWorker {
 
     for (int i = 0; i < maxTasks; i++) {
       zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS + i),
-        new SplitLogTask.Unassigned(ServerName.valueOf("mgr,1,1")).toByteArray(),
-        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+        new SplitLogTask.Unassigned(
+          ServerName.valueOf("mgr,1,1"), TEST_UTIL.getConfiguration()).toByteArray(),
+          Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
     }
 
     SplitLogWorker slw = new SplitLogWorker(zkw, testConf, mockedRS, neverEndingTask);
@@ -394,9 +401,9 @@ public class TestSplitLogWorker {
 
     for (int i = 0; i < maxTasks; i++) {
       zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS + i),
-        new SplitLogTask.Unassigned(ServerName.valueOf("mgr,1,1")).toByteArray(),
-        Ids.OPEN_ACL_UNSAFE,
-        CreateMode.PERSISTENT);
+        new SplitLogTask.Unassigned(
+          ServerName.valueOf("mgr,1,1"), TEST_UTIL.getConfiguration()).toByteArray(),
+          Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
     }
 
     SplitLogWorker slw = new SplitLogWorker(zkw, testConf, mockedRS, neverEndingTask);
@@ -416,6 +423,34 @@ public class TestSplitLogWorker {
       stopSplitLogWorker(slw);
     }
   }
+  
+  @Test(timeout=60000)
+  public void testNotAcquireTaskOfDifferentRecoveryMode() throws Exception {
+    LOG.info("testNotAcquireTaskOfDifferentRecoveryMode");
+    SplitLogCounters.resetCounters();
+    final String TATAS = "tatas";
+    final ServerName RS = ServerName.valueOf("rs,1,1");
+    Configuration testConf = HBaseConfiguration.create(TEST_UTIL.getConfiguration());
+    boolean isDistributedLogReplay = HLogSplitter.isDistributedLogReplay(testConf);
+    testConf.setBoolean(HConstants.DISTRIBUTED_LOG_REPLAY_KEY, !isDistributedLogReplay);
+    RegionServerServices mockedRS = getRegionServer(RS);
+
+    zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS),
+      new SplitLogTask.Unassigned(
+        ServerName.valueOf("mgr,1,1"), TEST_UTIL.getConfiguration()).toByteArray(),
+        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
+
+    SplitLogWorker slw = new SplitLogWorker(zkw, testConf, mockedRS, neverEndingTask);
+    slw.start();
+    Thread.sleep(WAIT_TIME);
+    try {
+      byte[] bytes = ZKUtil.getData(zkw, ZKSplitLog.getEncodedNodeName(zkw, TATAS));
+      SplitLogTask slt = SplitLogTask.parseFrom(bytes);
+      assertFalse(slt.isOwned(RS));
+    } finally {
+      stopSplitLogWorker(slw);
+    }
+  }
 
   /**
    * Create a mocked region server service instance
