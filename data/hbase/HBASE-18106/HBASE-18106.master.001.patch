From d1c63c8533d34b4022dd810fdf82b9d4ef9a71e8 Mon Sep 17 00:00:00 2001
From: Balazs Meszaros <balazs.meszaros@cloudera.com>
Date: Fri, 11 Aug 2017 11:30:29 +0200
Subject: [PATCH] HBASE-18106 Redo ProcedureInfo and LockInfo

Main changes:
- ProcedureInfo and LockInfo are used only on client side
- Procedure and LockedResource are their server side equivalent
- Procedure protobuf state_data became obsolate, it is only kept for
  reading previously written WAL
- Procedure protobuf contains a state_message field, which stores the internal
  state messages (Any type instead of bytes)
- Procedure.serializeStateData and deserializeStateDate changed slightly
- Procedures internal states are available on client side
- Procedures are displayed on web UI and in shell in the following jruby format:
  { ID => '1', PARENT_ID = '-1', PARAMETERS => [ ..extra state information.. ] }
---
 .../java/org/apache/hadoop/hbase/client/Admin.java |   2 +-
 .../org/apache/hadoop/hbase/client/AsyncAdmin.java |   2 +-
 .../hadoop/hbase/client/AsyncHBaseAdmin.java       |   2 +-
 .../org/apache/hadoop/hbase/client/HBaseAdmin.java |   6 +-
 .../hadoop/hbase/client/RawAsyncHBaseAdmin.java    |   2 +-
 .../hadoop/hbase/protobuf/ProtobufFormat.java      | 115 +++++++++++++
 .../hadoop/hbase/shaded/protobuf/ProtobufUtil.java | 186 ++++++--------------
 .../hbase/shaded/protobuf/TestProtobufUtil.java    | 149 ++++++++++++++++
 .../apache/hadoop/hbase/procedure2/LockInfo.java   |  50 +-----
 .../apache/hadoop/hbase/procedure2/LockType.java   |  26 +++
 .../hbase/procedure2/LockedResourceType.java       |  26 +++
 .../hbase/{ => procedure2}/ProcedureInfo.java      |  77 ++++-----
 .../org/apache/hadoop/hbase/util/JRubyFormat.java  | 116 +++++++++++++
 .../apache/hadoop/hbase/util/TestJRubyFormat.java  |  46 +++++
 .../hadoop/hbase/procedure2/LockedResource.java    |  69 ++++++++
 .../apache/hadoop/hbase/procedure2/Procedure.java  |   6 +-
 .../hadoop/hbase/procedure2/ProcedureExecutor.java |  36 ++--
 .../hbase/procedure2/ProcedureInMemoryChore.java   |  12 +-
 .../hbase/procedure2/ProcedureScheduler.java       |   8 +-
 .../procedure2/ProcedureStateDeserializer.java     |  26 +++
 .../hbase/procedure2/ProcedureStateSerializer.java |  26 +++
 .../hadoop/hbase/procedure2/ProcedureUtil.java     | 189 ++++++++++++---------
 .../hbase/procedure2/SequentialProcedure.java      |  13 +-
 .../hbase/procedure2/SimpleProcedureScheduler.java |   5 +-
 .../hbase/procedure2/StateMachineProcedure.java    |  12 +-
 .../hbase/procedure2/ProcedureTestingUtility.java  |  33 ++--
 .../hbase/procedure2/TestProcedureEvents.java      |  26 +--
 .../hbase/procedure2/TestProcedureRecovery.java    |  21 +--
 .../hbase/procedure2/TestProcedureReplayOrder.java |  16 +-
 .../hbase/procedure2/TestProcedureSuspended.java   |   8 +-
 .../hbase/procedure2/TestProcedureToString.java    |   9 +-
 .../hadoop/hbase/procedure2/TestProcedureUtil.java |  27 +--
 .../hbase/procedure2/TestYieldProcedures.java      |   9 +-
 .../store/wal/TestWALProcedureStore.java           |  22 +--
 .../src/main/protobuf/LockService.proto            |  13 +-
 .../src/main/protobuf/Master.proto                 |   2 +-
 .../src/main/protobuf/Procedure.proto              |   4 +-
 .../hadoop/hbase/rsgroup/RSGroupAdminServer.java   |  11 +-
 .../hadoop/hbase/coprocessor/MasterObserver.java   |  15 +-
 .../hbase/master/ExpiredMobFileCleanerChore.java   |   4 +-
 .../org/apache/hadoop/hbase/master/HMaster.java    |  25 ++-
 .../hadoop/hbase/master/MasterCoprocessorHost.java |  15 +-
 .../hbase/master/MasterMobCompactionThread.java    |   4 +-
 .../hadoop/hbase/master/MasterRpcServices.java     |  44 ++---
 .../apache/hadoop/hbase/master/MasterServices.java |   8 +-
 .../hadoop/hbase/master/MobCompactionChore.java    |   4 +-
 .../hbase/master/assignment/AssignProcedure.java   |  15 +-
 .../assignment/GCMergedRegionsProcedure.java       |  19 ++-
 .../hbase/master/assignment/GCRegionProcedure.java |  19 ++-
 .../assignment/MergeTableRegionsProcedure.java     |  25 +--
 .../master/assignment/MoveRegionProcedure.java     |  19 ++-
 .../assignment/SplitTableRegionProcedure.java      |  20 ++-
 .../hbase/master/assignment/UnassignProcedure.java |  15 +-
 .../hadoop/hbase/master/locking/LockManager.java   |  33 ++--
 .../hadoop/hbase/master/locking/LockProcedure.java |  18 +-
 .../AbstractStateMachineRegionProcedure.java       |  22 ++-
 .../master/procedure/AddColumnFamilyProcedure.java |  18 +-
 .../master/procedure/CloneSnapshotProcedure.java   |  18 +-
 .../master/procedure/CreateNamespaceProcedure.java |  19 ++-
 .../master/procedure/CreateTableProcedure.java     |  18 +-
 .../procedure/DeleteColumnFamilyProcedure.java     |  18 +-
 .../master/procedure/DeleteNamespaceProcedure.java |  19 ++-
 .../master/procedure/DeleteTableProcedure.java     |  18 +-
 .../master/procedure/DisableTableProcedure.java    |  19 ++-
 .../master/procedure/EnableTableProcedure.java     |  19 ++-
 .../master/procedure/MasterProcedureScheduler.java | 111 ++++++------
 .../procedure/ModifyColumnFamilyProcedure.java     |  19 ++-
 .../master/procedure/ModifyNamespaceProcedure.java |  19 ++-
 .../master/procedure/ModifyTableProcedure.java     |  18 +-
 .../hbase/master/procedure/ProcedureDescriber.java |  83 +++++++++
 .../master/procedure/RecoverMetaProcedure.java     |  17 +-
 .../master/procedure/RestoreSnapshotProcedure.java |  18 +-
 .../master/procedure/ServerCrashProcedure.java     |  17 +-
 .../master/procedure/TruncateTableProcedure.java   |  18 +-
 .../hbase/master/snapshot/TakeSnapshotHandler.java |  11 +-
 .../hbase/security/access/AccessController.java    |  29 ++--
 .../resources/hbase-webapps/master/procedures.jsp  |  37 ++--
 .../org/apache/hadoop/hbase/client/TestAdmin2.java |   2 +-
 .../hbase/client/TestAsyncProcedureAdminApi.java   |   2 +-
 .../hbase/coprocessor/TestMasterObserver.java      |  15 +-
 .../hbase/master/MockNoopMasterServices.java       |   9 +-
 .../hbase/master/locking/TestLockManager.java      |  23 ++-
 .../hbase/master/locking/TestLockProcedure.java    |  48 +++---
 .../procedure/TestMasterProcedureScheduler.java    | 141 +++++++--------
 .../hbase/master/procedure/TestProcedureAdmin.java |   4 +-
 .../hbase/procedure/TestProcedureDescriber.java    |  85 +++++++++
 .../hadoop/hbase/protobuf/TestProtobufUtil.java    |  41 -----
 .../security/access/TestAccessController.java      |  26 ++-
 .../hbase/shaded/protobuf/TestProtobufUtil.java    | 151 ----------------
 .../src/main/ruby/shell/commands/list_locks.rb     |  11 +-
 .../main/ruby/shell/commands/list_procedures.rb    |   5 +-
 hbase-shell/src/test/ruby/shell/list_locks_test.rb |  57 +++++--
 92 files changed, 1719 insertions(+), 1196 deletions(-)
 create mode 100644 hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufFormat.java
 create mode 100644 hbase-client/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java
 create mode 100644 hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockType.java
 create mode 100644 hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResourceType.java
 rename hbase-common/src/main/java/org/apache/hadoop/hbase/{ => procedure2}/ProcedureInfo.java (77%)
 create mode 100644 hbase-common/src/main/java/org/apache/hadoop/hbase/util/JRubyFormat.java
 create mode 100644 hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestJRubyFormat.java
 create mode 100644 hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResource.java
 create mode 100644 hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateDeserializer.java
 create mode 100644 hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateSerializer.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.java
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureDescriber.java
 delete mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java

diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java
index 4f5c128f4f..a4832dc259 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java
@@ -36,7 +36,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.NamespaceNotFoundException;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableExistsException;
@@ -47,6 +46,7 @@ import org.apache.hadoop.hbase.client.replication.TableCFs;
 import org.apache.hadoop.hbase.client.security.SecurityCapability;
 import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.quotas.QuotaFilter;
 import org.apache.hadoop.hbase.quotas.QuotaRetriever;
 import org.apache.hadoop.hbase.quotas.QuotaSettings;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java
index 7e4412dbf8..982d00352d 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncAdmin.java
@@ -28,13 +28,13 @@ import java.util.regex.Pattern;
 
 import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.quotas.QuotaFilter;
 import org.apache.hadoop.hbase.quotas.QuotaSettings;
 import org.apache.hadoop.hbase.client.RawAsyncTable.CoprocessorCallable;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncHBaseAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncHBaseAdmin.java
index 9ba3b739e0..9280b419e1 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncHBaseAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncHBaseAdmin.java
@@ -37,7 +37,6 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
@@ -47,6 +46,7 @@ import org.apache.hadoop.hbase.client.RawAsyncTable.CoprocessorCallable;
 import org.apache.hadoop.hbase.client.replication.TableCFs;
 import org.apache.hadoop.hbase.client.security.SecurityCapability;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.quotas.QuotaFilter;
 import org.apache.hadoop.hbase.quotas.QuotaSettings;
 import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index 66dbac507c..ec5d8feba6 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -58,7 +58,6 @@ import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.NamespaceNotFoundException;
 import org.apache.hadoop.hbase.NotServingRegionException;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.RegionLocations;
 import org.apache.hadoop.hbase.ServerName;
@@ -79,6 +78,7 @@ import org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
 import org.apache.hadoop.hbase.ipc.RpcControllerFactory;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.quotas.QuotaFilter;
 import org.apache.hadoop.hbase.quotas.QuotaRetriever;
 import org.apache.hadoop.hbase.quotas.QuotaSettings;
@@ -2268,12 +2268,12 @@ public class HBaseAdmin implements Admin {
       protected LockInfo[] rpcCall() throws Exception {
         ListLocksRequest request = ListLocksRequest.newBuilder().build();
         ListLocksResponse response = master.listLocks(getRpcController(), request);
-        List<LockServiceProtos.LockInfo> locksProto = response.getLockList();
+        List<LockServiceProtos.LockedResource> locksProto = response.getLockList();
 
         LockInfo[] locks = new LockInfo[locksProto.size()];
 
         for (int i = 0; i < locks.length; i++) {
-          LockServiceProtos.LockInfo lockProto = locksProto.get(i);
+          LockServiceProtos.LockedResource lockProto = locksProto.get(i);
           locks[i] = ProtobufUtil.toLockInfo(lockProto);
         }
 
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java
index 285286a483..c042c2c41b 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RawAsyncHBaseAdmin.java
@@ -56,7 +56,6 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.MetaTableAccessor.QueryType;
 import org.apache.hadoop.hbase.NotServingRegionException;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.RegionLocations;
 import org.apache.hadoop.hbase.ServerName;
@@ -81,6 +80,7 @@ import org.apache.hadoop.hbase.client.security.SecurityCapability;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.ipc.HBaseRpcController;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.quotas.QuotaFilter;
 import org.apache.hadoop.hbase.quotas.QuotaSettings;
 import org.apache.hadoop.hbase.quotas.QuotaTableUtil;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufFormat.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufFormat.java
new file mode 100644
index 0000000000..272ca9f2ea
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufFormat.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.protobuf;
+
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.shaded.com.google.gson.JsonArray;
+import org.apache.hadoop.hbase.shaded.com.google.gson.JsonElement;
+import org.apache.hadoop.hbase.shaded.com.google.gson.JsonObject;
+import org.apache.hadoop.hbase.shaded.com.google.gson.JsonParser;
+import org.apache.hadoop.hbase.shaded.com.google.gson.JsonPrimitive;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.BytesValue;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.MessageOrBuilder;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.util.JsonFormat;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.util.JsonFormat.TypeRegistry;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
+
+@InterfaceAudience.Private
+public class ProtobufFormat {
+  private static final JsonFormat.Printer jsonPrinter;
+
+  static {
+    TypeRegistry.Builder builder = TypeRegistry.newBuilder();
+    builder
+      .add(BytesValue.getDescriptor())
+      .add(LockServiceProtos.getDescriptor().getMessageTypes())
+      .add(MasterProcedureProtos.getDescriptor().getMessageTypes())
+      .add(ProcedureProtos.getDescriptor().getMessageTypes());
+    TypeRegistry typeRegistry = builder.build();
+    jsonPrinter = JsonFormat.printer()
+        .usingTypeRegistry(typeRegistry)
+        .omittingInsignificantWhitespace();
+  }
+
+  private ProtobufFormat() {
+  }
+
+  public static String toJsonString(MessageOrBuilder messageOrBuilder) throws InvalidProtocolBufferException {
+    return jsonPrinter.print(messageOrBuilder);
+  }
+
+  public static JsonElement toJsonElement(MessageOrBuilder messageOrBuilder) throws InvalidProtocolBufferException {
+    String jsonString = toJsonString(messageOrBuilder);
+    JsonParser parser = new JsonParser();
+    return parser.parse(jsonString);
+  }
+
+  private static Object toJavaObject(JsonElement element) {
+    if (element.isJsonNull()) {
+      return null;
+    } else if (element.isJsonPrimitive()) {
+      JsonPrimitive primitive = element.getAsJsonPrimitive();
+      if (primitive.isBoolean()) {
+        return primitive.getAsBoolean();
+      } else if (primitive.isNumber()) {
+        return primitive.getAsNumber();
+      } else if (primitive.isString()) {
+        return primitive.getAsString();
+      } else {
+        return null;
+      }
+    } else if (element.isJsonArray()) {
+      JsonArray array = element.getAsJsonArray();
+      List<Object> list = new ArrayList<>();
+
+      for (JsonElement arrayElement : array) {
+        Object javaObject = toJavaObject(arrayElement);
+        list.add(javaObject);
+      }
+
+      return list;
+    } else if (element.isJsonObject()) {
+      JsonObject object = element.getAsJsonObject();
+      Map<String, Object> map = new LinkedHashMap<>();
+
+      for (Entry<String, JsonElement> entry: object.entrySet()) {
+        Object javaObject = toJavaObject(entry.getValue());
+        map.put(entry.getKey(), javaObject);
+      }
+
+      return map;
+    } else {
+      return null;
+    }
+  }
+
+  public static Object toJavaObject(MessageOrBuilder messageOrBuilder) throws InvalidProtocolBufferException {
+    JsonElement element = toJsonElement(messageOrBuilder);
+    return toJavaObject(element);
+  }
+}
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
index 65e95b656d..2f571e9183 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/shaded/protobuf/ProtobufUtil.java
@@ -20,11 +20,9 @@ package org.apache.hadoop.hbase.shaded.protobuf;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.InterruptedIOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Method;
 import java.nio.ByteBuffer;
-import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -55,7 +53,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ProcedureState;
 import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
@@ -94,6 +91,10 @@ import org.apache.hadoop.hbase.io.LimitInputStream;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.master.RegionState;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResourceType;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
+import org.apache.hadoop.hbase.protobuf.ProtobufFormat;
 import org.apache.hadoop.hbase.protobuf.ProtobufMagic;
 import org.apache.hadoop.hbase.quotas.QuotaScope;
 import org.apache.hadoop.hbase.quotas.QuotaType;
@@ -101,7 +102,6 @@ import org.apache.hadoop.hbase.quotas.SpaceViolationPolicy;
 import org.apache.hadoop.hbase.quotas.ThrottleType;
 import org.apache.hadoop.hbase.replication.ReplicationLoadSink;
 import org.apache.hadoop.hbase.replication.ReplicationLoadSource;
-import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.visibility.Authorizations;
 import org.apache.hadoop.hbase.security.visibility.CellVisibility;
 import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;
@@ -164,7 +164,6 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.GetTableDe
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.ListNamespaceDescriptorsResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.MajorCompactionTimestampResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.Procedure;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.RegionServerStartupRequest;
@@ -181,11 +180,10 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.ZooKeeperProtos;
 import org.apache.hadoop.hbase.util.Addressing;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.DynamicClassLoader;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.ExceptionUtil;
 import org.apache.hadoop.hbase.util.ForeignExceptionUtil;
+import org.apache.hadoop.hbase.util.JRubyFormat;
 import org.apache.hadoop.hbase.util.Methods;
-import org.apache.hadoop.hbase.util.NonceKey;
 import org.apache.hadoop.hbase.util.VersionInfo;
 import org.apache.hadoop.ipc.RemoteException;
 
@@ -203,7 +201,6 @@ import org.apache.hadoop.ipc.RemoteException;
   value="DP_CREATE_CLASSLOADER_INSIDE_DO_PRIVILEGED", justification="None. Address sometime.")
 @InterfaceAudience.Private // TODO: some clients (Hive, etc) use this class
 public final class ProtobufUtil {
-
   private ProtobufUtil() {
   }
 
@@ -3302,174 +3299,97 @@ public final class ProtobufUtil {
     return ServerName.valueOf(hostname, port, -1L);
   }
 
-  /**
-   * @return Convert the current {@link ProcedureInfo} into a Protocol Buffers Procedure
-   * instance.
-   */
-  public static ProcedureProtos.Procedure toProtoProcedure(ProcedureInfo procedure) {
-    ProcedureProtos.Procedure.Builder builder = ProcedureProtos.Procedure.newBuilder();
-
-    builder.setClassName(procedure.getProcName());
-    builder.setProcId(procedure.getProcId());
-    builder.setSubmittedTime(procedure.getSubmittedTime());
-    builder.setState(ProcedureProtos.ProcedureState.valueOf(procedure.getProcState().name()));
-    builder.setLastUpdate(procedure.getLastUpdate());
-
-    if (procedure.hasParentId()) {
-      builder.setParentId(procedure.getParentId());
-    }
-
-    if (procedure.hasOwner()) {
-      builder.setOwner(procedure.getProcOwner());
-    }
-
-    if (procedure.isFailed()) {
-      builder.setException(ForeignExceptionUtil.toProtoForeignException(procedure.getException()));
-    }
+  public static String getProcedureStateData(
+      final ProcedureProtos.Procedure procProto) {
+    List<Object> stateData = procProto.getStateMessageList().stream()
+      .map((any) -> {
+        try {
+          return ProtobufFormat.toJavaObject(any);
+        } catch (InvalidProtocolBufferException e) {
+          return e.toString();
+        }
+      }).collect(Collectors.toList());
 
-    if (procedure.hasResultData()) {
-      builder.setResult(UnsafeByteOperations.unsafeWrap(procedure.getResult()));
-    }
+    return JRubyFormat.print(stateData);
+  }
 
-    return builder.build();
+  public static ProcedureState toProcedureState(ProcedureProtos.ProcedureState state) {
+    return ProcedureState.valueOf(state.name());
   }
 
   /**
-   * Helper to convert the protobuf object.
+   * Helper to convert the protobuf Procedure to ProcedureInfo.
    * @return Convert the current Protocol Buffers Procedure to {@link ProcedureInfo}
    * instance.
    */
-  public static ProcedureInfo toProcedureInfo(ProcedureProtos.Procedure procedureProto) {
-    NonceKey nonceKey = null;
-
-    if (procedureProto.getNonce() != HConstants.NO_NONCE) {
-      nonceKey = new NonceKey(procedureProto.getNonceGroup(), procedureProto.getNonce());
-    }
-
-    return new ProcedureInfo(procedureProto.getProcId(), procedureProto.getClassName(),
-        procedureProto.hasOwner() ? procedureProto.getOwner() : null,
-        ProcedureState.valueOf(procedureProto.getState().name()),
-        procedureProto.hasParentId() ? procedureProto.getParentId() : -1, nonceKey,
-        procedureProto.hasException() ?
-          ForeignExceptionUtil.toIOException(procedureProto.getException()) : null,
-        procedureProto.getLastUpdate(), procedureProto.getSubmittedTime(),
-        procedureProto.hasResult() ? procedureProto.getResult().toByteArray() : null);
+  public static ProcedureInfo toProcedureInfo(final ProcedureProtos.Procedure procProto) {
+    return new ProcedureInfo(procProto.getProcId(), procProto.getClassName(),
+        procProto.hasOwner() ? procProto.getOwner() : null,
+        toProcedureState(procProto.getState()),
+        getProcedureStateData(procProto),
+        procProto.hasParentId() ? procProto.getParentId() : -1,
+        procProto.hasException() ?
+          ForeignExceptionUtil.toIOException(procProto.getException()) : null,
+        procProto.getLastUpdate(), procProto.getSubmittedTime(),
+        procProto.hasResult() ? procProto.getResult().toByteArray() : null);
   }
 
-  public static LockServiceProtos.ResourceType toProtoResourceType(
-      LockInfo.ResourceType resourceType) {
+  public static LockServiceProtos.LockedResourceType toProtoLockedResourceType(
+      LockedResourceType resourceType) {
     switch (resourceType) {
     case SERVER:
-      return LockServiceProtos.ResourceType.RESOURCE_TYPE_SERVER;
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_SERVER;
     case NAMESPACE:
-      return LockServiceProtos.ResourceType.RESOURCE_TYPE_NAMESPACE;
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_NAMESPACE;
     case TABLE:
-      return LockServiceProtos.ResourceType.RESOURCE_TYPE_TABLE;
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_TABLE;
     case REGION:
-      return LockServiceProtos.ResourceType.RESOURCE_TYPE_REGION;
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_REGION;
     default:
       throw new IllegalArgumentException("Unknown resource type: " + resourceType);
     }
   }
 
-  public static LockInfo.ResourceType toResourceType(
-      LockServiceProtos.ResourceType resourceTypeProto) {
+  public static LockedResourceType toLockedResourceType(
+      LockServiceProtos.LockedResourceType resourceTypeProto) {
     switch (resourceTypeProto) {
     case RESOURCE_TYPE_SERVER:
-      return LockInfo.ResourceType.SERVER;
+      return LockedResourceType.SERVER;
     case RESOURCE_TYPE_NAMESPACE:
-      return LockInfo.ResourceType.NAMESPACE;
+      return LockedResourceType.NAMESPACE;
     case RESOURCE_TYPE_TABLE:
-      return LockInfo.ResourceType.TABLE;
+      return LockedResourceType.TABLE;
     case RESOURCE_TYPE_REGION:
-      return LockInfo.ResourceType.REGION;
+      return LockedResourceType.REGION;
     default:
       throw new IllegalArgumentException("Unknown resource type: " + resourceTypeProto);
     }
   }
 
-  public static LockServiceProtos.LockType toProtoLockType(
-      LockInfo.LockType lockType) {
-    return LockServiceProtos.LockType.valueOf(lockType.name());
-  }
-
-  public static LockInfo.LockType toLockType(
+  public static LockType toLockType(
       LockServiceProtos.LockType lockTypeProto) {
-    return LockInfo.LockType.valueOf(lockTypeProto.name());
-  }
-
-  public static LockServiceProtos.WaitingProcedure toProtoWaitingProcedure(
-      LockInfo.WaitingProcedure waitingProcedure) {
-    LockServiceProtos.WaitingProcedure.Builder builder = LockServiceProtos.WaitingProcedure.newBuilder();
-
-    ProcedureProtos.Procedure procedureProto =
-        toProtoProcedure(waitingProcedure.getProcedure());
-
-    builder
-        .setLockType(toProtoLockType(waitingProcedure.getLockType()))
-        .setProcedure(procedureProto);
-
-    return builder.build();
-  }
-
-  public static LockInfo.WaitingProcedure toWaitingProcedure(
-      LockServiceProtos.WaitingProcedure waitingProcedureProto) {
-    LockInfo.WaitingProcedure waiting = new LockInfo.WaitingProcedure();
-
-    waiting.setLockType(toLockType(waitingProcedureProto.getLockType()));
-
-    ProcedureInfo procedure =
-        toProcedureInfo(waitingProcedureProto.getProcedure());
-    waiting.setProcedure(procedure);
-
-    return waiting;
-  }
-
-  public static LockServiceProtos.LockInfo toProtoLockInfo(LockInfo lock)
-  {
-    LockServiceProtos.LockInfo.Builder builder = LockServiceProtos.LockInfo.newBuilder();
-
-    builder
-        .setResourceType(toProtoResourceType(lock.getResourceType()))
-        .setResourceName(lock.getResourceName())
-        .setLockType(toProtoLockType(lock.getLockType()));
-
-    ProcedureInfo exclusiveLockOwnerProcedure = lock.getExclusiveLockOwnerProcedure();
-
-    if (exclusiveLockOwnerProcedure != null) {
-      Procedure exclusiveLockOwnerProcedureProto =
-          toProtoProcedure(lock.getExclusiveLockOwnerProcedure());
-      builder.setExclusiveLockOwnerProcedure(exclusiveLockOwnerProcedureProto);
-    }
-
-    builder.setSharedLockCount(lock.getSharedLockCount());
-
-    for (LockInfo.WaitingProcedure waitingProcedure : lock.getWaitingProcedures()) {
-      builder.addWaitingProcedures(toProtoWaitingProcedure(waitingProcedure));
-    }
-
-    return builder.build();
+    return LockType.valueOf(lockTypeProto.name());
   }
 
-  public static LockInfo toLockInfo(LockServiceProtos.LockInfo lockProto)
+  public static LockInfo toLockInfo(LockServiceProtos.LockedResource lockedResourceProto)
   {
     LockInfo lock = new LockInfo();
 
-    lock.setResourceType(toResourceType(lockProto.getResourceType()));
-    lock.setResourceName(lockProto.getResourceName());
-    lock.setLockType(toLockType(lockProto.getLockType()));
+    lock.setResourceType(toLockedResourceType(lockedResourceProto.getResourceType()));
+    lock.setResourceName(lockedResourceProto.getResourceName());
+    lock.setLockType(toLockType(lockedResourceProto.getLockType()));
 
-    if (lockProto.hasExclusiveLockOwnerProcedure()) {
+    if (lockedResourceProto.hasExclusiveLockOwnerProcedure()) {
       ProcedureInfo exclusiveLockOwnerProcedureProto =
-          toProcedureInfo(lockProto.getExclusiveLockOwnerProcedure());
+          toProcedureInfo(lockedResourceProto.getExclusiveLockOwnerProcedure());
 
       lock.setExclusiveLockOwnerProcedure(exclusiveLockOwnerProcedureProto);
     }
 
-    lock.setSharedLockCount(lockProto.getSharedLockCount());
+    lock.setSharedLockCount(lockedResourceProto.getSharedLockCount());
 
-    for (LockServiceProtos.WaitingProcedure waitingProcedureProto : lockProto.getWaitingProceduresList()) {
-      lock.addWaitingProcedure(toWaitingProcedure(waitingProcedureProto));
+    for (ProcedureProtos.Procedure waitingProcedureProto : lockedResourceProto.getWaitingProceduresList()) {
+      lock.addWaitingProcedure(toProcedureInfo(waitingProcedureProto));
     }
 
     return lock;
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java
new file mode 100644
index 0000000000..bfdf14efc4
--- /dev/null
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java
@@ -0,0 +1,149 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.shaded.protobuf;
+
+import static org.junit.Assert.*;
+
+import org.apache.hadoop.hbase.ProcedureState;
+import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResourceType;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Any;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.BytesValue;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(SmallTests.class)
+public class TestProtobufUtil {
+  public TestProtobufUtil() {
+  }
+
+  private static ProcedureProtos.Procedure.Builder createProcedureBuilder(long procId)
+  {
+    ProcedureProtos.Procedure.Builder builder = ProcedureProtos.Procedure.newBuilder();
+    builder.setProcId(procId);
+    builder.setClassName("java.lang.Object");
+    builder.setSubmittedTime(0);
+    builder.setState(ProcedureProtos.ProcedureState.RUNNABLE);
+    builder.setLastUpdate(0);
+
+    return builder;
+  }
+
+  private static ProcedureProtos.Procedure createProcedure(long procId)
+  {
+    return createProcedureBuilder(procId).build();
+  }
+
+  private static LockServiceProtos.LockedResource createLockedResource(
+      LockServiceProtos.LockedResourceType resourceType, String resourceName,
+      LockServiceProtos.LockType lockType,
+      ProcedureProtos.Procedure exclusiveLockOwnerProcedure, int sharedLockCount)
+  {
+    LockServiceProtos.LockedResource.Builder build = LockServiceProtos.LockedResource.newBuilder();
+    build.setResourceType(resourceType);
+    build.setResourceName(resourceName);
+    build.setLockType(lockType);
+    if (exclusiveLockOwnerProcedure != null) {
+      build.setExclusiveLockOwnerProcedure(exclusiveLockOwnerProcedure);
+    }
+    build.setSharedLockCount(sharedLockCount);
+
+    return build.build();
+  }
+
+  private static void assertLockInfo(LockInfo result, LockedResourceType resourceType,
+      String resourceName, LockType lockType, int exclusiveLockOwnerProcedureId, int sharedLockCount)
+  {
+    assertEquals(resourceType, result.getResourceType());
+    assertEquals(resourceName, result.getResourceName());
+    assertEquals(lockType, result.getLockType());
+    if (exclusiveLockOwnerProcedureId > 0) {
+      assertEquals(exclusiveLockOwnerProcedureId, result.getExclusiveLockOwnerProcedure().getProcId());
+    }
+    assertEquals(sharedLockCount, result.getSharedLockCount());
+  }
+
+  @Test
+  public void testProcedureInfo() {
+    ProcedureProtos.Procedure.Builder builder = createProcedureBuilder(1);
+    ByteString stateBytes = ByteString.copyFrom(new byte[] { 65 });
+    BytesValue state = BytesValue.newBuilder().setValue(stateBytes).build();
+    builder.addStateMessage(Any.pack(state));
+    ProcedureProtos.Procedure procedure = builder.build();
+
+    final ProcedureInfo procInfo = ProtobufUtil.toProcedureInfo(procedure);
+    assertEquals(1, procInfo.getProcId());
+    assertEquals(0, procInfo.getSubmittedTime());
+    assertEquals(0, procInfo.getLastUpdate());
+    assertEquals(ProcedureState.RUNNABLE, procInfo.getProcState());
+    assertEquals("[ { " +
+        "@type => 'type.googleapis.com/google.protobuf.BytesValue', " +
+        "value => 'QQ==' " +
+        "} ]", procInfo.getProcStateData());
+  }
+
+  @Test
+  public void testServerLockInfo() {
+    LockServiceProtos.LockedResource resource = createLockedResource(
+        LockServiceProtos.LockedResourceType.RESOURCE_TYPE_SERVER, "server",
+        LockServiceProtos.LockType.SHARED, null, 2);
+
+    LockInfo lockInfo = ProtobufUtil.toLockInfo(resource);
+    assertLockInfo(lockInfo, LockedResourceType.SERVER, "server",
+        LockType.SHARED, 0, 2);
+  }
+
+  @Test
+  public void testNamespaceLockInfo() {
+    LockServiceProtos.LockedResource resource = createLockedResource(
+        LockServiceProtos.LockedResourceType.RESOURCE_TYPE_NAMESPACE, "ns",
+        LockServiceProtos.LockType.EXCLUSIVE, createProcedure(2), 0);
+
+    LockInfo lockInfo = ProtobufUtil.toLockInfo(resource);
+    assertLockInfo(lockInfo, LockedResourceType.NAMESPACE, "ns",
+        LockType.EXCLUSIVE, 2, 0);
+  }
+
+  @Test
+  public void testTableLockInfo() {
+    LockServiceProtos.LockedResource resource = createLockedResource(
+        LockServiceProtos.LockedResourceType.RESOURCE_TYPE_TABLE, "table",
+        LockServiceProtos.LockType.SHARED, null, 2);
+
+    LockInfo lockInfo = ProtobufUtil.toLockInfo(resource);
+    assertLockInfo(lockInfo, LockedResourceType.TABLE, "table",
+        LockType.SHARED, 0, 2);
+  }
+
+  @Test
+  public void testRegionLockInfo() {
+    LockServiceProtos.LockedResource resource = createLockedResource(
+        LockServiceProtos.LockedResourceType.RESOURCE_TYPE_REGION, "region",
+        LockServiceProtos.LockType.EXCLUSIVE, createProcedure(3), 0);
+
+    LockInfo lockInfo = ProtobufUtil.toLockInfo(resource);
+    assertLockInfo(lockInfo, LockedResourceType.REGION, "region",
+        LockType.EXCLUSIVE, 3, 0);
+  }
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockInfo.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockInfo.java
index 30ecee8e0a..fbcaeb1ade 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockInfo.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockInfo.java
@@ -21,62 +21,26 @@ package org.apache.hadoop.hbase.procedure2;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 
 @InterfaceAudience.Public
 public class LockInfo {
-  @InterfaceAudience.Public
-  public enum ResourceType {
-    SERVER, NAMESPACE, TABLE, REGION
-  }
-
-  @InterfaceAudience.Public
-  public enum LockType {
-    EXCLUSIVE, SHARED
-  }
-
-  @InterfaceAudience.Public
-  public static class WaitingProcedure {
-    private LockType lockType;
-    private ProcedureInfo procedure;
-
-    public WaitingProcedure() {
-    }
-
-    public LockType getLockType() {
-      return lockType;
-    }
-
-    public void setLockType(LockType lockType) {
-      this.lockType = lockType;
-    }
-
-    public ProcedureInfo getProcedure() {
-      return procedure;
-    }
-
-    public void setProcedure(ProcedureInfo procedure) {
-      this.procedure = procedure;
-    }
-  }
-
-  private ResourceType resourceType;
+  private LockedResourceType resourceType;
   private String resourceName;
   private LockType lockType;
   private ProcedureInfo exclusiveLockOwnerProcedure;
   private int sharedLockCount;
-  private final List<WaitingProcedure> waitingProcedures;
+  private final List<ProcedureInfo> waitingProcedures;
 
   public LockInfo() {
     waitingProcedures = new ArrayList<>();
   }
 
-  public ResourceType getResourceType() {
+  public LockedResourceType getResourceType() {
     return resourceType;
   }
 
-  public void setResourceType(ResourceType resourceType) {
+  public void setResourceType(LockedResourceType resourceType) {
     this.resourceType = resourceType;
   }
 
@@ -113,16 +77,16 @@ public class LockInfo {
     this.sharedLockCount = sharedLockCount;
   }
 
-  public List<WaitingProcedure> getWaitingProcedures() {
+  public List<ProcedureInfo> getWaitingProcedures() {
     return waitingProcedures;
   }
 
-  public void setWaitingProcedures(List<WaitingProcedure> waitingProcedures) {
+  public void setWaitingProcedures(List<ProcedureInfo> waitingProcedures) {
     this.waitingProcedures.clear();
     this.waitingProcedures.addAll(waitingProcedures);
   }
 
-  public void addWaitingProcedure(WaitingProcedure waitingProcedure) {
+  public void addWaitingProcedure(ProcedureInfo waitingProcedure) {
     waitingProcedures.add(waitingProcedure);
   }
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockType.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockType.java
new file mode 100644
index 0000000000..a2c9e02a0f
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockType.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.procedure2;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+
+@InterfaceAudience.Public
+public enum LockType {
+  EXCLUSIVE, SHARED
+}
\ No newline at end of file
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResourceType.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResourceType.java
new file mode 100644
index 0000000000..d7386c2660
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResourceType.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.procedure2;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+
+@InterfaceAudience.Public
+public enum LockedResourceType {
+  SERVER, NAMESPACE, TABLE, REGION
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/ProcedureInfo.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInfo.java
similarity index 77%
rename from hbase-common/src/main/java/org/apache/hadoop/hbase/ProcedureInfo.java
rename to hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInfo.java
index 36dabddf0d..db3b3d361a 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/ProcedureInfo.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInfo.java
@@ -16,14 +16,17 @@
  * limitations under the License.
  */
 
-package org.apache.hadoop.hbase;
+package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
+import java.util.Date;
+import java.util.LinkedHashMap;
+import java.util.Map;
 
+import org.apache.hadoop.hbase.ProcedureState;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-import org.apache.hadoop.hbase.util.NonceKey;
+import org.apache.hadoop.hbase.util.JRubyFormat;
 import org.apache.hadoop.util.StringUtils;
 
 /**
@@ -35,23 +38,21 @@ public class ProcedureInfo implements Cloneable {
   private final String procName;
   private final String procOwner;
   private final ProcedureState procState;
+  private final String procStateData;
   private final long parentId;
-  private final NonceKey nonceKey;
   private final IOException exception;
   private final long lastUpdate;
   private final long submittedTime;
   private final byte[] result;
 
-  private long clientAckTime = -1;
-
   @InterfaceAudience.Private
   public ProcedureInfo(
       final long procId,
       final String procName,
       final String procOwner,
       final ProcedureState procState,
+      final String procStateData,
       final long parentId,
-      final NonceKey nonceKey,
       final IOException exception,
       final long lastUpdate,
       final long submittedTime,
@@ -60,8 +61,8 @@ public class ProcedureInfo implements Cloneable {
     this.procName = procName;
     this.procOwner = procOwner;
     this.procState = procState;
+    this.procStateData = procStateData;
     this.parentId = parentId;
-    this.nonceKey = nonceKey;
     this.lastUpdate = lastUpdate;
     this.submittedTime = submittedTime;
 
@@ -70,11 +71,12 @@ public class ProcedureInfo implements Cloneable {
     this.result = result;
   }
 
+  @Override
   @edu.umd.cs.findbugs.annotations.SuppressWarnings(value="CN_IDIOM_NO_SUPER_CALL",
       justification="Intentional; calling super class clone doesn't make sense here.")
   public ProcedureInfo clone() {
-    return new ProcedureInfo(procId, procName, procOwner, procState, parentId, nonceKey,
-      exception, lastUpdate, submittedTime, result);
+    return new ProcedureInfo(procId, procName, procOwner, procState,
+        procStateData, parentId, exception, lastUpdate, submittedTime, result);
   }
 
   @Override
@@ -94,6 +96,9 @@ public class ProcedureInfo implements Cloneable {
     sb.append(", state=");
     sb.append(procState);
 
+    sb.append(", stateData=");
+    sb.append(procStateData);
+
     long now = EnvironmentEdgeManager.currentTime();
     sb.append(", submittedTime=");
     sb.append(StringUtils.formatTime(now - submittedTime));
@@ -129,6 +134,10 @@ public class ProcedureInfo implements Cloneable {
     return procState;
   }
 
+  public String getProcStateData() {
+    return procStateData;
+  }
+
   public boolean hasParentId() {
     return (parentId != -1);
   }
@@ -137,10 +146,6 @@ public class ProcedureInfo implements Cloneable {
     return parentId;
   }
 
-  public NonceKey getNonceKey() {
-    return nonceKey;
-  }
-
   public boolean isFailed() {
     return exception != null;
   }
@@ -178,38 +183,22 @@ public class ProcedureInfo implements Cloneable {
     return lastUpdate - submittedTime;
   }
 
-  @InterfaceAudience.Private
-  public boolean hasClientAckTime() {
-    return clientAckTime != -1;
-  }
-
-  @InterfaceAudience.Private
-  public long getClientAckTime() {
-    return clientAckTime;
-  }
+  public String describe() {
+    Map<String, Object> description = new LinkedHashMap<>();
 
-  @InterfaceAudience.Private
-  public void setClientAckTime(final long timestamp) {
-    this.clientAckTime = timestamp;
-  }
+    description.put("ID", procId);
+    description.put("PARENT_ID", parentId);
+    description.put("STATE", procState);
+    description.put("OWNER", procOwner);
+    description.put("TYPE", procName);
+    description.put("START_TIME", new Date(submittedTime));
+    description.put("LAST_UPDATE", new Date(lastUpdate));
 
-  /**
-   * Check if the user is this procedure's owner
-   * @param procInfo the procedure to check
-   * @param user the user
-   * @return true if the user is the owner of the procedure,
-   *   false otherwise or the owner is unknown.
-   */
-  @InterfaceAudience.Private
-  public static boolean isProcedureOwner(final ProcedureInfo procInfo, final User user) {
-    if (user == null) {
-      return false;
-    }
-    String procOwner = procInfo.getProcOwner();
-    if (procOwner == null) {
-      return false;
+    if (exception != null) {
+      description.put("ERRORS", exception.getMessage());
     }
-    return procOwner.equals(user.getShortName());
-  }
+    description.put("PARAMETERS", procStateData);
 
+    return JRubyFormat.print(description);
+  }
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JRubyFormat.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JRubyFormat.java
new file mode 100644
index 0000000000..9aba10f033
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/JRubyFormat.java
@@ -0,0 +1,116 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+
+/**
+ * Utility class for converting objects to JRuby.
+ *
+ * It handles null, Boolean, Number, String, List<Object>, Map<String, Object> structures.
+ *
+ * <p>
+ * E.g.
+ * <pre>
+ * Map<String, Object> map = new LinkedHashMap<>();
+ * map.put("null", null);
+ * map.put("boolean", true);
+ * map.put("number", 1);
+ * map.put("string", "str");
+ * map.put("list", Lists.newArrayList(1, "2", true));
+ * </pre>
+ * </p>
+ *
+ * <p>
+ * Calling {@link #print(Object)} method will result:
+ * <pre>
+ * { null => '', boolean => 'true', number => '1', string => 'str', list => [ '1', '2', 'true' ] }
+ * </pre>
+ * </p>
+ */
+@InterfaceAudience.Private
+public final class JRubyFormat {
+  private JRubyFormat() {
+  }
+
+  @SuppressWarnings({ "unchecked" })
+  private static void appendJRuby(StringBuilder builder, Object object) {
+    if (object == null) {
+      builder.append("''");
+    } else if (object instanceof List) {
+      builder.append("[");
+
+      boolean first = true;
+
+      for (Object element: (List<Object>)object) {
+        if (first) {
+          first = false;
+          builder.append(" ");
+        } else {
+          builder.append(", ");
+        }
+
+        appendJRuby(builder, element);
+      }
+
+      if (!first) {
+        builder.append(" ");
+      }
+
+      builder.append("]");
+    } else if (object instanceof Map) {
+      builder.append("{");
+
+      boolean first = true;
+
+      for (Entry<String, Object> entry: ((Map<String, Object>)object).entrySet()) {
+        if (first) {
+          first = false;
+          builder.append(" ");
+        } else {
+          builder.append(", ");
+        }
+
+        builder.append(entry.getKey());
+        builder.append(" => ");
+        appendJRuby(builder, entry.getValue());
+      }
+
+      if (!first) {
+        builder.append(" ");
+      }
+
+      builder.append("}");
+    } else {
+      builder.append("'").append(object).append("'");
+    }
+  }
+
+  public static String print(Object object) {
+    StringBuilder builder = new StringBuilder();
+
+    appendJRuby(builder, object);
+
+    return builder.toString();
+  }
+}
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestJRubyFormat.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestJRubyFormat.java
new file mode 100644
index 0000000000..c9ac4c5594
--- /dev/null
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestJRubyFormat.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util;
+
+import static org.junit.Assert.assertEquals;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists;
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(SmallTests.class)
+public class TestJRubyFormat {
+  @Test
+  public void testPrint() {
+    Map<String, Object> map = new LinkedHashMap<>();
+    map.put("null", null);
+    map.put("boolean", true);
+    map.put("number", 1);
+    map.put("string", "str");
+    map.put("list", Lists.newArrayList(1, "2", true));
+
+    String jrubyString = JRubyFormat.print(map);
+    assertEquals("{ null => '', boolean => 'true', number => '1', "
+        + "string => 'str', list => [ '1', '2', 'true' ] }", jrubyString);
+  }
+}
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResource.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResource.java
new file mode 100644
index 0000000000..e3320ab26f
--- /dev/null
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/LockedResource.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.procedure2;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.procedure2.LockedResourceType;
+
+@InterfaceAudience.Private
+public class LockedResource {
+  private final LockedResourceType resourceType;
+  private final String resourceName;
+  private final LockType lockType;
+  private final Procedure<?> exclusiveLockOwnerProcedure;
+  private final int sharedLockCount;
+  private final List<Procedure<?>> waitingProcedures;
+
+  public LockedResource(LockedResourceType resourceType, String resourceName,
+      LockType lockType, Procedure<?> exclusiveLockOwnerProcedure,
+      int sharedLockCount, List<Procedure<?>> waitingProcedures) {
+    this.resourceType = resourceType;
+    this.resourceName = resourceName;
+    this.lockType = lockType;
+    this.exclusiveLockOwnerProcedure = exclusiveLockOwnerProcedure;
+    this.sharedLockCount = sharedLockCount;
+    this.waitingProcedures = waitingProcedures;
+  }
+
+  public LockedResourceType getResourceType() {
+    return resourceType;
+  }
+
+  public String getResourceName() {
+    return resourceName;
+  }
+
+  public LockType getLockType() {
+    return lockType;
+  }
+
+  public Procedure<?> getExclusiveLockOwnerProcedure() {
+    return exclusiveLockOwnerProcedure;
+  }
+
+  public int getSharedLockCount() {
+    return sharedLockCount;
+  }
+
+  public List<Procedure<?>> getWaitingProcedures() {
+    return waitingProcedures;
+  }
+}
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java
index 335e83c205..e3520e99e8 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/Procedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
@@ -167,7 +165,7 @@ public abstract class Procedure<TEnvironment> implements Comparable<Procedure<TE
    * be able to resume on failure.
    * @param stream the stream that will contain the user serialized data
    */
-  protected abstract void serializeStateData(final OutputStream stream)
+  protected abstract void serializeStateData(final ProcedureStateSerializer serializer)
     throws IOException;
 
   /**
@@ -175,7 +173,7 @@ public abstract class Procedure<TEnvironment> implements Comparable<Procedure<TE
    * state.
    * @param stream the stream that contains the user serialized data
    */
-  protected abstract void deserializeStateData(final InputStream stream)
+  protected abstract void deserializeStateData(final ProcedureStateDeserializer deserializer)
     throws IOException;
 
   /**
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
index c110c2d3ca..c59ab34a8e 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureExecutor.java
@@ -22,8 +22,6 @@ import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTe
 import org.apache.hadoop.hbase.shaded.com.google.common.base.Preconditions;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
@@ -750,14 +748,22 @@ public class ProcedureExecutor<TEnvironment> {
     }
   }
 
-  private static class FailedProcedure<TEnvironment> extends Procedure<TEnvironment> {
+  public static class FailedProcedure<TEnvironment> extends Procedure<TEnvironment> {
     private String procName;
 
-    public FailedProcedure(NonceKey nonceKey, String procName, User owner,
-        IOException exception) {
+    public FailedProcedure() {
+    }
+
+    public FailedProcedure(long procId, String procName, User owner,
+        NonceKey nonceKey, IOException exception) {
       this.procName = procName;
-      setNonceKey(nonceKey);
+      setProcId(procId);
+      setState(ProcedureState.ROLLEDBACK);
       setOwner(owner);
+      setNonceKey(nonceKey);
+      long currentTime = EnvironmentEdgeManager.currentTime();
+      setSubmittedTime(currentTime);
+      setLastUpdate(currentTime);
       setFailure(Objects.toString(exception.getMessage(), ""), exception);
     }
 
@@ -785,11 +791,13 @@ public class ProcedureExecutor<TEnvironment> {
     }
 
     @Override
-    protected void serializeStateData(OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
     }
 
     @Override
-    protected void deserializeStateData(InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
     }
   }
 
@@ -809,7 +817,9 @@ public class ProcedureExecutor<TEnvironment> {
     final Long procId = nonceKeysToProcIdsMap.get(nonceKey);
     if (procId == null || completed.containsKey(procId)) return;
 
-    Procedure proc = new FailedProcedure(nonceKey, procName, procOwner, exception);
+    Procedure<?> proc = new FailedProcedure(procId.longValue(),
+        procName, procOwner, nonceKey, exception);
+
     completed.putIfAbsent(procId, new CompletedProcedureRetainer(proc));
   }
 
@@ -1048,9 +1058,11 @@ public class ProcedureExecutor<TEnvironment> {
    * List procedures.
    * @return the procedures in a list
    */
-  public List<Procedure> listProcedures() {
-    final List<Procedure> procedureLists = new ArrayList<>(procedures.size() + completed.size());
-    procedureLists.addAll(procedures.values());
+  public List<Procedure<?>> listProcedures() {
+    final List<Procedure<?>> procedureLists = new ArrayList<>(procedures.size() + completed.size());
+    for (Procedure<?> procedure : procedures.values()) {
+      procedureLists.add(procedure);
+    }
     // Note: The procedure could show up twice in the list with different state, as
     // it could complete after we walk through procedures list and insert into
     // procedureList - it is ok, as we will use the information in the ProcedureInfo
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInMemoryChore.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInMemoryChore.java
index b148dae98a..9c5c401195 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInMemoryChore.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureInMemoryChore.java
@@ -18,9 +18,7 @@
 
 package org.apache.hadoop.hbase.procedure2;
 
-import java.io.InputStream;
-import java.io.OutputStream;
-
+import java.io.IOException;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceStability;
 
@@ -58,12 +56,12 @@ public abstract class ProcedureInMemoryChore<TEnvironment> extends Procedure<TEn
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) {
-    throw new UnsupportedOperationException();
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) {
-    throw new UnsupportedOperationException();
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
   }
 }
\ No newline at end of file
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureScheduler.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureScheduler.java
index a5a126d556..25e254b6e5 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureScheduler.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureScheduler.java
@@ -125,13 +125,13 @@ public interface ProcedureScheduler {
    * List lock queues.
    * @return the locks
    */
-  // TODO: This seems to be the wrong place to hang this method.
-  List<LockInfo> listLocks();
+  List<LockedResource> listLocks();
 
   /**
-   * @return {@link LockInfo} for resource of specified type & name. null if resource is not locked.
+   * @return {@link LockedResource} for resource of specified type & name. null if resource is not locked.
    */
-  LockInfo getLockInfoForResource(LockInfo.ResourceType resourceType, String resourceName);
+  LockedResource getLockResource(LockedResourceType resourceType, String resourceName);
+
   /**
    * Returns the number of elements in this queue.
    * @return the number of elements in this queue.
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateDeserializer.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateDeserializer.java
new file mode 100644
index 0000000000..ab42dba9e0
--- /dev/null
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateDeserializer.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.procedure2;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Message;
+
+public interface ProcedureStateDeserializer {
+  <M extends Message> M deserialize(Class<M> clazz) throws IOException;
+}
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateSerializer.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateSerializer.java
new file mode 100644
index 0000000000..b691e49cf7
--- /dev/null
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureStateSerializer.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.procedure2;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Message;
+
+public interface ProcedureStateSerializer {
+  void serialize(Message message) throws IOException;
+}
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java
index 3232f2b3ba..9cecae20ac 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/ProcedureUtil.java
@@ -18,26 +18,43 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
+import java.io.InputStream;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Modifier;
-
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.ProcedureInfo;
-import org.apache.hadoop.hbase.ProcedureState;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;
+import org.apache.hadoop.hbase.shaded.com.google.common.base.Preconditions;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Any;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.BytesValue;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Internal;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Message;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Parser;
 import org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.util.JsonFormat;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.util.JsonFormat.TypeRegistry;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
-import org.apache.hadoop.hbase.util.ForeignExceptionUtil;
 import org.apache.hadoop.hbase.util.NonceKey;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.base.Preconditions;
-
 /**
  * Helper to convert to/from ProcedureProtos
  */
 @InterfaceAudience.Private
 public final class ProcedureUtil {
+  private static final JsonFormat.Printer jsonPrinter;
+
+  static {
+    TypeRegistry.Builder builder = TypeRegistry.newBuilder();
+    builder
+      .add(BytesValue.getDescriptor())
+      .add(ProcedureProtos.getDescriptor().getMessageTypes())
+      .add(MasterProcedureProtos.getDescriptor().getMessageTypes());
+    TypeRegistry typeRegistry = builder.build();
+    jsonPrinter = JsonFormat.printer().usingTypeRegistry(typeRegistry).omittingInsignificantWhitespace();
+  }
+
   private ProcedureUtil() { }
 
   // ==========================================================================
@@ -130,15 +147,14 @@ public final class ProcedureUtil {
       builder.setResult(UnsafeByteOperations.unsafeWrap(result));
     }
 
-    final ByteString.Output stateStream = ByteString.newOutput();
-    try {
-      proc.serializeStateData(stateStream);
-      if (stateStream.size() > 0) {
-        builder.setStateData(stateStream.toByteString());
+    ProcedureStateSerializer serializer = new ProcedureStateSerializer() {
+      @Override
+      public void serialize(Message message) throws IOException {
+        Any packedMessage = Any.pack(message);
+        builder.addStateMessage(packedMessage);
       }
-    } finally {
-      stateStream.close();
-    }
+    };
+    proc.serializeStateData(serializer);
 
     if (proc.getNonceKey() != null) {
       builder.setNonceGroup(proc.getNonceKey().getNonceGroup());
@@ -198,8 +214,47 @@ public final class ProcedureUtil {
       proc.setNonceKey(new NonceKey(proto.getNonceGroup(), proto.getNonce()));
     }
 
-    // we want to call deserialize even when the stream is empty, mainly for testing.
-    proc.deserializeStateData(proto.getStateData().newInput());
+    ProcedureStateDeserializer deserializer = null;
+
+    if (proto.getStateMessageCount() > 0) {
+      deserializer = new ProcedureStateDeserializer() {
+        private int index;
+
+        @Override
+        public <M extends Message> M deserialize(Class<M> clazz)
+            throws IOException {
+          if (index >= proto.getStateMessageCount()) {
+            throw new IOException("Invalid state message index: " + index);
+          }
+
+          try {
+            Any packedMessage = proto.getStateMessage(index++);
+            return packedMessage.unpack(clazz);
+          } catch (InvalidProtocolBufferException e) {
+            throw e.unwrapIOException();
+          }
+        }
+      };
+    } else if (proto.hasStateData()) {
+      InputStream inputStream = proto.getStateData().newInput();
+      deserializer = new ProcedureStateDeserializer() {
+        @SuppressWarnings("unchecked")
+        @Override
+        public <M extends Message> M deserialize(Class<M> clazz)
+            throws IOException {
+          Parser<M> parser = (Parser<M>) Internal.getDefaultInstance(clazz).getParserForType();
+          try {
+            return parser.parseDelimitedFrom(inputStream);
+          } catch (InvalidProtocolBufferException e) {
+            throw e.unwrapIOException();
+          }
+        }
+      };
+    }
+
+    if (deserializer != null) {
+      proc.deserializeStateData(deserializer);
+    }
 
     return proc;
   }
@@ -208,77 +263,53 @@ public final class ProcedureUtil {
   //  convert to and from ProcedureInfo object
   // ==========================================================================
 
-  /**
-   * @return Convert the current {@link ProcedureInfo} into a Protocol Buffers Procedure
-   * instance.
-   */
-  public static ProcedureProtos.Procedure convertToProtoProcedure(final ProcedureInfo procInfo) {
-    final ProcedureProtos.Procedure.Builder builder = ProcedureProtos.Procedure.newBuilder();
-
-    builder.setClassName(procInfo.getProcName());
-    builder.setProcId(procInfo.getProcId());
-    builder.setSubmittedTime(procInfo.getSubmittedTime());
-    builder.setState(ProcedureProtos.ProcedureState.valueOf(procInfo.getProcState().name()));
-    builder.setLastUpdate(procInfo.getLastUpdate());
-
-    if (procInfo.hasParentId()) {
-      builder.setParentId(procInfo.getParentId());
+  public static LockServiceProtos.LockedResourceType convertToProtoResourceType(
+      LockedResourceType resourceType) {
+    switch (resourceType) {
+    case SERVER:
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_SERVER;
+    case NAMESPACE:
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_NAMESPACE;
+    case TABLE:
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_TABLE;
+    case REGION:
+      return LockServiceProtos.LockedResourceType.RESOURCE_TYPE_REGION;
+    default:
+      throw new IllegalArgumentException("Unknown resource type: " + resourceType);
     }
+  }
 
-    if (procInfo.hasOwner()) {
-      builder.setOwner(procInfo.getProcOwner());
-    }
+  public static LockServiceProtos.LockType convertToProtoLockType(LockType lockType) {
+    return LockServiceProtos.LockType.valueOf(lockType.name());
+  }
 
-    if (procInfo.isFailed()) {
-      builder.setException(ForeignExceptionUtil.toProtoForeignException(procInfo.getException()));
-    }
+  public static LockServiceProtos.LockedResource convertToProtoLockedResource(
+      LockedResource lockedResource) throws IOException
+  {
+    LockServiceProtos.LockedResource.Builder builder =
+        LockServiceProtos.LockedResource.newBuilder();
 
-    if (procInfo.hasResultData()) {
-      builder.setResult(UnsafeByteOperations.unsafeWrap(procInfo.getResult()));
-    }
+    builder
+        .setResourceType(convertToProtoResourceType(lockedResource.getResourceType()))
+        .setResourceName(lockedResource.getResourceName())
+        .setLockType(convertToProtoLockType(lockedResource.getLockType()));
 
-    return builder.build();
-  }
+    Procedure<?> exclusiveLockOwnerProcedure = lockedResource.getExclusiveLockOwnerProcedure();
 
-  /**
-   * Helper to convert the protobuf object.
-   * @return Convert the current Protocol Buffers Procedure to {@link ProcedureInfo}
-   * instance.
-   */
-  public static ProcedureInfo convertToProcedureInfo(final ProcedureProtos.Procedure procProto) {
-    NonceKey nonceKey = null;
-    if (procProto.getNonce() != HConstants.NO_NONCE) {
-      nonceKey = new NonceKey(procProto.getNonceGroup(), procProto.getNonce());
+    if (exclusiveLockOwnerProcedure != null) {
+      ProcedureProtos.Procedure exclusiveLockOwnerProcedureProto =
+          convertToProtoProcedure(exclusiveLockOwnerProcedure);
+      builder.setExclusiveLockOwnerProcedure(exclusiveLockOwnerProcedureProto);
     }
 
-    return new ProcedureInfo(procProto.getProcId(), procProto.getClassName(),
-        procProto.hasOwner() ? procProto.getOwner() : null,
-        convertToProcedureState(procProto.getState()),
-        procProto.hasParentId() ? procProto.getParentId() : -1, nonceKey,
-        procProto.hasException() ?
-          ForeignExceptionUtil.toIOException(procProto.getException()) : null,
-        procProto.getLastUpdate(), procProto.getSubmittedTime(),
-        procProto.hasResult() ? procProto.getResult().toByteArray() : null);
-  }
-
-  public static ProcedureState convertToProcedureState(ProcedureProtos.ProcedureState state) {
-    return ProcedureState.valueOf(state.name());
-  }
+    builder.setSharedLockCount(lockedResource.getSharedLockCount());
 
-  public static ProcedureInfo convertToProcedureInfo(final Procedure proc) {
-    return convertToProcedureInfo(proc, null);
-  }
+    for (Procedure<?> waitingProcedure : lockedResource.getWaitingProcedures()) {
+      ProcedureProtos.Procedure waitingProcedureProto =
+          convertToProtoProcedure(waitingProcedure);
+      builder.addWaitingProcedures(waitingProcedureProto);
+    }
 
-  /**
-   * Helper to create the ProcedureInfo from Procedure.
-   */
-  public static ProcedureInfo convertToProcedureInfo(final Procedure proc,
-      final NonceKey nonceKey) {
-    final RemoteProcedureException exception = proc.hasException() ? proc.getException() : null;
-    return new ProcedureInfo(proc.getProcId(), proc.toStringClass(), proc.getOwner(),
-        convertToProcedureState(proc.getState()),
-        proc.hasParent() ? proc.getParentProcId() : -1, nonceKey,
-        exception != null ? exception.unwrapRemoteIOException() : null,
-        proc.getLastUpdate(), proc.getSubmittedTime(), proc.getResult());
+    return builder.build();
   }
 }
\ No newline at end of file
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SequentialProcedure.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SequentialProcedure.java
index 64bb27892e..b1111159f4 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SequentialProcedure.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SequentialProcedure.java
@@ -19,9 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.SequentialProcedureData;
@@ -69,15 +66,17 @@ public abstract class SequentialProcedure<TEnvironment> extends Procedure<TEnvir
   }
 
   @Override
-  protected void serializeStateData(final OutputStream stream) throws IOException {
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
     SequentialProcedureData.Builder data = SequentialProcedureData.newBuilder();
     data.setExecuted(executed);
-    data.build().writeDelimitedTo(stream);
+    serializer.serialize(data.build());
   }
 
   @Override
-  protected void deserializeStateData(final InputStream stream) throws IOException {
-    SequentialProcedureData data = SequentialProcedureData.parseDelimitedFrom(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    SequentialProcedureData data = deserializer.deserialize(SequentialProcedureData.class);
     executed = data.getExecuted();
   }
 }
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java
index 69c59c8139..3f519cbf20 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/SimpleProcedureScheduler.java
@@ -78,12 +78,13 @@ public class SimpleProcedureScheduler extends AbstractProcedureScheduler {
   }
 
   @Override
-  public List<LockInfo> listLocks() {
+  public List<LockedResource> listLocks() {
     return Collections.emptyList();
   }
 
   @Override
-  public LockInfo getLockInfoForResource(LockInfo.ResourceType resourceType, String resourceName) {
+  public LockedResource getLockResource(LockedResourceType resourceType,
+      String resourceName) {
     return null;
   }
 }
\ No newline at end of file
diff --git a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
index 5de50668fa..7a549c69c1 100644
--- a/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
+++ b/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/StateMachineProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
@@ -285,17 +283,19 @@ public abstract class StateMachineProcedure<TEnvironment, TState>
   }
 
   @Override
-  protected void serializeStateData(final OutputStream stream) throws IOException {
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
     StateMachineProcedureData.Builder data = StateMachineProcedureData.newBuilder();
     for (int i = 0; i < stateCount; ++i) {
       data.addState(states[i]);
     }
-    data.build().writeDelimitedTo(stream);
+    serializer.serialize(data.build());
   }
 
   @Override
-  protected void deserializeStateData(final InputStream stream) throws IOException {
-    StateMachineProcedureData data = StateMachineProcedureData.parseDelimitedFrom(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    StateMachineProcedureData data = deserializer.deserialize(StateMachineProcedureData.class);
     stateCount = data.getStateCount();
     if (stateCount > 0) {
       states = new int[stateCount];
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
index 5cdbc35481..a58fd883b1 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/ProcedureTestingUtility.java
@@ -23,8 +23,6 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Set;
 import java.util.concurrent.Callable;
@@ -37,11 +35,12 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.exceptions.IllegalArgumentIOException;
 import org.apache.hadoop.hbase.exceptions.TimeoutIOException;
-import org.apache.hadoop.hbase.io.util.StreamUtils;
 import org.apache.hadoop.hbase.procedure2.store.NoopProcedureStore;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore.ProcedureIterator;
 import org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.BytesValue;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState;
 import org.apache.hadoop.hbase.util.NonceKey;
 import org.apache.hadoop.hbase.util.Threads;
@@ -367,11 +366,13 @@ public class ProcedureTestingUtility {
     protected boolean abort(TEnv env) { return false; }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
     }
   }
 
@@ -416,19 +417,23 @@ public class ProcedureTestingUtility {
     }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
-      StreamUtils.writeRawVInt32(stream, data != null ? data.length : 0);
-      if (data != null) stream.write(data);
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
+      ByteString dataString = ByteString.copyFrom((data == null) ? new byte[0] : data);
+      BytesValue.Builder builder = BytesValue.newBuilder().setValue(dataString);
+      serializer.serialize(builder.build());
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
-      int len = StreamUtils.readRawVarint32(stream);
-      if (len > 0) {
-        data = new byte[len];
-        stream.read(data);
-      } else {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
+      BytesValue bytesValue = deserializer.deserialize(BytesValue.class);
+      ByteString dataString = bytesValue.getValue();
+
+      if (dataString.isEmpty()) {
         data = null;
+      } else {
+        data = dataString.toByteArray();
       }
     }
 
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureEvents.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureEvents.java
index b81e0f9045..28b4c1a80f 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureEvents.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureEvents.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.commons.logging.Log;
@@ -28,10 +26,10 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseCommonTestingUtility;
-import org.apache.hadoop.hbase.io.util.StreamUtils;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.NoopProcedure;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Int32Value;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
@@ -42,8 +40,6 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
 
 @Category({MasterTests.class, SmallTests.class})
 public class TestProcedureEvents {
@@ -163,15 +159,23 @@ public class TestProcedureEvents {
     }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
-      StreamUtils.writeRawVInt32(stream, ntimeouts.get());
-      StreamUtils.writeRawVInt32(stream, maxTimeouts);
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
+      Int32Value.Builder ntimeoutsBuilder = Int32Value.newBuilder().setValue(ntimeouts.get());
+      serializer.serialize(ntimeoutsBuilder.build());
+
+      Int32Value.Builder maxTimeoutsBuilder = Int32Value.newBuilder().setValue(maxTimeouts);
+      serializer.serialize(maxTimeoutsBuilder.build());
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
-      ntimeouts.set(StreamUtils.readRawVarint32(stream));
-      maxTimeouts = StreamUtils.readRawVarint32(stream);
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
+      Int32Value ntimeoutsValue = deserializer.deserialize(Int32Value.class);
+      ntimeouts.set(ntimeoutsValue.getValue());
+
+      Int32Value maxTimeoutsValue = deserializer.deserialize(Int32Value.class);
+      maxTimeouts = maxTimeoutsValue.getValue();
     }
   }
 
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java
index 9681bfb1e3..18ef5a1891 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureRecovery.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.CountDownLatch;
 
@@ -31,6 +29,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseCommonTestingUtility;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Int32Value;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -382,17 +381,19 @@ public class TestProcedureRecovery {
     }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
-      super.serializeStateData(stream);
-      stream.write(Bytes.toBytes(iResult));
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
+      super.serializeStateData(serializer);
+      Int32Value.Builder builder = Int32Value.newBuilder().setValue(iResult);
+      serializer.serialize(builder.build());
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
-      super.deserializeStateData(stream);
-      byte[] data = new byte[4];
-      stream.read(data);
-      iResult = Bytes.toInt(data);
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
+      super.deserializeStateData(deserializer);
+      Int32Value value = deserializer.deserialize(Int32Value.class);
+      iResult = value.getValue();
     }
   }
 
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
index bd614e38c9..4584d82417 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureReplayOrder.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.concurrent.atomic.AtomicLong;
 
@@ -29,9 +27,9 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseCommonTestingUtility;
-import org.apache.hadoop.hbase.io.util.StreamUtils;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
 import org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Int64Value;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
 
@@ -195,13 +193,17 @@ public class TestProcedureReplayOrder {
     protected boolean abort(TestProcedureEnv env) { return true; }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
-      StreamUtils.writeLong(stream, execId);
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
+      Int64Value.Builder builder = Int64Value.newBuilder().setValue(execId);
+      serializer.serialize(builder.build());
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
-      execId = StreamUtils.readLong(stream);
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
+      Int64Value value = deserializer.deserialize(Int64Value.class);
+      execId = value.getValue();
       step = 2;
     }
   }
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
index 0146bc7ea3..843005b487 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureSuspended.java
@@ -21,8 +21,6 @@ package org.apache.hadoop.hbase.procedure2;
 import static org.junit.Assert.assertEquals;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -251,11 +249,13 @@ public class TestProcedureSuspended {
     protected boolean abort(TestProcEnv env) { return false; }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
     }
   }
 
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureToString.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureToString.java
index 78daf5a92b..fc2259278f 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureToString.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureToString.java
@@ -20,9 +20,6 @@ package org.apache.hadoop.hbase.procedure2;
 import static org.junit.Assert.assertTrue;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ServerCrashState;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos.ProcedureState;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
@@ -57,11 +54,13 @@ public class TestProcedureToString {
     }
 
     @Override
-    protected void serializeStateData(OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
     }
 
     @Override
-    protected void deserializeStateData(InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
     }
   }
 
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureUtil.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureUtil.java
index 7f98b80abc..dec5854151 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureUtil.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestProcedureUtil.java
@@ -18,11 +18,7 @@
 
 package org.apache.hadoop.hbase.procedure2;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.TestProcedure;
-import org.apache.hadoop.hbase.shaded.com.google.protobuf.util.JsonFormat;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
@@ -34,8 +30,6 @@ import static org.junit.Assert.assertEquals;
 
 @Category({MasterTests.class, SmallTests.class})
 public class TestProcedureUtil {
-  private static final Log LOG = LogFactory.getLog(TestProcedureUtil.class);
-
   @Test
   public void testValidation() throws Exception {
     ProcedureUtil.validateClass(new TestProcedure(10));
@@ -49,34 +43,15 @@ public class TestProcedureUtil {
   @Test
   public void testConvert() throws Exception {
     // check Procedure to protobuf conversion
-    final TestProcedure proc1 = new TestProcedure(10);
+    final TestProcedure proc1 = new TestProcedure(10, 1, new byte[] { 65 });
     final ProcedureProtos.Procedure proto1 = ProcedureUtil.convertToProtoProcedure(proc1);
     final TestProcedure proc2 = (TestProcedure)ProcedureUtil.convertToProcedure(proto1);
     final ProcedureProtos.Procedure proto2 = ProcedureUtil.convertToProtoProcedure(proc2);
     assertEquals(false, proto2.hasResult());
     assertEquals("Procedure protobuf does not match", proto1, proto2);
-
-    // remove the state-data from the procedure protobuf to compare it to the gen ProcedureInfo
-    final ProcedureProtos.Procedure pbproc = proto2.toBuilder().clearStateData().build();
-
-    // check ProcedureInfo to protobuf conversion
-    final ProcedureInfo protoInfo1 = ProcedureUtil.convertToProcedureInfo(proc1);
-    final ProcedureProtos.Procedure proto3 = ProcedureUtil.convertToProtoProcedure(protoInfo1);
-    final ProcedureInfo protoInfo2 = ProcedureUtil.convertToProcedureInfo(proto3);
-    final ProcedureProtos.Procedure proto4 = ProcedureUtil.convertToProtoProcedure(protoInfo2);
-    assertEquals("ProcedureInfo protobuf does not match", proto3, proto4);
-    assertEquals("ProcedureInfo/Procedure protobuf does not match", pbproc, proto3);
-    assertEquals("ProcedureInfo/Procedure protobuf does not match", pbproc, proto4);
   }
 
   public static class TestProcedureNoDefaultConstructor extends TestProcedure {
     public TestProcedureNoDefaultConstructor(int x) {}
   }
-
-  public static void main(final String [] args) throws Exception {
-    final TestProcedure proc1 = new TestProcedure(10);
-    final ProcedureProtos.Procedure proto1 = ProcedureUtil.convertToProtoProcedure(proc1);
-    JsonFormat.Printer printer = JsonFormat.printer().omittingInsignificantWhitespace();
-    System.out.println(printer.print(proto1));
-  }
 }
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
index b1d0669394..3f0666bb03 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/TestYieldProcedures.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.procedure2;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -336,11 +334,13 @@ public class TestYieldProcedures {
     }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
     }
 
     @Override
-    protected void deserializeStateData(final InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
     }
   }
 
@@ -353,6 +353,7 @@ public class TestYieldProcedures {
 
     public TestScheduler() {}
 
+    @Override
     public void addFront(final Procedure proc) {
       addFrontCalls++;
       super.addFront(proc);
diff --git a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java
index 9b8c46fc58..62d06038f9 100644
--- a/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java
+++ b/hbase-procedure/src/test/java/org/apache/hadoop/hbase/procedure2/store/wal/TestWALProcedureStore.java
@@ -36,6 +36,8 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseCommonTestingUtility;
 import org.apache.hadoop.hbase.procedure2.Procedure;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.LoadCounter;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.TestProcedure;
@@ -43,9 +45,9 @@ import org.apache.hadoop.hbase.procedure2.SequentialProcedure;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStore.ProcedureIterator;
 import org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.Int64Value;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
-import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.IOUtils;
 
 import org.junit.After;
@@ -514,7 +516,7 @@ public class TestWALProcedureStore {
     storeRestart(loader);
     assertTrue(procStore.getCorruptedLogs() != null);
     assertEquals(1, procStore.getCorruptedLogs().size());
-    assertEquals(85, loader.getLoadedCount());
+    assertEquals(87, loader.getLoadedCount());
     assertEquals(0, loader.getCorruptedCount());
   }
 
@@ -911,22 +913,22 @@ public class TestWALProcedureStore {
     protected boolean abort(Void env) { return false; }
 
     @Override
-    protected void serializeStateData(final OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
       long procId = getProcId();
       if (procId % 2 == 0) {
-        stream.write(Bytes.toBytes(procId));
+        Int64Value.Builder builder = Int64Value.newBuilder().setValue(procId);
+        serializer.serialize(builder.build());
       }
     }
 
     @Override
-    protected void deserializeStateData(InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
       long procId = getProcId();
       if (procId % 2 == 0) {
-        byte[] bProcId = new byte[8];
-        assertEquals(8, stream.read(bProcId));
-        assertEquals(procId, Bytes.toLong(bProcId));
-      } else {
-        assertEquals(0, stream.available());
+        Int64Value value = deserializer.deserialize(Int64Value.class);
+        assertEquals(procId, value.getValue());
       }
     }
   }
diff --git a/hbase-protocol-shaded/src/main/protobuf/LockService.proto b/hbase-protocol-shaded/src/main/protobuf/LockService.proto
index 1898e68794..f216e3ff26 100644
--- a/hbase-protocol-shaded/src/main/protobuf/LockService.proto
+++ b/hbase-protocol-shaded/src/main/protobuf/LockService.proto
@@ -71,25 +71,20 @@ message LockProcedureData {
   optional bool is_master_lock = 6 [default = false];
 }
 
-enum ResourceType {
+enum LockedResourceType {
   RESOURCE_TYPE_SERVER = 1;
   RESOURCE_TYPE_NAMESPACE = 2;
   RESOURCE_TYPE_TABLE = 3;
   RESOURCE_TYPE_REGION = 4;
 }
 
-message WaitingProcedure {
-  required LockType lock_type = 1;
-  required Procedure procedure = 2;
-}
-
-message LockInfo {
-  required ResourceType resource_type = 1;
+message LockedResource {
+  required LockedResourceType resource_type = 1;
   optional string resource_name = 2;
   required LockType lock_type = 3;
   optional Procedure exclusive_lock_owner_procedure = 4;
   optional int32 shared_lock_count = 5;
-  repeated WaitingProcedure waitingProcedures = 6;
+  repeated Procedure waitingProcedures = 6;
 }
 
 service LockService {
diff --git a/hbase-protocol-shaded/src/main/protobuf/Master.proto b/hbase-protocol-shaded/src/main/protobuf/Master.proto
index 8d7cad94d9..3a903fdec1 100644
--- a/hbase-protocol-shaded/src/main/protobuf/Master.proto
+++ b/hbase-protocol-shaded/src/main/protobuf/Master.proto
@@ -554,7 +554,7 @@ message ListLocksRequest {
 }
 
 message ListLocksResponse {
-  repeated LockInfo lock = 1;
+  repeated LockedResource lock = 1;
 }
 
 message SetQuotaRequest {
diff --git a/hbase-protocol-shaded/src/main/protobuf/Procedure.proto b/hbase-protocol-shaded/src/main/protobuf/Procedure.proto
index 1a3ecf5ac1..c13a37ec55 100644
--- a/hbase-protocol-shaded/src/main/protobuf/Procedure.proto
+++ b/hbase-protocol-shaded/src/main/protobuf/Procedure.proto
@@ -23,6 +23,7 @@ option java_generic_services = true;
 option java_generate_equals_and_hash = true;
 option optimize_for = SPEED;
 
+import "google/protobuf/any.proto";
 import "ErrorHandling.proto";
 
 enum ProcedureState {
@@ -55,7 +56,8 @@ message Procedure {
   // user state/results
   optional ForeignExceptionMessage exception = 10;
   optional bytes result = 11;           // opaque (user) result structure
-  optional bytes state_data = 12;       // opaque (user) procedure internal-state
+  optional bytes state_data = 12;       // opaque (user) procedure internal-state - OBSOLATE
+  repeated google.protobuf.Any state_message = 15; // opaque (user) procedure internal-state
 
   // Nonce to prevent same procedure submit by multiple times
   optional uint64 nonce_group = 13 [default = 0];
diff --git a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
index b9c19227bb..914a6722fb 100644
--- a/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
+++ b/hbase-rsgroup/src/main/java/org/apache/hadoop/hbase/rsgroup/RSGroupAdminServer.java
@@ -38,17 +38,16 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.constraint.ConstraintException;
 import org.apache.hadoop.hbase.master.HMaster;
-import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
-import org.apache.hadoop.hbase.master.assignment.RegionStates.RegionStateNode;
 import org.apache.hadoop.hbase.master.LoadBalancer;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.RegionState;
 import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
+import org.apache.hadoop.hbase.master.assignment.RegionStates.RegionStateNode;
 import org.apache.hadoop.hbase.master.locking.LockManager;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.net.Address;
-
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists;
 import org.apache.hadoop.hbase.shaded.com.google.common.collect.Maps;
 
@@ -253,7 +252,7 @@ public class RSGroupAdminServer implements RSGroupAdmin {
     for (TableName table: tables) {
       LOG.info("Unassigning region(s) from " + table + " for table move to " + targetGroupName);
       LockManager.MasterLock lock = master.getLockManager().createMasterLock(table,
-              LockProcedure.LockType.EXCLUSIVE, this.getClass().getName() + ": RSGroup: table move");
+              LockType.EXCLUSIVE, this.getClass().getName() + ": RSGroup: table move");
       try {
         try {
           lock.acquire();
@@ -420,7 +419,7 @@ public class RSGroupAdminServer implements RSGroupAdmin {
     }
     for (TableName table: tables) {
       LockManager.MasterLock lock = master.getLockManager().createMasterLock(table,
-          LockProcedure.LockType.EXCLUSIVE, this.getClass().getName() + ": RSGroup: table move");
+          LockType.EXCLUSIVE, this.getClass().getName() + ": RSGroup: table move");
       try {
         try {
           lock.acquire();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
index f4f5db3150..b4e93ae4ed 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/MasterObserver.java
@@ -30,7 +30,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MetaMutationAnnotation;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
@@ -43,7 +42,9 @@ import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.net.Address;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.Quotas;
@@ -983,7 +984,7 @@ public interface MasterObserver extends Coprocessor {
    */
   default void postListProcedures(
       ObserverContext<MasterCoprocessorEnvironment> ctx,
-      List<ProcedureInfo> procInfoList) throws IOException {}
+      List<Procedure<?>> procList) throws IOException {}
 
   /**
    * Called before a listLocks request has been processed.
@@ -996,12 +997,12 @@ public interface MasterObserver extends Coprocessor {
   /**
    * Called after a listLocks request has been processed.
    * @param ctx the environment to interact with the framework and master
-   * @param lockInfoList the list of locks about to be returned
+   * @param lockedResources the list of locks about to be returned
    * @throws IOException if something went wrong
    */
   default void postListLocks(
       ObserverContext<MasterCoprocessorEnvironment> ctx,
-      List<LockInfo> lockInfoList) throws IOException {}
+      List<LockedResource> lockedResources) throws IOException {}
 
   /**
    * Called prior to moving a given region from one region server to another.
@@ -1890,7 +1891,7 @@ public interface MasterObserver extends Coprocessor {
    * @param ctx the environment to interact with the framework and master
    */
   default void preRequestLock(ObserverContext<MasterCoprocessorEnvironment> ctx, String namespace,
-      TableName tableName, HRegionInfo[] regionInfos, LockProcedure.LockType type,
+      TableName tableName, HRegionInfo[] regionInfos, LockType type,
       String description) throws IOException {}
 
   /**
@@ -1898,7 +1899,7 @@ public interface MasterObserver extends Coprocessor {
    * @param ctx the environment to interact with the framework and master
    */
   default void postRequestLock(ObserverContext<MasterCoprocessorEnvironment> ctx, String namespace,
-      TableName tableName, HRegionInfo[] regionInfos, LockProcedure.LockType type,
+      TableName tableName, HRegionInfo[] regionInfos, LockType type,
       String description) throws IOException {}
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java
index faa4f0ef90..3463df28e8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java
@@ -29,10 +29,10 @@ import org.apache.hadoop.hbase.ScheduledChore;
 import org.apache.hadoop.hbase.TableDescriptors;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.locking.LockManager;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.mob.ExpiredMobFileCleaner;
 import org.apache.hadoop.hbase.mob.MobConstants;
 import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.procedure2.LockType;
 
 /**
  * The Class ExpiredMobFileCleanerChore for running cleaner regularly to remove the expired
@@ -68,7 +68,7 @@ public class ExpiredMobFileCleanerChore extends ScheduledChore {
             // clean only for mob-enabled column.
             // obtain a read table lock before cleaning, synchronize with MobFileCompactionChore.
             final LockManager.MasterLock lock = master.getLockManager().createMasterLock(
-                MobUtils.getTableLockName(htd.getTableName()), LockProcedure.LockType.SHARED,
+                MobUtils.getTableLockName(htd.getTableName()), LockType.SHARED,
                 this.getClass().getSimpleName() + ": Cleaning expired mob files");
             try {
               lock.acquire();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 96bf85959f..4b0185c126 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -68,7 +68,6 @@ import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.PleaseHoldException;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableDescriptors;
@@ -135,9 +134,11 @@ import org.apache.hadoop.hbase.monitoring.TaskMonitor;
 import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;
 import org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager;
 import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.procedure2.ProcedureUtil;
 import org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore;
 import org.apache.hadoop.hbase.quotas.MasterQuotaManager;
@@ -2998,41 +2999,35 @@ public class HMaster extends HRegionServer implements MasterServices {
   }
 
   @Override
-  public List<ProcedureInfo> listProcedures() throws IOException {
+  public List<Procedure<?>> listProcedures() throws IOException {
     if (cpHost != null) {
       cpHost.preListProcedures();
     }
 
-    final List<Procedure> procList = this.procedureExecutor.listProcedures();
-    final List<ProcedureInfo> procInfoList = new ArrayList<>(procList.size());
-
-    for (Procedure proc : procList) {
-      ProcedureInfo procInfo = ProcedureUtil.convertToProcedureInfo(proc);
-      procInfoList.add(procInfo);
-    }
+    final List<Procedure<?>> procList = this.procedureExecutor.listProcedures();
 
     if (cpHost != null) {
-      cpHost.postListProcedures(procInfoList);
+      cpHost.postListProcedures(procList);
     }
 
-    return procInfoList;
+    return procList;
   }
 
   @Override
-  public List<LockInfo> listLocks() throws IOException {
+  public List<LockedResource> listLocks() throws IOException {
     if (cpHost != null) {
       cpHost.preListLocks();
     }
 
     MasterProcedureScheduler procedureScheduler = procedureExecutor.getEnvironment().getProcedureScheduler();
 
-    final List<LockInfo> lockInfoList = procedureScheduler.listLocks();
+    final List<LockedResource> lockedResources = procedureScheduler.listLocks();
 
     if (cpHost != null) {
-      cpHost.postListLocks(lockInfoList);
+      cpHost.postListLocks(lockedResources);
     }
 
-    return lockInfoList;
+    return lockedResources;
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
index 04bdacf60b..489871e7b3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
@@ -34,7 +34,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MetaMutationAnnotation;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
@@ -52,7 +51,9 @@ import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.metrics.MetricRegistry;
 import org.apache.hadoop.hbase.net.Address;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
 import org.apache.hadoop.hbase.security.User;
@@ -699,7 +700,7 @@ public class MasterCoprocessorHost
     });
   }
 
-  public void postListProcedures(final List<ProcedureInfo> procInfoList) throws IOException {
+  public void postListProcedures(final List<Procedure<?>> procInfoList) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(MasterObserver oserver, ObserverContext<MasterCoprocessorEnvironment> ctx)
@@ -719,12 +720,12 @@ public class MasterCoprocessorHost
     });
   }
 
-  public void postListLocks(final List<LockInfo> lockInfoList) throws IOException {
+  public void postListLocks(final List<LockedResource> lockedResources) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(MasterObserver oserver, ObserverContext<MasterCoprocessorEnvironment> ctx)
           throws IOException {
-        oserver.postListLocks(ctx, lockInfoList);
+        oserver.postListLocks(ctx, lockedResources);
       }
     });
   }
@@ -1835,7 +1836,7 @@ public class MasterCoprocessorHost
   }
 
   public void preRequestLock(String namespace, TableName tableName, HRegionInfo[] regionInfos,
-      LockProcedure.LockType type, String description) throws IOException {
+      LockType type, String description) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(MasterObserver oserver, ObserverContext<MasterCoprocessorEnvironment> ctx)
@@ -1846,7 +1847,7 @@ public class MasterCoprocessorHost
   }
 
   public void postRequestLock(String namespace, TableName tableName, HRegionInfo[] regionInfos,
-      LockProcedure.LockType type, String description) throws IOException {
+      LockType type, String description) throws IOException {
     execOperation(coprocessors.isEmpty() ? null : new CoprocessorOperation() {
       @Override
       public void call(MasterObserver oserver, ObserverContext<MasterCoprocessorEnvironment> ctx)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
index 2b1232a400..9b8fa1275b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterMobCompactionThread.java
@@ -35,8 +35,8 @@ import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.locking.LockManager;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 
 /**
@@ -120,7 +120,7 @@ public class MasterMobCompactionThread {
     public void run() {
       // These locks are on dummy table names, and only used for compaction/mob file cleaning.
       final LockManager.MasterLock lock = master.getLockManager().createMasterLock(
-          MobUtils.getTableLockName(tableName), LockProcedure.LockType.EXCLUSIVE,
+          MobUtils.getTableLockName(tableName), LockType.EXCLUSIVE,
           this.getClass().getName() + ": mob compaction");
       try {
         for (HColumnDescriptor hcd : hcds) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
index 5a2cd173b4..76fbed0d9f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
@@ -37,7 +37,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
@@ -61,8 +60,10 @@ import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.NonceProcedureRunnable;
 import org.apache.hadoop.hbase.mob.MobUtils;
 import org.apache.hadoop.hbase.procedure.MasterProcedureManager;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
 import org.apache.hadoop.hbase.procedure2.Procedure;
+import org.apache.hadoop.hbase.procedure2.ProcedureUtil;
 import org.apache.hadoop.hbase.quotas.MasterQuotaManager;
 import org.apache.hadoop.hbase.quotas.QuotaObserverChore;
 import org.apache.hadoop.hbase.quotas.QuotaUtil;
@@ -83,8 +84,10 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.CompactRegi
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.CompactRegionResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.GetRegionInfoRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.GetRegionInfoResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.*;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos.RegionStoreSequenceIds;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.NameStringPair;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.ProcedureDescription;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
@@ -93,34 +96,15 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockH
 import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockService;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.*;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SecurityCapabilitiesResponse.Capability;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetBalancerRunningRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetBalancerRunningResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetCleanerChoreRunningRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetCleanerChoreRunningResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetNormalizerRunningRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetNormalizerRunningResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetQuotaRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetQuotaResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetSplitOrMergeEnabledRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SetSplitOrMergeEnabledResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.ShutdownRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.ShutdownResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SnapshotRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.SnapshotResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.StopMasterRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.StopMasterResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.TruncateTableRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.TruncateTableResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.UnassignRegionRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.UnassignRegionResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetQuotaStatesRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetQuotaStatesResponse;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetSpaceQuotaRegionSizesRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetSpaceQuotaRegionSizesResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetQuotaStatesResponse.NamespaceQuotaSnapshot;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetQuotaStatesResponse.TableQuotaSnapshot;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetSpaceQuotaRegionSizesRequest;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetSpaceQuotaRegionSizesResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.GetSpaceQuotaRegionSizesResponse.RegionSizes;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdResponse;
@@ -1083,8 +1067,8 @@ public class MasterRpcServices extends RSRpcServices
       ListProceduresRequest request) throws ServiceException {
     try {
       final ListProceduresResponse.Builder response = ListProceduresResponse.newBuilder();
-      for (ProcedureInfo p: master.listProcedures()) {
-        response.addProcedure(ProtobufUtil.toProtoProcedure(p));
+      for (Procedure<?> p: master.listProcedures()) {
+        response.addProcedure(ProcedureUtil.convertToProtoProcedure(p));
       }
       return response.build();
     } catch (IOException e) {
@@ -1099,8 +1083,8 @@ public class MasterRpcServices extends RSRpcServices
     try {
       final ListLocksResponse.Builder builder = ListLocksResponse.newBuilder();
 
-      for (LockInfo lockInfo: master.listLocks()) {
-        builder.addLock(ProtobufUtil.toProtoLockInfo(lockInfo));
+      for (LockedResource lockedResource: master.listLocks()) {
+        builder.addLock(ProcedureUtil.convertToProtoLockedResource(lockedResource));
       }
 
       return builder.build();
@@ -1846,7 +1830,7 @@ public class MasterRpcServices extends RSRpcServices
         throw new IllegalArgumentException("Empty description");
       }
       NonceProcedureRunnable npr;
-      LockProcedure.LockType type = LockProcedure.LockType.valueOf(request.getLockType().name());
+      LockType type = LockType.valueOf(request.getLockType().name());
       if (request.getRegionInfoCount() > 0) {
         final HRegionInfo[] regionInfos = new HRegionInfo[request.getRegionInfoCount()];
         for (int i = 0; i < request.getRegionInfoCount(); ++i) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
index f7f5d069c1..6a06f86b48 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
@@ -24,7 +24,6 @@ import java.util.List;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableDescriptors;
@@ -42,7 +41,8 @@ import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;
 import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.quotas.MasterQuotaManager;
@@ -367,14 +367,14 @@ public interface MasterServices extends Server {
    * @return procedure list
    * @throws IOException
    */
-  public List<ProcedureInfo> listProcedures() throws IOException;
+  public List<Procedure<?>> listProcedures() throws IOException;
 
   /**
    * List locks
    * @return lock list
    * @throws IOException
    */
-  public List<LockInfo> listLocks() throws IOException;
+  public List<LockedResource> listLocks() throws IOException;
 
   /**
    * Get list of table descriptors by namespace
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MobCompactionChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MobCompactionChore.java
index 42a544568b..fdf8b9ddb6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MobCompactionChore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MobCompactionChore.java
@@ -31,8 +31,8 @@ import org.apache.hadoop.hbase.TableDescriptors;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.client.TableState;
 import org.apache.hadoop.hbase.master.locking.LockManager;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.procedure2.LockType;
 
 /**
  * The Class MobCompactChore for running compaction regularly to merge small mob files.
@@ -64,7 +64,7 @@ public class MobCompactionChore extends ScheduledChore {
         boolean reported = false;
         try {
           final LockManager.MasterLock lock = master.getLockManager().createMasterLock(
-              MobUtils.getTableLockName(htd.getTableName()), LockProcedure.LockType.EXCLUSIVE,
+              MobUtils.getTableLockName(htd.getTableName()), LockType.EXCLUSIVE,
               this.getClass().getName() + ": mob compaction");
           for (HColumnDescriptor hcd : htd.getColumnFamilies()) {
             if (!hcd.isMobEnabled()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java
index 63389833f8..6c4466e2dd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/AssignProcedure.java
@@ -20,9 +20,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HRegionInfo;
@@ -35,6 +32,8 @@ import org.apache.hadoop.hbase.master.assignment.RegionStates.RegionStateNode;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.RegionOpenOperation;
 import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.RemoteOperation;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
@@ -118,7 +117,8 @@ public class AssignProcedure extends RegionTransitionProcedure {
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
     final AssignRegionStateData.Builder state = AssignRegionStateData.newBuilder()
         .setTransitionState(getTransitionState())
         .setRegionInfo(HRegionInfo.convert(getRegionInfo()));
@@ -128,12 +128,13 @@ public class AssignProcedure extends RegionTransitionProcedure {
     if (this.targetServer != null) {
       state.setTargetServer(ProtobufUtil.toServerName(this.targetServer));
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    final AssignRegionStateData state = AssignRegionStateData.parseDelimitedFrom(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    final AssignRegionStateData state = deserializer.deserialize(AssignRegionStateData.class);
     setTransitionState(state.getTransitionState());
     setRegionInfo(HRegionInfo.convert(state.getRegionInfo()));
     forceNewPlan = state.getForceNewPlan();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure.java
index c7d97ee305..4365bc1a00 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure.java
@@ -18,9 +18,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HRegionInfo;
@@ -29,6 +26,8 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -132,21 +131,23 @@ extends AbstractStateMachineTableProcedure<GCMergedRegionsState> {
   }
 
   @Override
-  protected void serializeStateData(OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
     final MasterProcedureProtos.GCMergedRegionsStateData.Builder msg =
         MasterProcedureProtos.GCMergedRegionsStateData.newBuilder().
         setParentA(HRegionInfo.convert(this.father)).
         setParentB(HRegionInfo.convert(this.mother)).
         setMergedChild(HRegionInfo.convert(this.mergedChild));
-    msg.build().writeDelimitedTo(stream);
+    serializer.serialize(msg.build());
   }
 
   @Override
-  protected void deserializeStateData(InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
     final MasterProcedureProtos.GCMergedRegionsStateData msg =
-        MasterProcedureProtos.GCMergedRegionsStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.GCMergedRegionsStateData.class);
     this.father = HRegionInfo.convert(msg.getParentA());
     this.mother = HRegionInfo.convert(msg.getParentB());
     this.mergedChild = HRegionInfo.convert(msg.getMergedChild());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java
index 29d0676185..51fb142549 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.java
@@ -18,9 +18,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileSystem;
@@ -32,6 +29,8 @@ import org.apache.hadoop.hbase.favored.FavoredNodesManager;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -131,20 +130,22 @@ public class GCRegionProcedure extends AbstractStateMachineRegionProcedure<GCReg
   }
 
   @Override
-  protected void serializeStateData(OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
     // Double serialization of regionname. Superclass is also serializing. Fix.
     final MasterProcedureProtos.GCRegionStateData.Builder msg =
         MasterProcedureProtos.GCRegionStateData.newBuilder()
         .setRegionInfo(HRegionInfo.convert(getRegion()));
-    msg.build().writeDelimitedTo(stream);
+    serializer.serialize(msg.build());
   }
 
   @Override
-  protected void deserializeStateData(InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
     final MasterProcedureProtos.GCRegionStateData msg =
-        MasterProcedureProtos.GCRegionStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.GCRegionStateData.class);
     setRegion(HRegionInfo.convert(msg.getRegionInfo()));
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
index 74d9b75a95..b38da56826 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
@@ -53,13 +51,16 @@ import org.apache.hadoop.hbase.master.normalizer.NormalizationPlan;
 import org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;
+import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
-import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
 import org.apache.hadoop.hbase.quotas.QuotaExceededException;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.regionserver.HStoreFile;
 import org.apache.hadoop.hbase.regionserver.StoreFileInfo;
+import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.GetRegionInfoResponse;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.MergeTableRegionsState;
@@ -67,8 +68,6 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSUtils;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
-
 /**
  * The procedure to Merge a region in a table.
  * This procedure takes an exclusive table lock since it is working over multiple regions.
@@ -346,8 +345,9 @@ public class MergeTableRegionsProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     final MasterProcedureProtos.MergeTableRegionsStateData.Builder mergeTableRegionsMsg =
         MasterProcedureProtos.MergeTableRegionsStateData.newBuilder()
@@ -357,15 +357,16 @@ public class MergeTableRegionsProcedure
     for (int i = 0; i < regionsToMerge.length; ++i) {
       mergeTableRegionsMsg.addRegionInfo(HRegionInfo.convert(regionsToMerge[i]));
     }
-    mergeTableRegionsMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(mergeTableRegionsMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     final MasterProcedureProtos.MergeTableRegionsStateData mergeTableRegionsMsg =
-        MasterProcedureProtos.MergeTableRegionsStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.MergeTableRegionsStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(mergeTableRegionsMsg.getUserInfo()));
 
     assert(mergeTableRegionsMsg.getRegionInfoCount() == 2);
@@ -479,7 +480,7 @@ public class MergeTableRegionsProcedure
           new IOException("Merge of " + regionsStr + " failed because merge switch is off"));
       return false;
     }
-    
+
 
     // Ask the remote regionserver if regions are mergeable. If we get an IOE, report it
     // along w/ the failure so can see why we are not mergeable at this time.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.java
index 1907e9838d..7c2fe2c1c6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.java
@@ -20,8 +20,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -32,6 +30,8 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.MoveRegionState;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.MoveRegionStateData;
@@ -124,8 +124,9 @@ public class MoveRegionProcedure extends AbstractStateMachineRegionProcedure<Mov
   }
 
   @Override
-  protected void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     final MoveRegionStateData.Builder state = MoveRegionStateData.newBuilder()
         // No need to serialize the HRegionInfo. The super class has the region.
@@ -133,14 +134,16 @@ public class MoveRegionProcedure extends AbstractStateMachineRegionProcedure<Mov
     if (plan.getDestination() != null) {
       state.setDestinationServer(ProtobufUtil.toServerName(plan.getDestination()));
     }
-    state.build().writeDelimitedTo(stream);
+
+    serializer.serialize(state.build());
   }
 
   @Override
-  protected void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
-    final MoveRegionStateData state = MoveRegionStateData.parseDelimitedFrom(stream);
+    final MoveRegionStateData state = deserializer.deserialize(MoveRegionStateData.class);
     final HRegionInfo regionInfo = getRegion(); // Get it from super class deserialization.
     final ServerName sourceServer = ProtobufUtil.toServerName(state.getSourceServer());
     final ServerName destinationServer = state.hasDestinationServer() ?
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java
index 8a26380d65..b980c856c6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.java
@@ -22,9 +22,7 @@ import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTe
 import org.apache.hadoop.hbase.client.ColumnFamilyDescriptor;
 
 import java.io.IOException;
-import java.io.InputStream;
 import java.io.InterruptedIOException;
-import java.io.OutputStream;
 import java.util.*;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
@@ -39,7 +37,6 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
-import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
@@ -59,6 +56,8 @@ import org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProced
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;
 import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.quotas.QuotaExceededException;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.SplitTableRegionState;
@@ -323,8 +322,9 @@ public class SplitTableRegionProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     final MasterProcedureProtos.SplitTableRegionStateData.Builder splitTableRegionMsg =
         MasterProcedureProtos.SplitTableRegionStateData.newBuilder()
@@ -332,15 +332,16 @@ public class SplitTableRegionProcedure
         .setParentRegionInfo(HRegionInfo.convert(getRegion()))
         .addChildRegionInfo(HRegionInfo.convert(daughter_1_HRI))
         .addChildRegionInfo(HRegionInfo.convert(daughter_2_HRI));
-    splitTableRegionMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(splitTableRegionMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     final MasterProcedureProtos.SplitTableRegionStateData splitTableRegionsMsg =
-        MasterProcedureProtos.SplitTableRegionStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.SplitTableRegionStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(splitTableRegionsMsg.getUserInfo()));
     setRegion(HRegionInfo.convert(splitTableRegionsMsg.getParentRegionInfo()));
     assert(splitTableRegionsMsg.getChildRegionInfoCount() == 2);
@@ -678,6 +679,7 @@ public class SplitTableRegionProcedure
       this.family = family;
     }
 
+    @Override
     public Pair<Path,Path> call() throws IOException {
       return splitStoreFile(regionFs, family, sf);
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/UnassignProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/UnassignProcedure.java
index c6b7e4b118..71a3437401 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/UnassignProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/UnassignProcedure.java
@@ -20,8 +20,6 @@
 package org.apache.hadoop.hbase.master.assignment;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.commons.logging.Log;
@@ -39,6 +37,8 @@ import org.apache.hadoop.hbase.master.procedure.ServerCrashException;
 import org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.RegionCloseOperation;
 import org.apache.hadoop.hbase.master.RegionState.State;
 import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.RemoteOperation;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.RegionTransitionState;
@@ -119,7 +119,8 @@ public class UnassignProcedure extends RegionTransitionProcedure {
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
     UnassignRegionStateData.Builder state = UnassignRegionStateData.newBuilder()
         .setTransitionState(getTransitionState())
         .setHostingServer(ProtobufUtil.toServerName(this.hostingServer))
@@ -130,12 +131,14 @@ public class UnassignProcedure extends RegionTransitionProcedure {
     if (force) {
       state.setForce(true);
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    final UnassignRegionStateData state = UnassignRegionStateData.parseDelimitedFrom(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    final UnassignRegionStateData state =
+        deserializer.deserialize(UnassignRegionStateData.class);
     setTransitionState(state.getTransitionState());
     setRegionInfo(HRegionInfo.convert(state.getRegionInfo()));
     this.hostingServer = ProtobufUtil.toServerName(state.getHostingServer());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockManager.java
index 6c8bbbac3b..87ad557dcc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockManager.java
@@ -18,19 +18,20 @@
  */
 package org.apache.hadoop.hbase.master.locking;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
+import java.io.IOException;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.hbase.util.NonceKey;
 
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
 /**
  * Functions to acquire lock on table/namespace/regions.
  */
@@ -50,12 +51,12 @@ public final class LockManager {
   }
 
   public MasterLock createMasterLock(final String namespace,
-      final LockProcedure.LockType type, final String description) {
+      final LockType type, final String description) {
     return new MasterLock(namespace, type, description);
   }
 
   public MasterLock createMasterLock(final TableName tableName,
-      final LockProcedure.LockType type, final String description) {
+      final LockType type, final String description) {
     return new MasterLock(tableName, type, description);
   }
 
@@ -81,13 +82,13 @@ public final class LockManager {
     private final String namespace;
     private final TableName tableName;
     private final HRegionInfo[] regionInfos;
-    private final LockProcedure.LockType type;
+    private final LockType type;
     private final String description;
 
     private LockProcedure proc = null;
 
     public MasterLock(final String namespace,
-        final LockProcedure.LockType type, final String description) {
+        final LockType type, final String description) {
       this.namespace = namespace;
       this.tableName = null;
       this.regionInfos = null;
@@ -96,7 +97,7 @@ public final class LockManager {
     }
 
     public MasterLock(final TableName tableName,
-        final LockProcedure.LockType type, final String description) {
+        final LockType type, final String description) {
       this.namespace = null;
       this.tableName = tableName;
       this.regionInfos = null;
@@ -108,7 +109,7 @@ public final class LockManager {
       this.namespace = null;
       this.tableName = null;
       this.regionInfos = regionInfos;
-      this.type = LockProcedure.LockType.EXCLUSIVE;
+      this.type = LockType.EXCLUSIVE;
       this.description = description;
     }
 
@@ -203,7 +204,7 @@ public final class LockManager {
    * locks, regular heartbeats are required to keep the lock held.
    */
   public class RemoteLocks {
-    public long requestNamespaceLock(final String namespace, final LockProcedure.LockType type,
+    public long requestNamespaceLock(final String namespace, final LockType type,
         final String description, final NonceKey nonceKey)
         throws IllegalArgumentException, IOException {
       master.getMasterCoprocessorHost().preRequestLock(namespace, null, null, type, description);
@@ -214,7 +215,7 @@ public final class LockManager {
       return proc.getProcId();
     }
 
-    public long requestTableLock(final TableName tableName, final LockProcedure.LockType type,
+    public long requestTableLock(final TableName tableName, final LockType type,
         final String description, final NonceKey nonceKey)
         throws IllegalArgumentException, IOException {
       master.getMasterCoprocessorHost().preRequestLock(null, tableName, null, type, description);
@@ -232,12 +233,12 @@ public final class LockManager {
         final NonceKey nonceKey)
     throws IllegalArgumentException, IOException {
       master.getMasterCoprocessorHost().preRequestLock(null, null, regionInfos,
-            LockProcedure.LockType.EXCLUSIVE, description);
+            LockType.EXCLUSIVE, description);
       final LockProcedure proc = new LockProcedure(master.getConfiguration(), regionInfos,
-          LockProcedure.LockType.EXCLUSIVE, description, null);
+          LockType.EXCLUSIVE, description, null);
       submitProcedure(proc, nonceKey);
       master.getMasterCoprocessorHost().postRequestLock(null, null, regionInfos,
-            LockProcedure.LockType.EXCLUSIVE, description);
+            LockType.EXCLUSIVE, description);
       return proc.getProcId();
     }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
index edbba83336..f0225647e4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/locking/LockProcedure.java
@@ -27,8 +27,11 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.procedure.TableProcedureInterface;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
@@ -36,8 +39,6 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockP
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
@@ -66,9 +67,6 @@ public final class LockProcedure extends Procedure<MasterProcedureEnv>
   public static final String LOCAL_MASTER_LOCKS_TIMEOUT_MS_CONF =
       "hbase.master.procedure.local.master.locks.timeout.ms";
 
-  // Also used in serialized states, changes will affect backward compatibility.
-  public enum LockType { SHARED, EXCLUSIVE }
-
   private String namespace;
   private TableName tableName;
   private HRegionInfo[] regionInfos;
@@ -265,7 +263,8 @@ public final class LockProcedure extends Procedure<MasterProcedureEnv>
   }
 
   @Override
-  protected void serializeStateData(final OutputStream stream) throws IOException {
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
     final LockProcedureData.Builder builder = LockProcedureData.newBuilder()
           .setLockType(LockServiceProtos.LockType.valueOf(type.name()))
           .setDescription(description);
@@ -281,12 +280,13 @@ public final class LockProcedure extends Procedure<MasterProcedureEnv>
     if (lockAcquireLatch != null) {
       builder.setIsMasterLock(true);
     }
-    builder.build().writeDelimitedTo(stream);
+    serializer.serialize(builder.build());
   }
 
   @Override
-  protected void deserializeStateData(final InputStream stream) throws IOException {
-    final LockProcedureData state = LockProcedureData.parseDelimitedFrom(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    final LockProcedureData state = deserializer.deserialize(LockProcedureData.class);
     type = LockType.valueOf(state.getLockType().name());
     description = state.getDescription();
     if (state.getRegionInfoCount() > 0) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java
index 41502d45ff..2fdefcf8b5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.java
@@ -19,14 +19,13 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 
 /**
@@ -84,6 +83,7 @@ public abstract class AbstractStateMachineRegionProcedure<TState>
    * @param env MasterProcedureEnv
    * @throws IOException
    */
+  @Override
   protected void checkTableModifiable(final MasterProcedureEnv env) throws IOException {
     // Checks whether the table exists
     if (!MetaTableAccessor.tableExists(env.getMasterServices().getConnection(), getTableName())) {
@@ -96,6 +96,7 @@ public abstract class AbstractStateMachineRegionProcedure<TState>
     return true;
   }
 
+  @Override
   protected LockState acquireLock(final MasterProcedureEnv env) {
     if (env.waitInitialized(this)) return LockState.LOCK_EVENT_WAIT;
     if (env.getProcedureScheduler().waitRegions(this, getTableName(), getRegion())) {
@@ -105,6 +106,7 @@ public abstract class AbstractStateMachineRegionProcedure<TState>
     return LockState.LOCK_ACQUIRED;
   }
 
+  @Override
   protected void releaseLock(final MasterProcedureEnv env) {
     this.lock = false;
     env.getProcedureScheduler().wakeRegions(this, getTableName(), getRegion());
@@ -120,14 +122,16 @@ public abstract class AbstractStateMachineRegionProcedure<TState>
   }
 
   @Override
-  protected void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
-    HRegionInfo.convert(getRegion()).writeDelimitedTo(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
+    serializer.serialize(HRegionInfo.convert(getRegion()));
   }
 
   @Override
-  protected void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
-    this.hri = HRegionInfo.convert(HBaseProtos.RegionInfo.parseDelimitedFrom(stream));
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
+    this.hri = HRegionInfo.convert(deserializer.deserialize(HBaseProtos.RegionInfo.class));
   }
 }
\ No newline at end of file
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AddColumnFamilyProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AddColumnFamilyProcedure.java
index 34c18530a1..0c091b2db9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AddColumnFamilyProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/AddColumnFamilyProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -32,6 +30,8 @@ import org.apache.hadoop.hbase.InvalidFamilyOperationException;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.AddColumnFamilyState;
@@ -165,8 +165,9 @@ public class AddColumnFamilyProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.AddColumnFamilyStateData.Builder addCFMsg =
         MasterProcedureProtos.AddColumnFamilyStateData.newBuilder()
@@ -178,15 +179,16 @@ public class AddColumnFamilyProcedure
           .setUnmodifiedTableSchema(ProtobufUtil.convertToTableSchema(unmodifiedHTableDescriptor));
     }
 
-    addCFMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(addCFMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.AddColumnFamilyStateData addCFMsg =
-        MasterProcedureProtos.AddColumnFamilyStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.AddColumnFamilyStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(addCFMsg.getUserInfo()));
     tableName = ProtobufUtil.toTableName(addCFMsg.getTableName());
     cfDescriptor = ProtobufUtil.convertToHColumnDesc(addCFMsg.getColumnfamilySchema());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
index afe72e23e6..1e163c7ddc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -46,6 +44,8 @@ import org.apache.hadoop.hbase.master.MetricsSnapshot;
 import org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.CreateHdfsRegions;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -243,8 +243,9 @@ public class CloneSnapshotProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.CloneSnapshotStateData.Builder cloneSnapshotMsg =
       MasterProcedureProtos.CloneSnapshotStateData.newBuilder()
@@ -270,15 +271,16 @@ public class CloneSnapshotProcedure
         cloneSnapshotMsg.addParentToChildRegionsPairList(parentToChildrenPair);
       }
     }
-    cloneSnapshotMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(cloneSnapshotMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.CloneSnapshotStateData cloneSnapshotMsg =
-      MasterProcedureProtos.CloneSnapshotStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.CloneSnapshotStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(cloneSnapshotMsg.getUserInfo()));
     snapshot = cloneSnapshotMsg.getSnapshot();
     hTableDescriptor = ProtobufUtil.convertToHTableDesc(cloneSnapshotMsg.getTableSchema());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.java
index 7d651265c5..ea27d858cd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.java
@@ -19,9 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
@@ -29,6 +26,8 @@ import org.apache.hadoop.hbase.NamespaceExistException;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.TableNamespaceManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.CreateNamespaceState;
@@ -135,21 +134,23 @@ public class CreateNamespaceProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.CreateNamespaceStateData.Builder createNamespaceMsg =
         MasterProcedureProtos.CreateNamespaceStateData.newBuilder().setNamespaceDescriptor(
           ProtobufUtil.toProtoNamespaceDescriptor(this.nsDescriptor));
-    createNamespaceMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(createNamespaceMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.CreateNamespaceStateData createNamespaceMsg =
-        MasterProcedureProtos.CreateNamespaceStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.CreateNamespaceStateData.class);
     nsDescriptor = ProtobufUtil.toNamespaceDescriptor(createNamespaceMsg.getNamespaceDescriptor());
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
index cf55463d99..dc5d43f37b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -39,6 +37,8 @@ import org.apache.hadoop.hbase.client.RegionReplicaUtil;
 import org.apache.hadoop.hbase.client.TableState;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -183,8 +183,9 @@ public class CreateTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.CreateTableStateData.Builder state =
       MasterProcedureProtos.CreateTableStateData.newBuilder()
@@ -195,15 +196,16 @@ public class CreateTableProcedure
         state.addRegionInfo(HRegionInfo.convert(hri));
       }
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.CreateTableStateData state =
-      MasterProcedureProtos.CreateTableStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.CreateTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(state.getUserInfo()));
     hTableDescriptor = ProtobufUtil.convertToHTableDesc(state.getTableSchema());
     if (state.getRegionInfoCount() == 0) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteColumnFamilyProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteColumnFamilyProcedure.java
index 78bd715e37..925c2adb02 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteColumnFamilyProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteColumnFamilyProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -31,6 +29,8 @@ import org.apache.hadoop.hbase.InvalidFamilyOperationException;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -171,8 +171,9 @@ public class DeleteColumnFamilyProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.DeleteColumnFamilyStateData.Builder deleteCFMsg =
         MasterProcedureProtos.DeleteColumnFamilyStateData.newBuilder()
@@ -184,14 +185,15 @@ public class DeleteColumnFamilyProcedure
           .setUnmodifiedTableSchema(ProtobufUtil.convertToTableSchema(unmodifiedHTableDescriptor));
     }
 
-    deleteCFMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(deleteCFMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
     MasterProcedureProtos.DeleteColumnFamilyStateData deleteCFMsg =
-        MasterProcedureProtos.DeleteColumnFamilyStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.DeleteColumnFamilyStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(deleteCFMsg.getUserInfo()));
     tableName = ProtobufUtil.toTableName(deleteCFMsg.getTableName());
     familyName = deleteCFMsg.getColumnfamilyName().toByteArray();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java
index d91a6e14f6..3b203adf9a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.java
@@ -20,9 +20,6 @@ package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
@@ -35,6 +32,8 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.constraint.ConstraintException;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.TableNamespaceManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.DeleteNamespaceState;
@@ -147,8 +146,9 @@ public class DeleteNamespaceProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.DeleteNamespaceStateData.Builder deleteNamespaceMsg =
         MasterProcedureProtos.DeleteNamespaceStateData.newBuilder().setNamespaceName(namespaceName);
@@ -156,15 +156,16 @@ public class DeleteNamespaceProcedure
       deleteNamespaceMsg.setNamespaceDescriptor(
         ProtobufUtil.toProtoNamespaceDescriptor(this.nsDescriptor));
     }
-    deleteNamespaceMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(deleteNamespaceMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.DeleteNamespaceStateData deleteNamespaceMsg =
-        MasterProcedureProtos.DeleteNamespaceStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.DeleteNamespaceStateData.class);
     namespaceName = deleteNamespaceMsg.getNamespaceName();
     if (deleteNamespaceMsg.hasNamespaceDescriptor()) {
       nsDescriptor =
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
index 04dfc60a46..ac8f47c8f6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.List;
 
@@ -48,6 +46,8 @@ import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.mob.MobConstants;
 import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -203,8 +203,9 @@ public class DeleteTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.DeleteTableStateData.Builder state =
       MasterProcedureProtos.DeleteTableStateData.newBuilder()
@@ -215,15 +216,16 @@ public class DeleteTableProcedure
         state.addRegionInfo(HRegionInfo.convert(hri));
       }
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.DeleteTableStateData state =
-      MasterProcedureProtos.DeleteTableStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.DeleteTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(state.getUserInfo()));
     tableName = ProtobufUtil.toTableName(state.getTableName());
     if (state.getRegionInfoCount() == 0) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DisableTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DisableTableProcedure.java
index 409ca26ef1..bbf7100b13 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DisableTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/DisableTableProcedure.java
@@ -19,9 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.MetaTableAccessor;
@@ -33,6 +30,8 @@ import org.apache.hadoop.hbase.client.TableState;
 import org.apache.hadoop.hbase.constraint.ConstraintException;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.TableStateManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.DisableTableState;
@@ -177,8 +176,9 @@ public class DisableTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.DisableTableStateData.Builder disableTableMsg =
         MasterProcedureProtos.DisableTableStateData.newBuilder()
@@ -186,15 +186,16 @@ public class DisableTableProcedure
             .setTableName(ProtobufUtil.toProtoTableName(tableName))
             .setSkipTableStateCheck(skipTableStateCheck);
 
-    disableTableMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(disableTableMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.DisableTableStateData disableTableMsg =
-        MasterProcedureProtos.DisableTableStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.DisableTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(disableTableMsg.getUserInfo()));
     tableName = ProtobufUtil.toTableName(disableTableMsg.getTableName());
     skipTableStateCheck = disableTableMsg.getSkipTableStateCheck();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
index 4f4b5b1906..08e23d230d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.java
@@ -19,9 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.MetaTableAccessor;
@@ -32,6 +29,8 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.client.TableState;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.TableStateManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.EnableTableState;
@@ -170,8 +169,9 @@ public class EnableTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.EnableTableStateData.Builder enableTableMsg =
         MasterProcedureProtos.EnableTableStateData.newBuilder()
@@ -179,15 +179,16 @@ public class EnableTableProcedure
             .setTableName(ProtobufUtil.toProtoTableName(tableName))
             .setSkipTableStateCheck(skipTableStateCheck);
 
-    enableTableMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(enableTableMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.EnableTableStateData enableTableMsg =
-        MasterProcedureProtos.EnableTableStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.EnableTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(enableTableMsg.getUserInfo()));
     tableName = ProtobufUtil.toTableName(enableTableMsg.getTableName());
     skipTableStateCheck = enableTableMsg.getSkipTableStateCheck();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java
index 564b86d6bf..6ba77e1683 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.java
@@ -18,8 +18,6 @@
 
 package org.apache.hadoop.hbase.master.procedure;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
-
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -32,7 +30,6 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.TableName;
@@ -42,11 +39,13 @@ import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.master.procedure.TableProcedureInterface.TableOperationType;
 import org.apache.hadoop.hbase.procedure2.AbstractProcedureScheduler;
 import org.apache.hadoop.hbase.procedure2.LockAndQueue;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
 import org.apache.hadoop.hbase.procedure2.LockStatus;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.LockedResourceType;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureDeque;
-import org.apache.hadoop.hbase.procedure2.ProcedureUtil;
+import org.apache.hadoop.hbase.shaded.com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.hbase.util.AvlUtil.AvlIterableList;
 import org.apache.hadoop.hbase.util.AvlUtil.AvlKeyComparator;
 import org.apache.hadoop.hbase.util.AvlUtil.AvlLinkedNode;
@@ -238,57 +237,42 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
     return pollResult;
   }
 
-  private LockInfo createLockInfo(LockInfo.ResourceType resourceType,
+  private LockedResource createLockedResource(LockedResourceType resourceType,
       String resourceName, LockAndQueue queue) {
-    LockInfo info = new LockInfo();
-
-    info.setResourceType(resourceType);
-    info.setResourceName(resourceName);
+    LockType lockType;
+    Procedure<?> exclusiveLockOwnerProcedure;
+    int sharedLockCount;
 
     if (queue.hasExclusiveLock()) {
-      info.setLockType(LockInfo.LockType.EXCLUSIVE);
-
-      Procedure<?> exclusiveLockOwnerProcedure = queue.getExclusiveLockOwnerProcedure();
-      ProcedureInfo exclusiveLockOwnerProcedureInfo =
-          ProcedureUtil.convertToProcedureInfo(exclusiveLockOwnerProcedure);
-      info.setExclusiveLockOwnerProcedure(exclusiveLockOwnerProcedureInfo);
-    } else if (queue.getSharedLockCount() > 0) {
-      info.setLockType(LockInfo.LockType.SHARED);
-      info.setSharedLockCount(queue.getSharedLockCount());
+      lockType = LockType.EXCLUSIVE;
+      exclusiveLockOwnerProcedure = queue.getExclusiveLockOwnerProcedure();
+      sharedLockCount = 0;
+    } else {
+      lockType = LockType.SHARED;
+      exclusiveLockOwnerProcedure = null;
+      sharedLockCount = queue.getSharedLockCount();
     }
 
+    List<Procedure<?>> waitingProcedures = new ArrayList<>();
+
     for (Procedure<?> procedure : queue) {
       if (!(procedure instanceof LockProcedure)) {
         continue;
       }
 
-      LockProcedure lockProcedure = (LockProcedure)procedure;
-      LockInfo.WaitingProcedure waitingProcedure = new LockInfo.WaitingProcedure();
-
-      switch (lockProcedure.getType()) {
-      case EXCLUSIVE:
-        waitingProcedure.setLockType(LockInfo.LockType.EXCLUSIVE);
-        break;
-      case SHARED:
-        waitingProcedure.setLockType(LockInfo.LockType.SHARED);
-        break;
-      }
-
-      ProcedureInfo procedureInfo = ProcedureUtil.convertToProcedureInfo(lockProcedure);
-      waitingProcedure.setProcedure(procedureInfo);
-
-      info.addWaitingProcedure(waitingProcedure);
+      waitingProcedures.add(procedure);
     }
 
-    return info;
+    return new LockedResource(resourceType, resourceName, lockType,
+        exclusiveLockOwnerProcedure, sharedLockCount, waitingProcedures);
   }
 
   @Override
-  public List<LockInfo> listLocks() {
+  public List<LockedResource> listLocks() {
     schedLock();
 
     try {
-      List<LockInfo> lockInfos = new ArrayList<>();
+      List<LockedResource> lockedResources = new ArrayList<>();
 
       for (Entry<ServerName, LockAndQueue> entry : locking.serverLocks
           .entrySet()) {
@@ -296,9 +280,9 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
         LockAndQueue queue = entry.getValue();
 
         if (queue.isLocked()) {
-          LockInfo lockInfo = createLockInfo(LockInfo.ResourceType.SERVER,
-              serverName, queue);
-          lockInfos.add(lockInfo);
+          LockedResource lockedResource =
+            createLockedResource(LockedResourceType.SERVER, serverName, queue);
+          lockedResources.add(lockedResource);
         }
       }
 
@@ -308,9 +292,9 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
         LockAndQueue queue = entry.getValue();
 
         if (queue.isLocked()) {
-          LockInfo lockInfo = createLockInfo(LockInfo.ResourceType.NAMESPACE,
-              namespaceName, queue);
-          lockInfos.add(lockInfo);
+          LockedResource lockedResource =
+            createLockedResource(LockedResourceType.NAMESPACE, namespaceName, queue);
+          lockedResources.add(lockedResource);
         }
       }
 
@@ -320,9 +304,9 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
         LockAndQueue queue = entry.getValue();
 
         if (queue.isLocked()) {
-          LockInfo lockInfo = createLockInfo(LockInfo.ResourceType.TABLE,
-              tableName, queue);
-          lockInfos.add(lockInfo);
+          LockedResource lockedResource =
+            createLockedResource(LockedResourceType.TABLE, tableName, queue);
+          lockedResources.add(lockedResource);
         }
       }
 
@@ -331,20 +315,21 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
         LockAndQueue queue = entry.getValue();
 
         if (queue.isLocked()) {
-          LockInfo lockInfo = createLockInfo(LockInfo.ResourceType.REGION,
-              regionName, queue);
-          lockInfos.add(lockInfo);
+          LockedResource lockedResource =
+            createLockedResource(LockedResourceType.REGION, regionName, queue);
+          lockedResources.add(lockedResource);
         }
       }
 
-      return lockInfos;
+      return lockedResources;
     } finally {
       schedUnlock();
     }
   }
 
   @Override
-  public LockInfo getLockInfoForResource(LockInfo.ResourceType resourceType, String resourceName) {
+  public LockedResource getLockResource(LockedResourceType resourceType,
+      String resourceName) {
     LockAndQueue queue = null;
     schedLock();
     try {
@@ -363,7 +348,7 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
           break;
       }
 
-      return queue != null ? createLockInfo(resourceType, resourceName, queue) : null;
+      return queue != null ? createLockedResource(resourceType, resourceName, queue) : null;
     } finally {
       schedUnlock();
     }
@@ -624,17 +609,17 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
   /**
    * Get lock info for a resource of specified type and name and log details
    */
-  protected void logLockInfoForResource(LockInfo.ResourceType resourceType, String resourceName) {
+  protected void logLockInfoForResource(LockedResourceType resourceType, String resourceName) {
     if (!LOG.isDebugEnabled()) {
       return;
     }
 
-    LockInfo lockInfo = getLockInfoForResource(resourceType, resourceName);
-    if (lockInfo != null) {
+    LockedResource lockedResource = getLockResource(resourceType, resourceName);
+    if (lockedResource != null) {
       String msg = resourceType.toString() + " '" + resourceName + "', shared lock count=" +
-          lockInfo.getSharedLockCount();
+          lockedResource.getSharedLockCount();
 
-      ProcedureInfo proc = lockInfo.getExclusiveLockOwnerProcedure();
+      Procedure<?> proc = lockedResource.getExclusiveLockOwnerProcedure();
       if (proc != null) {
         msg += ", exclusively locked by procId=" + proc.getProcId();
       }
@@ -657,13 +642,13 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
       final LockAndQueue tableLock = locking.getTableLock(table);
       if (!namespaceLock.trySharedLock()) {
         waitProcedure(namespaceLock, procedure);
-        logLockInfoForResource(LockInfo.ResourceType.NAMESPACE, namespace);
+        logLockInfoForResource(LockedResourceType.NAMESPACE, namespace);
         return true;
       }
       if (!tableLock.tryExclusiveLock(procedure)) {
         namespaceLock.releaseSharedLock();
         waitProcedure(tableLock, procedure);
-        logLockInfoForResource(LockInfo.ResourceType.TABLE, table.getNameAsString());
+        logLockInfoForResource(LockedResourceType.TABLE, table.getNameAsString());
         return true;
       }
       removeFromRunQueue(tableRunQueue, getTableQueue(table));
@@ -920,7 +905,7 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
           locking.getTableLock(TableName.NAMESPACE_TABLE_NAME);
       if (!systemNamespaceTableLock.trySharedLock()) {
         waitProcedure(systemNamespaceTableLock, procedure);
-        logLockInfoForResource(LockInfo.ResourceType.TABLE,
+        logLockInfoForResource(LockedResourceType.TABLE,
             TableName.NAMESPACE_TABLE_NAME.getNameAsString());
         return true;
       }
@@ -929,7 +914,7 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
       if (!namespaceLock.tryExclusiveLock(procedure)) {
         systemNamespaceTableLock.releaseSharedLock();
         waitProcedure(namespaceLock, procedure);
-        logLockInfoForResource(LockInfo.ResourceType.NAMESPACE, namespace);
+        logLockInfoForResource(LockedResourceType.NAMESPACE, namespace);
         return true;
       }
       return false;
@@ -982,7 +967,7 @@ public class MasterProcedureScheduler extends AbstractProcedureScheduler {
         return false;
       }
       waitProcedure(lock, procedure);
-      logLockInfoForResource(LockInfo.ResourceType.SERVER, serverName.getServerName());
+      logLockInfoForResource(LockedResourceType.SERVER, serverName.getServerName());
       return true;
     } finally {
       schedUnlock();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyColumnFamilyProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyColumnFamilyProcedure.java
index 622c19f8ed..05d4f3461b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyColumnFamilyProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyColumnFamilyProcedure.java
@@ -19,9 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HColumnDescriptor;
@@ -30,6 +27,8 @@ import org.apache.hadoop.hbase.InvalidFamilyOperationException;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ModifyColumnFamilyState;
@@ -158,8 +157,9 @@ public class ModifyColumnFamilyProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.ModifyColumnFamilyStateData.Builder modifyCFMsg =
         MasterProcedureProtos.ModifyColumnFamilyStateData.newBuilder()
@@ -171,15 +171,16 @@ public class ModifyColumnFamilyProcedure
           .setUnmodifiedTableSchema(ProtobufUtil.convertToTableSchema(unmodifiedHTableDescriptor));
     }
 
-    modifyCFMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(modifyCFMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.ModifyColumnFamilyStateData modifyCFMsg =
-        MasterProcedureProtos.ModifyColumnFamilyStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.ModifyColumnFamilyStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(modifyCFMsg.getUserInfo()));
     tableName = ProtobufUtil.toTableName(modifyCFMsg.getTableName());
     cfDescriptor = ProtobufUtil.convertToHColumnDesc(modifyCFMsg.getColumnfamilySchema());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure.java
index 17e7197bcc..bd3d2422fc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure.java
@@ -19,15 +19,14 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.NamespaceNotFoundException;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.master.TableNamespaceManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ModifyNamespaceState;
@@ -130,8 +129,9 @@ public class ModifyNamespaceProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.ModifyNamespaceStateData.Builder modifyNamespaceMsg =
         MasterProcedureProtos.ModifyNamespaceStateData.newBuilder().setNamespaceDescriptor(
@@ -140,15 +140,16 @@ public class ModifyNamespaceProcedure
       modifyNamespaceMsg.setUnmodifiedNamespaceDescriptor(
         ProtobufUtil.toProtoNamespaceDescriptor(this.oldNsDescriptor));
     }
-    modifyNamespaceMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(modifyNamespaceMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.ModifyNamespaceStateData modifyNamespaceMsg =
-        MasterProcedureProtos.ModifyNamespaceStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.ModifyNamespaceStateData.class);
     newNsDescriptor =
         ProtobufUtil.toNamespaceDescriptor(modifyNamespaceMsg.getNamespaceDescriptor());
     if (modifyNamespaceMsg.hasUnmodifiedNamespaceDescriptor()) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
index 20a6a03ce7..7b744ad56d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
@@ -42,6 +40,8 @@ import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.Table;
 import org.apache.hadoop.hbase.client.TableState;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.ModifyTableState;
@@ -185,8 +185,9 @@ public class ModifyTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.ModifyTableStateData.Builder modifyTableMsg =
         MasterProcedureProtos.ModifyTableStateData.newBuilder()
@@ -199,15 +200,16 @@ public class ModifyTableProcedure
           .setUnmodifiedTableSchema(ProtobufUtil.convertToTableSchema(unmodifiedHTableDescriptor));
     }
 
-    modifyTableMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(modifyTableMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.ModifyTableStateData modifyTableMsg =
-        MasterProcedureProtos.ModifyTableStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.ModifyTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(modifyTableMsg.getUserInfo()));
     modifiedHTableDescriptor = ProtobufUtil.convertToHTableDesc(modifyTableMsg.getModifiedTableSchema());
     deleteColumnFamilyInModify = modifyTableMsg.getDeleteColumnFamilyInModify();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.java
new file mode 100644
index 0000000000..a962042e3a
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master.procedure;
+
+import java.io.IOException;
+import java.util.Date;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.classification.InterfaceStability;
+import org.apache.hadoop.hbase.procedure2.Procedure;
+import org.apache.hadoop.hbase.procedure2.ProcedureUtil;
+import org.apache.hadoop.hbase.protobuf.ProtobufFormat;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.InvalidProtocolBufferException;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
+import org.apache.hadoop.hbase.util.JRubyFormat;
+
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class ProcedureDescriber {
+  private ProcedureDescriber() {
+  }
+
+  private static Object parametersToObject(Procedure<?> proc) {
+    try {
+      ProcedureProtos.Procedure protoProc = ProcedureUtil.convertToProtoProcedure(proc);
+      List<Object> parameters = protoProc.getStateMessageList().stream()
+        .map((any) -> {
+          try {
+            return ProtobufFormat.toJavaObject(any);
+          } catch (InvalidProtocolBufferException e) {
+            return e.toString();
+          }
+        }).collect(Collectors.toList());
+      return parameters;
+    } catch (IOException e) {
+      return e.toString();
+    }
+  }
+
+  public static String describe(Procedure<?> proc) {
+    Map<String, Object> description = new LinkedHashMap<>();
+
+    description.put("ID", proc.getProcId());
+    description.put("PARENT_ID", proc.getParentProcId());
+    description.put("STATE", proc.getState());
+    description.put("OWNER", proc.getOwner());
+    description.put("TYPE", proc.getProcName());
+    description.put("START_TIME", new Date(proc.getSubmittedTime()));
+    description.put("LAST_UPDATE", new Date(proc.getLastUpdate()));
+
+    if (proc.isFailed()) {
+      description.put("ERRORS", proc.getException().unwrapRemoteIOException().getMessage());
+    }
+    description.put("PARAMETERS", parametersToObject(proc));
+
+    return JRubyFormat.print(description);
+  }
+
+  public static String describeParameters(Procedure<?> proc) {
+    Object object = parametersToObject(proc);
+    return JRubyFormat.print(object);
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java
index 72f0648bd1..870a52c663 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.java
@@ -27,6 +27,8 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.RegionReplicaUtil;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.assignment.AssignProcedure;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
 import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;
@@ -38,7 +40,6 @@ import org.apache.zookeeper.KeeperException;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.Set;
 
 /**
@@ -183,22 +184,24 @@ public class RecoverMetaProcedure
   }
 
   @Override
-  protected void serializeStateData(OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
     MasterProcedureProtos.RecoverMetaStateData.Builder state =
         MasterProcedureProtos.RecoverMetaStateData.newBuilder().setShouldSplitWal(shouldSplitWal);
     if (failedMetaServer != null) {
       state.setFailedMetaServer(ProtobufUtil.toServerName(failedMetaServer));
     }
     state.setReplicaId(replicaId);
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  protected void deserializeStateData(InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
     MasterProcedureProtos.RecoverMetaStateData state =
-        MasterProcedureProtos.RecoverMetaStateData.parseDelimitedFrom(stream);
+        deserializer.deserialize(MasterProcedureProtos.RecoverMetaStateData.class);
     this.shouldSplitWal = state.hasShouldSplitWal() && state.getShouldSplitWal();
     this.failedMetaServer = state.hasFailedMetaServer() ?
         ProtobufUtil.toServerName(state.getFailedMetaServer()) : null;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
index cfd9df94d4..5370960fef 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -45,6 +43,8 @@ import org.apache.hadoop.hbase.master.MasterFileSystem;
 import org.apache.hadoop.hbase.master.MetricsSnapshot;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -229,8 +229,9 @@ public class RestoreSnapshotProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.RestoreSnapshotStateData.Builder restoreSnapshotMsg =
       MasterProcedureProtos.RestoreSnapshotStateData.newBuilder()
@@ -267,15 +268,16 @@ public class RestoreSnapshotProcedure
         restoreSnapshotMsg.addParentToChildRegionsPairList (parentToChildrenPair);
       }
     }
-    restoreSnapshotMsg.build().writeDelimitedTo(stream);
+    serializer.serialize(restoreSnapshotMsg.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.RestoreSnapshotStateData restoreSnapshotMsg =
-      MasterProcedureProtos.RestoreSnapshotStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.RestoreSnapshotStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(restoreSnapshotMsg.getUserInfo()));
     snapshot = restoreSnapshotMsg.getSnapshot();
     modifiedHTableDescriptor =
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
index 4f3e5ce4dd..6e6220c35a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.java
@@ -19,7 +19,6 @@ package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
 import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
@@ -36,6 +35,8 @@ import org.apache.hadoop.hbase.master.assignment.AssignProcedure;
 import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
 import org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
 import org.apache.hadoop.hbase.procedure2.StateMachineProcedure;
@@ -283,8 +284,9 @@ implements ServerProcedureInterface {
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.ServerCrashStateData.Builder state =
       MasterProcedureProtos.ServerCrashStateData.newBuilder().
@@ -296,15 +298,16 @@ implements ServerProcedureInterface {
         state.addRegionsOnCrashedServer(HRegionInfo.convert(hri));
       }
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.ServerCrashStateData state =
-      MasterProcedureProtos.ServerCrashStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.ServerCrashStateData.class);
     this.serverName = ProtobufUtil.toServerName(state.getServerName());
     this.carryingMeta = state.hasCarryingMeta()? state.getCarryingMeta(): false;
     // shouldSplitWAL has a default over in pb so this invocation will always work.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
index e7f5eadb50..9875a7aa6d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.java
@@ -19,8 +19,6 @@
 package org.apache.hadoop.hbase.master.procedure;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
@@ -35,6 +33,8 @@ import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.exceptions.HBaseException;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos;
@@ -209,8 +209,9 @@ public class TruncateTableProcedure
   }
 
   @Override
-  public void serializeStateData(final OutputStream stream) throws IOException {
-    super.serializeStateData(stream);
+  protected void serializeStateData(ProcedureStateSerializer serializer)
+      throws IOException {
+    super.serializeStateData(serializer);
 
     MasterProcedureProtos.TruncateTableStateData.Builder state =
       MasterProcedureProtos.TruncateTableStateData.newBuilder()
@@ -226,15 +227,16 @@ public class TruncateTableProcedure
         state.addRegionInfo(HRegionInfo.convert(hri));
       }
     }
-    state.build().writeDelimitedTo(stream);
+    serializer.serialize(state.build());
   }
 
   @Override
-  public void deserializeStateData(final InputStream stream) throws IOException {
-    super.deserializeStateData(stream);
+  protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+      throws IOException {
+    super.deserializeStateData(deserializer);
 
     MasterProcedureProtos.TruncateTableStateData state =
-      MasterProcedureProtos.TruncateTableStateData.parseDelimitedFrom(stream);
+      deserializer.deserialize(MasterProcedureProtos.TruncateTableStateData.class);
     setUser(MasterProcedureUtil.toUserInfo(state.getUserInfo()));
     if (state.hasTableSchema()) {
       hTableDescriptor = ProtobufUtil.convertToHTableDesc(state.getTableSchema());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
index fce4eaa240..a3133c080b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.java
@@ -27,15 +27,15 @@ import java.util.concurrent.locks.ReentrantLock;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.MetaTableAccessor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.errorhandling.ForeignException;
 import org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher;
 import org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare;
@@ -45,9 +45,9 @@ import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.MetricsSnapshot;
 import org.apache.hadoop.hbase.master.SnapshotSentinel;
 import org.apache.hadoop.hbase.master.locking.LockManager;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
 import org.apache.hadoop.hbase.snapshot.SnapshotCreationException;
@@ -114,7 +114,7 @@ public abstract class TakeSnapshotHandler extends EventHandler implements Snapsh
     this.snapshotManifest = SnapshotManifest.create(conf, fs, workingDir, snapshot, monitor);
 
     this.tableLock = master.getLockManager().createMasterLock(
-        snapshotTable, LockProcedure.LockType.EXCLUSIVE,
+        snapshotTable, LockType.EXCLUSIVE,
         this.getClass().getName() + ": take snapshot " + snapshot.getName());
 
     // prepare the verify
@@ -134,6 +134,7 @@ public abstract class TakeSnapshotHandler extends EventHandler implements Snapsh
     return htd;
   }
 
+  @Override
   public TakeSnapshotHandler prepare() throws Exception {
     super.prepare();
     // after this, you should ensure to release this lock in case of exceptions
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index c40d481708..ce2961ebce 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -49,7 +49,6 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.MetaTableAccessor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.Tag;
@@ -88,9 +87,10 @@ import org.apache.hadoop.hbase.ipc.CoprocessorRpcUtils;
 import org.apache.hadoop.hbase.ipc.RpcServer;
 import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.locking.LockProcedure;
-import org.apache.hadoop.hbase.master.locking.LockProcedure.LockType;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.net.Address;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos;
@@ -110,6 +110,13 @@ import org.apache.hadoop.hbase.security.Superusers;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.security.access.Permission.Action;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.ArrayListMultimap;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.ImmutableSet;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.ListMultimap;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.MapMaker;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.Maps;
+import org.apache.hadoop.hbase.shaded.com.google.common.collect.Sets;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.WALEntry;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest;
@@ -123,13 +130,6 @@ import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.SimpleMutableByteRange;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.ArrayListMultimap;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.ImmutableSet;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.ListMultimap;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.MapMaker;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.Maps;
-import org.apache.hadoop.hbase.shaded.com.google.common.collect.Sets;
 import com.google.protobuf.Message;
 import com.google.protobuf.RpcCallback;
 import com.google.protobuf.RpcController;
@@ -1218,19 +1218,20 @@ public class AccessController implements MasterObserver, RegionObserver, RegionS
   @Override
   public void postListProcedures(
       ObserverContext<MasterCoprocessorEnvironment> ctx,
-      List<ProcedureInfo> procInfoList) throws IOException {
-    if (procInfoList.isEmpty()) {
+      List<Procedure<?>> procList) throws IOException {
+    if (procList.isEmpty()) {
       return;
     }
 
     // Retains only those which passes authorization checks, as the checks weren't done as part
     // of preListProcedures.
-    Iterator<ProcedureInfo> itr = procInfoList.iterator();
+    Iterator<Procedure<?>> itr = procList.iterator();
     User user = getActiveUser(ctx);
     while (itr.hasNext()) {
-      ProcedureInfo procInfo = itr.next();
+      Procedure<?> proc = itr.next();
       try {
-        if (!ProcedureInfo.isProcedureOwner(procInfo, user)) {
+        String owner = proc.getOwner();
+        if (owner == null || !owner.equals(user.getShortName())) {
           // If the user is not the procedure owner, then we should further probe whether
           // he can see the procedure.
           requirePermission(user, "listProcedures", Action.ADMIN);
diff --git a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp b/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp
index a3701fffe6..562d4a4e78 100644
--- a/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp
+++ b/hbase-server/src/main/resources/hbase-webapps/master/procedures.jsp
@@ -29,14 +29,18 @@
   import="org.apache.hadoop.hbase.HBaseConfiguration"
   import="org.apache.hadoop.hbase.master.HMaster"
   import="org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv"
-  import="org.apache.hadoop.hbase.ProcedureInfo"
+  import="org.apache.hadoop.hbase.master.procedure.ProcedureDescriber"
+  import="org.apache.hadoop.hbase.procedure2.LockedResource"
   import="org.apache.hadoop.hbase.procedure2.LockInfo"
   import="org.apache.hadoop.hbase.procedure2.Procedure"
   import="org.apache.hadoop.hbase.procedure2.ProcedureExecutor"
+  import="org.apache.hadoop.hbase.procedure2.ProcedureInfo"
+  import="org.apache.hadoop.hbase.procedure2.ProcedureUtil"
   import="org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFile"
   import="org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore"
   import="org.apache.hadoop.hbase.procedure2.util.StringUtils"
-
+  import="org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos"
+  import="org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil"
 %>
 <%
   HMaster master = (HMaster)getServletContext().getAttribute(HMaster.MASTER);
@@ -48,7 +52,7 @@
   long millisFromLastRoll = walStore.getMillisFromLastRoll();
   ArrayList<ProcedureWALFile> procedureWALFiles = walStore.getActiveLogs();
   Set<ProcedureWALFile> corruptedWALFiles = walStore.getCorruptedLogs();
-  List<Procedure> procedures = procExecutor.listProcedures();
+  List<Procedure<?>> procedures = procExecutor.listProcedures();
   Collections.sort(procedures, new Comparator<Procedure>() {
     @Override
     public int compare(Procedure lhs, Procedure rhs) {
@@ -58,7 +62,7 @@
     }
   });
 
-  List<LockInfo> locks = master.listLocks();
+  List<LockedResource> lockedResources = master.listLocks();
 %>
 <!DOCTYPE html>
 <?xml version="1.0" encoding="UTF-8" ?>
@@ -118,6 +122,7 @@
         <th>Start Time</th>
         <th>Last Update</th>
         <th>Errors</th>
+        <th>Parameters</th>
     </tr>
     <% for (Procedure<?> proc : procedures) { %>
       <tr>
@@ -129,6 +134,7 @@
         <td><%= new Date(proc.getSubmittedTime()) %></td>
         <td><%= new Date(proc.getLastUpdate()) %></td>
         <td><%= escapeXml(proc.isFailed() ? proc.getException().unwrapRemoteIOException().getMessage() : "") %></td>
+        <td><%= escapeXml(ProcedureDescriber.describeParameters(proc)) %></td>
       </tr>
     <% } %>
   </table>
@@ -244,40 +250,35 @@
           <h1>Locks</h1>
       </div>
   </div>
-  <% for (LockInfo lock : locks) { %>
-    <h2><%= lock.getResourceType() %>: <%= lock.getResourceName() %></h2>
+  <% for (LockedResource lockedResource : lockedResources) { %>
+    <h2><%= lockedResource.getResourceType() %>: <%= lockedResource.getResourceName() %></h2>
     <%
-      switch (lock.getLockType()) {
+      switch (lockedResource.getLockType()) {
       case EXCLUSIVE:
     %>
     <p>Lock type: EXCLUSIVE</p>
-    <p>Owner procedure ID: <%= lock.getExclusiveLockOwnerProcedure().getProcId() %></p>
+    <p>Owner procedure: <%= escapeXml(ProcedureDescriber.describe(lockedResource.getExclusiveLockOwnerProcedure())) %></p>
     <%
         break;
       case SHARED:
     %>
     <p>Lock type: SHARED</p>
-    <p>Number of shared locks: <%= lock.getSharedLockCount() %></p>
+    <p>Number of shared locks: <%= lockedResource.getSharedLockCount() %></p>
     <%
         break;
       }
 
-      List<LockInfo.WaitingProcedure> waitingProcedures = lock.getWaitingProcedures();
+      List<Procedure<?>> waitingProcedures = lockedResource.getWaitingProcedures();
 
       if (!waitingProcedures.isEmpty()) {
     %>
 	    <h3>Waiting procedures</h3>
 	    <table class="table table-striped" width="90%" >
-		    <tr>
-		      <th>Lock type</th>
-		      <th>Procedure ID</th>
-		    </tr>
-		    <% for (LockInfo.WaitingProcedure waitingProcedure : waitingProcedures) { %>
+		    <% for (Procedure<?> proc : procedures) { %>
 		      <tr>
-	          <td><%= waitingProcedure.getLockType() %></td>
-	          <td><%= waitingProcedure.getProcedure().getProcId() %></td>
+		        <td><%= escapeXml(ProcedureDescriber.describe(proc)) %></td>
 		      </tr>
-		    <% } %>
+        <% } %>
 	    </table>
     <% } %>
   <% } %>
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
index 7bdda808ae..e36c138570 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin2.java
@@ -45,7 +45,6 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.NotServingRegionException;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.TableName;
@@ -56,6 +55,7 @@ import org.apache.hadoop.hbase.UnknownRegionException;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.constraint.ConstraintException;
 import org.apache.hadoop.hbase.master.assignment.AssignmentManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.Region;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcedureAdminApi.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcedureAdminApi.java
index b7fac06141..5a07844171 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcedureAdminApi.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAsyncProcedureAdminApi.java
@@ -19,12 +19,12 @@
 package org.apache.hadoop.hbase.client;
 
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;
 import org.apache.hadoop.hbase.procedure.ProcedureManagerHost;
 import org.apache.hadoop.hbase.procedure.SimpleMasterProcedureManager;
 import org.apache.hadoop.hbase.procedure.SimpleRSProcedureManager;
+import org.apache.hadoop.hbase.procedure2.ProcedureInfo;
 import org.apache.hadoop.hbase.testclassification.ClientTests;
 import org.apache.hadoop.hbase.testclassification.LargeTests;
 import org.apache.hadoop.hbase.util.Bytes;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
index 1b8b27b061..29090baba6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
@@ -42,7 +42,6 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Admin;
@@ -60,7 +59,9 @@ import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.locking.LockProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.net.Address;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
@@ -718,7 +719,7 @@ public class TestMasterObserver {
     @Override
     public void postListProcedures(
         ObserverContext<MasterCoprocessorEnvironment> ctx,
-        List<ProcedureInfo> procInfoList) throws IOException {
+        List<Procedure<?>> procInfoList) throws IOException {
       postListProceduresCalled = true;
     }
 
@@ -736,7 +737,7 @@ public class TestMasterObserver {
     }
 
     @Override
-    public void postListLocks(ObserverContext<MasterCoprocessorEnvironment> ctx, List<LockInfo> lockInfoList)
+    public void postListLocks(ObserverContext<MasterCoprocessorEnvironment> ctx, List<LockedResource> lockedResources)
         throws IOException {
       postListLocksCalled = true;
     }
@@ -1546,14 +1547,14 @@ public class TestMasterObserver {
 
     @Override
     public void preRequestLock(ObserverContext<MasterCoprocessorEnvironment> ctx, String namespace,
-        TableName tableName, HRegionInfo[] regionInfos, LockProcedure.LockType type,
+        TableName tableName, HRegionInfo[] regionInfos, LockType type,
         String description) throws IOException {
       preRequestLockCalled = true;
     }
 
     @Override
     public void postRequestLock(ObserverContext<MasterCoprocessorEnvironment> ctx, String namespace,
-        TableName tableName, HRegionInfo[] regionInfos, LockProcedure.LockType type,
+        TableName tableName, HRegionInfo[] regionInfos, LockType type,
         String description) throws IOException {
       postRequestLockCalled = true;
     }
@@ -2222,7 +2223,7 @@ public class TestMasterObserver {
 
     final TableName tableName = TableName.valueOf("testLockedTable");
     long procId = master.getLockManager().remoteLocks().requestTableLock(tableName,
-          LockProcedure.LockType.EXCLUSIVE, "desc", null);
+          LockType.EXCLUSIVE, "desc", null);
     master.getLockManager().remoteLocks().lockHeartbeat(procId, false);
 
     assertTrue(cp.preAndPostForQueueLockAndHeartbeatLockCalled());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java
index aac0e212fe..0a4a65b510 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockNoopMasterServices.java
@@ -26,7 +26,6 @@ import org.apache.hadoop.hbase.CoordinatedStateManager;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableDescriptors;
@@ -42,7 +41,8 @@ import org.apache.hadoop.hbase.master.normalizer.RegionNormalizer;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;
 import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.quotas.MasterQuotaManager;
@@ -51,6 +51,7 @@ import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
 import org.apache.hadoop.hbase.replication.ReplicationPeerDescription;
 import org.apache.hadoop.hbase.zookeeper.MetaTableLocator;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+
 import com.google.protobuf.Service;
 
 import static org.mockito.Mockito.mock;
@@ -227,12 +228,12 @@ public class MockNoopMasterServices implements MasterServices, Server {
   }
 
   @Override
-  public List<ProcedureInfo> listProcedures() throws IOException {
+  public List<Procedure<?>> listProcedures() throws IOException {
     return null;  //To change body of implemented methods use File | Settings | File Templates.
   }
 
   @Override
-  public List<LockInfo> listLocks() throws IOException {
+  public List<LockedResource> listLocks() throws IOException {
     return null;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockManager.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockManager.java
index d85146af32..d51d4d5d57 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockManager.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockManager.java
@@ -18,6 +18,12 @@
 
 package org.apache.hadoop.hbase.master.locking;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.util.List;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -26,10 +32,9 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.master.MasterServices;
-import org.apache.hadoop.hbase.master.locking.LockProcedure;
-import org.apache.hadoop.hbase.master.locking.TestLockProcedure;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureConstants;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;
@@ -43,12 +48,6 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
-import java.util.List;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
 @Category({MasterTests.class, SmallTests.class})
 public class TestLockManager {
   @Rule
@@ -113,7 +112,7 @@ public class TestLockManager {
   @Test
   public void testMasterLockAcquire() throws Exception {
     LockManager.MasterLock lock = masterServices.getLockManager().createMasterLock(namespace,
-        LockProcedure.LockType.EXCLUSIVE, "desc");
+        LockType.EXCLUSIVE, "desc");
     assertTrue(lock.tryAcquire(2000));
     assertTrue(lock.getProc().isLocked());
     lock.release();
@@ -126,9 +125,9 @@ public class TestLockManager {
   @Test
   public void testMasterLockAcquireTimeout() throws Exception {
     LockManager.MasterLock lock = masterServices.getLockManager().createMasterLock(
-        tableName, LockProcedure.LockType.EXCLUSIVE, "desc");
+        tableName, LockType.EXCLUSIVE, "desc");
     LockManager.MasterLock lock2 = masterServices.getLockManager().createMasterLock(
-        tableName, LockProcedure.LockType.EXCLUSIVE, "desc");
+        tableName, LockType.EXCLUSIVE, "desc");
     assertTrue(lock.tryAcquire(2000));
     assertFalse(lock2.tryAcquire(LOCAL_LOCKS_TIMEOUT/2));  // wait less than other lock's timeout
     assertEquals(null, lock2.getProc());
@@ -146,7 +145,7 @@ public class TestLockManager {
     LockManager.MasterLock lock = masterServices.getLockManager().createMasterLock(
         tableRegions, "desc");
     LockManager.MasterLock lock2 = masterServices.getLockManager().createMasterLock(
-        tableName, LockProcedure.LockType.EXCLUSIVE, "desc");
+        tableName, LockType.EXCLUSIVE, "desc");
     assertTrue(lock.tryAcquire(2000));
     assertFalse(lock2.tryAcquire(LOCAL_LOCKS_TIMEOUT/2));  // wait less than other lock's timeout
     assertEquals(null, lock2.getProc());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java
index adaebf455c..da09ac9851 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/locking/TestLockProcedure.java
@@ -18,9 +18,20 @@
 
 package org.apache.hadoop.hbase.master.locking;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.CategoryBasedTimeout;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
@@ -28,40 +39,33 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.locking.LockServiceClient;
+import org.apache.hadoop.hbase.master.MasterRpcServices;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureConstants;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
-import org.apache.hadoop.hbase.master.MasterRpcServices;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility;
 import org.apache.hadoop.hbase.shaded.com.google.protobuf.ServiceException;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.*;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockHeartbeatRequest;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockHeartbeatResponse;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockRequest;
+import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos.LockResponse;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.hamcrest.core.IsInstanceOf;
 import org.hamcrest.core.StringStartsWith;
-import org.junit.rules.TestRule;
-import org.junit.experimental.categories.Category;
-
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Rule;
 import org.junit.Test;
+import org.junit.experimental.categories.Category;
 import org.junit.rules.ExpectedException;
 import org.junit.rules.TestName;
-import org.apache.hadoop.hbase.CategoryBasedTimeout;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
+import org.junit.rules.TestRule;
 
 @Category({MasterTests.class, SmallTests.class})
 public class TestLockProcedure {
@@ -138,17 +142,17 @@ public class TestLockProcedure {
   }
 
   private LockRequest getNamespaceLock(String namespace, String description) {
-    return LockServiceClient.buildLockRequest(LockType.EXCLUSIVE,
+    return LockServiceClient.buildLockRequest(LockServiceProtos.LockType.EXCLUSIVE,
         namespace, null, null, description, HConstants.NO_NONCE, HConstants.NO_NONCE);
   }
 
   private LockRequest getTableExclusiveLock(TableName tableName, String description) {
-    return LockServiceClient.buildLockRequest(LockType.EXCLUSIVE,
+    return LockServiceClient.buildLockRequest(LockServiceProtos.LockType.EXCLUSIVE,
         null, tableName, null, description, HConstants.NO_NONCE, HConstants.NO_NONCE);
   }
 
   private LockRequest getRegionLock(List<HRegionInfo> regionInfos, String description) {
-    return LockServiceClient.buildLockRequest(LockType.EXCLUSIVE,
+    return LockServiceClient.buildLockRequest(LockServiceProtos.LockType.EXCLUSIVE,
         null, null, regionInfos, description, HConstants.NO_NONCE, HConstants.NO_NONCE);
   }
 
@@ -345,7 +349,7 @@ public class TestLockProcedure {
     CountDownLatch latch = new CountDownLatch(1);
     // MasterRpcServices don't set latch with LockProcedure, so create one and submit it directly.
     LockProcedure lockProc = new LockProcedure(UTIL.getConfiguration(),
-        TableName.valueOf("table"), LockProcedure.LockType.EXCLUSIVE, "desc", latch);
+        TableName.valueOf("table"), org.apache.hadoop.hbase.procedure2.LockType.EXCLUSIVE, "desc", latch);
     procExec.submitProcedure(lockProc);
     assertTrue(latch.await(2000, TimeUnit.MILLISECONDS));
     releaseLock(lockProc.getProcId());
@@ -359,7 +363,7 @@ public class TestLockProcedure {
     CountDownLatch latch = new CountDownLatch(1);
     // MasterRpcServices don't set latch with LockProcedure, so create one and submit it directly.
     LockProcedure lockProc = new LockProcedure(UTIL.getConfiguration(),
-        TableName.valueOf("table"), LockProcedure.LockType.EXCLUSIVE, "desc", latch);
+        TableName.valueOf("table"), LockType.EXCLUSIVE, "desc", latch);
     procExec.submitProcedure(lockProc);
     assertTrue(awaitForLocked(lockProc.getProcId(), 2000));
     Thread.sleep(LOCAL_LOCKS_TIMEOUT / 2);
@@ -421,7 +425,7 @@ public class TestLockProcedure {
     ProcedureTestingUtility.setKillAndToggleBeforeStoreUpdate(procExec, true);
     CountDownLatch latch = new CountDownLatch(1);
     LockProcedure lockProc = new LockProcedure(UTIL.getConfiguration(),
-        TableName.valueOf("table"), LockProcedure.LockType.EXCLUSIVE, "desc", latch);
+        TableName.valueOf("table"), LockType.EXCLUSIVE, "desc", latch);
     procExec.submitProcedure(lockProc);
     assertTrue(latch.await(2000, TimeUnit.MILLISECONDS));
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureScheduler.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureScheduler.java
index 5f20c7ffa3..f0a275b86e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureScheduler.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestMasterProcedureScheduler.java
@@ -32,10 +32,11 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.master.locking.LockProcedure;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureEvent;
-import org.apache.hadoop.hbase.procedure2.LockInfo.WaitingProcedure;
+import org.apache.hadoop.hbase.procedure2.LockType;
+import org.apache.hadoop.hbase.procedure2.LockedResource;
+import org.apache.hadoop.hbase.procedure2.LockedResourceType;
 import org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.TestProcedure;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
@@ -903,7 +904,7 @@ public class TestMasterProcedureScheduler {
     }
   }
 
-  private static LockProcedure createLockProcedure(LockProcedure.LockType lockType, long procId) throws Exception {
+  private static LockProcedure createLockProcedure(LockType lockType, long procId) throws Exception {
     LockProcedure procedure = new LockProcedure();
 
     Field typeField = LockProcedure.class.getDeclaredField("type");
@@ -918,31 +919,31 @@ public class TestMasterProcedureScheduler {
   }
 
   private static LockProcedure createExclusiveLockProcedure(long procId) throws Exception {
-    return createLockProcedure(LockProcedure.LockType.EXCLUSIVE, procId);
+    return createLockProcedure(LockType.EXCLUSIVE, procId);
   }
 
   private static LockProcedure createSharedLockProcedure(long procId) throws Exception {
-    return createLockProcedure(LockProcedure.LockType.SHARED, procId);
+    return createLockProcedure(LockType.SHARED, procId);
   }
 
-  private static void assertLockResource(LockInfo lock,
-      LockInfo.ResourceType resourceType, String resourceName)
+  private static void assertLockResource(LockedResource resource,
+      LockedResourceType resourceType, String resourceName)
   {
-    assertEquals(resourceType, lock.getResourceType());
-    assertEquals(resourceName, lock.getResourceName());
+    assertEquals(resourceType, resource.getResourceType());
+    assertEquals(resourceName, resource.getResourceName());
   }
 
-  private static void assertExclusiveLock(LockInfo lock, long procId)
+  private static void assertExclusiveLock(LockedResource resource, Procedure<?> procedure)
   {
-    assertEquals(LockInfo.LockType.EXCLUSIVE, lock.getLockType());
-    assertEquals(procId, lock.getExclusiveLockOwnerProcedure().getProcId());
-    assertEquals(0, lock.getSharedLockCount());
+    assertEquals(LockType.EXCLUSIVE, resource.getLockType());
+    assertEquals(procedure, resource.getExclusiveLockOwnerProcedure());
+    assertEquals(0, resource.getSharedLockCount());
   }
 
-  private static void assertSharedLock(LockInfo lock, int lockCount)
+  private static void assertSharedLock(LockedResource resource, int lockCount)
   {
-    assertEquals(LockInfo.LockType.SHARED, lock.getLockType());
-    assertEquals(lockCount, lock.getSharedLockCount());
+    assertEquals(LockType.SHARED, resource.getLockType());
+    assertEquals(lockCount, resource.getSharedLockCount());
   }
 
   @Test
@@ -950,13 +951,13 @@ public class TestMasterProcedureScheduler {
     LockProcedure procedure = createExclusiveLockProcedure(0);
     queue.waitServerExclusiveLock(procedure, ServerName.valueOf("server1,1234,0"));
 
-    List<LockInfo> locks = queue.listLocks();
-    assertEquals(1, locks.size());
+    List<LockedResource> resources = queue.listLocks();
+    assertEquals(1, resources.size());
 
-    LockInfo serverLock = locks.get(0);
-    assertLockResource(serverLock, LockInfo.ResourceType.SERVER, "server1,1234,0");
-    assertExclusiveLock(serverLock, 0);
-    assertTrue(serverLock.getWaitingProcedures().isEmpty());
+    LockedResource serverResource = resources.get(0);
+    assertLockResource(serverResource, LockedResourceType.SERVER, "server1,1234,0");
+    assertExclusiveLock(serverResource, procedure);
+    assertTrue(serverResource.getWaitingProcedures().isEmpty());
   }
 
   @Test
@@ -964,19 +965,19 @@ public class TestMasterProcedureScheduler {
     LockProcedure procedure = createExclusiveLockProcedure(1);
     queue.waitNamespaceExclusiveLock(procedure, "ns1");
 
-    List<LockInfo> locks = queue.listLocks();
+    List<LockedResource> locks = queue.listLocks();
     assertEquals(2, locks.size());
 
-    LockInfo namespaceLock = locks.get(0);
-    assertLockResource(namespaceLock, LockInfo.ResourceType.NAMESPACE, "ns1");
-    assertExclusiveLock(namespaceLock, 1);
-    assertTrue(namespaceLock.getWaitingProcedures().isEmpty());
+    LockedResource namespaceResource = locks.get(0);
+    assertLockResource(namespaceResource, LockedResourceType.NAMESPACE, "ns1");
+    assertExclusiveLock(namespaceResource, procedure);
+    assertTrue(namespaceResource.getWaitingProcedures().isEmpty());
 
-    LockInfo tableLock = locks.get(1);
-    assertLockResource(tableLock, LockInfo.ResourceType.TABLE,
+    LockedResource tableResource = locks.get(1);
+    assertLockResource(tableResource, LockedResourceType.TABLE,
         TableName.NAMESPACE_TABLE_NAME.getNameAsString());
-    assertSharedLock(tableLock, 1);
-    assertTrue(tableLock.getWaitingProcedures().isEmpty());
+    assertSharedLock(tableResource, 1);
+    assertTrue(tableResource.getWaitingProcedures().isEmpty());
   }
 
   @Test
@@ -984,18 +985,18 @@ public class TestMasterProcedureScheduler {
     LockProcedure procedure = createExclusiveLockProcedure(2);
     queue.waitTableExclusiveLock(procedure, TableName.valueOf("ns2", "table2"));
 
-    List<LockInfo> locks = queue.listLocks();
+    List<LockedResource> locks = queue.listLocks();
     assertEquals(2, locks.size());
 
-    LockInfo namespaceLock = locks.get(0);
-    assertLockResource(namespaceLock, LockInfo.ResourceType.NAMESPACE, "ns2");
-    assertSharedLock(namespaceLock, 1);
-    assertTrue(namespaceLock.getWaitingProcedures().isEmpty());
+    LockedResource namespaceResource = locks.get(0);
+    assertLockResource(namespaceResource, LockedResourceType.NAMESPACE, "ns2");
+    assertSharedLock(namespaceResource, 1);
+    assertTrue(namespaceResource.getWaitingProcedures().isEmpty());
 
-    LockInfo tableLock = locks.get(1);
-    assertLockResource(tableLock, LockInfo.ResourceType.TABLE, "ns2:table2");
-    assertExclusiveLock(tableLock, 2);
-    assertTrue(tableLock.getWaitingProcedures().isEmpty());
+    LockedResource tableResource = locks.get(1);
+    assertLockResource(tableResource, LockedResourceType.TABLE, "ns2:table2");
+    assertExclusiveLock(tableResource, procedure);
+    assertTrue(tableResource.getWaitingProcedures().isEmpty());
   }
 
   @Test
@@ -1005,23 +1006,23 @@ public class TestMasterProcedureScheduler {
 
     queue.waitRegion(procedure, regionInfo);
 
-    List<LockInfo> locks = queue.listLocks();
-    assertEquals(3, locks.size());
+    List<LockedResource> resources = queue.listLocks();
+    assertEquals(3, resources.size());
 
-    LockInfo namespaceLock = locks.get(0);
-    assertLockResource(namespaceLock, LockInfo.ResourceType.NAMESPACE, "ns3");
-    assertSharedLock(namespaceLock, 1);
-    assertTrue(namespaceLock.getWaitingProcedures().isEmpty());
+    LockedResource namespaceResource = resources.get(0);
+    assertLockResource(namespaceResource, LockedResourceType.NAMESPACE, "ns3");
+    assertSharedLock(namespaceResource, 1);
+    assertTrue(namespaceResource.getWaitingProcedures().isEmpty());
 
-    LockInfo tableLock = locks.get(1);
-    assertLockResource(tableLock, LockInfo.ResourceType.TABLE, "ns3:table3");
-    assertSharedLock(tableLock, 1);
-    assertTrue(tableLock.getWaitingProcedures().isEmpty());
+    LockedResource tableResource = resources.get(1);
+    assertLockResource(tableResource, LockedResourceType.TABLE, "ns3:table3");
+    assertSharedLock(tableResource, 1);
+    assertTrue(tableResource.getWaitingProcedures().isEmpty());
 
-    LockInfo regionLock = locks.get(2);
-    assertLockResource(regionLock, LockInfo.ResourceType.REGION, regionInfo.getEncodedName());
-    assertExclusiveLock(regionLock, 3);
-    assertTrue(regionLock.getWaitingProcedures().isEmpty());
+    LockedResource regionResource = resources.get(2);
+    assertLockResource(regionResource, LockedResourceType.REGION, regionInfo.getEncodedName());
+    assertExclusiveLock(regionResource, procedure);
+    assertTrue(regionResource.getWaitingProcedures().isEmpty());
   }
 
   @Test
@@ -1035,28 +1036,28 @@ public class TestMasterProcedureScheduler {
     LockProcedure procedure3 = createExclusiveLockProcedure(3);
     queue.waitTableExclusiveLock(procedure3, TableName.valueOf("ns4", "table4"));
 
-    List<LockInfo> locks = queue.listLocks();
-    assertEquals(2, locks.size());
+    List<LockedResource> resources = queue.listLocks();
+    assertEquals(2, resources.size());
 
-    LockInfo namespaceLock = locks.get(0);
-    assertLockResource(namespaceLock, LockInfo.ResourceType.NAMESPACE, "ns4");
-    assertSharedLock(namespaceLock, 1);
-    assertTrue(namespaceLock.getWaitingProcedures().isEmpty());
+    LockedResource namespaceResource = resources.get(0);
+    assertLockResource(namespaceResource, LockedResourceType.NAMESPACE, "ns4");
+    assertSharedLock(namespaceResource, 1);
+    assertTrue(namespaceResource.getWaitingProcedures().isEmpty());
 
-    LockInfo tableLock = locks.get(1);
-    assertLockResource(tableLock, LockInfo.ResourceType.TABLE, "ns4:table4");
-    assertExclusiveLock(tableLock, 1);
+    LockedResource tableLock = resources.get(1);
+    assertLockResource(tableLock, LockedResourceType.TABLE, "ns4:table4");
+    assertExclusiveLock(tableLock, procedure1);
 
-    List<WaitingProcedure> waitingProcedures = tableLock.getWaitingProcedures();
+    List<Procedure<?>> waitingProcedures = tableLock.getWaitingProcedures();
     assertEquals(2, waitingProcedures.size());
 
-    WaitingProcedure waitingProcedure1 = waitingProcedures.get(0);
-    assertEquals(LockInfo.LockType.SHARED, waitingProcedure1.getLockType());
-    assertEquals(2, waitingProcedure1.getProcedure().getProcId());
+    LockProcedure waitingProcedure2 = (LockProcedure) waitingProcedures.get(0);
+    assertEquals(LockType.SHARED, waitingProcedure2.getType());
+    assertEquals(procedure2, waitingProcedure2);
 
-    WaitingProcedure waitingProcedure2 = waitingProcedures.get(1);
-    assertEquals(LockInfo.LockType.EXCLUSIVE, waitingProcedure2.getLockType());
-    assertEquals(3, waitingProcedure2.getProcedure().getProcId());
+    LockProcedure waitingProcedure3 = (LockProcedure) waitingProcedures.get(1);
+    assertEquals(LockType.EXCLUSIVE, waitingProcedure3.getType());
+    assertEquals(procedure3, waitingProcedure3);
   }
 }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestProcedureAdmin.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestProcedureAdmin.java
index 692815fd8b..1174879b8a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestProcedureAdmin.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/procedure/TestProcedureAdmin.java
@@ -206,10 +206,10 @@ public class TestProcedureAdmin {
     // Wait for one step to complete
     ProcedureTestingUtility.waitProcedure(procExec, procId);
 
-    List<Procedure> listProcedures = procExec.listProcedures();
+    List<Procedure<?>> listProcedures = procExec.listProcedures();
     assertTrue(listProcedures.size() >= 1);
     boolean found = false;
-    for (Procedure proc: listProcedures) {
+    for (Procedure<?> proc: listProcedures) {
       if (proc.getProcId() == procId) {
         assertTrue(proc.isRunnable());
         found = true;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureDescriber.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureDescriber.java
new file mode 100644
index 0000000000..e83f3e79ad
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestProcedureDescriber.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.procedure;
+
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+import java.util.Date;
+
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.apache.hadoop.hbase.master.procedure.ProcedureDescriber;
+import org.apache.hadoop.hbase.procedure2.Procedure;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;
+import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.ByteString;
+import org.apache.hadoop.hbase.shaded.com.google.protobuf.BytesValue;
+import org.apache.hadoop.hbase.testclassification.MasterTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({MasterTests.class, SmallTests.class})
+public class TestProcedureDescriber {
+  public static class TestProcedure extends Procedure {
+    @Override
+    protected Procedure[] execute(Object env) throws ProcedureYieldException,
+        ProcedureSuspendedException, InterruptedException {
+      return null;
+    }
+
+    @Override
+    protected void rollback(Object env)
+        throws IOException, InterruptedException {
+    }
+
+    @Override
+    protected boolean abort(Object env) {
+      return false;
+    }
+
+    @Override
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
+      ByteString byteString = ByteString.copyFrom(new byte[] { 'A' });
+      BytesValue state = BytesValue.newBuilder().setValue(byteString).build();
+      serializer.serialize(state);
+    }
+
+    @Override
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
+    }
+  }
+
+  @Test
+  public void test() {
+    TestProcedure procedure = new TestProcedure();
+    String result = ProcedureDescriber.describe(procedure);
+
+    Date epoch = new Date(0);
+
+    assertEquals("{ ID => '-1', PARENT_ID => '-1', STATE => 'INITIALIZING', OWNER => '', "
+        + "TYPE => 'org.apache.hadoop.hbase.procedure.TestProcedureDescriber$TestProcedure', "
+        + "START_TIME => '" + epoch + "', LAST_UPDATE => '" + epoch + "', PARAMETERS => [ "
+        + "{ @type => 'type.googleapis.com/google.protobuf.BytesValue', value => 'QQ==' } "
+        + "] }", result);
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestProtobufUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestProtobufUtil.java
index c88c370cc4..209ff2b745 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestProtobufUtil.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/protobuf/TestProtobufUtil.java
@@ -20,8 +20,6 @@ package org.apache.hadoop.hbase.protobuf;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
-
 import java.io.IOException;
 import java.nio.ByteBuffer;
 
@@ -29,15 +27,12 @@ import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.ProcedureInfo;
-import org.apache.hadoop.hbase.ProcedureState;
 import org.apache.hadoop.hbase.ByteBufferKeyValue;
 import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Increment;
 import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Column;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto;
@@ -339,40 +334,4 @@ public class TestProtobufUtil {
     Cell newOffheapKV = org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.toCell(cell);
     assertTrue(CellComparator.COMPARATOR.compare(offheapKV, newOffheapKV) == 0);
   }
-
-  private static ProcedureInfo createProcedureInfo(long procId)
-  {
-    return new ProcedureInfo(procId, "java.lang.Object", null,
-        ProcedureState.RUNNABLE, -1, null, null, 0, 0, null);
-  }
-
-  private static void assertProcedureInfoEquals(ProcedureInfo expected,
-      ProcedureInfo result)
-  {
-    if (expected == result) {
-      return;
-    } else if (expected == null || result == null) {
-      fail();
-    }
-
-    assertEquals(expected.getProcId(), result.getProcId());
-  }
-
-  private static void assertLockInfoEquals(LockInfo expected, LockInfo result)
-  {
-    assertEquals(expected.getResourceType(), result.getResourceType());
-    assertEquals(expected.getResourceName(), result.getResourceName());
-    assertEquals(expected.getLockType(), result.getLockType());
-    assertProcedureInfoEquals(expected.getExclusiveLockOwnerProcedure(),
-        result.getExclusiveLockOwnerProcedure());
-    assertEquals(expected.getSharedLockCount(), result.getSharedLockCount());
-  }
-
-  private static void assertWaitingProcedureEquals(
-      LockInfo.WaitingProcedure expected, LockInfo.WaitingProcedure result)
-  {
-    assertEquals(expected.getLockType(), result.getLockType());
-    assertProcedureInfoEquals(expected.getProcedure(),
-        result.getProcedure());
-  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
index c1fbb28277..9e6bf8ccd0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
@@ -25,8 +25,6 @@ import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
 import java.security.PrivilegedAction;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -51,7 +49,6 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
-import org.apache.hadoop.hbase.ProcedureInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotFoundException;
@@ -96,12 +93,13 @@ import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.locking.LockProcedure;
-import org.apache.hadoop.hbase.master.locking.LockProcedure.LockType;
 import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
 import org.apache.hadoop.hbase.master.procedure.TableProcedureInterface;
+import org.apache.hadoop.hbase.procedure2.LockType;
 import org.apache.hadoop.hbase.procedure2.Procedure;
 import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
-import org.apache.hadoop.hbase.procedure2.ProcedureUtil;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateDeserializer;
+import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.procedure2.ProcedureYieldException;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos;
@@ -576,17 +574,19 @@ public class TestAccessController extends SecureTestUtil {
     }
 
     @Override
-    protected void serializeStateData(OutputStream stream) throws IOException {
+    protected void serializeStateData(ProcedureStateSerializer serializer)
+        throws IOException {
       TestProcedureProtos.TestTableDDLStateData.Builder testTableDDLMsg =
           TestProcedureProtos.TestTableDDLStateData.newBuilder()
           .setTableName(tableName.getNameAsString());
-      testTableDDLMsg.build().writeDelimitedTo(stream);
+      serializer.serialize(testTableDDLMsg.build());
     }
 
     @Override
-    protected void deserializeStateData(InputStream stream) throws IOException {
+    protected void deserializeStateData(ProcedureStateDeserializer deserializer)
+        throws IOException {
       TestProcedureProtos.TestTableDDLStateData testTableDDLMsg =
-          TestProcedureProtos.TestTableDDLStateData.parseDelimitedFrom(stream);
+          deserializer.deserialize(TestProcedureProtos.TestTableDDLStateData.class);
       tableName = TableName.valueOf(testTableDDLMsg.getTableName());
     }
 
@@ -635,17 +635,13 @@ public class TestAccessController extends SecureTestUtil {
     Procedure proc = new TestTableDDLProcedure(procExec.getEnvironment(), tableName);
     proc.setOwner(USER_OWNER);
     procExec.submitProcedure(proc);
-    final List<Procedure> procList = procExec.listProcedures();
+    final List<Procedure<?>> procList = procExec.listProcedures();
 
     AccessTestAction listProceduresAction = new AccessTestAction() {
       @Override
       public Object run() throws Exception {
-        List<ProcedureInfo> procInfoList = new ArrayList<>(procList.size());
-        for(Procedure p : procList) {
-          procInfoList.add(ProcedureUtil.convertToProcedureInfo(p));
-        }
         ACCESS_CONTROLLER
-        .postListProcedures(ObserverContext.createAndPrepare(CP_ENV, null), procInfoList);
+        .postListProcedures(ObserverContext.createAndPrepare(CP_ENV, null), procList);
        return null;
       }
     };
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java
deleted file mode 100644
index da7c7c43f6..0000000000
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/shaded/protobuf/TestProtobufUtil.java
+++ /dev/null
@@ -1,151 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.shaded.protobuf;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.fail;
-
-import org.apache.hadoop.hbase.ProcedureInfo;
-import org.apache.hadoop.hbase.ProcedureState;
-import org.apache.hadoop.hbase.procedure2.LockInfo;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos;
-import org.apache.hadoop.hbase.testclassification.SmallTests;
-import org.junit.Test;
-import org.junit.experimental.categories.Category;
-
-@Category(SmallTests.class)
-public class TestProtobufUtil {
-  public TestProtobufUtil() {
-  }
-
-  private static ProcedureInfo createProcedureInfo(long procId)
-  {
-    return new ProcedureInfo(procId, "java.lang.Object", null,
-        ProcedureState.RUNNABLE, -1, null, null, 0, 0, null);
-  }
-
-  private static void assertProcedureInfoEquals(ProcedureInfo expected,
-      ProcedureInfo result)
-  {
-    if (expected == result) {
-      return;
-    } else if (expected == null || result == null) {
-      fail();
-    }
-
-    assertEquals(expected.getProcId(), result.getProcId());
-  }
-
-  private static void assertLockInfoEquals(LockInfo expected, LockInfo result)
-  {
-    assertEquals(expected.getResourceType(), result.getResourceType());
-    assertEquals(expected.getResourceName(), result.getResourceName());
-    assertEquals(expected.getLockType(), result.getLockType());
-    assertProcedureInfoEquals(expected.getExclusiveLockOwnerProcedure(),
-        result.getExclusiveLockOwnerProcedure());
-    assertEquals(expected.getSharedLockCount(), result.getSharedLockCount());
-  }
-
-  private static void assertWaitingProcedureEquals(
-      LockInfo.WaitingProcedure expected, LockInfo.WaitingProcedure result)
-  {
-    assertEquals(expected.getLockType(), result.getLockType());
-    assertProcedureInfoEquals(expected.getProcedure(),
-        result.getProcedure());
-  }
-
-  @Test
-  public void testServerLockInfo() {
-    LockInfo lock = new LockInfo();
-    lock.setResourceType(LockInfo.ResourceType.SERVER);
-    lock.setResourceName("server");
-    lock.setLockType(LockInfo.LockType.SHARED);
-    lock.setSharedLockCount(2);
-
-    LockServiceProtos.LockInfo proto = ProtobufUtil.toProtoLockInfo(lock);
-    LockInfo lock2 = ProtobufUtil.toLockInfo(proto);
-
-    assertLockInfoEquals(lock, lock2);
-  }
-
-  @Test
-  public void testNamespaceLockInfo() {
-    LockInfo lock = new LockInfo();
-    lock.setResourceType(LockInfo.ResourceType.NAMESPACE);
-    lock.setResourceName("ns");
-    lock.setLockType(LockInfo.LockType.EXCLUSIVE);
-    lock.setExclusiveLockOwnerProcedure(createProcedureInfo(2));
-
-    LockServiceProtos.LockInfo proto = ProtobufUtil.toProtoLockInfo(lock);
-    LockInfo lock2 = ProtobufUtil.toLockInfo(proto);
-
-    assertLockInfoEquals(lock, lock2);
-  }
-
-  @Test
-  public void testTableLockInfo() {
-    LockInfo lock = new LockInfo();
-    lock.setResourceType(LockInfo.ResourceType.TABLE);
-    lock.setResourceName("table");
-    lock.setLockType(LockInfo.LockType.SHARED);
-    lock.setSharedLockCount(2);
-
-    LockServiceProtos.LockInfo proto = ProtobufUtil.toProtoLockInfo(lock);
-    LockInfo lock2 = ProtobufUtil.toLockInfo(proto);
-
-    assertLockInfoEquals(lock, lock2);
-  }
-
-  @Test
-  public void testRegionLockInfo() {
-    LockInfo lock = new LockInfo();
-    lock.setResourceType(LockInfo.ResourceType.REGION);
-    lock.setResourceName("region");
-    lock.setLockType(LockInfo.LockType.EXCLUSIVE);
-    lock.setExclusiveLockOwnerProcedure(createProcedureInfo(2));
-
-    LockServiceProtos.LockInfo proto = ProtobufUtil.toProtoLockInfo(lock);
-    LockInfo lock2 = ProtobufUtil.toLockInfo(proto);
-
-    assertLockInfoEquals(lock, lock2);
-  }
-
-  @Test
-  public void testExclusiveWaitingLockInfo() {
-    LockInfo.WaitingProcedure waitingProcedure = new LockInfo.WaitingProcedure();
-    waitingProcedure.setLockType(LockInfo.LockType.EXCLUSIVE);
-    waitingProcedure.setProcedure(createProcedureInfo(1));
-
-    LockServiceProtos.WaitingProcedure proto = ProtobufUtil.toProtoWaitingProcedure(waitingProcedure);
-    LockInfo.WaitingProcedure waitingProcedure2 = ProtobufUtil.toWaitingProcedure(proto);
-
-    assertWaitingProcedureEquals(waitingProcedure, waitingProcedure2);
-  }
-
-  @Test
-  public void testSharedWaitingLockInfo() {
-    LockInfo.WaitingProcedure waitingProcedure = new LockInfo.WaitingProcedure();
-    waitingProcedure.setLockType(LockInfo.LockType.SHARED);
-    waitingProcedure.setProcedure(createProcedureInfo(2));
-
-    LockServiceProtos.WaitingProcedure proto = ProtobufUtil.toProtoWaitingProcedure(waitingProcedure);
-    LockInfo.WaitingProcedure waitingProcedure2 = ProtobufUtil.toWaitingProcedure(proto);
-
-    assertWaitingProcedureEquals(waitingProcedure, waitingProcedure2);
-  }
-}
diff --git a/hbase-shell/src/main/ruby/shell/commands/list_locks.rb b/hbase-shell/src/main/ruby/shell/commands/list_locks.rb
index a7f7b7344a..3b0d7100af 100644
--- a/hbase-shell/src/main/ruby/shell/commands/list_locks.rb
+++ b/hbase-shell/src/main/ruby/shell/commands/list_locks.rb
@@ -35,18 +35,17 @@ EOF
           formatter.output_strln("#{lock.resourceType}(#{lock.resourceName})")
 
           case lock.lockType
-          when org.apache.hadoop.hbase.procedure2.LockInfo::LockType::EXCLUSIVE then
-            formatter.output_strln("Lock type: EXCLUSIVE, procedure: #{lock.exclusiveLockOwnerProcedure.procId}")
-          when org.apache.hadoop.hbase.procedure2.LockInfo::LockType::SHARED then
+          when org.apache.hadoop.hbase.procedure2.LockType::EXCLUSIVE then
+            formatter.output_strln("Lock type: EXCLUSIVE, procedure: #{lock.exclusiveLockOwnerProcedure.describe()}")
+          when org.apache.hadoop.hbase.procedure2.LockType::SHARED then
             formatter.output_strln("Lock type: SHARED, count: #{lock.sharedLockCount}")
           end
 
           if lock.waitingProcedures.any?
-            formatter.output_strln('Waiting procedures:')
-            formatter.header(['Lock type', 'Procedure Id'])
+            formatter.header(['Waiting procedures'])
 
             lock.waitingProcedures.each do |waitingProcedure|
-              formatter.row([waitingProcedure.lockType.to_s, waitingProcedure.procedure.procId.to_s])
+              formatter.row([waitingProcedure.describe()])
             end
 
             formatter.footer(lock.waitingProcedures.size)
diff --git a/hbase-shell/src/main/ruby/shell/commands/list_procedures.rb b/hbase-shell/src/main/ruby/shell/commands/list_procedures.rb
index a2bec3750b..641e4fb67d 100644
--- a/hbase-shell/src/main/ruby/shell/commands/list_procedures.rb
+++ b/hbase-shell/src/main/ruby/shell/commands/list_procedures.rb
@@ -29,13 +29,14 @@ EOF
       end
 
       def command
-        formatter.header(%w[Id Name State Submitted_Time Last_Update])
+        formatter.header(%w[Id Name State Submitted_Time Last_Update Parameters])
 
         list = admin.list_procedures
         list.each do |proc|
           submitted_time = Time.at(proc.getSubmittedTime / 1000).to_s
           last_update = Time.at(proc.getLastUpdate / 1000).to_s
-          formatter.row([proc.getProcId, proc.getProcName, proc.getProcState, submitted_time, last_update])
+          formatter.row([proc.getProcId, proc.getProcName, proc.getProcState,
+              submitted_time, last_update, proc.getProcStateData])
         end
 
         formatter.footer(list.size)
diff --git a/hbase-shell/src/test/ruby/shell/list_locks_test.rb b/hbase-shell/src/test/ruby/shell/list_locks_test.rb
index fe132dbb35..7c63961be0 100644
--- a/hbase-shell/src/test/ruby/shell/list_locks_test.rb
+++ b/hbase-shell/src/test/ruby/shell/list_locks_test.rb
@@ -31,6 +31,8 @@ class ListLocksTest < Test::Unit::TestCase
 
     @list_locks = Shell::Commands::ListLocks.new(@shell)
     @list_locks.set_formatter(Shell::Formatter::Base.new({ :output_stream => @string_io }))
+
+    @epoch = java.util.Date.new(0).to_s
   end
 
   def set_field(object, field_name, value)
@@ -39,20 +41,28 @@ class ListLocksTest < Test::Unit::TestCase
     field.set(object, value)
   end
 
-  def create_lock(type, proc_id)
+  def create_lock(type, opType, proc_id)
     lock = org.apache.hadoop.hbase.master.locking.LockProcedure.new()
     set_field(lock, "type", type)
+    set_field(lock, "opType", opType)
+    set_field(lock, "description", "description")
     lock.procId = proc_id
+    lock.submittedTime = 0
+    lock.lastUpdate = 0
 
     return lock
   end
 
   def create_exclusive_lock(proc_id)
-    return create_lock(org.apache.hadoop.hbase.master.locking.LockProcedure::LockType::EXCLUSIVE, proc_id)
+    return create_lock(org.apache.hadoop.hbase.procedure2.LockType::EXCLUSIVE,
+      org.apache.hadoop.hbase.master.procedure.TableProcedureInterface::TableOperationType::EDIT,
+      proc_id)
   end
 
   def create_shared_lock(proc_id)
-    return create_lock(org.apache.hadoop.hbase.master.locking.LockProcedure::LockType::SHARED, proc_id)
+    return create_lock(org.apache.hadoop.hbase.procedure2.LockType::SHARED,
+      org.apache.hadoop.hbase.master.procedure.TableProcedureInterface::TableOperationType::READ,
+      proc_id)
   end
 
   define_test "list server locks" do
@@ -66,7 +76,11 @@ class ListLocksTest < Test::Unit::TestCase
 
     assert_equal(
       "SERVER(server1,1234,0)\n" <<
-      "Lock type: EXCLUSIVE, procedure: 0\n\n",
+      "Lock type: EXCLUSIVE, procedure: { ID => '0', PARENT_ID => '-1', " <<
+        "STATE => 'RUNNABLE', OWNER => '', TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'EXCLUSIVE', description => 'description' } ]' }\n\n",
       @string_io.string)
   end
 
@@ -79,7 +93,11 @@ class ListLocksTest < Test::Unit::TestCase
 
     assert_equal(
       "NAMESPACE(ns1)\n" <<
-      "Lock type: EXCLUSIVE, procedure: 1\n\n" <<
+      "Lock type: EXCLUSIVE, procedure: { ID => '1', PARENT_ID => '-1', " <<
+        "STATE => 'RUNNABLE', OWNER => '', TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'EXCLUSIVE', description => 'description' } ]' }\n\n" <<
       "TABLE(hbase:namespace)\n" <<
       "Lock type: SHARED, count: 1\n\n",
       @string_io.string)
@@ -98,7 +116,11 @@ class ListLocksTest < Test::Unit::TestCase
       "NAMESPACE(ns2)\n" <<
       "Lock type: SHARED, count: 1\n\n" <<
       "TABLE(ns2:table2)\n" <<
-      "Lock type: EXCLUSIVE, procedure: 2\n\n",
+      "Lock type: EXCLUSIVE, procedure: { ID => '2', PARENT_ID => '-1', " <<
+        "STATE => 'RUNNABLE', OWNER => '', TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'EXCLUSIVE', description => 'description' } ]' }\n\n",
       @string_io.string)
   end
 
@@ -118,7 +140,11 @@ class ListLocksTest < Test::Unit::TestCase
       "TABLE(ns3:table3)\n" <<
       "Lock type: SHARED, count: 1\n\n" <<
       "REGION(" << region_info.getEncodedName << ")\n" <<
-      "Lock type: EXCLUSIVE, procedure: 3\n\n",
+      "Lock type: EXCLUSIVE, procedure: { ID => '3', PARENT_ID => '-1', " <<
+        "STATE => 'RUNNABLE', OWNER => '', TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'EXCLUSIVE', description => 'description' } ]' }\n\n",
       @string_io.string)
   end
 
@@ -141,10 +167,19 @@ class ListLocksTest < Test::Unit::TestCase
       "NAMESPACE(ns4)\n" <<
       "Lock type: SHARED, count: 1\n\n" <<
       "TABLE(ns4:table4)\n" <<
-      "Lock type: EXCLUSIVE, procedure: 1\n" <<
-      "Waiting procedures:\n" <<
-      "Lock type  Procedure Id\n" <<
-      " SHARED 2\n" <<
+      "Lock type: EXCLUSIVE, procedure: { ID => '1', PARENT_ID => '-1', " <<
+        "STATE => 'RUNNABLE', OWNER => '', TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'EXCLUSIVE', tableName => { namespace => 'bnM0', qualifier => 'dGFibGU0' }, " <<
+        "description => 'description' } ]' }\n" <<
+      "Waiting procedures\n" <<
+      "{ ID => '2', PARENT_ID => '-1', STATE => 'RUNNABLE', OWNER => '', " <<
+        "TYPE => 'org.apache.hadoop.hbase.master.locking.LockProcedure', " <<
+        "START_TIME => '" << @epoch << "', LAST_UPDATE => '" << @epoch << "', " <<
+        "PARAMETERS => '[ { @type => 'type.googleapis.com/hbase.pb.LockProcedureData', " <<
+        "lockType => 'SHARED', tableName => { namespace => 'bnM0', qualifier => 'dGFibGU0' }, " <<
+        "description => 'description' } ]' }\n" <<
       "1 row(s)\n\n",
       @string_io.string)
   end
-- 
2.11.0 (Apple Git-81)
