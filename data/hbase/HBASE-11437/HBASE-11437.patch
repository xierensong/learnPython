diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index c58d1e5..2c5abf1 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -1089,9 +1089,9 @@ public final class ProtobufUtil {
               kv.getQualifierArray(), kv.getQualifierOffset(), kv.getQualifierLength()));
           valueBuilder.setValue(HBaseZeroCopyByteString.wrap(
               kv.getValueArray(), kv.getValueOffset(), kv.getValueLength()));
-          if (kv.getTagsLength() > 0) {
+          if (kv.getTagsLengthUnsigned() > 0) {
             valueBuilder.setTags(HBaseZeroCopyByteString.wrap(kv.getTagsArray(),
-                kv.getTagsOffset(), kv.getTagsLength()));
+                kv.getTagsOffset(), kv.getTagsLengthUnsigned()));
           }
           columnBuilder.addQualifierValue(valueBuilder.build());
         }
@@ -1152,9 +1152,9 @@ public final class ProtobufUtil {
         valueBuilder.setValue(HBaseZeroCopyByteString.wrap(
             kv.getValueArray(), kv.getValueOffset(), kv.getValueLength()));
         valueBuilder.setTimestamp(kv.getTimestamp());
-        if(cell.getTagsLength() > 0) {
+        if(cell.getTagsLengthUnsigned() > 0) {
           valueBuilder.setTags(HBaseZeroCopyByteString.wrap(kv.getTagsArray(), kv.getTagsOffset(),
-              kv.getTagsLength()));
+              kv.getTagsLengthUnsigned()));
         }
         if (type == MutationType.DELETE) {
           KeyValue.Type keyValueType = KeyValue.Type.codeToType(kv.getType());
diff --git a/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestPayloadCarryingRpcController.java b/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestPayloadCarryingRpcController.java
index 088e609..affcdef 100644
--- a/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestPayloadCarryingRpcController.java
+++ b/hbase-client/src/test/java/org/apache/hadoop/hbase/ipc/TestPayloadCarryingRpcController.java
@@ -171,6 +171,12 @@ public class TestPayloadCarryingRpcController {
               }
 
               @Override
+              public int getTagsLengthUnsigned() {
+                // unused
+                return 0;
+              }
+
+              @Override
               public short getTagsLength() {
                 // unused
                 return 0;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/Cell.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/Cell.java
index f7e9272..ca60941 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/Cell.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/Cell.java
@@ -190,11 +190,19 @@ public interface Cell {
    * @return the first offset where the tags start in the Cell
    */
   int getTagsOffset();
-  
+
   /**
    * @return the total length of the tags in the Cell.
+   * @deprecated Instead use {@link #getTagsLengthUnsigned()}
    */
+  @Deprecated
   short getTagsLength();
+
+  /**
+   * @return the total length of the tags in the Cell.
+   * TODO write about the new method significance
+   */
+  int getTagsLengthUnsigned();
   
   /**
    * WARNING do not use, expensive.  This gets an arraycopy of the cell's value.
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparator.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparator.java
index edf1e52..f0af696 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparator.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellComparator.java
@@ -85,7 +85,7 @@ public class CellComparator implements Comparator<Cell>, Serializable{
    * @return Long.MAX_VALUE if there is no LOG_REPLAY_TAG
    */
   private static long getReplaySeqNum(final Cell c) {
-    Tag tag = Tag.getTag(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength(),
+    Tag tag = Tag.getTag(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned(),
         TagType.LOG_REPLAY_TAG_TYPE);
 
     if (tag != null) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
index d6564c2..3210f8c 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
@@ -54,7 +54,7 @@ public final class CellUtil {
   }
 
   public static ByteRange fillTagRange(Cell cell, ByteRange range) {
-    return range.set(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength());
+    return range.set(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLengthUnsigned());
   }
 
   /***************** get individual arrays for tests ************/
@@ -91,7 +91,7 @@ public final class CellUtil {
    * @return tag value in a new byte array.
    */
   public static byte[] getTagArray(Cell cell){
-    byte[] output = new byte[cell.getTagsLength()];
+    byte[] output = new byte[cell.getTagsLengthUnsigned()];
     copyTagTo(cell, output, 0);
     return output;
   }
@@ -132,8 +132,8 @@ public final class CellUtil {
    */
   public static int copyTagTo(Cell cell, byte[] destination, int destinationOffset) {
     System.arraycopy(cell.getTagsArray(), cell.getTagsOffset(), destination, destinationOffset,
-        cell.getTagsLength());
-    return destinationOffset + cell.getTagsLength();
+        cell.getTagsLengthUnsigned());
+    return destinationOffset + cell.getTagsLengthUnsigned();
   }
 
   /********************* misc *************************************/
@@ -489,8 +489,8 @@ public final class CellUtil {
       @Override
       public Tag next() {
         if (hasNext()) {
-          short curTagLen = Bytes.toShort(tags, this.pos);
-          Tag tag = new Tag(tags, pos, (short) (curTagLen + Bytes.SIZEOF_SHORT));
+          int curTagLen = Bytes.readAsInt(tags, this.pos, Tag.TAG_LENGTH_SIZE);
+          Tag tag = new Tag(tags, pos, curTagLen + Tag.TAG_LENGTH_SIZE);
           this.pos += Bytes.SIZEOF_SHORT + curTagLen;
           return tag;
         }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index 002642a..c2c2d49 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -143,6 +143,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
 
   public static final int KEYVALUE_WITH_TAGS_INFRASTRUCTURE_SIZE = ROW_OFFSET + TAGS_LENGTH_SIZE;
 
+  private static final int MAX_TAGS_LENGTH = (2 * Short.MAX_VALUE) + 1;
 
   /**
    * Computes the number of bytes that a <code>KeyValue</code> instance with the provided
@@ -743,7 +744,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
         c.getFamilyArray(), c.getFamilyOffset(), (int)c.getFamilyLength(), 
         c.getQualifierArray(), c.getQualifierOffset(), (int) c.getQualifierLength(), 
         c.getTimestamp(), Type.codeToType(c.getTypeByte()), c.getValueArray(), c.getValueOffset(), 
-        c.getValueLength(), c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+        c.getValueLength(), c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
   }
 
   /**
@@ -798,7 +799,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     pos = Bytes.putByte(bytes, pos, type.getCode());
     pos += vlength;
     if (tagsLength > 0) {
-      pos = Bytes.putShort(bytes, pos, (short)(tagsLength & 0x0000ffff));
+      pos = Bytes.putAsShort(bytes, pos, tagsLength);
     }
     return bytes;
   }
@@ -916,7 +917,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     }
     // Write the number of tags. If it is 0 then it means there are no tags.
     if (tagsLength > 0) {
-      pos = Bytes.putShort(buffer, pos, (short) tagsLength);
+      pos = Bytes.putAsShort(buffer, pos, tagsLength);
       for (Tag t : tags) {
         pos = Bytes.putBytes(buffer, pos, t.getBuffer(), t.getOffset(), t.getLength());
       }
@@ -925,8 +926,8 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
   }
 
   private static void checkForTagsLength(int tagsLength) {
-    if (tagsLength > Short.MAX_VALUE) {
-      throw new IllegalArgumentException("tagslength "+ tagsLength + " > " + Short.MAX_VALUE);
+    if (tagsLength > MAX_TAGS_LENGTH) {
+      throw new IllegalArgumentException("tagslength "+ tagsLength + " > " + MAX_TAGS_LENGTH);
     }
   }
 
@@ -981,7 +982,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     }
     // Add the tags after the value part
     if (tagsLength > 0) {
-      pos = Bytes.putShort(bytes, pos, (short) (tagsLength));
+      pos = Bytes.putAsShort(bytes, pos, tagsLength);
       pos = Bytes.putBytes(bytes, pos, tags, tagsOffset, tagsLength);
     }
     return bytes;
@@ -1041,7 +1042,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     }
     // Add the tags after the value part
     if (tagsLength > 0) {
-      pos = Bytes.putShort(bytes, pos, (short) (tagsLength));
+      pos = Bytes.putAsShort(bytes, pos, tagsLength);
       for (Tag t : tags) {
         pos = Bytes.putBytes(bytes, pos, t.getBuffer(), t.getOffset(), t.getLength());
       }
@@ -1554,7 +1555,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
    */
   @Override
   public int getTagsOffset() {
-    short tagsLen = getTagsLength();
+    int tagsLen = getTagsLengthUnsigned();
     if (tagsLen == 0) {
       return this.offset + this.length;
     }
@@ -1565,14 +1566,20 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
    * This returns the total length of the tag bytes
    */
   @Override
-  public short getTagsLength() {
+  public int getTagsLengthUnsigned() {
     int tagsLen = this.length - (getKeyLength() + getValueLength() + KEYVALUE_INFRASTRUCTURE_SIZE);
     if (tagsLen > 0) {
       // There are some Tag bytes in the byte[]. So reduce 2 bytes which is added to denote the tags
       // length
       tagsLen -= TAGS_LENGTH_SIZE;
     }
-    return (short) tagsLen;
+    return tagsLen;
+  }
+
+  @Override
+  @Deprecated
+  public short getTagsLength() {
+    return (short) getTagsLengthUnsigned();
   }
 
   /**
@@ -1580,7 +1587,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
    * @return The tags
    */
   public List<Tag> getTags() {
-    short tagsLength = getTagsLength();
+    int tagsLength = getTagsLengthUnsigned();
     if (tagsLength == 0) {
       return EMPTY_ARRAY_LIST;
     }
@@ -2361,8 +2368,8 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
    */
   public static KeyValue cloneAndAddTags(Cell c, List<Tag> newTags) {
     List<Tag> existingTags = null;
-    if(c.getTagsLength() > 0) {
-      existingTags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    if(c.getTagsLengthUnsigned() > 0) {
+      existingTags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
       existingTags.addAll(newTags);
     } else {
       existingTags = newTags;
@@ -2690,7 +2697,7 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
 
     @Override
     public int getTagsOffset() {
-      return (short) 0;
+      return 0;
     }
 
     @Override
@@ -2709,8 +2716,13 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     }
 
     @Override
+    public int getTagsLengthUnsigned() {
+      return 0;
+    }
+
+    @Override
     public short getTagsLength() {
-      return (short) 0;
+      return 0;
     }
 
     @Override
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
index c2a8826..7084c08 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtil.java
@@ -44,7 +44,7 @@ public class KeyValueUtil {
   public static int length(final Cell cell) {
     return (int) (KeyValue.getKeyValueDataStructureSize(cell.getRowLength(),
         cell.getFamilyLength(), cell.getQualifierLength(), cell.getValueLength(),
-        cell.getTagsLength()));
+        cell.getTagsLengthUnsigned()));
   }
 
   protected static int keyLength(final Cell cell) {
@@ -116,8 +116,8 @@ public class KeyValueUtil {
     pos = Bytes.putInt(output, pos, cell.getValueLength());
     pos = appendKeyToByteArrayWithoutValue(cell, output, pos);
     pos = CellUtil.copyValueTo(cell, output, pos);
-    if ((cell.getTagsLength() > 0)) {
-      pos = Bytes.putShort(output, pos, cell.getTagsLength());
+    if ((cell.getTagsLengthUnsigned() > 0)) {
+      pos = Bytes.putAsShort(output, pos, cell.getTagsLengthUnsigned());
       pos = CellUtil.copyTagTo(cell, output, pos);
     }
     return pos;
@@ -166,9 +166,10 @@ public class KeyValueUtil {
     int keyLength = bb.getInt();
     int valueLength = bb.getInt();
     ByteBufferUtils.skip(bb, keyLength + valueLength);
-    short tagsLength = 0;
+    int tagsLength = 0;
     if (includesTags) {
-      tagsLength = bb.getShort();
+      // Read short as unsigned, high byte first
+      tagsLength = ((bb.get() & 0xff) << 8) ^ (bb.get() & 0xff);
       ByteBufferUtils.skip(bb, tagsLength);
     }
     int kvLength = (int) KeyValue.getKeyValueDataStructureSize(keyLength, valueLength, tagsLength);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
index 332433f..b7c7e7a 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/Tag.java
@@ -35,11 +35,12 @@ public class Tag {
   public final static int TYPE_LENGTH_SIZE = Bytes.SIZEOF_BYTE;
   public final static int TAG_LENGTH_SIZE = Bytes.SIZEOF_SHORT;
   public final static int INFRASTRUCTURE_SIZE = TYPE_LENGTH_SIZE + TAG_LENGTH_SIZE;
+  private static final int MAX_TAG_LENGTH = (2 * Short.MAX_VALUE) + 1 - TAG_LENGTH_SIZE;
 
   private final byte type;
   private final byte[] bytes;
   private int offset = 0;
-  private short length = 0;
+  private int length = 0;
 
   // The special tag will write the length of each tag and that will be
   // followed by the type and then the actual tag.
@@ -54,13 +55,19 @@ public class Tag {
    * @param tag
    */
   public Tag(byte tagType, byte[] tag) {
-    /** <length of tag - 2 bytes><type code - 1 byte><tag>
-     * taglength maximum is Short.MAX_SIZE.  It includes 1 byte type length and actual tag bytes length.
+    /**
+     * Format for a tag : <length of tag - 2 bytes><type code - 1 byte><tag> taglength is serialized
+     * using 2 bytes only but as this will be unsigned, we can have max taglength of
+     * (Short.MAX_SIZE * 2) +1. It includes 1 byte type length and actual tag bytes length.
      */
-    short tagLength = (short) ((tag.length & 0x0000ffff) + TYPE_LENGTH_SIZE);
-    length = (short) (TAG_LENGTH_SIZE + tagLength);
+    int tagLength = tag.length + TYPE_LENGTH_SIZE;
+    if (tagLength > MAX_TAG_LENGTH) {
+      throw new IllegalArgumentException(
+          "Invalid tag data being passed. Its length can not exceed " + MAX_TAG_LENGTH);
+    }
+    length = TAG_LENGTH_SIZE + tagLength;
     bytes = new byte[length];
-    int pos = Bytes.putShort(bytes, 0, tagLength);
+    int pos = Bytes.putAsShort(bytes, 0, tagLength);
     pos = Bytes.putByte(bytes, pos, tagType);
     Bytes.putBytes(bytes, pos, tag, 0, tag.length);
     this.type = tagType;
@@ -80,8 +87,8 @@ public class Tag {
     this(bytes, offset, getLength(bytes, offset));
   }
 
-  private static short getLength(byte[] bytes, int offset) {
-    return (short) (TAG_LENGTH_SIZE + Bytes.toShort(bytes, offset));
+  private static int getLength(byte[] bytes, int offset) {
+    return TAG_LENGTH_SIZE + Bytes.readAsInt(bytes, offset, TAG_LENGTH_SIZE);
   }
 
   /**
@@ -94,8 +101,30 @@ public class Tag {
    *          offset to start of the Tag
    * @param length
    *          length of the Tag
+   * @deprecated Use {@link #Tag(byte[], int, int)}
    */
+  @Deprecated
   public Tag(byte[] bytes, int offset, short length) {
+    this(bytes, offset, (int) length);
+  }
+
+  /**
+   * Creates a Tag from the specified byte array, starting at offset, and for length
+   * <code>length</code>. Presumes <code>bytes</code> content starting at <code>offset</code> is
+   * formatted as a Tag blob.
+   * 
+   * @param bytes
+   *          byte array
+   * @param offset
+   *          offset to start of the Tag
+   * @param length
+   *          length of the Tag
+   */
+  public Tag(byte[] bytes, int offset, int length) {
+    if (length > MAX_TAG_LENGTH) {
+      throw new IllegalArgumentException(
+          "Invalid tag data being passed. Its length can not exceed " + MAX_TAG_LENGTH);
+    }
     this.bytes = bytes;
     this.offset = offset;
     this.length = length;
@@ -156,8 +185,8 @@ public class Tag {
     List<Tag> tags = new ArrayList<Tag>();
     int pos = offset;
     while (pos < offset + length) {
-      short tagLen = Bytes.toShort(b, pos);
-      tags.add(new Tag(b, pos, (short) (tagLen + TAG_LENGTH_SIZE)));
+      int tagLen = Bytes.readAsInt(b, pos, TAG_LENGTH_SIZE);
+      tags.add(new Tag(b, pos, tagLen + TAG_LENGTH_SIZE));
       pos += TAG_LENGTH_SIZE + tagLen;
     }
     return tags;
@@ -174,9 +203,9 @@ public class Tag {
   public static Tag getTag(byte[] b, int offset, int length, byte type) {
     int pos = offset;
     while (pos < offset + length) {
-      short tagLen = Bytes.toShort(b, pos);
+      int tagLen = Bytes.readAsInt(b, pos, TAG_LENGTH_SIZE);
       if(b[pos + TAG_LENGTH_SIZE] == type) {
-        return new Tag(b, pos, (short) (tagLen + TAG_LENGTH_SIZE));
+        return new Tag(b, pos, tagLen + TAG_LENGTH_SIZE);
       }
       pos += TAG_LENGTH_SIZE + tagLen;
     }
@@ -186,7 +215,7 @@ public class Tag {
   /**
    * Returns the total length of the entire tag entity
    */
-  short getLength() {
+  int getLength() {
     return this.length;
   }
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodecWithTags.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodecWithTags.java
index 809a65f..84f251f 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodecWithTags.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/codec/CellCodecWithTags.java
@@ -55,7 +55,7 @@ public class CellCodecWithTags implements Codec {
       // Value
       write(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength());
       // Tags
-      write(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength());
+      write(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLengthUnsigned());
       // MvccVersion
       this.out.write(Bytes.toBytes(cell.getMvccVersion()));
     }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java
index 7e7fe1d..ff2cdb3 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/TagCompressionContext.java
@@ -61,13 +61,13 @@ public class TagCompressionContext {
    * @param length Length of all tag bytes
    * @throws IOException
    */
-  public void compressTags(OutputStream out, byte[] in, int offset, short length)
+  public void compressTags(OutputStream out, byte[] in, int offset, int length)
       throws IOException {
     int pos = offset;
     int endOffset = pos + length;
     assert pos < endOffset;
     while (pos < endOffset) {
-      short tagLen = Bytes.toShort(in, pos);
+      int tagLen = Bytes.readAsInt(in, pos, Tag.TAG_LENGTH_SIZE);
       pos += Tag.TAG_LENGTH_SIZE;
       write(in, pos, tagLen, out);
       pos += tagLen;
@@ -81,7 +81,7 @@ public class TagCompressionContext {
    * @param length Length of all tag bytes
    * @throws IOException
    */
-  public void compressTags(OutputStream out, ByteBuffer in, short length) throws IOException {
+  public void compressTags(OutputStream out, ByteBuffer in, int length) throws IOException {
     if (in.hasArray()) {
       compressTags(out, in.array(), in.arrayOffset() + in.position(), length);
       ByteBufferUtils.skip(in, length);
@@ -100,15 +100,14 @@ public class TagCompressionContext {
    * @param length Length of all tag bytes
    * @throws IOException
    */
-  public void uncompressTags(InputStream src, byte[] dest, int offset, short length)
+  public void uncompressTags(InputStream src, byte[] dest, int offset, int length)
       throws IOException {
     int endOffset = offset + length;
     while (offset < endOffset) {
       byte status = (byte) src.read();
       if (status == Dictionary.NOT_IN_DICTIONARY) {
-        // We are writing short as tagLen. So can downcast this without any risk.
-        short tagLen = (short) StreamUtils.readRawVarint32(src);
-        offset = Bytes.putShort(dest, offset, tagLen);
+        int tagLen = StreamUtils.readRawVarint32(src);
+        offset = Bytes.putAsShort(dest, offset, tagLen);
         IOUtils.readFully(src, dest, offset, tagLen);
         tagDict.addEntry(dest, offset, tagLen);
         offset += tagLen;
@@ -118,7 +117,7 @@ public class TagCompressionContext {
         if (entry == null) {
           throw new IOException("Missing dictionary entry for index " + dictIdx);
         }
-        offset = Bytes.putShort(dest, offset, (short) entry.length);
+        offset = Bytes.putAsShort(dest, offset, entry.length);
         System.arraycopy(entry, 0, dest, offset, entry.length);
         offset += entry.length;
       }
@@ -140,11 +139,10 @@ public class TagCompressionContext {
     int endOffset = offset + length;
     while (offset < endOffset) {
       byte status = src.get();
-      short tagLen;
+      int tagLen;
       if (status == Dictionary.NOT_IN_DICTIONARY) {
-        // We are writing short as tagLen. So can downcast this without any risk.
-        tagLen = (short) StreamUtils.readRawVarint32(src);
-        offset = Bytes.putShort(dest, offset, tagLen);
+        tagLen = StreamUtils.readRawVarint32(src);
+        offset = Bytes.putAsShort(dest, offset, tagLen);
         src.get(dest, offset, tagLen);
         tagDict.addEntry(dest, offset, tagLen);
         offset += tagLen;
@@ -154,8 +152,8 @@ public class TagCompressionContext {
         if (entry == null) {
           throw new IOException("Missing dictionary entry for index " + dictIdx);
         }
-        tagLen = (short) entry.length;
-        offset = Bytes.putShort(dest, offset, tagLen);
+        tagLen = entry.length;
+        offset = Bytes.putAsShort(dest, offset, tagLen);
         System.arraycopy(entry, 0, dest, offset, tagLen);
         offset += tagLen;
       }
@@ -170,7 +168,7 @@ public class TagCompressionContext {
    * @param length Length of all tag bytes
    * @throws IOException
    */
-  public void uncompressTags(InputStream src, ByteBuffer dest, short length) throws IOException {
+  public void uncompressTags(InputStream src, ByteBuffer dest, int length) throws IOException {
     if (dest.hasArray()) {
       uncompressTags(src, dest.array(), dest.arrayOffset() + dest.position(), length);
     } else {
@@ -180,7 +178,7 @@ public class TagCompressionContext {
     }
   }
 
-  private void write(byte[] data, int offset, short length, OutputStream out) throws IOException {
+  private void write(byte[] data, int offset, int length, OutputStream out) throws IOException {
     short dictIdx = Dictionary.NOT_IN_DICTIONARY;
     if (tagDict != null) {
       dictIdx = tagDict.findEntry(data, offset, length);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index b1b384a..8f93d00 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -268,6 +268,11 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     }
 
     @Override
+    public int getTagsLengthUnsigned() {
+      return tagsLength;
+    }
+
+    @Override
     public short getTagsLength() {
       return (short) tagsLength;
     }
@@ -312,7 +317,6 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
           currentKey.getTimestamp(), currentKey.getTypeByte(), valueLength, valueOffset,
           memstoreTS, tagsOffset, tagsLength, tagCompressionContext, tagsBuffer);
     }
-          
   }
 
   /**
@@ -463,6 +467,11 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     }
 
     @Override
+    public int getTagsLengthUnsigned() {
+      return tagsLength;
+    }
+
+    @Override
     public short getTagsLength() {
       return (short) tagsLength;
     }
@@ -585,7 +594,9 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
           currentBuffer.arrayOffset() + current.valueOffset,
           current.valueLength);
       if (current.tagsLength > 0) {
-        kvBuffer.putShort((short) current.tagsLength);
+        // Put short as unsigned
+        kvBuffer.put((byte) (current.tagsLength >> 8 & 0xff));
+        kvBuffer.put((byte) (current.tagsLength & 0xff));
         if (current.tagsOffset != -1) {
           // the offset of the tags bytes in the underlying buffer is marked. So the temp
           // buffer,tagsBuffer was not been used.
@@ -833,7 +844,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
       HFileBlockDefaultEncodingContext encodingCtx) throws IOException {
     int size = 0;
     if (encodingCtx.getHFileContext().isIncludesTags()) {
-      short tagsLength = kv.getTagsLength();
+      int tagsLength = kv.getTagsLengthUnsigned();
       ByteBufferUtils.putCompressedInt(out, tagsLength);
       // There are some tags to be written
       if (tagsLength > 0) {
@@ -863,8 +874,10 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   protected final void afterDecodingKeyValue(DataInputStream source,
       ByteBuffer dest, HFileBlockDefaultDecodingContext decodingCtx) throws IOException {
     if (decodingCtx.getHFileContext().isIncludesTags()) {
-      short tagsLength = (short) ByteBufferUtils.readCompressedInt(source);
-      dest.putShort(tagsLength);
+      int tagsLength = ByteBufferUtils.readCompressedInt(source);
+      // Put as unsigned short
+      dest.put((byte) ((tagsLength >> 8) & 0xff));
+      dest.put((byte) (tagsLength & 0xff));
       if (tagsLength > 0) {
         TagCompressionContext tagCompressionContext = decodingCtx.getTagCompressionContext();
         // When tag compression is been used in this file, tagCompressionContext will have a not
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
index c1c9956..bace24b 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
@@ -48,7 +48,7 @@ public class CopyKeyDataBlockEncoder extends BufferedDataBlockEncoder {
     int size = klength + vlength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
     // Write the additional tag into the stream
     if (encodingContext.getHFileContext().isIncludesTags()) {
-      short tagsLength = kv.getTagsLength();
+      int tagsLength = kv.getTagsLengthUnsigned();
       out.writeShort(tagsLength);
       if (tagsLength > 0) {
         out.write(kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
@@ -88,7 +88,8 @@ public class CopyKeyDataBlockEncoder extends BufferedDataBlockEncoder {
         current.valueOffset = currentBuffer.position();
         ByteBufferUtils.skip(currentBuffer, current.valueLength);
         if (includesTags()) {
-          current.tagsLength = currentBuffer.getShort();
+          // Read short as unsigned, high byte first
+          current.tagsLength = ((currentBuffer.get() & 0xff) << 8) ^ (currentBuffer.get() & 0xff);
           ByteBufferUtils.skip(currentBuffer, current.tagsLength);
         }
         if (includesMvcc()) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
index d71d1a4..93d4bbf 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
@@ -114,11 +114,11 @@ public class EncodedDataBlock {
         int offset = decompressedData.position();
         int klen = decompressedData.getInt();
         int vlen = decompressedData.getInt();
-        short tagsLen = 0;
+        int tagsLen = 0;
         ByteBufferUtils.skip(decompressedData, klen + vlen);
         // Read the tag length in case when steam contain tags
         if (meta.isIncludesTags()) {
-          tagsLen = decompressedData.getShort();
+          tagsLen = ((decompressedData.get() & 0xff) << 8) ^ (decompressedData.get() & 0xff);
           ByteBufferUtils.skip(decompressedData, tagsLen);
         }
         KeyValue kv = new KeyValue(decompressedData.array(), offset,
@@ -227,7 +227,7 @@ public class EncodedDataBlock {
       ByteBuffer in = getUncompressedBuffer();
       in.rewind();
       int klength, vlength;
-      short tagsLength = 0;
+      int tagsLength = 0;
       long memstoreTS = 0L;
       KeyValue kv = null;
       while (in.hasRemaining()) {
@@ -236,7 +236,7 @@ public class EncodedDataBlock {
         vlength = in.getInt();
         ByteBufferUtils.skip(in, klength + vlength);
         if (this.meta.isIncludesTags()) {
-          tagsLength = in.getShort();
+          tagsLength = ((in.get() & 0xff) << 8) ^ (in.get() & 0xff);
           ByteBufferUtils.skip(in, tagsLength);
         }
         if (this.meta.isIncludesMvcc()) {
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
index deae034..12465b2 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
@@ -759,6 +759,28 @@ public class Bytes {
   }
 
   /**
+   * Converts a byte array to an int value
+   * @param bytes byte array
+   * @param offset offset into array
+   * @param length how many bytes should be considered for creating int
+   * @return the int value
+   * @throws IllegalArgumentException if there's not enough room in the array at the offset
+   * indicated.
+   */
+  public static int readAsInt(byte[] bytes, int offset, final int length) {
+    if (offset + length > bytes.length) {
+      throw new IllegalArgumentException("offset (" + offset + ") + length (" + length
+          + ") exceed the" + " capacity of the array: " + bytes.length);
+    }
+    int n = 0;
+    for(int i = offset; i < (offset + length); i++) {
+      n <<= 8;
+      n ^= bytes[i] & 0xFF;
+    }
+    return n;
+  }
+
+  /**
    * Put an int value out to the specified byte array position.
    * @param bytes the byte array
    * @param offset position in the array
@@ -866,6 +888,30 @@ public class Bytes {
   }
 
   /**
+   * Put an int value as short out to the specified byte array position. Only the lower 2 bytes of
+   * the short will be put into the array. The caller of the API need to make sure they will not
+   * loose the value by doing so. This is useful to store an unsigned short which is represented as
+   * int in other parts.
+   * 
+   * @param bytes the byte array
+   * @param offset position in the array
+   * @param val value to write out
+   * @return incremented offset
+   * @throws IllegalArgumentException if the byte array given doesn't have
+   * enough room at the offset specified.
+   */
+  public static int putAsShort(byte[] bytes, int offset, int val) {
+    if (bytes.length - offset < SIZEOF_SHORT) {
+      throw new IllegalArgumentException("Not enough room to put a short at"
+          + " offset " + offset + " in a " + bytes.length + " byte array");
+    }
+    bytes[offset+1] = (byte) val;
+    val >>= 8;
+    bytes[offset] = (byte) val;
+    return offset + SIZEOF_SHORT;
+  }
+
+  /**
    * Convert a BigDecimal value to a byte array
    *
    * @param val
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
index 51794d4..6d1ecda 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/TestKeyValue.java
@@ -538,7 +538,7 @@ public class TestKeyValue extends TestCase {
     byte[] metaValue2 = Bytes.toBytes("metaValue2");
     KeyValue kv = new KeyValue(row, cf, q, HConstants.LATEST_TIMESTAMP, value, new Tag[] {
         new Tag((byte) 1, metaValue1), new Tag((byte) 2, metaValue2) });
-    assertTrue(kv.getTagsLength() > 0);
+    assertTrue(kv.getTagsLengthUnsigned() > 0);
     assertTrue(Bytes.equals(kv.getRow(), row));
     assertTrue(Bytes.equals(kv.getFamily(), cf));
     assertTrue(Bytes.equals(kv.getQualifier(), q));
@@ -561,7 +561,7 @@ public class TestKeyValue extends TestCase {
     assertTrue(meta1Ok);
     assertTrue(meta2Ok);
     Iterator<Tag> tagItr = CellUtil.tagsIterator(kv.getTagsArray(), kv.getTagsOffset(),
-        kv.getTagsLength());
+        kv.getTagsLengthUnsigned());
     //Iterator<Tag> tagItr = kv.tagsIterator();
     assertTrue(tagItr.hasNext());
     Tag next = tagItr.next();
@@ -575,7 +575,8 @@ public class TestKeyValue extends TestCase {
     Bytes.equals(next.getValue(), metaValue2);
     assertFalse(tagItr.hasNext());
 
-    tagItr = CellUtil.tagsIterator(kv.getTagsArray(), kv.getTagsOffset(), kv.getTagsLength());
+    tagItr = CellUtil.tagsIterator(kv.getTagsArray(), kv.getTagsOffset(),
+        kv.getTagsLengthUnsigned());
     assertTrue(tagItr.hasNext());
     next = tagItr.next();
     assertEquals(10, next.getTagLength());
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestCellCodecWithTags.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestCellCodecWithTags.java
index 1499a91..2808cdf 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestCellCodecWithTags.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestCellCodecWithTags.java
@@ -76,7 +76,7 @@ public class TestCellCodecWithTags {
     assertTrue(decoder.advance());
     Cell c = decoder.current();
     assertTrue(CellComparator.equals(c, cell1));
-    List<Tag> tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    List<Tag> tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(2, tags.size());
     Tag tag = tags.get(0);
     assertEquals(1, tag.getType());
@@ -87,7 +87,7 @@ public class TestCellCodecWithTags {
     assertTrue(decoder.advance());
     c = decoder.current();
     assertTrue(CellComparator.equals(c, cell2));
-    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(1, tags.size());
     tag = tags.get(0);
     assertEquals(1, tag.getType());
@@ -95,7 +95,7 @@ public class TestCellCodecWithTags {
     assertTrue(decoder.advance());
     c = decoder.current();
     assertTrue(CellComparator.equals(c, cell3));
-    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(3, tags.size());
     tag = tags.get(0);
     assertEquals(2, tag.getType());
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestKeyValueCodecWithTags.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestKeyValueCodecWithTags.java
index d8dd7fe..9a158a9 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestKeyValueCodecWithTags.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/codec/TestKeyValueCodecWithTags.java
@@ -76,7 +76,7 @@ public class TestKeyValueCodecWithTags {
     assertTrue(decoder.advance());
     Cell c = decoder.current();
     assertTrue(CellComparator.equals(c, kv1));
-    List<Tag> tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    List<Tag> tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(2, tags.size());
     Tag tag = tags.get(0);
     assertEquals(1, tag.getType());
@@ -87,7 +87,7 @@ public class TestKeyValueCodecWithTags {
     assertTrue(decoder.advance());
     c = decoder.current();
     assertTrue(CellComparator.equals(c, kv2));
-    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(1, tags.size());
     tag = tags.get(0);
     assertEquals(1, tag.getType());
@@ -95,7 +95,7 @@ public class TestKeyValueCodecWithTags {
     assertTrue(decoder.advance());
     c = decoder.current();
     assertTrue(CellComparator.equals(c, kv3));
-    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLengthUnsigned());
     assertEquals(3, tags.size());
     tag = tags.get(0);
     assertEquals(2, tag.getType());
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
index 82739b9..369f633 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/io/TestTagCompressionContext.java
@@ -47,11 +47,11 @@ public class TestTagCompressionContext {
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     TagCompressionContext context = new TagCompressionContext(LRUDictionary.class, Byte.MAX_VALUE);
     KeyValue kv1 = createKVWithTags(2);
-    short tagsLength1 = kv1.getTagsLength();
+    int tagsLength1 = kv1.getTagsLengthUnsigned();
     ByteBuffer ib = ByteBuffer.wrap(kv1.getTagsArray(), kv1.getTagsOffset(), tagsLength1);
     context.compressTags(baos, ib, tagsLength1);
     KeyValue kv2 = createKVWithTags(3);
-    short tagsLength2 = kv2.getTagsLength();
+    int tagsLength2 = kv2.getTagsLengthUnsigned();
     ib = ByteBuffer.wrap(kv2.getTagsArray(), kv2.getTagsOffset(), tagsLength2);
     context.compressTags(baos, ib, tagsLength2);
 
@@ -73,10 +73,10 @@ public class TestTagCompressionContext {
     ByteArrayOutputStream baos = new ByteArrayOutputStream();
     TagCompressionContext context = new TagCompressionContext(LRUDictionary.class, Byte.MAX_VALUE);
     KeyValue kv1 = createKVWithTags(1);
-    short tagsLength1 = kv1.getTagsLength();
+    int tagsLength1 = kv1.getTagsLengthUnsigned();
     context.compressTags(baos, kv1.getTagsArray(), kv1.getTagsOffset(), tagsLength1);
     KeyValue kv2 = createKVWithTags(3);
-    short tagsLength2 = kv2.getTagsLength();
+    int tagsLength2 = kv2.getTagsLengthUnsigned();
     context.compressTags(baos, kv2.getTagsArray(), kv2.getTagsOffset(), tagsLength2);
 
     context.clear();
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
index d60aba9..2feadc8 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestByteRangeWithKVSerialization.java
@@ -35,8 +35,10 @@ public class TestByteRangeWithKVSerialization {
     pbr.putInt(kv.getValueLength());
     pbr.put(kv.getBuffer(), kv.getKeyOffset(), kv.getKeyLength());
     pbr.put(kv.getBuffer(), kv.getValueOffset(), kv.getValueLength());
-    pbr.putShort(kv.getTagsLength());
-    pbr.put(kv.getTagsArray(), kv.getTagsOffset(), kv.getTagsLength());
+    int tagsLen = kv.getTagsLengthUnsigned();
+    pbr.put((byte) (tagsLen >> 8 & 0xff));
+    pbr.put((byte) (tagsLen & 0xff));
+    pbr.put(kv.getTagsArray(), kv.getTagsOffset(), tagsLen);
     pbr.putVLong(kv.getMvccVersion());
   }
 
@@ -45,7 +47,7 @@ public class TestByteRangeWithKVSerialization {
     int keyLen = pbr.getInt();
     int valLen = pbr.getInt();
     pbr.setPosition(pbr.getPosition() + keyLen + valLen); // Skip the key and value section
-    short tagsLen = pbr.getShort();
+    int tagsLen = ((pbr.get() & 0xff) << 8) ^ (pbr.get() & 0xff);
     pbr.setPosition(pbr.getPosition() + tagsLen); // Skip the tags section
     long mvcc = pbr.getVLong();
     KeyValue kv = new KeyValue(pbr.getBytes(), kvStartPos,
@@ -82,8 +84,9 @@ public class TestByteRangeWithKVSerialization {
       Assert.assertTrue(kv.equals(kv1));
       Assert.assertTrue(Bytes.equals(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength(),
           kv1.getValueArray(), kv1.getValueOffset(), kv1.getValueLength()));
-      Assert.assertTrue(Bytes.equals(kv.getTagsArray(), kv.getTagsOffset(), kv.getTagsLength(),
-          kv1.getTagsArray(), kv1.getTagsOffset(), kv1.getTagsLength()));
+      Assert.assertTrue(Bytes.equals(kv.getTagsArray(), kv.getTagsOffset(),
+          kv.getTagsLengthUnsigned(), kv1.getTagsArray(), kv1.getTagsOffset(),
+          kv1.getTagsLengthUnsigned()));
       Assert.assertEquals(kv1.getMvccVersion(), kv.getMvccVersion());
     }
   }
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java
index cc79dbe..ef38f2d 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java
@@ -463,7 +463,7 @@ public class PrefixTreeArrayScanner extends PrefixTreeCell implements CellScanne
   protected void populateTag() {
     int tagTreeIndex = currentRowNode.getTagOffset(currentCellIndex, blockMeta);
     tagsOffset = tagsReader.populateBuffer(tagTreeIndex).getColumnOffset();
-    tagsLength = (short)tagsReader.getColumnLength();
+    tagsLength = tagsReader.getColumnLength();
   }
 
   protected void populateTimestamp() {
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
index 7763c6a..6edff1b 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/decode/PrefixTreeCell.java
@@ -72,7 +72,7 @@ public class PrefixTreeCell implements Cell, Comparable<Cell> {
 
   protected byte[] tagsBuffer;
   protected int tagsOffset;
-  protected short tagsLength;
+  protected int tagsLength;
 
   /********************** Cell methods ******************/
 
@@ -229,13 +229,18 @@ public class PrefixTreeCell implements Cell, Comparable<Cell> {
   }
 
   @Override
-  public short getTagsLength() {
+  public int getTagsLengthUnsigned() {
     return tagsLength;
   }
 
   @Override
+  @Deprecated
+  public short getTagsLength() {
+    return (short) tagsLength;
+  }
+
+  @Override
   public byte[] getTagsArray() {
     return this.tagsBuffer;
   }
-
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
index c110348..5460a14 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
@@ -216,7 +216,8 @@ public class HFileReaderV3 extends HFileReaderV2 {
       }
       ByteBufferUtils.skip(blockBuffer, currKeyLen + currValueLen);
       if (reader.hfileContext.isIncludesTags()) {
-        currTagsLen = blockBuffer.getShort();
+        // Read short as unsigned, high byte first
+        currTagsLen = ((blockBuffer.get() & 0xff) << 8) ^ (blockBuffer.get() & 0xff);
         if (currTagsLen < 0 || currTagsLen > blockBuffer.limit()) {
           throw new IllegalStateException("Invalid currTagsLen " + currTagsLen + ". Block offset: "
               + block.getOffset() + ", block length: " + blockBuffer.limit() + ", position: "
@@ -263,7 +264,8 @@ public class HFileReaderV3 extends HFileReaderV2 {
         }
         ByteBufferUtils.skip(blockBuffer, klen + vlen);
         if (reader.hfileContext.isIncludesTags()) {
-          tlen = blockBuffer.getShort();
+          // Read short as unsigned, high byte first
+          tlen = ((blockBuffer.get() & 0xff) << 8) ^ (blockBuffer.get() & 0xff);
           if (tlen < 0 || tlen > blockBuffer.limit()) {
             throw new IllegalStateException("Invalid tlen " + tlen + ". Block offset: "
                 + block.getOffset() + ", block length: " + blockBuffer.limit() + ", position: "
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
index 8c8cb2a..c6e4fe3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
@@ -85,7 +85,7 @@ public class HFileWriterV3 extends HFileWriterV2 {
   public void append(final KeyValue kv) throws IOException {
     // Currently get the complete arrays
     super.append(kv);
-    short tagsLength = kv.getTagsLength();
+    int tagsLength = kv.getTagsLengthUnsigned();
     if (tagsLength > this.maxTagsLength) {
       this.maxTagsLength = tagsLength;
     }
@@ -126,7 +126,7 @@ public class HFileWriterV3 extends HFileWriterV2 {
     pos = Bytes.putBytes(b, pos, key, 0, key.length);
     pos = Bytes.putBytes(b, pos, value, 0, value.length);
     if (tag.length > 0) {
-      pos = Bytes.putShort(b, pos, (short) tag.length);
+      pos = Bytes.putAsShort(b, pos, tag.length);
       Bytes.putBytes(b, pos, tag, 0, tag.length);
     }
     append(new KeyValue(b, 0, kvlen));
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
index f5b61f8..9afc8a9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
@@ -54,7 +54,7 @@ public class NoOpDataBlockEncoder implements HFileDataBlockEncoder {
     int encodedKvSize = klength + vlength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
     // Write the additional tag into the stream
     if (encodingCtx.getHFileContext().isIncludesTags()) {
-      short tagsLength = kv.getTagsLength();
+      int tagsLength = kv.getTagsLengthUnsigned();
       out.writeShort(tagsLength);
       if (tagsLength > 0) {
         out.write(kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 93eada8..772555a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -5161,7 +5161,7 @@ public class HRegion implements HeapSize { // , Writable{
                 newKV = new KeyValue(row.length, kv.getFamilyLength(),
                     kv.getQualifierLength(), now, KeyValue.Type.Put,
                     oldKv.getValueLength() + kv.getValueLength(),
-                    oldKv.getTagsLength() + kv.getTagsLength());
+                    oldKv.getTagsLengthUnsigned() + kv.getTagsLengthUnsigned());
                 // copy in the value
                 System.arraycopy(oldKv.getValueArray(), oldKv.getValueOffset(),
                     newKV.getValueArray(), newKV.getValueOffset(),
@@ -5172,9 +5172,10 @@ public class HRegion implements HeapSize { // , Writable{
                     kv.getValueLength());
                 // copy in the tags
                 System.arraycopy(oldKv.getTagsArray(), oldKv.getTagsOffset(), newKV.getTagsArray(),
-                    newKV.getTagsOffset(), oldKv.getTagsLength());
+                    newKV.getTagsOffset(), oldKv.getTagsLengthUnsigned());
                 System.arraycopy(kv.getTagsArray(), kv.getTagsOffset(), newKV.getTagsArray(),
-                    newKV.getTagsOffset() + oldKv.getTagsLength(), kv.getTagsLength());
+                    newKV.getTagsOffset() + oldKv.getTagsLengthUnsigned(),
+                    kv.getTagsLengthUnsigned());
                 // copy in row, family, and qualifier
                 System.arraycopy(kv.getRowArray(), kv.getRowOffset(),
                     newKV.getRowArray(), newKV.getRowOffset(), kv.getRowLength());
@@ -5386,8 +5387,8 @@ public class HRegion implements HeapSize { // , Writable{
               // Append new incremented KeyValue to list
               byte[] q = CellUtil.cloneQualifier(kv);
               byte[] val = Bytes.toBytes(amount);
-              int oldCellTagsLen = (c == null) ? 0 : c.getTagsLength();
-              int incCellTagsLen = kv.getTagsLength();
+              int oldCellTagsLen = (c == null) ? 0 : c.getTagsLengthUnsigned();
+              int incCellTagsLen = kv.getTagsLengthUnsigned();
               KeyValue newKV = new KeyValue(row.length, family.getKey().length, q.length, now,
                   KeyValue.Type.Put, val.length, oldCellTagsLen + incCellTagsLen);
               System.arraycopy(row, 0, newKV.getRowArray(), newKV.getRowOffset(), row.length);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
index 873e863..8392d14 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java
@@ -80,7 +80,6 @@ import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.exceptions.RegionOpeningException;
 import org.apache.hadoop.hbase.io.HeapSize;
-import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.master.SplitLogManager;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
@@ -1886,9 +1885,9 @@ public class HLogSplitter {
   private static Cell tagReplayLogSequenceNumber(long seqId, Cell cell) {
     // Tag puts with original sequence number if there is no LOG_REPLAY_TAG yet
     boolean needAddRecoveryTag = true;
-    if (cell.getTagsLength() > 0) {
-      Tag tmpTag = Tag.getTag(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength(),
-        TagType.LOG_REPLAY_TAG_TYPE);
+    if (cell.getTagsLengthUnsigned() > 0) {
+      Tag tmpTag = Tag.getTag(cell.getTagsArray(), cell.getTagsOffset(),
+          cell.getTagsLengthUnsigned(), TagType.LOG_REPLAY_TAG_TYPE);
       if (tmpTag != null) {
         // found an existing log replay tag so reuse it
         needAddRecoveryTag = false;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/KeyValueCompression.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/KeyValueCompression.java
index 0b22ed3..f2c8576 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/KeyValueCompression.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/KeyValueCompression.java
@@ -106,7 +106,7 @@ class KeyValueCompression {
     // we first write the KeyValue infrastructure as VInts.
     WritableUtils.writeVInt(out, keyVal.getKeyLength());
     WritableUtils.writeVInt(out, keyVal.getValueLength());
-    WritableUtils.writeVInt(out, keyVal.getTagsLength());
+    WritableUtils.writeVInt(out, keyVal.getTagsLengthUnsigned());
 
     // now we write the row key, as the row key is likely to be repeated
     // We save space only if we attempt to compress elements with duplicates
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java
index e36d741..ef6e879 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.java
@@ -194,7 +194,7 @@ public class SecureWALCellCodec extends WALCellCodec {
       StreamUtils.writeRawVInt32(cout, kv.getKeyLength());
       StreamUtils.writeRawVInt32(cout, kv.getValueLength());
       // To support tags
-      StreamUtils.writeRawVInt32(cout, kv.getTagsLength());
+      StreamUtils.writeRawVInt32(cout, kv.getTagsLengthUnsigned());
 
       // Write row, qualifier, and family
       StreamUtils.writeRawVInt32(cout, kv.getRowLength());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java
index 100e226..70dc575 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.java
@@ -170,7 +170,7 @@ public class WALCellCodec implements Codec {
       StreamUtils.writeRawVInt32(out, kv.getKeyLength());
       StreamUtils.writeRawVInt32(out, kv.getValueLength());
       // To support tags
-      short tagsLength = kv.getTagsLength();
+      int tagsLength = kv.getTagsLengthUnsigned();
       StreamUtils.writeRawVInt32(out, tagsLength);
 
       // Write row, qualifier, and family; use dictionary
@@ -227,7 +227,7 @@ public class WALCellCodec implements Codec {
       int keylength = StreamUtils.readRawVarint32(in);
       int vlength = StreamUtils.readRawVarint32(in);
       
-      short tagsLength = (short) StreamUtils.readRawVarint32(in);
+      int tagsLength = StreamUtils.readRawVarint32(in);
       int length = 0;
       if(tagsLength == 0) {
         length = KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keylength + vlength;
@@ -266,7 +266,7 @@ public class WALCellCodec implements Codec {
 
       // tags
       if (tagsLength > 0) {
-        pos = Bytes.putShort(backingArray, pos, tagsLength);
+        pos = Bytes.putAsShort(backingArray, pos, tagsLength);
         if (compression.tagCompressionContext != null) {
           compression.tagCompressionContext.uncompressTags(in, backingArray, pos, tagsLength);
         } else {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
index 7dd83e1..6194387 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
@@ -686,7 +686,7 @@ public class AccessControlLists {
        throws IOException {
      List<Permission> results = Lists.newArrayList();
      Iterator<Tag> tagsIterator = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-        cell.getTagsLength());
+        cell.getTagsLengthUnsigned());
      while (tagsIterator.hasNext()) {
        Tag tag = tagsIterator.next();
        if (tag.getType() == ACL_TAG_TYPE) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index 8f73431..4338c88 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -759,7 +759,7 @@ public class AccessController extends BaseRegionObserver
       for (Cell cell: e.getValue()) {
         List<Tag> tags = Lists.newArrayList(new Tag(AccessControlLists.ACL_TAG_TYPE, perms));
         Iterator<Tag> tagIterator = CellUtil.tagsIterator(cell.getTagsArray(),
-            cell.getTagsOffset(), cell.getTagsLength());
+            cell.getTagsOffset(), cell.getTagsLengthUnsigned());
         while (tagIterator.hasNext()) {
           tags.add(tagIterator.next());
         }
@@ -792,9 +792,9 @@ public class AccessController extends BaseRegionObserver
     }
     for (CellScanner cellScanner = m.cellScanner(); cellScanner.advance();) {
       Cell cell = cellScanner.current();
-      if (cell.getTagsLength() > 0) {
+      if (cell.getTagsLengthUnsigned() > 0) {
         Iterator<Tag> tagsItr = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-          cell.getTagsLength());
+          cell.getTagsLengthUnsigned());
         while (tagsItr.hasNext()) {
           if (tagsItr.next().getType() == AccessControlLists.ACL_TAG_TYPE) {
             throw new AccessDeniedException("Mutation contains cell with reserved type tag");
@@ -1854,7 +1854,7 @@ public class AccessController extends BaseRegionObserver
     ListMultimap<String,Permission> perms = ArrayListMultimap.create();
     if (oldCell != null) {
       Iterator<Tag> tagIterator = CellUtil.tagsIterator(oldCell.getTagsArray(),
-          oldCell.getTagsOffset(), oldCell.getTagsLength());
+          oldCell.getTagsOffset(), oldCell.getTagsLengthUnsigned());
       while (tagIterator.hasNext()) {
         Tag tag = tagIterator.next();
         if (tag.getType() != AccessControlLists.ACL_TAG_TYPE) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
index 39f65db..3113ccb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityController.java
@@ -732,7 +732,7 @@ public class VisibilityController extends BaseRegionObserver implements MasterOb
               for (CellScanner cellScanner = m.cellScanner(); cellScanner.advance();) {
                 Cell cell = cellScanner.current();
                 List<Tag> tags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(),
-                    cell.getTagsLength());
+                    cell.getTagsLengthUnsigned());
                 tags.addAll(visibilityTags);
                 Cell updatedCell = new KeyValue(cell.getRowArray(), cell.getRowOffset(),
                     cell.getRowLength(), cell.getFamilyArray(), cell.getFamilyOffset(),
@@ -899,9 +899,9 @@ public class VisibilityController extends BaseRegionObserver implements MasterOb
     if (isSystemOrSuperUser()) {
       return true;
     }
-    if (cell.getTagsLength() > 0) {
+    if (cell.getTagsLengthUnsigned() > 0) {
       Iterator<Tag> tagsItr = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-          cell.getTagsLength());
+          cell.getTagsLengthUnsigned());
       while (tagsItr.hasNext()) {
         if (reservedVisTagTypes.contains(tagsItr.next().getType())) {
           return false;
@@ -1233,7 +1233,7 @@ public class VisibilityController extends BaseRegionObserver implements MasterOb
     }
     // Adding all other tags
     Iterator<Tag> tagsItr = CellUtil.tagsIterator(newCell.getTagsArray(), newCell.getTagsOffset(),
-        newCell.getTagsLength());
+        newCell.getTagsLengthUnsigned());
     while (tagsItr.hasNext()) {
       Tag tag = tagsItr.next();
       if (tag.getType() != VisibilityUtils.VISIBILITY_TAG_TYPE) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelFilter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelFilter.java
index 14d69ed..bf8626f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelFilter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityLabelFilter.java
@@ -81,7 +81,7 @@ class VisibilityLabelFilter extends FilterBase {
     }
 
     Iterator<Tag> tagsItr = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-        cell.getTagsLength());
+        cell.getTagsLengthUnsigned());
     boolean visibilityTagPresent = false;
     while (tagsItr.hasNext()) {
       boolean includeKV = true;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
index c151b74..89b49c0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.java
@@ -108,7 +108,7 @@ public class VisibilityScanDeleteTracker extends ScanDeleteTracker {
 
   private void extractDeleteTags(Cell delCell, Type type) {
     // If tag is present in the delete
-    if (delCell.getTagsLength() > 0) {
+    if (delCell.getTagsLengthUnsigned() > 0) {
       switch (type) {
         case DeleteFamily:
           List<Tag> delTags = new ArrayList<Tag>();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
index b87f59a..7e9304a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityUtils.java
@@ -177,11 +177,11 @@ public class VisibilityUtils {
   public static boolean getVisibilityTags(Cell cell, List<Tag> tags) {
     boolean sortedOrder = false;
     Iterator<Tag> tagsIterator = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-        cell.getTagsLength());
+        cell.getTagsLengthUnsigned());
     while (tagsIterator.hasNext()) {
       Tag tag = tagsIterator.next();
-      if(tag.getType() == VisibilityUtils.VISIBILITY_EXP_SERIALIZATION_TAG_TYPE) {
-        int serializationVersion = Bytes.toShort(tag.getValue());
+      if (tag.getType() == VisibilityUtils.VISIBILITY_EXP_SERIALIZATION_TAG_TYPE) {
+        int serializationVersion = Bytes.toShort(tag.getBuffer());
         if (serializationVersion == VisibilityConstants.VISIBILITY_SERIALIZATION_VERSION) {
           sortedOrder = true;
           continue;
@@ -201,7 +201,7 @@ public class VisibilityUtils {
    */
   public static boolean isVisibilityTagsPresent(Cell cell) {
     Iterator<Tag> tagsIterator = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-        cell.getTagsLength());
+        cell.getTagsLengthUnsigned());
     while (tagsIterator.hasNext()) {
       Tag tag = tagsIterator.next();
       if (tag.getType() == VisibilityUtils.VISIBILITY_TAG_TYPE) {
@@ -271,7 +271,7 @@ public class VisibilityUtils {
 
   private static List<List<Integer>> sortTagsBasedOnOrdinal(Cell cell) throws IOException {
     Iterator<Tag> tagsItr = CellUtil.tagsIterator(cell.getTagsArray(), cell.getTagsOffset(),
-        cell.getTagsLength());
+        cell.getTagsLengthUnsigned());
     List<List<Integer>> fullTagsList = new ArrayList<List<Integer>>();
     while (tagsItr.hasNext()) {
       Tag tag = tagsItr.next();
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
index 034771c..c1befdf 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
@@ -171,9 +171,9 @@ public class TestPrefixTreeEncoding {
         fail("Current kv " + currentKV + " is smaller than previous keyvalue " + previousKV);
       }
       if (!includesTag) {
-        assertFalse(currentKV.getTagsLength() > 0);
+        assertFalse(currentKV.getTagsLengthUnsigned() > 0);
       } else {
-        Assert.assertTrue(currentKV.getTagsLength() > 0);
+        Assert.assertTrue(currentKV.getTagsLengthUnsigned() > 0);
       }
       previousKV = currentKV;
     } while (seeker.next());
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
index a2abefe..2fc701a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileWriterV3.java
@@ -237,7 +237,7 @@ public class TestHFileWriterV3 {
         buf.get(value);
         byte[] tagValue = null;
         if (useTags) {
-          int tagLen = buf.getShort();
+          int tagLen = ((buf.get() & 0xff) << 8) ^ (buf.get() & 0xff);
           tagValue = new byte[tagLen];
           buf.get(tagValue);
         }
@@ -257,9 +257,9 @@ public class TestHFileWriterV3 {
         if (useTags) {
           assertNotNull(tagValue);
           KeyValue tkv =  keyValues.get(entriesRead);
-          assertEquals(tagValue.length, tkv.getTagsLength());
+          assertEquals(tagValue.length, tkv.getTagsLengthUnsigned());
           assertTrue(Bytes.compareTo(tagValue, 0, tagValue.length, tkv.getTagsArray(),
-              tkv.getTagsOffset(), tkv.getTagsLength()) == 0);
+              tkv.getTagsOffset(), tkv.getTagsLengthUnsigned()) == 0);
         }
         ++entriesRead;
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
index 1927334..3d1a845 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/DataBlockEncodingTool.java
@@ -208,7 +208,7 @@ public class DataBlockEncodingTool {
     }
 
     rawKVs = uncompressedOutputStream.toByteArray();
-    boolean useTag = (currentKV.getTagsLength() > 0);
+    boolean useTag = (currentKV.getTagsLengthUnsigned() > 0);
     for (DataBlockEncoding encoding : encodings) {
       if (encoding == DataBlockEncoding.NONE) {
         continue;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
index fa86df7..753ed01 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestTags.java
@@ -27,6 +27,7 @@ import java.util.List;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellScanner;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
@@ -63,7 +64,7 @@ import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
 /**
- *  Class that test tags
+ * Class that test tags
  */
 @Category(MediumTests.class)
 public class TestTags {
@@ -123,9 +124,9 @@ public class TestTags {
       table.put(put);
       admin.flush(tableName.getName());
       List<HRegion> regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 0)) {
+        while (!(store.getStorefilesCount() > 0)) {
           Thread.sleep(10);
         }
       }
@@ -137,9 +138,9 @@ public class TestTags {
       table.put(put1);
       admin.flush(tableName.getName());
       regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 1)) {
+        while (!(store.getStorefilesCount() > 1)) {
           Thread.sleep(10);
         }
       }
@@ -152,15 +153,15 @@ public class TestTags {
 
       admin.flush(tableName.getName());
       regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 2)) {
+        while (!(store.getStorefilesCount() > 2)) {
           Thread.sleep(10);
         }
       }
       result(fam, row, qual, row2, table, value, value2, row1, value1);
       admin.compact(tableName.getName());
-      while(admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
+      while (admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
         Thread.sleep(10);
       }
       result(fam, row, qual, row2, table, value, value2, row1, value1);
@@ -201,9 +202,9 @@ public class TestTags {
       table.put(put);
       admin.flush(tableName.getName());
       List<HRegion> regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 0)) {
+        while (!(store.getStorefilesCount() > 0)) {
           Thread.sleep(10);
         }
       }
@@ -214,9 +215,9 @@ public class TestTags {
       table.put(put1);
       admin.flush(tableName.getName());
       regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 1)) {
+        while (!(store.getStorefilesCount() > 1)) {
           Thread.sleep(10);
         }
       }
@@ -228,9 +229,9 @@ public class TestTags {
 
       admin.flush(tableName.getName());
       regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
+      for (HRegion region : regions) {
         Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 2)) {
+        while (!(store.getStorefilesCount() > 2)) {
           Thread.sleep(10);
         }
       }
@@ -240,7 +241,7 @@ public class TestTags {
         Result[] next = scanner.next(3);
         for (Result result : next) {
           CellScanner cellScanner = result.cellScanner();
-          boolean advance = cellScanner.advance();
+          cellScanner.advance();
           KeyValue current = (KeyValue) cellScanner.current();
           assertTrue(current.getValueOffset() + current.getValueLength() == current.getLength());
         }
@@ -249,7 +250,7 @@ public class TestTags {
           scanner.close();
       }
       admin.compact(tableName.getName());
-      while(admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
+      while (admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
         Thread.sleep(10);
       }
       s = new Scan(row);
@@ -258,7 +259,7 @@ public class TestTags {
         Result[] next = scanner.next(3);
         for (Result result : next) {
           CellScanner cellScanner = result.cellScanner();
-          boolean advance = cellScanner.advance();
+          cellScanner.advance();
           KeyValue current = (KeyValue) cellScanner.current();
           assertTrue(current.getValueOffset() + current.getValueLength() == current.getLength());
         }
@@ -273,128 +274,135 @@ public class TestTags {
       }
     }
   }
+
   @Test
   public void testFlushAndCompactionwithCombinations() throws Exception {
-    HTable table = null;
-    try {
-      TableName tableName = TableName.valueOf(TEST_NAME.getMethodName());
-      byte[] fam = Bytes.toBytes("info");
-      byte[] row = Bytes.toBytes("rowa");
-      // column names
-      byte[] qual = Bytes.toBytes("qual");
-
-      byte[] row1 = Bytes.toBytes("rowb");
+    TableName tableName = TableName.valueOf(TEST_NAME.getMethodName());
+    byte[] fam = Bytes.toBytes("info");
+    byte[] row = Bytes.toBytes("rowa");
+    // column names
+    byte[] qual = Bytes.toBytes("qual");
 
-      byte[] row2 = Bytes.toBytes("rowc");
-      byte[] rowd = Bytes.toBytes("rowd");
-      byte[] rowe = Bytes.toBytes("rowe");
+    byte[] row1 = Bytes.toBytes("rowb");
 
+    byte[] row2 = Bytes.toBytes("rowc");
+    byte[] rowd = Bytes.toBytes("rowd");
+    byte[] rowe = Bytes.toBytes("rowe");
+    HTable table = null;
+    for (DataBlockEncoding encoding : DataBlockEncoding.values()) {
       HTableDescriptor desc = new HTableDescriptor(tableName);
       HColumnDescriptor colDesc = new HColumnDescriptor(fam);
       colDesc.setBlockCacheEnabled(true);
-      // colDesc.setDataBlockEncoding(DataBlockEncoding.NONE);
-      colDesc.setDataBlockEncoding(DataBlockEncoding.PREFIX_TREE);
+      colDesc.setDataBlockEncoding(encoding);
       desc.addFamily(colDesc);
       HBaseAdmin admin = TEST_UTIL.getHBaseAdmin();
       admin.createTable(desc);
-
-      table = new HTable(TEST_UTIL.getConfiguration(), tableName);
-      Put put = new Put(row);
-      byte[] value = Bytes.toBytes("value");
-      put.add(fam, qual, HConstants.LATEST_TIMESTAMP, value);
-      put.setAttribute("visibility", Bytes.toBytes("ram"));
-      table.put(put);
-      Put put1 = new Put(row1);
-      byte[] value1 = Bytes.toBytes("1000dfsdf");
-      put1.add(fam, qual, HConstants.LATEST_TIMESTAMP, value1);
-      table.put(put1);
-      admin.flush(tableName.getName());
-      List<HRegion> regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
-        Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 0)) {
-          Thread.sleep(10);
+      try {
+        table = new HTable(TEST_UTIL.getConfiguration(), tableName);
+        Put put = new Put(row);
+        byte[] value = Bytes.toBytes("value");
+        put.add(fam, qual, HConstants.LATEST_TIMESTAMP, value);
+        int bigTagLen = Short.MAX_VALUE + 5;
+        put.setAttribute("visibility", new byte[bigTagLen]);
+        table.put(put);
+        Put put1 = new Put(row1);
+        byte[] value1 = Bytes.toBytes("1000dfsdf");
+        put1.add(fam, qual, HConstants.LATEST_TIMESTAMP, value1);
+        table.put(put1);
+        admin.flush(tableName.getName());
+        List<HRegion> regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
+        for (HRegion region : regions) {
+          Store store = region.getStore(fam);
+          while (!(store.getStorefilesCount() > 0)) {
+            Thread.sleep(10);
+          }
         }
-      }
 
-      put1 = new Put(row2);
-      value1 = Bytes.toBytes("1000dfsdf");
-      put1.add(fam, qual, HConstants.LATEST_TIMESTAMP, value1);
-      table.put(put1);
-      admin.flush(tableName.getName());
-      regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
-        Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 1)) {
-          Thread.sleep(10);
+        put1 = new Put(row2);
+        value1 = Bytes.toBytes("1000dfsdf");
+        put1.add(fam, qual, HConstants.LATEST_TIMESTAMP, value1);
+        table.put(put1);
+        admin.flush(tableName.getName());
+        regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
+        for (HRegion region : regions) {
+          Store store = region.getStore(fam);
+          while (!(store.getStorefilesCount() > 1)) {
+            Thread.sleep(10);
+          }
         }
-      }
-      Put put2 = new Put(rowd);
-      byte[] value2 = Bytes.toBytes("1000dfsdf");
-      put2.add(fam, qual, HConstants.LATEST_TIMESTAMP, value2);
-      table.put(put2);
-      put2 = new Put(rowe);
-      value2 = Bytes.toBytes("1000dfsddfdf");
-      put2.add(fam, qual, HConstants.LATEST_TIMESTAMP, value2);
-      put.setAttribute("visibility", Bytes.toBytes("ram"));
-      table.put(put2);
-      admin.flush(tableName.getName());
-      regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
-      for(HRegion region : regions) {
-        Store store = region.getStore(fam);
-        while(!(store.getStorefilesCount() > 2)) {
-          Thread.sleep(10);
+        Put put2 = new Put(rowd);
+        byte[] value2 = Bytes.toBytes("1000dfsdf");
+        put2.add(fam, qual, HConstants.LATEST_TIMESTAMP, value2);
+        table.put(put2);
+        put2 = new Put(rowe);
+        value2 = Bytes.toBytes("1000dfsddfdf");
+        put2.add(fam, qual, HConstants.LATEST_TIMESTAMP, value2);
+        put.setAttribute("visibility", Bytes.toBytes("ram"));
+        table.put(put2);
+        admin.flush(tableName.getName());
+        regions = TEST_UTIL.getHBaseCluster().getRegions(tableName.getName());
+        for (HRegion region : regions) {
+          Store store = region.getStore(fam);
+          while (!(store.getStorefilesCount() > 2)) {
+            Thread.sleep(10);
+          }
         }
-      }
-      Scan s = new Scan(row);
-      ResultScanner scanner = table.getScanner(s);
-      try {
-        Result[] next = scanner.next(5);
-        for (Result result : next) {
-          CellScanner cellScanner = result.cellScanner();
-          boolean advance = cellScanner.advance();
-          KeyValue current = (KeyValue) cellScanner.current();
-          // System.out.println(current);
-          int tagsLength = current.getTagsLength();
-          if (tagsLength == 0) {
-            assertTrue(current.getValueOffset() + current.getValueLength() == current.getLength());
-          } else {
-            // even if taglength is going to be > 0 the byte array would be same
-            assertTrue(current.getValueOffset() + current.getValueLength() != current.getLength());
+        TestCoprocessorForTags.checkTagPresence = true;
+        Scan s = new Scan(row);
+        s.setCaching(1);
+        ResultScanner scanner = table.getScanner(s);
+        try {
+          Result next = null;
+          while ((next = scanner.next()) != null) {
+            CellScanner cellScanner = next.cellScanner();
+            cellScanner.advance();
+            KeyValue current = (KeyValue) cellScanner.current();
+            if (CellUtil.matchingRow(current, row)) {
+              assertEquals(1, TestCoprocessorForTags.tags.size());
+              Tag tag = TestCoprocessorForTags.tags.get(0);
+              assertEquals(bigTagLen, tag.getTagLength());
+            } else {
+              assertEquals(0, TestCoprocessorForTags.tags.size());
+            }
           }
+        } finally {
+          if (scanner != null) {
+            scanner.close();
+          }
+          TestCoprocessorForTags.checkTagPresence = false;
         }
-      } finally {
-        if (scanner != null) {
-          scanner.close();
+        while (admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
+          Thread.sleep(10);
         }
-      }
-      while(admin.getCompactionState(tableName.getName()) != CompactionState.NONE) {
-        Thread.sleep(10);
-      }
-      s = new Scan(row);
-      scanner = table.getScanner(s);
-      try {
-        Result[] next = scanner.next(5);
-        for (Result result : next) {
-          CellScanner cellScanner = result.cellScanner();
-          boolean advance = cellScanner.advance();
-          KeyValue current = (KeyValue) cellScanner.current();
-          // System.out.println(current);
-          if (current.getTagsLength() == 0) {
-            assertTrue(current.getValueOffset() + current.getValueLength() == current.getLength());
-          } else {
-            // even if taglength is going to be > 0 the byte array would be same
-            assertTrue(current.getValueOffset() + current.getValueLength() != current.getLength());
+        TestCoprocessorForTags.checkTagPresence = true;
+        scanner = table.getScanner(s);
+        try {
+          Result next = null;
+          while ((next = scanner.next()) != null) {
+            CellScanner cellScanner = next.cellScanner();
+            cellScanner.advance();
+            KeyValue current = (KeyValue) cellScanner.current();
+            if (CellUtil.matchingRow(current, row)) {
+              assertEquals(1, TestCoprocessorForTags.tags.size());
+              Tag tag = TestCoprocessorForTags.tags.get(0);
+              assertEquals(bigTagLen, tag.getTagLength());
+            } else {
+              assertEquals(0, TestCoprocessorForTags.tags.size());
+            }
           }
+        } finally {
+          if (scanner != null) {
+            scanner.close();
+          }
+          TestCoprocessorForTags.checkTagPresence = false;
         }
       } finally {
-        if (scanner != null) {
-          scanner.close();
+        if (table != null) {
+          table.close();
         }
-      }
-    } finally {
-      if (table != null) {
-        table.close();
+        // delete the table
+        admin.disableTable(tableName);
+        admin.deleteTable(tableName);
       }
     }
   }
@@ -544,9 +552,6 @@ public class TestTags {
     try {
       scanner = table.getScanner(s);
       Result next = scanner.next();
-      CellScanner cellScanner = next.cellScanner();
-      boolean advance = cellScanner.advance();
-      KeyValue current = (KeyValue) cellScanner.current();
 
       assertTrue(Bytes.equals(next.getRow(), row));
       assertTrue(Bytes.equals(next.getValue(fam, qual), value));
@@ -630,7 +635,8 @@ public class TestTags {
           CellScanner cellScanner = result.cellScanner();
           if (cellScanner.advance()) {
             Cell cell = cellScanner.current();
-            tags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength());
+            tags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(),
+                cell.getTagsLengthUnsigned());
           }
         }
       }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
index 491a9db..f99ee58 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationWithTags.java
@@ -240,7 +240,8 @@ public class TestReplicationWithTags {
         // Check tag presence in the 1st cell in 1st Result
         if (!results.isEmpty()) {
           Cell cell = results.get(0);
-          tags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLength());
+          tags = Tag
+              .asList(cell.getTagsArray(), cell.getTagsOffset(), cell.getTagsLengthUnsigned());
         }
       }
     }
diff --git a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
index 9cad902..6add90f 100644
--- a/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
+++ b/hbase-thrift/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
@@ -161,7 +161,7 @@ public class ThriftUtilities {
       col.setQualifier(CellUtil.cloneQualifier(kv));
       col.setTimestamp(kv.getTimestamp());
       col.setValue(CellUtil.cloneValue(kv));
-      if (kv.getTagsLength() > 0) {
+      if (kv.getTagsLengthUnsigned() > 0) {
         col.setTags(CellUtil.getTagArray(kv));
       }
       columnValues.add(col);
