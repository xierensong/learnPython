diff --git conf/hbase-policy.xml conf/hbase-policy.xml
new file mode 100644
index 0000000..e45f23c
--- /dev/null
+++ conf/hbase-policy.xml
@@ -0,0 +1,53 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+
+<configuration>
+  <property>
+    <name>security.client.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HRegionInterface protocol implementations (ie. 
+    clients talking to HRegionServers)
+    The ACL is a comma-separated list of user and group names. The user and 
+    group list is separated by a blank. For e.g. "alice,bob users,wheel". 
+    A special value of "*" means all users are allowed.</description>
+  </property>
+
+  <property>
+    <name>security.admin.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HMasterInterface protocol implementation (ie. 
+    clients talking to HMaster for admin operations).
+    The ACL is a comma-separated list of user and group names. The user and 
+    group list is separated by a blank. For e.g. "alice,bob users,wheel". 
+    A special value of "*" means all users are allowed.</description>
+  </property>
+
+  <property>
+    <name>security.masterregion.protocol.acl</name>
+    <value>*</value>
+    <description>ACL for HMasterRegionInterface protocol implementations
+    (for HRegionServers communicating with HMaster)
+    The ACL is a comma-separated list of user and group names. The user and 
+    group list is separated by a blank. For e.g. "alice,bob users,wheel". 
+    A special value of "*" means all users are allowed.</description>
+  </property>
+</configuration>
diff --git pom.xml pom.xml
index 2847416..eccf41f 100644
--- pom.xml
+++ pom.xml
@@ -276,6 +276,7 @@
         <enabled>false</enabled>
       </snapshots>
     </repository>
+
   </repositories>
 
   <build>
@@ -404,6 +405,14 @@
         </includes>
       </resource>
     </resources>
+    <testResources>
+      <testResource>
+        <directory>src/test/resources</directory>
+        <excludes>
+          <exclude>hbase-site.xml</exclude>
+        </excludes>
+      </testResource>
+    </testResources>
 
     <plugins>
       <plugin>
@@ -692,12 +701,12 @@
               <target>
                 <!-- Complements the assembly -->
 
-                <mkdir dir="${project.build.directory}/${project.artifactId}-${project.version}/${project.artifactId}-${project.version}/lib/native/${build.platform}"/>
+                <mkdir dir="${project.build.directory}/${project.build.finalName}/${project.build.finalName}/lib/native/${build.platform}"/>
 
                 <!-- Using Unix cp to preserve symlinks, using script to handle wildcards -->
                 <echo file="${project.build.directory}/copynativelibs.sh">
                     if [ `ls ${project.build.directory}/nativelib | wc -l` -ne 0 ]; then
-                      cp -PR ${project.build.directory}/nativelib/lib* ${project.build.directory}/${project.artifactId}-${project.version}/${project.artifactId}-${project.version}/lib/native/${build.platform}
+                      cp -PR ${project.build.directory}/nativelib/lib* ${project.build.directory}/${project.build.finalName}/${project.build.finalName}/lib/native/${build.platform}
                     fi
                 </echo>
                 <exec executable="sh" dir="${project.build.directory}" failonerror="true">
@@ -706,9 +715,9 @@
 
                 <!-- Using Unix tar to preserve symlinks -->
                 <exec executable="tar" failonerror="yes"
-                  dir="${project.build.directory}/${project.artifactId}-${project.version}">
+                  dir="${project.build.directory}/${project.build.finalName}">
                     <arg value="czf"/>
-                    <arg value="${project.build.directory}/${project.artifactId}-${project.version}.tar.gz"/>
+                    <arg value="${project.build.directory}/${project.build.finalName}.tar.gz"/>
                     <arg value="."/>
                 </exec>
 
@@ -1320,6 +1329,32 @@
           <scope>test</scope>
         </dependency>
       </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>build-helper-maven-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>add-test-resource</id>
+                <goals>
+                  <goal>add-test-resource</goal>
+                </goals>
+                <configuration>
+                  <resources>
+                    <resource>
+                      <directory>src/test/resources</directory>
+                      <includes>
+                        <include>hbase-site.xml</include>
+                      </includes>
+                    </resource>
+                  </resources>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
     </profile>
     
     
@@ -1363,6 +1398,61 @@
       </properties>
     </profile>     
 
+    <!-- profile for building against Hadoop 0.20+security-->
+    <profile>
+      <id>security</id>
+      <build>
+        <finalName>${artifactId}-${version}-security</finalName>
+        <plugins>
+          <plugin>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>build-helper-maven-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>add-source</id>
+                <goals>
+                  <goal>add-source</goal>
+                </goals>
+                <configuration>
+                  <sources>
+                    <source>${project.basedir}/security/src/main/java</source>
+                  </sources>
+                </configuration>
+              </execution>
+              <execution>
+                <id>add-test-source</id>
+                <goals>
+                  <goal>add-test-source</goal>
+                </goals>
+                <configuration>
+                  <sources>
+                    <source>${project.basedir}/security/src/test/java</source>
+                  </sources>
+                </configuration>
+              </execution>
+              <execution>
+                <id>add-test-resource</id>
+                <goals>
+                  <goal>add-test-resource</goal>
+                </goals>
+                <configuration>
+                  <resources>
+                    <resource>
+                      <directory>${project.basedir}/security/src/test/resources</directory>
+                      <includes>
+                        <include>hbase-site.xml</include>
+                      </includes>
+                    </resource>
+                  </resources>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+
+
     <!--
       profile for building against Hadoop 0.22.0. Activate using:
        mvn -Dhadoop.profile=22
@@ -1507,6 +1597,32 @@
           <scope>test</scope>
         </dependency>
       </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>build-helper-maven-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>add-test-resource</id>
+                <goals>
+                  <goal>add-test-resource</goal>
+                </goals>
+                <configuration>
+                  <resources>
+                    <resource>
+                      <directory>src/test/resources</directory>
+                      <includes>
+                        <include>hbase-site.xml</include>
+                      </includes>
+                    </resource>
+                  </resources>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
     </profile>
 
     <!--
@@ -1663,6 +1779,32 @@
           <scope>test</scope>
         </dependency>
       </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>build-helper-maven-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>add-test-resource</id>
+                <goals>
+                  <goal>add-test-resource</goal>
+                </goals>
+                <configuration>
+                  <resources>
+                    <resource>
+                      <directory>src/test/resources</directory>
+                      <includes>
+                        <include>hbase-site.xml</include>
+                      </includes>
+                    </resource>
+                  </resources>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
     </profile>
   </profiles>
  
diff --git security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java
new file mode 100644
index 0000000..e85bf42
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java
@@ -0,0 +1,488 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.ipc;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcClient;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.AuthMethod;
+import org.apache.hadoop.hbase.security.KerberosInfo;
+import org.apache.hadoop.hbase.security.TokenInfo;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier;
+import org.apache.hadoop.hbase.security.token.AuthenticationTokenSelector;
+import org.apache.hadoop.hbase.util.PoolMap;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.ipc.RemoteException;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.security.token.TokenSelector;
+import org.apache.hadoop.util.ReflectionUtils;
+
+import javax.net.SocketFactory;
+import java.io.*;
+import java.net.*;
+import java.security.PrivilegedExceptionAction;
+import java.util.HashMap;
+import java.util.Hashtable;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Random;
+import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
+
+/**
+ * A client for an IPC service, which support SASL authentication of connections
+ * using either GSSAPI for Kerberos authentication or DIGEST-MD5 for
+ * authentication using signed tokens.
+ *
+ * <p>
+ * This is a copy of org.apache.hadoop.ipc.Client from secure Hadoop,
+ * reworked to remove code duplicated with
+ * {@link org.apache.hadoop.hbase.HBaseClient}.  This is part of the loadable
+ * {@link SecureRpcEngine}, and only functions in connection with a
+ * {@link SecureServer} instance.
+ * </p>
+ */
+public class SecureClient extends HBaseClient {
+
+  private static final Log LOG =
+    LogFactory.getLog("org.apache.hadoop.ipc.SecureClient");
+
+  protected static Map<String,TokenSelector<? extends TokenIdentifier>> tokenHandlers =
+      new HashMap<String,TokenSelector<? extends TokenIdentifier>>();
+  static {
+    tokenHandlers.put(AuthenticationTokenIdentifier.AUTH_TOKEN_TYPE.toString(),
+        new AuthenticationTokenSelector());
+  }
+
+  /** Thread that reads responses and notifies callers.  Each connection owns a
+   * socket connected to a remote address.  Calls are multiplexed through this
+   * socket: responses may be delivered out of order. */
+  protected class SecureConnection extends Connection {
+    private InetSocketAddress server;             // server ip:port
+    private String serverPrincipal;  // server's krb5 principal name
+    private SecureConnectionHeader header;              // connection header
+    private AuthMethod authMethod; // authentication method
+    private boolean useSasl;
+    private Token<? extends TokenIdentifier> token;
+    private HBaseSaslRpcClient saslRpcClient;
+    private int reloginMaxBackoff; // max pause before relogin on sasl failure
+
+    public SecureConnection(ConnectionId remoteId) throws IOException {
+      super(remoteId);
+      this.server = remoteId.getAddress();
+
+      User ticket = remoteId.getTicket();
+      Class<?> protocol = remoteId.getProtocol();
+      this.useSasl = User.isSecurityEnabled();
+      if (useSasl && protocol != null) {
+        TokenInfo tokenInfo = protocol.getAnnotation(TokenInfo.class);
+        if (tokenInfo != null) {
+          TokenSelector<? extends TokenIdentifier> tokenSelector =
+              tokenHandlers.get(tokenInfo.value());
+          if (tokenSelector != null) {
+            token = tokenSelector.selectToken(new Text(clusterId),
+                ticket.getUGI().getTokens());
+          } else if (LOG.isDebugEnabled()) {
+            LOG.debug("No token selector found for type "+tokenInfo.value());
+          }
+        }
+        KerberosInfo krbInfo = protocol.getAnnotation(KerberosInfo.class);
+        if (krbInfo != null) {
+          String serverKey = krbInfo.serverPrincipal();
+          if (serverKey == null) {
+            throw new IOException(
+                "Can't obtain server Kerberos config key from KerberosInfo");
+          }
+          serverPrincipal = SecurityUtil.getServerPrincipal(
+              conf.get(serverKey), server.getAddress().getCanonicalHostName().toLowerCase());
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("RPC Server Kerberos principal name for protocol="
+                + protocol.getCanonicalName() + " is " + serverPrincipal);
+          }
+        }
+      }
+
+      if (!useSasl) {
+        authMethod = AuthMethod.SIMPLE;
+      } else if (token != null) {
+        authMethod = AuthMethod.DIGEST;
+      } else {
+        authMethod = AuthMethod.KERBEROS;
+      }
+
+      header = new SecureConnectionHeader(
+          protocol == null ? null : protocol.getName(), ticket, authMethod);
+
+      if (LOG.isDebugEnabled())
+        LOG.debug("Use " + authMethod + " authentication for protocol "
+            + protocol.getSimpleName());
+
+      reloginMaxBackoff = conf.getInt("hbase.security.relogin.maxbackoff", 5000);
+    }
+
+    private synchronized void disposeSasl() {
+      if (saslRpcClient != null) {
+        try {
+          saslRpcClient.dispose();
+          saslRpcClient = null;
+        } catch (IOException ioe) {
+          LOG.info("Error disposing of SASL client", ioe);
+        }
+      }
+    }
+
+    private synchronized boolean shouldAuthenticateOverKrb() throws IOException {
+      UserGroupInformation loginUser = UserGroupInformation.getLoginUser();
+      UserGroupInformation currentUser =
+        UserGroupInformation.getCurrentUser();
+      UserGroupInformation realUser = currentUser.getRealUser();
+      return authMethod == AuthMethod.KERBEROS &&
+          loginUser != null &&
+          //Make sure user logged in using Kerberos either keytab or TGT
+          loginUser.hasKerberosCredentials() &&
+          // relogin only in case it is the login user (e.g. JT)
+          // or superuser (like oozie).
+          (loginUser.equals(currentUser) || loginUser.equals(realUser));
+    }
+
+    private synchronized boolean setupSaslConnection(final InputStream in2,
+        final OutputStream out2)
+        throws IOException {
+      saslRpcClient = new HBaseSaslRpcClient(authMethod, token, serverPrincipal);
+      return saslRpcClient.saslConnect(in2, out2);
+    }
+
+    /**
+     * If multiple clients with the same principal try to connect
+     * to the same server at the same time, the server assumes a
+     * replay attack is in progress. This is a feature of kerberos.
+     * In order to work around this, what is done is that the client
+     * backs off randomly and tries to initiate the connection
+     * again.
+     * The other problem is to do with ticket expiry. To handle that,
+     * a relogin is attempted.
+     */
+    private synchronized void handleSaslConnectionFailure(
+        final int currRetries,
+        final int maxRetries, final Exception ex, final Random rand,
+        final User user)
+    throws IOException, InterruptedException{
+      user.runAs(new PrivilegedExceptionAction<Object>() {
+        public Object run() throws IOException, InterruptedException {
+          closeConnection();
+          if (shouldAuthenticateOverKrb()) {
+            if (currRetries < maxRetries) {
+              LOG.debug("Exception encountered while connecting to " +
+                  "the server : " + ex);
+              //try re-login
+              if (UserGroupInformation.isLoginKeytabBased()) {
+                UserGroupInformation.getLoginUser().reloginFromKeytab();
+              } else {
+                UserGroupInformation.getLoginUser().reloginFromTicketCache();
+              }
+              disposeSasl();
+              //have granularity of milliseconds
+              //we are sleeping with the Connection lock held but since this
+              //connection instance is being used for connecting to the server
+              //in question, it is okay
+              Thread.sleep((rand.nextInt(reloginMaxBackoff) + 1));
+              return null;
+            } else {
+              String msg = "Couldn't setup connection for " +
+              UserGroupInformation.getLoginUser().getUserName() +
+              " to " + serverPrincipal;
+              LOG.warn(msg);
+              throw (IOException) new IOException(msg).initCause(ex);
+            }
+          } else {
+            LOG.warn("Exception encountered while connecting to " +
+                "the server : " + ex);
+          }
+          if (ex instanceof RemoteException)
+            throw (RemoteException)ex;
+          throw new IOException(ex);
+        }
+      });
+    }
+
+    @Override
+    protected synchronized void setupIOstreams()
+        throws IOException, InterruptedException {
+      if (socket != null || shouldCloseConnection.get()) {
+        return;
+      }
+
+      try {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Connecting to "+server);
+        }
+        short numRetries = 0;
+        final short MAX_RETRIES = 5;
+        Random rand = null;
+        while (true) {
+          setupConnection();
+          InputStream inStream = NetUtils.getInputStream(socket);
+          OutputStream outStream = NetUtils.getOutputStream(socket);
+          writeRpcHeader(outStream);
+          if (useSasl) {
+            final InputStream in2 = inStream;
+            final OutputStream out2 = outStream;
+            User ticket = remoteId.getTicket();
+            if (authMethod == AuthMethod.KERBEROS) {
+              UserGroupInformation ugi = ticket.getUGI();
+              if (ugi != null && ugi.getRealUser() != null) {
+                ticket = User.create(ugi.getRealUser());
+              }
+            }
+            boolean continueSasl = false;
+            try {
+              continueSasl =
+                ticket.runAs(new PrivilegedExceptionAction<Boolean>() {
+                  @Override
+                  public Boolean run() throws IOException {
+                    return setupSaslConnection(in2, out2);
+                  }
+                });
+            } catch (Exception ex) {
+              if (rand == null) {
+                rand = new Random();
+              }
+              handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,
+                   ticket);
+              continue;
+            }
+            if (continueSasl) {
+              // Sasl connect is successful. Let's set up Sasl i/o streams.
+              inStream = saslRpcClient.getInputStream(inStream);
+              outStream = saslRpcClient.getOutputStream(outStream);
+            } else {
+              // fall back to simple auth because server told us so.
+              authMethod = AuthMethod.SIMPLE;
+              header = new SecureConnectionHeader(header.getProtocol(),
+                  header.getUser(), authMethod);
+              useSasl = false;
+            }
+          }
+          this.in = new DataInputStream(new BufferedInputStream
+              (new PingInputStream(inStream)));
+          this.out = new DataOutputStream
+          (new BufferedOutputStream(outStream));
+          writeHeader();
+
+          // update last activity time
+          touch();
+
+          // start the receiver thread after the socket connection has been set up
+          start();
+          return;
+        }
+      } catch (IOException e) {
+        markClosed(e);
+        close();
+
+        throw e;
+      }
+    }
+
+    /* Write the RPC header */
+    private void writeRpcHeader(OutputStream outStream) throws IOException {
+      DataOutputStream out = new DataOutputStream(new BufferedOutputStream(outStream));
+      // Write out the header, version and authentication method
+      out.write(SecureServer.HEADER.array());
+      out.write(SecureServer.CURRENT_VERSION);
+      authMethod.write(out);
+      out.flush();
+    }
+
+    /**
+     * Write the protocol header for each connection
+     * Out is not synchronized because only the first thread does this.
+     */
+    private void writeHeader() throws IOException {
+      // Write out the ConnectionHeader
+      DataOutputBuffer buf = new DataOutputBuffer();
+      header.write(buf);
+
+      // Write out the payload length
+      int bufLen = buf.getLength();
+      out.writeInt(bufLen);
+      out.write(buf.getData(), 0, bufLen);
+    }
+
+    @Override
+    protected void receiveResponse() {
+      if (shouldCloseConnection.get()) {
+        return;
+      }
+      touch();
+
+      try {
+        int id = in.readInt();                    // try to read an id
+
+        if (LOG.isDebugEnabled())
+          LOG.debug(getName() + " got value #" + id);
+
+        Call call = calls.remove(id);
+
+        int state = in.readInt();     // read call status
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("call #"+id+" state is " + state);
+        }
+        if (state == Status.SUCCESS.state) {
+          Writable value = ReflectionUtils.newInstance(valueClass, conf);
+          value.readFields(in);                 // read value
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("call #"+id+", response is:\n"+value.toString());
+          }
+          call.setValue(value);
+        } else if (state == Status.ERROR.state) {
+          call.setException(new RemoteException(WritableUtils.readString(in),
+                                                WritableUtils.readString(in)));
+        } else if (state == Status.FATAL.state) {
+          // Close the connection
+          markClosed(new RemoteException(WritableUtils.readString(in),
+                                         WritableUtils.readString(in)));
+        }
+      } catch (IOException e) {
+        if (e instanceof SocketTimeoutException && remoteId.rpcTimeout > 0) {
+          // Clean up open calls but don't treat this as a fatal condition,
+          // since we expect certain responses to not make it by the specified
+          // {@link ConnectionId#rpcTimeout}.
+          closeException = e;
+        } else {
+          // Since the server did not respond within the default ping interval
+          // time, treat this as a fatal condition and close this connection
+          markClosed(e);
+        }
+      } finally {
+        if (remoteId.rpcTimeout > 0) {
+          cleanupCalls(remoteId.rpcTimeout);
+        }
+      }
+    }
+
+    /** Close the connection. */
+    protected synchronized void close() {
+      if (!shouldCloseConnection.get()) {
+        LOG.error("The connection is not in the closed state");
+        return;
+      }
+
+      // release the resources
+      // first thing to do;take the connection out of the connection list
+      synchronized (connections) {
+        if (connections.get(remoteId) == this) {
+          connections.remove(remoteId);
+        }
+      }
+
+      // close the streams and therefore the socket
+      IOUtils.closeStream(out);
+      IOUtils.closeStream(in);
+      disposeSasl();
+
+      // clean up all calls
+      if (closeException == null) {
+        if (!calls.isEmpty()) {
+          LOG.warn(
+              "A connection is closed for no cause and calls are not empty");
+
+          // clean up calls anyway
+          closeException = new IOException("Unexpected closed connection");
+          cleanupCalls();
+        }
+      } else {
+        // log the info
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("closing ipc connection to " + server + ": " +
+              closeException.getMessage(),closeException);
+        }
+
+        // cleanup calls
+        cleanupCalls();
+      }
+      if (LOG.isDebugEnabled())
+        LOG.debug(getName() + ": closed");
+    }
+  }
+
+  /**
+   * Construct an IPC client whose values are of the given {@link org.apache.hadoop.io.Writable}
+   * class.
+   * @param valueClass value class
+   * @param conf configuration
+   * @param factory socket factory
+   */
+  public SecureClient(Class<? extends Writable> valueClass, Configuration conf,
+      SocketFactory factory) {
+    super(valueClass, conf, factory);
+  }
+
+  /**
+   * Construct an IPC client with the default SocketFactory
+   * @param valueClass value class
+   * @param conf configuration
+   */
+  public SecureClient(Class<? extends Writable> valueClass, Configuration conf) {
+    this(valueClass, conf, NetUtils.getDefaultSocketFactory(conf));
+  }
+
+  @Override
+  protected SecureConnection getConnection(InetSocketAddress addr,
+                                   Class<? extends VersionedProtocol> protocol,
+                                   User ticket,
+                                   int rpcTimeout,
+                                   Call call)
+                                   throws IOException, InterruptedException {
+    if (!running.get()) {
+      // the client is stopped
+      throw new IOException("The client is stopped");
+    }
+    SecureConnection connection;
+    /* we could avoid this allocation for each RPC by having a
+     * connectionsId object and with set() method. We need to manage the
+     * refs for keys in HashMap properly. For now its ok.
+     */
+    ConnectionId remoteId = new ConnectionId(addr, protocol, ticket, rpcTimeout);
+    do {
+      synchronized (connections) {
+        connection = (SecureConnection)connections.get(remoteId);
+        if (connection == null) {
+          connection = new SecureConnection(remoteId);
+          connections.put(remoteId, connection);
+        }
+      }
+    } while (!connection.addCall(call));
+
+    //we don't invoke the method below inside "synchronized (connections)"
+    //block above. The reason for that is if the server happens to be slow,
+    //it will take longer to establish a connection and that will slow the
+    //entire system down.
+    connection.setupIOstreams();
+    return connection;
+  }
+}
\ No newline at end of file
diff --git security/src/main/java/org/apache/hadoop/hbase/ipc/SecureConnectionHeader.java security/src/main/java/org/apache/hadoop/hbase/ipc/SecureConnectionHeader.java
new file mode 100644
index 0000000..5060821
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/ipc/SecureConnectionHeader.java
@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.ipc;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.AuthMethod;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.security.UserGroupInformation;
+
+/**
+ * The IPC connection header sent by the client to the server
+ * on connection establishment.  Part of the {@link SecureRpcEngine}
+ * implementation.
+ */
+class SecureConnectionHeader extends ConnectionHeader {
+  private User user = null;
+  private AuthMethod authMethod;
+
+  public SecureConnectionHeader() {}
+
+  /**
+   * Create a new {@link org.apache.hadoop.hbase.ipc.SecureConnectionHeader} with the given <code>protocol</code>
+   * and {@link org.apache.hadoop.security.UserGroupInformation}.
+   * @param protocol protocol used for communication between the IPC client
+   *                 and the server
+   * @param ugi {@link org.apache.hadoop.security.UserGroupInformation} of the client communicating with
+   *            the server
+   */
+  public SecureConnectionHeader(String protocol, User user, AuthMethod authMethod) {
+    this.protocol = protocol;
+    this.user = user;
+    this.authMethod = authMethod;
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    protocol = Text.readString(in);
+    if (protocol.isEmpty()) {
+      protocol = null;
+    }
+    boolean ugiUsernamePresent = in.readBoolean();
+    if (ugiUsernamePresent) {
+      String username = in.readUTF();
+      boolean realUserNamePresent = in.readBoolean();
+      if (realUserNamePresent) {
+        String realUserName = in.readUTF();
+        UserGroupInformation realUserUgi =
+            UserGroupInformation.createRemoteUser(realUserName);
+        user = User.create(
+            UserGroupInformation.createProxyUser(username, realUserUgi));
+      } else {
+        user = User.create(UserGroupInformation.createRemoteUser(username));
+      }
+    } else {
+      user = null;
+    }
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    Text.writeString(out, (protocol == null) ? "" : protocol);
+    if (user != null) {
+      UserGroupInformation ugi = user.getUGI();
+      if (authMethod == AuthMethod.KERBEROS) {
+        // Send effective user for Kerberos auth
+        out.writeBoolean(true);
+        out.writeUTF(ugi.getUserName());
+        out.writeBoolean(false);
+      } else if (authMethod == AuthMethod.DIGEST) {
+        // Don't send user for token auth
+        out.writeBoolean(false);
+      } else {
+        //Send both effective user and real user for simple auth
+        out.writeBoolean(true);
+        out.writeUTF(ugi.getUserName());
+        if (ugi.getRealUser() != null) {
+          out.writeBoolean(true);
+          out.writeUTF(ugi.getRealUser().getUserName());
+        } else {
+          out.writeBoolean(false);
+        }
+      }
+    } else {
+      out.writeBoolean(false);
+    }
+  }
+
+  public String getProtocol() {
+    return protocol;
+  }
+
+  public User getUser() {
+    return user;
+  }
+
+  public String toString() {
+    return protocol + "-" + user;
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java
new file mode 100644
index 0000000..8219bea
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java
@@ -0,0 +1,413 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.ipc;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.client.RetriesExhaustedException;
+import org.apache.hadoop.hbase.io.HbaseObjectWritable;
+import org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler;
+import org.apache.hadoop.hbase.security.HBasePolicyProvider;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager;
+import org.apache.hadoop.hbase.util.Objects;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.metrics.util.MetricsTimeVaryingRate;
+import org.apache.hadoop.net.NetUtils;
+import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;
+
+import javax.net.SocketFactory;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.lang.reflect.*;
+import java.net.ConnectException;
+import java.net.InetSocketAddress;
+import java.net.SocketTimeoutException;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * A loadable RPC engine supporting SASL authentication of connections, using
+ * GSSAPI for Kerberos authentication or DIGEST-MD5 for authentication via
+ * signed tokens.
+ *
+ * <p>
+ * This is a fork of the {@code org.apache.hadoop.ipc.WriteableRpcEngine} from
+ * secure Hadoop, reworked to eliminate code duplication with the existing
+ * HBase {@link WritableRpcEngine}.
+ * </p>
+ *
+ * @see SecureClient
+ * @see SecureServer
+ */
+public class SecureRpcEngine implements RpcEngine {
+  // Leave this out in the hadoop ipc package but keep class name.  Do this
+  // so that we dont' get the logging of this class's invocations by doing our
+  // blanket enabling DEBUG on the o.a.h.h. package.
+  protected static final Log LOG =
+    LogFactory.getLog("org.apache.hadoop.ipc.SecureRpcEngine");
+
+  private SecureRpcEngine() {
+    super();
+  }                                  // no public ctor
+
+  /* Cache a client using its socket factory as the hash key */
+  static private class ClientCache {
+    private Map<SocketFactory, SecureClient> clients =
+      new HashMap<SocketFactory, SecureClient>();
+
+    protected ClientCache() {}
+
+    /**
+     * Construct & cache an IPC client with the user-provided SocketFactory
+     * if no cached client exists.
+     *
+     * @param conf Configuration
+     * @param factory socket factory
+     * @return an IPC client
+     */
+    protected synchronized SecureClient getClient(Configuration conf,
+        SocketFactory factory) {
+      // Construct & cache client.  The configuration is only used for timeout,
+      // and Clients have connection pools.  So we can either (a) lose some
+      // connection pooling and leak sockets, or (b) use the same timeout for all
+      // configurations.  Since the IPC is usually intended globally, not
+      // per-job, we choose (a).
+      SecureClient client = clients.get(factory);
+      if (client == null) {
+        // Make an hbase client instead of hadoop Client.
+        client = new SecureClient(HbaseObjectWritable.class, conf, factory);
+        clients.put(factory, client);
+      } else {
+        client.incCount();
+      }
+      return client;
+    }
+
+    /**
+     * Construct & cache an IPC client with the default SocketFactory
+     * if no cached client exists.
+     *
+     * @param conf Configuration
+     * @return an IPC client
+     */
+    protected synchronized SecureClient getClient(Configuration conf) {
+      return getClient(conf, SocketFactory.getDefault());
+    }
+
+    /**
+     * Stop a RPC client connection
+     * A RPC client is closed only when its reference count becomes zero.
+     * @param client client to stop
+     */
+    protected void stopClient(SecureClient client) {
+      synchronized (this) {
+        client.decCount();
+        if (client.isZeroReference()) {
+          clients.remove(client.getSocketFactory());
+        }
+      }
+      if (client.isZeroReference()) {
+        client.stop();
+      }
+    }
+  }
+
+  protected final static ClientCache CLIENTS = new ClientCache();
+
+  private static class Invoker implements InvocationHandler {
+    private Class<? extends VersionedProtocol> protocol;
+    private InetSocketAddress address;
+    private User ticket;
+    private SecureClient client;
+    private boolean isClosed = false;
+    final private int rpcTimeout;
+
+    public Invoker(Class<? extends VersionedProtocol> protocol,
+        InetSocketAddress address, User ticket,
+        Configuration conf, SocketFactory factory, int rpcTimeout) {
+      this.protocol = protocol;
+      this.address = address;
+      this.ticket = ticket;
+      this.client = CLIENTS.getClient(conf, factory);
+      this.rpcTimeout = rpcTimeout;
+    }
+
+    public Object invoke(Object proxy, Method method, Object[] args)
+        throws Throwable {
+      final boolean logDebug = LOG.isDebugEnabled();
+      long startTime = 0;
+      if (logDebug) {
+        startTime = System.currentTimeMillis();
+      }
+      HbaseObjectWritable value = (HbaseObjectWritable)
+        client.call(new Invocation(method, args), address,
+                    protocol, ticket, rpcTimeout);
+      if (logDebug) {
+        long callTime = System.currentTimeMillis() - startTime;
+        LOG.debug("Call: " + method.getName() + " " + callTime);
+      }
+      return value.get();
+    }
+
+    /* close the IPC client that's responsible for this invoker's RPCs */
+    synchronized protected void close() {
+      if (!isClosed) {
+        isClosed = true;
+        CLIENTS.stopClient(client);
+      }
+    }
+  }
+
+  /**
+   * Construct a client-side proxy object that implements the named protocol,
+   * talking to a server at the named address.
+   *
+   * @param protocol interface
+   * @param clientVersion version we are expecting
+   * @param addr remote address
+   * @param ticket ticket
+   * @param conf configuration
+   * @param factory socket factory
+   * @return proxy
+   * @throws java.io.IOException e
+   */
+  public VersionedProtocol getProxy(
+      Class<? extends VersionedProtocol> protocol, long clientVersion,
+      InetSocketAddress addr, User ticket,
+      Configuration conf, SocketFactory factory, int rpcTimeout)
+  throws IOException {
+    if (User.isSecurityEnabled()) {
+      HBaseSaslRpcServer.init(conf);
+    }
+    VersionedProtocol proxy =
+        (VersionedProtocol) Proxy.newProxyInstance(
+            protocol.getClassLoader(), new Class[] { protocol },
+            new Invoker(protocol, addr, ticket, conf, factory, rpcTimeout));
+    long serverVersion = proxy.getProtocolVersion(protocol.getName(),
+                                                  clientVersion);
+    if (serverVersion != clientVersion) {
+      throw new HBaseRPC.VersionMismatch(protocol.getName(), clientVersion,
+                                serverVersion);
+    }
+    return proxy;
+  }
+
+  /**
+   * Stop this proxy and release its invoker's resource
+   * @param proxy the proxy to be stopped
+   */
+  public void stopProxy(VersionedProtocol proxy) {
+    if (proxy!=null) {
+      ((Invoker)Proxy.getInvocationHandler(proxy)).close();
+    }
+  }
+
+
+  /** Expert: Make multiple, parallel calls to a set of servers. */
+  public Object[] call(Method method, Object[][] params,
+                       InetSocketAddress[] addrs,
+                       Class<? extends VersionedProtocol> protocol,
+                       User ticket, Configuration conf)
+    throws IOException, InterruptedException {
+
+    Invocation[] invocations = new Invocation[params.length];
+    for (int i = 0; i < params.length; i++)
+      invocations[i] = new Invocation(method, params[i]);
+    SecureClient client = CLIENTS.getClient(conf);
+    try {
+      Writable[] wrappedValues =
+        client.call(invocations, addrs, protocol, ticket);
+
+      if (method.getReturnType() == Void.TYPE) {
+        return null;
+      }
+
+      Object[] values =
+          (Object[])Array.newInstance(method.getReturnType(), wrappedValues.length);
+      for (int i = 0; i < values.length; i++)
+        if (wrappedValues[i] != null)
+          values[i] = ((HbaseObjectWritable)wrappedValues[i]).get();
+
+      return values;
+    } finally {
+      CLIENTS.stopClient(client);
+    }
+  }
+
+  /** Construct a server for a protocol implementation instance listening on a
+   * port and address, with a secret manager. */
+  public Server getServer(Class<? extends VersionedProtocol> protocol,
+      final Object instance,
+      Class<?>[] ifaces,
+      final String bindAddress, final int port,
+      final int numHandlers,
+      int metaHandlerCount, final boolean verbose, Configuration conf,
+       int highPriorityLevel)
+    throws IOException {
+    Server server = new Server(instance, ifaces, conf, bindAddress, port,
+            numHandlers, metaHandlerCount, verbose,
+            highPriorityLevel);
+    return server;
+  }
+
+  /** An RPC Server. */
+  public static class Server extends SecureServer {
+    private Object instance;
+    private Class<?> implementation;
+    private Class<?>[] ifaces;
+    private boolean verbose;
+
+    private static String classNameBase(String className) {
+      String[] names = className.split("\\.", -1);
+      if (names == null || names.length == 0) {
+        return className;
+      }
+      return names[names.length-1];
+    }
+
+    /** Construct an RPC server.
+     * @param instance the instance whose methods will be called
+     * @param conf the configuration to use
+     * @param bindAddress the address to bind on to listen for connection
+     * @param port the port to listen for connections on
+     * @param numHandlers the number of method handler threads to run
+     * @param verbose whether each call should be logged
+     * @throws java.io.IOException e
+     */
+    public Server(Object instance, final Class<?>[] ifaces,
+                  Configuration conf, String bindAddress,  int port,
+                  int numHandlers, int metaHandlerCount, boolean verbose,
+                  int highPriorityLevel)
+        throws IOException {
+      super(bindAddress, port, Invocation.class, numHandlers, metaHandlerCount, conf,
+          classNameBase(instance.getClass().getName()), highPriorityLevel);
+      this.instance = instance;
+      this.implementation = instance.getClass();
+      this.verbose = verbose;
+
+      this.ifaces = ifaces;
+
+      // create metrics for the advertised interfaces this server implements.
+      this.rpcMetrics.createMetrics(this.ifaces);
+    }
+
+    public AuthenticationTokenSecretManager createSecretManager(){
+      if (instance instanceof org.apache.hadoop.hbase.Server) {
+        org.apache.hadoop.hbase.Server server =
+            (org.apache.hadoop.hbase.Server)instance;
+        Configuration conf = server.getConfiguration();
+        long keyUpdateInterval =
+            conf.getLong("hbase.auth.key.update.interval", 24*60*60*1000);
+        long maxAge =
+            conf.getLong("hbase.auth.token.max.lifetime", 7*24*60*60*1000);
+        return new AuthenticationTokenSecretManager(conf, server.getZooKeeper(),
+            server.getServerName().toString(), keyUpdateInterval, maxAge);
+      }
+      return null;
+    }
+
+    @Override
+    public void startThreads() {
+      AuthenticationTokenSecretManager mgr = createSecretManager();
+      if (mgr != null) {
+        setSecretManager(mgr);
+        mgr.start();
+      }
+      this.authManager = new ServiceAuthorizationManager();
+      HBasePolicyProvider.init(conf, authManager);
+
+      // continue with base startup
+      super.startThreads();
+    }
+
+    @Override
+    public Writable call(Class<? extends VersionedProtocol> protocol,
+        Writable param, long receivedTime, MonitoredRPCHandler status)
+    throws IOException {
+      try {
+        Invocation call = (Invocation)param;
+        if(call.getMethodName() == null) {
+          throw new IOException("Could not find requested method, the usual " +
+              "cause is a version mismatch between client and server.");
+        }
+        if (verbose) log("Call: " + call);
+
+        Method method =
+          protocol.getMethod(call.getMethodName(),
+                                   call.getParameterClasses());
+        method.setAccessible(true);
+
+        Object impl = null;
+        if (protocol.isAssignableFrom(this.implementation)) {
+          impl = this.instance;
+        }
+        else {
+          throw new HBaseRPC.UnknownProtocolException(protocol);
+        }
+
+        long startTime = System.currentTimeMillis();
+        Object[] params = call.getParameters();
+        Object value = method.invoke(impl, params);
+        int processingTime = (int) (System.currentTimeMillis() - startTime);
+        int qTime = (int) (startTime-receivedTime);
+        if (TRACELOG.isDebugEnabled()) {
+          TRACELOG.debug("Call #" + CurCall.get().id +
+              "; Served: " + protocol.getSimpleName()+"#"+call.getMethodName() +
+              " queueTime=" + qTime +
+              " processingTime=" + processingTime +
+              " contents=" + Objects.describeQuantity(params));
+        }
+        rpcMetrics.rpcQueueTime.inc(qTime);
+        rpcMetrics.rpcProcessingTime.inc(processingTime);
+        rpcMetrics.inc(call.getMethodName(), processingTime);
+        if (verbose) log("Return: "+value);
+
+        return new HbaseObjectWritable(method.getReturnType(), value);
+      } catch (InvocationTargetException e) {
+        Throwable target = e.getTargetException();
+        if (target instanceof IOException) {
+          throw (IOException)target;
+        }
+        IOException ioe = new IOException(target.toString());
+        ioe.setStackTrace(target.getStackTrace());
+        throw ioe;
+      } catch (Throwable e) {
+        if (!(e instanceof IOException)) {
+          LOG.error("Unexpected throwable object ", e);
+        }
+        IOException ioe = new IOException(e.toString());
+        ioe.setStackTrace(e.getStackTrace());
+        throw ioe;
+      }
+    }
+  }
+
+  protected static void log(String value) {
+    String v = value;
+    if (v != null && v.length() > 55)
+      v = v.substring(0, 55)+"...";
+    LOG.info(v);
+  }
+}
\ No newline at end of file
diff --git security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java
new file mode 100644
index 0000000..0766f5d
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java
@@ -0,0 +1,728 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.ipc;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.io.HbaseObjectWritable;
+import org.apache.hadoop.hbase.io.WritableWithSize;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.AuthMethod;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.SaslDigestCallbackHandler;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.SaslGssCallbackHandler;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.SaslStatus;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.util.ByteBufferOutputStream;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.security.AccessControlException;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
+import org.apache.hadoop.security.authorize.AuthorizationException;
+import org.apache.hadoop.security.authorize.ProxyUsers;
+import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;
+import org.apache.hadoop.security.token.SecretManager;
+import org.apache.hadoop.security.token.SecretManager.InvalidToken;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.util.StringUtils;
+
+import javax.security.sasl.Sasl;
+import javax.security.sasl.SaslException;
+import javax.security.sasl.SaslServer;
+import java.io.*;
+import java.net.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.*;
+import java.security.PrivilegedExceptionAction;
+import java.util.*;
+
+import static org.apache.hadoop.fs.CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION;
+
+/**
+ * An abstract IPC service, supporting SASL authentication of connections,
+ * using GSSAPI for Kerberos authentication or DIGEST-MD5 for authentication
+ * via signed tokens.
+ *
+ * <p>
+ * This is part of the {@link SecureRpcEngine} implementation.
+ * </p>
+ *
+ * @see org.apache.hadoop.hbase.ipc.SecureClient
+ */
+public abstract class SecureServer extends HBaseServer {
+  private final boolean authorize;
+  private boolean isSecurityEnabled;
+
+  /**
+   * The first four bytes of secure RPC connections
+   */
+  public static final ByteBuffer HEADER = ByteBuffer.wrap("srpc".getBytes());
+
+  // 1 : Introduce ping and server does not throw away RPCs
+  // 3 : Introduce the protocol into the RPC connection header
+  // 4 : Introduced SASL security layer
+  public static final byte CURRENT_VERSION = 4;
+
+  public static final Log LOG = LogFactory.getLog("org.apache.hadoop.ipc.SecureServer");
+  private static final Log AUDITLOG =
+    LogFactory.getLog("SecurityLogger.org.apache.hadoop.ipc.SecureServer");
+  private static final String AUTH_FAILED_FOR = "Auth failed for ";
+  private static final String AUTH_SUCCESSFUL_FOR = "Auth successful for ";
+
+  protected SecretManager<TokenIdentifier> secretManager;
+  protected ServiceAuthorizationManager authManager;
+
+  protected class SecureCall extends HBaseServer.Call {
+    public SecureCall(int id, Writable param, Connection connection,
+        Responder responder) {
+      super(id, param, connection, responder);
+    }
+
+    @Override
+    protected synchronized void setResponse(Object value, Status status,
+        String errorClass, String error) {
+      Writable result = null;
+      if (value instanceof Writable) {
+        result = (Writable) value;
+      } else {
+        /* We might have a null value and errors. Avoid creating a
+         * HbaseObjectWritable, because the constructor fails on null. */
+        if (value != null) {
+          result = new HbaseObjectWritable(value);
+        }
+      }
+
+      int size = BUFFER_INITIAL_SIZE;
+      if (result instanceof WritableWithSize) {
+        // get the size hint.
+        WritableWithSize ohint = (WritableWithSize) result;
+        long hint = ohint.getWritableSize() + Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+        if (hint > Integer.MAX_VALUE) {
+          // oops, new problem.
+          IOException ioe =
+            new IOException("Result buffer size too large: " + hint);
+          errorClass = ioe.getClass().getName();
+          error = StringUtils.stringifyException(ioe);
+        } else {
+          size = (int)hint;
+        }
+      }
+
+      ByteBufferOutputStream buf = new ByteBufferOutputStream(size);
+      DataOutputStream out = new DataOutputStream(buf);
+      try {
+        out.writeInt(this.id);                // write call id
+        out.writeInt(status.state);           // write status
+      } catch (IOException e) {
+        errorClass = e.getClass().getName();
+        error = StringUtils.stringifyException(e);
+      }
+
+      try {
+        if (status == Status.SUCCESS) {
+          result.write(out);
+        } else {
+          WritableUtils.writeString(out, errorClass);
+          WritableUtils.writeString(out, error);
+        }
+        if (((SecureConnection)connection).useWrap) {
+          wrapWithSasl(buf);
+        }
+      } catch (IOException e) {
+        LOG.warn("Error sending response to call: ", e);
+      }
+
+      this.response = buf.getByteBuffer();
+    }
+
+    private void wrapWithSasl(ByteBufferOutputStream response)
+        throws IOException {
+      if (((SecureConnection)connection).useSasl) {
+        // getByteBuffer calls flip()
+        ByteBuffer buf = response.getByteBuffer();
+        byte[] token;
+        // synchronization may be needed since there can be multiple Handler
+        // threads using saslServer to wrap responses.
+        synchronized (((SecureConnection)connection).saslServer) {
+          token = ((SecureConnection)connection).saslServer.wrap(buf.array(),
+              buf.arrayOffset(), buf.remaining());
+        }
+        if (LOG.isDebugEnabled())
+          LOG.debug("Adding saslServer wrapped token of size " + token.length
+              + " as call response.");
+        buf.clear();
+        DataOutputStream saslOut = new DataOutputStream(response);
+        saslOut.writeInt(token.length);
+        saslOut.write(token, 0, token.length);
+      }
+    }
+  }
+
+  /** Reads calls from a connection and queues them for handling. */
+  public class SecureConnection extends HBaseServer.Connection  {
+    private boolean rpcHeaderRead = false; // if initial rpc header is read
+    private boolean headerRead = false;  //if the connection header that
+                                         //follows version is read.
+    private ByteBuffer data;
+    private ByteBuffer dataLengthBuffer;
+    protected final LinkedList<SecureCall> responseQueue;
+    private int dataLength;
+    private InetAddress addr;
+
+    boolean useSasl;
+    SaslServer saslServer;
+    private AuthMethod authMethod;
+    private boolean saslContextEstablished;
+    private boolean skipInitialSaslHandshake;
+    private ByteBuffer rpcHeaderBuffer;
+    private ByteBuffer unwrappedData;
+    private ByteBuffer unwrappedDataLengthBuffer;
+
+    public UserGroupInformation attemptingUser = null; // user name before auth
+
+    // Fake 'call' for failed authorization response
+    private final int AUTHORIZATION_FAILED_CALLID = -1;
+    // Fake 'call' for SASL context setup
+    private static final int SASL_CALLID = -33;
+    private final SecureCall saslCall = new SecureCall(SASL_CALLID, null, this, null);
+
+    private boolean useWrap = false;
+
+    public SecureConnection(SocketChannel channel, long lastContact) {
+      super(channel, lastContact);
+      this.header = new SecureConnectionHeader();
+      this.channel = channel;
+      this.data = null;
+      this.dataLengthBuffer = ByteBuffer.allocate(4);
+      this.unwrappedData = null;
+      this.unwrappedDataLengthBuffer = ByteBuffer.allocate(4);
+      this.socket = channel.socket();
+      this.addr = socket.getInetAddress();
+      this.responseQueue = new LinkedList<SecureCall>();
+    }
+
+    @Override
+    public String toString() {
+      return getHostAddress() + ":" + remotePort;
+    }
+
+    public String getHostAddress() {
+      return hostAddress;
+    }
+
+    public InetAddress getHostInetAddress() {
+      return addr;
+    }
+
+    private User getAuthorizedUgi(String authorizedId)
+        throws IOException {
+      if (authMethod == AuthMethod.DIGEST) {
+        TokenIdentifier tokenId = HBaseSaslRpcServer.getIdentifier(authorizedId,
+            secretManager);
+        UserGroupInformation ugi = tokenId.getUser();
+        if (ugi == null) {
+          throw new AccessControlException(
+              "Can't retrieve username from tokenIdentifier.");
+        }
+        ugi.addTokenIdentifier(tokenId);
+        return User.create(ugi);
+      } else {
+        return User.create(UserGroupInformation.createRemoteUser(authorizedId));
+      }
+    }
+
+    private void saslReadAndProcess(byte[] saslToken) throws IOException,
+        InterruptedException {
+      if (!saslContextEstablished) {
+        byte[] replyToken = null;
+        try {
+          if (saslServer == null) {
+            switch (authMethod) {
+            case DIGEST:
+              if (secretManager == null) {
+                throw new AccessControlException(
+                    "Server is not configured to do DIGEST authentication.");
+              }
+              saslServer = Sasl.createSaslServer(AuthMethod.DIGEST
+                  .getMechanismName(), null, HBaseSaslRpcServer.SASL_DEFAULT_REALM,
+                  HBaseSaslRpcServer.SASL_PROPS, new SaslDigestCallbackHandler(
+                      secretManager, this));
+              break;
+            default:
+              UserGroupInformation current = UserGroupInformation
+                  .getCurrentUser();
+              String fullName = current.getUserName();
+              if (LOG.isDebugEnabled())
+                LOG.debug("Kerberos principal name is " + fullName);
+              final String names[] = HBaseSaslRpcServer.splitKerberosName(fullName);
+              if (names.length != 3) {
+                throw new AccessControlException(
+                    "Kerberos principal name does NOT have the expected "
+                        + "hostname part: " + fullName);
+              }
+              current.doAs(new PrivilegedExceptionAction<Object>() {
+                @Override
+                public Object run() throws SaslException {
+                  saslServer = Sasl.createSaslServer(AuthMethod.KERBEROS
+                      .getMechanismName(), names[0], names[1],
+                      HBaseSaslRpcServer.SASL_PROPS, new SaslGssCallbackHandler());
+                  return null;
+                }
+              });
+            }
+            if (saslServer == null)
+              throw new AccessControlException(
+                  "Unable to find SASL server implementation for "
+                      + authMethod.getMechanismName());
+            if (LOG.isDebugEnabled())
+              LOG.debug("Created SASL server with mechanism = "
+                  + authMethod.getMechanismName());
+          }
+          if (LOG.isDebugEnabled())
+            LOG.debug("Have read input token of size " + saslToken.length
+                + " for processing by saslServer.evaluateResponse()");
+          replyToken = saslServer.evaluateResponse(saslToken);
+        } catch (IOException e) {
+          IOException sendToClient = e;
+          Throwable cause = e;
+          while (cause != null) {
+            if (cause instanceof InvalidToken) {
+              sendToClient = (InvalidToken) cause;
+              break;
+            }
+            cause = cause.getCause();
+          }
+          doSaslReply(SaslStatus.ERROR, null, sendToClient.getClass().getName(),
+              sendToClient.getLocalizedMessage());
+          rpcMetrics.authenticationFailures.inc();
+          String clientIP = this.toString();
+          // attempting user could be null
+          AUDITLOG.warn(AUTH_FAILED_FOR + clientIP + ":" + attemptingUser);
+          throw e;
+        }
+        if (replyToken != null) {
+          if (LOG.isDebugEnabled())
+            LOG.debug("Will send token of size " + replyToken.length
+                + " from saslServer.");
+          doSaslReply(SaslStatus.SUCCESS, new BytesWritable(replyToken), null,
+              null);
+        }
+        if (saslServer.isComplete()) {
+          LOG.debug("SASL server context established. Negotiated QoP is "
+              + saslServer.getNegotiatedProperty(Sasl.QOP));
+          String qop = (String) saslServer.getNegotiatedProperty(Sasl.QOP);
+          useWrap = qop != null && !"auth".equalsIgnoreCase(qop);
+          ticket = getAuthorizedUgi(saslServer.getAuthorizationID());
+          LOG.debug("SASL server successfully authenticated client: " + ticket);
+          rpcMetrics.authenticationSuccesses.inc();
+          AUDITLOG.trace(AUTH_SUCCESSFUL_FOR + ticket);
+          saslContextEstablished = true;
+        }
+      } else {
+        if (LOG.isDebugEnabled())
+          LOG.debug("Have read input token of size " + saslToken.length
+              + " for processing by saslServer.unwrap()");
+
+        if (!useWrap) {
+          processOneRpc(saslToken);
+        } else {
+          byte[] plaintextData = saslServer.unwrap(saslToken, 0,
+              saslToken.length);
+          processUnwrappedData(plaintextData);
+        }
+      }
+    }
+
+    private void doSaslReply(SaslStatus status, Writable rv,
+        String errorClass, String error) throws IOException {
+      saslCall.setResponse(rv,
+          status == SaslStatus.SUCCESS ? Status.SUCCESS : Status.ERROR,
+           errorClass, error);
+      saslCall.responder = responder;
+      saslCall.sendResponseIfReady();
+    }
+
+    private void disposeSasl() {
+      if (saslServer != null) {
+        try {
+          saslServer.dispose();
+        } catch (SaslException ignored) {
+        }
+      }
+    }
+
+    public int readAndProcess() throws IOException, InterruptedException {
+      while (true) {
+        /* Read at most one RPC. If the header is not read completely yet
+         * then iterate until we read first RPC or until there is no data left.
+         */
+        int count = -1;
+        if (dataLengthBuffer.remaining() > 0) {
+          count = channelRead(channel, dataLengthBuffer);
+          if (count < 0 || dataLengthBuffer.remaining() > 0)
+            return count;
+        }
+
+        if (!rpcHeaderRead) {
+          //Every connection is expected to send the header.
+          if (rpcHeaderBuffer == null) {
+            rpcHeaderBuffer = ByteBuffer.allocate(2);
+          }
+          count = channelRead(channel, rpcHeaderBuffer);
+          if (count < 0 || rpcHeaderBuffer.remaining() > 0) {
+            return count;
+          }
+          int version = rpcHeaderBuffer.get(0);
+          byte[] method = new byte[] {rpcHeaderBuffer.get(1)};
+          authMethod = AuthMethod.read(new DataInputStream(
+              new ByteArrayInputStream(method)));
+          dataLengthBuffer.flip();
+          if (!HEADER.equals(dataLengthBuffer) || version != CURRENT_VERSION) {
+            //Warning is ok since this is not supposed to happen.
+            LOG.warn("Incorrect header or version mismatch from " +
+                     hostAddress + ":" + remotePort +
+                     " got version " + version +
+                     " expected version " + CURRENT_VERSION);
+            return -1;
+          }
+          dataLengthBuffer.clear();
+          if (authMethod == null) {
+            throw new IOException("Unable to read authentication method");
+          }
+          if (isSecurityEnabled && authMethod == AuthMethod.SIMPLE) {
+            AccessControlException ae = new AccessControlException(
+                "Authentication is required");
+            SecureCall failedCall = new SecureCall(AUTHORIZATION_FAILED_CALLID, null, this,
+                null);
+            failedCall.setResponse(null, Status.FATAL, ae.getClass().getName(),
+                ae.getMessage());
+            responder.doRespond(failedCall);
+            throw ae;
+          }
+          if (!isSecurityEnabled && authMethod != AuthMethod.SIMPLE) {
+            doSaslReply(SaslStatus.SUCCESS, new IntWritable(
+                HBaseSaslRpcServer.SWITCH_TO_SIMPLE_AUTH), null, null);
+            authMethod = AuthMethod.SIMPLE;
+            // client has already sent the initial Sasl message and we
+            // should ignore it. Both client and server should fall back
+            // to simple auth from now on.
+            skipInitialSaslHandshake = true;
+          }
+          if (authMethod != AuthMethod.SIMPLE) {
+            useSasl = true;
+          }
+
+          rpcHeaderBuffer = null;
+          rpcHeaderRead = true;
+          continue;
+        }
+
+        if (data == null) {
+          dataLengthBuffer.flip();
+          dataLength = dataLengthBuffer.getInt();
+
+          if (dataLength == HBaseClient.PING_CALL_ID) {
+            if(!useWrap) { //covers the !useSasl too
+              dataLengthBuffer.clear();
+              return 0;  //ping message
+            }
+          }
+          if (dataLength < 0) {
+            LOG.warn("Unexpected data length " + dataLength + "!! from " +
+                getHostAddress());
+          }
+          data = ByteBuffer.allocate(dataLength);
+          incRpcCount();  // Increment the rpc count
+        }
+
+        count = channelRead(channel, data);
+
+        if (data.remaining() == 0) {
+          dataLengthBuffer.clear();
+          data.flip();
+          if (skipInitialSaslHandshake) {
+            data = null;
+            skipInitialSaslHandshake = false;
+            continue;
+          }
+          boolean isHeaderRead = headerRead;
+          if (useSasl) {
+            saslReadAndProcess(data.array());
+          } else {
+            processOneRpc(data.array());
+          }
+          data = null;
+          if (!isHeaderRead) {
+            continue;
+          }
+        }
+        return count;
+      }
+    }
+
+    /// Reads the connection header following version
+    private void processHeader(byte[] buf) throws IOException {
+      DataInputStream in =
+        new DataInputStream(new ByteArrayInputStream(buf));
+      header.readFields(in);
+      try {
+        String protocolClassName = header.getProtocol();
+        if (protocolClassName != null) {
+          protocol = getProtocolClass(header.getProtocol(), conf);
+        }
+      } catch (ClassNotFoundException cnfe) {
+        throw new IOException("Unknown protocol: " + header.getProtocol());
+      }
+
+      User protocolUser = header.getUser();
+      if (!useSasl) {
+        ticket = protocolUser;
+        if (ticket != null) {
+          ticket.getUGI().setAuthenticationMethod(AuthMethod.SIMPLE.authenticationMethod);
+        }
+      } else {
+        // user is authenticated
+        ticket.getUGI().setAuthenticationMethod(authMethod.authenticationMethod);
+        //Now we check if this is a proxy user case. If the protocol user is
+        //different from the 'user', it is a proxy user scenario. However,
+        //this is not allowed if user authenticated with DIGEST.
+        if ((protocolUser != null)
+            && (!protocolUser.getName().equals(ticket.getName()))) {
+          if (authMethod == AuthMethod.DIGEST) {
+            // Not allowed to doAs if token authentication is used
+            throw new AccessControlException("Authenticated user (" + ticket
+                + ") doesn't match what the client claims to be ("
+                + protocolUser + ")");
+          } else {
+            // Effective user can be different from authenticated user
+            // for simple auth or kerberos auth
+            // The user is the real user. Now we create a proxy user
+            UserGroupInformation realUser = ticket.getUGI();
+            ticket = User.create(
+                UserGroupInformation.createProxyUser(protocolUser.getName(),
+                    realUser));
+            // Now the user is a proxy user, set Authentication method Proxy.
+            ticket.getUGI().setAuthenticationMethod(AuthenticationMethod.PROXY);
+          }
+        }
+      }
+    }
+
+    private void processUnwrappedData(byte[] inBuf) throws IOException,
+        InterruptedException {
+      ReadableByteChannel ch = Channels.newChannel(new ByteArrayInputStream(
+          inBuf));
+      // Read all RPCs contained in the inBuf, even partial ones
+      while (true) {
+        int count = -1;
+        if (unwrappedDataLengthBuffer.remaining() > 0) {
+          count = channelRead(ch, unwrappedDataLengthBuffer);
+          if (count <= 0 || unwrappedDataLengthBuffer.remaining() > 0)
+            return;
+        }
+
+        if (unwrappedData == null) {
+          unwrappedDataLengthBuffer.flip();
+          int unwrappedDataLength = unwrappedDataLengthBuffer.getInt();
+
+          if (unwrappedDataLength == HBaseClient.PING_CALL_ID) {
+            if (LOG.isDebugEnabled())
+              LOG.debug("Received ping message");
+            unwrappedDataLengthBuffer.clear();
+            continue; // ping message
+          }
+          unwrappedData = ByteBuffer.allocate(unwrappedDataLength);
+        }
+
+        count = channelRead(ch, unwrappedData);
+        if (count <= 0 || unwrappedData.remaining() > 0)
+          return;
+
+        if (unwrappedData.remaining() == 0) {
+          unwrappedDataLengthBuffer.clear();
+          unwrappedData.flip();
+          processOneRpc(unwrappedData.array());
+          unwrappedData = null;
+        }
+      }
+    }
+
+    private void processOneRpc(byte[] buf) throws IOException,
+        InterruptedException {
+      if (headerRead) {
+        processData(buf);
+      } else {
+        processHeader(buf);
+        headerRead = true;
+        if (!authorizeConnection()) {
+          throw new AccessControlException("Connection from " + this
+              + " for protocol " + header.getProtocol()
+              + " is unauthorized for user " + ticket);
+        }
+      }
+    }
+
+    protected void processData(byte[] buf) throws  IOException, InterruptedException {
+      DataInputStream dis =
+        new DataInputStream(new ByteArrayInputStream(buf));
+      int id = dis.readInt();                    // try to read an id
+
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(" got #" + id);
+      }
+
+      Writable param = ReflectionUtils.newInstance(paramClass, conf);           // read param
+      param.readFields(dis);
+
+      SecureCall call = new SecureCall(id, param, this, responder);
+
+      if (priorityCallQueue != null && getQosLevel(param) > highPriorityLevel) {
+        priorityCallQueue.put(call);
+      } else {
+        callQueue.put(call);              // queue the call; maybe blocked here
+      }
+    }
+
+    private boolean authorizeConnection() throws IOException {
+      try {
+        // If auth method is DIGEST, the token was obtained by the
+        // real user for the effective user, therefore not required to
+        // authorize real user. doAs is allowed only for simple or kerberos
+        // authentication
+        if (ticket != null && ticket.getUGI().getRealUser() != null
+            && (authMethod != AuthMethod.DIGEST)) {
+          ProxyUsers.authorize(ticket.getUGI(), this.getHostAddress(), conf);
+        }
+        authorize(ticket, header, getHostInetAddress());
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Successfully authorized " + header);
+        }
+        rpcMetrics.authorizationSuccesses.inc();
+      } catch (AuthorizationException ae) {
+        LOG.debug("Connection authorization failed: "+ae.getMessage(), ae);
+        rpcMetrics.authorizationFailures.inc();
+        SecureCall failedCall = new SecureCall(AUTHORIZATION_FAILED_CALLID, null, this,
+            null);
+        failedCall.setResponse(null, Status.FATAL, ae.getClass().getName(),
+            ae.getMessage());
+        responder.doRespond(failedCall);
+        return false;
+      }
+      return true;
+    }
+
+    protected synchronized void close() {
+      disposeSasl();
+      data = null;
+      dataLengthBuffer = null;
+      if (!channel.isOpen())
+        return;
+      try {socket.shutdownOutput();} catch(Exception ignored) {} // FindBugs DE_MIGHT_IGNORE
+      if (channel.isOpen()) {
+        try {channel.close();} catch(Exception ignored) {}
+      }
+      try {socket.close();} catch(Exception ignored) {}
+    }
+  }
+
+  /** Constructs a server listening on the named port and address.  Parameters passed must
+   * be of the named class.  The <code>handlerCount</handlerCount> determines
+   * the number of handler threads that will be used to process calls.
+   *
+   */
+  @SuppressWarnings("unchecked")
+  protected SecureServer(String bindAddress, int port,
+                  Class<? extends Writable> paramClass, int handlerCount,
+                  int priorityHandlerCount, Configuration conf, String serverName,
+                  int highPriorityLevel)
+    throws IOException {
+    super(bindAddress, port, paramClass, handlerCount, priorityHandlerCount,
+        conf, serverName, highPriorityLevel);
+    this.authorize =
+      conf.getBoolean(HADOOP_SECURITY_AUTHORIZATION, false);
+    this.isSecurityEnabled = UserGroupInformation.isSecurityEnabled();
+    LOG.debug("security enabled="+isSecurityEnabled);
+
+    if (isSecurityEnabled) {
+      HBaseSaslRpcServer.init(conf);
+    }
+  }
+
+  @Override
+  protected Connection getConnection(SocketChannel channel, long time) {
+    return new SecureConnection(channel, time);
+  }
+
+  Configuration getConf() {
+    return conf;
+  }
+
+  /** for unit testing only, should be called before server is started */
+  void disableSecurity() {
+    this.isSecurityEnabled = false;
+  }
+
+  /** for unit testing only, should be called before server is started */
+  void enableSecurity() {
+    this.isSecurityEnabled = true;
+  }
+
+  /** Stops the service.  No new calls will be handled after this is called. */
+  public synchronized void stop() {
+    super.stop();
+  }
+
+  public SecretManager<? extends TokenIdentifier> getSecretManager() {
+    return this.secretManager;
+  }
+
+  public void setSecretManager(SecretManager<? extends TokenIdentifier> secretManager) {
+    this.secretManager = (SecretManager<TokenIdentifier>) secretManager;    
+  }
+
+  /**
+   * Authorize the incoming client connection.
+   *
+   * @param user client user
+   * @param connection incoming connection
+   * @param addr InetAddress of incoming connection
+   * @throws org.apache.hadoop.security.authorize.AuthorizationException when the client isn't authorized to talk the protocol
+   */
+  public void authorize(User user,
+                        ConnectionHeader connection,
+                        InetAddress addr
+                        ) throws AuthorizationException {
+    if (authorize) {
+      Class<?> protocol = null;
+      try {
+        protocol = getProtocolClass(connection.getProtocol(), getConf());
+      } catch (ClassNotFoundException cfne) {
+        throw new AuthorizationException("Unknown protocol: " +
+                                         connection.getProtocol());
+      }
+      authManager.authorize(user != null ? user.getUGI() : null,
+          protocol, getConf(), addr);
+    }
+  }
+}
\ No newline at end of file
diff --git security/src/main/java/org/apache/hadoop/hbase/security/AccessDeniedException.java security/src/main/java/org/apache/hadoop/hbase/security/AccessDeniedException.java
new file mode 100644
index 0000000..b8c5d3b
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/AccessDeniedException.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.security;
+
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+
+/**
+ * Exception thrown by access-related methods.
+ */
+public class AccessDeniedException extends DoNotRetryIOException {
+  private static final long serialVersionUID = 1913879564363001780L;
+
+  public AccessDeniedException() {
+    super();
+  }
+
+  public AccessDeniedException(Class<?> clazz, String s) {
+    super( "AccessDenied [" + clazz.getName() + "]: " + s);
+  }
+
+  public AccessDeniedException(String s) {
+    super(s);
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java security/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java
new file mode 100644
index 0000000..0c4b4cb
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.security;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ipc.HMasterInterface;
+import org.apache.hadoop.hbase.ipc.HMasterRegionInterface;
+import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.security.authorize.PolicyProvider;
+import org.apache.hadoop.security.authorize.Service;
+import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;
+
+/**
+ * Implementation of secure Hadoop policy provider for mapping
+ * protocol interfaces to hbase-policy.xml entries.
+ */
+public class HBasePolicyProvider extends PolicyProvider {
+  protected static Service[] services = {
+      new Service("security.client.protocol.acl", HRegionInterface.class),
+      new Service("security.admin.protocol.acl", HMasterInterface.class),
+      new Service("security.masterregion.protocol.acl", HMasterRegionInterface.class)
+  };
+
+  @Override
+  public Service[] getServices() {
+    return services;
+  }
+
+  public static void init(Configuration conf,
+      ServiceAuthorizationManager authManager) {
+    // set service-level authorization security policy
+    conf.set("hadoop.policy.file", "hbase-policy.xml");
+    if (conf.getBoolean(
+          ServiceAuthorizationManager.SERVICE_AUTHORIZATION_CONFIG, false)) {
+      authManager.refresh(conf, new HBasePolicyProvider());
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java
new file mode 100644
index 0000000..8090973
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcClient.java
@@ -0,0 +1,280 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+
+import javax.security.auth.callback.Callback;
+import javax.security.auth.callback.CallbackHandler;
+import javax.security.auth.callback.NameCallback;
+import javax.security.auth.callback.PasswordCallback;
+import javax.security.auth.callback.UnsupportedCallbackException;
+import javax.security.sasl.RealmCallback;
+import javax.security.sasl.RealmChoiceCallback;
+import javax.security.sasl.Sasl;
+import javax.security.sasl.SaslException;
+import javax.security.sasl.SaslClient;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.ipc.RemoteException;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.AuthMethod;
+import org.apache.hadoop.hbase.security.HBaseSaslRpcServer.SaslStatus;
+import org.apache.hadoop.security.SaslInputStream;
+import org.apache.hadoop.security.SaslOutputStream;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+
+/**
+ * A utility class that encapsulates SASL logic for RPC client.
+ * Copied from <code>org.apache.hadoop.security</code>
+ */
+public class HBaseSaslRpcClient {
+  public static final Log LOG = LogFactory.getLog(HBaseSaslRpcClient.class);
+
+  private final SaslClient saslClient;
+
+  /**
+   * Create a HBaseSaslRpcClient for an authentication method
+   * 
+   * @param method
+   *          the requested authentication method
+   * @param token
+   *          token to use if needed by the authentication method
+   */
+  public HBaseSaslRpcClient(AuthMethod method,
+      Token<? extends TokenIdentifier> token, String serverPrincipal)
+      throws IOException {
+    switch (method) {
+    case DIGEST:
+      if (LOG.isDebugEnabled())
+        LOG.debug("Creating SASL " + AuthMethod.DIGEST.getMechanismName()
+            + " client to authenticate to service at " + token.getService());
+      saslClient = Sasl.createSaslClient(new String[] { AuthMethod.DIGEST
+          .getMechanismName() }, null, null, HBaseSaslRpcServer.SASL_DEFAULT_REALM,
+          HBaseSaslRpcServer.SASL_PROPS, new SaslClientCallbackHandler(token));
+      break;
+    case KERBEROS:
+      if (LOG.isDebugEnabled()) {
+        LOG
+            .debug("Creating SASL " + AuthMethod.KERBEROS.getMechanismName()
+                + " client. Server's Kerberos principal name is "
+                + serverPrincipal);
+      }
+      if (serverPrincipal == null || serverPrincipal.length() == 0) {
+        throw new IOException(
+            "Failed to specify server's Kerberos principal name");
+      }
+      String names[] = HBaseSaslRpcServer.splitKerberosName(serverPrincipal);
+      if (names.length != 3) {
+        throw new IOException(
+          "Kerberos principal name does NOT have the expected hostname part: "
+                + serverPrincipal);
+      }
+      saslClient = Sasl.createSaslClient(new String[] { AuthMethod.KERBEROS
+          .getMechanismName() }, null, names[0], names[1],
+          HBaseSaslRpcServer.SASL_PROPS, null);
+      break;
+    default:
+      throw new IOException("Unknown authentication method " + method);
+    }
+    if (saslClient == null)
+      throw new IOException("Unable to find SASL client implementation");
+  }
+
+  private static void readStatus(DataInputStream inStream) throws IOException {
+    int id = inStream.readInt(); // read and discard dummy id
+    int status = inStream.readInt(); // read status
+    if (status != SaslStatus.SUCCESS.state) {
+      throw new RemoteException(WritableUtils.readString(inStream),
+          WritableUtils.readString(inStream));
+    }
+  }
+  
+  /**
+   * Do client side SASL authentication with server via the given InputStream
+   * and OutputStream
+   * 
+   * @param inS
+   *          InputStream to use
+   * @param outS
+   *          OutputStream to use
+   * @return true if connection is set up, or false if needs to switch 
+   *             to simple Auth.
+   * @throws IOException
+   */
+  public boolean saslConnect(InputStream inS, OutputStream outS)
+      throws IOException {
+    DataInputStream inStream = new DataInputStream(new BufferedInputStream(inS));
+    DataOutputStream outStream = new DataOutputStream(new BufferedOutputStream(
+        outS));
+
+    try {
+      byte[] saslToken = new byte[0];
+      if (saslClient.hasInitialResponse())
+        saslToken = saslClient.evaluateChallenge(saslToken);
+      if (saslToken != null) {
+        outStream.writeInt(saslToken.length);
+        outStream.write(saslToken, 0, saslToken.length);
+        outStream.flush();
+        if (LOG.isDebugEnabled())
+          LOG.debug("Have sent token of size " + saslToken.length
+              + " from initSASLContext.");
+      }
+      if (!saslClient.isComplete()) {
+        readStatus(inStream);
+        int len = inStream.readInt();
+        if (len == HBaseSaslRpcServer.SWITCH_TO_SIMPLE_AUTH) {
+          if (LOG.isDebugEnabled())
+            LOG.debug("Server asks us to fall back to simple auth.");
+          saslClient.dispose();
+          return false;
+        }
+        saslToken = new byte[len];
+        if (LOG.isDebugEnabled())
+          LOG.debug("Will read input token of size " + saslToken.length
+              + " for processing by initSASLContext");
+        inStream.readFully(saslToken);
+      }
+
+      while (!saslClient.isComplete()) {
+        saslToken = saslClient.evaluateChallenge(saslToken);
+        if (saslToken != null) {
+          if (LOG.isDebugEnabled())
+            LOG.debug("Will send token of size " + saslToken.length
+                + " from initSASLContext.");
+          outStream.writeInt(saslToken.length);
+          outStream.write(saslToken, 0, saslToken.length);
+          outStream.flush();
+        }
+        if (!saslClient.isComplete()) {
+          readStatus(inStream);
+          saslToken = new byte[inStream.readInt()];
+          if (LOG.isDebugEnabled())
+            LOG.debug("Will read input token of size " + saslToken.length
+                + " for processing by initSASLContext");
+          inStream.readFully(saslToken);
+        }
+      }
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("SASL client context established. Negotiated QoP: "
+            + saslClient.getNegotiatedProperty(Sasl.QOP));
+      }
+      return true;
+    } catch (IOException e) {
+      try {
+        saslClient.dispose();
+      } catch (SaslException ignored) {
+        // ignore further exceptions during cleanup
+      }
+      throw e;
+    }
+  }
+
+  /**
+   * Get a SASL wrapped InputStream. Can be called only after saslConnect() has
+   * been called.
+   * 
+   * @param in
+   *          the InputStream to wrap
+   * @return a SASL wrapped InputStream
+   * @throws IOException
+   */
+  public InputStream getInputStream(InputStream in) throws IOException {
+    if (!saslClient.isComplete()) {
+      throw new IOException("Sasl authentication exchange hasn't completed yet");
+    }
+    return new SaslInputStream(in, saslClient);
+  }
+
+  /**
+   * Get a SASL wrapped OutputStream. Can be called only after saslConnect() has
+   * been called.
+   * 
+   * @param out
+   *          the OutputStream to wrap
+   * @return a SASL wrapped OutputStream
+   * @throws IOException
+   */
+  public OutputStream getOutputStream(OutputStream out) throws IOException {
+    if (!saslClient.isComplete()) {
+      throw new IOException("Sasl authentication exchange hasn't completed yet");
+    }
+    return new SaslOutputStream(out, saslClient);
+  }
+
+  /** Release resources used by wrapped saslClient */
+  public void dispose() throws SaslException {
+    saslClient.dispose();
+  }
+
+  private static class SaslClientCallbackHandler implements CallbackHandler {
+    private final String userName;
+    private final char[] userPassword;
+
+    public SaslClientCallbackHandler(Token<? extends TokenIdentifier> token) {
+      this.userName = HBaseSaslRpcServer.encodeIdentifier(token.getIdentifier());
+      this.userPassword = HBaseSaslRpcServer.encodePassword(token.getPassword());
+    }
+
+    public void handle(Callback[] callbacks)
+        throws UnsupportedCallbackException {
+      NameCallback nc = null;
+      PasswordCallback pc = null;
+      RealmCallback rc = null;
+      for (Callback callback : callbacks) {
+        if (callback instanceof RealmChoiceCallback) {
+          continue;
+        } else if (callback instanceof NameCallback) {
+          nc = (NameCallback) callback;
+        } else if (callback instanceof PasswordCallback) {
+          pc = (PasswordCallback) callback;
+        } else if (callback instanceof RealmCallback) {
+          rc = (RealmCallback) callback;
+        } else {
+          throw new UnsupportedCallbackException(callback,
+              "Unrecognized SASL client callback");
+        }
+      }
+      if (nc != null) {
+        if (LOG.isDebugEnabled())
+          LOG.debug("SASL client callback: setting username: " + userName);
+        nc.setName(userName);
+      }
+      if (pc != null) {
+        if (LOG.isDebugEnabled())
+          LOG.debug("SASL client callback: setting userPassword");
+        pc.setPassword(userPassword);
+      }
+      if (rc != null) {
+        if (LOG.isDebugEnabled())
+          LOG.debug("SASL client callback: setting realm: "
+              + rc.getDefaultText());
+        rc.setText(rc.getDefaultText());
+      }
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcServer.java security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcServer.java
new file mode 100644
index 0000000..5ffbb4f
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/HBaseSaslRpcServer.java
@@ -0,0 +1,280 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInput;
+import java.io.DataInputStream;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.TreeMap;
+
+import javax.security.auth.callback.Callback;
+import javax.security.auth.callback.CallbackHandler;
+import javax.security.auth.callback.NameCallback;
+import javax.security.auth.callback.PasswordCallback;
+import javax.security.auth.callback.UnsupportedCallbackException;
+import javax.security.sasl.AuthorizeCallback;
+import javax.security.sasl.RealmCallback;
+import javax.security.sasl.Sasl;
+
+import org.apache.commons.codec.binary.Base64;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ipc.HBaseServer;
+import org.apache.hadoop.hbase.ipc.SecureServer;
+import org.apache.hadoop.ipc.Server;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.SecretManager;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.security.UserGroupInformation.AuthenticationMethod;
+import org.apache.hadoop.security.token.SecretManager.InvalidToken;
+
+/**
+ * A utility class for dealing with SASL on RPC server
+ */
+public class HBaseSaslRpcServer {
+  public static final Log LOG = LogFactory.getLog(HBaseSaslRpcServer.class);
+  public static final String SASL_DEFAULT_REALM = "default";
+  public static final Map<String, String> SASL_PROPS =
+      new TreeMap<String, String>();
+
+  public static final int SWITCH_TO_SIMPLE_AUTH = -88;
+
+  public static enum QualityOfProtection {
+    AUTHENTICATION("auth"),
+    INTEGRITY("auth-int"),
+    PRIVACY("auth-conf");
+
+    public final String saslQop;
+
+    private QualityOfProtection(String saslQop) {
+      this.saslQop = saslQop;
+    }
+
+    public String getSaslQop() {
+      return saslQop;
+    }
+  }
+
+  public static void init(Configuration conf) {
+    QualityOfProtection saslQOP = QualityOfProtection.AUTHENTICATION;
+    String rpcProtection = conf.get("hbase.rpc.protection",
+        QualityOfProtection.AUTHENTICATION.name().toLowerCase());
+    if (QualityOfProtection.INTEGRITY.name().toLowerCase()
+        .equals(rpcProtection)) {
+      saslQOP = QualityOfProtection.INTEGRITY;
+    } else if (QualityOfProtection.PRIVACY.name().toLowerCase().equals(
+        rpcProtection)) {
+      saslQOP = QualityOfProtection.PRIVACY;
+    }
+
+    SASL_PROPS.put(Sasl.QOP, saslQOP.getSaslQop());
+    SASL_PROPS.put(Sasl.SERVER_AUTH, "true");
+  }
+
+  static String encodeIdentifier(byte[] identifier) {
+    return new String(Base64.encodeBase64(identifier));
+  }
+
+  static byte[] decodeIdentifier(String identifier) {
+    return Base64.decodeBase64(identifier.getBytes());
+  }
+
+  public static <T extends TokenIdentifier> T getIdentifier(String id,
+      SecretManager<T> secretManager) throws InvalidToken {
+    byte[] tokenId = decodeIdentifier(id);
+    T tokenIdentifier = secretManager.createIdentifier();
+    try {
+      tokenIdentifier.readFields(new DataInputStream(new ByteArrayInputStream(
+          tokenId)));
+    } catch (IOException e) {
+      throw (InvalidToken) new InvalidToken(
+          "Can't de-serialize tokenIdentifier").initCause(e);
+    }
+    return tokenIdentifier;
+  }
+
+  static char[] encodePassword(byte[] password) {
+    return new String(Base64.encodeBase64(password)).toCharArray();
+  }
+
+  /** Splitting fully qualified Kerberos name into parts */
+  public static String[] splitKerberosName(String fullName) {
+    return fullName.split("[/@]");
+  }
+
+  public enum SaslStatus {
+    SUCCESS (0),
+    ERROR (1);
+    
+    public final int state;
+    private SaslStatus(int state) {
+      this.state = state;
+    }
+  }
+  
+  /** Authentication method */
+  public static enum AuthMethod {
+    SIMPLE((byte) 80, "", AuthenticationMethod.SIMPLE),
+    KERBEROS((byte) 81, "GSSAPI", AuthenticationMethod.KERBEROS),
+    DIGEST((byte) 82, "DIGEST-MD5", AuthenticationMethod.TOKEN);
+
+    /** The code for this method. */
+    public final byte code;
+    public final String mechanismName;
+    public final AuthenticationMethod authenticationMethod;
+
+    private AuthMethod(byte code, String mechanismName, 
+                       AuthenticationMethod authMethod) {
+      this.code = code;
+      this.mechanismName = mechanismName;
+      this.authenticationMethod = authMethod;
+    }
+
+    private static final int FIRST_CODE = values()[0].code;
+
+    /** Return the object represented by the code. */
+    private static AuthMethod valueOf(byte code) {
+      final int i = (code & 0xff) - FIRST_CODE;
+      return i < 0 || i >= values().length ? null : values()[i];
+    }
+
+    /** Return the SASL mechanism name */
+    public String getMechanismName() {
+      return mechanismName;
+    }
+
+    /** Read from in */
+    public static AuthMethod read(DataInput in) throws IOException {
+      return valueOf(in.readByte());
+    }
+
+    /** Write to out */
+    public void write(DataOutput out) throws IOException {
+      out.write(code);
+    }
+  };
+
+  /** CallbackHandler for SASL DIGEST-MD5 mechanism */
+  public static class SaslDigestCallbackHandler implements CallbackHandler {
+    private SecretManager<TokenIdentifier> secretManager;
+    private SecureServer.SecureConnection connection;
+
+    public SaslDigestCallbackHandler(
+        SecretManager<TokenIdentifier> secretManager,
+        SecureServer.SecureConnection connection) {
+      this.secretManager = secretManager;
+      this.connection = connection;
+    }
+
+    private char[] getPassword(TokenIdentifier tokenid) throws InvalidToken {
+      return encodePassword(secretManager.retrievePassword(tokenid));
+    }
+
+    /** {@inheritDoc} */
+    @Override
+    public void handle(Callback[] callbacks) throws InvalidToken,
+        UnsupportedCallbackException {
+      NameCallback nc = null;
+      PasswordCallback pc = null;
+      AuthorizeCallback ac = null;
+      for (Callback callback : callbacks) {
+        if (callback instanceof AuthorizeCallback) {
+          ac = (AuthorizeCallback) callback;
+        } else if (callback instanceof NameCallback) {
+          nc = (NameCallback) callback;
+        } else if (callback instanceof PasswordCallback) {
+          pc = (PasswordCallback) callback;
+        } else if (callback instanceof RealmCallback) {
+          continue; // realm is ignored
+        } else {
+          throw new UnsupportedCallbackException(callback,
+              "Unrecognized SASL DIGEST-MD5 Callback");
+        }
+      }
+      if (pc != null) {
+        TokenIdentifier tokenIdentifier = getIdentifier(nc.getDefaultName(), secretManager);
+        char[] password = getPassword(tokenIdentifier);
+        UserGroupInformation user = null;
+        user = tokenIdentifier.getUser(); // may throw exception
+        connection.attemptingUser = user;
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("SASL server DIGEST-MD5 callback: setting password "
+              + "for client: " + tokenIdentifier.getUser());
+        }
+        pc.setPassword(password);
+      }
+      if (ac != null) {
+        String authid = ac.getAuthenticationID();
+        String authzid = ac.getAuthorizationID();
+        if (authid.equals(authzid)) {
+          ac.setAuthorized(true);
+        } else {
+          ac.setAuthorized(false);
+        }
+        if (ac.isAuthorized()) {
+          if (LOG.isDebugEnabled()) {
+            String username =
+              getIdentifier(authzid, secretManager).getUser().getUserName();
+            LOG.debug("SASL server DIGEST-MD5 callback: setting "
+                + "canonicalized client ID: " + username);
+          }
+          ac.setAuthorizedID(authzid);
+        }
+      }
+    }
+  }
+
+  /** CallbackHandler for SASL GSSAPI Kerberos mechanism */
+  public static class SaslGssCallbackHandler implements CallbackHandler {
+
+    /** {@inheritDoc} */
+    @Override
+    public void handle(Callback[] callbacks) throws
+        UnsupportedCallbackException {
+      AuthorizeCallback ac = null;
+      for (Callback callback : callbacks) {
+        if (callback instanceof AuthorizeCallback) {
+          ac = (AuthorizeCallback) callback;
+        } else {
+          throw new UnsupportedCallbackException(callback,
+              "Unrecognized SASL GSSAPI Callback");
+        }
+      }
+      if (ac != null) {
+        String authid = ac.getAuthenticationID();
+        String authzid = ac.getAuthorizationID();
+        if (authid.equals(authzid)) {
+          ac.setAuthorized(true);
+        } else {
+          ac.setAuthorized(false);
+        }
+        if (ac.isAuthorized()) {
+          if (LOG.isDebugEnabled())
+            LOG.debug("SASL server GSSAPI callback: setting "
+                + "canonicalized client ID: " + authzid);
+          ac.setAuthorizedID(authzid);
+        }
+      }
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java
new file mode 100644
index 0000000..c68af4e
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationKey.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import javax.crypto.SecretKey;
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.WritableUtils;
+
+/**
+ * Represents a secret key used for signing and verifying authentication tokens
+ * by {@link AuthenticationTokenSecretManager}.
+ */
+public class AuthenticationKey implements Writable {
+  private int id;
+  private long expirationDate;
+  private SecretKey secret;
+
+  public AuthenticationKey() {
+    // for Writable
+  }
+
+  public AuthenticationKey(int keyId, long expirationDate, SecretKey key) {
+    this.id = keyId;
+    this.expirationDate = expirationDate;
+    this.secret = key;
+  }
+
+  public int getKeyId() {
+    return id;
+  }
+
+  public long getExpiration() {
+    return expirationDate;
+  }
+
+  public void setExpiration(long timestamp) {
+    expirationDate = timestamp;
+  }
+
+  SecretKey getKey() {
+    return secret;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || !(obj instanceof AuthenticationKey)) {
+      return false;
+    }
+    AuthenticationKey other = (AuthenticationKey)obj;
+    return id == other.getKeyId() &&
+        expirationDate == other.getExpiration() &&
+        (secret == null ? other.getKey() == null :
+            other.getKey() != null &&
+                Bytes.equals(secret.getEncoded(), other.getKey().getEncoded()));       
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder buf = new StringBuilder();
+    buf.append("AuthenticationKey[ ")
+       .append("id=").append(id)
+       .append(", expiration=").append(expirationDate)
+       .append(" ]");
+    return buf.toString();
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    WritableUtils.writeVInt(out, id);
+    WritableUtils.writeVLong(out, expirationDate);
+    if (secret == null) {
+      WritableUtils.writeVInt(out, -1);
+    } else {
+      byte[] keyBytes = secret.getEncoded();
+      WritableUtils.writeVInt(out, keyBytes.length);
+      out.write(keyBytes);
+    }
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    id = WritableUtils.readVInt(in);
+    expirationDate = WritableUtils.readVLong(in);
+    int keyLength = WritableUtils.readVInt(in);
+    if (keyLength < 0) {
+      secret = null;
+    } else {
+      byte[] keyBytes = new byte[keyLength];
+      in.readFully(keyBytes);
+      secret = AuthenticationTokenSecretManager.createSecretKey(keyBytes);
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationProtocol.java security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationProtocol.java
new file mode 100644
index 0000000..2eb9e15
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationProtocol.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+import org.apache.hadoop.security.token.Token;
+
+/**
+ * Defines a custom RPC protocol for obtaining authentication tokens
+ */
+public interface AuthenticationProtocol extends CoprocessorProtocol {
+  /**
+   * Obtains a token capable of authenticating as the current user for future
+   * connections.
+   * @return an authentication token for the current user
+   * @throws IOException If obtaining a token is denied or encounters an error
+   */
+  public Token<AuthenticationTokenIdentifier> getAuthenticationToken()
+      throws IOException;
+
+  /**
+   * Returns the currently authenticated username.
+   */
+  public String whoami();
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java
new file mode 100644
index 0000000..62d2c96
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenIdentifier.java
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.TokenIdentifier;
+
+/**
+ * Represents the identity information stored in an HBase authentication token.
+ */
+public class AuthenticationTokenIdentifier extends TokenIdentifier {
+  public static final byte VERSION = 1;
+  public static final Text AUTH_TOKEN_TYPE = new Text("HBASE_AUTH_TOKEN");
+
+  protected String username;
+  protected int keyId;
+  protected long issueDate;
+  protected long expirationDate;
+  protected long sequenceNumber;
+  
+  public AuthenticationTokenIdentifier() {
+  }
+
+  public AuthenticationTokenIdentifier(String username) {
+    this.username = username;
+  }
+
+  public AuthenticationTokenIdentifier(String username, int keyId,
+      long issueDate, long expirationDate) {
+    this.username = username;
+    this.keyId = keyId;
+    this.issueDate = issueDate;
+    this.expirationDate = expirationDate;
+  }
+
+  @Override
+  public Text getKind() {
+    return AUTH_TOKEN_TYPE;
+  }
+
+  @Override
+  public UserGroupInformation getUser() {
+    if (username == null || "".equals(username)) {
+      return null;
+    }
+    return UserGroupInformation.createRemoteUser(username);
+  }
+
+  public String getUsername() {
+    return username;
+  }
+
+  void setUsername(String name) {
+    this.username = name;
+  }
+
+  public int getKeyId() {
+    return keyId;
+  }
+
+  void setKeyId(int id) {
+    this.keyId = id;
+  }
+
+  public long getIssueDate() {
+    return issueDate;
+  }
+
+  void setIssueDate(long timestamp) {
+    this.issueDate = timestamp;
+  }
+
+  public long getExpirationDate() {
+    return expirationDate;
+  }
+
+  void setExpirationDate(long timestamp) {
+    this.expirationDate = timestamp;
+  }
+
+  public long getSequenceNumber() {
+    return sequenceNumber;
+  }
+
+  void setSequenceNumber(long seq) {
+    this.sequenceNumber = seq;
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeByte(VERSION);
+    WritableUtils.writeString(out, username);
+    WritableUtils.writeVInt(out, keyId);
+    WritableUtils.writeVLong(out, issueDate);
+    WritableUtils.writeVLong(out, expirationDate);
+    WritableUtils.writeVLong(out, sequenceNumber);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    byte version = in.readByte();
+    if (version != VERSION) {
+      throw new IOException("Version mismatch in deserialization: " +
+          "expected="+VERSION+", got="+version);
+    }
+    username = WritableUtils.readString(in);
+    keyId = WritableUtils.readVInt(in);
+    issueDate = WritableUtils.readVLong(in);
+    expirationDate = WritableUtils.readVLong(in);
+    sequenceNumber = WritableUtils.readVLong(in);
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (other == null) {
+      return false;
+    }
+    if (other instanceof AuthenticationTokenIdentifier) {
+      AuthenticationTokenIdentifier ident = (AuthenticationTokenIdentifier)other;
+      return sequenceNumber == ident.getSequenceNumber()
+          && keyId == ident.getKeyId()
+          && issueDate == ident.getIssueDate()
+          && expirationDate == ident.getExpirationDate()
+          && (username == null ? ident.getUsername() == null :
+              username.equals(ident.getUsername()));
+    }
+    return false;
+  }
+
+  @Override
+  public int hashCode() {
+    return (int)sequenceNumber;
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
new file mode 100644
index 0000000..8b1ba7b
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.java
@@ -0,0 +1,326 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import javax.crypto.SecretKey;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.zookeeper.ClusterId;
+import org.apache.hadoop.hbase.zookeeper.ZKLeaderManager;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hadoop.security.token.SecretManager;
+import org.apache.hadoop.security.token.Token;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * Manages an internal list of secret keys used to sign new authentication
+ * tokens as they are generated, and to valid existing tokens used for
+ * authentication.
+ *
+ * <p>
+ * A single instance of {@code AuthenticationTokenSecretManager} will be
+ * running as the "leader" in a given HBase cluster.  The leader is responsible
+ * for periodically generating new secret keys, which are then distributed to
+ * followers via ZooKeeper, and for expiring previously used secret keys that
+ * are no longer needed (as any tokens using them have expired).
+ * </p>
+ */
+public class AuthenticationTokenSecretManager
+    extends SecretManager<AuthenticationTokenIdentifier> {
+
+  static final String NAME_PREFIX = "SecretManager-";
+
+  private static Log LOG = LogFactory.getLog(
+      AuthenticationTokenSecretManager.class);
+
+  private long lastKeyUpdate;
+  private long keyUpdateInterval;
+  private long tokenMaxLifetime;
+  private ZKSecretWatcher zkWatcher;
+  private LeaderElector leaderElector;
+  private ClusterId clusterId;
+
+  private Map<Integer,AuthenticationKey> allKeys =
+      new ConcurrentHashMap<Integer, AuthenticationKey>();
+  private AuthenticationKey currentKey;
+
+  private int idSeq;
+  private AtomicLong tokenSeq = new AtomicLong();
+  private String name;
+
+  /**
+   * Create a new secret manager instance for generating keys.
+   * @param conf Configuration to use
+   * @param zk Connection to zookeeper for handling leader elections
+   * @param keyUpdateInterval Time (in milliseconds) between rolling a new master key for token signing
+   * @param tokenMaxLifetime Maximum age (in milliseconds) before a token expires and is no longer valid
+   */
+  /* TODO: Restrict access to this constructor to make rogues instances more difficult.
+   * For the moment this class is instantiated from
+   * org.apache.hadoop.hbase.ipc.SecureServer so public access is needed.
+   */
+  public AuthenticationTokenSecretManager(Configuration conf,
+      ZooKeeperWatcher zk, String serverName,
+      long keyUpdateInterval, long tokenMaxLifetime) {
+    this.zkWatcher = new ZKSecretWatcher(conf, zk, this);
+    this.keyUpdateInterval = keyUpdateInterval;
+    this.tokenMaxLifetime = tokenMaxLifetime;
+    this.leaderElector = new LeaderElector(zk, serverName);
+    this.name = NAME_PREFIX+serverName;
+    this.clusterId = new ClusterId(zk, zk);
+  }
+
+  public void start() {
+    try {
+      // populate any existing keys
+      this.zkWatcher.start();
+      // try to become leader
+      this.leaderElector.start();
+    } catch (KeeperException ke) {
+      LOG.error("Zookeeper initialization failed", ke);
+    }
+  }
+
+  public void stop() {
+    this.leaderElector.stop("SecretManager stopping");
+  }
+
+  public boolean isMaster() {
+    return leaderElector.isMaster();
+  }
+
+  public String getName() {
+    return name;
+  }
+
+  @Override
+  protected byte[] createPassword(AuthenticationTokenIdentifier identifier) {
+    long now = EnvironmentEdgeManager.currentTimeMillis();
+    AuthenticationKey secretKey = currentKey;
+    identifier.setKeyId(secretKey.getKeyId());
+    identifier.setIssueDate(now);
+    identifier.setExpirationDate(now + tokenMaxLifetime);
+    identifier.setSequenceNumber(tokenSeq.getAndIncrement());
+    return createPassword(WritableUtils.toByteArray(identifier),
+        secretKey.getKey());
+  }
+
+  @Override
+  public byte[] retrievePassword(AuthenticationTokenIdentifier identifier)
+      throws InvalidToken {
+    long now = EnvironmentEdgeManager.currentTimeMillis();
+    if (identifier.getExpirationDate() < now) {
+      throw new InvalidToken("Token has expired");
+    }
+    AuthenticationKey masterKey = allKeys.get(identifier.getKeyId());
+    if (masterKey == null) {
+      throw new InvalidToken("Unknown master key for token (id="+
+          identifier.getKeyId()+")");
+    }
+    // regenerate the password
+    return createPassword(WritableUtils.toByteArray(identifier),
+        masterKey.getKey());
+  }
+
+  @Override
+  public AuthenticationTokenIdentifier createIdentifier() {
+    return new AuthenticationTokenIdentifier();
+  }
+
+  public Token<AuthenticationTokenIdentifier> generateToken(String username) {
+    AuthenticationTokenIdentifier ident =
+        new AuthenticationTokenIdentifier(username);
+    Token<AuthenticationTokenIdentifier> token =
+        new Token<AuthenticationTokenIdentifier>(ident, this);
+    if (clusterId.hasId()) {
+      token.setService(new Text(clusterId.getId()));
+    }
+    return token;
+  }
+
+  public synchronized void addKey(AuthenticationKey key) throws IOException {
+    // ignore zk changes when running as master
+    if (leaderElector.isMaster()) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Running as master, ignoring new key "+key.getKeyId());
+      }
+      return;
+    }
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Adding key "+key.getKeyId());
+    }
+
+    allKeys.put(key.getKeyId(), key);
+    if (currentKey == null || key.getKeyId() > currentKey.getKeyId()) {
+      currentKey = key;
+    }
+    // update current sequence
+    if (key.getKeyId() > idSeq) {
+      idSeq = key.getKeyId();
+    }
+  }
+
+  synchronized void removeKey(Integer keyId) {
+    // ignore zk changes when running as master
+    if (leaderElector.isMaster()) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Running as master, ignoring removed key "+keyId);
+      }
+      return;
+    }
+
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Removing key "+keyId);
+    }
+
+    allKeys.remove(keyId);
+  }
+
+  AuthenticationKey getCurrentKey() {
+    return currentKey;
+  }
+
+  AuthenticationKey getKey(int keyId) {
+    return allKeys.get(keyId);
+  }
+
+  synchronized void removeExpiredKeys() {
+    if (!leaderElector.isMaster()) {
+      LOG.info("Skipping removeExpiredKeys() because not running as master.");
+      return;
+    }
+
+    long now = EnvironmentEdgeManager.currentTimeMillis();
+    Iterator<AuthenticationKey> iter = allKeys.values().iterator();
+    while (iter.hasNext()) {
+      AuthenticationKey key = iter.next();
+      if (key.getExpiration() < now) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Removing expired key "+key.getKeyId());
+        }
+        iter.remove();
+        zkWatcher.removeKeyFromZK(key);
+      }
+    }
+  }
+
+  synchronized void rollCurrentKey() {
+    if (!leaderElector.isMaster()) {
+      LOG.info("Skipping rollCurrentKey() because not running as master.");
+      return;
+    }
+
+    long now = EnvironmentEdgeManager.currentTimeMillis();
+    AuthenticationKey prev = currentKey;
+    AuthenticationKey newKey = new AuthenticationKey(++idSeq,
+        Long.MAX_VALUE, // don't allow to expire until it's replaced by a new key
+        generateSecret());
+    allKeys.put(newKey.getKeyId(), newKey);
+    currentKey = newKey;
+    zkWatcher.addKeyToZK(newKey);
+    lastKeyUpdate = now;
+
+    if (prev != null) {
+      // make sure previous key is still stored
+      prev.setExpiration(now + tokenMaxLifetime);
+      allKeys.put(prev.getKeyId(), prev);
+      zkWatcher.updateKeyInZK(prev);
+    }
+  }
+
+  public static SecretKey createSecretKey(byte[] raw) {
+    return SecretManager.createSecretKey(raw);
+  }
+
+  private class LeaderElector extends Thread implements Stoppable {
+    private boolean stopped = false;
+    /** Flag indicating whether we're in charge of rolling/expiring keys */
+    private boolean isMaster = false;
+    private ZKLeaderManager zkLeader;
+
+    public LeaderElector(ZooKeeperWatcher watcher, String serverName) {
+      setDaemon(true);
+      setName("ZKSecretWatcher-leaderElector");
+      zkLeader = new ZKLeaderManager(watcher,
+          ZKUtil.joinZNode(zkWatcher.getRootKeyZNode(), "keymaster"),
+          Bytes.toBytes(serverName), this);
+    }
+
+    public boolean isMaster() {
+      return isMaster;
+    }
+
+    @Override
+    public boolean isStopped() {
+      return stopped;
+    }
+
+    @Override
+    public void stop(String reason) {
+      stopped = true;
+      // prevent further key generation when stopping
+      if (isMaster) {
+        zkLeader.stepDownAsLeader();
+      }
+      isMaster = false;
+      LOG.info("Stopping leader election, because: "+reason);
+      interrupt();
+    }
+
+    public void run() {
+      zkLeader.start();
+      zkLeader.waitToBecomeLeader();
+      isMaster = true;
+
+      while (!stopped) {
+        long now = EnvironmentEdgeManager.currentTimeMillis();
+
+        // clear any expired
+        removeExpiredKeys();
+
+        if (lastKeyUpdate + keyUpdateInterval < now) {
+          // roll a new master key
+          rollCurrentKey();
+        }
+
+        try {
+          Thread.sleep(5000);
+        } catch (InterruptedException ie) {
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Interrupted waiting for next update", ie);
+          }
+        }
+      }
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java
new file mode 100644
index 0000000..943a89d
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/AuthenticationTokenSelector.java
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.util.Collection;
+
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.security.token.Token;
+import org.apache.hadoop.security.token.TokenIdentifier;
+import org.apache.hadoop.security.token.TokenSelector;
+
+public class AuthenticationTokenSelector
+    implements TokenSelector<AuthenticationTokenIdentifier> {
+
+  public AuthenticationTokenSelector() {
+  }
+
+  @Override
+  public Token<AuthenticationTokenIdentifier> selectToken(Text serviceName,
+      Collection<Token<? extends TokenIdentifier>> tokens) {
+    if (serviceName != null) {
+      for (Token ident : tokens) {
+        if (serviceName.equals(ident.getService()) &&
+            AuthenticationTokenIdentifier.AUTH_TOKEN_TYPE.equals(ident.getKind())) {
+          return (Token<AuthenticationTokenIdentifier>)ident;
+        }
+      }
+    }
+    return null;
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java security/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java
new file mode 100644
index 0000000..0e7e872
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor;
+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
+import org.apache.hadoop.hbase.ipc.RequestContext;
+import org.apache.hadoop.hbase.ipc.RpcServer;
+import org.apache.hadoop.hbase.ipc.SecureServer;
+import org.apache.hadoop.hbase.security.AccessDeniedException;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.SecretManager;
+import org.apache.hadoop.security.token.Token;
+
+/**
+ * Provides a service for obtaining authentication tokens via the
+ * {@link AuthenticationProtocol} coprocessor protocol.
+ */
+public class TokenProvider extends BaseEndpointCoprocessor
+    implements AuthenticationProtocol {
+
+  public static final long VERSION = 0L;
+  private static Log LOG = LogFactory.getLog(TokenProvider.class);
+
+  private AuthenticationTokenSecretManager secretManager;
+
+
+  @Override
+  public void start(CoprocessorEnvironment env) {
+    super.start(env);
+
+    // if running at region
+    if (env instanceof RegionCoprocessorEnvironment) {
+      RegionCoprocessorEnvironment regionEnv =
+          (RegionCoprocessorEnvironment)env;
+      RpcServer server = regionEnv.getRegionServerServices().getRpcServer();
+      if (server instanceof SecureServer) {
+        SecretManager mgr = ((SecureServer)server).getSecretManager();
+        if (mgr instanceof AuthenticationTokenSecretManager) {
+          secretManager = (AuthenticationTokenSecretManager)mgr;
+        }
+      }
+    }
+  }
+
+  @Override
+  public Token<AuthenticationTokenIdentifier> getAuthenticationToken()
+      throws IOException {
+    if (secretManager == null) {
+      throw new IOException(
+          "No secret manager configured for token authentication");
+    }
+
+    User currentUser = RequestContext.getRequestUser();
+    UserGroupInformation ugi = null;
+    if (currentUser != null) {
+      ugi = currentUser.getUGI();
+    }
+    if (currentUser == null) {
+      throw new AccessDeniedException("No authenticated user for request!");
+    } else if (ugi.getAuthenticationMethod() !=
+        UserGroupInformation.AuthenticationMethod.KERBEROS) {
+      LOG.warn("Token generation denied for user="+currentUser.getName()
+          +", authMethod="+ugi.getAuthenticationMethod());
+      throw new AccessDeniedException(
+          "Token generation only allowed for Kerberos authenticated clients");
+    }
+
+    return secretManager.generateToken(currentUser.getName());
+  }
+
+  @Override
+  public String whoami() {
+    return RequestContext.getRequestUserName();
+  }
+
+  @Override
+  public long getProtocolVersion(String protocol, long clientVersion)
+      throws IOException {
+    if (AuthenticationProtocol.class.getName().equals(protocol)) {
+      return TokenProvider.VERSION;
+    }
+    LOG.warn("Unknown protocol requested: "+protocol);
+    return -1;
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java security/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
new file mode 100644
index 0000000..49feeac
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/TokenUtil.java
@@ -0,0 +1,183 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.io.IOException;
+import java.lang.reflect.UndeclaredThrowableException;
+import java.security.PrivilegedExceptionAction;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
+
+/**
+ * Utility methods for obtaining authentication tokens.
+ */
+public class TokenUtil {
+  private static Log LOG = LogFactory.getLog(TokenUtil.class);
+
+  /**
+   * Obtain and return an authentication token for the current user.
+   * @param conf The configuration for connecting to the cluster
+   * @return the authentication token instance
+   */
+  public static Token<AuthenticationTokenIdentifier> obtainToken(
+      Configuration conf) throws IOException {
+    HTable meta = null;
+    try {
+      meta = new HTable(conf, ".META.");
+      AuthenticationProtocol prot = meta.coprocessorProxy(
+          AuthenticationProtocol.class, HConstants.EMPTY_START_ROW);
+      return prot.getAuthenticationToken();
+    } finally {
+      if (meta != null) {
+        meta.close();
+      }
+    }
+  }
+
+  private static Text getClusterId(Token<AuthenticationTokenIdentifier> token)
+      throws IOException {
+    return token.getService() != null
+        ? token.getService() : new Text("default");
+  }
+
+  /**
+   * Obtain an authentication token for the given user and add it to the
+   * user's credentials.
+   * @param conf The configuration for connecting to the cluster
+   * @param user The user for whom to obtain the token
+   * @throws IOException If making a remote call to the {@link TokenProvider} fails
+   * @throws InterruptedException If executing as the given user is interrupted
+   */
+  public static void obtainAndCacheToken(final Configuration conf,
+      UserGroupInformation user)
+      throws IOException, InterruptedException {
+    try {
+      Token<AuthenticationTokenIdentifier> token =
+          user.doAs(new PrivilegedExceptionAction<Token<AuthenticationTokenIdentifier>>() {
+            public Token<AuthenticationTokenIdentifier> run() throws Exception {
+              return obtainToken(conf);
+            }
+          });
+
+      if (token == null) {
+        throw new IOException("No token returned for user "+user.getUserName());
+      }
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Obtained token "+token.getKind().toString()+" for user "+
+            user.getUserName());
+      }
+      user.addToken(token);
+    } catch (IOException ioe) {
+      throw ioe;
+    } catch (InterruptedException ie) {
+      throw ie;
+    } catch (RuntimeException re) {
+      throw re;
+    } catch (Exception e) {
+      throw new UndeclaredThrowableException(e,
+          "Unexpected exception obtaining token for user "+user.getUserName());
+    }
+  }
+
+  /**
+   * Obtain an authentication token on behalf of the given user and add it to
+   * the credentials for the given map reduce job.
+   * @param conf The configuration for connecting to the cluster
+   * @param user The user for whom to obtain the token
+   * @param job The job instance in which the token should be stored
+   * @throws IOException If making a remote call to the {@link TokenProvider} fails
+   * @throws InterruptedException If executing as the given user is interrupted
+   */
+  public static void obtainTokenForJob(final Configuration conf,
+      UserGroupInformation user, Job job)
+      throws IOException, InterruptedException {
+    try {
+      Token<AuthenticationTokenIdentifier> token =
+          user.doAs(new PrivilegedExceptionAction<Token<AuthenticationTokenIdentifier>>() {
+            public Token<AuthenticationTokenIdentifier> run() throws Exception {
+              return obtainToken(conf);
+            }
+          });
+
+      if (token == null) {
+        throw new IOException("No token returned for user "+user.getUserName());
+      }
+      Text clusterId = getClusterId(token);
+      LOG.info("Obtained token "+token.getKind().toString()+" for user "+
+          user.getUserName() + " on cluster "+clusterId.toString());
+      job.getCredentials().addToken(clusterId, token);
+    } catch (IOException ioe) {
+      throw ioe;
+    } catch (InterruptedException ie) {
+      throw ie;
+    } catch (RuntimeException re) {
+      throw re;
+    } catch (Exception e) {
+      throw new UndeclaredThrowableException(e,
+          "Unexpected exception obtaining token for user "+user.getUserName());
+    }
+  }
+
+  /**
+   * Obtain an authentication token on behalf of the given user and add it to
+   * the credentials for the given map reduce job.
+   * @param user The user for whom to obtain the token
+   * @param job The job configuration in which the token should be stored
+   * @throws IOException If making a remote call to the {@link TokenProvider} fails
+   * @throws InterruptedException If executing as the given user is interrupted
+   */
+  public static void obtainTokenForJob(final JobConf job,
+      UserGroupInformation user)
+      throws IOException, InterruptedException {
+    try {
+      Token<AuthenticationTokenIdentifier> token =
+          user.doAs(new PrivilegedExceptionAction<Token<AuthenticationTokenIdentifier>>() {
+            public Token<AuthenticationTokenIdentifier> run() throws Exception {
+              return obtainToken(job);
+            }
+          });
+
+      if (token == null) {
+        throw new IOException("No token returned for user "+user.getUserName());
+      }
+      Text clusterId = getClusterId(token);
+      LOG.info("Obtained token "+token.getKind().toString()+" for user "+
+          user.getUserName()+" on cluster "+clusterId.toString());
+      job.getCredentials().addToken(clusterId, token);
+    } catch (IOException ioe) {
+      throw ioe;
+    } catch (InterruptedException ie) {
+      throw ie;
+    } catch (RuntimeException re) {
+      throw re;
+    } catch (Exception e) {
+      throw new UndeclaredThrowableException(e,
+          "Unexpected exception obtaining token for user "+user.getUserName());
+    }
+  }
+}
diff --git security/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java security/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java
new file mode 100644
index 0000000..d780d50
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/token/ZKSecretWatcher.java
@@ -0,0 +1,212 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperListener;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * Synchronizes token encryption keys across cluster nodes.
+ */
+public class ZKSecretWatcher extends ZooKeeperListener {
+  private static final String DEFAULT_ROOT_NODE = "tokenauth";
+  private static final String DEFAULT_KEYS_PARENT = "keys";
+  private static Log LOG = LogFactory.getLog(ZKSecretWatcher.class);
+
+  private AuthenticationTokenSecretManager secretManager;
+  private String baseKeyZNode;
+  private String keysParentZNode;
+
+  public ZKSecretWatcher(Configuration conf,
+      ZooKeeperWatcher watcher,
+      AuthenticationTokenSecretManager secretManager) {
+    super(watcher);
+    this.secretManager = secretManager;
+    String keyZNodeParent = conf.get("zookeeper.znode.tokenauth.parent", DEFAULT_ROOT_NODE);
+    this.baseKeyZNode = ZKUtil.joinZNode(watcher.baseZNode, keyZNodeParent);
+    this.keysParentZNode = ZKUtil.joinZNode(baseKeyZNode, DEFAULT_KEYS_PARENT);
+  }
+
+  public void start() throws KeeperException {
+    watcher.registerListener(this);
+    // make sure the base node exists
+    ZKUtil.createWithParents(watcher, keysParentZNode);
+
+    if (ZKUtil.watchAndCheckExists(watcher, keysParentZNode)) {
+      List<ZKUtil.NodeAndData> nodes =
+          ZKUtil.getChildDataAndWatchForNewChildren(watcher, keysParentZNode);
+      refreshNodes(nodes);
+    }
+  }
+
+  @Override
+  public void nodeCreated(String path) {
+    if (path.equals(keysParentZNode)) {
+      try {
+        List<ZKUtil.NodeAndData> nodes =
+            ZKUtil.getChildDataAndWatchForNewChildren(watcher, keysParentZNode);
+        refreshNodes(nodes);
+      } catch (KeeperException ke) {
+        LOG.fatal("Error reading data from zookeeper", ke);
+        watcher.abort("Error reading new key znode "+path, ke);
+      }
+    }
+  }
+
+  @Override
+  public void nodeDeleted(String path) {
+    if (keysParentZNode.equals(ZKUtil.getParent(path))) {
+      String keyId = ZKUtil.getNodeName(path);
+      try {
+        Integer id = new Integer(keyId);
+        secretManager.removeKey(id);
+      } catch (NumberFormatException nfe) {
+        LOG.error("Invalid znode name for key ID '"+keyId+"'", nfe);
+      }
+    }
+  }
+
+  @Override
+  public void nodeDataChanged(String path) {
+    if (keysParentZNode.equals(ZKUtil.getParent(path))) {
+      try {
+        byte[] data = ZKUtil.getDataAndWatch(watcher, path);
+        if (data == null || data.length == 0) {
+          LOG.debug("Ignoring empty node "+path);
+          return;
+        }
+
+        AuthenticationKey key = (AuthenticationKey)Writables.getWritable(data,
+            new AuthenticationKey());
+        secretManager.addKey(key);
+      } catch (KeeperException ke) {
+        LOG.fatal("Error reading data from zookeeper", ke);
+        watcher.abort("Error reading updated key znode "+path, ke);
+      } catch (IOException ioe) {
+        LOG.fatal("Error reading key writables", ioe);
+        watcher.abort("Error reading key writables from znode "+path, ioe);
+      }
+    }
+  }
+
+  @Override
+  public void nodeChildrenChanged(String path) {
+    if (path.equals(keysParentZNode)) {
+      // keys changed
+      try {
+        List<ZKUtil.NodeAndData> nodes =
+            ZKUtil.getChildDataAndWatchForNewChildren(watcher, keysParentZNode);
+        refreshNodes(nodes);
+      } catch (KeeperException ke) {
+        LOG.fatal("Error reading data from zookeeper", ke);
+        watcher.abort("Error reading changed keys from zookeeper", ke);
+      }
+    }
+  }
+
+  public String getRootKeyZNode() {
+    return baseKeyZNode;
+  }
+
+  private void refreshNodes(List<ZKUtil.NodeAndData> nodes) {
+    for (ZKUtil.NodeAndData n : nodes) {
+      String path = n.getNode();
+      String keyId = ZKUtil.getNodeName(path);
+      try {
+        byte[] data = n.getData();
+        if (data == null || data.length == 0) {
+          LOG.debug("Ignoring empty node "+path);
+          continue;
+        }
+        AuthenticationKey key = (AuthenticationKey)Writables.getWritable(
+            data, new AuthenticationKey());
+        secretManager.addKey(key);
+      } catch (IOException ioe) {
+        LOG.fatal("Failed reading new secret key for id '" + keyId +
+            "' from zk", ioe);
+        watcher.abort("Error deserializing key from znode "+path, ioe);
+      }
+    }
+  }
+
+  private String getKeyNode(int keyId) {
+    return ZKUtil.joinZNode(keysParentZNode, Integer.toString(keyId));
+  }
+
+  public void removeKeyFromZK(AuthenticationKey key) {
+    String keyZNode = getKeyNode(key.getKeyId());
+    try {
+      ZKUtil.deleteNode(watcher, keyZNode);
+    } catch (KeeperException.NoNodeException nne) {
+      LOG.error("Non-existent znode "+keyZNode+" for key "+key.getKeyId(), nne);
+    } catch (KeeperException ke) {
+      LOG.fatal("Failed removing znode "+keyZNode+" for key "+key.getKeyId(),
+          ke);
+      watcher.abort("Unhandled zookeeper error removing znode "+keyZNode+
+          " for key "+key.getKeyId(), ke);
+    }
+  }
+
+  public void addKeyToZK(AuthenticationKey key) {
+    String keyZNode = getKeyNode(key.getKeyId());
+    try {
+      byte[] keyData = Writables.getBytes(key);
+      // TODO: is there any point in retrying beyond what ZK client does?
+      ZKUtil.createSetData(watcher, keyZNode, keyData);
+    } catch (KeeperException ke) {
+      LOG.fatal("Unable to synchronize master key "+key.getKeyId()+
+          " to znode "+keyZNode, ke);
+      watcher.abort("Unable to synchronize secret key "+
+          key.getKeyId()+" in zookeeper", ke);
+    } catch (IOException ioe) {
+      // this can only happen from an error serializing the key
+      watcher.abort("Failed serializing key "+key.getKeyId(), ioe);
+    }
+  }
+
+  public void updateKeyInZK(AuthenticationKey key) {
+    String keyZNode = getKeyNode(key.getKeyId());
+    try {
+      byte[] keyData = Writables.getBytes(key);
+      try {
+        ZKUtil.updateExistingNodeData(watcher, keyZNode, keyData, -1);
+      } catch (KeeperException.NoNodeException ne) {
+        // node was somehow removed, try adding it back
+        ZKUtil.createSetData(watcher, keyZNode, keyData);
+      }
+    } catch (KeeperException ke) {
+      LOG.fatal("Unable to update master key "+key.getKeyId()+
+          " in znode "+keyZNode);
+      watcher.abort("Unable to synchronize secret key "+
+          key.getKeyId()+" in zookeeper", ke);
+    } catch (IOException ioe) {
+      // this can only happen from an error serializing the key
+      watcher.abort("Failed serializing key "+key.getKeyId(), ioe);
+    }
+  }
+}
diff --git security/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java security/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
new file mode 100644
index 0000000..c2c27b9
--- /dev/null
+++ security/src/test/java/org/apache/hadoop/hbase/security/token/TestTokenAuthentication.java
@@ -0,0 +1,146 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.security.PrivilegedExceptionAction;
+import java.util.UUID;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+import org.apache.hadoop.hbase.ipc.HBaseRPC;
+import org.apache.hadoop.hbase.ipc.RequestContext;
+import org.apache.hadoop.hbase.ipc.RpcServer;
+import org.apache.hadoop.hbase.ipc.SecureRpcEngine;
+import org.apache.hadoop.hbase.ipc.SecureServer;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.SecretManager;
+
+import org.apache.hadoop.security.token.Token;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Tests for authentication token creation and usage
+ */
+public class TestTokenAuthentication {
+  public static interface IdentityProtocol extends CoprocessorProtocol {
+    public String whoami();
+    public String getAuthMethod();
+  }
+
+  public static class IdentityCoprocessor extends BaseEndpointCoprocessor
+      implements IdentityProtocol {
+    public String whoami() {
+      return RequestContext.getRequestUserName();
+    }
+
+    public String getAuthMethod() {
+      UserGroupInformation ugi = null;
+      User user = RequestContext.getRequestUser();
+      if (user != null) {
+        ugi = user.getUGI();
+      }
+      if (ugi != null) {
+        return ugi.getAuthenticationMethod().toString();
+      }
+      return null;
+    }
+  }
+
+  private static HBaseTestingUtility TEST_UTIL;
+  private static AuthenticationTokenSecretManager secretManager;
+
+  @BeforeClass
+  public static void setupBeforeClass() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    Configuration conf = TEST_UTIL.getConfiguration();
+    conf.set(HBaseRPC.RPC_ENGINE_PROP, SecureRpcEngine.class.getName());
+    conf.set("hbase.coprocessor.region.classes",
+        IdentityCoprocessor.class.getName());
+    TEST_UTIL.startMiniCluster();
+    HRegionServer rs = TEST_UTIL.getMiniHBaseCluster().getRegionServer(0);
+    RpcServer server = rs.getRpcServer();
+    assertTrue(server instanceof SecureServer);
+    SecretManager mgr =
+        ((SecureServer)server).getSecretManager();
+    assertTrue(mgr instanceof AuthenticationTokenSecretManager);
+    secretManager = (AuthenticationTokenSecretManager)mgr;
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Test
+  public void testTokenCreation() throws Exception {
+    Token<AuthenticationTokenIdentifier> token =
+        secretManager.generateToken("testuser");
+
+    AuthenticationTokenIdentifier ident = new AuthenticationTokenIdentifier();
+    Writables.getWritable(token.getIdentifier(), ident);
+    assertEquals("Token username should match", "testuser",
+        ident.getUsername());
+    byte[] passwd = secretManager.retrievePassword(ident);
+    assertTrue("Token password and password from secret manager should match",
+        Bytes.equals(token.getPassword(), passwd));
+  }
+
+  // @Test - Disable due to kerberos requirement
+  public void testTokenAuthentication() throws Exception {
+    UserGroupInformation testuser =
+        UserGroupInformation.createUserForTesting("testuser", new String[]{"testgroup"});
+
+    testuser.setAuthenticationMethod(
+        UserGroupInformation.AuthenticationMethod.TOKEN);
+    final Configuration conf = TEST_UTIL.getConfiguration();
+    conf.set("hadoop.security.authentication", "kerberos");
+    conf.set("randomkey", UUID.randomUUID().toString());
+    testuser.setConfiguration(conf);
+    Token<AuthenticationTokenIdentifier> token =
+        secretManager.generateToken("testuser");
+    testuser.addToken(token);
+
+    // verify the server authenticates us as this token user
+    testuser.doAs(new PrivilegedExceptionAction<Object>() {
+      public Object run() throws Exception {
+        HTable table = new HTable(conf, ".META.");
+        IdentityProtocol prot = table.coprocessorProxy(
+            IdentityProtocol.class, HConstants.EMPTY_START_ROW);
+        String myname = prot.whoami();
+        assertEquals("testuser", myname);
+        String authMethod = prot.getAuthMethod();
+        assertEquals("TOKEN", authMethod);
+        return null;
+      }
+    });
+  }
+}
diff --git security/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java security/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java
new file mode 100644
index 0000000..f93c189
--- /dev/null
+++ security/src/test/java/org/apache/hadoop/hbase/security/token/TestZKSecretWatcher.java
@@ -0,0 +1,251 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.token;
+
+import static org.junit.Assert.*;
+
+import java.util.Map;
+
+import com.google.common.collect.Maps;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Abortable;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * Test the synchronization of token authentication master keys through
+ * ZKSecretWatcher
+ */
+public class TestZKSecretWatcher {
+  private static Log LOG = LogFactory.getLog(TestZKSecretWatcher.class);
+  private static HBaseTestingUtility TEST_UTIL;
+  private static AuthenticationTokenSecretManager KEY_MASTER;
+  private static AuthenticationTokenSecretManager KEY_SLAVE;
+  private static AuthenticationTokenSecretManager KEY_SLAVE2;
+  private static AuthenticationTokenSecretManager KEY_SLAVE3;
+
+  private static class MockAbortable implements Abortable {
+    private boolean abort;
+    public void abort(String reason, Throwable e) {
+      LOG.info("Aborting: "+reason, e);
+      abort = true;
+    }
+
+    public boolean isAborted() {
+      return abort;
+    }
+  }
+
+  @BeforeClass
+  public static void setupBeforeClass() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.startMiniZKCluster();
+    Configuration conf = TEST_UTIL.getConfiguration();
+
+    ZooKeeperWatcher zk = newZK(conf, "server1", new MockAbortable());
+    AuthenticationTokenSecretManager[] tmp = new AuthenticationTokenSecretManager[2];
+    tmp[0] = new AuthenticationTokenSecretManager(
+        conf, zk, "server1", 60*60*1000, 60*1000);
+    tmp[0].start();
+
+    zk = newZK(conf, "server2", new MockAbortable());
+    tmp[1] = new AuthenticationTokenSecretManager(
+        conf, zk, "server2", 60*60*1000, 60*1000);
+    tmp[1].start();
+
+    while (KEY_MASTER == null) {
+      for (int i=0; i<2; i++) {
+        if (tmp[i].isMaster()) {
+          KEY_MASTER = tmp[i];
+          KEY_SLAVE = tmp[ i+1 % 2 ];
+          break;
+        }
+      }
+      Thread.sleep(500);
+    }
+    LOG.info("Master is "+KEY_MASTER.getName()+
+        ", slave is "+KEY_SLAVE.getName());
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniZKCluster();
+  }
+
+  @Test
+  public void testKeyUpdate() throws Exception {
+    // sanity check
+    assertTrue(KEY_MASTER.isMaster());
+    assertFalse(KEY_SLAVE.isMaster());
+    int maxKeyId = 0;
+
+    KEY_MASTER.rollCurrentKey();
+    AuthenticationKey key1 = KEY_MASTER.getCurrentKey();
+    assertNotNull(key1);
+    LOG.debug("Master current key: "+key1.getKeyId());
+
+    // wait for slave to update
+    Thread.sleep(1000);
+    AuthenticationKey slaveCurrent = KEY_SLAVE.getCurrentKey();
+    assertNotNull(slaveCurrent);
+    assertEquals(key1, slaveCurrent);
+    LOG.debug("Slave current key: "+slaveCurrent.getKeyId());
+
+    // generate two more keys then expire the original
+    KEY_MASTER.rollCurrentKey();
+    AuthenticationKey key2 = KEY_MASTER.getCurrentKey();
+    LOG.debug("Master new current key: "+key2.getKeyId());
+    KEY_MASTER.rollCurrentKey();
+    AuthenticationKey key3 = KEY_MASTER.getCurrentKey();
+    LOG.debug("Master new current key: "+key3.getKeyId());
+
+    // force expire the original key
+    key1.setExpiration(EnvironmentEdgeManager.currentTimeMillis() - 1000);
+    KEY_MASTER.removeExpiredKeys();
+    // verify removed from master
+    assertNull(KEY_MASTER.getKey(key1.getKeyId()));
+
+    // wait for slave to catch up
+    Thread.sleep(1000);
+    // make sure the slave has both new keys
+    AuthenticationKey slave2 = KEY_SLAVE.getKey(key2.getKeyId());
+    assertNotNull(slave2);
+    assertEquals(key2, slave2);
+    AuthenticationKey slave3 = KEY_SLAVE.getKey(key3.getKeyId());
+    assertNotNull(slave3);
+    assertEquals(key3, slave3);
+    slaveCurrent = KEY_SLAVE.getCurrentKey();
+    assertEquals(key3, slaveCurrent);
+    LOG.debug("Slave current key: "+slaveCurrent.getKeyId());
+
+    // verify that the expired key has been removed
+    assertNull(KEY_SLAVE.getKey(key1.getKeyId()));
+
+    // bring up a new slave
+    Configuration conf = TEST_UTIL.getConfiguration();
+    ZooKeeperWatcher zk = newZK(conf, "server3", new MockAbortable());
+    KEY_SLAVE2 = new AuthenticationTokenSecretManager(
+        conf, zk, "server3", 60*60*1000, 60*1000);
+    KEY_SLAVE2.start();
+
+    Thread.sleep(1000);
+    // verify the new slave has current keys (and not expired)
+    slave2 = KEY_SLAVE2.getKey(key2.getKeyId());
+    assertNotNull(slave2);
+    assertEquals(key2, slave2);
+    slave3 = KEY_SLAVE2.getKey(key3.getKeyId());
+    assertNotNull(slave3);
+    assertEquals(key3, slave3);
+    slaveCurrent = KEY_SLAVE2.getCurrentKey();
+    assertEquals(key3, slaveCurrent);
+    assertNull(KEY_SLAVE2.getKey(key1.getKeyId()));
+
+    // test leader failover
+    KEY_MASTER.stop();
+
+    // wait for master to stop
+    Thread.sleep(1000);
+    assertFalse(KEY_MASTER.isMaster());
+
+    // check for a new master
+    AuthenticationTokenSecretManager[] mgrs =
+        new AuthenticationTokenSecretManager[]{ KEY_SLAVE, KEY_SLAVE2 };
+    AuthenticationTokenSecretManager newMaster = null;
+    int tries = 0;
+    while (newMaster == null && tries++ < 5) {
+      for (AuthenticationTokenSecretManager mgr : mgrs) {
+        if (mgr.isMaster()) {
+          newMaster = mgr;
+          break;
+        }
+      }
+      if (newMaster == null) {
+        Thread.sleep(500);
+      }
+    }
+    assertNotNull(newMaster);
+
+    AuthenticationKey current = newMaster.getCurrentKey();
+    // new master will immediately roll the current key, so it's current may be greater
+    assertTrue(current.getKeyId() >= slaveCurrent.getKeyId());
+    LOG.debug("New master, current key: "+current.getKeyId());
+
+    // roll the current key again on new master and verify the key ID increments
+    newMaster.rollCurrentKey();
+    AuthenticationKey newCurrent = newMaster.getCurrentKey();
+    LOG.debug("New master, rolled new current key: "+newCurrent.getKeyId());
+    assertTrue(newCurrent.getKeyId() > current.getKeyId());
+
+    // add another slave
+    ZooKeeperWatcher zk3 = newZK(conf, "server4", new MockAbortable());
+    KEY_SLAVE3 = new AuthenticationTokenSecretManager(
+        conf, zk3, "server4", 60*60*1000, 60*1000);
+    KEY_SLAVE3.start();
+    Thread.sleep(5000);
+
+    // check master failover again
+    newMaster.stop();
+
+    // wait for master to stop
+    Thread.sleep(5000);
+    assertFalse(newMaster.isMaster());
+
+    // check for a new master
+    mgrs = new AuthenticationTokenSecretManager[]{ KEY_SLAVE, KEY_SLAVE2, KEY_SLAVE3 };
+    newMaster = null;
+    tries = 0;
+    while (newMaster == null && tries++ < 5) {
+      for (AuthenticationTokenSecretManager mgr : mgrs) {
+        if (mgr.isMaster()) {
+          newMaster = mgr;
+          break;
+        }
+      }
+      if (newMaster == null) {
+        Thread.sleep(500);
+      }
+    }
+    assertNotNull(newMaster);
+
+    AuthenticationKey current2 = newMaster.getCurrentKey();
+    // new master will immediately roll the current key, so it's current may be greater
+    assertTrue(current2.getKeyId() >= newCurrent.getKeyId());
+    LOG.debug("New master 2, current key: "+current2.getKeyId());
+
+    // roll the current key again on new master and verify the key ID increments
+    newMaster.rollCurrentKey();
+    AuthenticationKey newCurrent2 = newMaster.getCurrentKey();
+    LOG.debug("New master 2, rolled new current key: "+newCurrent2.getKeyId());
+    assertTrue(newCurrent2.getKeyId() > current2.getKeyId());
+  }
+
+  private static ZooKeeperWatcher newZK(Configuration conf, String name,
+      Abortable abort) throws Exception {
+    Configuration copy = HBaseConfiguration.create(conf);
+    ZooKeeperWatcher zk = new ZooKeeperWatcher(copy, name, abort);
+    return zk;
+  }
+}
diff --git security/src/test/resources/hbase-site.xml security/src/test/resources/hbase-site.xml
new file mode 100644
index 0000000..dcc7df2
--- /dev/null
+++ security/src/test/resources/hbase-site.xml
@@ -0,0 +1,153 @@
+<?xml version="1.0"?>
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
+<!--
+/**
+ * Copyright 2007 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+<configuration>
+  <property>
+    <name>hbase.regionserver.msginterval</name>
+    <value>1000</value>
+    <description>Interval between messages from the RegionServer to HMaster
+    in milliseconds.  Default is 15. Set this value low if you want unit
+    tests to be responsive.
+    </description>
+  </property>
+  <property>
+    <name>hbase.client.pause</name>
+    <value>1000</value>
+    <description>General client pause value.  Used mostly as value to wait
+    before running a retry of a failed get, region lookup, etc.</description>
+  </property>
+  <property>
+    <name>hbase.client.retries.number</name>
+    <value>10</value>
+    <description>Maximum retries.  Used as maximum for all retryable
+    operations such as fetching of the root region from root region
+    server, getting a cell's value, starting a row update, etc.
+    Default: 10.
+    </description>
+  </property>
+  <property>
+    <name>hbase.server.thread.wakefrequency</name>
+    <value>1000</value>
+    <description>Time to sleep in between searches for work (in milliseconds).
+    Used as sleep interval by service threads such as META scanner and log roller.
+    </description>
+  </property>
+  <property>
+    <name>hbase.master.event.waiting.time</name>
+    <value>50</value>
+    <description>Time to sleep between checks to see if a table event took place.
+    </description>
+  </property>
+  <property>
+    <name>hbase.regionserver.handler.count</name>
+    <value>5</value>
+    <description>Count of RPC Server instances spun up on RegionServers
+    Same property is used by the HMaster for count of master handlers.
+    Default is 10.
+    </description>
+  </property>
+  <property>
+    <name>hbase.master.info.port</name>
+    <value>-1</value>
+    <description>The port for the hbase master web UI
+    Set to -1 if you do not want the info server to run.
+    </description>
+  </property>
+  <property>
+    <name>hbase.regionserver.info.port</name>
+    <value>-1</value>
+    <description>The port for the hbase regionserver web UI
+    Set to -1 if you do not want the info server to run.
+    </description>
+  </property>
+  <property>
+    <name>hbase.regionserver.info.port.auto</name>
+    <value>true</value>
+    <description>Info server auto port bind. Enables automatic port
+    search if hbase.regionserver.info.port is already in use.
+    Enabled for testing to run multiple tests on one machine.
+    </description>
+  </property>
+  <property>
+    <name>hbase.master.lease.thread.wakefrequency</name>
+    <value>3000</value>
+    <description>The interval between checks for expired region server leases.
+    This value has been reduced due to the other reduced values above so that
+    the master will notice a dead region server sooner. The default is 15 seconds.
+    </description>
+  </property>
+  <property>
+    <name>hbase.regionserver.optionalcacheflushinterval</name>
+    <value>1000</value>
+    <description>
+    Amount of time to wait since the last time a region was flushed before
+    invoking an optional cache flush. Default 60,000.
+    </description>
+  </property>
+  <property>
+    <name>hbase.regionserver.safemode</name>
+    <value>false</value>
+    <description>
+    Turn on/off safe mode in region server. Always on for production, always off
+    for tests.
+    </description>
+  </property>
+  <property>
+    <name>hbase.hregion.max.filesize</name>
+    <value>67108864</value>
+    <description>
+    Maximum desired file size for an HRegion.  If filesize exceeds
+    value + (value / 2), the HRegion is split in two.  Default: 256M.
+
+    Keep the maximum filesize small so we split more often in tests.
+    </description>
+  </property>
+  <property>
+    <name>hadoop.log.dir</name>
+    <value>${user.dir}/../logs</value>
+  </property>
+  <property>
+    <name>hbase.zookeeper.property.clientPort</name>
+    <value>21818</value>
+    <description>Property from ZooKeeper's config zoo.cfg.
+    The port at which the clients will connect.
+    </description>
+  </property>
+  <property>
+    <name>hbase.defaults.for.version.skip</name>
+    <value>true</value>
+    <description>
+    Set to true to skip the 'hbase.defaults.for.version'.
+    Setting this to true can be useful in contexts other than
+    the other side of a maven generation; i.e. running in an
+    ide.  You'll want to set this boolean to true to avoid
+    seeing the RuntimException complaint: "hbase-default.xml file
+    seems to be for and old version of HBase (@@@VERSION@@@), this
+    version is X.X.X-SNAPSHOT"
+    </description>
+  </property>
+  <property>
+   <name>hbase.rpc.engine</name>
+   <value>org.apache.hadoop.hbase.ipc.SecureRpcEngine</value>
+  </property>
+</configuration>
diff --git src/assembly/all.xml src/assembly/all.xml
index 3ad8ab3..2b3e6be 100644
--- src/assembly/all.xml
+++ src/assembly/all.xml
@@ -57,8 +57,8 @@
       <directory>target</directory>
       <outputDirectory>/</outputDirectory>
       <includes>
-          <include>hbase-${project.version}.jar</include>
-          <include>hbase-${project.version}-tests.jar</include>
+          <include>${project.build.finalName}.jar</include>
+          <include>${project.build.finalName}-tests.jar</include>
       </includes>
     </fileSet>
     <fileSet>
diff --git src/main/java/org/apache/hadoop/hbase/HServerAddress.java src/main/java/org/apache/hadoop/hbase/HServerAddress.java
index f28240c..e189aaf 100644
--- src/main/java/org/apache/hadoop/hbase/HServerAddress.java
+++ src/main/java/org/apache/hadoop/hbase/HServerAddress.java
@@ -121,7 +121,10 @@ public class HServerAddress implements WritableComparable<HServerAddress> {
 
   /** @return Hostname */
   public String getHostname() {
-    return this.address.getHostName();
+    // Kerberos is case-sensitive, and dictates that, where hostnames are
+    // case-insensitive (as in DNS), the lowercase version must be used
+    // So here we lowercase to properly interact with kerberos auth
+    return this.address.getHostName().toLowerCase();
   }
 
   /**
diff --git src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
index cba7bd1..c1ecdf6 100644
--- src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
+++ src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java
@@ -68,6 +68,7 @@ import org.apache.hadoop.hbase.ipc.ExecRPCInvoker;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Addressing;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -137,6 +138,8 @@ public class HConnectionManager {
 
   public static final int MAX_CACHED_HBASE_INSTANCES;
 
+  private static Log LOG = LogFactory.getLog(HConnectionManager.class);
+
   static {
     // We set instances to one more than the value specified for {@link
     // HConstants#ZOOKEEPER_MAX_CLIENT_CNXNS}. By default, the zk default max
@@ -373,6 +376,7 @@ public class HConnectionManager {
         HConstants.HBASE_CLIENT_INSTANCE_ID };
 
     private Map<String, String> properties;
+    private String username;
 
     public HConnectionKey(Configuration conf) {
       Map<String, String> m = new HashMap<String, String>();
@@ -385,12 +389,25 @@ public class HConnectionManager {
         }
       }
       this.properties = Collections.unmodifiableMap(m);
+
+      try {
+        User currentUser = User.getCurrent();
+        if (currentUser != null) {
+          username = currentUser.getName();
+        }
+      } catch (IOException ioe) {
+        LOG.warn("Error obtaining current user, skipping username in HConnectionKey",
+            ioe);
+      }
     }
 
     @Override
     public int hashCode() {
       final int prime = 31;
       int result = 1;
+      if (username != null) {
+        result = username.hashCode();
+      }
       for (String property : CONNECTION_PROPERTIES) {
         String value = properties.get(property);
         if (value != null) {
@@ -410,6 +427,11 @@ public class HConnectionManager {
       if (getClass() != obj.getClass())
         return false;
       HConnectionKey that = (HConnectionKey) obj;
+      if (this.username != null && !this.username.equals(that.username)) {
+        return false;
+      } else if (this.username == null && that.username != null) {
+        return false;
+      }
       if (this.properties == null) {
         if (that.properties != null) {
           return false;
diff --git src/main/java/org/apache/hadoop/hbase/ipc/ConnectionHeader.java src/main/java/org/apache/hadoop/hbase/ipc/ConnectionHeader.java
index 904c107..6328310 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/ConnectionHeader.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/ConnectionHeader.java
@@ -25,26 +25,26 @@ import java.io.IOException;
 
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.hbase.security.User;
 
 /**
  * The IPC connection header sent by the client to the server
  * on connection establishment.
  */
 class ConnectionHeader implements Writable {
-  private String protocol;
+  protected String protocol;
 
   public ConnectionHeader() {}
 
   /**
    * Create a new {@link ConnectionHeader} with the given <code>protocol</code>
-   * and {@link UserGroupInformation}.
+   * and {@link User}.
    * @param protocol protocol used for communication between the IPC client
    *                 and the server
-   * @param ugi {@link UserGroupInformation} of the client communicating with
+   * @param user {@link User} of the client communicating with
    *            the server
    */
-  public ConnectionHeader(String protocol, UserGroupInformation ugi) {
+  public ConnectionHeader(String protocol, User user) {
     this.protocol = protocol;
   }
 
@@ -65,7 +65,7 @@ class ConnectionHeader implements Writable {
     return protocol;
   }
 
-  public UserGroupInformation getUgi() {
+  public User getUser() {
     return null;
   }
 
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
index 4086829..7084a2f 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java
@@ -45,6 +45,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.PoolMap;
 import org.apache.hadoop.hbase.util.PoolMap.PoolType;
@@ -53,9 +54,7 @@ import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.ipc.RemoteException;
-import org.apache.hadoop.hbase.ipc.VersionedProtocol;
 import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /** A client for an IPC service.  IPC calls take a single {@link Writable} as a
@@ -159,7 +158,7 @@ public class HBaseClient {
   }
 
   /** A call waiting for a value. */
-  private class Call {
+  protected class Call {
     final int id;                                       // call id
     final Writable param;                               // parameter
     Writable value;                               // value, null if error
@@ -210,18 +209,18 @@ public class HBaseClient {
   /** Thread that reads responses and notifies callers.  Each connection owns a
    * socket connected to a remote address.  Calls are multiplexed through this
    * socket: responses may be delivered out of order. */
-  private class Connection extends Thread {
+  protected class Connection extends Thread {
     private ConnectionHeader header;              // connection header
-    private ConnectionId remoteId;
-    private Socket socket = null;                 // connected socket
-    private DataInputStream in;
-    private DataOutputStream out;
+    protected ConnectionId remoteId;
+    protected Socket socket = null;                 // connected socket
+    protected DataInputStream in;
+    protected DataOutputStream out;
 
     // currently active calls
-    private final ConcurrentSkipListMap<Integer, Call> calls = new ConcurrentSkipListMap<Integer, Call>();
-    private final AtomicLong lastActivity = new AtomicLong();// last I/O activity time
+    protected final ConcurrentSkipListMap<Integer, Call> calls = new ConcurrentSkipListMap<Integer, Call>();
+    protected final AtomicLong lastActivity = new AtomicLong();// last I/O activity time
     protected final AtomicBoolean shouldCloseConnection = new AtomicBoolean();  // indicate if the connection is closed
-    private IOException closeException; // close reason
+    protected IOException closeException; // close reason
 
     public Connection(ConnectionId remoteId) throws IOException {
       if (remoteId.getAddress().isUnresolved()) {
@@ -229,7 +228,7 @@ public class HBaseClient {
                                        remoteId.getAddress().getHostName());
       }
       this.remoteId = remoteId;
-      UserGroupInformation ticket = remoteId.getTicket();
+      User ticket = remoteId.getTicket();
       Class<? extends VersionedProtocol> protocol = remoteId.getProtocol();
 
       header = new ConnectionHeader(
@@ -237,12 +236,12 @@ public class HBaseClient {
 
       this.setName("IPC Client (" + socketFactory.hashCode() +") connection to " +
         remoteId.getAddress().toString() +
-        ((ticket==null)?" from an unknown user": (" from " + ticket.getUserName())));
+        ((ticket==null)?" from an unknown user": (" from " + ticket.getName())));
       this.setDaemon(true);
     }
 
     /** Update lastActivity with the current time. */
-    private void touch() {
+    protected void touch() {
       lastActivity.set(System.currentTimeMillis());
     }
 
@@ -265,7 +264,7 @@ public class HBaseClient {
      * reading. If no failure is detected, it retries until at least
      * a byte is read.
      */
-    private class PingInputStream extends FilterInputStream {
+    protected class PingInputStream extends FilterInputStream {
       /* constructor */
       protected PingInputStream(InputStream in) {
         super(in);
@@ -317,40 +316,50 @@ public class HBaseClient {
       }
     }
 
+    protected synchronized void setupConnection() throws IOException {
+      short ioFailures = 0;
+      short timeoutFailures = 0;
+      while (true) {
+        try {
+          this.socket = socketFactory.createSocket();
+          this.socket.setTcpNoDelay(tcpNoDelay);
+          this.socket.setKeepAlive(tcpKeepAlive);
+          // connection time out is 20s
+          NetUtils.connect(this.socket, remoteId.getAddress(),
+              getSocketTimeout(conf));
+          if (remoteId.rpcTimeout > 0) {
+            pingInterval = remoteId.rpcTimeout; // overwrite pingInterval
+          }
+          this.socket.setSoTimeout(pingInterval);
+          return;
+        } catch (SocketTimeoutException toe) {
+          /* The max number of retries is 45,
+           * which amounts to 20s*45 = 15 minutes retries.
+           */
+          handleConnectionFailure(timeoutFailures++, maxRetries, toe);
+        } catch (IOException ie) {
+          handleConnectionFailure(ioFailures++, maxRetries, ie);
+        }
+      }
+    }
+
     /** Connect to the server and set up the I/O streams. It then sends
      * a header to the server and starts
      * the connection thread that waits for responses.
      * @throws java.io.IOException e
      */
-    protected synchronized void setupIOstreams() throws IOException {
+    protected synchronized void setupIOstreams()
+        throws IOException, InterruptedException {
+
       if (socket != null || shouldCloseConnection.get()) {
         return;
       }
 
-      short ioFailures = 0;
-      short timeoutFailures = 0;
       try {
         if (LOG.isDebugEnabled()) {
-          LOG.debug("Connecting to "+remoteId.getAddress());
-        }
-        while (true) {
-          try {
-            this.socket = socketFactory.createSocket();
-            this.socket.setTcpNoDelay(tcpNoDelay);
-            this.socket.setKeepAlive(tcpKeepAlive);
-            NetUtils.connect(this.socket, remoteId.getAddress(),
-              getSocketTimeout(conf));
-            if (remoteId.rpcTimeout > 0) {
-              pingInterval = remoteId.rpcTimeout; // overwrite pingInterval
-            }
-            this.socket.setSoTimeout(pingInterval);
-            break;
-          } catch (SocketTimeoutException toe) {
-            handleConnectionFailure(timeoutFailures++, maxRetries, toe);
-          } catch (IOException ie) {
-            handleConnectionFailure(ioFailures++, maxRetries, ie);
-          }
+          LOG.debug("Connecting to "+remoteId);
         }
+        setupConnection();
         this.in = new DataInputStream(new BufferedInputStream
             (new PingInputStream(NetUtils.getInputStream(socket))));
         this.out = new DataOutputStream
@@ -370,7 +379,22 @@ public class HBaseClient {
       }
     }
 
-    /* Handle connection failures
+    protected void closeConnection() {
+      // close the current connection
+      if (socket != null) {
+        try {
+          socket.close();
+        } catch (IOException e) {
+          LOG.warn("Not able to close a socket", e);
+        }
+      }
+      // set socket to null so that the next call to setupIOstreams
+      // can start the process of connect all over again.
+      socket = null;
+    }
+
+    /**
+     *  Handle connection failures
      *
      * If the current number of retries is equal to the max number of retries,
      * stop retrying and throw the exception; Otherwise backoff N seconds and
@@ -386,17 +410,8 @@ public class HBaseClient {
      */
     private void handleConnectionFailure(
         int curRetries, int maxRetries, IOException ioe) throws IOException {
-      // close the current connection
-      if (socket != null) { // could be null if the socket creation failed
-        try {
-          socket.close();
-        } catch (IOException e) {
-          LOG.warn("Not able to close a socket", e);
-        }
-      }
-      // set socket to null so that the next call to setupIOstreams
-      // can start the process of connect all over again.
-      socket = null;
+
+      closeConnection();
 
       // throw the exception if the maximum number of retries is reached
       if (curRetries >= maxRetries) {
@@ -435,7 +450,7 @@ public class HBaseClient {
      * Return true if it is time to read a response; false otherwise.
      */
     @SuppressWarnings({"ThrowableInstanceNeverThrown"})
-    private synchronized boolean waitForWork() {
+    protected synchronized boolean waitForWork() {
       if (calls.isEmpty() && !shouldCloseConnection.get()  && running.get())  {
         long timeout = maxIdleTime-
               (System.currentTimeMillis()-lastActivity.get());
@@ -541,7 +556,7 @@ public class HBaseClient {
     /* Receive a response.
      * Because only one receiver, so no synchronization on in.
      */
-    private void receiveResponse() {
+    protected void receiveResponse() {
       if (shouldCloseConnection.get()) {
         return;
       }
@@ -598,7 +613,7 @@ public class HBaseClient {
       }
     }
 
-    private synchronized void markClosed(IOException e) {
+    protected synchronized void markClosed(IOException e) {
       if (shouldCloseConnection.compareAndSet(false, true)) {
         closeException = e;
         notifyAll();
@@ -606,7 +621,7 @@ public class HBaseClient {
     }
 
     /** Close the connection. */
-    private synchronized void close() {
+    protected synchronized void close() {
       if (!shouldCloseConnection.get()) {
         LOG.error("The connection is not in the closed state");
         return;
@@ -647,11 +662,11 @@ public class HBaseClient {
     }
 
     /* Cleanup all calls and mark them as done */
-    private void cleanupCalls() {
+    protected void cleanupCalls() {
       cleanupCalls(0);
     }
 
-    private void cleanupCalls(long rpcTimeout) {
+    protected void cleanupCalls(long rpcTimeout) {
       Iterator<Entry<Integer, Call>> itor = calls.entrySet().iterator();
       while (itor.hasNext()) {
         Call c = itor.next().getValue();
@@ -687,7 +702,7 @@ public class HBaseClient {
   }
 
   /** Call implementation used for parallel calls. */
-  private class ParallelCall extends Call {
+  protected class ParallelCall extends Call {
     private final ParallelResults results;
     protected final int index;
 
@@ -705,7 +720,7 @@ public class HBaseClient {
   }
 
   /** Result collector for parallel calls. */
-  private static class ParallelResults {
+  protected static class ParallelResults {
     protected final Writable[] values;
     protected int size;
     protected int count;
@@ -778,7 +793,7 @@ public class HBaseClient {
    * @return either a {@link PoolType#RoundRobin} or
    *         {@link PoolType#ThreadLocal}
    */
-  private static PoolType getPoolType(Configuration config) {
+  protected static PoolType getPoolType(Configuration config) {
     return PoolType.valueOf(config.get(HConstants.HBASE_CLIENT_IPC_POOL_TYPE),
         PoolType.RoundRobin, PoolType.ThreadLocal);
   }
@@ -790,7 +805,7 @@ public class HBaseClient {
    * @param config
    * @return the maximum pool size
    */
-  private static int getPoolSize(Configuration config) {
+  protected static int getPoolSize(Configuration config) {
     return config.getInt(HConstants.HBASE_CLIENT_IPC_POOL_SIZE, 1);
   }
 
@@ -843,7 +858,7 @@ public class HBaseClient {
   }
 
   public Writable call(Writable param, InetSocketAddress addr,
-                       UserGroupInformation ticket, int rpcTimeout)
+                       User ticket, int rpcTimeout)
                        throws IOException, InterruptedException {
     return call(param, addr, null, ticket, rpcTimeout);
   }
@@ -855,7 +870,7 @@ public class HBaseClient {
    * threw an exception. */
   public Writable call(Writable param, InetSocketAddress addr,
                        Class<? extends VersionedProtocol> protocol,
-                       UserGroupInformation ticket, int rpcTimeout)
+                       User ticket, int rpcTimeout)
       throws InterruptedException, IOException {
     Call call = new Call(param);
     Connection connection = getConnection(addr, protocol, ticket, rpcTimeout, call);
@@ -902,7 +917,7 @@ public class HBaseClient {
    * @return an exception to throw
    */
   @SuppressWarnings({"ThrowableInstanceNeverThrown"})
-  private IOException wrapException(InetSocketAddress addr,
+  protected IOException wrapException(InetSocketAddress addr,
                                          IOException exception) {
     if (exception instanceof ConnectException) {
       //connection refused; include the host:port in the error
@@ -929,7 +944,7 @@ public class HBaseClient {
    * @param addresses socket addresses
    * @return  Writable[]
    * @throws IOException e
-   * @deprecated Use {@link #call(Writable[], InetSocketAddress[], Class, UserGroupInformation)} instead
+   * @deprecated Use {@link #call(Writable[], InetSocketAddress[], Class, User)} instead
    */
   @Deprecated
   public Writable[] call(Writable[] params, InetSocketAddress[] addresses)
@@ -943,7 +958,7 @@ public class HBaseClient {
    * contains nulls for calls that timed out or errored.  */
   public Writable[] call(Writable[] params, InetSocketAddress[] addresses,
                          Class<? extends VersionedProtocol> protocol,
-                         UserGroupInformation ticket)
+                         User ticket)
       throws IOException, InterruptedException {
     if (addresses.length == 0) return new Writable[0];
 
@@ -976,12 +991,12 @@ public class HBaseClient {
 
   /* Get a connection from the pool, or create a new one and add it to the
    * pool.  Connections to a given host/port are reused. */
-  private Connection getConnection(InetSocketAddress addr,
+  protected Connection getConnection(InetSocketAddress addr,
                                    Class<? extends VersionedProtocol> protocol,
-                                   UserGroupInformation ticket,
+                                   User ticket,
                                    int rpcTimeout,
                                    Call call)
-                                   throws IOException {
+                                   throws IOException, InterruptedException {
     if (!running.get()) {
       // the client is stopped
       throw new IOException("The client is stopped");
@@ -1014,16 +1029,16 @@ public class HBaseClient {
    * This class holds the address and the user ticket. The client connections
    * to servers are uniquely identified by <remoteAddress, ticket>
    */
-  private static class ConnectionId {
+  protected static class ConnectionId {
     final InetSocketAddress address;
-    final UserGroupInformation ticket;
-    final private int rpcTimeout;
+    final User ticket;
+    final int rpcTimeout;
     Class<? extends VersionedProtocol> protocol;
     private static final int PRIME = 16777619;
 
     ConnectionId(InetSocketAddress address,
         Class<? extends VersionedProtocol> protocol,
-        UserGroupInformation ticket,
+        User ticket,
         int rpcTimeout) {
       this.protocol = protocol;
       this.address = address;
@@ -1039,7 +1054,7 @@ public class HBaseClient {
       return protocol;
     }
 
-    UserGroupInformation getTicket() {
+    User getTicket() {
       return ticket;
     }
 
@@ -1048,8 +1063,8 @@ public class HBaseClient {
      if (obj instanceof ConnectionId) {
        ConnectionId id = (ConnectionId) obj;
        return address.equals(id.address) && protocol == id.protocol &&
-           ticket == id.ticket && rpcTimeout == id.rpcTimeout;
-       //Note : ticket is a ref comparision.
+              ((ticket != null && ticket.equals(id.ticket)) ||
+               (ticket == id.ticket)) && rpcTimeout == id.rpcTimeout;
      }
      return false;
     }
@@ -1058,8 +1073,7 @@ public class HBaseClient {
     public int hashCode() {
       return (address.hashCode() + PRIME * (
                   PRIME * System.identityHashCode(protocol) ^
-                  System.identityHashCode(ticket)
-                )) ^ rpcTimeout;
+             (ticket == null ? 0 : ticket.hashCode()) )) ^ rpcTimeout;
     }
   }
 }
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
index 054b92b..7eb9e03 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
@@ -26,12 +26,10 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.client.RetriesExhaustedException;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.hbase.ipc.VersionedProtocol;
 import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.ReflectionUtils;
-
 import javax.net.SocketFactory;
 import java.io.IOException;
 import java.lang.reflect.Method;
@@ -78,7 +76,12 @@ public class HBaseRPC {
     super();
   }                                  // no public ctor
 
-  private static final String RPC_ENGINE_PROP = "hbase.rpc.engine";
+  /**
+   * Configuration key for the {@link RpcEngine} implementation to load to
+   * handle connection protocols.  Handlers for individual protocols can be
+   * configured using {@code "hbase.rpc.engine." + protocol.class.name}.
+   */
+  public static final String RPC_ENGINE_PROP = "hbase.rpc.engine";
 
   // cache of RpcEngines by protocol
   private static final Map<Class,RpcEngine> PROTOCOL_ENGINES
@@ -274,8 +277,8 @@ public class HBaseRPC {
   public static VersionedProtocol getProxy(Class<? extends VersionedProtocol> protocol,
       long clientVersion, InetSocketAddress addr, Configuration conf,
       SocketFactory factory, int rpcTimeout) throws IOException {
-    return getProxy(protocol, clientVersion, addr, null, conf, factory,
-        rpcTimeout);
+    return getProxy(protocol, clientVersion, addr,
+        User.getCurrent(), conf, factory, rpcTimeout);
   }
 
   /**
@@ -294,7 +297,7 @@ public class HBaseRPC {
    */
   public static VersionedProtocol getProxy(
       Class<? extends VersionedProtocol> protocol,
-      long clientVersion, InetSocketAddress addr, UserGroupInformation ticket,
+      long clientVersion, InetSocketAddress addr, User ticket,
       Configuration conf, SocketFactory factory, int rpcTimeout)
   throws IOException {
     VersionedProtocol proxy =
@@ -349,11 +352,16 @@ public class HBaseRPC {
    * @param conf configuration
    * @return values
    * @throws IOException e
+   * @deprecated Instead of calling statically, use
+   *     {@link HBaseRPC#getProtocolEngine(Class, org.apache.hadoop.conf.Configuration)}
+   *     to obtain an {@link RpcEngine} instance and then use
+   *     {@link RpcEngine#call(java.lang.reflect.Method, Object[][], java.net.InetSocketAddress[], Class, org.apache.hadoop.hbase.security.User, org.apache.hadoop.conf.Configuration)}
    */
+  @Deprecated
   public static Object[] call(Method method, Object[][] params,
       InetSocketAddress[] addrs,
       Class<? extends VersionedProtocol> protocol,
-      UserGroupInformation ticket,
+      User ticket,
       Configuration conf)
     throws IOException, InterruptedException {
     return getProtocolEngine(protocol, conf)
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
index 337da78..7f6a2de 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HBaseRpcMetrics.java
@@ -27,8 +27,7 @@ import org.apache.hadoop.metrics.MetricsContext;
 import org.apache.hadoop.metrics.MetricsRecord;
 import org.apache.hadoop.metrics.MetricsUtil;
 import org.apache.hadoop.metrics.Updater;
-import org.apache.hadoop.metrics.util.MetricsRegistry;
-import org.apache.hadoop.metrics.util.MetricsTimeVaryingRate;
+import org.apache.hadoop.metrics.util.*;
 
 import java.lang.reflect.Method;
 
@@ -46,11 +45,15 @@ import java.lang.reflect.Method;
  */
 public class HBaseRpcMetrics implements Updater {
   public static final String NAME_DELIM = "$";
-  private MetricsRecord metricsRecord;
+  private final MetricsRegistry registry = new MetricsRegistry();
+  private final MetricsRecord metricsRecord;
+  private final RpcServer myServer;
   private static Log LOG = LogFactory.getLog(HBaseRpcMetrics.class);
   private final HBaseRPCStatistics rpcStatistics;
 
-  public HBaseRpcMetrics(String hostName, String port) {
+  public HBaseRpcMetrics(String hostName, String port,
+      final RpcServer server) {
+    myServer = server;
     MetricsContext context = MetricsUtil.getContext("rpc");
     metricsRecord = MetricsUtil.createRecord(context, "metrics");
 
@@ -73,13 +76,29 @@ public class HBaseRpcMetrics implements Updater {
    *  - they can be set directly by calling their set/inc methods
    *  -they can also be read directly - e.g. JMX does this.
    */
-  public final MetricsRegistry registry = new MetricsRegistry();
 
-  public MetricsTimeVaryingRate rpcQueueTime = new MetricsTimeVaryingRate("RpcQueueTime", registry);
-  public MetricsTimeVaryingRate rpcProcessingTime = new MetricsTimeVaryingRate("RpcProcessingTime", registry);
-  public MetricsTimeVaryingRate rpcSlowResponseTime = new MetricsTimeVaryingRate("RpcSlowResponse", registry);
-
-  //public Map <String, MetricsTimeVaryingRate> metricsList = Collections.synchronizedMap(new HashMap<String, MetricsTimeVaryingRate>());
+  public final MetricsTimeVaryingLong receivedBytes =
+         new MetricsTimeVaryingLong("ReceivedBytes", registry);
+  public final MetricsTimeVaryingLong sentBytes =
+         new MetricsTimeVaryingLong("SentBytes", registry);
+  public final MetricsTimeVaryingRate rpcQueueTime =
+          new MetricsTimeVaryingRate("RpcQueueTime", registry);
+  public MetricsTimeVaryingRate rpcProcessingTime =
+          new MetricsTimeVaryingRate("RpcProcessingTime", registry);
+  public final MetricsIntValue numOpenConnections =
+          new MetricsIntValue("NumOpenConnections", registry);
+  public final MetricsIntValue callQueueLen =
+          new MetricsIntValue("callQueueLen", registry);
+  public final MetricsTimeVaryingInt authenticationFailures = 
+          new MetricsTimeVaryingInt("rpcAuthenticationFailures", registry);
+  public final MetricsTimeVaryingInt authenticationSuccesses =
+          new MetricsTimeVaryingInt("rpcAuthenticationSuccesses", registry);
+  public final MetricsTimeVaryingInt authorizationFailures =
+          new MetricsTimeVaryingInt("rpcAuthorizationFailures", registry);
+  public final MetricsTimeVaryingInt authorizationSuccesses =
+         new MetricsTimeVaryingInt("rpcAuthorizationSuccesses", registry);
+  public MetricsTimeVaryingRate rpcSlowResponseTime =
+      new MetricsTimeVaryingRate("RpcSlowResponse", registry);
 
   private void initMethods(Class<? extends VersionedProtocol> protocol) {
     for (Method m : protocol.getDeclaredMethods()) {
@@ -182,19 +201,15 @@ public class HBaseRpcMetrics implements Updater {
 
   /**
    * Push the metrics to the monitoring subsystem on doUpdate() call.
-   * @param context ctx
    */
-  public void doUpdates(MetricsContext context) {
-    rpcQueueTime.pushMetric(metricsRecord);
-    rpcProcessingTime.pushMetric(metricsRecord);
-
-    synchronized (registry) {
-      // Iterate through the registry to propagate the different rpc metrics.
-
-      for (String metricName : registry.getKeyList() ) {
-        MetricsTimeVaryingRate value = (MetricsTimeVaryingRate) registry.get(metricName);
-
-        value.pushMetric(metricsRecord);
+  public void doUpdates(final MetricsContext context) {
+    synchronized (this) {
+      // ToFix - fix server to use the following two metrics directly so
+      // the metrics do not have be copied here.
+      numOpenConnections.set(myServer.getNumOpenConnections());
+      callQueueLen.set(myServer.getCallQueueLen());
+      for (MetricsBase m : registry.getMetricsList()) {
+        m.pushMetric(metricsRecord);
       }
     }
     metricsRecord.update();
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
index d9ba0ea..07ddbca 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
@@ -62,13 +62,12 @@ import org.apache.hadoop.hbase.io.HbaseObjectWritable;
 import org.apache.hadoop.hbase.io.WritableWithSize;
 import org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.ByteBufferOutputStream;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.ipc.RPC.VersionMismatch;
-import org.apache.hadoop.hbase.ipc.VersionedProtocol;
-import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
@@ -200,7 +199,7 @@ public abstract class HBaseServer implements RpcServer {
   protected BlockingQueue<Call> callQueue; // queued calls
   protected BlockingQueue<Call> priorityCallQueue;
 
-  private int highPriorityLevel;  // what level a high priority call is at
+  protected int highPriorityLevel;  // what level a high priority call is at
 
   protected final List<Connection> connectionList =
     Collections.synchronizedList(new LinkedList<Connection>());
@@ -275,7 +274,7 @@ public abstract class HBaseServer implements RpcServer {
       return param.toString() + " from " + connection.toString();
     }
 
-    private synchronized void setResponse(Object value, Status status,
+    protected synchronized void setResponse(Object value, Status status,
         String errorClass, String error) {
       // Avoid overwriting an error value in the response.  This can happen if
       // endDelayThrowing is called by another thread before the actual call
@@ -608,7 +607,7 @@ public abstract class HBaseServer implements RpcServer {
           if (errorHandler != null) {
             if (errorHandler.checkOOME(e)) {
               LOG.info(getName() + ": exiting on OOME");
-              closeCurrentConnection(key);
+              closeCurrentConnection(key, e);
               cleanupConnections(true);
               return;
             }
@@ -617,12 +616,12 @@ public abstract class HBaseServer implements RpcServer {
             // log the event and sleep for a minute and give
             // some thread(s) a chance to finish
             LOG.warn("Out of Memory in server select", e);
-            closeCurrentConnection(key);
+            closeCurrentConnection(key, e);
             cleanupConnections(true);
             try { Thread.sleep(60000); } catch (Exception ignored) {}
       }
         } catch (Exception e) {
-          closeCurrentConnection(key);
+          closeCurrentConnection(key, e);
         }
         cleanupConnections(false);
       }
@@ -644,13 +643,16 @@ public abstract class HBaseServer implements RpcServer {
       }
     }
 
-    private void closeCurrentConnection(SelectionKey key) {
+    private void closeCurrentConnection(SelectionKey key, Throwable e) {
       if (key != null) {
         Connection c = (Connection)key.attachment();
         if (c != null) {
-          if (LOG.isDebugEnabled())
-            LOG.debug(getName() + ": disconnecting client " + c.getHostAddress());
+          if (LOG.isDebugEnabled()) {
+            LOG.debug(getName() + ": disconnecting client " + c.getHostAddress() +
+                (e != null ? " on error " + e.getMessage() : ""));
+          }
           closeConnection(c);
+          key.attach(null);
         }
       }
     }
@@ -673,7 +675,7 @@ public abstract class HBaseServer implements RpcServer {
         try {
           reader.startAdd();
           SelectionKey readKey = reader.registerChannel(channel);
-          c = new Connection(channel, System.currentTimeMillis());
+          c = getConnection(channel, System.currentTimeMillis());
           readKey.attach(c);
           synchronized (connectionList) {
             connectionList.add(numConnections, c);
@@ -742,7 +744,7 @@ public abstract class HBaseServer implements RpcServer {
   }
 
   // Sends responses of RPC back to clients.
-  private class Responder extends Thread {
+  protected class Responder extends Thread {
     private final Selector writeSelector;
     private int pending;         // connections waiting to register
 
@@ -816,7 +818,11 @@ public abstract class HBaseServer implements RpcServer {
           }
 
           for(Call call : calls) {
-            doPurge(call, now);
+            try {
+              doPurge(call, now);
+            } catch (IOException e) {
+              LOG.warn("Error in purging old calls " + e);
+            }
           }
         } catch (OutOfMemoryError e) {
           if (errorHandler != null) {
@@ -832,7 +838,7 @@ public abstract class HBaseServer implements RpcServer {
             //
             LOG.warn("Out of Memory in server select", e);
             try { Thread.sleep(60000); } catch (Exception ignored) {}
-      }
+          }
         } catch (Exception e) {
           LOG.warn("Exception in Responder " +
                    StringUtils.stringifyException(e));
@@ -870,7 +876,7 @@ public abstract class HBaseServer implements RpcServer {
     // Remove calls that have been pending in the responseQueue
     // for a long time.
     //
-    private void doPurge(Call call, long now) {
+    private void doPurge(Call call, long now) throws IOException {
       synchronized (call.connection.responseQueue) {
         Iterator<Call> iter = call.connection.responseQueue.listIterator(0);
         while (iter.hasNext()) {
@@ -1019,7 +1025,7 @@ public abstract class HBaseServer implements RpcServer {
   }
 
   /** Reads calls from a connection and queues them for handling. */
-  private class Connection {
+  protected class Connection {
     private boolean versionRead = false; //if initial signature and
                                          //version are read
     private boolean headerRead = false;  //if the connection header that
@@ -1034,11 +1040,11 @@ public abstract class HBaseServer implements RpcServer {
     protected Socket socket;
     // Cache the remote host & port info so that even if the socket is
     // disconnected, we can say where it used to connect to.
-    private String hostAddress;
-    private int remotePort;
+    protected String hostAddress;
+    protected int remotePort;
     ConnectionHeader header = new ConnectionHeader();
     Class<? extends VersionedProtocol> protocol;
-    protected UserGroupInformation ticket = null;
+    protected User ticket = null;
 
     public Connection(SocketChannel channel, long lastContact) {
       this.channel = channel;
@@ -1096,7 +1102,7 @@ public abstract class HBaseServer implements RpcServer {
     }
 
     /* Increment the outstanding RPC count */
-    private void incRpcCount() {
+    protected void incRpcCount() {
       rpcCount++;
     }
 
@@ -1158,7 +1164,7 @@ public abstract class HBaseServer implements RpcServer {
           dataLengthBuffer.clear();
           data.flip();
           if (headerRead) {
-            processData();
+            processData(data.array());
             data = null;
             return count;
           }
@@ -1214,17 +1220,16 @@ public abstract class HBaseServer implements RpcServer {
         throw new IOException("Unknown protocol: " + header.getProtocol());
       }
 
-      ticket = header.getUgi();
+      ticket = header.getUser();
     }
 
-    private void processData() throws  IOException, InterruptedException {
-      byte[] array = data.array();
+    protected void processData(byte[] buf) throws  IOException, InterruptedException {
       DataInputStream dis =
-        new DataInputStream(new ByteArrayInputStream(array));
+        new DataInputStream(new ByteArrayInputStream(buf));
       int id = dis.readInt();                    // try to read an id
 
       if (LOG.isDebugEnabled())
-        LOG.debug(" got call #" + id + ", " + array.length + " bytes");
+        LOG.debug(" got call #" + id + ", " + buf.length + " bytes");
 
       Writable param;
       try {
@@ -1307,6 +1312,15 @@ public abstract class HBaseServer implements RpcServer {
           try {
             if (!started)
               throw new ServerNotRunningYetException("Server is not running yet");
+
+            if (LOG.isDebugEnabled()) {
+              User remoteUser = call.connection.ticket;
+              LOG.debug(getName() + ": call #" + call.id + " executing as "
+                  + (remoteUser == null ? "NULL principal" : remoteUser.getName()));
+            }
+
+            RequestContext.set(call.connection.ticket, getRemoteIp(),
+                call.connection.protocol);
             // make the call
             value = call(call.connection.protocol, call.param, call.timestamp, 
                 status);
@@ -1314,6 +1328,10 @@ public abstract class HBaseServer implements RpcServer {
             LOG.debug(getName()+", call "+call+": error: " + e, e);
             errorClass = e.getClass().getName();
             error = StringUtils.stringifyException(e);
+          } finally {
+            // Must always clear the request context to avoid leaking
+            // credentials between requests.
+            RequestContext.clear();
           }
           CurCall.set(null);
 
@@ -1414,7 +1432,7 @@ public abstract class HBaseServer implements RpcServer {
     listener = new Listener();
     this.port = listener.getAddress().getPort();
     this.rpcMetrics = new HBaseRpcMetrics(serverName,
-                          Integer.toString(this.port));
+                          Integer.toString(this.port), this);
     this.tcpNoDelay = conf.getBoolean("ipc.server.tcpnodelay", false);
     this.tcpKeepAlive = conf.getBoolean("ipc.server.tcpkeepalive", true);
 
@@ -1428,6 +1446,14 @@ public abstract class HBaseServer implements RpcServer {
   }
 
   /**
+   * Subclasses of HBaseServer can override this to provide their own
+   * Connection implementations.
+   */
+  protected Connection getConnection(SocketChannel channel, long time) {
+    return new Connection(channel, time);
+  }
+
+  /**
    * Setup response for the IPC Call.
    *
    * @param response buffer to serialize the response into
@@ -1606,7 +1632,7 @@ public abstract class HBaseServer implements RpcServer {
   private static int NIO_BUFFER_LIMIT = 8*1024; //should not be more than 64KB.
 
   /**
-   * This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
+   * This is a wrapper around {@link java.nio.channels.WritableByteChannel#write(java.nio.ByteBuffer)}.
    * If the amount of data is large, it writes to channel in smaller chunks.
    * This is to avoid jdk from creating many direct buffers as the size of
    * buffer increases. This also minimizes extra copies in NIO layer
@@ -1617,16 +1643,21 @@ public abstract class HBaseServer implements RpcServer {
    * @param buffer buffer to write
    * @return number of bytes written
    * @throws java.io.IOException e
-   * @see WritableByteChannel#write(ByteBuffer)
+   * @see java.nio.channels.WritableByteChannel#write(java.nio.ByteBuffer)
    */
-  protected static int channelWrite(WritableByteChannel channel,
+  protected int channelWrite(WritableByteChannel channel,
                                     ByteBuffer buffer) throws IOException {
-    return (buffer.remaining() <= NIO_BUFFER_LIMIT) ?
+
+    int count =  (buffer.remaining() <= NIO_BUFFER_LIMIT) ?
            channel.write(buffer) : channelIO(null, channel, buffer);
+    if (count > 0) {
+      rpcMetrics.sentBytes.inc(count);
+    }
+    return count;
   }
 
   /**
-   * This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
+   * This is a wrapper around {@link java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)}.
    * If the amount of data is large, it writes to channel in smaller chunks.
    * This is to avoid jdk from creating many direct buffers as the size of
    * ByteBuffer increases. There should not be any performance degredation.
@@ -1635,17 +1666,22 @@ public abstract class HBaseServer implements RpcServer {
    * @param buffer buffer to write
    * @return number of bytes written
    * @throws java.io.IOException e
-   * @see ReadableByteChannel#read(ByteBuffer)
+   * @see java.nio.channels.ReadableByteChannel#read(java.nio.ByteBuffer)
    */
-  protected static int channelRead(ReadableByteChannel channel,
+  protected int channelRead(ReadableByteChannel channel,
                                    ByteBuffer buffer) throws IOException {
-    return (buffer.remaining() <= NIO_BUFFER_LIMIT) ?
+
+    int count = (buffer.remaining() <= NIO_BUFFER_LIMIT) ?
            channel.read(buffer) : channelIO(channel, null, buffer);
+    if (count > 0) {
+      rpcMetrics.receivedBytes.inc(count);
+  }
+    return count;
   }
 
   /**
-   * Helper for {@link #channelRead(ReadableByteChannel, ByteBuffer)}
-   * and {@link #channelWrite(WritableByteChannel, ByteBuffer)}. Only
+   * Helper for {@link #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)}
+   * and {@link #channelWrite(java.nio.channels.WritableByteChannel, java.nio.ByteBuffer)}. Only
    * one of readCh or writeCh should be non-null.
    *
    * @param readCh read channel
@@ -1653,8 +1689,8 @@ public abstract class HBaseServer implements RpcServer {
    * @param buf buffer to read or write into/out of
    * @return bytes written
    * @throws java.io.IOException e
-   * @see #channelRead(ReadableByteChannel, ByteBuffer)
-   * @see #channelWrite(WritableByteChannel, ByteBuffer)
+   * @see #channelRead(java.nio.channels.ReadableByteChannel, java.nio.ByteBuffer)
+   * @see #channelWrite(java.nio.channels.WritableByteChannel, java.nio.ByteBuffer)
    */
   private static int channelIO(ReadableByteChannel readCh,
                                WritableByteChannel writeCh,
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
index 9e0535e..ce4b53d 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
@@ -26,6 +26,8 @@ import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.UnknownRegionException;
+import org.apache.hadoop.hbase.security.TokenInfo;
+import org.apache.hadoop.hbase.security.KerberosInfo;
 import org.apache.hadoop.hbase.util.Pair;
 
 /**
@@ -37,6 +39,9 @@ import org.apache.hadoop.hbase.util.Pair;
  * number in HBaseRPCProtocolVersion
  *
  */
+@KerberosInfo(
+    serverPrincipal = "hbase.master.kerberos.principal")
+@TokenInfo("HBASE_AUTH_TOKEN")
 public interface HMasterInterface extends VersionedProtocol {
   /**
    * This Interfaces' version. Version changes when the Interface changes.
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java
index 0dc5bea..dfb9133 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HMasterRegionInterface.java
@@ -23,6 +23,7 @@ import java.io.IOException;
 
 import org.apache.hadoop.hbase.HServerLoad;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.security.KerberosInfo;
 import org.apache.hadoop.io.MapWritable;
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
 
@@ -30,6 +31,9 @@ import org.apache.hadoop.hbase.ipc.VersionedProtocol;
  * The Master publishes this Interface for RegionServers to register themselves
  * on.
  */
+@KerberosInfo(
+    serverPrincipal = "hbase.master.kerberos.principal",
+    clientPrincipal = "hbase.regionserver.kerberos.principal")
 public interface HMasterRegionInterface extends VersionedProtocol {
   /**
    * This Interfaces' version. Version changes when the Interface changes.
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
index e8b2c9e..3b3c101 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
@@ -45,6 +45,8 @@ import org.apache.hadoop.hbase.io.hfile.BlockCacheColumnFamilySummary;
 import org.apache.hadoop.hbase.regionserver.RegionOpeningState;
 import org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.security.TokenInfo;
+import org.apache.hadoop.hbase.security.KerberosInfo;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
@@ -55,6 +57,9 @@ import org.apache.hadoop.hbase.ipc.VersionedProtocol;
  * <p>NOTE: if you change the interface, you must change the RPC version
  * number in HBaseRPCProtocolVersion
  */
+@KerberosInfo(
+    serverPrincipal = "hbase.regionserver.kerberos.principal")
+@TokenInfo("HBASE_AUTH_TOKEN")
 public interface HRegionInterface extends VersionedProtocol, Stoppable, Abortable {
   /**
    * This Interfaces' version. Version changes when the Interface changes.
diff --git src/main/java/org/apache/hadoop/hbase/ipc/RequestContext.java src/main/java/org/apache/hadoop/hbase/ipc/RequestContext.java
new file mode 100644
index 0000000..0e185a2
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/ipc/RequestContext.java
@@ -0,0 +1,138 @@
+/*
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.ipc;
+
+import org.apache.hadoop.hbase.security.User;
+
+import java.net.InetAddress;
+
+/**
+ * Represents client information (authenticated username, remote address, protocol)
+ * for the currently executing request within a RPC server handler thread.  If
+ * called outside the context of a RPC request, all values will be
+ * <code>null</code>.
+ */
+public class RequestContext {
+  private static ThreadLocal<RequestContext> instance =
+      new ThreadLocal<RequestContext>() {
+        protected RequestContext initialValue() {
+          return new RequestContext(null, null, null);
+        }
+      };
+
+  public static RequestContext get() {
+    return instance.get();
+  }
+
+
+  /**
+   * Returns the user credentials associated with the current RPC request or
+   * <code>null</code> if no credentials were provided.
+   * @return
+   */
+  public static User getRequestUser() {
+    RequestContext ctx = instance.get();
+    if (ctx != null) {
+      return ctx.getUser();
+    }
+    return null;
+  }
+
+  /**
+   * Returns the username for any user associated with the current RPC
+   * request or <code>null</code> if no user is set.
+   */
+  public static String getRequestUserName() {
+    User user = getRequestUser();
+    if (user != null) {
+      return user.getShortName();
+    }
+    return null;
+  }
+
+  /**
+   * Indicates whether or not the current thread is within scope of executing
+   * an RPC request.
+   */
+  public static boolean isInRequestContext() {
+    RequestContext ctx = instance.get();
+    if (ctx != null) {
+      return ctx.isInRequest();
+    }
+    return false;
+  }
+
+  /**
+   * Initializes the client credentials for the current request.
+   * @param user
+   * @param remoteAddress
+   * @param protocol
+   */
+  public static void set(User user,
+      InetAddress remoteAddress,
+      Class<? extends VersionedProtocol> protocol) {
+    RequestContext ctx = instance.get();
+    ctx.user = user;
+    ctx.remoteAddress = remoteAddress;
+    ctx.protocol = protocol;
+    ctx.inRequest = true;
+  }
+
+  /**
+   * Clears out the client credentials for a given request.
+   */
+  public static void clear() {
+    RequestContext ctx = instance.get();
+    ctx.user = null;
+    ctx.remoteAddress = null;
+    ctx.protocol = null;
+    ctx.inRequest = false;
+  }
+
+  private User user;
+  private InetAddress remoteAddress;
+  private Class<? extends VersionedProtocol> protocol;
+  // indicates we're within a RPC request invocation
+  private boolean inRequest;
+
+  private RequestContext(User user, InetAddress remoteAddr,
+      Class<? extends VersionedProtocol> protocol) {
+    this.user = user;
+    this.remoteAddress = remoteAddr;
+    this.protocol = protocol;
+  }
+
+  public User getUser() {
+    return user;
+  }
+
+  public InetAddress getRemoteAddress() {
+    return remoteAddress;
+  }
+
+  public Class<? extends VersionedProtocol> getProtocol() {
+    return protocol;
+  }
+
+  public boolean isInRequest() {
+    return inRequest;
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/ipc/RpcEngine.java src/main/java/org/apache/hadoop/hbase/ipc/RpcEngine.java
index 1b5629a..d48aeae 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/RpcEngine.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/RpcEngine.java
@@ -25,7 +25,7 @@ import java.net.InetSocketAddress;
 import javax.net.SocketFactory;
 
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
-import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.conf.Configuration;
 
 /** An RPC implementation. */
@@ -34,7 +34,7 @@ interface RpcEngine {
   /** Construct a client-side proxy object. */
   VersionedProtocol getProxy(Class<? extends VersionedProtocol> protocol,
                   long clientVersion, InetSocketAddress addr,
-                  UserGroupInformation ticket, Configuration conf,
+                  User ticket, Configuration conf,
                   SocketFactory factory, int rpcTimeout) throws IOException;
 
   /** Stop this proxy. */
@@ -43,7 +43,7 @@ interface RpcEngine {
   /** Expert: Make multiple, parallel calls to a set of servers. */
   Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs,
                 Class<? extends VersionedProtocol> protocol,
-                UserGroupInformation ticket, Configuration conf)
+                User ticket, Configuration conf)
     throws IOException, InterruptedException;
 
   /** Construct a server for a protocol implementation instance. */
diff --git src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java
index c8e030c..ccc2809 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java
@@ -45,7 +45,7 @@ import org.apache.hadoop.hbase.util.Objects;
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.ipc.RPC;
 import org.apache.hadoop.hbase.ipc.VersionedProtocol;
-import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;
 import org.apache.hadoop.conf.*;
 
@@ -124,13 +124,13 @@ class WritableRpcEngine implements RpcEngine {
   private static class Invoker implements InvocationHandler {
     private Class<? extends VersionedProtocol> protocol;
     private InetSocketAddress address;
-    private UserGroupInformation ticket;
+    private User ticket;
     private HBaseClient client;
     private boolean isClosed = false;
     final private int rpcTimeout;
 
     public Invoker(Class<? extends VersionedProtocol> protocol,
-                   InetSocketAddress address, UserGroupInformation ticket,
+                   InetSocketAddress address, User ticket,
                    Configuration conf, SocketFactory factory, int rpcTimeout) {
       this.protocol = protocol;
       this.address = address;
@@ -171,7 +171,7 @@ class WritableRpcEngine implements RpcEngine {
    * talking to a server at the named address. */
   public VersionedProtocol getProxy(
       Class<? extends VersionedProtocol> protocol, long clientVersion,
-      InetSocketAddress addr, UserGroupInformation ticket,
+      InetSocketAddress addr, User ticket,
       Configuration conf, SocketFactory factory, int rpcTimeout)
     throws IOException {
 
@@ -205,7 +205,7 @@ class WritableRpcEngine implements RpcEngine {
   public Object[] call(Method method, Object[][] params,
                        InetSocketAddress[] addrs,
                        Class<? extends VersionedProtocol> protocol,
-                       UserGroupInformation ticket, Configuration conf)
+                       User ticket, Configuration conf)
     throws IOException, InterruptedException {
 
     Invocation[] invocations = new Invocation[params.length];
diff --git src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
index db07ed1..fb80cbb 100644
--- src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
+++ src/main/java/org/apache/hadoop/hbase/mapred/TableMapReduceUtil.java
@@ -25,6 +25,7 @@ import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapred.FileInputFormat;
@@ -90,6 +91,12 @@ public class TableMapReduceUtil {
         e.printStackTrace();
       }
     }
+    try {
+      initCredentials(job);
+    } catch (IOException ioe) {
+      // just spit out the stack trace?  really?
+      ioe.printStackTrace();
+    }
   }
 
   /**
@@ -158,6 +165,18 @@ public class TableMapReduceUtil {
     if (addDependencyJars) {
       addDependencyJars(job);
     }
+    initCredentials(job);
+  }
+
+  public static void initCredentials(JobConf job) throws IOException {
+    if (User.isHBaseSecurityEnabled(job)) {
+      try {
+        User.getCurrent().obtainAuthTokenForJob(job);
+      } catch (InterruptedException ie) {
+        ie.printStackTrace();
+        Thread.interrupted();
+      }
+    }
   }
 
   /**
diff --git src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
index ad88b76..ff38731 100644
--- src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
+++ src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java
@@ -40,9 +40,11 @@ import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.util.Base64;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.mapreduce.InputFormat;
@@ -133,6 +135,7 @@ public class TableMapReduceUtil {
     if (addDependencyJars) {
       addDependencyJars(job);
     }
+    initCredentials(job);
   }
   
   /**
@@ -211,6 +214,17 @@ public class TableMapReduceUtil {
               outputValueClass, job, addDependencyJars, TableInputFormat.class);
   }
 
+  public static void initCredentials(Job job) throws IOException {
+    if (User.isHBaseSecurityEnabled(job.getConfiguration())) {
+      try {
+        User.getCurrent().obtainAuthTokenForJob(job.getConfiguration(), job);
+      } catch (InterruptedException ie) {
+        LOG.info("Interrupted obtaining user authentication token");
+        Thread.interrupted();
+      }
+    }
+  }
+
   /**
    * Writes the given scan into a Base64 encoded string.
    *
@@ -364,6 +378,8 @@ public class TableMapReduceUtil {
     if (addDependencyJars) {
       addDependencyJars(job);
     }
+
+    initCredentials(job);
   }
 
   /**
diff --git src/main/java/org/apache/hadoop/hbase/master/HMaster.java src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 19ed5b8..73a6343 100644
--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -244,14 +244,15 @@ implements HMasterInterface, HMasterRegionInterface, MasterServices, Server {
     setName(MASTER + "-" + this.serverName.toString());
 
     Replication.decorateMasterConfiguration(this.conf);
-    this.rpcServer.startThreads();
 
     // Hack! Maps DFSClient => Master for logs.  HDFS made this
     // config param for task trackers, but we can piggyback off of it.
     if (this.conf.get("mapred.task.id") == null) {
       this.conf.set("mapred.task.id", "hb_m_" + this.serverName.toString());
     }
+
     this.zooKeeper = new ZooKeeperWatcher(conf, MASTER + ":" + isa.getPort(), this, true);
+    this.rpcServer.startThreads();
     this.metrics = new MasterMetrics(getServerName().toString());
   }
 
diff --git src/main/java/org/apache/hadoop/hbase/security/KerberosInfo.java src/main/java/org/apache/hadoop/hbase/security/KerberosInfo.java
new file mode 100644
index 0000000..da21145
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/security/KerberosInfo.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+/**
+ * Indicates Kerberos related information to be used for authorizing connections
+ * over a given RPC protocol interface.
+ */
+@Retention(RetentionPolicy.RUNTIME)
+@Target(ElementType.TYPE)
+public @interface KerberosInfo {
+  /** Key for getting server's Kerberos principal name from Configuration */
+  String serverPrincipal();
+  String clientPrincipal() default "";
+}
diff --git src/main/java/org/apache/hadoop/hbase/security/TokenInfo.java src/main/java/org/apache/hadoop/hbase/security/TokenInfo.java
new file mode 100644
index 0000000..22a9a5f
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/security/TokenInfo.java
@@ -0,0 +1,35 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security;
+
+import java.lang.annotation.ElementType;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.lang.annotation.Target;
+
+/**
+ * Indicates Token related information to be used in authorizing connections
+ * over a given RPC protocol interface.
+ */
+@Retention(RetentionPolicy.RUNTIME)
+@Target(ElementType.TYPE)
+public @interface TokenInfo {
+  /** The type of Token.getKind() to be handled */
+  String value();
+}
\ No newline at end of file
diff --git src/main/java/org/apache/hadoop/hbase/security/User.java src/main/java/org/apache/hadoop/hbase/security/User.java
index 00bd06d..b0d1ab0 100644
--- src/main/java/org/apache/hadoop/hbase/security/User.java
+++ src/main/java/org/apache/hadoop/hbase/security/User.java
@@ -33,6 +33,7 @@ import java.lang.reflect.Constructor;
 import java.lang.reflect.UndeclaredThrowableException;
 import java.security.PrivilegedAction;
 import java.security.PrivilegedExceptionAction;
+
 import org.apache.commons.logging.Log;
 
 /**
@@ -47,6 +48,9 @@ import org.apache.commons.logging.Log;
  * </p>
  */
 public abstract class User {
+  public static final String HBASE_SECURITY_CONF_KEY =
+      "hbase.security.authentication";
+
   /**
    * Flag to differentiate between API-incompatible changes to
    * {@link org.apache.hadoop.security.UserGroupInformation} between vanilla
@@ -61,8 +65,13 @@ public abstract class User {
     }
   }
   private static Log LOG = LogFactory.getLog(User.class);
+
   protected UserGroupInformation ugi;
 
+  public UserGroupInformation getUGI() {
+    return ugi;
+  }
+
   /**
    * Returns the full user name.  For Kerberos principals this will include
    * the host and realm portions of the principal name.
@@ -73,6 +82,15 @@ public abstract class User {
   }
 
   /**
+   * Returns the list of groups of which this user is a member.  On secure
+   * Hadoop this returns the group information for the user as resolved on the
+   * server.  For 0.20 based Hadoop, the group names are passed from the client.
+   */
+  public String[] getGroupNames() {
+    return ugi.getGroupNames();
+  }
+
+  /**
    * Returns the shortened version of the user name -- the portion that maps
    * to an operating system user name.
    * @return Short name
@@ -90,6 +108,24 @@ public abstract class User {
   public abstract <T> T runAs(PrivilegedExceptionAction<T> action)
       throws IOException, InterruptedException;
 
+  /**
+   * Requests an authentication token for this user and stores it in the
+   * user's credentials.
+   *
+   * @throws IOException
+   */
+  public abstract void obtainAuthTokenForJob(Configuration conf, Job job)
+      throws IOException, InterruptedException;
+
+  /**
+   * Requests an authentication token for this user and stores it in the
+   * user's credentials.
+   *
+   * @throws IOException
+   */
+  public abstract void obtainAuthTokenForJob(JobConf job)
+      throws IOException, InterruptedException;
+
   public String toString() {
     return ugi.toString();
   }
@@ -104,13 +140,29 @@ public abstract class User {
     } else {
       user = new HadoopUser();
     }
-    if (user.ugi == null) {
+    if (user.getUGI() == null) {
       return null;
     }
     return user;
   }
 
   /**
+   * Wraps an underlying {@code UserGroupInformation} instance.
+   * @param ugi The base Hadoop user
+   * @return
+   */
+  public static User create(UserGroupInformation ugi) {
+    if (ugi == null) {
+      return null;
+    }
+
+    if (IS_SECURE_HADOOP) {
+      return new SecureHadoopUser(ugi);
+    }
+    return new HadoopUser(ugi);
+  }
+
+  /**
    * Generates a new {@code User} instance specifically for use in test code.
    * @param name the full username
    * @param groups the group names to which the test user will belong
@@ -150,8 +202,8 @@ public abstract class User {
   }
 
   /**
-   * Returns whether or not Kerberos authentication is configured.  For
-   * non-secure Hadoop, this always returns <code>false</code>.
+   * Returns whether or not Kerberos authentication is configured for Hadoop.
+   * For non-secure Hadoop, this always returns <code>false</code>.
    * For secure Hadoop, it will return the value from
    * {@code UserGroupInformation.isSecurityEnabled()}.
    */
@@ -163,6 +215,15 @@ public abstract class User {
     }
   }
 
+  /**
+   * Returns whether or not secure authentication is enabled for HBase
+   * (whether <code>hbase.security.authentication</code> is set to
+   * <code>kerberos</code>.
+   */
+  public static boolean isHBaseSecurityEnabled(Configuration conf) {
+    return "kerberos".equalsIgnoreCase(conf.get(HBASE_SECURITY_CONF_KEY));
+  }
+
   /* Concrete implementations */
 
   /**
@@ -201,7 +262,7 @@ public abstract class User {
 
     @Override
     public String getShortName() {
-      return ugi.getUserName();
+      return ugi != null ? ugi.getUserName() : null;
     }
 
     @Override
@@ -260,6 +321,20 @@ public abstract class User {
       return result;
     }
 
+    @Override
+    public void obtainAuthTokenForJob(Configuration conf, Job job)
+        throws IOException, InterruptedException {
+      // this is a no-op.  token creation is only supported for kerberos
+      // authenticated clients
+    }
+
+    @Override
+    public void obtainAuthTokenForJob(JobConf job)
+        throws IOException, InterruptedException {
+      // this is a no-op.  token creation is only supported for kerberos
+      // authenticated clients
+    }
+
     /** @see User#createUserForTesting(org.apache.hadoop.conf.Configuration, String, String[]) */
     public static User createUserForTesting(Configuration conf,
         String name, String[] groups) {
@@ -311,6 +386,8 @@ public abstract class User {
    * 0.20 and versions 0.21 and above.
    */
   private static class SecureHadoopUser extends User {
+    private String shortName;
+
     private SecureHadoopUser() throws IOException {
       try {
         ugi = (UserGroupInformation) callStatic("getCurrentUser");
@@ -330,8 +407,11 @@ public abstract class User {
 
     @Override
     public String getShortName() {
+      if (shortName != null) return shortName;
+
       try {
-        return (String)call(ugi, "getShortUserName", null, null);
+        shortName = (String)call(ugi, "getShortUserName", null, null);
+        return shortName;
       } catch (RuntimeException re) {
         throw re;
       } catch (Exception e) {
@@ -372,6 +452,55 @@ public abstract class User {
       }
     }
 
+    @Override
+    public void obtainAuthTokenForJob(Configuration conf, Job job)
+        throws IOException, InterruptedException {
+      try {
+        Class c = Class.forName(
+            "org.apache.hadoop.hbase.security.token.TokenUtil");
+        Methods.call(c, null, "obtainTokenForJob",
+            new Class[]{Configuration.class, UserGroupInformation.class,
+                Job.class},
+            new Object[]{conf, ugi, job});
+      } catch (ClassNotFoundException cnfe) {
+        throw new RuntimeException("Failure loading TokenUtil class, "
+            +"is secure RPC available?", cnfe);
+      } catch (IOException ioe) {
+        throw ioe;
+      } catch (InterruptedException ie) {
+        throw ie;
+      } catch (RuntimeException re) {
+        throw re;
+      } catch (Exception e) {
+        throw new UndeclaredThrowableException(e,
+            "Unexpected error calling TokenUtil.obtainAndCacheToken()");
+      }
+    }
+
+    @Override
+    public void obtainAuthTokenForJob(JobConf job)
+        throws IOException, InterruptedException {
+      try {
+        Class c = Class.forName(
+            "org.apache.hadoop.hbase.security.token.TokenUtil");
+        Methods.call(c, null, "obtainTokenForJob",
+            new Class[]{JobConf.class, UserGroupInformation.class},
+            new Object[]{job, ugi});
+      } catch (ClassNotFoundException cnfe) {
+        throw new RuntimeException("Failure loading TokenUtil class, "
+            +"is secure RPC available?", cnfe);
+      } catch (IOException ioe) {
+        throw ioe;
+      } catch (InterruptedException ie) {
+        throw ie;
+      } catch (RuntimeException re) {
+        throw re;
+      } catch (Exception e) {
+        throw new UndeclaredThrowableException(e,
+            "Unexpected error calling TokenUtil.obtainAndCacheToken()");
+      }
+    }
+
     /** @see User#createUserForTesting(org.apache.hadoop.conf.Configuration, String, String[]) */
     public static User createUserForTesting(Configuration conf,
         String name, String[] groups) {
diff --git src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java
new file mode 100644
index 0000000..c12c7ee
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/zookeeper/ZKLeaderManager.java
@@ -0,0 +1,175 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.zookeeper;
+
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * Handles coordination of a single "leader" instance among many possible
+ * candidates.  The first {@code ZKLeaderManager} to successfully create
+ * the given znode becomes the leader, allowing the instance to continue
+ * with whatever processing must be protected.  Other {@ZKLeaderManager}
+ * instances will wait to be notified of changes to the leader znode.
+ * If the current master instance fails, the ephemeral leader znode will
+ * be removed, and all waiting instances will be notified, with the race
+ * to claim the leader znode beginning all over again.
+ */
+public class ZKLeaderManager extends ZooKeeperListener {
+  private static Log LOG = LogFactory.getLog(ZKLeaderManager.class);
+
+  private final AtomicBoolean leaderExists = new AtomicBoolean();
+  private String leaderZNode;
+  private byte[] nodeId;
+  private Stoppable candidate;
+
+  public ZKLeaderManager(ZooKeeperWatcher watcher, String leaderZNode,
+      byte[] identifier, Stoppable candidate) {
+    super(watcher);
+    this.leaderZNode = leaderZNode;
+    this.nodeId = identifier;
+    this.candidate = candidate;
+  }
+
+  public void start() {
+    try {
+      watcher.registerListener(this);
+      String parent = ZKUtil.getParent(leaderZNode);
+      if (ZKUtil.checkExists(watcher, parent) < 0) {
+        ZKUtil.createWithParents(watcher, parent);
+      }
+    } catch (KeeperException ke) {
+      watcher.abort("Unhandled zk exception when starting", ke);
+      candidate.stop("Unhandled zk exception starting up: "+ke.getMessage());
+    }
+  }
+
+  @Override
+  public void nodeCreated(String path) {
+    if (leaderZNode.equals(path) && !candidate.isStopped()) {
+      handleLeaderChange();
+    }
+  }
+
+  @Override
+  public void nodeDeleted(String path) {
+    if (leaderZNode.equals(path) && !candidate.isStopped()) {
+      handleLeaderChange();
+    }
+  }
+
+  private void handleLeaderChange() {
+    try {
+      synchronized(leaderExists) {
+        if (ZKUtil.watchAndCheckExists(watcher, leaderZNode)) {
+          LOG.info("Found new leader for znode: "+leaderZNode);
+          leaderExists.set(true);
+        } else {
+          LOG.info("Leader change, but no new leader found");
+          leaderExists.set(false);
+          leaderExists.notifyAll();
+        }
+      }
+    } catch (KeeperException ke) {
+      watcher.abort("ZooKeeper error checking for leader znode", ke);
+      candidate.stop("ZooKeeper error checking for leader: "+ke.getMessage());
+    }
+  }
+
+  /**
+   * Blocks until this instance has claimed the leader ZNode in ZooKeeper
+   */
+  public void waitToBecomeLeader() {
+    while (!candidate.isStopped()) {
+      try {
+        if (ZKUtil.createEphemeralNodeAndWatch(watcher, leaderZNode, nodeId)) {
+          // claimed the leader znode
+          leaderExists.set(true);
+          if (LOG.isDebugEnabled()) {
+            LOG.debug("Claimed the leader znode as '"+
+                Bytes.toStringBinary(nodeId)+"'");
+          }
+          return;
+        }
+
+        // if claiming the node failed, there should be another existing node
+        byte[] currentId = ZKUtil.getDataAndWatch(watcher, leaderZNode);
+        if (currentId != null && Bytes.equals(currentId, nodeId)) {
+          // claimed with our ID, but we didn't grab it, possibly restarted?
+          LOG.info("Found existing leader with our ID ("+
+              Bytes.toStringBinary(nodeId)+"), removing");
+          ZKUtil.deleteNode(watcher, leaderZNode);
+          leaderExists.set(false);
+        } else {
+          LOG.info("Found existing leader with ID: "+Bytes.toStringBinary(nodeId));
+          leaderExists.set(true);
+        }
+      } catch (KeeperException ke) {
+        watcher.abort("Unexpected error from ZK, stopping candidate", ke);
+        candidate.stop("Unexpected error from ZK: "+ke.getMessage());
+        return;
+      }
+
+      // wait for next chance
+      synchronized(leaderExists) {
+        while (leaderExists.get() && !candidate.isStopped()) {
+          try {
+            leaderExists.wait();
+          } catch (InterruptedException ie) {
+            LOG.debug("Interrupted waiting on leader", ie);
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Removes the leader znode, if it is currently claimed by this instance.
+   */
+  public void stepDownAsLeader() {
+    try {
+      synchronized(leaderExists) {
+        if (!leaderExists.get()) {
+          return;
+        }
+        byte[] leaderId = ZKUtil.getData(watcher, leaderZNode);
+        if (leaderId != null && Bytes.equals(nodeId, leaderId)) {
+          LOG.info("Stepping down as leader");
+          ZKUtil.deleteNodeFailSilent(watcher, leaderZNode);
+          leaderExists.set(false);
+        } else {
+          LOG.info("Not current leader, no need to step down");
+        }
+      }
+    } catch (KeeperException ke) {
+      watcher.abort("Unhandled zookeeper exception removing leader node", ke);
+      candidate.stop("Unhandled zookeeper exception removing leader node: "
+          + ke.getMessage());
+    }
+  }
+
+  public boolean hasLeader() {
+    return leaderExists.get();
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
index 4c5e844..d729626 100644
--- src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
+++ src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java
@@ -567,6 +567,35 @@ public class ZKUtil {
   }
 
   /**
+   * Returns the date of child znodes of the specified znode.  Also sets a watch on
+   * the specified znode which will capture a NodeDeleted event on the specified
+   * znode as well as NodeChildrenChanged if any children of the specified znode
+   * are created or deleted.
+   *
+   * Returns null if the specified node does not exist.  Otherwise returns a
+   * list of children of the specified node.  If the node exists but it has no
+   * children, an empty list will be returned.
+   *
+   * @param zkw zk reference
+   * @param znode path of node to list and watch children of
+   * @return list of data of children of the specified node, an empty list if the node
+   *          exists but has no children, and null if the node does not exist
+   * @throws KeeperException if unexpected zookeeper exception
+   */
+  public static List<NodeAndData> getChildDataAndWatchForNewChildren(
+      ZooKeeperWatcher zkw, String baseNode) throws KeeperException {
+    List<String> nodes =
+      ZKUtil.listChildrenAndWatchForNewChildren(zkw, baseNode);
+    List<NodeAndData> newNodes = new ArrayList<NodeAndData>();
+    for (String node: nodes) {
+      String nodePath = ZKUtil.joinZNode(baseNode, node);
+      byte [] data = ZKUtil.getDataAndWatch(zkw, nodePath);
+      newNodes.add(new NodeAndData(nodePath, data));
+    }
+    return newNodes;
+  }
+
+  /**
    * Update the data of an existing node with the expected version to have the
    * specified data.
    *
diff --git src/main/resources/hbase-default.xml src/main/resources/hbase-default.xml
index 6f98f5d..df70f9f 100644
--- src/main/resources/hbase-default.xml
+++ src/main/resources/hbase-default.xml
@@ -558,6 +558,39 @@
     specified in hbase.regionserver.keytab.file
     </description>
   </property>
+
+  <!-- Additional configuration specific to HBase security -->
+  <property>
+    <name>hadoop.policy.file</name>
+    <value>hbase-policy.xml</value>
+    <description>The policy configuration file used by RPC servers to make
+      authorization decisions on client requests.  Only used when HBase
+      security is enabled.
+    </description>
+  </property>
+  <property>
+    <name>hbase.superuser</name>
+    <value></value>
+    <description>List of users or groups (comma-separated), who are allowed
+    full privileges, regardless of stored ACLs, across the cluster.
+    Only used when HBase security is enabled.
+    </description>
+  </property>
+  <property>
+    <name>hbase.auth.key.update.interval</name>
+    <value>86400000</value>
+    <description>The update interval for master key for authentication tokens 
+    in servers in milliseconds.  Only used when HBase security is enabled.
+    </description>
+  </property>
+  <property>
+    <name>hbase.auth.token.max.lifetime</name>
+    <value>604800000</value>
+    <description>The maximum lifetime in milliseconds after which an
+    authentication token expires.  Only used when HBase security is enabled.
+    </description>
+  </property>
+
   <property>
     <name>zookeeper.session.timeout</name>
     <value>180000</value>
diff --git src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
index c0634f4..56b1818 100644
--- src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
+++ src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
@@ -222,7 +222,7 @@ public class MiniHBaseCluster {
       t.start();
       t.waitForServerOnline();
     } catch (InterruptedException ie) {
-      throw new IOException("Interrupted executing UserGroupInformation.doAs()", ie);
+      throw new IOException("Interrupted adding regionserver to cluster", ie);
     }
     return t;
   }
@@ -295,7 +295,7 @@ public class MiniHBaseCluster {
       t.start();
       t.waitForServerOnline();
     } catch (InterruptedException ie) {
-      throw new IOException("Interrupted executing UserGroupInformation.doAs()", ie);
+      throw new IOException("Interrupted adding master to cluster", ie);
     }
     return t;
   }
diff --git src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
index bb9ea45..8b00597 100644
--- src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
+++ src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
@@ -573,6 +573,9 @@ public class PerformanceEvaluation {
     // Add a Class from the hbase.jar so it gets registered too.
     TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
       org.apache.hadoop.hbase.util.Bytes.class);
+
+    TableMapReduceUtil.initCredentials(job);
+
     job.waitForCompletion(true);
   }
 
diff --git src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java
new file mode 100644
index 0000000..d55d27b
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/zookeeper/TestZKLeaderManager.java
@@ -0,0 +1,233 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.zookeeper;
+
+import static org.junit.Assert.*;
+
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Abortable;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.Stoppable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ */
+public class TestZKLeaderManager {
+  private static Log LOG = LogFactory.getLog(TestZKLeaderManager.class);
+
+  private static final String LEADER_ZNODE =
+      "/test/" + TestZKLeaderManager.class.getSimpleName();
+
+  private static class MockAbortable implements Abortable {
+    private boolean aborted;
+
+    @Override
+    public void abort(String why, Throwable e) {
+      aborted = true;
+      LOG.fatal("Aborting during test: "+why, e);
+      fail("Aborted during test: " + why);
+    }
+
+    @Override
+    public boolean isAborted() {
+      return aborted;
+    }
+  }
+
+  private static class MockLeader extends Thread implements Stoppable {
+    private boolean stopped;
+    private ZooKeeperWatcher watcher;
+    private ZKLeaderManager zkLeader;
+    private AtomicBoolean master = new AtomicBoolean(false);
+    private int index;
+
+    public MockLeader(ZooKeeperWatcher watcher, int index) {
+      setDaemon(true);
+      setName("TestZKLeaderManager-leader-" + index);
+      this.index = index;
+      this.watcher = watcher;
+      this.zkLeader = new ZKLeaderManager(watcher, LEADER_ZNODE,
+          Bytes.toBytes(index), this);
+    }
+
+    public boolean isMaster() {
+      return master.get();
+    }
+
+    public int getIndex() {
+      return index;
+    }
+
+    public ZooKeeperWatcher getWatcher() {
+      return watcher;
+    }
+
+    public void run() {
+      while (!stopped) {
+        zkLeader.start();
+        zkLeader.waitToBecomeLeader();
+        master.set(true);
+
+        while (master.get() && !stopped) {
+          try {
+            Thread.sleep(200);
+          } catch (InterruptedException ignored) {}
+        }
+      }
+    }
+
+    public void abdicate() {
+      zkLeader.stepDownAsLeader();
+      master.set(false);
+    }
+
+    @Override
+    public void stop(String why) {
+      stopped = true;
+      abdicate();
+      watcher.close();
+    }
+
+    @Override
+    public boolean isStopped() {
+      return stopped;
+    }
+  }
+
+  private static HBaseTestingUtility TEST_UTIL;
+  private static MockLeader[] CANDIDATES;
+
+  @BeforeClass
+  public static void setupBeforeClass() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.startMiniZKCluster();
+    Configuration conf = TEST_UTIL.getConfiguration();
+
+    // use an abortable to fail the test in the case of any KeeperExceptions
+    MockAbortable abortable = new MockAbortable();
+    CANDIDATES = new MockLeader[3];
+    for (int i = 0; i < 3; i++) {
+      ZooKeeperWatcher watcher = newZK(conf, "server"+i, abortable);
+      CANDIDATES[i] = new MockLeader(watcher, i);
+      CANDIDATES[i].start();
+    }
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniZKCluster();
+  }
+
+  @Test
+  public void testLeaderSelection() throws Exception {
+    MockLeader currentLeader = getCurrentLeader();
+    // one leader should have been found
+    assertNotNull("Leader should exist", currentLeader);
+    LOG.debug("Current leader index is "+currentLeader.getIndex());
+
+    byte[] znodeData = ZKUtil.getData(CANDIDATES[0].getWatcher(), LEADER_ZNODE);
+    assertNotNull("Leader znode should contain leader index", znodeData);
+    assertTrue("Leader znode should not be empty", znodeData.length > 0);
+    int storedIndex = Bytes.toInt(znodeData);
+    LOG.debug("Stored leader index in ZK is "+storedIndex);
+    assertEquals("Leader znode should match leader index",
+        currentLeader.getIndex(), storedIndex);
+
+    // force a leader transition
+    currentLeader.abdicate();
+    assertFalse(currentLeader.isMaster());
+
+    // check for new leader
+    currentLeader = getCurrentLeader();
+    // one leader should have been found
+    assertNotNull("New leader should exist after abdication", currentLeader);
+    LOG.debug("New leader index is "+currentLeader.getIndex());
+
+    znodeData = ZKUtil.getData(CANDIDATES[0].getWatcher(), LEADER_ZNODE);
+    assertNotNull("Leader znode should contain leader index", znodeData);
+    assertTrue("Leader znode should not be empty", znodeData.length > 0);
+    storedIndex = Bytes.toInt(znodeData);
+    LOG.debug("Stored leader index in ZK is "+storedIndex);
+    assertEquals("Leader znode should match leader index",
+        currentLeader.getIndex(), storedIndex);
+
+    // force another transition by stopping the current
+    currentLeader.stop("Stopping for test");
+    assertFalse(currentLeader.isMaster());
+
+    // check for new leader
+    currentLeader = getCurrentLeader();
+    // one leader should have been found
+    assertNotNull("New leader should exist after stop", currentLeader);
+    LOG.debug("New leader index is "+currentLeader.getIndex());
+
+    znodeData = ZKUtil.getData(CANDIDATES[0].getWatcher(), LEADER_ZNODE);
+    assertNotNull("Leader znode should contain leader index", znodeData);
+    assertTrue("Leader znode should not be empty", znodeData.length > 0);
+    storedIndex = Bytes.toInt(znodeData);
+    LOG.debug("Stored leader index in ZK is "+storedIndex);
+    assertEquals("Leader znode should match leader index",
+        currentLeader.getIndex(), storedIndex);
+
+    // with a second stop we can guarantee that a previous leader has resumed leading
+    currentLeader.stop("Stopping for test");
+    assertFalse(currentLeader.isMaster());
+
+    // check for new
+    currentLeader = getCurrentLeader();
+    assertNotNull("New leader should exist", currentLeader);
+  }
+
+  private MockLeader getCurrentLeader() throws Exception {
+    MockLeader currentLeader = null;
+    outer:
+    // wait up to 2 secs for initial leader
+    for (int i = 0; i < 20; i++) {
+      for (int j = 0; j < CANDIDATES.length; j++) {
+        if (CANDIDATES[j].isMaster()) {
+          // should only be one leader
+          if (currentLeader != null) {
+            fail("Both candidate "+currentLeader.getIndex()+" and "+j+" claim to be leader!");
+          }
+          currentLeader = CANDIDATES[j];
+        }
+      }
+      if (currentLeader != null) {
+        break outer;
+      }
+      Thread.sleep(100);
+    }
+    return currentLeader;
+  }
+
+  private static ZooKeeperWatcher newZK(Configuration conf, String name,
+      Abortable abort) throws Exception {
+    Configuration copy = HBaseConfiguration.create(conf);
+    ZooKeeperWatcher zk = new ZooKeeperWatcher(copy, name, abort);
+    return zk;
+  }
+}
