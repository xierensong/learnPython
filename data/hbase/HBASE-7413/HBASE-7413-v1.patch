diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index 66885fc..3ce99e7 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -126,6 +126,7 @@ import org.apache.hadoop.hbase.util.DynamicClassLoader;
 import org.apache.hadoop.hbase.util.Methods;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.security.token.Token;
 
 /**
@@ -1997,4 +1998,23 @@ public final class ProtobufUtil {
       throw new IOException(e);
     }
   }
+
+  /**
+   * C/p from Bytes that uses PB ByteString instead of the byte buffer.
+   * Reads a zero-compressed encoded long and returns it.
+   */
+  public static long readVLong(final ByteString buffer, int offset) throws IOException {
+    byte firstByte = buffer.byteAt(offset);
+    int len = WritableUtils.decodeVIntSize(firstByte);
+    if (len == 1) {
+      return firstByte;
+    }
+    long i = 0;
+    for (int idx = 0; idx < len-1; idx++) {
+      byte b = buffer.byteAt(1 + offset + idx);
+      i = i << 8;
+      i = i | (b & 0xFF);
+    }
+    return (WritableUtils.isNegativeVInt(firstByte) ? ~i : i);
+  }
 }
diff --git hbase-common/src/main/resources/hbase-default.xml hbase-common/src/main/resources/hbase-default.xml
index 78e597c..51ace9d 100644
--- hbase-common/src/main/resources/hbase-default.xml
+++ hbase-common/src/main/resources/hbase-default.xml
@@ -215,12 +215,12 @@
   </property>
   <property>
     <name>hbase.regionserver.hlog.reader.impl</name>
-    <value>org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader</value>
+    <value>org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader</value>
     <description>The HLog file reader implementation.</description>
   </property>
   <property>
     <name>hbase.regionserver.hlog.writer.impl</name>
-    <value>org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter</value>
+    <value>org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter</value>
     <description>The HLog file writer implementation.</description>
   </property>
   <property>
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
index a23c498..2b71a87 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
@@ -9591,1849 +9591,81 @@ public final class AdminProtos {
     // @@protoc_insertion_point(class_scope:MergeRegionsResponse)
   }
   
-  public interface UUIDOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required uint64 leastSigBits = 1;
-    boolean hasLeastSigBits();
-    long getLeastSigBits();
-    
-    // required uint64 mostSigBits = 2;
-    boolean hasMostSigBits();
-    long getMostSigBits();
-  }
-  public static final class UUID extends
-      com.google.protobuf.GeneratedMessage
-      implements UUIDOrBuilder {
-    // Use UUID.newBuilder() to construct.
-    private UUID(Builder builder) {
-      super(builder);
-    }
-    private UUID(boolean noInit) {}
-    
-    private static final UUID defaultInstance;
-    public static UUID getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public UUID getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_UUID_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_UUID_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required uint64 leastSigBits = 1;
-    public static final int LEASTSIGBITS_FIELD_NUMBER = 1;
-    private long leastSigBits_;
-    public boolean hasLeastSigBits() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public long getLeastSigBits() {
-      return leastSigBits_;
-    }
-    
-    // required uint64 mostSigBits = 2;
-    public static final int MOSTSIGBITS_FIELD_NUMBER = 2;
-    private long mostSigBits_;
-    public boolean hasMostSigBits() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public long getMostSigBits() {
-      return mostSigBits_;
-    }
-    
-    private void initFields() {
-      leastSigBits_ = 0L;
-      mostSigBits_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasLeastSigBits()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!hasMostSigBits()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, leastSigBits_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(2, mostSigBits_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, leastSigBits_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(2, mostSigBits_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID other = (org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID) obj;
-      
-      boolean result = true;
-      result = result && (hasLeastSigBits() == other.hasLeastSigBits());
-      if (hasLeastSigBits()) {
-        result = result && (getLeastSigBits()
-            == other.getLeastSigBits());
-      }
-      result = result && (hasMostSigBits() == other.hasMostSigBits());
-      if (hasMostSigBits()) {
-        result = result && (getMostSigBits()
-            == other.getMostSigBits());
-      }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasLeastSigBits()) {
-        hash = (37 * hash) + LEASTSIGBITS_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLeastSigBits());
-      }
-      if (hasMostSigBits()) {
-        hash = (37 * hash) + MOSTSIGBITS_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getMostSigBits());
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_UUID_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_UUID_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        leastSigBits_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        mostSigBits_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID build() {
-        org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID result = new org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.leastSigBits_ = leastSigBits_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.mostSigBits_ = mostSigBits_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance()) return this;
-        if (other.hasLeastSigBits()) {
-          setLeastSigBits(other.getLeastSigBits());
-        }
-        if (other.hasMostSigBits()) {
-          setMostSigBits(other.getMostSigBits());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasLeastSigBits()) {
-          
-          return false;
-        }
-        if (!hasMostSigBits()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              leastSigBits_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              mostSigBits_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required uint64 leastSigBits = 1;
-      private long leastSigBits_ ;
-      public boolean hasLeastSigBits() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public long getLeastSigBits() {
-        return leastSigBits_;
-      }
-      public Builder setLeastSigBits(long value) {
-        bitField0_ |= 0x00000001;
-        leastSigBits_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLeastSigBits() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        leastSigBits_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // required uint64 mostSigBits = 2;
-      private long mostSigBits_ ;
-      public boolean hasMostSigBits() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public long getMostSigBits() {
-        return mostSigBits_;
-      }
-      public Builder setMostSigBits(long value) {
-        bitField0_ |= 0x00000002;
-        mostSigBits_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearMostSigBits() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        mostSigBits_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:UUID)
-    }
-    
-    static {
-      defaultInstance = new UUID(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:UUID)
-  }
-  
-  public interface WALEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required .WALEntry.WALKey key = 1;
-    boolean hasKey();
-    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey getKey();
-    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder getKeyOrBuilder();
-    
-    // required .WALEntry.WALEdit edit = 2;
-    boolean hasEdit();
-    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit getEdit();
-    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEditOrBuilder getEditOrBuilder();
-  }
-  public static final class WALEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements WALEntryOrBuilder {
-    // Use WALEntry.newBuilder() to construct.
-    private WALEntry(Builder builder) {
-      super(builder);
-    }
-    private WALEntry(boolean noInit) {}
-    
-    private static final WALEntry defaultInstance;
-    public static WALEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public WALEntry getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_fieldAccessorTable;
-    }
-    
-    public interface WALKeyOrBuilder
-        extends com.google.protobuf.MessageOrBuilder {
-      
-      // required bytes encodedRegionName = 1;
-      boolean hasEncodedRegionName();
-      com.google.protobuf.ByteString getEncodedRegionName();
-      
-      // required bytes tableName = 2;
-      boolean hasTableName();
-      com.google.protobuf.ByteString getTableName();
-      
-      // required uint64 logSequenceNumber = 3;
-      boolean hasLogSequenceNumber();
-      long getLogSequenceNumber();
-      
-      // required uint64 writeTime = 4;
-      boolean hasWriteTime();
-      long getWriteTime();
-      
-      // optional .UUID clusterId = 5;
-      boolean hasClusterId();
-      org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID getClusterId();
-      org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder getClusterIdOrBuilder();
-    }
-    public static final class WALKey extends
-        com.google.protobuf.GeneratedMessage
-        implements WALKeyOrBuilder {
-      // Use WALKey.newBuilder() to construct.
-      private WALKey(Builder builder) {
-        super(builder);
-      }
-      private WALKey(boolean noInit) {}
-      
-      private static final WALKey defaultInstance;
-      public static WALKey getDefaultInstance() {
-        return defaultInstance;
-      }
-      
-      public WALKey getDefaultInstanceForType() {
-        return defaultInstance;
-      }
-      
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALKey_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALKey_fieldAccessorTable;
-      }
-      
-      private int bitField0_;
-      // required bytes encodedRegionName = 1;
-      public static final int ENCODEDREGIONNAME_FIELD_NUMBER = 1;
-      private com.google.protobuf.ByteString encodedRegionName_;
-      public boolean hasEncodedRegionName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public com.google.protobuf.ByteString getEncodedRegionName() {
-        return encodedRegionName_;
-      }
-      
-      // required bytes tableName = 2;
-      public static final int TABLENAME_FIELD_NUMBER = 2;
-      private com.google.protobuf.ByteString tableName_;
-      public boolean hasTableName() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public com.google.protobuf.ByteString getTableName() {
-        return tableName_;
-      }
-      
-      // required uint64 logSequenceNumber = 3;
-      public static final int LOGSEQUENCENUMBER_FIELD_NUMBER = 3;
-      private long logSequenceNumber_;
-      public boolean hasLogSequenceNumber() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      public long getLogSequenceNumber() {
-        return logSequenceNumber_;
-      }
-      
-      // required uint64 writeTime = 4;
-      public static final int WRITETIME_FIELD_NUMBER = 4;
-      private long writeTime_;
-      public boolean hasWriteTime() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      public long getWriteTime() {
-        return writeTime_;
-      }
-      
-      // optional .UUID clusterId = 5;
-      public static final int CLUSTERID_FIELD_NUMBER = 5;
-      private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID clusterId_;
-      public boolean hasClusterId() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID getClusterId() {
-        return clusterId_;
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder getClusterIdOrBuilder() {
-        return clusterId_;
-      }
-      
-      private void initFields() {
-        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
-        tableName_ = com.google.protobuf.ByteString.EMPTY;
-        logSequenceNumber_ = 0L;
-        writeTime_ = 0L;
-        clusterId_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance();
-      }
-      private byte memoizedIsInitialized = -1;
-      public final boolean isInitialized() {
-        byte isInitialized = memoizedIsInitialized;
-        if (isInitialized != -1) return isInitialized == 1;
-        
-        if (!hasEncodedRegionName()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        if (!hasTableName()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        if (!hasLogSequenceNumber()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        if (!hasWriteTime()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
-        if (hasClusterId()) {
-          if (!getClusterId().isInitialized()) {
-            memoizedIsInitialized = 0;
-            return false;
-          }
-        }
-        memoizedIsInitialized = 1;
-        return true;
-      }
-      
-      public void writeTo(com.google.protobuf.CodedOutputStream output)
-                          throws java.io.IOException {
-        getSerializedSize();
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          output.writeBytes(1, encodedRegionName_);
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          output.writeBytes(2, tableName_);
-        }
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          output.writeUInt64(3, logSequenceNumber_);
-        }
-        if (((bitField0_ & 0x00000008) == 0x00000008)) {
-          output.writeUInt64(4, writeTime_);
-        }
-        if (((bitField0_ & 0x00000010) == 0x00000010)) {
-          output.writeMessage(5, clusterId_);
-        }
-        getUnknownFields().writeTo(output);
-      }
-      
-      private int memoizedSerializedSize = -1;
-      public int getSerializedSize() {
-        int size = memoizedSerializedSize;
-        if (size != -1) return size;
-      
-        size = 0;
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeBytesSize(1, encodedRegionName_);
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeBytesSize(2, tableName_);
-        }
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeUInt64Size(3, logSequenceNumber_);
-        }
-        if (((bitField0_ & 0x00000008) == 0x00000008)) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeUInt64Size(4, writeTime_);
-        }
-        if (((bitField0_ & 0x00000010) == 0x00000010)) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeMessageSize(5, clusterId_);
-        }
-        size += getUnknownFields().getSerializedSize();
-        memoizedSerializedSize = size;
-        return size;
-      }
-      
-      private static final long serialVersionUID = 0L;
-      @java.lang.Override
-      protected java.lang.Object writeReplace()
-          throws java.io.ObjectStreamException {
-        return super.writeReplace();
-      }
-      
-      @java.lang.Override
-      public boolean equals(final java.lang.Object obj) {
-        if (obj == this) {
-         return true;
-        }
-        if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey)) {
-          return super.equals(obj);
-        }
-        org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey other = (org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey) obj;
-        
-        boolean result = true;
-        result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
-        if (hasEncodedRegionName()) {
-          result = result && getEncodedRegionName()
-              .equals(other.getEncodedRegionName());
-        }
-        result = result && (hasTableName() == other.hasTableName());
-        if (hasTableName()) {
-          result = result && getTableName()
-              .equals(other.getTableName());
-        }
-        result = result && (hasLogSequenceNumber() == other.hasLogSequenceNumber());
-        if (hasLogSequenceNumber()) {
-          result = result && (getLogSequenceNumber()
-              == other.getLogSequenceNumber());
-        }
-        result = result && (hasWriteTime() == other.hasWriteTime());
-        if (hasWriteTime()) {
-          result = result && (getWriteTime()
-              == other.getWriteTime());
-        }
-        result = result && (hasClusterId() == other.hasClusterId());
-        if (hasClusterId()) {
-          result = result && getClusterId()
-              .equals(other.getClusterId());
-        }
-        result = result &&
-            getUnknownFields().equals(other.getUnknownFields());
-        return result;
-      }
-      
-      @java.lang.Override
-      public int hashCode() {
-        int hash = 41;
-        hash = (19 * hash) + getDescriptorForType().hashCode();
-        if (hasEncodedRegionName()) {
-          hash = (37 * hash) + ENCODEDREGIONNAME_FIELD_NUMBER;
-          hash = (53 * hash) + getEncodedRegionName().hashCode();
-        }
-        if (hasTableName()) {
-          hash = (37 * hash) + TABLENAME_FIELD_NUMBER;
-          hash = (53 * hash) + getTableName().hashCode();
-        }
-        if (hasLogSequenceNumber()) {
-          hash = (37 * hash) + LOGSEQUENCENUMBER_FIELD_NUMBER;
-          hash = (53 * hash) + hashLong(getLogSequenceNumber());
-        }
-        if (hasWriteTime()) {
-          hash = (37 * hash) + WRITETIME_FIELD_NUMBER;
-          hash = (53 * hash) + hashLong(getWriteTime());
-        }
-        if (hasClusterId()) {
-          hash = (37 * hash) + CLUSTERID_FIELD_NUMBER;
-          hash = (53 * hash) + getClusterId().hashCode();
-        }
-        hash = (29 * hash) + getUnknownFields().hashCode();
-        return hash;
-      }
-      
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          com.google.protobuf.ByteString data)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return newBuilder().mergeFrom(data).buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          com.google.protobuf.ByteString data,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return newBuilder().mergeFrom(data, extensionRegistry)
-                 .buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(byte[] data)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return newBuilder().mergeFrom(data).buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          byte[] data,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return newBuilder().mergeFrom(data, extensionRegistry)
-                 .buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(java.io.InputStream input)
-          throws java.io.IOException {
-        return newBuilder().mergeFrom(input).buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          java.io.InputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        return newBuilder().mergeFrom(input, extensionRegistry)
-                 .buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseDelimitedFrom(java.io.InputStream input)
-          throws java.io.IOException {
-        Builder builder = newBuilder();
-        if (builder.mergeDelimitedFrom(input)) {
-          return builder.buildParsed();
-        } else {
-          return null;
-        }
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseDelimitedFrom(
-          java.io.InputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        Builder builder = newBuilder();
-        if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-          return builder.buildParsed();
-        } else {
-          return null;
-        }
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          com.google.protobuf.CodedInputStream input)
-          throws java.io.IOException {
-        return newBuilder().mergeFrom(input).buildParsed();
-      }
-      public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey parseFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        return newBuilder().mergeFrom(input, extensionRegistry)
-                 .buildParsed();
-      }
-      
-      public static Builder newBuilder() { return Builder.create(); }
-      public Builder newBuilderForType() { return newBuilder(); }
-      public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey prototype) {
-        return newBuilder().mergeFrom(prototype);
-      }
-      public Builder toBuilder() { return newBuilder(this); }
-      
-      @java.lang.Override
-      protected Builder newBuilderForType(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        Builder builder = new Builder(parent);
-        return builder;
-      }
-      public static final class Builder extends
-          com.google.protobuf.GeneratedMessage.Builder<Builder>
-         implements org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder {
-        public static final com.google.protobuf.Descriptors.Descriptor
-            getDescriptor() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALKey_descriptor;
-        }
-        
-        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-            internalGetFieldAccessorTable() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALKey_fieldAccessorTable;
-        }
-        
-        // Construct using org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.newBuilder()
-        private Builder() {
-          maybeForceBuilderInitialization();
-        }
-        
-        private Builder(BuilderParent parent) {
-          super(parent);
-          maybeForceBuilderInitialization();
-        }
-        private void maybeForceBuilderInitialization() {
-          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-            getClusterIdFieldBuilder();
-          }
-        }
-        private static Builder create() {
-          return new Builder();
-        }
-        
-        public Builder clear() {
-          super.clear();
-          encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
-          bitField0_ = (bitField0_ & ~0x00000001);
-          tableName_ = com.google.protobuf.ByteString.EMPTY;
-          bitField0_ = (bitField0_ & ~0x00000002);
-          logSequenceNumber_ = 0L;
-          bitField0_ = (bitField0_ & ~0x00000004);
-          writeTime_ = 0L;
-          bitField0_ = (bitField0_ & ~0x00000008);
-          if (clusterIdBuilder_ == null) {
-            clusterId_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance();
-          } else {
-            clusterIdBuilder_.clear();
-          }
-          bitField0_ = (bitField0_ & ~0x00000010);
-          return this;
-        }
-        
-        public Builder clone() {
-          return create().mergeFrom(buildPartial());
-        }
-        
-        public com.google.protobuf.Descriptors.Descriptor
-            getDescriptorForType() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDescriptor();
-        }
-        
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey getDefaultInstanceForType() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance();
-        }
-        
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey build() {
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey result = buildPartial();
-          if (!result.isInitialized()) {
-            throw newUninitializedMessageException(result);
-          }
-          return result;
-        }
-        
-        private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey buildParsed()
-            throws com.google.protobuf.InvalidProtocolBufferException {
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey result = buildPartial();
-          if (!result.isInitialized()) {
-            throw newUninitializedMessageException(
-              result).asInvalidProtocolBufferException();
-          }
-          return result;
-        }
-        
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey buildPartial() {
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey result = new org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey(this);
-          int from_bitField0_ = bitField0_;
-          int to_bitField0_ = 0;
-          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-            to_bitField0_ |= 0x00000001;
-          }
-          result.encodedRegionName_ = encodedRegionName_;
-          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-            to_bitField0_ |= 0x00000002;
-          }
-          result.tableName_ = tableName_;
-          if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-            to_bitField0_ |= 0x00000004;
-          }
-          result.logSequenceNumber_ = logSequenceNumber_;
-          if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-            to_bitField0_ |= 0x00000008;
-          }
-          result.writeTime_ = writeTime_;
-          if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-            to_bitField0_ |= 0x00000010;
-          }
-          if (clusterIdBuilder_ == null) {
-            result.clusterId_ = clusterId_;
-          } else {
-            result.clusterId_ = clusterIdBuilder_.build();
-          }
-          result.bitField0_ = to_bitField0_;
-          onBuilt();
-          return result;
-        }
-        
-        public Builder mergeFrom(com.google.protobuf.Message other) {
-          if (other instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey) {
-            return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey)other);
-          } else {
-            super.mergeFrom(other);
-            return this;
-          }
-        }
-        
-        public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey other) {
-          if (other == org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance()) return this;
-          if (other.hasEncodedRegionName()) {
-            setEncodedRegionName(other.getEncodedRegionName());
-          }
-          if (other.hasTableName()) {
-            setTableName(other.getTableName());
-          }
-          if (other.hasLogSequenceNumber()) {
-            setLogSequenceNumber(other.getLogSequenceNumber());
-          }
-          if (other.hasWriteTime()) {
-            setWriteTime(other.getWriteTime());
-          }
-          if (other.hasClusterId()) {
-            mergeClusterId(other.getClusterId());
-          }
-          this.mergeUnknownFields(other.getUnknownFields());
-          return this;
-        }
-        
-        public final boolean isInitialized() {
-          if (!hasEncodedRegionName()) {
-            
-            return false;
-          }
-          if (!hasTableName()) {
-            
-            return false;
-          }
-          if (!hasLogSequenceNumber()) {
-            
-            return false;
-          }
-          if (!hasWriteTime()) {
-            
-            return false;
-          }
-          if (hasClusterId()) {
-            if (!getClusterId().isInitialized()) {
-              
-              return false;
-            }
-          }
-          return true;
-        }
-        
-        public Builder mergeFrom(
-            com.google.protobuf.CodedInputStream input,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws java.io.IOException {
-          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-            com.google.protobuf.UnknownFieldSet.newBuilder(
-              this.getUnknownFields());
-          while (true) {
-            int tag = input.readTag();
-            switch (tag) {
-              case 0:
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              default: {
-                if (!parseUnknownField(input, unknownFields,
-                                       extensionRegistry, tag)) {
-                  this.setUnknownFields(unknownFields.build());
-                  onChanged();
-                  return this;
-                }
-                break;
-              }
-              case 10: {
-                bitField0_ |= 0x00000001;
-                encodedRegionName_ = input.readBytes();
-                break;
-              }
-              case 18: {
-                bitField0_ |= 0x00000002;
-                tableName_ = input.readBytes();
-                break;
-              }
-              case 24: {
-                bitField0_ |= 0x00000004;
-                logSequenceNumber_ = input.readUInt64();
-                break;
-              }
-              case 32: {
-                bitField0_ |= 0x00000008;
-                writeTime_ = input.readUInt64();
-                break;
-              }
-              case 42: {
-                org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.newBuilder();
-                if (hasClusterId()) {
-                  subBuilder.mergeFrom(getClusterId());
-                }
-                input.readMessage(subBuilder, extensionRegistry);
-                setClusterId(subBuilder.buildPartial());
-                break;
-              }
-            }
-          }
-        }
-        
-        private int bitField0_;
-        
-        // required bytes encodedRegionName = 1;
-        private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
-        public boolean hasEncodedRegionName() {
-          return ((bitField0_ & 0x00000001) == 0x00000001);
-        }
-        public com.google.protobuf.ByteString getEncodedRegionName() {
-          return encodedRegionName_;
-        }
-        public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
-          if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-          encodedRegionName_ = value;
-          onChanged();
-          return this;
-        }
-        public Builder clearEncodedRegionName() {
-          bitField0_ = (bitField0_ & ~0x00000001);
-          encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
-          onChanged();
-          return this;
-        }
-        
-        // required bytes tableName = 2;
-        private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
-        public boolean hasTableName() {
-          return ((bitField0_ & 0x00000002) == 0x00000002);
-        }
-        public com.google.protobuf.ByteString getTableName() {
-          return tableName_;
-        }
-        public Builder setTableName(com.google.protobuf.ByteString value) {
-          if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-          tableName_ = value;
-          onChanged();
-          return this;
-        }
-        public Builder clearTableName() {
-          bitField0_ = (bitField0_ & ~0x00000002);
-          tableName_ = getDefaultInstance().getTableName();
-          onChanged();
-          return this;
-        }
-        
-        // required uint64 logSequenceNumber = 3;
-        private long logSequenceNumber_ ;
-        public boolean hasLogSequenceNumber() {
-          return ((bitField0_ & 0x00000004) == 0x00000004);
-        }
-        public long getLogSequenceNumber() {
-          return logSequenceNumber_;
-        }
-        public Builder setLogSequenceNumber(long value) {
-          bitField0_ |= 0x00000004;
-          logSequenceNumber_ = value;
-          onChanged();
-          return this;
-        }
-        public Builder clearLogSequenceNumber() {
-          bitField0_ = (bitField0_ & ~0x00000004);
-          logSequenceNumber_ = 0L;
-          onChanged();
-          return this;
-        }
-        
-        // required uint64 writeTime = 4;
-        private long writeTime_ ;
-        public boolean hasWriteTime() {
-          return ((bitField0_ & 0x00000008) == 0x00000008);
-        }
-        public long getWriteTime() {
-          return writeTime_;
-        }
-        public Builder setWriteTime(long value) {
-          bitField0_ |= 0x00000008;
-          writeTime_ = value;
-          onChanged();
-          return this;
-        }
-        public Builder clearWriteTime() {
-          bitField0_ = (bitField0_ & ~0x00000008);
-          writeTime_ = 0L;
-          onChanged();
-          return this;
-        }
-        
-        // optional .UUID clusterId = 5;
-        private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID clusterId_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance();
-        private com.google.protobuf.SingleFieldBuilder<
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder> clusterIdBuilder_;
-        public boolean hasClusterId() {
-          return ((bitField0_ & 0x00000010) == 0x00000010);
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID getClusterId() {
-          if (clusterIdBuilder_ == null) {
-            return clusterId_;
-          } else {
-            return clusterIdBuilder_.getMessage();
-          }
-        }
-        public Builder setClusterId(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID value) {
-          if (clusterIdBuilder_ == null) {
-            if (value == null) {
-              throw new NullPointerException();
-            }
-            clusterId_ = value;
-            onChanged();
-          } else {
-            clusterIdBuilder_.setMessage(value);
-          }
-          bitField0_ |= 0x00000010;
-          return this;
-        }
-        public Builder setClusterId(
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder builderForValue) {
-          if (clusterIdBuilder_ == null) {
-            clusterId_ = builderForValue.build();
-            onChanged();
-          } else {
-            clusterIdBuilder_.setMessage(builderForValue.build());
-          }
-          bitField0_ |= 0x00000010;
-          return this;
-        }
-        public Builder mergeClusterId(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID value) {
-          if (clusterIdBuilder_ == null) {
-            if (((bitField0_ & 0x00000010) == 0x00000010) &&
-                clusterId_ != org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance()) {
-              clusterId_ =
-                org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.newBuilder(clusterId_).mergeFrom(value).buildPartial();
-            } else {
-              clusterId_ = value;
-            }
-            onChanged();
-          } else {
-            clusterIdBuilder_.mergeFrom(value);
-          }
-          bitField0_ |= 0x00000010;
-          return this;
-        }
-        public Builder clearClusterId() {
-          if (clusterIdBuilder_ == null) {
-            clusterId_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.getDefaultInstance();
-            onChanged();
-          } else {
-            clusterIdBuilder_.clear();
-          }
-          bitField0_ = (bitField0_ & ~0x00000010);
-          return this;
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder getClusterIdBuilder() {
-          bitField0_ |= 0x00000010;
-          onChanged();
-          return getClusterIdFieldBuilder().getBuilder();
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder getClusterIdOrBuilder() {
-          if (clusterIdBuilder_ != null) {
-            return clusterIdBuilder_.getMessageOrBuilder();
-          } else {
-            return clusterId_;
-          }
-        }
-        private com.google.protobuf.SingleFieldBuilder<
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder> 
-            getClusterIdFieldBuilder() {
-          if (clusterIdBuilder_ == null) {
-            clusterIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-                org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUIDOrBuilder>(
-                    clusterId_,
-                    getParentForChildren(),
-                    isClean());
-            clusterId_ = null;
-          }
-          return clusterIdBuilder_;
-        }
-        
-        // @@protoc_insertion_point(builder_scope:WALEntry.WALKey)
-      }
-      
-      static {
-        defaultInstance = new WALKey(true);
-        defaultInstance.initFields();
-      }
-      
-      // @@protoc_insertion_point(class_scope:WALEntry.WALKey)
-    }
-    
-    public interface WALEditOrBuilder
-        extends com.google.protobuf.MessageOrBuilder {
-      
-      // repeated bytes keyValueBytes = 1;
-      java.util.List<com.google.protobuf.ByteString> getKeyValueBytesList();
-      int getKeyValueBytesCount();
-      com.google.protobuf.ByteString getKeyValueBytes(int index);
-      
-      // repeated .WALEntry.WALEdit.FamilyScope familyScope = 2;
-      java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> 
-          getFamilyScopeList();
-      org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope getFamilyScope(int index);
-      int getFamilyScopeCount();
-      java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder> 
-          getFamilyScopeOrBuilderList();
-      org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder getFamilyScopeOrBuilder(
-          int index);
-    }
-    public static final class WALEdit extends
-        com.google.protobuf.GeneratedMessage
-        implements WALEditOrBuilder {
-      // Use WALEdit.newBuilder() to construct.
-      private WALEdit(Builder builder) {
-        super(builder);
-      }
-      private WALEdit(boolean noInit) {}
-      
-      private static final WALEdit defaultInstance;
-      public static WALEdit getDefaultInstance() {
-        return defaultInstance;
-      }
-      
-      public WALEdit getDefaultInstanceForType() {
-        return defaultInstance;
-      }
-      
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_fieldAccessorTable;
-      }
-      
-      public enum ScopeType
-          implements com.google.protobuf.ProtocolMessageEnum {
-        REPLICATION_SCOPE_LOCAL(0, 0),
-        REPLICATION_SCOPE_GLOBAL(1, 1),
-        ;
-        
-        public static final int REPLICATION_SCOPE_LOCAL_VALUE = 0;
-        public static final int REPLICATION_SCOPE_GLOBAL_VALUE = 1;
-        
-        
-        public final int getNumber() { return value; }
-        
-        public static ScopeType valueOf(int value) {
-          switch (value) {
-            case 0: return REPLICATION_SCOPE_LOCAL;
-            case 1: return REPLICATION_SCOPE_GLOBAL;
-            default: return null;
-          }
-        }
-        
-        public static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
-            internalGetValueMap() {
-          return internalValueMap;
-        }
-        private static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
-            internalValueMap =
-              new com.google.protobuf.Internal.EnumLiteMap<ScopeType>() {
-                public ScopeType findValueByNumber(int number) {
-                  return ScopeType.valueOf(number);
-                }
-              };
-        
-        public final com.google.protobuf.Descriptors.EnumValueDescriptor
-            getValueDescriptor() {
-          return getDescriptor().getValues().get(index);
-        }
-        public final com.google.protobuf.Descriptors.EnumDescriptor
-            getDescriptorForType() {
-          return getDescriptor();
-        }
-        public static final com.google.protobuf.Descriptors.EnumDescriptor
-            getDescriptor() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.getDescriptor().getEnumTypes().get(0);
-        }
-        
-        private static final ScopeType[] VALUES = {
-          REPLICATION_SCOPE_LOCAL, REPLICATION_SCOPE_GLOBAL, 
-        };
-        
-        public static ScopeType valueOf(
-            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
-          if (desc.getType() != getDescriptor()) {
-            throw new java.lang.IllegalArgumentException(
-              "EnumValueDescriptor is not for this type.");
-          }
-          return VALUES[desc.getIndex()];
-        }
-        
-        private final int index;
-        private final int value;
-        
-        private ScopeType(int index, int value) {
-          this.index = index;
-          this.value = value;
-        }
-        
-        // @@protoc_insertion_point(enum_scope:WALEntry.WALEdit.ScopeType)
-      }
-      
-      public interface FamilyScopeOrBuilder
-          extends com.google.protobuf.MessageOrBuilder {
-        
-        // required bytes family = 1;
-        boolean hasFamily();
-        com.google.protobuf.ByteString getFamily();
-        
-        // required .WALEntry.WALEdit.ScopeType scopeType = 2;
-        boolean hasScopeType();
-        org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType getScopeType();
-      }
-      public static final class FamilyScope extends
-          com.google.protobuf.GeneratedMessage
-          implements FamilyScopeOrBuilder {
-        // Use FamilyScope.newBuilder() to construct.
-        private FamilyScope(Builder builder) {
-          super(builder);
-        }
-        private FamilyScope(boolean noInit) {}
-        
-        private static final FamilyScope defaultInstance;
-        public static FamilyScope getDefaultInstance() {
-          return defaultInstance;
-        }
-        
-        public FamilyScope getDefaultInstanceForType() {
-          return defaultInstance;
-        }
-        
-        public static final com.google.protobuf.Descriptors.Descriptor
-            getDescriptor() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_FamilyScope_descriptor;
-        }
-        
-        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-            internalGetFieldAccessorTable() {
-          return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_FamilyScope_fieldAccessorTable;
-        }
-        
-        private int bitField0_;
-        // required bytes family = 1;
-        public static final int FAMILY_FIELD_NUMBER = 1;
-        private com.google.protobuf.ByteString family_;
-        public boolean hasFamily() {
-          return ((bitField0_ & 0x00000001) == 0x00000001);
-        }
-        public com.google.protobuf.ByteString getFamily() {
-          return family_;
-        }
-        
-        // required .WALEntry.WALEdit.ScopeType scopeType = 2;
-        public static final int SCOPETYPE_FIELD_NUMBER = 2;
-        private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType scopeType_;
-        public boolean hasScopeType() {
-          return ((bitField0_ & 0x00000002) == 0x00000002);
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType getScopeType() {
-          return scopeType_;
-        }
-        
-        private void initFields() {
-          family_ = com.google.protobuf.ByteString.EMPTY;
-          scopeType_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType.REPLICATION_SCOPE_LOCAL;
-        }
-        private byte memoizedIsInitialized = -1;
-        public final boolean isInitialized() {
-          byte isInitialized = memoizedIsInitialized;
-          if (isInitialized != -1) return isInitialized == 1;
-          
-          if (!hasFamily()) {
-            memoizedIsInitialized = 0;
-            return false;
-          }
-          if (!hasScopeType()) {
-            memoizedIsInitialized = 0;
-            return false;
-          }
-          memoizedIsInitialized = 1;
-          return true;
-        }
-        
-        public void writeTo(com.google.protobuf.CodedOutputStream output)
-                            throws java.io.IOException {
-          getSerializedSize();
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            output.writeBytes(1, family_);
-          }
-          if (((bitField0_ & 0x00000002) == 0x00000002)) {
-            output.writeEnum(2, scopeType_.getNumber());
-          }
-          getUnknownFields().writeTo(output);
-        }
-        
-        private int memoizedSerializedSize = -1;
-        public int getSerializedSize() {
-          int size = memoizedSerializedSize;
-          if (size != -1) return size;
-        
-          size = 0;
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            size += com.google.protobuf.CodedOutputStream
-              .computeBytesSize(1, family_);
-          }
-          if (((bitField0_ & 0x00000002) == 0x00000002)) {
-            size += com.google.protobuf.CodedOutputStream
-              .computeEnumSize(2, scopeType_.getNumber());
-          }
-          size += getUnknownFields().getSerializedSize();
-          memoizedSerializedSize = size;
-          return size;
-        }
-        
-        private static final long serialVersionUID = 0L;
-        @java.lang.Override
-        protected java.lang.Object writeReplace()
-            throws java.io.ObjectStreamException {
-          return super.writeReplace();
-        }
-        
-        @java.lang.Override
-        public boolean equals(final java.lang.Object obj) {
-          if (obj == this) {
-           return true;
-          }
-          if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope)) {
-            return super.equals(obj);
-          }
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope other = (org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope) obj;
-          
-          boolean result = true;
-          result = result && (hasFamily() == other.hasFamily());
-          if (hasFamily()) {
-            result = result && getFamily()
-                .equals(other.getFamily());
-          }
-          result = result && (hasScopeType() == other.hasScopeType());
-          if (hasScopeType()) {
-            result = result &&
-                (getScopeType() == other.getScopeType());
-          }
-          result = result &&
-              getUnknownFields().equals(other.getUnknownFields());
-          return result;
-        }
-        
-        @java.lang.Override
-        public int hashCode() {
-          int hash = 41;
-          hash = (19 * hash) + getDescriptorForType().hashCode();
-          if (hasFamily()) {
-            hash = (37 * hash) + FAMILY_FIELD_NUMBER;
-            hash = (53 * hash) + getFamily().hashCode();
-          }
-          if (hasScopeType()) {
-            hash = (37 * hash) + SCOPETYPE_FIELD_NUMBER;
-            hash = (53 * hash) + hashEnum(getScopeType());
-          }
-          hash = (29 * hash) + getUnknownFields().hashCode();
-          return hash;
-        }
-        
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            com.google.protobuf.ByteString data)
-            throws com.google.protobuf.InvalidProtocolBufferException {
-          return newBuilder().mergeFrom(data).buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            com.google.protobuf.ByteString data,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws com.google.protobuf.InvalidProtocolBufferException {
-          return newBuilder().mergeFrom(data, extensionRegistry)
-                   .buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(byte[] data)
-            throws com.google.protobuf.InvalidProtocolBufferException {
-          return newBuilder().mergeFrom(data).buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            byte[] data,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws com.google.protobuf.InvalidProtocolBufferException {
-          return newBuilder().mergeFrom(data, extensionRegistry)
-                   .buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(java.io.InputStream input)
-            throws java.io.IOException {
-          return newBuilder().mergeFrom(input).buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            java.io.InputStream input,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws java.io.IOException {
-          return newBuilder().mergeFrom(input, extensionRegistry)
-                   .buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseDelimitedFrom(java.io.InputStream input)
-            throws java.io.IOException {
-          Builder builder = newBuilder();
-          if (builder.mergeDelimitedFrom(input)) {
-            return builder.buildParsed();
-          } else {
-            return null;
-          }
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseDelimitedFrom(
-            java.io.InputStream input,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws java.io.IOException {
-          Builder builder = newBuilder();
-          if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-            return builder.buildParsed();
-          } else {
-            return null;
-          }
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            com.google.protobuf.CodedInputStream input)
-            throws java.io.IOException {
-          return newBuilder().mergeFrom(input).buildParsed();
-        }
-        public static org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope parseFrom(
-            com.google.protobuf.CodedInputStream input,
-            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-            throws java.io.IOException {
-          return newBuilder().mergeFrom(input, extensionRegistry)
-                   .buildParsed();
-        }
-        
-        public static Builder newBuilder() { return Builder.create(); }
-        public Builder newBuilderForType() { return newBuilder(); }
-        public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope prototype) {
-          return newBuilder().mergeFrom(prototype);
-        }
-        public Builder toBuilder() { return newBuilder(this); }
-        
-        @java.lang.Override
-        protected Builder newBuilderForType(
-            com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-          Builder builder = new Builder(parent);
-          return builder;
-        }
-        public static final class Builder extends
-            com.google.protobuf.GeneratedMessage.Builder<Builder>
-           implements org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder {
-          public static final com.google.protobuf.Descriptors.Descriptor
-              getDescriptor() {
-            return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_FamilyScope_descriptor;
-          }
-          
-          protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-              internalGetFieldAccessorTable() {
-            return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_FamilyScope_fieldAccessorTable;
-          }
-          
-          // Construct using org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.newBuilder()
-          private Builder() {
-            maybeForceBuilderInitialization();
-          }
-          
-          private Builder(BuilderParent parent) {
-            super(parent);
-            maybeForceBuilderInitialization();
-          }
-          private void maybeForceBuilderInitialization() {
-            if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-            }
-          }
-          private static Builder create() {
-            return new Builder();
-          }
-          
-          public Builder clear() {
-            super.clear();
-            family_ = com.google.protobuf.ByteString.EMPTY;
-            bitField0_ = (bitField0_ & ~0x00000001);
-            scopeType_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType.REPLICATION_SCOPE_LOCAL;
-            bitField0_ = (bitField0_ & ~0x00000002);
-            return this;
-          }
-          
-          public Builder clone() {
-            return create().mergeFrom(buildPartial());
-          }
-          
-          public com.google.protobuf.Descriptors.Descriptor
-              getDescriptorForType() {
-            return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.getDescriptor();
-          }
-          
-          public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope getDefaultInstanceForType() {
-            return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.getDefaultInstance();
-          }
-          
-          public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope build() {
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope result = buildPartial();
-            if (!result.isInitialized()) {
-              throw newUninitializedMessageException(result);
-            }
-            return result;
-          }
-          
-          private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope buildParsed()
-              throws com.google.protobuf.InvalidProtocolBufferException {
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope result = buildPartial();
-            if (!result.isInitialized()) {
-              throw newUninitializedMessageException(
-                result).asInvalidProtocolBufferException();
-            }
-            return result;
-          }
-          
-          public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope buildPartial() {
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope result = new org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope(this);
-            int from_bitField0_ = bitField0_;
-            int to_bitField0_ = 0;
-            if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-              to_bitField0_ |= 0x00000001;
-            }
-            result.family_ = family_;
-            if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-              to_bitField0_ |= 0x00000002;
-            }
-            result.scopeType_ = scopeType_;
-            result.bitField0_ = to_bitField0_;
-            onBuilt();
-            return result;
-          }
-          
-          public Builder mergeFrom(com.google.protobuf.Message other) {
-            if (other instanceof org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope) {
-              return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope)other);
-            } else {
-              super.mergeFrom(other);
-              return this;
-            }
-          }
-          
-          public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope other) {
-            if (other == org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.getDefaultInstance()) return this;
-            if (other.hasFamily()) {
-              setFamily(other.getFamily());
-            }
-            if (other.hasScopeType()) {
-              setScopeType(other.getScopeType());
-            }
-            this.mergeUnknownFields(other.getUnknownFields());
-            return this;
-          }
-          
-          public final boolean isInitialized() {
-            if (!hasFamily()) {
-              
-              return false;
-            }
-            if (!hasScopeType()) {
-              
-              return false;
-            }
-            return true;
-          }
-          
-          public Builder mergeFrom(
-              com.google.protobuf.CodedInputStream input,
-              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-              throws java.io.IOException {
-            com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-              com.google.protobuf.UnknownFieldSet.newBuilder(
-                this.getUnknownFields());
-            while (true) {
-              int tag = input.readTag();
-              switch (tag) {
-                case 0:
-                  this.setUnknownFields(unknownFields.build());
-                  onChanged();
-                  return this;
-                default: {
-                  if (!parseUnknownField(input, unknownFields,
-                                         extensionRegistry, tag)) {
-                    this.setUnknownFields(unknownFields.build());
-                    onChanged();
-                    return this;
-                  }
-                  break;
-                }
-                case 10: {
-                  bitField0_ |= 0x00000001;
-                  family_ = input.readBytes();
-                  break;
-                }
-                case 16: {
-                  int rawValue = input.readEnum();
-                  org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType value = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType.valueOf(rawValue);
-                  if (value == null) {
-                    unknownFields.mergeVarintField(2, rawValue);
-                  } else {
-                    bitField0_ |= 0x00000002;
-                    scopeType_ = value;
-                  }
-                  break;
-                }
-              }
-            }
-          }
-          
-          private int bitField0_;
-          
-          // required bytes family = 1;
-          private com.google.protobuf.ByteString family_ = com.google.protobuf.ByteString.EMPTY;
-          public boolean hasFamily() {
-            return ((bitField0_ & 0x00000001) == 0x00000001);
-          }
-          public com.google.protobuf.ByteString getFamily() {
-            return family_;
-          }
-          public Builder setFamily(com.google.protobuf.ByteString value) {
-            if (value == null) {
-    throw new NullPointerException();
+  public interface WALEntryOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required .WALKey key = 1;
+    boolean hasKey();
+    org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey getKey();
+    org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder getKeyOrBuilder();
+    
+    // required .WALEntry.WALEdit edit = 2;
+    boolean hasEdit();
+    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit getEdit();
+    org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEditOrBuilder getEditOrBuilder();
   }
-  bitField0_ |= 0x00000001;
-            family_ = value;
-            onChanged();
-            return this;
-          }
-          public Builder clearFamily() {
-            bitField0_ = (bitField0_ & ~0x00000001);
-            family_ = getDefaultInstance().getFamily();
-            onChanged();
-            return this;
-          }
-          
-          // required .WALEntry.WALEdit.ScopeType scopeType = 2;
-          private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType scopeType_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType.REPLICATION_SCOPE_LOCAL;
-          public boolean hasScopeType() {
-            return ((bitField0_ & 0x00000002) == 0x00000002);
-          }
-          public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType getScopeType() {
-            return scopeType_;
-          }
-          public Builder setScopeType(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType value) {
-            if (value == null) {
-              throw new NullPointerException();
-            }
-            bitField0_ |= 0x00000002;
-            scopeType_ = value;
-            onChanged();
-            return this;
-          }
-          public Builder clearScopeType() {
-            bitField0_ = (bitField0_ & ~0x00000002);
-            scopeType_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.ScopeType.REPLICATION_SCOPE_LOCAL;
-            onChanged();
-            return this;
-          }
-          
-          // @@protoc_insertion_point(builder_scope:WALEntry.WALEdit.FamilyScope)
-        }
-        
-        static {
-          defaultInstance = new FamilyScope(true);
-          defaultInstance.initFields();
-        }
-        
-        // @@protoc_insertion_point(class_scope:WALEntry.WALEdit.FamilyScope)
+  public static final class WALEntry extends
+      com.google.protobuf.GeneratedMessage
+      implements WALEntryOrBuilder {
+    // Use WALEntry.newBuilder() to construct.
+    private WALEntry(Builder builder) {
+      super(builder);
+    }
+    private WALEntry(boolean noInit) {}
+    
+    private static final WALEntry defaultInstance;
+    public static WALEntry getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public WALEntry getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_fieldAccessorTable;
+    }
+    
+    public interface WALEditOrBuilder
+        extends com.google.protobuf.MessageOrBuilder {
+      
+      // repeated bytes keyValueBytes = 1;
+      java.util.List<com.google.protobuf.ByteString> getKeyValueBytesList();
+      int getKeyValueBytesCount();
+      com.google.protobuf.ByteString getKeyValueBytes(int index);
+    }
+    public static final class WALEdit extends
+        com.google.protobuf.GeneratedMessage
+        implements WALEditOrBuilder {
+      // Use WALEdit.newBuilder() to construct.
+      private WALEdit(Builder builder) {
+        super(builder);
+      }
+      private WALEdit(boolean noInit) {}
+      
+      private static final WALEdit defaultInstance;
+      public static WALEdit getDefaultInstance() {
+        return defaultInstance;
+      }
+      
+      public WALEdit getDefaultInstanceForType() {
+        return defaultInstance;
+      }
+      
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.AdminProtos.internal_static_WALEntry_WALEdit_fieldAccessorTable;
       }
       
       // repeated bytes keyValueBytes = 1;
@@ -11450,42 +9682,14 @@ public final class AdminProtos {
         return keyValueBytes_.get(index);
       }
       
-      // repeated .WALEntry.WALEdit.FamilyScope familyScope = 2;
-      public static final int FAMILYSCOPE_FIELD_NUMBER = 2;
-      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> familyScope_;
-      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> getFamilyScopeList() {
-        return familyScope_;
-      }
-      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder> 
-          getFamilyScopeOrBuilderList() {
-        return familyScope_;
-      }
-      public int getFamilyScopeCount() {
-        return familyScope_.size();
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope getFamilyScope(int index) {
-        return familyScope_.get(index);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder getFamilyScopeOrBuilder(
-          int index) {
-        return familyScope_.get(index);
-      }
-      
       private void initFields() {
         keyValueBytes_ = java.util.Collections.emptyList();;
-        familyScope_ = java.util.Collections.emptyList();
       }
       private byte memoizedIsInitialized = -1;
       public final boolean isInitialized() {
         byte isInitialized = memoizedIsInitialized;
         if (isInitialized != -1) return isInitialized == 1;
         
-        for (int i = 0; i < getFamilyScopeCount(); i++) {
-          if (!getFamilyScope(i).isInitialized()) {
-            memoizedIsInitialized = 0;
-            return false;
-          }
-        }
         memoizedIsInitialized = 1;
         return true;
       }
@@ -11496,9 +9700,6 @@ public final class AdminProtos {
         for (int i = 0; i < keyValueBytes_.size(); i++) {
           output.writeBytes(1, keyValueBytes_.get(i));
         }
-        for (int i = 0; i < familyScope_.size(); i++) {
-          output.writeMessage(2, familyScope_.get(i));
-        }
         getUnknownFields().writeTo(output);
       }
       
@@ -11517,10 +9718,6 @@ public final class AdminProtos {
           size += dataSize;
           size += 1 * getKeyValueBytesList().size();
         }
-        for (int i = 0; i < familyScope_.size(); i++) {
-          size += com.google.protobuf.CodedOutputStream
-            .computeMessageSize(2, familyScope_.get(i));
-        }
         size += getUnknownFields().getSerializedSize();
         memoizedSerializedSize = size;
         return size;
@@ -11546,8 +9743,6 @@ public final class AdminProtos {
         boolean result = true;
         result = result && getKeyValueBytesList()
             .equals(other.getKeyValueBytesList());
-        result = result && getFamilyScopeList()
-            .equals(other.getFamilyScopeList());
         result = result &&
             getUnknownFields().equals(other.getUnknownFields());
         return result;
@@ -11561,10 +9756,6 @@ public final class AdminProtos {
           hash = (37 * hash) + KEYVALUEBYTES_FIELD_NUMBER;
           hash = (53 * hash) + getKeyValueBytesList().hashCode();
         }
-        if (getFamilyScopeCount() > 0) {
-          hash = (37 * hash) + FAMILYSCOPE_FIELD_NUMBER;
-          hash = (53 * hash) + getFamilyScopeList().hashCode();
-        }
         hash = (29 * hash) + getUnknownFields().hashCode();
         return hash;
       }
@@ -11673,7 +9864,6 @@ public final class AdminProtos {
         }
         private void maybeForceBuilderInitialization() {
           if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-            getFamilyScopeFieldBuilder();
           }
         }
         private static Builder create() {
@@ -11684,12 +9874,6 @@ public final class AdminProtos {
           super.clear();
           keyValueBytes_ = java.util.Collections.emptyList();;
           bitField0_ = (bitField0_ & ~0x00000001);
-          if (familyScopeBuilder_ == null) {
-            familyScope_ = java.util.Collections.emptyList();
-            bitField0_ = (bitField0_ & ~0x00000002);
-          } else {
-            familyScopeBuilder_.clear();
-          }
           return this;
         }
         
@@ -11732,15 +9916,6 @@ public final class AdminProtos {
             bitField0_ = (bitField0_ & ~0x00000001);
           }
           result.keyValueBytes_ = keyValueBytes_;
-          if (familyScopeBuilder_ == null) {
-            if (((bitField0_ & 0x00000002) == 0x00000002)) {
-              familyScope_ = java.util.Collections.unmodifiableList(familyScope_);
-              bitField0_ = (bitField0_ & ~0x00000002);
-            }
-            result.familyScope_ = familyScope_;
-          } else {
-            result.familyScope_ = familyScopeBuilder_.build();
-          }
           onBuilt();
           return result;
         }
@@ -11766,43 +9941,11 @@ public final class AdminProtos {
             }
             onChanged();
           }
-          if (familyScopeBuilder_ == null) {
-            if (!other.familyScope_.isEmpty()) {
-              if (familyScope_.isEmpty()) {
-                familyScope_ = other.familyScope_;
-                bitField0_ = (bitField0_ & ~0x00000002);
-              } else {
-                ensureFamilyScopeIsMutable();
-                familyScope_.addAll(other.familyScope_);
-              }
-              onChanged();
-            }
-          } else {
-            if (!other.familyScope_.isEmpty()) {
-              if (familyScopeBuilder_.isEmpty()) {
-                familyScopeBuilder_.dispose();
-                familyScopeBuilder_ = null;
-                familyScope_ = other.familyScope_;
-                bitField0_ = (bitField0_ & ~0x00000002);
-                familyScopeBuilder_ = 
-                  com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                     getFamilyScopeFieldBuilder() : null;
-              } else {
-                familyScopeBuilder_.addAllMessages(other.familyScope_);
-              }
-            }
-          }
           this.mergeUnknownFields(other.getUnknownFields());
           return this;
         }
         
         public final boolean isInitialized() {
-          for (int i = 0; i < getFamilyScopeCount(); i++) {
-            if (!getFamilyScope(i).isInitialized()) {
-              
-              return false;
-            }
-          }
           return true;
         }
         
@@ -11834,12 +9977,6 @@ public final class AdminProtos {
                 keyValueBytes_.add(input.readBytes());
                 break;
               }
-              case 18: {
-                org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.newBuilder();
-                input.readMessage(subBuilder, extensionRegistry);
-                addFamilyScope(subBuilder.buildPartial());
-                break;
-              }
             }
           }
         }
@@ -11897,192 +10034,6 @@ public final class AdminProtos {
           return this;
         }
         
-        // repeated .WALEntry.WALEdit.FamilyScope familyScope = 2;
-        private java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> familyScope_ =
-          java.util.Collections.emptyList();
-        private void ensureFamilyScopeIsMutable() {
-          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-            familyScope_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope>(familyScope_);
-            bitField0_ |= 0x00000002;
-           }
-        }
-        
-        private com.google.protobuf.RepeatedFieldBuilder<
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder> familyScopeBuilder_;
-        
-        public java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> getFamilyScopeList() {
-          if (familyScopeBuilder_ == null) {
-            return java.util.Collections.unmodifiableList(familyScope_);
-          } else {
-            return familyScopeBuilder_.getMessageList();
-          }
-        }
-        public int getFamilyScopeCount() {
-          if (familyScopeBuilder_ == null) {
-            return familyScope_.size();
-          } else {
-            return familyScopeBuilder_.getCount();
-          }
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope getFamilyScope(int index) {
-          if (familyScopeBuilder_ == null) {
-            return familyScope_.get(index);
-          } else {
-            return familyScopeBuilder_.getMessage(index);
-          }
-        }
-        public Builder setFamilyScope(
-            int index, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope value) {
-          if (familyScopeBuilder_ == null) {
-            if (value == null) {
-              throw new NullPointerException();
-            }
-            ensureFamilyScopeIsMutable();
-            familyScope_.set(index, value);
-            onChanged();
-          } else {
-            familyScopeBuilder_.setMessage(index, value);
-          }
-          return this;
-        }
-        public Builder setFamilyScope(
-            int index, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder builderForValue) {
-          if (familyScopeBuilder_ == null) {
-            ensureFamilyScopeIsMutable();
-            familyScope_.set(index, builderForValue.build());
-            onChanged();
-          } else {
-            familyScopeBuilder_.setMessage(index, builderForValue.build());
-          }
-          return this;
-        }
-        public Builder addFamilyScope(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope value) {
-          if (familyScopeBuilder_ == null) {
-            if (value == null) {
-              throw new NullPointerException();
-            }
-            ensureFamilyScopeIsMutable();
-            familyScope_.add(value);
-            onChanged();
-          } else {
-            familyScopeBuilder_.addMessage(value);
-          }
-          return this;
-        }
-        public Builder addFamilyScope(
-            int index, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope value) {
-          if (familyScopeBuilder_ == null) {
-            if (value == null) {
-              throw new NullPointerException();
-            }
-            ensureFamilyScopeIsMutable();
-            familyScope_.add(index, value);
-            onChanged();
-          } else {
-            familyScopeBuilder_.addMessage(index, value);
-          }
-          return this;
-        }
-        public Builder addFamilyScope(
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder builderForValue) {
-          if (familyScopeBuilder_ == null) {
-            ensureFamilyScopeIsMutable();
-            familyScope_.add(builderForValue.build());
-            onChanged();
-          } else {
-            familyScopeBuilder_.addMessage(builderForValue.build());
-          }
-          return this;
-        }
-        public Builder addFamilyScope(
-            int index, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder builderForValue) {
-          if (familyScopeBuilder_ == null) {
-            ensureFamilyScopeIsMutable();
-            familyScope_.add(index, builderForValue.build());
-            onChanged();
-          } else {
-            familyScopeBuilder_.addMessage(index, builderForValue.build());
-          }
-          return this;
-        }
-        public Builder addAllFamilyScope(
-            java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope> values) {
-          if (familyScopeBuilder_ == null) {
-            ensureFamilyScopeIsMutable();
-            super.addAll(values, familyScope_);
-            onChanged();
-          } else {
-            familyScopeBuilder_.addAllMessages(values);
-          }
-          return this;
-        }
-        public Builder clearFamilyScope() {
-          if (familyScopeBuilder_ == null) {
-            familyScope_ = java.util.Collections.emptyList();
-            bitField0_ = (bitField0_ & ~0x00000002);
-            onChanged();
-          } else {
-            familyScopeBuilder_.clear();
-          }
-          return this;
-        }
-        public Builder removeFamilyScope(int index) {
-          if (familyScopeBuilder_ == null) {
-            ensureFamilyScopeIsMutable();
-            familyScope_.remove(index);
-            onChanged();
-          } else {
-            familyScopeBuilder_.remove(index);
-          }
-          return this;
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder getFamilyScopeBuilder(
-            int index) {
-          return getFamilyScopeFieldBuilder().getBuilder(index);
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder getFamilyScopeOrBuilder(
-            int index) {
-          if (familyScopeBuilder_ == null) {
-            return familyScope_.get(index);  } else {
-            return familyScopeBuilder_.getMessageOrBuilder(index);
-          }
-        }
-        public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder> 
-             getFamilyScopeOrBuilderList() {
-          if (familyScopeBuilder_ != null) {
-            return familyScopeBuilder_.getMessageOrBuilderList();
-          } else {
-            return java.util.Collections.unmodifiableList(familyScope_);
-          }
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder addFamilyScopeBuilder() {
-          return getFamilyScopeFieldBuilder().addBuilder(
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.getDefaultInstance());
-        }
-        public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder addFamilyScopeBuilder(
-            int index) {
-          return getFamilyScopeFieldBuilder().addBuilder(
-              index, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.getDefaultInstance());
-        }
-        public java.util.List<org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder> 
-             getFamilyScopeBuilderList() {
-          return getFamilyScopeFieldBuilder().getBuilderList();
-        }
-        private com.google.protobuf.RepeatedFieldBuilder<
-            org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder> 
-            getFamilyScopeFieldBuilder() {
-          if (familyScopeBuilder_ == null) {
-            familyScopeBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-                org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScopeOrBuilder>(
-                    familyScope_,
-                    ((bitField0_ & 0x00000002) == 0x00000002),
-                    getParentForChildren(),
-                    isClean());
-            familyScope_ = null;
-          }
-          return familyScopeBuilder_;
-        }
-        
         // @@protoc_insertion_point(builder_scope:WALEntry.WALEdit)
       }
       
@@ -12095,16 +10046,16 @@ public final class AdminProtos {
     }
     
     private int bitField0_;
-    // required .WALEntry.WALKey key = 1;
+    // required .WALKey key = 1;
     public static final int KEY_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey key_;
+    private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey key_;
     public boolean hasKey() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
-    public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey getKey() {
+    public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey getKey() {
       return key_;
     }
-    public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder getKeyOrBuilder() {
+    public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder getKeyOrBuilder() {
       return key_;
     }
     
@@ -12122,7 +10073,7 @@ public final class AdminProtos {
     }
     
     private void initFields() {
-      key_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance();
+      key_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance();
       edit_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.getDefaultInstance();
     }
     private byte memoizedIsInitialized = -1;
@@ -12142,10 +10093,6 @@ public final class AdminProtos {
         memoizedIsInitialized = 0;
         return false;
       }
-      if (!getEdit().isInitialized()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
       memoizedIsInitialized = 1;
       return true;
     }
@@ -12345,7 +10292,7 @@ public final class AdminProtos {
       public Builder clear() {
         super.clear();
         if (keyBuilder_ == null) {
-          key_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance();
+          key_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance();
         } else {
           keyBuilder_.clear();
         }
@@ -12449,10 +10396,6 @@ public final class AdminProtos {
           
           return false;
         }
-        if (!getEdit().isInitialized()) {
-          
-          return false;
-        }
         return true;
       }
       
@@ -12480,7 +10423,7 @@ public final class AdminProtos {
               break;
             }
             case 10: {
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.newBuilder();
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.newBuilder();
               if (hasKey()) {
                 subBuilder.mergeFrom(getKey());
               }
@@ -12503,21 +10446,21 @@ public final class AdminProtos {
       
       private int bitField0_;
       
-      // required .WALEntry.WALKey key = 1;
-      private org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey key_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance();
+      // required .WALKey key = 1;
+      private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey key_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder> keyBuilder_;
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder> keyBuilder_;
       public boolean hasKey() {
         return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey getKey() {
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey getKey() {
         if (keyBuilder_ == null) {
           return key_;
         } else {
           return keyBuilder_.getMessage();
         }
       }
-      public Builder setKey(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey value) {
+      public Builder setKey(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey value) {
         if (keyBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
@@ -12531,7 +10474,7 @@ public final class AdminProtos {
         return this;
       }
       public Builder setKey(
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder builderForValue) {
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder builderForValue) {
         if (keyBuilder_ == null) {
           key_ = builderForValue.build();
           onChanged();
@@ -12541,12 +10484,12 @@ public final class AdminProtos {
         bitField0_ |= 0x00000001;
         return this;
       }
-      public Builder mergeKey(org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey value) {
+      public Builder mergeKey(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey value) {
         if (keyBuilder_ == null) {
           if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              key_ != org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance()) {
+              key_ != org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance()) {
             key_ =
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.newBuilder(key_).mergeFrom(value).buildPartial();
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.newBuilder(key_).mergeFrom(value).buildPartial();
           } else {
             key_ = value;
           }
@@ -12559,7 +10502,7 @@ public final class AdminProtos {
       }
       public Builder clearKey() {
         if (keyBuilder_ == null) {
-          key_ = org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.getDefaultInstance();
+          key_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance();
           onChanged();
         } else {
           keyBuilder_.clear();
@@ -12567,12 +10510,12 @@ public final class AdminProtos {
         bitField0_ = (bitField0_ & ~0x00000001);
         return this;
       }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder getKeyBuilder() {
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder getKeyBuilder() {
         bitField0_ |= 0x00000001;
         onChanged();
         return getKeyFieldBuilder().getBuilder();
       }
-      public org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder getKeyOrBuilder() {
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder getKeyOrBuilder() {
         if (keyBuilder_ != null) {
           return keyBuilder_.getMessageOrBuilder();
         } else {
@@ -12580,11 +10523,11 @@ public final class AdminProtos {
         }
       }
       private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder> 
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder> 
           getKeyFieldBuilder() {
         if (keyBuilder_ == null) {
           keyBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKeyOrBuilder>(
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder>(
                   key_,
                   getParentForChildren(),
                   isClean());
@@ -17425,31 +15368,16 @@ public final class AdminProtos {
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_MergeRegionsResponse_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_UUID_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_UUID_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_WALEntry_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_WALEntry_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_WALEntry_WALKey_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_WALEntry_WALKey_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_WALEntry_WALEdit_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_WALEntry_WALEdit_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_WALEntry_WALEdit_FamilyScope_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_WALEntry_WALEdit_FamilyScope_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_ReplicateWALEntryRequest_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
@@ -17503,88 +15431,78 @@ public final class AdminProtos {
       descriptor;
   static {
     java.lang.String[] descriptorData = {
-      "\n\013Admin.proto\032\013hbase.proto\"Q\n\024GetRegionI" +
-      "nfoRequest\022 \n\006region\030\001 \002(\0132\020.RegionSpeci" +
-      "fier\022\027\n\017compactionState\030\002 \001(\010\"\301\001\n\025GetReg" +
-      "ionInfoResponse\022\037\n\nregionInfo\030\001 \002(\0132\013.Re" +
-      "gionInfo\022?\n\017compactionState\030\002 \001(\0162&.GetR" +
-      "egionInfoResponse.CompactionState\"F\n\017Com" +
-      "pactionState\022\010\n\004NONE\020\000\022\t\n\005MINOR\020\001\022\t\n\005MAJ" +
-      "OR\020\002\022\023\n\017MAJOR_AND_MINOR\020\003\"G\n\023GetStoreFil" +
-      "eRequest\022 \n\006region\030\001 \002(\0132\020.RegionSpecifi" +
-      "er\022\016\n\006family\030\002 \003(\014\")\n\024GetStoreFileRespon",
-      "se\022\021\n\tstoreFile\030\001 \003(\t\"\030\n\026GetOnlineRegion" +
-      "Request\":\n\027GetOnlineRegionResponse\022\037\n\nre" +
-      "gionInfo\030\001 \003(\0132\013.RegionInfo\"\225\001\n\021OpenRegi" +
-      "onRequest\0223\n\010openInfo\030\001 \003(\0132!.OpenRegion" +
-      "Request.RegionOpenInfo\032K\n\016RegionOpenInfo" +
-      "\022\033\n\006region\030\001 \002(\0132\013.RegionInfo\022\034\n\024version" +
-      "OfOfflineNode\030\002 \001(\r\"\234\001\n\022OpenRegionRespon" +
-      "se\022<\n\014openingState\030\001 \003(\0162&.OpenRegionRes" +
-      "ponse.RegionOpeningState\"H\n\022RegionOpenin" +
-      "gState\022\n\n\006OPENED\020\000\022\022\n\016ALREADY_OPENED\020\001\022\022",
-      "\n\016FAILED_OPENING\020\002\"\232\001\n\022CloseRegionReques" +
-      "t\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\034\n\024v" +
-      "ersionOfClosingNode\030\002 \001(\r\022\034\n\016transitionI" +
-      "nZK\030\003 \001(\010:\004true\022&\n\021destinationServer\030\004 \001" +
-      "(\0132\013.ServerName\"%\n\023CloseRegionResponse\022\016" +
-      "\n\006closed\030\001 \002(\010\"M\n\022FlushRegionRequest\022 \n\006" +
-      "region\030\001 \002(\0132\020.RegionSpecifier\022\025\n\rifOlde" +
-      "rThanTs\030\002 \001(\004\"=\n\023FlushRegionResponse\022\025\n\r" +
-      "lastFlushTime\030\001 \002(\004\022\017\n\007flushed\030\002 \001(\010\"J\n\022" +
-      "SplitRegionRequest\022 \n\006region\030\001 \002(\0132\020.Reg",
-      "ionSpecifier\022\022\n\nsplitPoint\030\002 \001(\014\"\025\n\023Spli" +
-      "tRegionResponse\"W\n\024CompactRegionRequest\022" +
-      " \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\r\n\005maj" +
-      "or\030\002 \001(\010\022\016\n\006family\030\003 \001(\014\"\027\n\025CompactRegio" +
-      "nResponse\"t\n\023MergeRegionsRequest\022!\n\007regi" +
-      "onA\030\001 \002(\0132\020.RegionSpecifier\022!\n\007regionB\030\002" +
-      " \002(\0132\020.RegionSpecifier\022\027\n\010forcible\030\003 \001(\010" +
-      ":\005false\"\026\n\024MergeRegionsResponse\"1\n\004UUID\022" +
-      "\024\n\014leastSigBits\030\001 \002(\004\022\023\n\013mostSigBits\030\002 \002" +
-      "(\004\"\270\003\n\010WALEntry\022\035\n\003key\030\001 \002(\0132\020.WALEntry.",
-      "WALKey\022\037\n\004edit\030\002 \002(\0132\021.WALEntry.WALEdit\032" +
-      "~\n\006WALKey\022\031\n\021encodedRegionName\030\001 \002(\014\022\021\n\t" +
-      "tableName\030\002 \002(\014\022\031\n\021logSequenceNumber\030\003 \002" +
-      "(\004\022\021\n\twriteTime\030\004 \002(\004\022\030\n\tclusterId\030\005 \001(\013" +
-      "2\005.UUID\032\353\001\n\007WALEdit\022\025\n\rkeyValueBytes\030\001 \003" +
-      "(\014\0222\n\013familyScope\030\002 \003(\0132\035.WALEntry.WALEd" +
-      "it.FamilyScope\032M\n\013FamilyScope\022\016\n\006family\030" +
-      "\001 \002(\014\022.\n\tscopeType\030\002 \002(\0162\033.WALEntry.WALE" +
-      "dit.ScopeType\"F\n\tScopeType\022\033\n\027REPLICATIO" +
-      "N_SCOPE_LOCAL\020\000\022\034\n\030REPLICATION_SCOPE_GLO",
-      "BAL\020\001\"4\n\030ReplicateWALEntryRequest\022\030\n\005ent" +
-      "ry\030\001 \003(\0132\t.WALEntry\"\033\n\031ReplicateWALEntry" +
-      "Response\"\026\n\024RollWALWriterRequest\".\n\025Roll" +
-      "WALWriterResponse\022\025\n\rregionToFlush\030\001 \003(\014" +
-      "\"#\n\021StopServerRequest\022\016\n\006reason\030\001 \002(\t\"\024\n" +
-      "\022StopServerResponse\"\026\n\024GetServerInfoRequ" +
-      "est\"@\n\nServerInfo\022\037\n\nserverName\030\001 \002(\0132\013." +
-      "ServerName\022\021\n\twebuiPort\030\002 \001(\r\"8\n\025GetServ" +
-      "erInfoResponse\022\037\n\nserverInfo\030\001 \002(\0132\013.Ser" +
-      "verInfo2\266\006\n\014AdminService\022>\n\rgetRegionInf",
-      "o\022\025.GetRegionInfoRequest\032\026.GetRegionInfo" +
-      "Response\022;\n\014getStoreFile\022\024.GetStoreFileR" +
-      "equest\032\025.GetStoreFileResponse\022D\n\017getOnli" +
-      "neRegion\022\027.GetOnlineRegionRequest\032\030.GetO" +
-      "nlineRegionResponse\0225\n\nopenRegion\022\022.Open" +
-      "RegionRequest\032\023.OpenRegionResponse\0228\n\013cl" +
-      "oseRegion\022\023.CloseRegionRequest\032\024.CloseRe" +
-      "gionResponse\0228\n\013flushRegion\022\023.FlushRegio" +
-      "nRequest\032\024.FlushRegionResponse\0228\n\013splitR" +
-      "egion\022\023.SplitRegionRequest\032\024.SplitRegion",
-      "Response\022>\n\rcompactRegion\022\025.CompactRegio" +
-      "nRequest\032\026.CompactRegionResponse\022;\n\014merg" +
-      "eRegions\022\024.MergeRegionsRequest\032\025.MergeRe" +
-      "gionsResponse\022J\n\021replicateWALEntry\022\031.Rep" +
-      "licateWALEntryRequest\032\032.ReplicateWALEntr" +
-      "yResponse\022>\n\rrollWALWriter\022\025.RollWALWrit" +
-      "erRequest\032\026.RollWALWriterResponse\022>\n\rget" +
-      "ServerInfo\022\025.GetServerInfoRequest\032\026.GetS" +
-      "erverInfoResponse\0225\n\nstopServer\022\022.StopSe" +
-      "rverRequest\032\023.StopServerResponseBA\n*org.",
-      "apache.hadoop.hbase.protobuf.generatedB\013" +
-      "AdminProtosH\001\210\001\001\240\001\001"
+      "\n\013Admin.proto\032\013hbase.proto\032\nHLog.proto\"Q" +
+      "\n\024GetRegionInfoRequest\022 \n\006region\030\001 \002(\0132\020" +
+      ".RegionSpecifier\022\027\n\017compactionState\030\002 \001(" +
+      "\010\"\301\001\n\025GetRegionInfoResponse\022\037\n\nregionInf" +
+      "o\030\001 \002(\0132\013.RegionInfo\022?\n\017compactionState\030" +
+      "\002 \001(\0162&.GetRegionInfoResponse.Compaction" +
+      "State\"F\n\017CompactionState\022\010\n\004NONE\020\000\022\t\n\005MI" +
+      "NOR\020\001\022\t\n\005MAJOR\020\002\022\023\n\017MAJOR_AND_MINOR\020\003\"G\n" +
+      "\023GetStoreFileRequest\022 \n\006region\030\001 \002(\0132\020.R" +
+      "egionSpecifier\022\016\n\006family\030\002 \003(\014\")\n\024GetSto",
+      "reFileResponse\022\021\n\tstoreFile\030\001 \003(\t\"\030\n\026Get" +
+      "OnlineRegionRequest\":\n\027GetOnlineRegionRe" +
+      "sponse\022\037\n\nregionInfo\030\001 \003(\0132\013.RegionInfo\"" +
+      "\225\001\n\021OpenRegionRequest\0223\n\010openInfo\030\001 \003(\0132" +
+      "!.OpenRegionRequest.RegionOpenInfo\032K\n\016Re" +
+      "gionOpenInfo\022\033\n\006region\030\001 \002(\0132\013.RegionInf" +
+      "o\022\034\n\024versionOfOfflineNode\030\002 \001(\r\"\234\001\n\022Open" +
+      "RegionResponse\022<\n\014openingState\030\001 \003(\0162&.O" +
+      "penRegionResponse.RegionOpeningState\"H\n\022" +
+      "RegionOpeningState\022\n\n\006OPENED\020\000\022\022\n\016ALREAD",
+      "Y_OPENED\020\001\022\022\n\016FAILED_OPENING\020\002\"\232\001\n\022Close" +
+      "RegionRequest\022 \n\006region\030\001 \002(\0132\020.RegionSp" +
+      "ecifier\022\034\n\024versionOfClosingNode\030\002 \001(\r\022\034\n" +
+      "\016transitionInZK\030\003 \001(\010:\004true\022&\n\021destinati" +
+      "onServer\030\004 \001(\0132\013.ServerName\"%\n\023CloseRegi" +
+      "onResponse\022\016\n\006closed\030\001 \002(\010\"M\n\022FlushRegio" +
+      "nRequest\022 \n\006region\030\001 \002(\0132\020.RegionSpecifi" +
+      "er\022\025\n\rifOlderThanTs\030\002 \001(\004\"=\n\023FlushRegion" +
+      "Response\022\025\n\rlastFlushTime\030\001 \002(\004\022\017\n\007flush" +
+      "ed\030\002 \001(\010\"J\n\022SplitRegionRequest\022 \n\006region",
+      "\030\001 \002(\0132\020.RegionSpecifier\022\022\n\nsplitPoint\030\002" +
+      " \001(\014\"\025\n\023SplitRegionResponse\"W\n\024CompactRe" +
+      "gionRequest\022 \n\006region\030\001 \002(\0132\020.RegionSpec" +
+      "ifier\022\r\n\005major\030\002 \001(\010\022\016\n\006family\030\003 \001(\014\"\027\n\025" +
+      "CompactRegionResponse\"t\n\023MergeRegionsReq" +
+      "uest\022!\n\007regionA\030\001 \002(\0132\020.RegionSpecifier\022" +
+      "!\n\007regionB\030\002 \002(\0132\020.RegionSpecifier\022\027\n\010fo" +
+      "rcible\030\003 \001(\010:\005false\"\026\n\024MergeRegionsRespo" +
+      "nse\"c\n\010WALEntry\022\024\n\003key\030\001 \002(\0132\007.WALKey\022\037\n" +
+      "\004edit\030\002 \002(\0132\021.WALEntry.WALEdit\032 \n\007WALEdi",
+      "t\022\025\n\rkeyValueBytes\030\001 \003(\014\"4\n\030ReplicateWAL" +
+      "EntryRequest\022\030\n\005entry\030\001 \003(\0132\t.WALEntry\"\033" +
+      "\n\031ReplicateWALEntryResponse\"\026\n\024RollWALWr" +
+      "iterRequest\".\n\025RollWALWriterResponse\022\025\n\r" +
+      "regionToFlush\030\001 \003(\014\"#\n\021StopServerRequest" +
+      "\022\016\n\006reason\030\001 \002(\t\"\024\n\022StopServerResponse\"\026" +
+      "\n\024GetServerInfoRequest\"@\n\nServerInfo\022\037\n\n" +
+      "serverName\030\001 \002(\0132\013.ServerName\022\021\n\twebuiPo" +
+      "rt\030\002 \001(\r\"8\n\025GetServerInfoResponse\022\037\n\nser" +
+      "verInfo\030\001 \002(\0132\013.ServerInfo2\266\006\n\014AdminServ",
+      "ice\022>\n\rgetRegionInfo\022\025.GetRegionInfoRequ" +
+      "est\032\026.GetRegionInfoResponse\022;\n\014getStoreF" +
+      "ile\022\024.GetStoreFileRequest\032\025.GetStoreFile" +
+      "Response\022D\n\017getOnlineRegion\022\027.GetOnlineR" +
+      "egionRequest\032\030.GetOnlineRegionResponse\0225" +
+      "\n\nopenRegion\022\022.OpenRegionRequest\032\023.OpenR" +
+      "egionResponse\0228\n\013closeRegion\022\023.CloseRegi" +
+      "onRequest\032\024.CloseRegionResponse\0228\n\013flush" +
+      "Region\022\023.FlushRegionRequest\032\024.FlushRegio" +
+      "nResponse\0228\n\013splitRegion\022\023.SplitRegionRe",
+      "quest\032\024.SplitRegionResponse\022>\n\rcompactRe" +
+      "gion\022\025.CompactRegionRequest\032\026.CompactReg" +
+      "ionResponse\022;\n\014mergeRegions\022\024.MergeRegio" +
+      "nsRequest\032\025.MergeRegionsResponse\022J\n\021repl" +
+      "icateWALEntry\022\031.ReplicateWALEntryRequest" +
+      "\032\032.ReplicateWALEntryResponse\022>\n\rrollWALW" +
+      "riter\022\025.RollWALWriterRequest\032\026.RollWALWr" +
+      "iterResponse\022>\n\rgetServerInfo\022\025.GetServe" +
+      "rInfoRequest\032\026.GetServerInfoResponse\0225\n\n" +
+      "stopServer\022\022.StopServerRequest\032\023.StopSer",
+      "verResponseBA\n*org.apache.hadoop.hbase.p" +
+      "rotobuf.generatedB\013AdminProtosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -17743,48 +15661,24 @@ public final class AdminProtos {
               new java.lang.String[] { },
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsResponse.Builder.class);
-          internal_static_UUID_descriptor =
-            getDescriptor().getMessageTypes().get(18);
-          internal_static_UUID_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UUID_descriptor,
-              new java.lang.String[] { "LeastSigBits", "MostSigBits", },
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.class,
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UUID.Builder.class);
           internal_static_WALEntry_descriptor =
-            getDescriptor().getMessageTypes().get(19);
+            getDescriptor().getMessageTypes().get(18);
           internal_static_WALEntry_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_WALEntry_descriptor,
               new java.lang.String[] { "Key", "Edit", },
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.Builder.class);
-          internal_static_WALEntry_WALKey_descriptor =
-            internal_static_WALEntry_descriptor.getNestedTypes().get(0);
-          internal_static_WALEntry_WALKey_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_WALEntry_WALKey_descriptor,
-              new java.lang.String[] { "EncodedRegionName", "TableName", "LogSequenceNumber", "WriteTime", "ClusterId", },
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.class,
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALKey.Builder.class);
           internal_static_WALEntry_WALEdit_descriptor =
-            internal_static_WALEntry_descriptor.getNestedTypes().get(1);
+            internal_static_WALEntry_descriptor.getNestedTypes().get(0);
           internal_static_WALEntry_WALEdit_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_WALEntry_WALEdit_descriptor,
-              new java.lang.String[] { "KeyValueBytes", "FamilyScope", },
+              new java.lang.String[] { "KeyValueBytes", },
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.Builder.class);
-          internal_static_WALEntry_WALEdit_FamilyScope_descriptor =
-            internal_static_WALEntry_WALEdit_descriptor.getNestedTypes().get(0);
-          internal_static_WALEntry_WALEdit_FamilyScope_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_WALEntry_WALEdit_FamilyScope_descriptor,
-              new java.lang.String[] { "Family", "ScopeType", },
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.class,
-              org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry.WALEdit.FamilyScope.Builder.class);
           internal_static_ReplicateWALEntryRequest_descriptor =
-            getDescriptor().getMessageTypes().get(20);
+            getDescriptor().getMessageTypes().get(19);
           internal_static_ReplicateWALEntryRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ReplicateWALEntryRequest_descriptor,
@@ -17792,7 +15686,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryRequest.Builder.class);
           internal_static_ReplicateWALEntryResponse_descriptor =
-            getDescriptor().getMessageTypes().get(21);
+            getDescriptor().getMessageTypes().get(20);
           internal_static_ReplicateWALEntryResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ReplicateWALEntryResponse_descriptor,
@@ -17800,7 +15694,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryResponse.Builder.class);
           internal_static_RollWALWriterRequest_descriptor =
-            getDescriptor().getMessageTypes().get(22);
+            getDescriptor().getMessageTypes().get(21);
           internal_static_RollWALWriterRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_RollWALWriterRequest_descriptor,
@@ -17808,7 +15702,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterRequest.Builder.class);
           internal_static_RollWALWriterResponse_descriptor =
-            getDescriptor().getMessageTypes().get(23);
+            getDescriptor().getMessageTypes().get(22);
           internal_static_RollWALWriterResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_RollWALWriterResponse_descriptor,
@@ -17816,7 +15710,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterResponse.Builder.class);
           internal_static_StopServerRequest_descriptor =
-            getDescriptor().getMessageTypes().get(24);
+            getDescriptor().getMessageTypes().get(23);
           internal_static_StopServerRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_StopServerRequest_descriptor,
@@ -17824,7 +15718,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerRequest.Builder.class);
           internal_static_StopServerResponse_descriptor =
-            getDescriptor().getMessageTypes().get(25);
+            getDescriptor().getMessageTypes().get(24);
           internal_static_StopServerResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_StopServerResponse_descriptor,
@@ -17832,7 +15726,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerResponse.Builder.class);
           internal_static_GetServerInfoRequest_descriptor =
-            getDescriptor().getMessageTypes().get(26);
+            getDescriptor().getMessageTypes().get(25);
           internal_static_GetServerInfoRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_GetServerInfoRequest_descriptor,
@@ -17840,7 +15734,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoRequest.Builder.class);
           internal_static_ServerInfo_descriptor =
-            getDescriptor().getMessageTypes().get(27);
+            getDescriptor().getMessageTypes().get(26);
           internal_static_ServerInfo_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ServerInfo_descriptor,
@@ -17848,7 +15742,7 @@ public final class AdminProtos {
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo.Builder.class);
           internal_static_GetServerInfoResponse_descriptor =
-            getDescriptor().getMessageTypes().get(28);
+            getDescriptor().getMessageTypes().get(27);
           internal_static_GetServerInfoResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_GetServerInfoResponse_descriptor,
@@ -17862,6 +15756,7 @@ public final class AdminProtos {
       .internalBuildGeneratedFileFrom(descriptorData,
         new com.google.protobuf.Descriptors.FileDescriptor[] {
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.getDescriptor(),
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.getDescriptor(),
         }, assigner);
   }
   
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
index 23a4f4f..d25caef 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HBaseProtos.java
@@ -13545,6 +13545,459 @@ public final class HBaseProtos {
     // @@protoc_insertion_point(class_scope:BigDecimalMsg)
   }
   
+  public interface UUIDOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required uint64 leastSigBits = 1;
+    boolean hasLeastSigBits();
+    long getLeastSigBits();
+    
+    // required uint64 mostSigBits = 2;
+    boolean hasMostSigBits();
+    long getMostSigBits();
+  }
+  public static final class UUID extends
+      com.google.protobuf.GeneratedMessage
+      implements UUIDOrBuilder {
+    // Use UUID.newBuilder() to construct.
+    private UUID(Builder builder) {
+      super(builder);
+    }
+    private UUID(boolean noInit) {}
+    
+    private static final UUID defaultInstance;
+    public static UUID getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public UUID getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.internal_static_UUID_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.internal_static_UUID_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required uint64 leastSigBits = 1;
+    public static final int LEASTSIGBITS_FIELD_NUMBER = 1;
+    private long leastSigBits_;
+    public boolean hasLeastSigBits() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public long getLeastSigBits() {
+      return leastSigBits_;
+    }
+    
+    // required uint64 mostSigBits = 2;
+    public static final int MOSTSIGBITS_FIELD_NUMBER = 2;
+    private long mostSigBits_;
+    public boolean hasMostSigBits() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    public long getMostSigBits() {
+      return mostSigBits_;
+    }
+    
+    private void initFields() {
+      leastSigBits_ = 0L;
+      mostSigBits_ = 0L;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasLeastSigBits()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasMostSigBits()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeUInt64(1, leastSigBits_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeUInt64(2, mostSigBits_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(1, leastSigBits_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(2, mostSigBits_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID other = (org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID) obj;
+      
+      boolean result = true;
+      result = result && (hasLeastSigBits() == other.hasLeastSigBits());
+      if (hasLeastSigBits()) {
+        result = result && (getLeastSigBits()
+            == other.getLeastSigBits());
+      }
+      result = result && (hasMostSigBits() == other.hasMostSigBits());
+      if (hasMostSigBits()) {
+        result = result && (getMostSigBits()
+            == other.getMostSigBits());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasLeastSigBits()) {
+        hash = (37 * hash) + LEASTSIGBITS_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getLeastSigBits());
+      }
+      if (hasMostSigBits()) {
+        hash = (37 * hash) + MOSTSIGBITS_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getMostSigBits());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.internal_static_UUID_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.internal_static_UUID_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        leastSigBits_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        mostSigBits_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID build() {
+        org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID result = new org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.leastSigBits_ = leastSigBits_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.mostSigBits_ = mostSigBits_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance()) return this;
+        if (other.hasLeastSigBits()) {
+          setLeastSigBits(other.getLeastSigBits());
+        }
+        if (other.hasMostSigBits()) {
+          setMostSigBits(other.getMostSigBits());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasLeastSigBits()) {
+          
+          return false;
+        }
+        if (!hasMostSigBits()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              leastSigBits_ = input.readUInt64();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              mostSigBits_ = input.readUInt64();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required uint64 leastSigBits = 1;
+      private long leastSigBits_ ;
+      public boolean hasLeastSigBits() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public long getLeastSigBits() {
+        return leastSigBits_;
+      }
+      public Builder setLeastSigBits(long value) {
+        bitField0_ |= 0x00000001;
+        leastSigBits_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearLeastSigBits() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        leastSigBits_ = 0L;
+        onChanged();
+        return this;
+      }
+      
+      // required uint64 mostSigBits = 2;
+      private long mostSigBits_ ;
+      public boolean hasMostSigBits() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      public long getMostSigBits() {
+        return mostSigBits_;
+      }
+      public Builder setMostSigBits(long value) {
+        bitField0_ |= 0x00000002;
+        mostSigBits_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearMostSigBits() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        mostSigBits_ = 0L;
+        onChanged();
+        return this;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:UUID)
+    }
+    
+    static {
+      defaultInstance = new UUID(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:UUID)
+  }
+  
   private static com.google.protobuf.Descriptors.Descriptor
     internal_static_Cell_descriptor;
   private static
@@ -13645,6 +14098,11 @@ public final class HBaseProtos {
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_BigDecimalMsg_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_UUID_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_UUID_fieldAccessorTable;
   
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
@@ -13706,14 +14164,15 @@ public final class HBaseProtos {
       "n.Type:\005FLUSH\022\017\n\007version\030\005 \001(\005\"\037\n\004Type\022\014",
       "\n\010DISABLED\020\000\022\t\n\005FLUSH\020\001\"\n\n\010EmptyMsg\"\032\n\007L" +
       "ongMsg\022\017\n\007longMsg\030\001 \002(\003\"&\n\rBigDecimalMsg" +
-      "\022\025\n\rbigdecimalMsg\030\001 \002(\014*`\n\010CellType\022\013\n\007M" +
-      "INIMUM\020\000\022\007\n\003PUT\020\004\022\n\n\006DELETE\020\010\022\021\n\rDELETE_" +
-      "COLUMN\020\014\022\021\n\rDELETE_FAMILY\020\016\022\014\n\007MAXIMUM\020\377" +
-      "\001*r\n\013CompareType\022\010\n\004LESS\020\000\022\021\n\rLESS_OR_EQ" +
-      "UAL\020\001\022\t\n\005EQUAL\020\002\022\r\n\tNOT_EQUAL\020\003\022\024\n\020GREAT" +
-      "ER_OR_EQUAL\020\004\022\013\n\007GREATER\020\005\022\t\n\005NO_OP\020\006B>\n" +
-      "*org.apache.hadoop.hbase.protobuf.genera" +
-      "tedB\013HBaseProtosH\001\240\001\001"
+      "\022\025\n\rbigdecimalMsg\030\001 \002(\014\"1\n\004UUID\022\024\n\014least" +
+      "SigBits\030\001 \002(\004\022\023\n\013mostSigBits\030\002 \002(\004*`\n\010Ce" +
+      "llType\022\013\n\007MINIMUM\020\000\022\007\n\003PUT\020\004\022\n\n\006DELETE\020\010" +
+      "\022\021\n\rDELETE_COLUMN\020\014\022\021\n\rDELETE_FAMILY\020\016\022\014" +
+      "\n\007MAXIMUM\020\377\001*r\n\013CompareType\022\010\n\004LESS\020\000\022\021\n" +
+      "\rLESS_OR_EQUAL\020\001\022\t\n\005EQUAL\020\002\022\r\n\tNOT_EQUAL" +
+      "\020\003\022\024\n\020GREATER_OR_EQUAL\020\004\022\013\n\007GREATER\020\005\022\t\n" +
+      "\005NO_OP\020\006B>\n*org.apache.hadoop.hbase.prot",
+      "obuf.generatedB\013HBaseProtosH\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -13880,6 +14339,14 @@ public final class HBaseProtos {
               new java.lang.String[] { "BigdecimalMsg", },
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.BigDecimalMsg.class,
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.BigDecimalMsg.Builder.class);
+          internal_static_UUID_descriptor =
+            getDescriptor().getMessageTypes().get(20);
+          internal_static_UUID_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_UUID_descriptor,
+              new java.lang.String[] { "LeastSigBits", "MostSigBits", },
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.class,
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder.class);
           return null;
         }
       };
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HLogProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HLogProtos.java
new file mode 100644
index 0000000..129af5e
--- /dev/null
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/HLogProtos.java
@@ -0,0 +1,2129 @@
+// Generated by the protocol buffer compiler.  DO NOT EDIT!
+// source: HLog.proto
+
+package org.apache.hadoop.hbase.protobuf.generated;
+
+public final class HLogProtos {
+  private HLogProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistry registry) {
+  }
+  public enum ScopeType
+      implements com.google.protobuf.ProtocolMessageEnum {
+    REPLICATION_SCOPE_LOCAL(0, 0),
+    REPLICATION_SCOPE_GLOBAL(1, 1),
+    ;
+    
+    public static final int REPLICATION_SCOPE_LOCAL_VALUE = 0;
+    public static final int REPLICATION_SCOPE_GLOBAL_VALUE = 1;
+    
+    
+    public final int getNumber() { return value; }
+    
+    public static ScopeType valueOf(int value) {
+      switch (value) {
+        case 0: return REPLICATION_SCOPE_LOCAL;
+        case 1: return REPLICATION_SCOPE_GLOBAL;
+        default: return null;
+      }
+    }
+    
+    public static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
+        internalGetValueMap() {
+      return internalValueMap;
+    }
+    private static com.google.protobuf.Internal.EnumLiteMap<ScopeType>
+        internalValueMap =
+          new com.google.protobuf.Internal.EnumLiteMap<ScopeType>() {
+            public ScopeType findValueByNumber(int number) {
+              return ScopeType.valueOf(number);
+            }
+          };
+    
+    public final com.google.protobuf.Descriptors.EnumValueDescriptor
+        getValueDescriptor() {
+      return getDescriptor().getValues().get(index);
+    }
+    public final com.google.protobuf.Descriptors.EnumDescriptor
+        getDescriptorForType() {
+      return getDescriptor();
+    }
+    public static final com.google.protobuf.Descriptors.EnumDescriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.getDescriptor().getEnumTypes().get(0);
+    }
+    
+    private static final ScopeType[] VALUES = {
+      REPLICATION_SCOPE_LOCAL, REPLICATION_SCOPE_GLOBAL, 
+    };
+    
+    public static ScopeType valueOf(
+        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
+      if (desc.getType() != getDescriptor()) {
+        throw new java.lang.IllegalArgumentException(
+          "EnumValueDescriptor is not for this type.");
+      }
+      return VALUES[desc.getIndex()];
+    }
+    
+    private final int index;
+    private final int value;
+    
+    private ScopeType(int index, int value) {
+      this.index = index;
+      this.value = value;
+    }
+    
+    // @@protoc_insertion_point(enum_scope:ScopeType)
+  }
+  
+  public interface WALHeaderOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required bool hasCompression = 1;
+    boolean hasHasCompression();
+    boolean getHasCompression();
+  }
+  public static final class WALHeader extends
+      com.google.protobuf.GeneratedMessage
+      implements WALHeaderOrBuilder {
+    // Use WALHeader.newBuilder() to construct.
+    private WALHeader(Builder builder) {
+      super(builder);
+    }
+    private WALHeader(boolean noInit) {}
+    
+    private static final WALHeader defaultInstance;
+    public static WALHeader getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public WALHeader getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALHeader_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALHeader_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required bool hasCompression = 1;
+    public static final int HASCOMPRESSION_FIELD_NUMBER = 1;
+    private boolean hasCompression_;
+    public boolean hasHasCompression() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public boolean getHasCompression() {
+      return hasCompression_;
+    }
+    
+    private void initFields() {
+      hasCompression_ = false;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasHasCompression()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBool(1, hasCompression_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBoolSize(1, hasCompression_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader other = (org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader) obj;
+      
+      boolean result = true;
+      result = result && (hasHasCompression() == other.hasHasCompression());
+      if (hasHasCompression()) {
+        result = result && (getHasCompression()
+            == other.getHasCompression());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasHasCompression()) {
+        hash = (37 * hash) + HASCOMPRESSION_FIELD_NUMBER;
+        hash = (53 * hash) + hashBoolean(getHasCompression());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeaderOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALHeader_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALHeader_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        hasCompression_ = false;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader build() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader result = new org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.hasCompression_ = hasCompression_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.getDefaultInstance()) return this;
+        if (other.hasHasCompression()) {
+          setHasCompression(other.getHasCompression());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasHasCompression()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              hasCompression_ = input.readBool();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required bool hasCompression = 1;
+      private boolean hasCompression_ ;
+      public boolean hasHasCompression() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public boolean getHasCompression() {
+        return hasCompression_;
+      }
+      public Builder setHasCompression(boolean value) {
+        bitField0_ |= 0x00000001;
+        hasCompression_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearHasCompression() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        hasCompression_ = false;
+        onChanged();
+        return this;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:WALHeader)
+    }
+    
+    static {
+      defaultInstance = new WALHeader(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:WALHeader)
+  }
+  
+  public interface WALKeyOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required bytes encodedRegionName = 1;
+    boolean hasEncodedRegionName();
+    com.google.protobuf.ByteString getEncodedRegionName();
+    
+    // required bytes tableName = 2;
+    boolean hasTableName();
+    com.google.protobuf.ByteString getTableName();
+    
+    // required uint64 logSequenceNumber = 3;
+    boolean hasLogSequenceNumber();
+    long getLogSequenceNumber();
+    
+    // required uint64 writeTime = 4;
+    boolean hasWriteTime();
+    long getWriteTime();
+    
+    // optional .UUID clusterId = 5;
+    boolean hasClusterId();
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId();
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder();
+    
+    // repeated .FamilyScope scopes = 6;
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> 
+        getScopesList();
+    org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope getScopes(int index);
+    int getScopesCount();
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder> 
+        getScopesOrBuilderList();
+    org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder getScopesOrBuilder(
+        int index);
+    
+    // optional uint32 followingKvCount = 7;
+    boolean hasFollowingKvCount();
+    int getFollowingKvCount();
+  }
+  public static final class WALKey extends
+      com.google.protobuf.GeneratedMessage
+      implements WALKeyOrBuilder {
+    // Use WALKey.newBuilder() to construct.
+    private WALKey(Builder builder) {
+      super(builder);
+    }
+    private WALKey(boolean noInit) {}
+    
+    private static final WALKey defaultInstance;
+    public static WALKey getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public WALKey getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALKey_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALKey_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required bytes encodedRegionName = 1;
+    public static final int ENCODEDREGIONNAME_FIELD_NUMBER = 1;
+    private com.google.protobuf.ByteString encodedRegionName_;
+    public boolean hasEncodedRegionName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public com.google.protobuf.ByteString getEncodedRegionName() {
+      return encodedRegionName_;
+    }
+    
+    // required bytes tableName = 2;
+    public static final int TABLENAME_FIELD_NUMBER = 2;
+    private com.google.protobuf.ByteString tableName_;
+    public boolean hasTableName() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    public com.google.protobuf.ByteString getTableName() {
+      return tableName_;
+    }
+    
+    // required uint64 logSequenceNumber = 3;
+    public static final int LOGSEQUENCENUMBER_FIELD_NUMBER = 3;
+    private long logSequenceNumber_;
+    public boolean hasLogSequenceNumber() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    public long getLogSequenceNumber() {
+      return logSequenceNumber_;
+    }
+    
+    // required uint64 writeTime = 4;
+    public static final int WRITETIME_FIELD_NUMBER = 4;
+    private long writeTime_;
+    public boolean hasWriteTime() {
+      return ((bitField0_ & 0x00000008) == 0x00000008);
+    }
+    public long getWriteTime() {
+      return writeTime_;
+    }
+    
+    // optional .UUID clusterId = 5;
+    public static final int CLUSTERID_FIELD_NUMBER = 5;
+    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_;
+    public boolean hasClusterId() {
+      return ((bitField0_ & 0x00000010) == 0x00000010);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
+      return clusterId_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
+      return clusterId_;
+    }
+    
+    // repeated .FamilyScope scopes = 6;
+    public static final int SCOPES_FIELD_NUMBER = 6;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> scopes_;
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> getScopesList() {
+      return scopes_;
+    }
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder> 
+        getScopesOrBuilderList() {
+      return scopes_;
+    }
+    public int getScopesCount() {
+      return scopes_.size();
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope getScopes(int index) {
+      return scopes_.get(index);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder getScopesOrBuilder(
+        int index) {
+      return scopes_.get(index);
+    }
+    
+    // optional uint32 followingKvCount = 7;
+    public static final int FOLLOWINGKVCOUNT_FIELD_NUMBER = 7;
+    private int followingKvCount_;
+    public boolean hasFollowingKvCount() {
+      return ((bitField0_ & 0x00000020) == 0x00000020);
+    }
+    public int getFollowingKvCount() {
+      return followingKvCount_;
+    }
+    
+    private void initFields() {
+      encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
+      tableName_ = com.google.protobuf.ByteString.EMPTY;
+      logSequenceNumber_ = 0L;
+      writeTime_ = 0L;
+      clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
+      scopes_ = java.util.Collections.emptyList();
+      followingKvCount_ = 0;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasEncodedRegionName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasTableName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasLogSequenceNumber()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasWriteTime()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (hasClusterId()) {
+        if (!getClusterId().isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      for (int i = 0; i < getScopesCount(); i++) {
+        if (!getScopes(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, encodedRegionName_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeBytes(2, tableName_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeUInt64(3, logSequenceNumber_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        output.writeUInt64(4, writeTime_);
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        output.writeMessage(5, clusterId_);
+      }
+      for (int i = 0; i < scopes_.size(); i++) {
+        output.writeMessage(6, scopes_.get(i));
+      }
+      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+        output.writeUInt32(7, followingKvCount_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, encodedRegionName_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(2, tableName_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(3, logSequenceNumber_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(4, writeTime_);
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(5, clusterId_);
+      }
+      for (int i = 0; i < scopes_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(6, scopes_.get(i));
+      }
+      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(7, followingKvCount_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey other = (org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey) obj;
+      
+      boolean result = true;
+      result = result && (hasEncodedRegionName() == other.hasEncodedRegionName());
+      if (hasEncodedRegionName()) {
+        result = result && getEncodedRegionName()
+            .equals(other.getEncodedRegionName());
+      }
+      result = result && (hasTableName() == other.hasTableName());
+      if (hasTableName()) {
+        result = result && getTableName()
+            .equals(other.getTableName());
+      }
+      result = result && (hasLogSequenceNumber() == other.hasLogSequenceNumber());
+      if (hasLogSequenceNumber()) {
+        result = result && (getLogSequenceNumber()
+            == other.getLogSequenceNumber());
+      }
+      result = result && (hasWriteTime() == other.hasWriteTime());
+      if (hasWriteTime()) {
+        result = result && (getWriteTime()
+            == other.getWriteTime());
+      }
+      result = result && (hasClusterId() == other.hasClusterId());
+      if (hasClusterId()) {
+        result = result && getClusterId()
+            .equals(other.getClusterId());
+      }
+      result = result && getScopesList()
+          .equals(other.getScopesList());
+      result = result && (hasFollowingKvCount() == other.hasFollowingKvCount());
+      if (hasFollowingKvCount()) {
+        result = result && (getFollowingKvCount()
+            == other.getFollowingKvCount());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasEncodedRegionName()) {
+        hash = (37 * hash) + ENCODEDREGIONNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getEncodedRegionName().hashCode();
+      }
+      if (hasTableName()) {
+        hash = (37 * hash) + TABLENAME_FIELD_NUMBER;
+        hash = (53 * hash) + getTableName().hashCode();
+      }
+      if (hasLogSequenceNumber()) {
+        hash = (37 * hash) + LOGSEQUENCENUMBER_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getLogSequenceNumber());
+      }
+      if (hasWriteTime()) {
+        hash = (37 * hash) + WRITETIME_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getWriteTime());
+      }
+      if (hasClusterId()) {
+        hash = (37 * hash) + CLUSTERID_FIELD_NUMBER;
+        hash = (53 * hash) + getClusterId().hashCode();
+      }
+      if (getScopesCount() > 0) {
+        hash = (37 * hash) + SCOPES_FIELD_NUMBER;
+        hash = (53 * hash) + getScopesList().hashCode();
+      }
+      if (hasFollowingKvCount()) {
+        hash = (37 * hash) + FOLLOWINGKVCOUNT_FIELD_NUMBER;
+        hash = (53 * hash) + getFollowingKvCount();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKeyOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALKey_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_WALKey_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getClusterIdFieldBuilder();
+          getScopesFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        tableName_ = com.google.protobuf.ByteString.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        logSequenceNumber_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000004);
+        writeTime_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000008);
+        if (clusterIdBuilder_ == null) {
+          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
+        } else {
+          clusterIdBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000010);
+        if (scopesBuilder_ == null) {
+          scopes_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000020);
+        } else {
+          scopesBuilder_.clear();
+        }
+        followingKvCount_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000040);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey build() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey result = new org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.encodedRegionName_ = encodedRegionName_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.tableName_ = tableName_;
+        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+          to_bitField0_ |= 0x00000004;
+        }
+        result.logSequenceNumber_ = logSequenceNumber_;
+        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+          to_bitField0_ |= 0x00000008;
+        }
+        result.writeTime_ = writeTime_;
+        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+          to_bitField0_ |= 0x00000010;
+        }
+        if (clusterIdBuilder_ == null) {
+          result.clusterId_ = clusterId_;
+        } else {
+          result.clusterId_ = clusterIdBuilder_.build();
+        }
+        if (scopesBuilder_ == null) {
+          if (((bitField0_ & 0x00000020) == 0x00000020)) {
+            scopes_ = java.util.Collections.unmodifiableList(scopes_);
+            bitField0_ = (bitField0_ & ~0x00000020);
+          }
+          result.scopes_ = scopes_;
+        } else {
+          result.scopes_ = scopesBuilder_.build();
+        }
+        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
+          to_bitField0_ |= 0x00000020;
+        }
+        result.followingKvCount_ = followingKvCount_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.getDefaultInstance()) return this;
+        if (other.hasEncodedRegionName()) {
+          setEncodedRegionName(other.getEncodedRegionName());
+        }
+        if (other.hasTableName()) {
+          setTableName(other.getTableName());
+        }
+        if (other.hasLogSequenceNumber()) {
+          setLogSequenceNumber(other.getLogSequenceNumber());
+        }
+        if (other.hasWriteTime()) {
+          setWriteTime(other.getWriteTime());
+        }
+        if (other.hasClusterId()) {
+          mergeClusterId(other.getClusterId());
+        }
+        if (scopesBuilder_ == null) {
+          if (!other.scopes_.isEmpty()) {
+            if (scopes_.isEmpty()) {
+              scopes_ = other.scopes_;
+              bitField0_ = (bitField0_ & ~0x00000020);
+            } else {
+              ensureScopesIsMutable();
+              scopes_.addAll(other.scopes_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.scopes_.isEmpty()) {
+            if (scopesBuilder_.isEmpty()) {
+              scopesBuilder_.dispose();
+              scopesBuilder_ = null;
+              scopes_ = other.scopes_;
+              bitField0_ = (bitField0_ & ~0x00000020);
+              scopesBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getScopesFieldBuilder() : null;
+            } else {
+              scopesBuilder_.addAllMessages(other.scopes_);
+            }
+          }
+        }
+        if (other.hasFollowingKvCount()) {
+          setFollowingKvCount(other.getFollowingKvCount());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasEncodedRegionName()) {
+          
+          return false;
+        }
+        if (!hasTableName()) {
+          
+          return false;
+        }
+        if (!hasLogSequenceNumber()) {
+          
+          return false;
+        }
+        if (!hasWriteTime()) {
+          
+          return false;
+        }
+        if (hasClusterId()) {
+          if (!getClusterId().isInitialized()) {
+            
+            return false;
+          }
+        }
+        for (int i = 0; i < getScopesCount(); i++) {
+          if (!getScopes(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              encodedRegionName_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              bitField0_ |= 0x00000002;
+              tableName_ = input.readBytes();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              logSequenceNumber_ = input.readUInt64();
+              break;
+            }
+            case 32: {
+              bitField0_ |= 0x00000008;
+              writeTime_ = input.readUInt64();
+              break;
+            }
+            case 42: {
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.newBuilder();
+              if (hasClusterId()) {
+                subBuilder.mergeFrom(getClusterId());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setClusterId(subBuilder.buildPartial());
+              break;
+            }
+            case 50: {
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.newBuilder();
+              input.readMessage(subBuilder, extensionRegistry);
+              addScopes(subBuilder.buildPartial());
+              break;
+            }
+            case 56: {
+              bitField0_ |= 0x00000040;
+              followingKvCount_ = input.readUInt32();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required bytes encodedRegionName = 1;
+      private com.google.protobuf.ByteString encodedRegionName_ = com.google.protobuf.ByteString.EMPTY;
+      public boolean hasEncodedRegionName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public com.google.protobuf.ByteString getEncodedRegionName() {
+        return encodedRegionName_;
+      }
+      public Builder setEncodedRegionName(com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        encodedRegionName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearEncodedRegionName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        encodedRegionName_ = getDefaultInstance().getEncodedRegionName();
+        onChanged();
+        return this;
+      }
+      
+      // required bytes tableName = 2;
+      private com.google.protobuf.ByteString tableName_ = com.google.protobuf.ByteString.EMPTY;
+      public boolean hasTableName() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      public com.google.protobuf.ByteString getTableName() {
+        return tableName_;
+      }
+      public Builder setTableName(com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
+        tableName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearTableName() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        tableName_ = getDefaultInstance().getTableName();
+        onChanged();
+        return this;
+      }
+      
+      // required uint64 logSequenceNumber = 3;
+      private long logSequenceNumber_ ;
+      public boolean hasLogSequenceNumber() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      public long getLogSequenceNumber() {
+        return logSequenceNumber_;
+      }
+      public Builder setLogSequenceNumber(long value) {
+        bitField0_ |= 0x00000004;
+        logSequenceNumber_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearLogSequenceNumber() {
+        bitField0_ = (bitField0_ & ~0x00000004);
+        logSequenceNumber_ = 0L;
+        onChanged();
+        return this;
+      }
+      
+      // required uint64 writeTime = 4;
+      private long writeTime_ ;
+      public boolean hasWriteTime() {
+        return ((bitField0_ & 0x00000008) == 0x00000008);
+      }
+      public long getWriteTime() {
+        return writeTime_;
+      }
+      public Builder setWriteTime(long value) {
+        bitField0_ |= 0x00000008;
+        writeTime_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearWriteTime() {
+        bitField0_ = (bitField0_ & ~0x00000008);
+        writeTime_ = 0L;
+        onChanged();
+        return this;
+      }
+      
+      // optional .UUID clusterId = 5;
+      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> clusterIdBuilder_;
+      public boolean hasClusterId() {
+        return ((bitField0_ & 0x00000010) == 0x00000010);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID getClusterId() {
+        if (clusterIdBuilder_ == null) {
+          return clusterId_;
+        } else {
+          return clusterIdBuilder_.getMessage();
+        }
+      }
+      public Builder setClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
+        if (clusterIdBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          clusterId_ = value;
+          onChanged();
+        } else {
+          clusterIdBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000010;
+        return this;
+      }
+      public Builder setClusterId(
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder builderForValue) {
+        if (clusterIdBuilder_ == null) {
+          clusterId_ = builderForValue.build();
+          onChanged();
+        } else {
+          clusterIdBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000010;
+        return this;
+      }
+      public Builder mergeClusterId(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID value) {
+        if (clusterIdBuilder_ == null) {
+          if (((bitField0_ & 0x00000010) == 0x00000010) &&
+              clusterId_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance()) {
+            clusterId_ =
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.newBuilder(clusterId_).mergeFrom(value).buildPartial();
+          } else {
+            clusterId_ = value;
+          }
+          onChanged();
+        } else {
+          clusterIdBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000010;
+        return this;
+      }
+      public Builder clearClusterId() {
+        if (clusterIdBuilder_ == null) {
+          clusterId_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.getDefaultInstance();
+          onChanged();
+        } else {
+          clusterIdBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000010);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder getClusterIdBuilder() {
+        bitField0_ |= 0x00000010;
+        onChanged();
+        return getClusterIdFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder getClusterIdOrBuilder() {
+        if (clusterIdBuilder_ != null) {
+          return clusterIdBuilder_.getMessageOrBuilder();
+        } else {
+          return clusterId_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder> 
+          getClusterIdFieldBuilder() {
+        if (clusterIdBuilder_ == null) {
+          clusterIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUID.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.UUIDOrBuilder>(
+                  clusterId_,
+                  getParentForChildren(),
+                  isClean());
+          clusterId_ = null;
+        }
+        return clusterIdBuilder_;
+      }
+      
+      // repeated .FamilyScope scopes = 6;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> scopes_ =
+        java.util.Collections.emptyList();
+      private void ensureScopesIsMutable() {
+        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
+          scopes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope>(scopes_);
+          bitField0_ |= 0x00000020;
+         }
+      }
+      
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder> scopesBuilder_;
+      
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> getScopesList() {
+        if (scopesBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(scopes_);
+        } else {
+          return scopesBuilder_.getMessageList();
+        }
+      }
+      public int getScopesCount() {
+        if (scopesBuilder_ == null) {
+          return scopes_.size();
+        } else {
+          return scopesBuilder_.getCount();
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope getScopes(int index) {
+        if (scopesBuilder_ == null) {
+          return scopes_.get(index);
+        } else {
+          return scopesBuilder_.getMessage(index);
+        }
+      }
+      public Builder setScopes(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope value) {
+        if (scopesBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureScopesIsMutable();
+          scopes_.set(index, value);
+          onChanged();
+        } else {
+          scopesBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      public Builder setScopes(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder builderForValue) {
+        if (scopesBuilder_ == null) {
+          ensureScopesIsMutable();
+          scopes_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          scopesBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addScopes(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope value) {
+        if (scopesBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureScopesIsMutable();
+          scopes_.add(value);
+          onChanged();
+        } else {
+          scopesBuilder_.addMessage(value);
+        }
+        return this;
+      }
+      public Builder addScopes(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope value) {
+        if (scopesBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureScopesIsMutable();
+          scopes_.add(index, value);
+          onChanged();
+        } else {
+          scopesBuilder_.addMessage(index, value);
+        }
+        return this;
+      }
+      public Builder addScopes(
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder builderForValue) {
+        if (scopesBuilder_ == null) {
+          ensureScopesIsMutable();
+          scopes_.add(builderForValue.build());
+          onChanged();
+        } else {
+          scopesBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addScopes(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder builderForValue) {
+        if (scopesBuilder_ == null) {
+          ensureScopesIsMutable();
+          scopes_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          scopesBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addAllScopes(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope> values) {
+        if (scopesBuilder_ == null) {
+          ensureScopesIsMutable();
+          super.addAll(values, scopes_);
+          onChanged();
+        } else {
+          scopesBuilder_.addAllMessages(values);
+        }
+        return this;
+      }
+      public Builder clearScopes() {
+        if (scopesBuilder_ == null) {
+          scopes_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000020);
+          onChanged();
+        } else {
+          scopesBuilder_.clear();
+        }
+        return this;
+      }
+      public Builder removeScopes(int index) {
+        if (scopesBuilder_ == null) {
+          ensureScopesIsMutable();
+          scopes_.remove(index);
+          onChanged();
+        } else {
+          scopesBuilder_.remove(index);
+        }
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder getScopesBuilder(
+          int index) {
+        return getScopesFieldBuilder().getBuilder(index);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder getScopesOrBuilder(
+          int index) {
+        if (scopesBuilder_ == null) {
+          return scopes_.get(index);  } else {
+          return scopesBuilder_.getMessageOrBuilder(index);
+        }
+      }
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder> 
+           getScopesOrBuilderList() {
+        if (scopesBuilder_ != null) {
+          return scopesBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(scopes_);
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder addScopesBuilder() {
+        return getScopesFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.getDefaultInstance());
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder addScopesBuilder(
+          int index) {
+        return getScopesFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.getDefaultInstance());
+      }
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder> 
+           getScopesBuilderList() {
+        return getScopesFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder> 
+          getScopesFieldBuilder() {
+        if (scopesBuilder_ == null) {
+          scopesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder, org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder>(
+                  scopes_,
+                  ((bitField0_ & 0x00000020) == 0x00000020),
+                  getParentForChildren(),
+                  isClean());
+          scopes_ = null;
+        }
+        return scopesBuilder_;
+      }
+      
+      // optional uint32 followingKvCount = 7;
+      private int followingKvCount_ ;
+      public boolean hasFollowingKvCount() {
+        return ((bitField0_ & 0x00000040) == 0x00000040);
+      }
+      public int getFollowingKvCount() {
+        return followingKvCount_;
+      }
+      public Builder setFollowingKvCount(int value) {
+        bitField0_ |= 0x00000040;
+        followingKvCount_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearFollowingKvCount() {
+        bitField0_ = (bitField0_ & ~0x00000040);
+        followingKvCount_ = 0;
+        onChanged();
+        return this;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:WALKey)
+    }
+    
+    static {
+      defaultInstance = new WALKey(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:WALKey)
+  }
+  
+  public interface FamilyScopeOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required bytes family = 1;
+    boolean hasFamily();
+    com.google.protobuf.ByteString getFamily();
+    
+    // required .ScopeType scopeType = 2;
+    boolean hasScopeType();
+    org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType getScopeType();
+  }
+  public static final class FamilyScope extends
+      com.google.protobuf.GeneratedMessage
+      implements FamilyScopeOrBuilder {
+    // Use FamilyScope.newBuilder() to construct.
+    private FamilyScope(Builder builder) {
+      super(builder);
+    }
+    private FamilyScope(boolean noInit) {}
+    
+    private static final FamilyScope defaultInstance;
+    public static FamilyScope getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public FamilyScope getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_FamilyScope_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_FamilyScope_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required bytes family = 1;
+    public static final int FAMILY_FIELD_NUMBER = 1;
+    private com.google.protobuf.ByteString family_;
+    public boolean hasFamily() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public com.google.protobuf.ByteString getFamily() {
+      return family_;
+    }
+    
+    // required .ScopeType scopeType = 2;
+    public static final int SCOPETYPE_FIELD_NUMBER = 2;
+    private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType scopeType_;
+    public boolean hasScopeType() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType getScopeType() {
+      return scopeType_;
+    }
+    
+    private void initFields() {
+      family_ = com.google.protobuf.ByteString.EMPTY;
+      scopeType_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasFamily()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasScopeType()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, family_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeEnum(2, scopeType_.getNumber());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, family_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeEnumSize(2, scopeType_.getNumber());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope other = (org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope) obj;
+      
+      boolean result = true;
+      result = result && (hasFamily() == other.hasFamily());
+      if (hasFamily()) {
+        result = result && getFamily()
+            .equals(other.getFamily());
+      }
+      result = result && (hasScopeType() == other.hasScopeType());
+      if (hasScopeType()) {
+        result = result &&
+            (getScopeType() == other.getScopeType());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasFamily()) {
+        hash = (37 * hash) + FAMILY_FIELD_NUMBER;
+        hash = (53 * hash) + getFamily().hashCode();
+      }
+      if (hasScopeType()) {
+        hash = (37 * hash) + SCOPETYPE_FIELD_NUMBER;
+        hash = (53 * hash) + hashEnum(getScopeType());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScopeOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_FamilyScope_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.internal_static_FamilyScope_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        family_ = com.google.protobuf.ByteString.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope build() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope result = new org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.family_ = family_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.scopeType_ = scopeType_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.getDefaultInstance()) return this;
+        if (other.hasFamily()) {
+          setFamily(other.getFamily());
+        }
+        if (other.hasScopeType()) {
+          setScopeType(other.getScopeType());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasFamily()) {
+          
+          return false;
+        }
+        if (!hasScopeType()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              family_ = input.readBytes();
+              break;
+            }
+            case 16: {
+              int rawValue = input.readEnum();
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType value = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType.valueOf(rawValue);
+              if (value == null) {
+                unknownFields.mergeVarintField(2, rawValue);
+              } else {
+                bitField0_ |= 0x00000002;
+                scopeType_ = value;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required bytes family = 1;
+      private com.google.protobuf.ByteString family_ = com.google.protobuf.ByteString.EMPTY;
+      public boolean hasFamily() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public com.google.protobuf.ByteString getFamily() {
+        return family_;
+      }
+      public Builder setFamily(com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        family_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearFamily() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        family_ = getDefaultInstance().getFamily();
+        onChanged();
+        return this;
+      }
+      
+      // required .ScopeType scopeType = 2;
+      private org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType scopeType_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
+      public boolean hasScopeType() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType getScopeType() {
+        return scopeType_;
+      }
+      public Builder setScopeType(org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType value) {
+        if (value == null) {
+          throw new NullPointerException();
+        }
+        bitField0_ |= 0x00000002;
+        scopeType_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearScopeType() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        scopeType_ = org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType.REPLICATION_SCOPE_LOCAL;
+        onChanged();
+        return this;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:FamilyScope)
+    }
+    
+    static {
+      defaultInstance = new FamilyScope(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:FamilyScope)
+  }
+  
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_WALHeader_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_WALHeader_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_WALKey_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_WALKey_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_FamilyScope_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_FamilyScope_fieldAccessorTable;
+  
+  public static com.google.protobuf.Descriptors.FileDescriptor
+      getDescriptor() {
+    return descriptor;
+  }
+  private static com.google.protobuf.Descriptors.FileDescriptor
+      descriptor;
+  static {
+    java.lang.String[] descriptorData = {
+      "\n\nHLog.proto\032\013hbase.proto\"#\n\tWALHeader\022\026" +
+      "\n\016hasCompression\030\001 \002(\010\"\266\001\n\006WALKey\022\031\n\021enc" +
+      "odedRegionName\030\001 \002(\014\022\021\n\ttableName\030\002 \002(\014\022" +
+      "\031\n\021logSequenceNumber\030\003 \002(\004\022\021\n\twriteTime\030" +
+      "\004 \002(\004\022\030\n\tclusterId\030\005 \001(\0132\005.UUID\022\034\n\006scope" +
+      "s\030\006 \003(\0132\014.FamilyScope\022\030\n\020followingKvCoun" +
+      "t\030\007 \001(\r\"<\n\013FamilyScope\022\016\n\006family\030\001 \002(\014\022\035" +
+      "\n\tscopeType\030\002 \002(\0162\n.ScopeType*F\n\tScopeTy" +
+      "pe\022\033\n\027REPLICATION_SCOPE_LOCAL\020\000\022\034\n\030REPLI" +
+      "CATION_SCOPE_GLOBAL\020\001B@\n*org.apache.hado",
+      "op.hbase.protobuf.generatedB\nHLogProtosH" +
+      "\001\210\001\001\240\001\001"
+    };
+    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
+      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
+        public com.google.protobuf.ExtensionRegistry assignDescriptors(
+            com.google.protobuf.Descriptors.FileDescriptor root) {
+          descriptor = root;
+          internal_static_WALHeader_descriptor =
+            getDescriptor().getMessageTypes().get(0);
+          internal_static_WALHeader_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_WALHeader_descriptor,
+              new java.lang.String[] { "HasCompression", },
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.class,
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader.Builder.class);
+          internal_static_WALKey_descriptor =
+            getDescriptor().getMessageTypes().get(1);
+          internal_static_WALKey_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_WALKey_descriptor,
+              new java.lang.String[] { "EncodedRegionName", "TableName", "LogSequenceNumber", "WriteTime", "ClusterId", "Scopes", "FollowingKvCount", },
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.class,
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey.Builder.class);
+          internal_static_FamilyScope_descriptor =
+            getDescriptor().getMessageTypes().get(2);
+          internal_static_FamilyScope_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_FamilyScope_descriptor,
+              new java.lang.String[] { "Family", "ScopeType", },
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.class,
+              org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope.Builder.class);
+          return null;
+        }
+      };
+    com.google.protobuf.Descriptors.FileDescriptor
+      .internalBuildGeneratedFileFrom(descriptorData,
+        new com.google.protobuf.Descriptors.FileDescriptor[] {
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.getDescriptor(),
+        }, assigner);
+  }
+  
+  // @@protoc_insertion_point(outer_class_scope)
+}
diff --git hbase-protocol/src/main/protobuf/Admin.proto hbase-protocol/src/main/protobuf/Admin.proto
index b56ea1d..1f75d62 100644
--- hbase-protocol/src/main/protobuf/Admin.proto
+++ hbase-protocol/src/main/protobuf/Admin.proto
@@ -25,6 +25,7 @@ option java_generate_equals_and_hash = true;
 option optimize_for = SPEED;
 
 import "hbase.proto";
+import "HLog.proto";
 
 message GetRegionInfoRequest {
   required RegionSpecifier region = 1;
@@ -155,38 +156,13 @@ message MergeRegionsRequest {
 message MergeRegionsResponse {
 }
 
-message UUID {
-  required uint64 leastSigBits = 1;
-  required uint64 mostSigBits = 2;
-}
-
 // Protocol buffer version of HLog
 message WALEntry {
   required WALKey key = 1;
   required WALEdit edit = 2;
 
-  // Protocol buffer version of HLogKey
-  message WALKey {
-    required bytes encodedRegionName = 1;
-    required bytes tableName = 2;
-    required uint64 logSequenceNumber = 3;
-    required uint64 writeTime = 4;
-    optional UUID clusterId = 5;
-  }
-
   message WALEdit {
     repeated bytes keyValueBytes = 1;
-    repeated FamilyScope familyScope = 2;
-
-    enum ScopeType {
-      REPLICATION_SCOPE_LOCAL = 0;
-      REPLICATION_SCOPE_GLOBAL = 1;
-    }
-
-    message FamilyScope {
-      required bytes family = 1;
-      required ScopeType scopeType = 2;
-    }
   }
 }
 
diff --git hbase-protocol/src/main/protobuf/HLog.proto hbase-protocol/src/main/protobuf/HLog.proto
new file mode 100644
index 0000000..309290a
--- /dev/null
+++ hbase-protocol/src/main/protobuf/HLog.proto
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+option java_package = "org.apache.hadoop.hbase.protobuf.generated";
+option java_outer_classname = "HLogProtos";
+option java_generic_services = true;
+option java_generate_equals_and_hash = true;
+option optimize_for = SPEED;
+
+import "hbase.proto";
+
+message WALHeader {
+  required bool hasCompression = 1;
+}
+
+// Protocol buffer version of HLogKey; see HLogKey comment, not really a key but WALEdit header for some KVs
+message WALKey {
+  required bytes encodedRegionName = 1;
+  required bytes tableName = 2;
+  required uint64 logSequenceNumber = 3;
+  required uint64 writeTime = 4;
+  optional UUID clusterId = 5;
+  
+  repeated FamilyScope scopes = 6;
+  optional uint32 followingKvCount = 7;
+}
+
+enum ScopeType {
+  REPLICATION_SCOPE_LOCAL = 0;
+  REPLICATION_SCOPE_GLOBAL = 1;
+}
+
+message FamilyScope {
+  required bytes family = 1;
+  required ScopeType scopeType = 2;
+}
diff --git hbase-protocol/src/main/protobuf/hbase.proto hbase-protocol/src/main/protobuf/hbase.proto
index cd04872..b43ad26 100644
--- hbase-protocol/src/main/protobuf/hbase.proto
+++ hbase-protocol/src/main/protobuf/hbase.proto
@@ -301,3 +301,8 @@ message LongMsg {
 message BigDecimalMsg {
   required bytes bigdecimalMsg = 1;
 }
+
+message UUID {
+  required uint64 leastSigBits = 1;
+  required uint64 mostSigBits = 2;
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
index 820e70b..e9c8d0f 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ReplicationProtbufUtil.java
@@ -26,6 +26,8 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.AdminProtocol;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
@@ -47,32 +49,24 @@ public class ReplicationProtbufUtil {
    * @return an array of HLog entries
    */
   public static HLog.Entry[]
-      toHLogEntries(final List<AdminProtos.WALEntry> protoList) {
+      toHLogEntries(final List<AdminProtos.WALEntry> protoList) throws IOException {
     List<HLog.Entry> entries = new ArrayList<HLog.Entry>();
     for (AdminProtos.WALEntry entry: protoList) {
-      AdminProtos.WALEntry.WALKey walKey = entry.getKey();
-      java.util.UUID clusterId = HConstants.DEFAULT_CLUSTER_ID;
-      if (walKey.hasClusterId()) {
-        AdminProtos.UUID protoUuid = walKey.getClusterId();
-        clusterId = new java.util.UUID(
-          protoUuid.getMostSigBits(), protoUuid.getLeastSigBits());
-      }
-      HLogKey key = new HLogKey(walKey.getEncodedRegionName().toByteArray(),
-        walKey.getTableName().toByteArray(), walKey.getLogSequenceNumber(),
-        walKey.getWriteTime(), clusterId);
+      HLogProtos.WALKey walKey = entry.getKey();
+      HLogKey key = new HLogKey(walKey);
       AdminProtos.WALEntry.WALEdit walEdit = entry.getEdit();
       WALEdit edit = new WALEdit();
       for (ByteString keyValue: walEdit.getKeyValueBytesList()) {
         edit.add(new KeyValue(keyValue.toByteArray()));
       }
-      if (walEdit.getFamilyScopeCount() > 0) {
+      if (walKey.getScopesCount() > 0) {
         TreeMap<byte[], Integer> scopes =
           new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
-        for (AdminProtos.WALEntry.WALEdit.FamilyScope scope: walEdit.getFamilyScopeList()) {
+        for (HLogProtos.FamilyScope scope: walKey.getScopesList()) {
           scopes.put(scope.getFamily().toByteArray(),
             Integer.valueOf(scope.getScopeType().ordinal()));
         }
-        edit.setScopes(scopes);
+        key.setScopes(scopes);
       }
       entries.add(new HLog.Entry(key, edit));
     }
@@ -105,16 +99,13 @@ public class ReplicationProtbufUtil {
    */
   public static AdminProtos.ReplicateWALEntryRequest
       buildReplicateWALEntryRequest(final HLog.Entry[] entries) {
-    AdminProtos.WALEntry.WALEdit.FamilyScope.Builder scopeBuilder = AdminProtos.WALEntry
-        .WALEdit
-        .FamilyScope
-        .newBuilder();
+    HLogProtos.FamilyScope.Builder scopeBuilder = HLogProtos.FamilyScope.newBuilder();
     AdminProtos.WALEntry.Builder entryBuilder = AdminProtos.WALEntry.newBuilder();
     AdminProtos.ReplicateWALEntryRequest.Builder builder =
       AdminProtos.ReplicateWALEntryRequest.newBuilder();
     for (HLog.Entry entry: entries) {
       entryBuilder.clear();
-      AdminProtos.WALEntry.WALKey.Builder keyBuilder = entryBuilder.getKeyBuilder();
+      HLogProtos.WALKey.Builder keyBuilder = entryBuilder.getKeyBuilder();
       HLogKey key = entry.getKey();
       keyBuilder.setEncodedRegionName(
         ByteString.copyFrom(key.getEncodedRegionName()));
@@ -123,23 +114,20 @@ public class ReplicationProtbufUtil {
       keyBuilder.setWriteTime(key.getWriteTime());
       UUID clusterId = key.getClusterId();
       if (clusterId != null) {
-        AdminProtos.UUID.Builder uuidBuilder = keyBuilder.getClusterIdBuilder();
+        HBaseProtos.UUID.Builder uuidBuilder = keyBuilder.getClusterIdBuilder();
         uuidBuilder.setLeastSigBits(clusterId.getLeastSignificantBits());
         uuidBuilder.setMostSigBits(clusterId.getMostSignificantBits());
       }
       WALEdit edit = entry.getEdit();
       AdminProtos.WALEntry.WALEdit.Builder editBuilder = entryBuilder.getEditBuilder();
-      NavigableMap<byte[], Integer> scopes = edit.getScopes();
+      NavigableMap<byte[], Integer> scopes = key.getScopes();
       if (scopes != null && !scopes.isEmpty()) {
         for (Map.Entry<byte[], Integer> scope: scopes.entrySet()) {
           scopeBuilder.setFamily(ByteString.copyFrom(scope.getKey()));
-          AdminProtos.WALEntry.WALEdit.ScopeType
-              scopeType = AdminProtos.WALEntry
-              .WALEdit
-              .ScopeType
-              .valueOf(scope.getValue().intValue());
+          HLogProtos.ScopeType scopeType =
+              HLogProtos.ScopeType.valueOf(scope.getValue().intValue());
           scopeBuilder.setScopeType(scopeType);
-          editBuilder.addFamilyScope(scopeBuilder.build());
+          keyBuilder.addScopes(scopeBuilder.build());
         }
       }
       List<KeyValue> keyValues = edit.getKeyValues();
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java
index a7b301d..c401fba 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/Compressor.java
@@ -25,6 +25,7 @@ import java.io.IOException;
 
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.WritableUtils;
@@ -32,6 +33,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import com.google.common.base.Preconditions;
+import com.google.protobuf.ByteString;
 
 /**
  * A set of static functions for running our custom WAL compression/decompression.
@@ -63,26 +65,30 @@ public class Compressor {
 
   private static void transformFile(Path input, Path output)
       throws IOException {
-    SequenceFileLogReader in = new SequenceFileLogReader();
-    SequenceFileLogWriter out = new SequenceFileLogWriter();
+    Configuration conf = HBaseConfiguration.create();
 
-    try {
-      Configuration conf = HBaseConfiguration.create();
-
-      FileSystem inFS = input.getFileSystem(conf);
-      FileSystem outFS = output.getFileSystem(conf);
+    FileSystem inFS = input.getFileSystem(conf);
+    FileSystem outFS = output.getFileSystem(conf);
 
-      in.init(inFS, input, conf);
-      boolean compress = in.reader.isWALCompressionEnabled();
+    HLog.Reader in = HLogFactory.createReader(inFS, input, conf, null, false);
+    HLog.Writer out = null;
 
+    try {
+      if (!(in instanceof SequenceFileLogReaderBase)) {
+        System.err.println("Cannot proceed, invalid reader type: " + in.getClass().getName());
+        return;
+      }
+      boolean compress = ((SequenceFileLogReaderBase)in).hasCompression();
       conf.setBoolean(HConstants.ENABLE_WAL_COMPRESSION, !compress);
-      out.init(outFS, output, conf);
+      out = HLogFactory.createWriter(outFS, output, conf);
 
       HLog.Entry e = null;
       while ((e = in.next()) != null) out.append(e);
     } finally {
       in.close();
-      out.close();
+      if (out != null) {
+        out.close();
+      }
     }
   }
 
@@ -184,9 +190,58 @@ public class Compressor {
     }
   }
 
+  static byte[] uncompressFromByteString(ByteString in, Dictionary dict)
+      throws IOException {
+    byte status = in.byteAt(0);
+    if (status == Dictionary.NOT_IN_DICTIONARY) {
+      int length = (int)ProtobufUtil.readVLong(in, 1);
+      int offset = WritableUtils.decodeVIntSize(in.byteAt(1));
+      byte[] arr = new byte[length];
+      in.copyTo(arr, offset + 1, 0, length);
+      // if this isn't in the dictionary, we need to add to the dictionary.
+      if (dict != null) dict.addEntry(arr, 0, length);
+      return arr;
+    } else {
+      // Status here is the higher-order byte of index of the dictionary entry
+      // (when its not Dictionary.NOT_IN_DICTIONARY -- dictionary indices are
+      // shorts).
+      short dictIdx = toShort(status, in.byteAt(1));
+      byte[] entry = dict.getEntry(dictIdx);
+      if (entry == null) {
+        throw new IOException("Missing dictionary entry for index " + dictIdx);
+      }
+      return entry;
+    }
+  }
+
+  public static ByteString compressIntoByteString(
+      byte[] data, Dictionary dict) throws IOException {
+    short dictIdx = Dictionary.NOT_IN_DICTIONARY;
+    if (dict != null) {
+      dictIdx = dict.findEntry(data, 0, data.length);
+    }
+    if (dictIdx == Dictionary.NOT_IN_DICTIONARY) {
+      byte[] compressed = new byte[data.length + 1 + WritableUtils.getVIntSize(data.length)];
+      compressed[0] = Dictionary.NOT_IN_DICTIONARY;
+      Bytes.writeByteArray(compressed, 1, data, 0, data.length);
+      return ByteString.copyFrom(compressed);
+    } else {
+      return ByteString.copyFrom(fromShort(dictIdx));
+    }
+  }
+
   static short toShort(byte hi, byte lo) {
     short s = (short) (((hi & 0xFF) << 8) | (lo & 0xFF));
     Preconditions.checkArgument(s >= 0);
     return s;
   }
+
+  /** Identical to DataOutput.writeShort encoding, and works with toShort above. */
+  static byte[] fromShort(short v) {
+    Preconditions.checkArgument(v >= 0);
+    byte[] result = new byte[2];
+    result[0] = (byte)(0xff & (v >> 8));
+    result[1] =(byte)(0xff & v);
+    return result;
+  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
index a375c8b..898adec 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSHLog.java
@@ -531,12 +531,10 @@ class FSHLog implements HLog, Syncable {
           }
         }
         FSHLog.Writer nextWriter = this.createWriterInstance(fs, newPath, conf);
-        // Can we get at the dfsclient outputstream?  If an instance of
-        // SFLW, it'll have done the necessary reflection to get at the
-        // protected field name.
+        // Can we get at the dfsclient outputstream?
         FSDataOutputStream nextHdfsOut = null;
-        if (nextWriter instanceof SequenceFileLogWriter) {
-          nextHdfsOut = ((SequenceFileLogWriter)nextWriter).getWriterFSDataOutputStream();
+        if (nextWriter instanceof ProtobufLogWriter) {
+          nextHdfsOut = ((ProtobufLogWriter)nextWriter).getStream();
         }
 
         Path oldFile = null;
@@ -855,43 +853,6 @@ class FSHLog implements HLog, Syncable {
   }
 
   @Override
-  public long append(HRegionInfo regionInfo, HLogKey logKey, WALEdit logEdit,
-                     HTableDescriptor htd, boolean doSync)
-  throws IOException {
-    if (this.closed) {
-      throw new IOException("Cannot append; log is closed");
-    }
-    long txid = 0;
-    synchronized (updateLock) {
-      long seqNum = obtainSeqNum();
-      logKey.setLogSeqNum(seqNum);
-      // The 'lastSeqWritten' map holds the sequence number of the oldest
-      // write for each region (i.e. the first edit added to the particular
-      // memstore). When the cache is flushed, the entry for the
-      // region being flushed is removed if the sequence number of the flush
-      // is greater than or equal to the value in lastSeqWritten.
-      this.oldestUnflushedSeqNums.putIfAbsent(regionInfo.getEncodedNameAsBytes(),
-        Long.valueOf(seqNum));
-      doWrite(regionInfo, logKey, logEdit, htd);
-      txid = this.unflushedEntries.incrementAndGet();
-      this.numEntries.incrementAndGet();
-      if (htd.isDeferredLogFlush()) {
-        lastDeferredTxid = txid;
-      }
-    }
-
-    // Sync if catalog region, and if not then check if that table supports
-    // deferred log flushing
-    if (doSync &&
-        (regionInfo.isMetaRegion() ||
-        !htd.isDeferredLogFlush())) {
-      // sync txn to file system
-      this.sync(txid);
-    }
-    return txid;
-  }
-
-  @Override
   public void append(HRegionInfo info, byte [] tableName, WALEdit edits,
     final long now, HTableDescriptor htd)
   throws IOException {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 9442180..4d9f2a5 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -28,6 +28,7 @@ import java.util.regex.Pattern;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.exceptions.FailedLogCloseException;
@@ -55,7 +56,11 @@ public interface HLog {
   public static final String RECOVERED_LOG_TMPFILE_SUFFIX = ".temp";
 
   public interface Reader {
-    void init(FileSystem fs, Path path, Configuration c) throws IOException;
+
+    /**
+     * @param s Input stream that may have been pre-opened by the caller; may be null.
+     */
+    void init(FileSystem fs, Path path, Configuration c, FSDataInputStream s) throws IOException;
 
     void close() throws IOException;
 
@@ -239,22 +244,8 @@ public interface HLog {
   public void closeAndDelete() throws IOException;
 
   /**
-   * Append an entry to the log.
-   * 
-   * @param regionInfo
-   * @param logEdit
-   * @param logKey
-   * @param doSync
-   *          shall we sync after writing the transaction
-   * @return The txid of this transaction
-   * @throws IOException
-   */
-  public long append(HRegionInfo regionInfo, HLogKey logKey, WALEdit logEdit,
-      HTableDescriptor htd, boolean doSync) throws IOException;
-
-  /**
    * Only used in tests.
-   * 
+   *
    * @param info
    * @param tableName
    * @param edits
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java
index a91284a..83db320 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogFactory.java
@@ -21,23 +21,26 @@
 package org.apache.hadoop.hbase.regionserver.wal;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.io.InterruptedIOException;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.regionserver.wal.HLog.Reader;
 import org.apache.hadoop.hbase.regionserver.wal.HLog.Writer;
+import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.CancelableProgressable;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 
 public class HLogFactory {
     private static final Log LOG = LogFactory.getLog(HLogFactory.class);
-    
+
     public static HLog createHLog(final FileSystem fs, final Path root, final String logName,
         final Configuration conf) throws IOException {
       return new FSHLog(fs, root, logName, conf);
@@ -60,13 +63,12 @@ public class HLogFactory {
       return new FSHLog(fs, root, logName, HConstants.HREGION_OLDLOGDIR_NAME, 
             conf, listeners, false, prefix, true);
     }
-    
+
     /*
      * WAL Reader
      */
-    
     private static Class<? extends Reader> logReaderClass;
-    
+
     static void resetLogReaderClass() {
       logReaderClass = null;
     }
@@ -85,10 +87,17 @@ public class HLogFactory {
      */
     public static HLog.Reader createReader(final FileSystem fs, final Path path,
         Configuration conf, CancelableProgressable reporter) throws IOException {
-      if (logReaderClass == null) {
+      return createReader(fs, path, conf, reporter, true);
+    }
+
+    public static HLog.Reader createReader(final FileSystem fs, final Path path,
+      Configuration conf, CancelableProgressable reporter, boolean allowCustom)
+        throws IOException {
+      if (allowCustom && (logReaderClass == null)) {
         logReaderClass = conf.getClass("hbase.regionserver.hlog.reader.impl",
-          SequenceFileLogReader.class, Reader.class);
+          ProtobufLogReader.class, Reader.class);
       }
+      Class<? extends Reader> lrClass = allowCustom ? logReaderClass : ProtobufLogReader.class;
 
       try {
         // A hlog file could be under recovery, so it may take several
@@ -99,9 +108,25 @@ public class HLogFactory {
         int nbAttempt = 0;
         while (true) {
           try {
-            HLog.Reader reader = logReaderClass.newInstance();
-            reader.init(fs, path, conf);
-            return reader;
+            if (lrClass != ProtobufLogReader.class) {
+              // User is overriding the WAL reader, let them.
+              HLog.Reader reader = lrClass.newInstance();
+              reader.init(fs, path, conf, null);
+              return reader;
+            } else {
+              FSDataInputStream stream = fs.open(path);
+              // Note that zero-length file will fail to read PB magic, and attempt to create
+              // a non-PB reader and fail the same way existing code expects it to. If we get
+              // rid of the old reader entirely, we need to handle 0-size files differently from
+              // merely non-PB files.
+              byte[] magic = new byte[ProtobufLogReader.PB_WAL_MAGIC.length];
+              boolean isPbWal = (stream.read(magic) == magic.length)
+                  && Arrays.equals(magic, ProtobufLogReader.PB_WAL_MAGIC);
+              HLog.Reader reader =
+                  isPbWal ? new ProtobufLogReader() : new SequenceFileLogReader();
+              reader.init(fs, path, conf, stream);
+              return reader;
+            }
           } catch (IOException e) {
             String msg = e.getMessage();
             if (msg != null && msg.contains("Cannot obtain block length")) {
@@ -139,9 +164,8 @@ public class HLogFactory {
     /*
      * WAL writer
      */
-    
     private static Class<? extends Writer> logWriterClass;
-    
+
     /**
      * Create a writer for the WAL.
      * @return A WAL writer.  Close when done with it.
@@ -153,9 +177,9 @@ public class HLogFactory {
       try {
         if (logWriterClass == null) {
           logWriterClass = conf.getClass("hbase.regionserver.hlog.writer.impl",
-              SequenceFileLogWriter.class, Writer.class);
+              ProtobufLogWriter.class, Writer.class);
         }
-        HLog.Writer writer = (HLog.Writer) logWriterClass.newInstance();
+        HLog.Writer writer = (HLog.Writer)logWriterClass.newInstance();
         writer.init(fs, path, conf);
         return writer;
       } catch (Exception e) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java
index ac90a53..f4669de 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogKey.java
@@ -22,16 +22,30 @@ import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.EOFException;
 import java.io.IOException;
+import java.nio.ByteBuffer;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.NavigableMap;
+import java.util.TreeMap;
 import java.util.UUID;
 
+import javax.naming.OperationNotSupportedException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos.FamilyScope;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos.ScopeType;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.WritableUtils;
 
+import com.google.protobuf.ByteString;
+
 /**
  * A Key for an entry in the change log.
  *
@@ -42,8 +56,12 @@ import org.apache.hadoop.io.WritableUtils;
  * <p>Some Transactional edits (START, COMMIT, ABORT) will not have an
  * associated row.
  */
+// TODO: Key and WALEdit are never used separately, or in one-to-many relation, for practical
+//       purposes. They need to be merged into HLogEntry.
 @InterfaceAudience.Private
 public class HLogKey implements WritableComparable<HLogKey> {
+  public static final Log LOG = LogFactory.getLog(HLogKey.class);
+
   // should be < 0 (@see #readFields(DataInput))
   // version 2 supports HLog compression
   enum Version {
@@ -89,16 +107,17 @@ public class HLogKey implements WritableComparable<HLogKey> {
 
   private UUID clusterId;
 
+  private NavigableMap<byte[], Integer> scopes;
+
   private CompressionContext compressionContext;
 
-  /** Writable Constructor -- Do not use. */
   public HLogKey() {
     this(null, null, 0L, HConstants.LATEST_TIMESTAMP,
         HConstants.DEFAULT_CLUSTER_ID);
   }
 
   /**
-   * Create the log key!
+   * Create the log key for writing to somewhere.
    * We maintain the tablename mainly for debugging purposes.
    * A regionName is always a sub-table object.
    *
@@ -111,11 +130,19 @@ public class HLogKey implements WritableComparable<HLogKey> {
    */
   public HLogKey(final byte [] encodedRegionName, final byte [] tablename,
       long logSeqNum, final long now, UUID clusterId) {
-    this.encodedRegionName = encodedRegionName;
-    this.tablename = tablename;
     this.logSeqNum = logSeqNum;
     this.writeTime = now;
     this.clusterId = clusterId;
+    this.encodedRegionName = encodedRegionName;
+    this.tablename = tablename;
+  }
+
+  /**
+   * Create HLogKey wrapper around protobuf WAL key; takes care of compression.
+   * @throws IOException Never, as the compression is not enabled.
+   */
+  public HLogKey(WALKey walKey) throws IOException {
+    readFieldsFromPb(walKey);
   }
 
   /**
@@ -137,11 +164,7 @@ public class HLogKey implements WritableComparable<HLogKey> {
 
   /** @return log sequence number */
   public long getLogSeqNum() {
-    return logSeqNum;
-  }
-
-  void setLogSeqNum(long logSeqNum) {
-    this.logSeqNum = logSeqNum;
+    return this.logSeqNum;
   }
 
   /**
@@ -159,8 +182,16 @@ public class HLogKey implements WritableComparable<HLogKey> {
     return clusterId;
   }
 
+  public NavigableMap<byte[], Integer> getScopes() {
+    return scopes;
+  }
+
+  public void setScopes(NavigableMap<byte[], Integer> scopes) {
+    this.scopes = scopes;
+  }
+
   /**
-   * Set the cluster id of this key
+   * Set the cluster id of this key.
    * @param clusterId
    */
   public void setClusterId(UUID clusterId) {
@@ -213,7 +244,7 @@ public class HLogKey implements WritableComparable<HLogKey> {
     if (result == 0) {
       if (this.logSeqNum < o.logSeqNum) {
         result = -1;
-      } else if (this.logSeqNum > o.logSeqNum) {
+      } else if (this.logSeqNum  > o.logSeqNum ) {
         result = 1;
       }
       if (result == 0) {
@@ -255,7 +286,9 @@ public class HLogKey implements WritableComparable<HLogKey> {
   }
 
   @Override
+  @Deprecated
   public void write(DataOutput out) throws IOException {
+    LOG.warn("HLogKey is being serialized to writable - only expected in test code");
     WritableUtils.writeVInt(out, VERSION.code);
     if (compressionContext == null) {
       Bytes.writeByteArray(out, this.encodedRegionName);
@@ -290,6 +323,7 @@ public class HLogKey implements WritableComparable<HLogKey> {
     // encodes the length of encodedRegionName.
     // If < 0 we just read the version and the next vint is the length.
     // @see Bytes#readByteArray(DataInput)
+    this.scopes = null; // writable HLogKey does not contain scopes
     int len = WritableUtils.readVInt(in);
     if (len < 0) {
       // what we just read was the version
@@ -308,7 +342,7 @@ public class HLogKey implements WritableComparable<HLogKey> {
       this.encodedRegionName = Compressor.readCompressed(in, compressionContext.regionDict);
       this.tablename = Compressor.readCompressed(in, compressionContext.tableDict);
     }
-    
+
     this.logSeqNum = in.readLong();
     this.writeTime = in.readLong();
     this.clusterId = HConstants.DEFAULT_CLUSTER_ID;
@@ -325,4 +359,61 @@ public class HLogKey implements WritableComparable<HLogKey> {
       }
     }
   }
+
+  public WALKey.Builder getBuilder() throws IOException {
+    WALKey.Builder builder = WALKey.newBuilder();
+    if (compressionContext == null) {
+      builder.setEncodedRegionName(ByteString.copyFrom(this.encodedRegionName));
+      builder.setTableName(ByteString.copyFrom(this.tablename));
+    } else {
+      builder.setEncodedRegionName(Compressor.compressIntoByteString(
+          this.encodedRegionName, compressionContext.regionDict));
+      builder.setTableName(Compressor.compressIntoByteString(
+              this.tablename, compressionContext.tableDict));
+    }
+    builder.setLogSequenceNumber(this.logSeqNum);
+    builder.setWriteTime(writeTime);
+    if (this.clusterId != HConstants.DEFAULT_CLUSTER_ID) {
+      builder.setClusterId(HBaseProtos.UUID.newBuilder()
+          .setLeastSigBits(this.clusterId.getLeastSignificantBits())
+          .setMostSigBits(this.clusterId.getMostSignificantBits()));
+    }
+    if (scopes != null) {
+      for (Map.Entry<byte[], Integer> e : scopes.entrySet()) {
+        ByteString family = (compressionContext == null) ? ByteString.copyFrom(e.getKey())
+            : Compressor.compressIntoByteString(e.getKey(), compressionContext.familyDict);
+        builder.addScopes(FamilyScope.newBuilder()
+            .setFamily(family).setScopeType(ScopeType.valueOf(e.getValue())));
+      }
+    }
+    return builder;
+  }
+
+  public void readFieldsFromPb(WALKey walKey) throws IOException {
+    if (this.compressionContext != null) {
+      this.encodedRegionName = Compressor.uncompressFromByteString(
+          walKey.getEncodedRegionName(), compressionContext.regionDict);
+      this.tablename = Compressor.uncompressFromByteString(
+          walKey.getTableName(), compressionContext.tableDict);
+    } else {
+      this.encodedRegionName = walKey.getEncodedRegionName().toByteArray();
+      this.tablename = walKey.getTableName().toByteArray();
+    }
+    this.clusterId = HConstants.DEFAULT_CLUSTER_ID;
+    if (walKey.hasClusterId()) {
+      this.clusterId = new UUID(
+          walKey.getClusterId().getMostSigBits(), walKey.getClusterId().getLeastSigBits());
+    }
+    this.scopes = null;
+    if (walKey.getScopesCount() > 0) {
+      this.scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
+      for (FamilyScope scope : walKey.getScopesList()) {
+        byte[] family = (compressionContext == null) ? scope.getFamily().toByteArray() :
+          Compressor.uncompressFromByteString(scope.getFamily(), compressionContext.familyDict);
+        this.scopes.put(family, scope.getScopeType().getNumber());
+      }
+    }
+    this.logSeqNum = walKey.getLogSequenceNumber();
+    this.writeTime = walKey.getWriteTime();
+  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java
index b2cd2f6..eadf545 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogUtil.java
@@ -49,23 +49,6 @@ public class HLogUtil {
     return Bytes.equals(HLog.METAFAMILY, family);
   }
 
-  @SuppressWarnings("unchecked")
-  public static Class<? extends HLogKey> getKeyClass(Configuration conf) {
-    return (Class<? extends HLogKey>) conf.getClass(
-        "hbase.regionserver.hlog.keyclass", HLogKey.class);
-  }
-
-  public static HLogKey newKey(Configuration conf) throws IOException {
-    Class<? extends HLogKey> keyClass = getKeyClass(conf);
-    try {
-      return keyClass.newInstance();
-    } catch (InstantiationException e) {
-      throw new IOException("cannot create hlog key");
-    } catch (IllegalAccessException e) {
-      throw new IOException("cannot create hlog key");
-    }
-  }
-
   /**
    * Pattern used to validate a HLog file name
    */
@@ -82,52 +65,6 @@ public class HLogUtil {
     return pattern.matcher(filename).matches();
   }
 
-  /*
-   * Get a reader for the WAL.
-   * 
-   * @param fs
-   * 
-   * @param path
-   * 
-   * @param conf
-   * 
-   * @return A WAL reader. Close when done with it.
-   * 
-   * @throws IOException
-   * 
-   * public static HLog.Reader getReader(final FileSystem fs, final Path path,
-   * Configuration conf) throws IOException { try {
-   * 
-   * if (logReaderClass == null) {
-   * 
-   * logReaderClass = conf.getClass("hbase.regionserver.hlog.reader.impl",
-   * SequenceFileLogReader.class, Reader.class); }
-   * 
-   * 
-   * HLog.Reader reader = logReaderClass.newInstance(); reader.init(fs, path,
-   * conf); return reader; } catch (IOException e) { throw e; } catch (Exception
-   * e) { throw new IOException("Cannot get log reader", e); } }
-   * 
-   * * Get a writer for the WAL.
-   * 
-   * @param path
-   * 
-   * @param conf
-   * 
-   * @return A WAL writer. Close when done with it.
-   * 
-   * @throws IOException
-   * 
-   * public static HLog.Writer createWriter(final FileSystem fs, final Path
-   * path, Configuration conf) throws IOException { try { if (logWriterClass ==
-   * null) { logWriterClass =
-   * conf.getClass("hbase.regionserver.hlog.writer.impl",
-   * SequenceFileLogWriter.class, Writer.class); } FSHLog.Writer writer =
-   * (FSHLog.Writer) logWriterClass.newInstance(); writer.init(fs, path, conf);
-   * return writer; } catch (Exception e) { throw new
-   * IOException("cannot get log writer", e); } }
-   */
-
   /**
    * Construct the HLog directory name
    * 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java
new file mode 100644
index 0000000..ad0aa8b
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.java
@@ -0,0 +1,127 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import java.io.EOFException;
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALKey;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.protobuf.InvalidProtocolBufferException;
+
+public class ProtobufLogReader extends SequenceFileLogReaderBase {
+  private static final Log LOG = LogFactory.getLog(ProtobufLogReader.class);
+  static final byte[] PB_WAL_MAGIC = Bytes.toBytes("PWAL");
+
+  private FSDataInputStream inputStream;
+  private boolean hasCompression = false;
+
+  public ProtobufLogReader() {
+    super();
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (this.inputStream != null) {
+      this.inputStream.close();
+      this.inputStream = null;
+    }
+  }
+
+  @Override
+  public long getPosition() throws IOException {
+    return inputStream.getPos();
+  }
+
+  @Override
+  public void reset() throws IOException {
+    initInternal(null, false);
+  }
+
+  @Override
+  protected void initReader(FSDataInputStream stream) throws IOException {
+    initInternal(stream, true);
+  }
+
+  private void initInternal(FSDataInputStream stream, boolean isFirst) throws IOException {
+    close();
+    long expectedPos = PB_WAL_MAGIC.length;
+    if (stream == null) {
+      stream = fs.open(path);
+      stream.seek(expectedPos);
+    }
+    if (stream.getPos() != expectedPos) {
+      throw new IOException("The stream is at invalid position: " + stream.getPos());
+    }
+    // Initialize metadata or, when we reset, just skip the header.
+    HLogProtos.WALHeader.Builder builder = HLogProtos.WALHeader.newBuilder();
+    boolean hasHeader = builder.mergeDelimitedFrom(stream); // TODO: just skip? what's this for
+    if (!hasHeader) {
+      throw new EOFException("Couldn't read WAL PB header");
+    }
+    if (isFirst) {
+      HLogProtos.WALHeader header = builder.build();
+      this.hasCompression = header.hasHasCompression() && header.getHasCompression();
+    }
+    this.inputStream = stream;
+  }
+
+  @Override
+  protected boolean hasCompression() {
+    return this.hasCompression;
+  }
+
+  @Override
+  protected boolean readNext(HLog.Entry entry) throws IOException {
+    WALKey.Builder builder = WALKey.newBuilder();
+    boolean hasNext = false;
+    try {
+      hasNext = builder.mergeDelimitedFrom(inputStream);
+    } catch (InvalidProtocolBufferException ipbe) {
+      LOG.error("Invalid PB while reading WAL, probably an unexpected EOF, ignoring", ipbe);
+    }
+    if (!hasNext) return false;
+    if (!builder.isInitialized()) {
+      // TODO: not clear if we should try to recover from corrupt PB that looks semi-legit.
+      //       If we can get the KV count, we could, theoretically, try to get next record.
+      LOG.error("Partial PB while reading WAL, probably an unexpected EOF, ignoring");
+      return false;
+    }
+    WALKey walKey = builder.build();
+    entry.getKey().readFieldsFromPb(walKey);
+    try {
+      entry.getEdit().readKvs(inputStream, walKey.getFollowingKvCount());
+    } catch (EOFException ex) {
+      LOG.error("EOF while reading KVs, ignoring", ex);
+      return false;
+    }
+    return true;
+  }
+
+  @Override
+  protected void seekOnFs(long pos) throws IOException {
+    this.inputStream.seek(pos);
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java
new file mode 100644
index 0000000..7a0f926
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.java
@@ -0,0 +1,119 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.protobuf.generated.HLogProtos.WALHeader;
+
+@InterfaceAudience.Private
+public class ProtobufLogWriter implements HLog.Writer {
+  private final Log LOG = LogFactory.getLog(this.getClass());
+  private FSDataOutputStream output;
+
+  /** Context used by our wal dictionary compressor.
+   * Null if we're not to do our custom dictionary compression. */
+  private CompressionContext compressionContext;
+
+  public ProtobufLogWriter() {
+    super();
+  }
+
+  @Override
+  public void init(FileSystem fs, Path path, Configuration conf)
+  throws IOException {
+    // Should we do our custom WAL compression?
+    boolean compress = conf.getBoolean(HConstants.ENABLE_WAL_COMPRESSION, false);
+    if (compress) {
+      try {
+        if (this.compressionContext == null) {
+          this.compressionContext = new CompressionContext(LRUDictionary.class);
+        } else {
+          this.compressionContext.clear();
+        }
+      } catch (Exception e) {
+        throw new IOException("Failed to initiate CompressionContext", e);
+      }
+    }
+    int bufferSize = fs.getConf().getInt("io.file.buffer.size", 4096);
+    short replication = (short)conf.getInt(
+        "hbase.regionserver.hlog.replication", fs.getDefaultReplication());
+    long blockSize = conf.getLong("hbase.regionserver.hlog.blocksize", fs.getDefaultBlockSize());
+    output = fs.create(path, false, bufferSize, replication, blockSize);
+    output.write(ProtobufLogReader.PB_WAL_MAGIC);
+    WALHeader.newBuilder().setHasCompression(compress).build().writeDelimitedTo(output);
+
+    LOG.debug("Writing protobuf WAL; path=" + path + ", compression=" + compress);
+  }
+
+  @Override
+  public void append(HLog.Entry entry) throws IOException {
+    entry.setCompressionContext(compressionContext);
+    entry.getKey().getBuilder().setFollowingKvCount(entry.getEdit().size())
+      .build().writeDelimitedTo(output);
+    entry.getEdit().writeKvs(output);
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (this.output != null) {
+      try {
+        this.output.close();
+      } catch (NullPointerException npe) {
+        // Can get a NPE coming up from down in DFSClient$DFSOutputStream#close
+        LOG.warn(npe);
+      }
+      this.output = null;
+    }
+  }
+
+  @Override
+  public void sync() throws IOException {
+    try {
+      this.output.flush();
+      this.output.sync();
+    } catch (NullPointerException npe) {
+      // Concurrent close...
+      throw new IOException(npe);
+    }
+  }
+
+  @Override
+  public long getLength() throws IOException {
+    try {
+      return this.output.getPos();
+    } catch (NullPointerException npe) {
+      // Concurrent close...
+      throw new IOException(npe);
+    }
+  }
+
+  public FSDataOutputStream getStream() {
+    return this.output;
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
index fea92a3..6632f88 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java
@@ -23,6 +23,8 @@ import java.io.FilterInputStream;
 import java.io.IOException;
 import java.lang.reflect.Field;
 import java.lang.reflect.Method;
+import java.util.NavigableMap;
+
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -31,12 +33,24 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.regionserver.wal.HLog.Entry;
 import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.SequenceFile.Metadata;
 
 @InterfaceAudience.Private
-public class SequenceFileLogReader implements HLog.Reader {
+public class SequenceFileLogReader extends SequenceFileLogReaderBase {
   private static final Log LOG = LogFactory.getLog(SequenceFileLogReader.class);
 
+  // Legacy stuff from pre-PB WAL metadata.
+  private static final Text WAL_VERSION_KEY = new Text("version");
+  // Let the version be 1.  Let absence of a version meta tag be old, version 0.
+  // Set this version '1' to be the version that introduces compression,
+  // the COMPRESSION_VERSION.
+  private static final int COMPRESSION_VERSION = 1;
+  private static final Text WAL_COMPRESSION_TYPE_KEY = new Text("compression.type");
+  private static final Text DICTIONARY_COMPRESSION_TYPE = new Text("dictionary");
+
   /**
    * Hack just to set the correct file length up in SequenceFile.Reader.
    * See HADOOP-6307.  The below is all about setting the right length on the
@@ -49,7 +63,7 @@ public class SequenceFileLogReader implements HLog.Reader {
    *         this.end = in.getPos() + length;
    *
    */
-  static class WALReader extends SequenceFile.Reader {
+  private static class WALReader extends SequenceFile.Reader {
 
     WALReader(final FileSystem fs, final Path p, final Configuration c)
     throws IOException {
@@ -65,15 +79,6 @@ public class SequenceFileLogReader implements HLog.Reader {
     }
 
     /**
-     * Call this method after init() has been executed
-     * 
-     * @return whether WAL compression is enabled
-     */
-    public boolean isWALCompressionEnabled() {
-      return SequenceFileLogWriter.isWALCompressionEnabled(this.getMetadata());
-    }
-
-    /**
      * Override just so can intercept first call to getPos.
      */
     static class WALReaderFSDataInputStream extends FSDataInputStream {
@@ -138,59 +143,12 @@ public class SequenceFileLogReader implements HLog.Reader {
     }
   }
 
-  Configuration conf;
-  WALReader reader;
-  FileSystem fs;
-
-  // Needed logging exceptions
-  Path path;
-  int edit = 0;
-  long entryStart = 0;
-  boolean emptyCompressionContext = true;
-  /**
-   * Compression context to use reading.  Can be null if no compression.
-   */
-  protected CompressionContext compressionContext = null;
-
-  protected Class<? extends HLogKey> keyClass;
+  // Protected for tests.
+  protected SequenceFile.Reader reader;
+  long entryStart = 0; // needed for logging exceptions
 
-  /**
-   * Default constructor.
-   */
   public SequenceFileLogReader() {
-  }
-
-  /**
-   * This constructor allows a specific HLogKey implementation to override that
-   * which would otherwise be chosen via configuration property.
-   *
-   * @param keyClass
-   */
-  public SequenceFileLogReader(Class<? extends HLogKey> keyClass) {
-    this.keyClass = keyClass;
-  }
-
-  @Override
-  public void init(FileSystem fs, Path path, Configuration conf)
-      throws IOException {
-    this.conf = conf;
-    this.path = path;
-    reader = new WALReader(fs, path, conf);
-    this.fs = fs;
-
-    // If compression is enabled, new dictionaries are created here.
-    boolean compression = reader.isWALCompressionEnabled();
-    if (compression) {
-      try {
-        if (compressionContext == null) {
-          compressionContext = new CompressionContext(LRUDictionary.class);
-        } else {
-          compressionContext.clear();
-        }
-      } catch (Exception e) {
-        throw new IOException("Failed to initialize CompressionContext", e);
-      }
-    }
+    super();
   }
 
   @Override
@@ -206,57 +164,65 @@ public class SequenceFileLogReader implements HLog.Reader {
   }
 
   @Override
-  public HLog.Entry next() throws IOException {
-    return next(null);
+  public long getPosition() throws IOException {
+    return reader != null ? reader.getPosition() : 0;
   }
 
   @Override
-  public HLog.Entry next(HLog.Entry reuse) throws IOException {
-    this.entryStart = this.reader.getPosition();
-    HLog.Entry e = reuse;
-    if (e == null) {
-      HLogKey key;
-      if (keyClass == null) {
-        key = HLogUtil.newKey(conf);
-      } else {
-        try {
-          key = keyClass.newInstance();
-        } catch (InstantiationException ie) {
-          throw new IOException(ie);
-        } catch (IllegalAccessException iae) {
-          throw new IOException(iae);
-        }
-      }
+  public void reset() throws IOException {
+    // Resetting the reader lets us see newly added data if the file is being written to
+    // We also keep the same compressionContext which was previously populated for this file
+    reader = new WALReader(fs, path, conf);
+  }
+
+  @Override
+  protected void initReader(FSDataInputStream stream) throws IOException {
+    // We don't use the stream because we have to have the magic stream above.
+    if (stream != null) {
+      stream.close();
+    }
+    reset();
+  }
+
+  @Override
+  protected boolean hasCompression() {
+    return isWALCompressionEnabled(reader.getMetadata());
+  }
 
-      WALEdit val = new WALEdit();
-      e = new HLog.Entry(key, val);
+  /**
+   * Call this method after init() has been executed
+   * @return whether WAL compression is enabled
+   */
+  static boolean isWALCompressionEnabled(final Metadata metadata) {
+    // Check version is >= VERSION?
+    Text txt = metadata.get(WAL_VERSION_KEY);
+    if (txt == null || Integer.parseInt(txt.toString()) < COMPRESSION_VERSION) {
+      return false;
     }
-    boolean b = false;
+    // Now check that compression type is present.  Currently only one value.
+    txt = metadata.get(WAL_COMPRESSION_TYPE_KEY);
+    return txt != null && txt.equals(DICTIONARY_COMPRESSION_TYPE);
+  }
+
+
+  @Override
+  protected boolean readNext(Entry e) throws IOException {
     try {
-      if (compressionContext != null) {
-        e.setCompressionContext(compressionContext);
+      boolean hasNext = this.reader.next(e.getKey(), e.getEdit());
+      if (!hasNext) return false;
+      // Scopes are probably in WAL edit, move to key
+      NavigableMap<byte[], Integer> scopes = e.getEdit().getAndRemoveScopes();
+      if (scopes != null) {
+        e.getKey().setScopes(scopes);
       }
-      b = this.reader.next(e.getKey(), e.getEdit());
+      return true;
     } catch (IOException ioe) {
       throw addFileInfoToException(ioe);
     }
-    edit++;
-    if (compressionContext != null && emptyCompressionContext) {
-      emptyCompressionContext = false;
-    }
-    return b? e: null;
   }
 
   @Override
-  public void seek(long pos) throws IOException {
-    if (compressionContext != null && emptyCompressionContext) {
-      while (next() != null) {
-        if (getPosition() == pos) {
-          emptyCompressionContext = false;
-          break;
-        }
-      }
-    }
+  protected void seekOnFs(long pos) throws IOException {
     try {
       reader.seek(pos);
     } catch (IOException ioe) {
@@ -264,11 +230,6 @@ public class SequenceFileLogReader implements HLog.Reader {
     }
   }
 
-  @Override
-  public long getPosition() throws IOException {
-    return reader != null ? reader.getPosition() : 0;
-  }
-
   protected IOException addFileInfoToException(final IOException ioe)
   throws IOException {
     long pos = -1;
@@ -301,11 +262,4 @@ public class SequenceFileLogReader implements HLog.Reader {
 
     return ioe;
   }
-
-  @Override
-  public void reset() throws IOException {
-    // Resetting the reader lets us see newly added data if the file is being written to
-    // We also keep the same compressionContext which was previously populated for this file
-    reader = new WALReader(fs, path, conf);
-  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReaderBase.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReaderBase.java
new file mode 100644
index 0000000..8e899a1
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReaderBase.java
@@ -0,0 +1,132 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+
+@InterfaceAudience.Private
+public abstract class SequenceFileLogReaderBase implements HLog.Reader {
+  protected Configuration conf;
+  protected FileSystem fs;
+  protected Path path;
+  protected long edit = 0;
+  /**
+   * Compression context to use reading.  Can be null if no compression.
+   */
+  protected CompressionContext compressionContext = null;
+  protected boolean emptyCompressionContext = true;
+
+  /**
+   * Default constructor.
+   */
+  public SequenceFileLogReaderBase() {
+  }
+
+  @Override
+  public void init(FileSystem fs, Path path, Configuration conf, FSDataInputStream stream)
+      throws IOException {
+    this.conf = conf;
+    this.path = path;
+    this.fs = fs;
+
+    initReader(stream);
+
+    boolean compression = hasCompression();
+    if (compression) {
+      // If compression is enabled, new dictionaries are created here.
+      try {
+        if (compressionContext == null) {
+          compressionContext = new CompressionContext(LRUDictionary.class);
+        } else {
+          compressionContext.clear();
+        }
+      } catch (Exception e) {
+        throw new IOException("Failed to initialize CompressionContext", e);
+      }
+    }
+  }
+
+  @Override
+  public HLog.Entry next() throws IOException {
+    return next(null);
+  }
+
+  @Override
+  public HLog.Entry next(HLog.Entry reuse) throws IOException {
+    HLog.Entry e = reuse;
+    if (e == null) {
+      e = new HLog.Entry(new HLogKey(), new WALEdit());
+    }
+    if (compressionContext != null) {
+      e.setCompressionContext(compressionContext);
+    }
+
+    boolean hasEntry = readNext(e);
+    edit++;
+    if (compressionContext != null && emptyCompressionContext) {
+      emptyCompressionContext = false;
+    }
+    return hasEntry ? e : null;
+  }
+
+
+  @Override
+  public void seek(long pos) throws IOException {
+    if (compressionContext != null && emptyCompressionContext) {
+      while (next() != null) {
+        if (getPosition() == pos) {
+          emptyCompressionContext = false;
+          break;
+        }
+      }
+    }
+    seekOnFs(pos);
+  }
+
+  /**
+   * Initializes the log reader with a particular stream (may be null).
+   * Reader assumes ownership of the stream if not null and may use it.
+   */
+  protected abstract void initReader(FSDataInputStream stream) throws IOException;
+
+  /**
+   * @return Whether compression is enabled for this log.
+   */
+  protected abstract boolean hasCompression();
+
+  /**
+   * Read next entry.
+   * @param e The entry to read into.
+   * @return Whether there was anything to read.
+   */
+  protected abstract boolean readNext(HLog.Entry e) throws IOException;
+
+  /**
+   * Performs a filesystem-level seek to a certain position in an underlying file.
+   */
+  protected abstract void seekOnFs(long pos) throws IOException;
+
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
deleted file mode 100644
index 88f0ebf..0000000
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
+++ /dev/null
@@ -1,272 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase.regionserver.wal;
-
-import java.io.IOException;
-import java.lang.reflect.Field;
-import java.lang.reflect.InvocationTargetException;
-import java.util.TreeMap;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.SequenceFile.CompressionType;
-import org.apache.hadoop.io.SequenceFile.Metadata;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.DefaultCodec;
-
-/**
- * Implementation of {@link HLog.Writer} that delegates to
- * SequenceFile.Writer.
- */
-@InterfaceAudience.Private
-public class SequenceFileLogWriter implements HLog.Writer {
-  static final Text WAL_VERSION_KEY = new Text("version");
-  // Let the version be 1.  Let absence of a version meta tag be old, version 0.
-  // Set this version '1' to be the version that introduces compression,
-  // the COMPRESSION_VERSION.
-  private static final int COMPRESSION_VERSION = 1;
-  static final int VERSION = COMPRESSION_VERSION;
-  static final Text WAL_VERSION = new Text("" + VERSION);
-  static final Text WAL_COMPRESSION_TYPE_KEY = new Text("compression.type");
-  static final Text DICTIONARY_COMPRESSION_TYPE = new Text("dictionary");
-
-  private final Log LOG = LogFactory.getLog(this.getClass());
-  // The sequence file we delegate to.
-  private SequenceFile.Writer writer;
-  // This is the FSDataOutputStream instance that is the 'out' instance
-  // in the SequenceFile.Writer 'writer' instance above.
-  private FSDataOutputStream writer_out;
-
-  private Class<? extends HLogKey> keyClass;
-
-  /**
-   * Context used by our wal dictionary compressor.  Null if we're not to do
-   * our custom dictionary compression.  This custom WAL compression is distinct
-   * from sequencefile native compression.
-   */
-  private CompressionContext compressionContext;
-
-  /**
-   * Default constructor.
-   */
-  public SequenceFileLogWriter() {
-    super();
-  }
-
-  /**
-   * This constructor allows a specific HLogKey implementation to override that
-   * which would otherwise be chosen via configuration property.
-   * 
-   * @param keyClass
-   */
-  public SequenceFileLogWriter(Class<? extends HLogKey> keyClass) {
-    this.keyClass = keyClass;
-  }
-
-  /**
-   * Create sequence file Metadata for our WAL file with version and compression
-   * type (if any).
-   * @param conf
-   * @param compress
-   * @return Metadata instance.
-   */
-  private static Metadata createMetadata(final Configuration conf,
-      final boolean compress) {
-    TreeMap<Text, Text> metaMap = new TreeMap<Text, Text>();
-    metaMap.put(WAL_VERSION_KEY, WAL_VERSION);
-    if (compress) {
-      // Currently we only do one compression type.
-      metaMap.put(WAL_COMPRESSION_TYPE_KEY, DICTIONARY_COMPRESSION_TYPE);
-    }
-    return new Metadata(metaMap);
-  }
-
-  /**
-   * Call this method after init() has been executed
-   * 
-   * @return whether WAL compression is enabled
-   */
-  static boolean isWALCompressionEnabled(final Metadata metadata) {
-    // Check version is >= VERSION?
-    Text txt = metadata.get(WAL_VERSION_KEY);
-    if (txt == null || Integer.parseInt(txt.toString()) < COMPRESSION_VERSION) {
-      return false;
-    }
-    // Now check that compression type is present.  Currently only one value.
-    txt = metadata.get(WAL_COMPRESSION_TYPE_KEY);
-    return txt != null && txt.equals(DICTIONARY_COMPRESSION_TYPE);
-  }
-
-  @Override
-  public void init(FileSystem fs, Path path, Configuration conf)
-  throws IOException {
-    // Should we do our custom WAL compression?
-    boolean compress = conf.getBoolean(HConstants.ENABLE_WAL_COMPRESSION, false);
-    if (compress) {
-      try {
-        if (this.compressionContext == null) {
-          this.compressionContext = new CompressionContext(LRUDictionary.class);
-        } else {
-          this.compressionContext.clear();
-        }
-      } catch (Exception e) {
-        throw new IOException("Failed to initiate CompressionContext", e);
-      }
-    }
-
-    if (null == keyClass) {
-      keyClass = HLogUtil.getKeyClass(conf);
-    }
-
-    // Create a SF.Writer instance.
-    try {
-      // reflection for a version of SequenceFile.createWriter that doesn't
-      // automatically create the parent directory (see HBASE-2312)
-      this.writer = (SequenceFile.Writer) SequenceFile.class
-        .getMethod("createWriter", new Class[] {FileSystem.class,
-            Configuration.class, Path.class, Class.class, Class.class,
-            Integer.TYPE, Short.TYPE, Long.TYPE, Boolean.TYPE,
-            CompressionType.class, CompressionCodec.class, Metadata.class})
-        .invoke(null, new Object[] {fs, conf, path, HLogUtil.getKeyClass(conf),
-            WALEdit.class,
-            Integer.valueOf(fs.getConf().getInt("io.file.buffer.size", 4096)),
-            Short.valueOf((short)
-              conf.getInt("hbase.regionserver.hlog.replication",
-              fs.getDefaultReplication())),
-            Long.valueOf(conf.getLong("hbase.regionserver.hlog.blocksize",
-                fs.getDefaultBlockSize())),
-            Boolean.valueOf(false) /*createParent*/,
-            SequenceFile.CompressionType.NONE, new DefaultCodec(),
-            createMetadata(conf, compress)
-            });
-    } catch (InvocationTargetException ite) {
-      // function was properly called, but threw it's own exception
-      throw new IOException(ite.getCause());
-    } catch (Exception e) {
-      // ignore all other exceptions. related to reflection failure
-    }
-
-    // if reflection failed, use the old createWriter
-    if (this.writer == null) {
-      LOG.debug("new createWriter -- HADOOP-6840 -- not available");
-      this.writer = SequenceFile.createWriter(fs, conf, path,
-        HLogUtil.getKeyClass(conf), WALEdit.class,
-        fs.getConf().getInt("io.file.buffer.size", 4096),
-        (short) conf.getInt("hbase.regionserver.hlog.replication",
-          fs.getDefaultReplication()),
-        conf.getLong("hbase.regionserver.hlog.blocksize",
-          fs.getDefaultBlockSize()),
-        SequenceFile.CompressionType.NONE,
-        new DefaultCodec(),
-        null,
-        createMetadata(conf, compress));
-    } else {
-      LOG.debug("using new createWriter -- HADOOP-6840");
-    }
-    
-    this.writer_out = getSequenceFilePrivateFSDataOutputStreamAccessible();
-    LOG.debug("Path=" + path + ", compression=" + compress);
-  }
-
-  // Get at the private FSDataOutputStream inside in SequenceFile so we can
-  // call sync on it.  Make it accessible.
-  private FSDataOutputStream getSequenceFilePrivateFSDataOutputStreamAccessible()
-  throws IOException {
-    FSDataOutputStream out = null;
-    final Field fields [] = this.writer.getClass().getDeclaredFields();
-    final String fieldName = "out";
-    for (int i = 0; i < fields.length; ++i) {
-      if (fieldName.equals(fields[i].getName())) {
-        try {
-          // Make the 'out' field up in SF.Writer accessible.
-          fields[i].setAccessible(true);
-          out = (FSDataOutputStream)fields[i].get(this.writer);
-          break;
-        } catch (IllegalAccessException ex) {
-          throw new IOException("Accessing " + fieldName, ex);
-        } catch (SecurityException e) {
-          LOG.warn("Does not have access to out field from FSDataOutputStream",
-              e);
-        }
-      }
-    }
-    return out;
-  }
-
-  @Override
-  public void append(HLog.Entry entry) throws IOException {
-    entry.setCompressionContext(compressionContext);
-    try {
-      this.writer.append(entry.getKey(), entry.getEdit());
-    } catch (NullPointerException npe) {
-      // Concurrent close...
-      throw new IOException(npe);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (this.writer != null) {
-      try {
-        this.writer.close();
-      } catch (NullPointerException npe) {
-        // Can get a NPE coming up from down in DFSClient$DFSOutputStream#close
-        LOG.warn(npe);
-      }
-      this.writer = null;
-    }
-  }
-
-  @Override
-  public void sync() throws IOException {
-    try {
-      this.writer.syncFs();
-    } catch (NullPointerException npe) {
-      // Concurrent close...
-      throw new IOException(npe);
-    }
-  }
-
-  @Override
-  public long getLength() throws IOException {
-    try {
-      return this.writer.getLength();
-    } catch (NullPointerException npe) {
-      // Concurrent close...
-      throw new IOException(npe);
-    }
-  }
-
-  /**
-   * @return The dfsclient out stream up inside SF.Writer made accessible, or
-   * null if not available.
-   */
-  public FSDataOutputStream getWriterFSDataOutputStream() {
-    return this.writer_out;
-  }
-}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
index 81f8fba..30de348 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java
@@ -19,13 +19,17 @@
 package org.apache.hadoop.hbase.regionserver.wal;
 
 import java.io.DataInput;
+import java.io.DataInputStream;
 import java.io.DataOutput;
+import java.io.DataOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.NavigableMap;
 import java.util.TreeMap;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.KeyValue;
@@ -33,6 +37,7 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.io.Writable;
 
+
 /**
  * WALEdit: Used in HBase's transaction log (WAL) to represent
  * the collection of edits (KeyValue objects) corresponding to a
@@ -69,10 +74,13 @@ import org.apache.hadoop.io.Writable;
  */
 @InterfaceAudience.Private
 public class WALEdit implements Writable, HeapSize {
-
+  public static final Log LOG = LogFactory.getLog(WALEdit.class);
   private final int VERSION_2 = -1;
 
   private final ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
+
+  // Only here for legacy writable deserialization
+  @Deprecated
   private NavigableMap<byte[], Integer> scopes;
 
   private CompressionContext compressionContext;
@@ -100,15 +108,10 @@ public class WALEdit implements Writable, HeapSize {
     return kvs;
   }
 
-  public NavigableMap<byte[], Integer> getScopes() {
-    return scopes;
-  }
-
-
-  public void setScopes (NavigableMap<byte[], Integer> scopes) {
-    // We currently process the map outside of WALEdit,
-    // TODO revisit when replication is part of core
-    this.scopes = scopes;
+  public NavigableMap<byte[], Integer> getAndRemoveScopes() {
+    NavigableMap<byte[], Integer> result = scopes;
+    scopes = null;
+    return result;
   }
 
   public void readFields(DataInput in) throws IOException {
@@ -148,6 +151,7 @@ public class WALEdit implements Writable, HeapSize {
   }
 
   public void write(DataOutput out) throws IOException {
+    LOG.warn("WALEdit is being serialized to writable - only expected in test code");
     out.writeInt(VERSION_2);
     out.writeInt(kvs.size());
     // We interleave the two lists for code simplicity
@@ -169,6 +173,27 @@ public class WALEdit implements Writable, HeapSize {
     }
   }
 
+  public void writeKvs(DataOutputStream out) throws IOException {
+    for (KeyValue kv : kvs) {
+      if (compressionContext != null) {
+        KeyValueCompression.writeKV(out, kv, compressionContext);
+      } else{
+        KeyValue.write(kv, out);
+      }
+    }
+  }
+
+  public void readKvs(DataInputStream inputStream, int kvCount) throws IOException {
+    kvs.clear();
+    for (int idx = 0; idx < kvCount; idx++) {
+      if (compressionContext != null) {
+        this.add(KeyValueCompression.readKV(inputStream, compressionContext));
+      } else {
+        this.add(KeyValue.create(inputStream));
+      }
+    }
+  }
+
   public long heapSize() {
     long ret = 0;
     for (KeyValue kv : kvs) {
@@ -196,5 +221,4 @@ public class WALEdit implements Writable, HeapSize {
     sb.append(">]");
     return sb.toString();
   }
-
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
index 71aa2c7..0d26e24 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
@@ -215,7 +215,7 @@ public class Replication implements WALActionsListener,
       }
     }
     if (!scopes.isEmpty()) {
-      logEdit.setScopes(scopes);
+      logKey.setScopes(scopes);
     }
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
index bde3546..1f81e90 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
@@ -491,7 +491,7 @@ public class ReplicationSource extends Thread
       HLogKey logKey = entry.getKey();
       // don't replicate if the log entries originated in the peer
       if (!logKey.getClusterId().equals(peerClusterId)) {
-        removeNonReplicableEdits(edit);
+        removeNonReplicableEdits(entry);
         // Don't replicate catalog entries, if the WALEdit wasn't
         // containing anything to replicate and if we're currently not set to replicate
         if (!(Bytes.equals(logKey.getTablename(), HConstants.ROOT_TABLE_NAME) ||
@@ -655,12 +655,12 @@ public class ReplicationSource extends Thread
 
   /**
    * We only want KVs that are scoped other than local
-   * @param edit The KV to check for replication
+   * @param entry The entry to check for replication
    */
-  protected void removeNonReplicableEdits(WALEdit edit) {
-    NavigableMap<byte[], Integer> scopes = edit.getScopes();
-    List<KeyValue> kvs = edit.getKeyValues();
-    for (int i = edit.size()-1; i >= 0; i--) {
+  protected void removeNonReplicableEdits(HLog.Entry entry) {
+    NavigableMap<byte[], Integer> scopes = entry.getKey().getScopes();
+    List<KeyValue> kvs = entry.getEdit().getKeyValues();
+    for (int i = kvs.size()-1; i >= 0; i--) {
       KeyValue kv = kvs.get(i);
       // The scope will be null or empty if
       // there's nothing to replicate in that WALEdit
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java
index 57e04f2..485b22c 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHLogRecordReader.java
@@ -110,34 +110,25 @@ public class TestHLogRecordReader {
    */
   @Test
   public void testPartialRead() throws Exception {
-    HLog log = HLogFactory.createHLog(fs, hbaseDir,
-                                      logName, conf);
+    HLog log = HLogFactory.createHLog(fs, hbaseDir, logName, conf);
     long ts = System.currentTimeMillis();
     WALEdit edit = new WALEdit();
-    edit.add(new KeyValue(rowName, family, Bytes.toBytes("1"),
-        ts, value));
-    log.append(info, tableName, edit,
-      ts, htd);
+    edit.add(new KeyValue(rowName, family, Bytes.toBytes("1"), ts, value));
+    log.append(info, tableName, edit, ts, htd);
     edit = new WALEdit();
-    edit.add(new KeyValue(rowName, family, Bytes.toBytes("2"),
-        ts+1, value));
-    log.append(info, tableName, edit,
-        ts+1, htd);
+    edit.add(new KeyValue(rowName, family, Bytes.toBytes("2"), ts+1, value));
+    log.append(info, tableName, edit, ts+1, htd);
     log.rollWriter();
 
     Thread.sleep(1);
     long ts1 = System.currentTimeMillis();
 
     edit = new WALEdit();
-    edit.add(new KeyValue(rowName, family, Bytes.toBytes("3"),
-        ts1+1, value));
-    log.append(info, tableName, edit,
-        ts1+1, htd);
+    edit.add(new KeyValue(rowName, family, Bytes.toBytes("3"), ts1+1, value));
+    log.append(info, tableName, edit, ts1+1, htd);
     edit = new WALEdit();
-    edit.add(new KeyValue(rowName, family, Bytes.toBytes("4"),
-        ts1+2, value));
-    log.append(info, tableName, edit,
-        ts1+2, htd);
+    edit.add(new KeyValue(rowName, family, Bytes.toBytes("4"), ts1+2, value));
+    log.append(info, tableName, edit, ts1+2, htd);
     log.close();
 
     HLogInputFormat input = new HLogInputFormat();
@@ -229,8 +220,11 @@ public class TestHLogRecordReader {
 
     for (byte[] column : columns) {
       assertTrue(reader.nextKeyValue());
-      assertTrue(Bytes
-          .equals(column, reader.getCurrentValue().getKeyValues().get(0).getQualifier()));
+      KeyValue kv = reader.getCurrentValue().getKeyValues().get(0);
+      if (!Bytes.equals(column, kv.getQualifier())) {
+        assertTrue("expected [" + Bytes.toString(column) + "], actual ["
+            + Bytes.toString(kv.getQualifier()) + "]", false);
+      }
     }
     assertFalse(reader.nextKeyValue());
     reader.close();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java
index b79e438..2164a43 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/FaultySequenceFileLogReader.java
@@ -41,14 +41,12 @@ public class FaultySequenceFileLogReader extends SequenceFileLogReader {
 
   @Override
   public HLog.Entry next(HLog.Entry reuse) throws IOException {
-    this.entryStart = this.reader.getPosition();
+    this.entryStart = this.getPosition();
     boolean b = true;
 
     if (nextQueue.isEmpty()) { // Read the whole thing at once and fake reading
       while (b == true) {
-        HLogKey key = HLogUtil.newKey(conf);
-        WALEdit val = new WALEdit();
-        HLog.Entry e = new HLog.Entry(key, val);
+        HLog.Entry e = new HLog.Entry(new HLogKey(), new WALEdit());
         if (compressionContext != null) {
           e.setCompressionContext(compressionContext);
         }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java
index 1e669a4..d240e66 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/InstrumentedSequenceFileLogWriter.java
@@ -22,12 +22,12 @@ import java.io.IOException;
 
 import org.apache.hadoop.hbase.util.Bytes;
 
-public class InstrumentedSequenceFileLogWriter extends SequenceFileLogWriter {
+public class InstrumentedSequenceFileLogWriter extends ProtobufLogWriter {
 
   public InstrumentedSequenceFileLogWriter() {
-    super(HLogKey.class);
+    super();
   }
-  
+
   public static boolean activateFailure = false;
   @Override
     public void append(HLog.Entry entry) throws IOException {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
new file mode 100644
index 0000000..721b87c
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java
@@ -0,0 +1,234 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver.wal;
+
+import java.io.IOException;
+import java.lang.reflect.Field;
+import java.lang.reflect.InvocationTargetException;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.SequenceFile.Metadata;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+
+/**
+ * Implementation of {@link HLog.Writer} that delegates to
+ * SequenceFile.Writer. Legacy implementation only used for compat tests.
+ */
+@InterfaceAudience.Private
+public class SequenceFileLogWriter implements HLog.Writer {
+  private final Log LOG = LogFactory.getLog(this.getClass());
+  // The sequence file we delegate to.
+  private SequenceFile.Writer writer;
+  // This is the FSDataOutputStream instance that is the 'out' instance
+  // in the SequenceFile.Writer 'writer' instance above.
+  private FSDataOutputStream writer_out;
+
+  // Legacy stuff from pre-PB WAL metadata.
+  private static final Text WAL_VERSION_KEY = new Text("version");
+  private static final Text WAL_COMPRESSION_TYPE_KEY = new Text("compression.type");
+  private static final Text DICTIONARY_COMPRESSION_TYPE = new Text("dictionary");
+  
+  /**
+   * Context used by our wal dictionary compressor.  Null if we're not to do
+   * our custom dictionary compression.  This custom WAL compression is distinct
+   * from sequencefile native compression.
+   */
+  private CompressionContext compressionContext;
+
+  /**
+   * Default constructor.
+   */
+  public SequenceFileLogWriter() {
+    super();
+  }
+
+  /**
+   * Create sequence file Metadata for our WAL file with version and compression
+   * type (if any).
+   * @param conf
+   * @param compress
+   * @return Metadata instance.
+   */
+  private static Metadata createMetadata(final Configuration conf,
+      final boolean compress) {
+    TreeMap<Text, Text> metaMap = new TreeMap<Text, Text>();
+    metaMap.put(WAL_VERSION_KEY, new Text("1"));
+    if (compress) {
+      // Currently we only do one compression type.
+      metaMap.put(WAL_COMPRESSION_TYPE_KEY, DICTIONARY_COMPRESSION_TYPE);
+    }
+    return new Metadata(metaMap);
+  }
+
+  @Override
+  public void init(FileSystem fs, Path path, Configuration conf)
+  throws IOException {
+    // Should we do our custom WAL compression?
+    boolean compress = conf.getBoolean(HConstants.ENABLE_WAL_COMPRESSION, false);
+    if (compress) {
+      try {
+        if (this.compressionContext == null) {
+          this.compressionContext = new CompressionContext(LRUDictionary.class);
+        } else {
+          this.compressionContext.clear();
+        }
+      } catch (Exception e) {
+        throw new IOException("Failed to initiate CompressionContext", e);
+      }
+    }
+
+    // Create a SF.Writer instance.
+    try {
+      // reflection for a version of SequenceFile.createWriter that doesn't
+      // automatically create the parent directory (see HBASE-2312)
+      this.writer = (SequenceFile.Writer) SequenceFile.class
+        .getMethod("createWriter", new Class[] {FileSystem.class,
+            Configuration.class, Path.class, Class.class, Class.class,
+            Integer.TYPE, Short.TYPE, Long.TYPE, Boolean.TYPE,
+            CompressionType.class, CompressionCodec.class, Metadata.class})
+        .invoke(null, new Object[] {fs, conf, path, HLogKey.class, WALEdit.class,
+            Integer.valueOf(fs.getConf().getInt("io.file.buffer.size", 4096)),
+            Short.valueOf((short)
+              conf.getInt("hbase.regionserver.hlog.replication",
+              fs.getDefaultReplication())),
+            Long.valueOf(conf.getLong("hbase.regionserver.hlog.blocksize",
+                fs.getDefaultBlockSize())),
+            Boolean.valueOf(false) /*createParent*/,
+            SequenceFile.CompressionType.NONE, new DefaultCodec(),
+            createMetadata(conf, compress)
+            });
+    } catch (InvocationTargetException ite) {
+      // function was properly called, but threw it's own exception
+      throw new IOException(ite.getCause());
+    } catch (Exception e) {
+      // ignore all other exceptions. related to reflection failure
+    }
+
+    // if reflection failed, use the old createWriter
+    if (this.writer == null) {
+      LOG.debug("new createWriter -- HADOOP-6840 -- not available");
+      this.writer = SequenceFile.createWriter(fs, conf, path,
+        HLogKey.class, WALEdit.class,
+        fs.getConf().getInt("io.file.buffer.size", 4096),
+        (short) conf.getInt("hbase.regionserver.hlog.replication",
+          fs.getDefaultReplication()),
+        conf.getLong("hbase.regionserver.hlog.blocksize",
+          fs.getDefaultBlockSize()),
+        SequenceFile.CompressionType.NONE,
+        new DefaultCodec(),
+        null,
+        createMetadata(conf, compress));
+    } else {
+      LOG.debug("using new createWriter -- HADOOP-6840");
+    }
+    
+    this.writer_out = getSequenceFilePrivateFSDataOutputStreamAccessible();
+    LOG.debug("Path=" + path + ", compression=" + compress);
+  }
+
+  // Get at the private FSDataOutputStream inside in SequenceFile so we can
+  // call sync on it.  Make it accessible.
+  private FSDataOutputStream getSequenceFilePrivateFSDataOutputStreamAccessible()
+  throws IOException {
+    FSDataOutputStream out = null;
+    final Field fields [] = this.writer.getClass().getDeclaredFields();
+    final String fieldName = "out";
+    for (int i = 0; i < fields.length; ++i) {
+      if (fieldName.equals(fields[i].getName())) {
+        try {
+          // Make the 'out' field up in SF.Writer accessible.
+          fields[i].setAccessible(true);
+          out = (FSDataOutputStream)fields[i].get(this.writer);
+          break;
+        } catch (IllegalAccessException ex) {
+          throw new IOException("Accessing " + fieldName, ex);
+        } catch (SecurityException e) {
+          LOG.warn("Does not have access to out field from FSDataOutputStream",
+              e);
+        }
+      }
+    }
+    return out;
+  }
+
+  @Override
+  public void append(HLog.Entry entry) throws IOException {
+    entry.setCompressionContext(compressionContext);
+    try {
+      this.writer.append(entry.getKey(), entry.getEdit());
+    } catch (NullPointerException npe) {
+      // Concurrent close...
+      throw new IOException(npe);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (this.writer != null) {
+      try {
+        this.writer.close();
+      } catch (NullPointerException npe) {
+        // Can get a NPE coming up from down in DFSClient$DFSOutputStream#close
+        LOG.warn(npe);
+      }
+      this.writer = null;
+    }
+  }
+
+  @Override
+  public void sync() throws IOException {
+    try {
+      this.writer.syncFs();
+    } catch (NullPointerException npe) {
+      // Concurrent close...
+      throw new IOException(npe);
+    }
+  }
+
+  @Override
+  public long getLength() throws IOException {
+    try {
+      return this.writer.getLength();
+    } catch (NullPointerException npe) {
+      // Concurrent close...
+      throw new IOException(npe);
+    }
+  }
+
+  /**
+   * @return The dfsclient out stream up inside SF.Writer made accessible, or
+   * null if not available.
+   */
+  public FSDataOutputStream getWriterFSDataOutputStream() {
+    return this.writer_out;
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
index 3c24a22..e025d7d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLog.java
@@ -754,6 +754,73 @@ public class TestHLog  {
       log.append(hri, tableName, cols, timestamp, htd);
     }
   }
+  
+
+  /**
+   * @throws IOException
+   */
+  @Test
+  public void testReadLegacyLog() throws IOException {
+    final int columnCount = 5;
+    final int recordCount = 5;
+    final byte[] tableName = Bytes.toBytes("tablename");
+    final byte[] row = Bytes.toBytes("row");
+    long timestamp = System.currentTimeMillis();
+    Path path = new Path(dir, "temphlog");
+    SequenceFileLogWriter sflw = null;
+    HLog.Reader reader = null;
+    try {
+      HRegionInfo hri = new HRegionInfo(tableName,
+          HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
+      HTableDescriptor htd = new HTableDescriptor(tableName);
+      fs.mkdirs(dir);
+      // Write log in pre-PB format.
+      sflw = new SequenceFileLogWriter();
+      sflw.init(fs, path, conf);
+      for (int i = 0; i < recordCount; ++i) {
+        HLogKey key = new HLogKey(
+            hri.getEncodedNameAsBytes(), tableName, i, timestamp, HConstants.DEFAULT_CLUSTER_ID);
+        WALEdit edit = new WALEdit();
+        for (int j = 0; j < columnCount; ++j) {
+          if (i == 0) {
+            htd.addFamily(new HColumnDescriptor("column" + j));
+          }
+          String value = i + "" + j;
+          edit.add(new KeyValue(row, row, row, timestamp, Bytes.toBytes(value)));
+        }
+        sflw.append(new HLog.Entry(key, edit));
+      }
+      sflw.sync();
+      sflw.close();
+
+      // Now read the log using standard means.
+      reader = HLogFactory.createReader(fs, path, conf);
+      assertTrue(reader instanceof SequenceFileLogReader);
+      for (int i = 0; i < recordCount; ++i) {
+        HLog.Entry entry = reader.next();
+        assertNotNull(entry);
+        assertEquals(columnCount, entry.getEdit().size());
+        assertArrayEquals(hri.getEncodedNameAsBytes(), entry.getKey().getEncodedRegionName());
+        assertArrayEquals(tableName, entry.getKey().getTablename());
+        int idx = 0;
+        for (KeyValue val : entry.getEdit().getKeyValues()) {
+          assertTrue(Bytes.equals(row, val.getRow()));
+          String value = i + "" + idx;
+          assertArrayEquals(Bytes.toBytes(value), val.getValue());
+          idx++;
+        }
+      }
+      HLog.Entry entry = reader.next();
+      assertNull(entry);
+    } finally {
+      if (sflw != null) {
+        sflw.close();
+      }
+      if (reader != null) {
+        reader.close();
+      }
+    }
+  }
 
   static class DumbWALActionsListener implements WALActionsListener {
     int increments = 0;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
index 2858679..9438fb9 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogSplit.java
@@ -652,7 +652,7 @@ public class TestHLogSplit {
     int actualCount = 0;
     HLog.Reader in = HLogFactory.createReader(fs, splitLog, conf);
     @SuppressWarnings("unused")
-	HLog.Entry entry;
+    HLog.Entry entry;
     while ((entry = in.next()) != null) ++actualCount;
     assertEquals(entryCount-1, actualCount);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
index 819721c..86047ad 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALActionsListener.java
@@ -100,8 +100,7 @@ public class TestWALActionsListener {
       HTableDescriptor htd = new HTableDescriptor();
       htd.addFamily(new HColumnDescriptor(b));
 
-      HLogKey key = new HLogKey(b,b, 0, 0, HConstants.DEFAULT_CLUSTER_ID);
-      hlog.append(hri, key, edit, htd, true);
+      hlog.append(hri, b, edit, 0, htd);
       if (i == 10) {
         hlog.registerWALActionsListener(laterobserver);
       }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
index 4ac4540..6c670b5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
@@ -819,7 +819,7 @@ public class TestWALReplay {
         "The sequence number of the recoverd.edits and the current edit seq should be same",
         lastestSeqNumber, editCount);
   }
-  
+
   static class MockHLog extends FSHLog {
     boolean doCompleteCacheFlush = false;
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
index eef6d92..e6a514e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestReplicationSourceManager.java
@@ -191,7 +191,7 @@ public class TestReplicationSourceManager {
       LOG.info(i);
       HLogKey key = new HLogKey(hri.getRegionName(), test, seq++,
           System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
-      hlog.append(hri, key, edit, htd, true);
+      hlog.append(hri, test, edit, System.currentTimeMillis(), htd);
     }
 
     // Simulate a rapid insert that's followed
@@ -202,9 +202,7 @@ public class TestReplicationSourceManager {
     LOG.info(baseline + " and " + time);
 
     for (int i = 0; i < 3; i++) {
-      HLogKey key = new HLogKey(hri.getRegionName(), test, seq++,
-          System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
-      hlog.append(hri, key, edit, htd, true);
+      hlog.append(hri, test, edit, System.currentTimeMillis(), htd);
     }
 
     assertEquals(6, manager.getHLogs().get(slaveId).size());
@@ -214,9 +212,7 @@ public class TestReplicationSourceManager {
     manager.logPositionAndCleanOldLogs(manager.getSources().get(0).getCurrentPath(),
         "1", 0, false, false);
 
-    HLogKey key = new HLogKey(hri.getRegionName(), test, seq++,
-        System.currentTimeMillis(), HConstants.DEFAULT_CLUSTER_ID);
-    hlog.append(hri, key, edit, htd, true);
+    hlog.append(hri, test, edit, System.currentTimeMillis(), htd);
 
     assertEquals(1, manager.getHLogs().size());
 
