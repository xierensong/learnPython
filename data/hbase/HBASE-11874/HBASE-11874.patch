diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
index fd7d252..a136231 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
@@ -530,4 +530,34 @@ public final class CellUtil {
           + SettableSequenceId.class.getName()));
     }
   }
+
+  /**
+   * Creates a contiguous array of key bytes by combining rk, cf, qualifier, ts and type. <br>
+   * Note: This is an expensive operation as it recreates key copying the rk, cf, qualifier, ts and
+   * type. When a Cell implementation is having a contiguous array backing the key, implement it as
+   * {@link SingleBufferKey} so that HBase core code won't use this expensive operation to make the
+   * key.
+   * @param cell
+   * @return contiguous array of key
+   */
+  public static byte[] createKey(Cell cell) {
+    int rlength = cell.getRowLength();
+    int flength = cell.getFamilyLength();
+    int qlength = cell.getQualifierLength();
+    int keySize = (int) KeyValue.getKeyDataStructureSize(rlength, flength, qlength);
+    byte[] key = new byte[keySize];
+    int pos = 0;
+    pos = Bytes.putShort(key, pos, (short) (rlength & 0x0000ffff));
+    pos = Bytes.putBytes(key, pos, cell.getRowArray(), cell.getRowOffset(), rlength);
+    pos = Bytes.putByte(key, pos, (byte) (flength & 0x0000ff));
+    if (flength != 0) {
+      pos = Bytes.putBytes(key, pos, cell.getFamilyArray(), cell.getFamilyOffset(), flength);
+    }
+    if (qlength != 0) {
+      pos = Bytes.putBytes(key, pos, cell.getQualifierArray(), cell.getQualifierOffset(), qlength);
+    }
+    pos = Bytes.putLong(key, pos, cell.getTimestamp());
+    pos = Bytes.putByte(key, pos, cell.getTypeByte());
+    return key;
+  }
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index e75147b..ffa051d 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -78,7 +78,7 @@ import com.google.common.annotations.VisibleForTesting;
  * and actual tag bytes length.
  */
 @InterfaceAudience.Private
-public class KeyValue implements Cell, HeapSize, Cloneable, SettableSequenceId {
+public class KeyValue implements Cell, HeapSize, Cloneable, SettableSequenceId, SingleBufferKey {
   private static final ArrayList<Tag> EMPTY_ARRAY_LIST = new ArrayList<Tag>();
 
   static final Log LOG = LogFactory.getLog(KeyValue.class);
@@ -1243,6 +1243,11 @@ public class KeyValue implements Cell, HeapSize, Cloneable, SettableSequenceId {
     return klength + vlength;
   }
 
+  @Override
+  public byte[] getKeyArray() {
+    return this.bytes;
+  }
+
   /**
    * @return Key offset in backing buffer..
    */
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/SingleBufferKey.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/SingleBufferKey.java
new file mode 100644
index 0000000..87638cf
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/SingleBufferKey.java
@@ -0,0 +1,46 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * Using this Interface one can mark a Cell implementation as backed by a single contiguous key
+ * array. The key should be of the format <br>&lt;2 bytes rk len&gt;&lt;rk&gt;&lt;1 byte cf
+ * len&gt;&lt;cf&gt;&lt;qualifier&gt;&lt;8 bytes timestamp&gt;&lt;1 byte type&gt;
+ */
+@InterfaceAudience.LimitedPrivate(HBaseInterfaceAudience.COPROC)
+public interface SingleBufferKey {
+
+  /**
+   * Contiguous raw bytes that may start at any index in the containing array. Max length is
+   * Integer.MAX_VALUE.
+   * @return The array containing the key bytes.
+   */
+  byte[] getKeyArray();
+
+  /**
+   * @return offset to the key in backing buffer.
+   */
+  int getKeyOffset();
+
+  /**
+   * @return total length of the key.
+   */
+  int getKeyLength();
+}
\ No newline at end of file
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index 4772358..08e03eb 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.KeyValue.SamePrefixComparator;
 import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.SingleBufferKey;
 import org.apache.hadoop.hbase.io.TagCompressionContext;
 import org.apache.hadoop.hbase.io.hfile.BlockType;
 import org.apache.hadoop.hbase.io.hfile.HFileContext;
@@ -325,7 +326,7 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   // there. So this has to be an instance of SettableSequenceId. SeekerState need not be
   // SettableSequenceId as we never return that to top layers. When we have to, we make
   // ClonedSeekerState from it.
-  protected static class ClonedSeekerState implements Cell, SettableSequenceId {
+  protected static class ClonedSeekerState implements Cell, SettableSequenceId, SingleBufferKey {
     private byte[] keyOnlyBuffer;
     private ByteBuffer currentBuffer;
     private short rowLength;
@@ -509,6 +510,21 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     public void setSequenceId(long seqId) {
       this.seqId = seqId;
     }
+
+    @Override
+    public byte[] getKeyArray() {
+      return this.keyOnlyBuffer;
+    }
+
+    @Override
+    public int getKeyOffset() {
+      return 0;
+    }
+
+    @Override
+    public int getKeyLength() {
+      return this.keyOnlyBuffer.length;
+    }
   }
 
   protected abstract static class
@@ -835,17 +851,17 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   }
 
   /**
-   * @param kv
+   * @param cell
    * @param out
    * @param encodingCtx
    * @return unencoded size added
    * @throws IOException
    */
-  protected final int afterEncodingKeyValue(KeyValue kv, DataOutputStream out,
+  protected final int afterEncodingKeyValue(Cell cell, DataOutputStream out,
       HFileBlockDefaultEncodingContext encodingCtx) throws IOException {
     int size = 0;
     if (encodingCtx.getHFileContext().isIncludesTags()) {
-      int tagsLength = kv.getTagsLength();
+      int tagsLength = cell.getTagsLength();
       ByteBufferUtils.putCompressedInt(out, tagsLength);
       // There are some tags to be written
       if (tagsLength > 0) {
@@ -854,20 +870,20 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
         // the tags using Dictionary compression in such a case
         if (tagCompressionContext != null) {
           tagCompressionContext
-              .compressTags(out, kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
+              .compressTags(out, cell.getTagsArray(), cell.getTagsOffset(), tagsLength);
         } else {
-          out.write(kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
+          out.write(cell.getTagsArray(), cell.getTagsOffset(), tagsLength);
         }
       }
       size += tagsLength + KeyValue.TAGS_LENGTH_SIZE;
     }
     if (encodingCtx.getHFileContext().isIncludesMvcc()) {
-      // Copy memstore timestamp from the byte buffer to the output stream.
-      long memstoreTS = kv.getMvccVersion();
-      WritableUtils.writeVLong(out, memstoreTS);
+      // Copy seqId from the byte buffer to the output stream.
+      long seqId = cell.getSequenceId();
+      WritableUtils.writeVLong(out, seqId);
       // TODO use a writeVLong which returns the #bytes written so that 2 time parsing can be
       // avoided.
-      size += WritableUtils.getVIntSize(memstoreTS);
+      size += WritableUtils.getVIntSize(seqId);
     }
     return size;
   }
@@ -973,17 +989,18 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   }
 
   @Override
-  public int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException {
+  public int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException {
     BufferedDataBlockEncodingState state = (BufferedDataBlockEncodingState) encodingCtx
         .getEncodingState();
-    int encodedKvSize = internalEncode(kv, (HFileBlockDefaultEncodingContext) encodingCtx, out);
+    int encodedKvSize = internalEncode(key, koffset, klength, cell,
+        (HFileBlockDefaultEncodingContext) encodingCtx, out);
     state.unencodedDataSizeWritten += encodedKvSize;
     return encodedKvSize;
   }
 
-  public abstract int internalEncode(KeyValue kv, HFileBlockDefaultEncodingContext encodingCtx,
-      DataOutputStream out) throws IOException;
+  public abstract int internalEncode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockDefaultEncodingContext encodingCtx, DataOutputStream out) throws IOException;
 
   @Override
   public void endBlockEncoding(HFileBlockEncodingContext encodingCtx, DataOutputStream out,
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
index f2bcae4..8bbc4e7 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/CopyKeyDataBlockEncoder.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
@@ -36,28 +37,27 @@ import org.apache.hadoop.io.WritableUtils;
 public class CopyKeyDataBlockEncoder extends BufferedDataBlockEncoder {
 
   @Override
-  public int internalEncode(KeyValue kv, HFileBlockDefaultEncodingContext encodingContext,
-      DataOutputStream out) throws IOException {
-    int klength = kv.getKeyLength();
-    int vlength = kv.getValueLength();
+  public int internalEncode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockDefaultEncodingContext encodingContext, DataOutputStream out) throws IOException {
+    int vlength = cell.getValueLength();
 
     out.writeInt(klength);
     out.writeInt(vlength);
-    out.write(kv.getBuffer(), kv.getKeyOffset(), klength);
-    out.write(kv.getValueArray(), kv.getValueOffset(), vlength);
+    out.write(key, koffset, klength);
+    out.write(cell.getValueArray(), cell.getValueOffset(), vlength);
     int size = klength + vlength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
     // Write the additional tag into the stream
     if (encodingContext.getHFileContext().isIncludesTags()) {
-      int tagsLength = kv.getTagsLength();
+      int tagsLength = cell.getTagsLength();
       out.writeShort(tagsLength);
       if (tagsLength > 0) {
-        out.write(kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
+        out.write(cell.getTagsArray(), cell.getTagsOffset(), tagsLength);
       }
       size += tagsLength + KeyValue.TAGS_LENGTH_SIZE;
     }
     if (encodingContext.getHFileContext().isIncludesMvcc()) {
-      WritableUtils.writeVLong(out, kv.getMvccVersion());
-      size += WritableUtils.getVIntSize(kv.getMvccVersion());
+      WritableUtils.writeVLong(out, cell.getSequenceId());
+      size += WritableUtils.getVIntSize(cell.getSequenceId());
     }
     return size;
   }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
index 99f6a7f..843cd05 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
@@ -23,7 +23,6 @@ import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.Cell;
-import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.io.hfile.HFileContext;
 
@@ -52,15 +51,18 @@ public interface DataBlockEncoder {
       throws IOException;
 
   /**
-   * Encodes a KeyValue.
-   * @param kv
+   * Encodes a Cell.
+   * @param key
+   * @param koffset
+   * @param klength
+   * @param cell
    * @param encodingCtx
    * @param out
    * @return unencoded kv size written
    * @throws IOException
    */
-  int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException;
+  int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException;
 
   /**
    * Ends encoding for a block of KeyValues. Gives a chance for the encoder to do the finishing
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
index fc4c314..6fa6b17 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
@@ -192,30 +193,28 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public int internalEncode(KeyValue kv, HFileBlockDefaultEncodingContext encodingContext,
-      DataOutputStream out) throws IOException {
+  public int internalEncode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockDefaultEncodingContext encodingContext, DataOutputStream out) throws IOException {
     EncodingState state = encodingContext.getEncodingState();
-    int size = compressSingleKeyValue(out, kv, state.prevKv);
-    size += afterEncodingKeyValue(kv, out, encodingContext);
-    state.prevKv = kv;
+    int size = compressSingleKeyValue(out, key, koffset, klength, cell, state);
+    size += afterEncodingKeyValue(cell, out, encodingContext);
+    state.set(cell, key, koffset, klength);
     return size;
   }
 
-  private int compressSingleKeyValue(DataOutputStream out, KeyValue kv, KeyValue prevKv)
-      throws IOException {
+  private int compressSingleKeyValue(DataOutputStream out, byte[] keyBuf, int koffset,
+      int klength, Cell cell, EncodingState state) throws IOException {
     byte flag = 0;
-    int kLength = kv.getKeyLength();
-    int vLength = kv.getValueLength();
+    int vLength = cell.getValueLength();
 
     long timestamp;
     long diffTimestamp = 0;
     int diffTimestampFitsInBytes = 0;
     int timestampFitsInBytes;
     int commonPrefix;
-    byte[] curKvBuf = kv.getBuffer();
 
-    if (prevKv == null) {
-      timestamp = kv.getTimestamp();
+    if (state.prevCell == null) {
+      timestamp = cell.getTimestamp();
       if (timestamp < 0) {
         flag |= FLAG_TIMESTAMP_SIGN;
         timestamp = -timestamp;
@@ -224,27 +223,27 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
       flag |= (timestampFitsInBytes - 1) << SHIFT_TIMESTAMP_LENGTH;
       commonPrefix = 0;
       // put column family
-      byte familyLength = kv.getFamilyLength();
+      byte familyLength = cell.getFamilyLength();
       out.write(familyLength);
-      out.write(kv.getFamilyArray(), kv.getFamilyOffset(), familyLength);
+      out.write(cell.getFamilyArray(), cell.getFamilyOffset(), familyLength);
     } else {
       // Finding common prefix
-      int preKeyLength = prevKv.getKeyLength();
-      commonPrefix = ByteBufferUtils.findCommonPrefix(curKvBuf, kv.getKeyOffset(), kLength
-          - KeyValue.TIMESTAMP_TYPE_SIZE, prevKv.getBuffer(), prevKv.getKeyOffset(), preKeyLength
+      int preKeyLength = state.prevKeyLen;
+      commonPrefix = ByteBufferUtils.findCommonPrefix(keyBuf, koffset, klength
+          - KeyValue.TIMESTAMP_TYPE_SIZE, state.prevKey, state.prevKeyOffset, preKeyLength
           - KeyValue.TIMESTAMP_TYPE_SIZE);
-      if (kLength == preKeyLength) {
+      if (klength == preKeyLength) {
         flag |= FLAG_SAME_KEY_LENGTH;
       }
-      if (vLength == prevKv.getValueLength()) {
+      if (vLength == state.prevCell.getValueLength()) {
         flag |= FLAG_SAME_VALUE_LENGTH;
       }
-      if (kv.getTypeByte() == prevKv.getTypeByte()) {
+      if (cell.getTypeByte() == state.prevCell.getTypeByte()) {
         flag |= FLAG_SAME_TYPE;
       }
       // don't compress timestamp and type using prefix encode timestamp
-      timestamp = kv.getTimestamp();
-      diffTimestamp = prevKv.getTimestamp() - timestamp;
+      timestamp = cell.getTimestamp();
+      diffTimestamp = state.prevCell.getTimestamp() - timestamp;
       boolean negativeTimestamp = timestamp < 0;
       if (negativeTimestamp) {
         timestamp = -timestamp;
@@ -270,18 +269,18 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
     }
     out.write(flag);
     if ((flag & FLAG_SAME_KEY_LENGTH) == 0) {
-      ByteBufferUtils.putCompressedInt(out, kLength);
+      ByteBufferUtils.putCompressedInt(out, klength);
     }
     if ((flag & FLAG_SAME_VALUE_LENGTH) == 0) {
       ByteBufferUtils.putCompressedInt(out, vLength);
     }
     ByteBufferUtils.putCompressedInt(out, commonPrefix);
-    if (prevKv == null || commonPrefix < kv.getRowLength() + KeyValue.ROW_LENGTH_SIZE) {
-      int restRowLength = kv.getRowLength() + KeyValue.ROW_LENGTH_SIZE - commonPrefix;
-      out.write(curKvBuf, kv.getKeyOffset() + commonPrefix, restRowLength);
-      out.write(curKvBuf, kv.getQualifierOffset(), kv.getQualifierLength());
+    if (state.prevCell == null || commonPrefix < cell.getRowLength() + KeyValue.ROW_LENGTH_SIZE) {
+      int restRowLength = cell.getRowLength() + KeyValue.ROW_LENGTH_SIZE - commonPrefix;
+      out.write(keyBuf, koffset + commonPrefix, restRowLength);
+      out.write(keyBuf, cell.getQualifierOffset(), cell.getQualifierLength());
     } else {
-      out.write(curKvBuf, kv.getKeyOffset() + commonPrefix, kLength - commonPrefix
+      out.write(keyBuf, koffset + commonPrefix, klength - commonPrefix
           - KeyValue.TIMESTAMP_TYPE_SIZE);
     }
     if ((flag & FLAG_TIMESTAMP_IS_DIFF) == 0) {
@@ -291,10 +290,10 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
     }
 
     if ((flag & FLAG_SAME_TYPE) == 0) {
-      out.write(kv.getTypeByte());
+      out.write(cell.getTypeByte());
     }
-    out.write(kv.getValueArray(), kv.getValueOffset(), vLength);
-    return kLength + vLength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
+    out.write(cell.getValueArray(), cell.getValueOffset(), vLength);
+    return klength + vLength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
   }
 
   @Override
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
index 93d4bbf..f1e2597 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
@@ -245,7 +245,8 @@ public class EncodedDataBlock {
         kv = new KeyValue(in.array(), kvOffset, (int) KeyValue.getKeyValueDataStructureSize(
             klength, vlength, tagsLength));
         kv.setSequenceId(memstoreTS);
-        this.dataBlockEncoder.encode(kv, encodingCtx, out);
+        this.dataBlockEncoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv,
+            encodingCtx, out);
       }
       BufferGrabbingByteArrayOutputStream stream = new BufferGrabbingByteArrayOutputStream();
       baos.writeTo(stream);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java
index b16f099..5057af7 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodingState.java
@@ -19,16 +19,22 @@
 package org.apache.hadoop.hbase.io.encoding;
 
 import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.Cell;
 
 /**
  * Keeps track of the encoding state.
  */
 @InterfaceAudience.Private
 public class EncodingState {
+  protected Cell prevCell = null;
+  protected byte[] prevKey = null;
+  protected int prevKeyOffset = -1;
+  protected int prevKeyLen = -1;
 
-  /**
-   * The previous KeyValue the encoder encoded.
-   */
-  protected KeyValue prevKv = null;
+  public void set(Cell prevCell, byte[] prevKey, int prevKeyOffset, int prevKeyLen) {
+    this.prevCell = prevCell;
+    this.prevKey = prevKey;
+    this.prevKeyOffset = prevKeyOffset;
+    this.prevKeyLen = prevKeyLen;
+  }
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
index c133308..ff085e5 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
@@ -237,50 +238,48 @@ public class FastDiffDeltaEncoder extends BufferedDataBlockEncoder {
   }
 
   @Override
-  public int internalEncode(KeyValue kv, HFileBlockDefaultEncodingContext encodingContext,
-      DataOutputStream out) throws IOException {
+  public int internalEncode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockDefaultEncodingContext encodingContext, DataOutputStream out) throws IOException {
     EncodingState state = encodingContext.getEncodingState();
-    int size = compressSingleKeyValue(out, kv, state.prevKv);
-    size += afterEncodingKeyValue(kv, out, encodingContext);
-    state.prevKv = kv;
+    int size = compressSingleKeyValue(out, key, koffset, klength, cell, state);
+    size += afterEncodingKeyValue(cell, out, encodingContext);
+    state.set(cell, key, koffset, klength);
     return size;
   }
 
-  private int compressSingleKeyValue(DataOutputStream out, KeyValue kv, KeyValue prevKv)
-      throws IOException {
+  private int compressSingleKeyValue(DataOutputStream out, byte[] keyBuf, int koffset,
+      int klength, Cell cell, EncodingState state) throws IOException {
     byte flag = 0;
-    int kLength = kv.getKeyLength();
-    int vLength = kv.getValueLength();
-    byte[] curKvBuf = kv.getBuffer();
+    int vLength = cell.getValueLength();
 
-    if (prevKv == null) {
+    if (state.prevCell == null) {
       // copy the key, there is no common prefix with none
       out.write(flag);
-      ByteBufferUtils.putCompressedInt(out, kLength);
+      ByteBufferUtils.putCompressedInt(out, klength);
       ByteBufferUtils.putCompressedInt(out, vLength);
       ByteBufferUtils.putCompressedInt(out, 0);
-      out.write(curKvBuf, kv.getKeyOffset(), kLength + vLength);
+      out.write(keyBuf, koffset, klength + vLength);
     } else {
-      byte[] preKvBuf = prevKv.getBuffer();
-      int preKeyLength = prevKv.getKeyLength();
-      int preValLength = prevKv.getValueLength();
+      byte[] prevKeyBuf = state.prevKey;
+      int preKeyLength = state.prevKeyLen;
+      int preValLength = state.prevCell.getValueLength();
       // find a common prefix and skip it
-      int commonPrefix = ByteBufferUtils.findCommonPrefix(curKvBuf, kv.getKeyOffset(), kLength
-          - KeyValue.TIMESTAMP_TYPE_SIZE, preKvBuf, prevKv.getKeyOffset(), preKeyLength
+      int commonPrefix = ByteBufferUtils.findCommonPrefix(keyBuf, koffset, klength
+          - KeyValue.TIMESTAMP_TYPE_SIZE, prevKeyBuf, state.prevKeyOffset, preKeyLength
           - KeyValue.TIMESTAMP_TYPE_SIZE);
 
-      if (kLength == prevKv.getKeyLength()) {
+      if (klength == preKeyLength) {
         flag |= FLAG_SAME_KEY_LENGTH;
       }
-      if (vLength == prevKv.getValueLength()) {
+      if (vLength == preValLength) {
         flag |= FLAG_SAME_VALUE_LENGTH;
       }
-      if (kv.getTypeByte() == prevKv.getTypeByte()) {
+      if (cell.getTypeByte() == state.prevCell.getTypeByte()) {
         flag |= FLAG_SAME_TYPE;
       }
 
-      int commonTimestampPrefix = findCommonTimestampPrefix(curKvBuf, kv.getKeyOffset() + kLength
-          - KeyValue.TIMESTAMP_TYPE_SIZE, preKvBuf, prevKv.getKeyOffset() + preKeyLength
+      int commonTimestampPrefix = findCommonTimestampPrefix(keyBuf, koffset + klength
+          - KeyValue.TIMESTAMP_TYPE_SIZE, prevKeyBuf, state.prevKeyOffset + preKeyLength
           - KeyValue.TIMESTAMP_TYPE_SIZE);
 
       flag |= commonTimestampPrefix << SHIFT_TIMESTAMP_LENGTH;
@@ -288,47 +287,47 @@ public class FastDiffDeltaEncoder extends BufferedDataBlockEncoder {
       // Check if current and previous values are the same. Compare value
       // length first as an optimization.
       if (vLength == preValLength
-          && Bytes.equals(kv.getValueArray(), kv.getValueOffset(), vLength,
-              prevKv.getValueArray(), prevKv.getValueOffset(), preValLength)) {
+          && Bytes.equals(cell.getValueArray(), cell.getValueOffset(), vLength,
+              state.prevCell.getValueArray(), state.prevCell.getValueOffset(), preValLength)) {
         flag |= FLAG_SAME_VALUE;
       }
 
       out.write(flag);
       if ((flag & FLAG_SAME_KEY_LENGTH) == 0) {
-        ByteBufferUtils.putCompressedInt(out, kLength);
+        ByteBufferUtils.putCompressedInt(out, klength);
       }
       if ((flag & FLAG_SAME_VALUE_LENGTH) == 0) {
         ByteBufferUtils.putCompressedInt(out, vLength);
       }
       ByteBufferUtils.putCompressedInt(out, commonPrefix);
 
-      if (commonPrefix < kv.getRowLength() + KeyValue.ROW_LENGTH_SIZE) {
+      if (commonPrefix < cell.getRowLength() + KeyValue.ROW_LENGTH_SIZE) {
         // Previous and current rows are different. Copy the differing part of
         // the row, skip the column family, and copy the qualifier.
-        out.write(curKvBuf, kv.getKeyOffset() + commonPrefix, kv.getRowLength()
+        out.write(keyBuf, koffset + commonPrefix, cell.getRowLength()
             + KeyValue.ROW_LENGTH_SIZE - commonPrefix);
-        out.write(curKvBuf, kv.getQualifierOffset(), kv.getQualifierLength());
+        out.write(keyBuf, cell.getQualifierOffset(), cell.getQualifierLength());
       } else {
         // The common part includes the whole row. As the column family is the
         // same across the whole file, it will automatically be included in the
         // common prefix, so we need not special-case it here.
-        int restKeyLength = kLength - commonPrefix - KeyValue.TIMESTAMP_TYPE_SIZE;
-        out.write(curKvBuf, kv.getKeyOffset() + commonPrefix, restKeyLength);
+        int restKeyLength = klength - commonPrefix - KeyValue.TIMESTAMP_TYPE_SIZE;
+        out.write(keyBuf, koffset + commonPrefix, restKeyLength);
       }
-      out.write(curKvBuf, kv.getKeyOffset() + kLength - KeyValue.TIMESTAMP_TYPE_SIZE
+      out.write(keyBuf, koffset + klength - KeyValue.TIMESTAMP_TYPE_SIZE
           + commonTimestampPrefix, KeyValue.TIMESTAMP_SIZE - commonTimestampPrefix);
 
       // Write the type if it is not the same as before.
       if ((flag & FLAG_SAME_TYPE) == 0) {
-        out.write(kv.getTypeByte());
+        out.write(cell.getTypeByte());
       }
 
       // Write the value if it is not the same as before.
       if ((flag & FLAG_SAME_VALUE) == 0) {
-        out.write(kv.getValueArray(), kv.getValueOffset(), vLength);
+        out.write(cell.getValueArray(), cell.getValueOffset(), vLength);
       }
     }
-    return kLength + vLength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
+    return klength + vLength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
   }
 
   @Override
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
index a6db8ee..6564142 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
@@ -45,31 +46,28 @@ import org.apache.hadoop.hbase.util.Bytes;
 public class PrefixKeyDeltaEncoder extends BufferedDataBlockEncoder {
 
   @Override
-  public int internalEncode(KeyValue kv, HFileBlockDefaultEncodingContext encodingContext,
-      DataOutputStream out) throws IOException {
-    byte[] kvBuf = kv.getBuffer();
-    int klength = kv.getKeyLength();
-    int vlength = kv.getValueLength();
+  public int internalEncode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockDefaultEncodingContext encodingContext, DataOutputStream out) throws IOException {
+    int vlength = cell.getValueLength();
     EncodingState state = encodingContext.getEncodingState();
-    if (state.prevKv == null) {
+    if (state.prevCell == null) {
       // copy the key, there is no common prefix with none
       ByteBufferUtils.putCompressedInt(out, klength);
       ByteBufferUtils.putCompressedInt(out, vlength);
       ByteBufferUtils.putCompressedInt(out, 0);
-      out.write(kvBuf, kv.getKeyOffset(), klength + vlength);
+      out.write(key, koffset, klength + vlength);
     } else {
       // find a common prefix and skip it
-      int common = ByteBufferUtils.findCommonPrefix(state.prevKv.getBuffer(),
-          state.prevKv.getKeyOffset(), state.prevKv.getKeyLength(), kvBuf, kv.getKeyOffset(),
-          kv.getKeyLength());
+      int common = ByteBufferUtils.findCommonPrefix(state.prevKey, state.prevKeyOffset,
+          state.prevKeyLen, key, koffset, klength);
       ByteBufferUtils.putCompressedInt(out, klength - common);
       ByteBufferUtils.putCompressedInt(out, vlength);
       ByteBufferUtils.putCompressedInt(out, common);
-      out.write(kvBuf, kv.getKeyOffset() + common, klength - common + vlength);
+      out.write(key, koffset + common, klength - common + vlength);
     }
     int size = klength + vlength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
-    size += afterEncodingKeyValue(kv, out, encodingContext);
-    state.prevKv = kv;
+    size += afterEncodingKeyValue(cell, out, encodingContext);
+    state.set(cell, key, koffset, klength);
     return size;
   }
 
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/PrefixTreeCodec.java b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/PrefixTreeCodec.java
index 2a0c459..5292438 100644
--- a/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/PrefixTreeCodec.java
+++ b/hbase-prefix-tree/src/main/java/org/apache/hadoop/hbase/codec/prefixtree/PrefixTreeCodec.java
@@ -24,6 +24,7 @@ import java.io.IOException;
 import java.nio.ByteBuffer;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.KeyValue.MetaComparator;
@@ -161,14 +162,14 @@ public class PrefixTreeCodec implements DataBlockEncoder{
   }
 
   @Override
-  public int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException {
+  public int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException {
     PrefixTreeEncodingState state = (PrefixTreeEncodingState) encodingCtx.getEncodingState();
     PrefixTreeEncoder builder = state.builder;
-    builder.write(kv);
-    int size = kv.getLength();
+    builder.write(cell);
+    int size = KeyValueUtil.length(cell);
     if (encodingCtx.getHFileContext().isIncludesMvcc()) {
-      size += WritableUtils.getVIntSize(kv.getMvccVersion());
+      size += WritableUtils.getVIntSize(cell.getSequenceId());
     }
     return size;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
index c0dd672..0c50c24 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java
@@ -35,12 +35,8 @@ import java.util.Map;
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
-import java.util.concurrent.ArrayBlockingQueue;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 
-import org.apache.hadoop.hbase.util.ByteStringer;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
@@ -51,6 +47,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
@@ -63,13 +60,13 @@ import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.BytesBytesPair;
 import org.apache.hadoop.hbase.protobuf.generated.HFileProtos;
 import org.apache.hadoop.hbase.util.BloomFilterWriter;
+import org.apache.hadoop.hbase.util.ByteStringer;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ChecksumType;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.io.Writable;
 
 import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
 
 /**
  * File format for hbase.
@@ -202,7 +199,7 @@ public class HFile {
     /** Add an element to the file info map. */
     void appendFileInfo(byte[] key, byte[] value) throws IOException;
 
-    void append(KeyValue kv) throws IOException;
+    void append(Cell cell) throws IOException;
 
     void append(byte[] key, byte[] value) throws IOException;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
index 3e26107..e2eb160 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
@@ -32,8 +32,8 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.compress.Compression;
@@ -744,14 +744,17 @@ public class HFileBlock implements Cacheable {
     }
 
     /**
-     * Writes the kv to this block
-     * @param kv
+     * Writes the cell to this block
+     * @param key
+     * @param koffset
+     * @param klength
+     * @param cell
      * @throws IOException
      */
-    public void write(KeyValue kv) throws IOException{
+    public void write(byte[] key, int koffset, int klength, Cell cell) throws IOException {
       expectState(State.WRITING);
-      this.unencodedDataSizeWritten += this.dataBlockEncoder.encode(kv, dataBlockEncodingCtx,
-          this.userDataStream);
+      this.unencodedDataSizeWritten += this.dataBlockEncoder.encode(key, koffset, klength, cell,
+          dataBlockEncodingCtx, this.userDataStream);
     }
 
     /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java
index 7049e4c..68cbb48 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.java
@@ -20,7 +20,7 @@ import java.io.DataOutputStream;
 import java.io.IOException;
 
 import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext;
@@ -48,15 +48,18 @@ public interface HFileDataBlockEncoder {
       throws IOException;
 
   /**
-   * Encodes a KeyValue.
-   * @param kv
+   * Encodes a Cell.
+   * @param key
+   * @param koffset
+   * @param klength
+   * @param cell
    * @param encodingCtx
    * @param out
    * @return unencoded kv size
    * @throws IOException
    */
-  int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException;
+  int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException;
 
   /**
    * Ends encoding for a block of KeyValues. Gives a chance for the encoder to do the finishing
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
index edf4cc6..9a77efd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
@@ -20,7 +20,7 @@ import java.io.DataOutputStream;
 import java.io.IOException;
 
 import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoder;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext;
@@ -92,9 +92,9 @@ public class HFileDataBlockEncoderImpl implements HFileDataBlockEncoder {
   }
 
   @Override
-  public int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException {
-    return this.encoding.getEncoder().encode(kv, encodingCtx, out);
+  public int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException {
+    return this.encoding.getEncoder().encode(key, koffset, klength, cell, encodingCtx, out);
   }
 
   @Override
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
index e6201bf..8634648 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java
@@ -32,8 +32,11 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.SingleBufferKey;
 import org.apache.hadoop.hbase.io.hfile.HFile.Writer;
 import org.apache.hadoop.hbase.io.hfile.HFileBlock.BlockWritable;
 import org.apache.hadoop.hbase.util.BloomFilterWriter;
@@ -242,18 +245,27 @@ public class HFileWriterV2 extends AbstractHFileWriter {
    * Add key/value to file. Keys must be added in an order that agrees with the
    * Comparator passed on construction.
    *
-   * @param kv
-   *          KeyValue to add. Cannot be empty nor null.
+   * @param cell
+   *          Cell to add. Cannot be empty nor null.
    * @throws IOException
    */
   @Override
-  public void append(final KeyValue kv) throws IOException {
-    byte[] key = kv.getBuffer();
-    int koffset = kv.getKeyOffset();
-    int klength = kv.getKeyLength();
-    byte[] value = kv.getValueArray();
-    int voffset = kv.getValueOffset();
-    int vlength = kv.getValueLength();
+  public void append(final Cell cell) throws IOException {
+    byte[] key = null;
+    int koffset, klength;
+    if (cell instanceof SingleBufferKey) {
+      SingleBufferKey sbKey = (SingleBufferKey) cell;
+      key = sbKey.getKeyArray();
+      koffset = sbKey.getKeyOffset();
+      klength = sbKey.getKeyLength();
+    } else {
+      key = CellUtil.createKey(cell);
+      koffset = 0;
+      klength = key.length;
+    }
+    byte[] value = cell.getValueArray();
+    int voffset = cell.getValueOffset();
+    int vlength = cell.getValueLength();
     boolean dupKey = checkKey(key, koffset, klength);
     checkValue(value, voffset, vlength);
     if (!dupKey) {
@@ -263,7 +275,7 @@ public class HFileWriterV2 extends AbstractHFileWriter {
     if (!fsBlockWriter.isWriting())
       newBlock();
 
-    fsBlockWriter.write(kv);
+    fsBlockWriter.write(key, koffset, klength, cell);
 
     totalKeyLength += klength;
     totalValueLength += vlength;
@@ -279,7 +291,7 @@ public class HFileWriterV2 extends AbstractHFileWriter {
     lastKeyOffset = koffset;
     lastKeyLength = klength;
     entryCount++;
-    this.maxMemstoreTS = Math.max(this.maxMemstoreTS, kv.getMvccVersion());
+    this.maxMemstoreTS = Math.max(this.maxMemstoreTS, cell.getSequenceId());
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
index b7885f3..a0a2372 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV3.java
@@ -26,6 +26,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
@@ -77,15 +78,15 @@ public class HFileWriterV3 extends HFileWriterV2 {
    * Add key/value to file. Keys must be added in an order that agrees with the
    * Comparator passed on construction.
    * 
-   * @param kv
-   *          KeyValue to add. Cannot be empty nor null.
+   * @param cell
+   *          Cell to add. Cannot be empty nor null.
    * @throws IOException
    */
   @Override
-  public void append(final KeyValue kv) throws IOException {
+  public void append(final Cell cell) throws IOException {
     // Currently get the complete arrays
-    super.append(kv);
-    int tagsLength = kv.getTagsLength();
+    super.append(cell);
+    int tagsLength = cell.getTagsLength();
     if (tagsLength > this.maxTagsLength) {
       this.maxTagsLength = tagsLength;
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
index b544798..d3d6779 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.java
@@ -20,6 +20,7 @@ import java.io.DataOutputStream;
 import java.io.IOException;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext;
@@ -42,28 +43,27 @@ public class NoOpDataBlockEncoder implements HFileDataBlockEncoder {
   }
 
   @Override
-  public int encode(KeyValue kv, HFileBlockEncodingContext encodingCtx, DataOutputStream out)
-      throws IOException {
-    int klength = kv.getKeyLength();
-    int vlength = kv.getValueLength();
+  public int encode(byte[] key, int koffset, int klength, Cell cell,
+      HFileBlockEncodingContext encodingCtx, DataOutputStream out) throws IOException {
+    int vlength = cell.getValueLength();
 
     out.writeInt(klength);
     out.writeInt(vlength);
-    out.write(kv.getBuffer(), kv.getKeyOffset(), klength);
-    out.write(kv.getValueArray(), kv.getValueOffset(), vlength);
+    out.write(key, koffset, klength);
+    out.write(cell.getValueArray(), cell.getValueOffset(), vlength);
     int encodedKvSize = klength + vlength + KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE;
     // Write the additional tag into the stream
     if (encodingCtx.getHFileContext().isIncludesTags()) {
-      int tagsLength = kv.getTagsLength();
+      int tagsLength = cell.getTagsLength();
       out.writeShort(tagsLength);
       if (tagsLength > 0) {
-        out.write(kv.getTagsArray(), kv.getTagsOffset(), tagsLength);
+        out.write(cell.getTagsArray(), cell.getTagsOffset(), tagsLength);
       }
       encodedKvSize += tagsLength + KeyValue.TAGS_LENGTH_SIZE;
     }
     if (encodingCtx.getHFileContext().isIncludesMvcc()) {
-      WritableUtils.writeVLong(out, kv.getMvccVersion());
-      encodedKvSize += WritableUtils.getVIntSize(kv.getMvccVersion());
+      WritableUtils.writeVLong(out, cell.getSequenceId());
+      encodedKvSize += WritableUtils.getVIntSize(cell.getSequenceId());
     }
     return encodedKvSize;
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
index 27c64f0..87e3ca4 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
@@ -37,6 +37,7 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HDFSBlocksDistribution;
@@ -685,9 +686,9 @@ public class StoreFile {
     private byte[] lastBloomKey;
     private int lastBloomKeyOffset, lastBloomKeyLen;
     private KVComparator kvComparator;
-    private KeyValue lastKv = null;
+    private Cell lastCell = null;
     private long earliestPutTs = HConstants.LATEST_TIMESTAMP;
-    private KeyValue lastDeleteFamilyKV = null;
+    private Cell lastDeleteFamilyCell = null;
     private long deleteFamilyCnt = 0;
 
 
@@ -801,28 +802,28 @@ public class StoreFile {
      *
      * If the timeRangeTracker is not set,
      * update TimeRangeTracker to include the timestamp of this key
-     * @param kv
+     * @param cell
      */
-    public void trackTimestamps(final KeyValue kv) {
-      if (KeyValue.Type.Put.getCode() == kv.getTypeByte()) {
-        earliestPutTs = Math.min(earliestPutTs, kv.getTimestamp());
+    public void trackTimestamps(final Cell cell) {
+      if (KeyValue.Type.Put.getCode() == cell.getTypeByte()) {
+        earliestPutTs = Math.min(earliestPutTs, cell.getTimestamp());
       }
       if (!isTimeRangeTrackerSet) {
-        timeRangeTracker.includeTimestamp(kv);
+        timeRangeTracker.includeTimestamp(cell);
       }
     }
 
-    private void appendGeneralBloomfilter(final KeyValue kv) throws IOException {
+    private void appendGeneralBloomfilter(final Cell cell) throws IOException {
       if (this.generalBloomFilterWriter != null) {
         // only add to the bloom filter on a new, unique key
         boolean newKey = true;
-        if (this.lastKv != null) {
+        if (this.lastCell != null) {
           switch(bloomType) {
           case ROW:
-            newKey = ! kvComparator.matchingRows(kv, lastKv);
+            newKey = ! kvComparator.matchingRows(cell, lastCell);
             break;
           case ROWCOL:
-            newKey = ! kvComparator.matchingRowColumn(kv, lastKv);
+            newKey = ! kvComparator.matchingRowColumn(cell, lastCell);
             break;
           case NONE:
             newKey = false;
@@ -846,17 +847,17 @@ public class StoreFile {
 
           switch (bloomType) {
           case ROW:
-            bloomKey = kv.getRowArray();
-            bloomKeyOffset = kv.getRowOffset();
-            bloomKeyLen = kv.getRowLength();
+            bloomKey = cell.getRowArray();
+            bloomKeyOffset = cell.getRowOffset();
+            bloomKeyLen = cell.getRowLength();
             break;
           case ROWCOL:
             // merge(row, qualifier)
             // TODO: could save one buffer copy in case of compound Bloom
             // filters when this involves creating a KeyValue
-            bloomKey = generalBloomFilterWriter.createBloomKey(kv.getRowArray(),
-                kv.getRowOffset(), kv.getRowLength(), kv.getQualifierArray(),
-                kv.getQualifierOffset(), kv.getQualifierLength());
+            bloomKey = generalBloomFilterWriter.createBloomKey(cell.getRowArray(),
+                cell.getRowOffset(), cell.getRowLength(), cell.getQualifierArray(),
+                cell.getQualifierOffset(), cell.getQualifierLength());
             bloomKeyOffset = 0;
             bloomKeyLen = bloomKey.length;
             break;
@@ -878,14 +879,14 @@ public class StoreFile {
           lastBloomKey = bloomKey;
           lastBloomKeyOffset = bloomKeyOffset;
           lastBloomKeyLen = bloomKeyLen;
-          this.lastKv = kv;
+          this.lastCell = cell;
         }
       }
     }
 
-    private void appendDeleteFamilyBloomFilter(final KeyValue kv)
+    private void appendDeleteFamilyBloomFilter(final Cell cell)
         throws IOException {
-      if (!CellUtil.isDeleteFamily(kv) && !CellUtil.isDeleteFamilyVersion(kv)) {
+      if (!CellUtil.isDeleteFamily(cell) && !CellUtil.isDeleteFamilyVersion(cell)) {
         return;
       }
 
@@ -893,22 +894,22 @@ public class StoreFile {
       deleteFamilyCnt++;
       if (null != this.deleteFamilyBloomFilterWriter) {
         boolean newKey = true;
-        if (lastDeleteFamilyKV != null) {
-          newKey = !kvComparator.matchingRows(kv, lastDeleteFamilyKV);
+        if (lastDeleteFamilyCell != null) {
+          newKey = !kvComparator.matchingRows(cell, lastDeleteFamilyCell);
         }
         if (newKey) {
-          this.deleteFamilyBloomFilterWriter.add(kv.getRowArray(),
-              kv.getRowOffset(), kv.getRowLength());
-          this.lastDeleteFamilyKV = kv;
+          this.deleteFamilyBloomFilterWriter.add(cell.getRowArray(),
+              cell.getRowOffset(), cell.getRowLength());
+          this.lastDeleteFamilyCell = cell;
         }
       }
     }
 
-    public void append(final KeyValue kv) throws IOException {
-      appendGeneralBloomfilter(kv);
-      appendDeleteFamilyBloomFilter(kv);
-      writer.append(kv);
-      trackTimestamps(kv);
+    public void append(final Cell cell) throws IOException {
+      appendGeneralBloomfilter(cell);
+      appendDeleteFamilyBloomFilter(cell);
+      writer.append(cell);
+      trackTimestamps(cell);
     }
 
     public Path getPath() {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
index 891bb6b..4906299 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
@@ -25,7 +25,7 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.regionserver.compactions.Compactor;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -157,7 +157,7 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
     private StoreFile.Writer currentWriter;
     private byte[] currentWriterEndKey;
 
-    private KeyValue lastKv;
+    private Cell lastCell;
     private long kvsInCurrentWriter = 0;
     private int majorRangeFromIndex = -1, majorRangeToIndex = -1;
     private boolean hasAnyWriter = false;
@@ -191,21 +191,21 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
     }
 
     @Override
-    public void append(KeyValue kv) throws IOException {
+    public void append(Cell cell) throws IOException {
       if (currentWriter == null && existingWriters.isEmpty()) {
         // First append ever, do a sanity check.
         sanityCheckLeft(this.boundaries.get(0),
-            kv.getRowArray(), kv.getRowOffset(), kv.getRowLength());
+            cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());
       }
-      prepareWriterFor(kv);
-      currentWriter.append(kv);
-      lastKv = kv; // for the sanity check
+      prepareWriterFor(cell);
+      currentWriter.append(cell);
+      lastCell = cell; // for the sanity check
       ++kvsInCurrentWriter;
     }
 
-    private boolean isKvAfterCurrentWriter(KeyValue kv) {
+    private boolean isCellAfterCurrentWriter(Cell cell) {
       return ((currentWriterEndKey != StripeStoreFileManager.OPEN_KEY) &&
-            (comparator.compareRows(kv.getRowArray(), kv.getRowOffset(), kv.getRowLength(),
+            (comparator.compareRows(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength(),
                 currentWriterEndKey, 0, currentWriterEndKey.length) >= 0));
     }
 
@@ -215,18 +215,18 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
       while (existingWriters.size() < boundaries.size() - 1) {
         createEmptyWriter();
       }
-      if (lastKv != null) {
+      if (lastCell != null) {
         sanityCheckRight(boundaries.get(boundaries.size() - 1),
-            lastKv.getRowArray(), lastKv.getRowOffset(), lastKv.getRowLength());
+            lastCell.getRowArray(), lastCell.getRowOffset(), lastCell.getRowLength());
       }
     }
 
-    private void prepareWriterFor(KeyValue kv) throws IOException {
-      if (currentWriter != null && !isKvAfterCurrentWriter(kv)) return; // Use same writer.
+    private void prepareWriterFor(Cell cell) throws IOException {
+      if (currentWriter != null && !isCellAfterCurrentWriter(cell)) return; // Use same writer.
 
       stopUsingCurrentWriter();
       // See if KV will be past the writer we are about to create; need to add another one.
-      while (isKvAfterCurrentWriter(kv)) {
+      while (isCellAfterCurrentWriter(cell)) {
         checkCanCreateWriter();
         createEmptyWriter();
       }
@@ -293,7 +293,7 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
     private byte[] left;
     private byte[] right;
 
-    private KeyValue lastKv;
+    private Cell lastCell;
     private StoreFile.Writer currentWriter;
     protected byte[] lastRowInCurrentWriter = null;
     private long kvsInCurrentWriter = 0;
@@ -318,16 +318,16 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
     }
 
     @Override
-    public void append(KeyValue kv) throws IOException {
+    public void append(Cell cell) throws IOException {
       // If we are waiting for opportunity to close and we started writing different row,
       // discard the writer and stop waiting.
       boolean doCreateWriter = false;
       if (currentWriter == null) {
         // First append ever, do a sanity check.
-        sanityCheckLeft(left, kv.getRowArray(), kv.getRowOffset(), kv.getRowLength());
+        sanityCheckLeft(left, cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());
         doCreateWriter = true;
       } else if (lastRowInCurrentWriter != null
-          && !comparator.matchingRows(kv.getRowArray(), kv.getRowOffset(), kv.getRowLength(),
+          && !comparator.matchingRows(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength(),
               lastRowInCurrentWriter, 0, lastRowInCurrentWriter.length)) {
         if (LOG.isDebugEnabled()) {
           LOG.debug("Stopping to use a writer after [" + Bytes.toString(lastRowInCurrentWriter)
@@ -339,7 +339,7 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
         doCreateWriter = true;
       }
       if (doCreateWriter) {
-        byte[] boundary = existingWriters.isEmpty() ? left : kv.getRow(); // make a copy
+        byte[] boundary = existingWriters.isEmpty() ? left : cell.getRow(); // make a copy
         if (LOG.isDebugEnabled()) {
           LOG.debug("Creating new writer starting at [" + Bytes.toString(boundary) + "]");
         }
@@ -348,8 +348,8 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
         existingWriters.add(currentWriter);
       }
 
-      currentWriter.append(kv);
-      lastKv = kv; // for the sanity check
+      currentWriter.append(cell);
+      lastCell = cell; // for the sanity check
       ++kvsInCurrentWriter;
       kvsSeen = kvsInCurrentWriter;
       if (this.sourceScanner != null) {
@@ -362,7 +362,7 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
       if (lastRowInCurrentWriter == null
           && existingWriters.size() < targetCount
           && kvsSeen >= targetKvs) {
-        lastRowInCurrentWriter = kv.getRow(); // make a copy
+        lastRowInCurrentWriter = cell.getRow(); // make a copy
         if (LOG.isDebugEnabled()) {
           LOG.debug("Preparing to start a new writer after [" + Bytes.toString(
               lastRowInCurrentWriter) + "] row; observed " + kvsSeen + " kvs and wrote out "
@@ -378,9 +378,9 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
             ((this.sourceScanner == null) ? "" : ("; observed estimated "
                 + this.sourceScanner.getEstimatedNumberOfKvsScanned() + " KVs total")));
       }
-      if (lastKv != null) {
+      if (lastCell != null) {
         sanityCheckRight(
-            right, lastKv.getRowArray(), lastKv.getRowOffset(), lastKv.getRowLength());
+            right, lastCell.getRowArray(), lastCell.getRowOffset(), lastCell.getRowLength());
       }
 
       // When expired stripes were going to be merged into one, and if no writer was created during
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java
index 1546c55..8f5585c 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java
@@ -23,6 +23,7 @@ import java.io.DataOutput;
 import java.io.IOException;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
@@ -66,14 +67,14 @@ public class TimeRangeTracker implements Writable {
   }
 
   /**
-   * Update the current TimestampRange to include the timestamp from KeyValue
+   * Update the current TimestampRange to include the timestamp from Cell
    * If the Key is of type DeleteColumn or DeleteFamily, it includes the
    * entire time range from 0 to timestamp of the key.
-   * @param kv the KeyValue to include
+   * @param cell the Cell to include
    */
-  public void includeTimestamp(final KeyValue kv) {
-    includeTimestamp(kv.getTimestamp());
-    if (CellUtil.isDeleteColumnOrFamily(kv)) {
+  public void includeTimestamp(final Cell cell) {
+    includeTimestamp(cell.getTimestamp());
+    if (CellUtil.isDeleteColumnOrFamily(cell)) {
       includeTimestamp(0);
     }
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
index 2b053a6..37fe987 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
@@ -30,7 +30,6 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.compress.Compression;
@@ -75,11 +74,8 @@ public abstract class Compactor {
       HConstants.MIN_KEEP_SEQID_PERIOD), HConstants.MIN_KEEP_SEQID_PERIOD);
   }
 
-  /**
-   * TODO: Replace this with CellOutputStream when StoreFile.Writer uses cells.
-   */
   public interface CellSink {
-    void append(KeyValue kv) throws IOException;
+    void append(Cell cell) throws IOException;
   }
 
   public CompactionProgress getProgress() {
@@ -227,7 +223,7 @@ public abstract class Compactor {
     int bytesWritten = 0;
     // Since scanner.next() can return 'false' but still be delivering data,
     // we have to use a do/while loop.
-    List<Cell> kvs = new ArrayList<Cell>();
+    List<Cell> cells = new ArrayList<Cell>();
     int closeCheckInterval = HStore.getCloseCheckInterval();
     long lastMillis;
     if (LOG.isDebugEnabled()) {
@@ -237,20 +233,20 @@ public abstract class Compactor {
     }
     boolean hasMore;
     do {
-      hasMore = scanner.next(kvs, compactionKVMax);
+      hasMore = scanner.next(cells, compactionKVMax);
       // output to writer:
-      for (Cell c : kvs) {
-        KeyValue kv = KeyValueUtil.ensureKeyValue(c);
-        if (cleanSeqId && kv.getSequenceId() <= smallestReadPoint) {
-          CellUtil.setSequenceId(kv, 0);
+      for (Cell c : cells) {
+        if (cleanSeqId && c.getSequenceId() <= smallestReadPoint) {
+          CellUtil.setSequenceId(c, 0);
         }
-        writer.append(kv);
+        writer.append(c);
         ++progress.currentCompactedKVs;
-        progress.totalCompactedSize += kv.getLength();
+        int len = KeyValueUtil.length(c);
+        progress.totalCompactedSize += len;
 
         // check periodically to see if a system stop is requested
         if (closeCheckInterval > 0) {
-          bytesWritten += kv.getLength();
+          bytesWritten += len;
           if (bytesWritten > closeCheckInterval) {
             // Log the progress of long running compactions every minute if
             // logging at DEBUG level
@@ -270,7 +266,7 @@ public abstract class Compactor {
           }
         }
       }
-      kvs.clear();
+      cells.clear();
     } while (hasMore);
     progress.complete();
     return true;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
index 15c32ba..bb5d59b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
@@ -223,7 +223,8 @@ public class TestDataBlockEncoders {
     DataOutputStream dos = new DataOutputStream(baos);
     encoder.startBlockEncoding(encodingContext, dos);
     for (KeyValue kv : kvs) {
-      encoder.encode(kv, encodingContext, dos);
+      encoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv, encodingContext,
+          dos);
     }
     BufferGrabbingByteArrayOutputStream stream = new BufferGrabbingByteArrayOutputStream();
     baos.writeTo(stream);
@@ -367,7 +368,8 @@ public class TestDataBlockEncoders {
       DataOutputStream dos = new DataOutputStream(baos);
       encoder.startBlockEncoding(encodingContext, dos);
       for (KeyValue kv : kvList) {
-        encoder.encode(kv, encodingContext, dos);
+        encoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv, encodingContext,
+            dos);
       }
       BufferGrabbingByteArrayOutputStream stream = new BufferGrabbingByteArrayOutputStream();
       baos.writeTo(stream);
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
index 034771c..bb5890d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestPrefixTreeEncoding.java
@@ -287,7 +287,8 @@ public class TestPrefixTreeEncoding {
     }
     encoder.startBlockEncoding(blkEncodingCtx, userDataStream);
     for (KeyValue kv : kvset) {
-      encoder.encode(kv, blkEncodingCtx, userDataStream);
+      encoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv, blkEncodingCtx,
+          userDataStream);
     }
     encoder.endBlockEncoding(blkEncodingCtx, userDataStream, null);
   }
@@ -315,7 +316,8 @@ public class TestPrefixTreeEncoding {
     }
     encoder.startBlockEncoding(blkEncodingCtx, userDataStream);
     for (KeyValue kv : kvset) {
-      encoder.encode(kv, blkEncodingCtx, userDataStream);
+      encoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv, blkEncodingCtx,
+          userDataStream);
     }
     encoder.endBlockEncoding(blkEncodingCtx, userDataStream, null);
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
index 09561cb..4acd9d5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
@@ -179,7 +179,7 @@ public class TestHFileBlock {
         kv.setSequenceId(memstoreTS);
         totalSize += WritableUtils.getVIntSize(memstoreTS);
       }
-      hbw.write(kv);
+      hbw.write(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv);
     }
     return totalSize;
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
index 88fdb77..fa1e5fa 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
@@ -38,9 +38,10 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
@@ -447,6 +448,7 @@ public class TestHFileBlockCompatibility {
      * @return the stream the user can write their data into
      * @throws IOException
      */
+    @Override
     public DataOutputStream startWriting(BlockType newBlockType)
         throws IOException {
       if (state == State.BLOCK_READY && startOffset != -1) {
@@ -472,12 +474,14 @@ public class TestHFileBlockCompatibility {
       return userDataStream;
     }
 
-    public void write(KeyValue kv) throws IOException{
+    @Override
+    public void write(byte[] key, int koffset, int klength, Cell cell) throws IOException {
       expectState(State.WRITING);
-      this.dataBlockEncoder.encode(kv, dataBlockEncodingCtx, this.userDataStream);
-      this.unencodedDataSizeWritten += kv.getLength();
+      this.dataBlockEncoder.encode(key, koffset, klength, cell, dataBlockEncodingCtx,
+          this.userDataStream);
+      this.unencodedDataSizeWritten += KeyValueUtil.length(cell);
       if (dataBlockEncodingCtx.getHFileContext().isIncludesMvcc()) {
-        this.unencodedDataSizeWritten += WritableUtils.getVIntSize(kv.getMvccVersion());
+        this.unencodedDataSizeWritten += WritableUtils.getVIntSize(cell.getSequenceId());
       }
     }
 
@@ -488,6 +492,7 @@ public class TestHFileBlockCompatibility {
      *
      * @return the data output stream for the user to write to
      */
+    @Override
     DataOutputStream getUserDataStream() {
       expectState(State.WRITING);
       return userDataStream;
@@ -497,6 +502,7 @@ public class TestHFileBlockCompatibility {
      * Transitions the block writer from the "writing" state to the "block
      * ready" state.  Does nothing if a block is already finished.
      */
+    @Override
     void ensureBlockReady() throws IOException {
       Preconditions.checkState(state != State.INIT,
           "Unexpected state: " + state);
@@ -569,6 +575,7 @@ public class TestHFileBlockCompatibility {
      * @param out
      * @throws IOException
      */
+    @Override
     public void writeHeaderAndData(FSDataOutputStream out) throws IOException {
       long offset = out.getPos();
       if (startOffset != -1 && offset != startOffset) {
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
index 3f2c84b..c4679a2 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
@@ -189,7 +189,7 @@ public class TestHFileDataBlockEncoder {
     DataOutputStream dos = new DataOutputStream(baos);
     blockEncoder.startBlockEncoding(context, dos);
     for (KeyValue kv : kvs) {
-      blockEncoder.encode(kv, context, dos);
+      blockEncoder.encode(kv.getKeyArray(), kv.getKeyOffset(), kv.getKeyLength(), kv, context, dos);
     }
     BufferGrabbingByteArrayOutputStream stream = new BufferGrabbingByteArrayOutputStream();
     baos.writeTo(stream);
