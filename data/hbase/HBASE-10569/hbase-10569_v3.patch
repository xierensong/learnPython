diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
index 0b97c3f..e7d7f36 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java
@@ -129,7 +129,7 @@ public class CatalogTracker {
    * @throws IOException
    */
   public CatalogTracker(final Configuration conf) throws IOException {
-    this(null, conf, null);
+    this(null, conf, HConnectionManager.getConnection(conf), null);
   }
 
   /**
@@ -145,18 +145,14 @@ public class CatalogTracker {
    * @throws IOException
    */
   public CatalogTracker(final ZooKeeperWatcher zk, final Configuration conf,
-      Abortable abortable)
-  throws IOException {
-    this(zk, conf, HConnectionManager.getConnection(conf), abortable);
-  }
-
-  public CatalogTracker(final ZooKeeperWatcher zk, final Configuration conf,
       HConnection connection, Abortable abortable)
   throws IOException {
     this.connection = connection;
     if (abortable == null) {
       // A connection is abortable.
       this.abortable = this.connection;
+    } else {
+      this.abortable = abortable;
     }
     Abortable throwableAborter = new Abortable() {
 
@@ -322,6 +318,7 @@ public class CatalogTracker {
    * invocation, or may be null.
    * @throws IOException
    */
+  @SuppressWarnings("deprecation")
   private AdminService.BlockingInterface getCachedConnection(ServerName sn)
   throws IOException {
     if (sn == null) {
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionAdapter.java hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionAdapter.java
new file mode 100644
index 0000000..167a24f
--- /dev/null
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionAdapter.java
@@ -0,0 +1,409 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HRegionLocation;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.MasterNotRunningException;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.ZooKeeperConnectionException;
+import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.AdminService;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MasterService;
+
+/**
+ * An internal class that adapts a {@link HConnection}.
+ * HConnection is created from HConnectionManager. The default
+ * implementation talks to region servers over RPC since it
+ * doesn't know if the connection is used by one region server
+ * itself. This adapter makes it possible to change some of the
+ * default logic. Especially, when the connection is used
+ * internally by some the region server.
+ *
+ * @see ConnectionUtils#createShortCircuitHConnection(HConnection, ServerName,
+ * AdminService.BlockingInterface, ClientService.BlockingInterface)
+ */
+@InterfaceAudience.Private
+@SuppressWarnings("deprecation")
+//NOTE: DO NOT make this class public. It was made package-private on purpose.
+class ConnectionAdapter implements ClusterConnection {
+
+  private final ClusterConnection wrappedConnection;
+
+  public ConnectionAdapter(HConnection c) {
+    wrappedConnection = (ClusterConnection)c;
+  }
+
+  @Override
+  public void abort(String why, Throwable e) {
+    wrappedConnection.abort(why, e);
+  }
+
+  @Override
+  public boolean isAborted() {
+    return wrappedConnection.isAborted();
+  }
+
+  @Override
+  public void close() throws IOException {
+    wrappedConnection.close();
+  }
+
+  @Override
+  public Configuration getConfiguration() {
+    return wrappedConnection.getConfiguration();
+  }
+
+  @Override
+  public HTableInterface getTable(String tableName) throws IOException {
+    return wrappedConnection.getTable(tableName);
+  }
+
+  @Override
+  public HTableInterface getTable(byte[] tableName) throws IOException {
+    return wrappedConnection.getTable(tableName);
+  }
+
+  @Override
+  public HTableInterface getTable(TableName tableName) throws IOException {
+    return wrappedConnection.getTable(tableName);
+  }
+
+  @Override
+  public HTableInterface getTable(String tableName, ExecutorService pool)
+      throws IOException {
+    return wrappedConnection.getTable(tableName, pool);
+  }
+
+  @Override
+  public HTableInterface getTable(byte[] tableName, ExecutorService pool)
+      throws IOException {
+    return wrappedConnection.getTable(tableName, pool);
+  }
+
+  @Override
+  public HTableInterface getTable(TableName tableName, ExecutorService pool)
+      throws IOException {
+    return wrappedConnection.getTable(tableName, pool);
+  }
+
+  @Override
+  public boolean isMasterRunning() throws MasterNotRunningException,
+      ZooKeeperConnectionException {
+    return wrappedConnection.isMasterRunning();
+  }
+
+  @Override
+  public boolean isTableEnabled(TableName tableName) throws IOException {
+    return wrappedConnection.isTableEnabled(tableName);
+  }
+
+  @Override
+  public boolean isTableEnabled(byte[] tableName) throws IOException {
+    return wrappedConnection.isTableEnabled(tableName);
+  }
+
+  @Override
+  public boolean isTableDisabled(TableName tableName) throws IOException {
+    return wrappedConnection.isTableDisabled(tableName);
+  }
+
+  @Override
+  public boolean isTableDisabled(byte[] tableName) throws IOException {
+    return wrappedConnection.isTableDisabled(tableName);
+  }
+
+  @Override
+  public boolean isTableAvailable(TableName tableName) throws IOException {
+    return wrappedConnection.isTableAvailable(tableName);
+  }
+
+  @Override
+  public boolean isTableAvailable(byte[] tableName) throws IOException {
+    return wrappedConnection.isTableAvailable(tableName);
+  }
+
+  @Override
+  public boolean isTableAvailable(TableName tableName, byte[][] splitKeys)
+      throws IOException {
+    return wrappedConnection.isTableAvailable(tableName, splitKeys);
+  }
+
+  @Override
+  public boolean isTableAvailable(byte[] tableName, byte[][] splitKeys)
+      throws IOException {
+    return wrappedConnection.isTableAvailable(tableName, splitKeys);
+  }
+
+  @Override
+  public HTableDescriptor[] listTables() throws IOException {
+    return wrappedConnection.listTables();
+  }
+
+  @Override
+  public String[] getTableNames() throws IOException {
+    return wrappedConnection.getTableNames();
+  }
+
+  @Override
+  public TableName[] listTableNames() throws IOException {
+    return wrappedConnection.listTableNames();
+  }
+
+  @Override
+  public HTableDescriptor getHTableDescriptor(TableName tableName)
+      throws IOException {
+    return wrappedConnection.getHTableDescriptor(tableName);
+  }
+
+  @Override
+  public HTableDescriptor getHTableDescriptor(byte[] tableName)
+      throws IOException {
+    return wrappedConnection.getHTableDescriptor(tableName);
+  }
+
+  @Override
+  public HRegionLocation locateRegion(TableName tableName, byte[] row)
+      throws IOException {
+    return wrappedConnection.locateRegion(tableName, row);
+  }
+
+  @Override
+  public HRegionLocation locateRegion(byte[] tableName, byte[] row)
+      throws IOException {
+    return wrappedConnection.locateRegion(tableName, row);
+  }
+
+  @Override
+  public void clearRegionCache() {
+    wrappedConnection.clearRegionCache();
+  }
+
+  @Override
+  public void clearRegionCache(TableName tableName) {
+    wrappedConnection.clearRegionCache(tableName);
+  }
+
+  @Override
+  public void clearRegionCache(byte[] tableName) {
+    wrappedConnection.clearRegionCache(tableName);
+  }
+
+  @Override
+  public void deleteCachedRegionLocation(HRegionLocation location) {
+    wrappedConnection.deleteCachedRegionLocation(location);
+  }
+
+  @Override
+  public HRegionLocation relocateRegion(TableName tableName, byte[] row)
+      throws IOException {
+    return wrappedConnection.relocateRegion(tableName, row);
+  }
+
+  @Override
+  public HRegionLocation relocateRegion(byte[] tableName, byte[] row)
+      throws IOException {
+    return wrappedConnection.relocateRegion(tableName, row);
+  }
+
+  @Override
+  public void updateCachedLocations(TableName tableName, byte[] rowkey,
+      Object exception, HRegionLocation source) {
+    wrappedConnection.updateCachedLocations(tableName, rowkey, exception, source);
+  }
+
+  @Override
+  public void updateCachedLocations(TableName tableName, byte[] rowkey,
+      Object exception, ServerName source) {
+    wrappedConnection.updateCachedLocations(tableName, rowkey, exception, source);
+  }
+
+  @Override
+  public void updateCachedLocations(byte[] tableName, byte[] rowkey,
+      Object exception, HRegionLocation source) {
+    wrappedConnection.updateCachedLocations(tableName, rowkey, exception, source);
+  }
+
+  @Override
+  public HRegionLocation locateRegion(byte[] regionName) throws IOException {
+    return wrappedConnection.locateRegion(regionName);
+  }
+
+  @Override
+  public List<HRegionLocation> locateRegions(TableName tableName)
+      throws IOException {
+    return wrappedConnection.locateRegions(tableName);
+  }
+
+  @Override
+  public List<HRegionLocation> locateRegions(byte[] tableName)
+      throws IOException {
+    return wrappedConnection.locateRegions(tableName);
+  }
+
+  @Override
+  public List<HRegionLocation> locateRegions(TableName tableName,
+      boolean useCache, boolean offlined) throws IOException {
+    return wrappedConnection.locateRegions(tableName, useCache, offlined);
+  }
+
+  @Override
+  public List<HRegionLocation> locateRegions(byte[] tableName,
+      boolean useCache, boolean offlined) throws IOException {
+    return wrappedConnection.locateRegions(tableName, useCache, offlined);
+  }
+
+  @Override
+  public MasterService.BlockingInterface getMaster() throws IOException {
+    return wrappedConnection.getMaster();
+  }
+
+  @Override
+  public AdminService.BlockingInterface getAdmin(
+      ServerName serverName) throws IOException {
+    return wrappedConnection.getAdmin(serverName);
+  }
+
+  @Override
+  public ClientService.BlockingInterface getClient(
+      ServerName serverName) throws IOException {
+    return wrappedConnection.getClient(serverName);
+  }
+
+  @Override
+  public AdminService.BlockingInterface getAdmin(
+      ServerName serverName, boolean getMaster) throws IOException {
+    return wrappedConnection.getAdmin(serverName, getMaster);
+  }
+
+  @Override
+  public HRegionLocation getRegionLocation(TableName tableName, byte[] row,
+      boolean reload) throws IOException {
+    return wrappedConnection.getRegionLocation(tableName, row, reload);
+  }
+
+  @Override
+  public HRegionLocation getRegionLocation(byte[] tableName, byte[] row,
+      boolean reload) throws IOException {
+    return wrappedConnection.getRegionLocation(tableName, row, reload);
+  }
+
+  @Override
+  public void processBatch(List<? extends Row> actions, TableName tableName,
+      ExecutorService pool, Object[] results) throws IOException,
+      InterruptedException {
+    wrappedConnection.processBatch(actions, tableName, pool, results);
+  }
+
+  @Override
+  public void processBatch(List<? extends Row> actions, byte[] tableName,
+      ExecutorService pool, Object[] results) throws IOException,
+      InterruptedException {
+    wrappedConnection.processBatch(actions, tableName, pool, results);
+  }
+
+  @Override
+  public <R> void processBatchCallback(List<? extends Row> list,
+      TableName tableName, ExecutorService pool, Object[] results,
+      Callback<R> callback) throws IOException, InterruptedException {
+    wrappedConnection.processBatchCallback(list, tableName, pool, results, callback);
+  }
+
+  @Override
+  public <R> void processBatchCallback(List<? extends Row> list,
+      byte[] tableName, ExecutorService pool, Object[] results,
+      Callback<R> callback) throws IOException, InterruptedException {
+    wrappedConnection.processBatchCallback(list, tableName, pool, results, callback);
+  }
+
+  @Override
+  public void setRegionCachePrefetch(TableName tableName, boolean enable) {
+    wrappedConnection.setRegionCachePrefetch(tableName, enable);
+  }
+
+  @Override
+  public void setRegionCachePrefetch(byte[] tableName, boolean enable) {
+    wrappedConnection.setRegionCachePrefetch(tableName, enable);
+  }
+
+  @Override
+  public boolean getRegionCachePrefetch(TableName tableName) {
+    return wrappedConnection.getRegionCachePrefetch(tableName);
+  }
+
+  @Override
+  public boolean getRegionCachePrefetch(byte[] tableName) {
+     return wrappedConnection.getRegionCachePrefetch(tableName);
+  }
+
+  @Override
+  public int getCurrentNrHRS() throws IOException {
+    return wrappedConnection.getCurrentNrHRS();
+  }
+
+  @Override
+  public HTableDescriptor[] getHTableDescriptorsByTableName(
+      List<TableName> tableNames) throws IOException {
+    return wrappedConnection.getHTableDescriptorsByTableName(tableNames);
+  }
+
+  @Override
+  public HTableDescriptor[] getHTableDescriptors(List<String> tableNames)
+      throws IOException {
+    return wrappedConnection.getHTableDescriptors(tableNames);
+  }
+
+  @Override
+  public boolean isClosed() {
+    return wrappedConnection.isClosed();
+  }
+
+  @Override
+  public void clearCaches(ServerName sn) {
+    wrappedConnection.clearCaches(sn);
+  }
+
+  @Override
+  public MasterKeepAliveConnection getKeepAliveMasterService()
+      throws MasterNotRunningException {
+    return wrappedConnection.getKeepAliveMasterService();
+  }
+
+  @Override
+  public boolean isDeadServer(ServerName serverName) {
+    return wrappedConnection.isDeadServer(serverName);
+  }
+
+  @Override
+  public NonceGenerator getNonceGenerator() {
+    return wrappedConnection.getNonceGenerator();
+  }
+
+  @Override
+  public AsyncProcess getAsyncProcess() {
+    return wrappedConnection.getAsyncProcess();
+  }
+}
\ No newline at end of file
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
index 1e04b7f..4117b64 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/client/ConnectionUtils.java
@@ -17,12 +17,16 @@
  */
 package org.apache.hadoop.hbase.client;
 
+import java.io.IOException;
 import java.util.Random;
 
 import org.apache.commons.logging.Log;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.AdminService;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
 
 /**
  * Utility used by client connections.
@@ -92,4 +96,31 @@ public class ConnectionUtils {
     c.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, retries);
     log.debug(sn + " HConnection server-to-server retries=" + retries);
   }
+
+  /**
+   * Adapt a HConnection so that it can bypass the RPC layer (serialization,
+   * deserialization, networking, etc..) when it talks to a local server.
+   * @param conn the connection to adapt
+   * @param serverName the local server name
+   * @param admin the admin interface of the local server
+   * @param client the client interface of the local server
+   * @return an adapted/decorated HConnection
+   */
+  public static HConnection createShortCircuitHConnection(final HConnection conn,
+      final ServerName serverName, final AdminService.BlockingInterface admin,
+      final ClientService.BlockingInterface client) {
+    return new ConnectionAdapter(conn) {
+      @Override
+      public AdminService.BlockingInterface getAdmin(
+          ServerName sn, boolean getMaster) throws IOException {
+        return serverName.equals(sn) ? admin : super.getAdmin(sn, getMaster);
+      }
+
+      @Override
+      public ClientService.BlockingInterface getClient(
+          ServerName sn) throws IOException {
+        return serverName.equals(sn) ? client : super.getClient(sn);
+      }
+    };
+  }
 }
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
index f1e0b16..9ae00f9 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/DistributedHBaseCluster.java
@@ -136,7 +136,7 @@ public class DistributedHBaseCluster extends HBaseCluster {
   }
 
   @Override
-  public MasterService.BlockingInterface getMaster()
+  public MasterService.BlockingInterface getMasterAdminService()
   throws IOException {
     HConnection conn = HConnectionManager.getConnection(conf);
     return conn.getMaster();
@@ -170,7 +170,7 @@ public class DistributedHBaseCluster extends HBaseCluster {
     long start = System.currentTimeMillis();
     while (System.currentTimeMillis() - start < timeout) {
       try {
-        getMaster();
+        getMasterAdminService();
         return true;
       } catch (MasterNotRunningException m) {
         LOG.warn("Master not started yet " + m);
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
index e22dd7f..3ead509 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/Action.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hbase.chaos.actions;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.LinkedList;
 import java.util.List;
@@ -58,11 +59,25 @@ public class Action {
 
   public void perform() throws Exception { }
 
-  /** Returns current region servers */
+  /** Returns current region servers - active master */
   protected ServerName[] getCurrentServers() throws IOException {
-    Collection<ServerName> regionServers = cluster.getClusterStatus().getServers();
-    if (regionServers == null || regionServers.size() <= 0) return new ServerName [] {};
-    return regionServers.toArray(new ServerName[regionServers.size()]);
+    ClusterStatus clusterStatus = cluster.getClusterStatus();
+    Collection<ServerName> regionServers = clusterStatus.getServers();
+    int count = regionServers == null ? 0 : regionServers.size();
+    if (count <= 0) {
+      return new ServerName [] {};
+    }
+    ServerName master = clusterStatus.getMaster();
+    if (master == null || !regionServers.contains(master)) {
+      return regionServers.toArray(new ServerName[count]);
+    }
+    if (count == 1) {
+      return new ServerName [] {};
+    }
+    ArrayList<ServerName> tmp = new ArrayList<ServerName>(count);
+    tmp.addAll(regionServers);
+    tmp.remove(master);
+    return tmp.toArray(new ServerName[count-1]);
   }
 
   protected void killMaster(ServerName server) throws IOException {
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRsHoldingMetaAction.java hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRsHoldingMetaAction.java
index 4efc8f2..a6b4fc7 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRsHoldingMetaAction.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/chaos/actions/RestartRsHoldingMetaAction.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hbase.chaos.actions;
 
+import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.ServerName;
 
 /**
@@ -35,6 +36,12 @@ public class RestartRsHoldingMetaAction extends RestartActionBaseAction {
       LOG.warn("No server is holding hbase:meta right now.");
       return;
     }
-    restartRs(server, sleepTime);
+    ClusterStatus clusterStatus = cluster.getClusterStatus();
+    if (server.equals(clusterStatus.getMaster())) {
+      // Master holds the meta, so restart the master.
+      restartMaster(server, sleepTime);
+    } else {
+      restartRs(server, sleepTime);
+    }
   }
 }
diff --git hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon
index e6c8613..209d0aa 100644
--- hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon
+++ hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.jamon
@@ -44,9 +44,9 @@ ServerName [] serverNames = masters.toArray(new ServerName[masters.size()]);
 <%if (!master.isActiveMaster()) %>
     <%if serverNames[0] != null %>
         <h2>Master</h2>
-        <a href="//<% serverNames[0].getHostname() %>:
-	   <% master.getConfiguration().getInt("hbase.master.info.port", 16010) %>/master-status"
-	     target="_blank">
+        <a href="//<% serverNames[0].getHostname() %>:<%
+        master.getConfiguration().getInt("hbase.master.info.port", 16010)
+        %>/master-status" target="_blank">
 	   <% serverNames[0].getHostname() %>
 	</a>
     <%else>
@@ -66,9 +66,9 @@ ServerName [] serverNames = masters.toArray(new ServerName[masters.size()]);
     for (ServerName serverName : serverNames) {
     </%java>
     <tr>
-        <td><a href="//<% serverName.getHostname() %>:
-	  <% master.getConfiguration().getInt("hbase.master.info.port", 16010) %>/master-status"
-	     target="_blank">
+        <td><a href="//<% serverName.getHostname() %>:<%
+        master.getConfiguration().getInt("hbase.master.info.port", 16010)
+        %>/master-status" target="_blank">
 	  <% serverName.getHostname() %></a>
 	</td>
         <td><% serverName.getPort() %></td>
diff --git hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
index e0617da..4b5a160 100644
--- hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
+++ hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
@@ -63,7 +63,7 @@ AssignmentManager assignmentManager = master.getAssignmentManager();
 <%class>
   public String formatZKString() {
     StringBuilder quorums = new StringBuilder();
-    String zkQuorum = master.getZooKeeperWatcher().getQuorum();
+    String zkQuorum = master.getZooKeeper().getQuorum();
 
     if (null == zkQuorum) {
       return quorums.toString();
@@ -278,7 +278,7 @@ AssignmentManager assignmentManager = master.getAssignmentManager();
 	                </%if>
 	                <tr>
 	                    <td>Coprocessors</td>
-	                    <td><% java.util.Arrays.toString(master.getCoprocessors()) %></td>
+	                    <td><% java.util.Arrays.toString(master.getMasterCoprocessors()) %></td>
 	                    <td>Coprocessors currently loaded by the master</td>
 	                </tr>
                 </%if>
diff --git hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl.jamon hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl.jamon
index 022ca41..f063e74 100644
--- hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl.jamon
+++ hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl.jamon
@@ -283,7 +283,7 @@ if  (sl.getTotalCompactingKVs() > 0) {
         </%args>
         <%java>
         int infoPort = master.getRegionServerInfoPort(serverName);
-        String url = "//" + serverName.getHostname() + ":" + infoPort + "/";
+        String url = "//" + serverName.getHostname() + ":" + infoPort + "/rs-status";
         </%java>
 
         <%if (infoPort > 0) %>
diff --git hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon
index 1efb0b0..4b7a39e 100644
--- hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon
+++ hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.jamon
@@ -36,10 +36,10 @@ org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
   <%java return; %>
 </%if>
 <%java>
-  ServerInfo serverInfo = ProtobufUtil.getServerInfo(regionServer);
+  ServerInfo serverInfo = ProtobufUtil.getServerInfo(regionServer.getRSRpcServices());
   ServerName serverName = ProtobufUtil.toServerName(serverInfo.getServerName());
-  List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(regionServer);
-  int masterInfoPort = regionServer.getConfiguration().getInt("hbase.master.info.port", 16010);
+  List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(regionServer.getRSRpcServices());
+  int infoPort = regionServer.getConfiguration().getInt("hbase.master.info.port", 16010);
   MasterAddressTracker masterAddressTracker = regionServer.getMasterAddressTracker();
   ServerName masterServerName = masterAddressTracker == null ? null
     : masterAddressTracker.getMasterAddress();
@@ -98,7 +98,7 @@ org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
 
     <section>
     <h2>Server Metrics</h2>
-    <& ServerMetricsTmpl; mWrap = regionServer.getMetrics().getRegionServerWrapper(); &>
+    <& ServerMetricsTmpl; mWrap = regionServer.getRegionServerMetrics().getRegionServerWrapper(); &>
     </section>
 
     <section>
@@ -135,7 +135,7 @@ org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
         </tr>
         <tr>
             <td>Coprocessors</td>
-            <td><% java.util.Arrays.toString(regionServer.getCoprocessors()) %></td>
+            <td><% java.util.Arrays.toString(regionServer.getRegionServerCoprocessors()) %></td>
             <td>Coprocessors currently loaded by this regionserver</td>
         </tr>
         <tr>
@@ -146,14 +146,12 @@ org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
         <tr>
             <td>HBase Master</td>
             <td>
-                <%if (masterInfoPort < 0) %>
-                No hbase.master.info.port found
-                <%elseif masterServerName == null %>
+                <%if masterServerName == null %>
                 No master found
                 <%else>
                 <%java>
-                String host = masterServerName.getHostname() + ":" + masterInfoPort;
-                String url = "//" + host + "/";
+                String host = masterServerName.getHostname() + ":" + infoPort;
+                String url = "//" + host + "/master-status";
                 </%java>
                 <a href="<% url %>"><% host %></a>
                 </%if>
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/CoprocessorHConnection.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/CoprocessorHConnection.java
index 536bbf0..789cbba 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/CoprocessorHConnection.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/CoprocessorHConnection.java
@@ -32,25 +32,12 @@ import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.client.HConnection;
-import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.client.HTableInterface;
 import org.apache.hadoop.hbase.client.Row;
 import org.apache.hadoop.hbase.client.coprocessor.Batch.Callback;
 import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
-import org.apache.hadoop.hbase.ipc.RpcServerInterface;
-import org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler;
-import org.apache.hadoop.hbase.monitoring.TaskMonitor;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
-import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
-
-import com.google.protobuf.BlockingRpcChannel;
-import com.google.protobuf.BlockingService;
-import com.google.protobuf.Descriptors.MethodDescriptor;
-import com.google.protobuf.Message;
-import com.google.protobuf.RpcController;
-import com.google.protobuf.ServiceException;
 
 /**
  * Connection to an HTable from within a Coprocessor. We can do some nice tricks since we know we
@@ -106,28 +93,7 @@ public class CoprocessorHConnection implements ClusterConnection {
     }
     // the client is attempting to write to the same regionserver, we can short-circuit to our
     // local regionserver
-    final BlockingService blocking = ClientService.newReflectiveBlockingService(this.server);
-    final RpcServerInterface rpc = this.server.getRpcServer();
-
-    final MonitoredRPCHandler status =
-        TaskMonitor.get().createRPCStatus(Thread.currentThread().getName());
-    status.pause("Setting up server-local call");
-
-    final long timestamp = EnvironmentEdgeManager.currentTimeMillis();
-    BlockingRpcChannel channel = new BlockingRpcChannel() {
-
-      @Override
-      public Message callBlockingMethod(MethodDescriptor method, RpcController controller,
-          Message request, Message responsePrototype) throws ServiceException {
-        try {
-          // we never need a cell-scanner - everything is already fully formed
-          return rpc.call(blocking, method, request, null, timestamp, status).getFirst();
-        } catch (IOException e) {
-          throw new ServiceException(e);
-        }
-      }
-    };
-    return ClientService.newBlockingStub(channel);
+    return server.getRSRpcServices();
   }
 
   public void abort(String why, Throwable e) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
index 46c8d11..d063742 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServer.java
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hbase.ipc;
 
-import static org.apache.hadoop.fs.CommonConfigurationKeys.HADOOP_SECURITY_AUTHORIZATION;
+import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION;
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
@@ -121,7 +121,6 @@ import com.google.protobuf.Message;
 import com.google.protobuf.Message.Builder;
 import com.google.protobuf.ServiceException;
 import com.google.protobuf.TextFormat;
-// Uses Writables doing sasl
 
 /**
  * An RPC server that hosts protobuf described Services.
@@ -254,7 +253,7 @@ public class RpcServer implements RpcServerInterface {
 
   private final int warnResponseTime;
   private final int warnResponseSize;
-  private final Object serverInstance;
+  private final Server server;
   private final List<BlockingServiceAndInterface> services;
 
   private final RpcScheduler scheduler;
@@ -318,6 +317,7 @@ public class RpcServer implements RpcServerInterface {
      * Short string representation without param info because param itself could be huge depends on
      * the payload of a command
      */
+    @SuppressWarnings("deprecation")
     String toShortString() {
       String serviceName = this.connection.service != null?
         this.connection.service.getDescriptorForType().getName() : "null";
@@ -1837,7 +1837,7 @@ public class RpcServer implements RpcServerInterface {
 
   /**
    * Constructs a server listening on the named port and address.
-   * @param serverInstance hosting instance of {@link Server}. We will do authentications if an
+   * @param server hosting instance of {@link Server}. We will do authentications if an
    * instance else pass null for no authentication check.
    * @param name Used keying this rpc servers' metrics and for naming the Listener thread.
    * @param services A list of services.
@@ -1845,12 +1845,12 @@ public class RpcServer implements RpcServerInterface {
    * @param conf
    * @throws IOException
    */
-  public RpcServer(final Server serverInstance, final String name,
+  public RpcServer(final Server server, final String name,
       final List<BlockingServiceAndInterface> services,
       final InetSocketAddress isa, Configuration conf,
       RpcScheduler scheduler)
   throws IOException {
-    this.serverInstance = serverInstance;
+    this.server = server;
     this.services = services;
     this.isa = isa;
     this.conf = conf;
@@ -1933,32 +1933,15 @@ public class RpcServer implements RpcServerInterface {
   @Override
   public void setSocketSendBufSize(int size) { this.socketSendBufferSize = size; }
 
-  /** Starts the service.  Must be called before any calls will be handled. */
-  @Override
-  public void start() {
-    startThreads();
-    openServer();
-  }
-
-  /**
-   * Open a previously started server.
-   */
-  @Override
-  public void openServer() {
-    this.started = true;
-  }
-
   @Override
   public boolean isStarted() {
     return this.started;
   }
 
-  /**
-   * Starts the service threads but does not allow requests to be responded yet.
-   * Client will get {@link ServerNotRunningYetException} instead.
-   */
+  /** Starts the service.  Must be called before any calls will be handled. */
   @Override
-  public synchronized void startThreads() {
+  public synchronized void start() {
+    if (started) return;
     AuthenticationTokenSecretManager mgr = createSecretManager();
     if (mgr != null) {
       setSecretManager(mgr);
@@ -1969,6 +1952,7 @@ public class RpcServer implements RpcServerInterface {
     responder.start();
     listener.start();
     scheduler.start();
+    started = true;
   }
 
   @Override
@@ -1980,9 +1964,7 @@ public class RpcServer implements RpcServerInterface {
 
   private AuthenticationTokenSecretManager createSecretManager() {
     if (!isSecurityEnabled) return null;
-    if (serverInstance == null) return null;
-    if (!(serverInstance instanceof org.apache.hadoop.hbase.Server)) return null;
-    org.apache.hadoop.hbase.Server server = (org.apache.hadoop.hbase.Server)serverInstance;
+    if (server == null) return null;
     Configuration conf = server.getConfiguration();
     long keyUpdateInterval =
         conf.getLong("hbase.auth.key.update.interval", 24*60*60*1000);
@@ -2084,9 +2066,9 @@ public class RpcServer implements RpcServerInterface {
     responseInfo.put("queuetimems", qTime);
     responseInfo.put("responsesize", responseSize);
     responseInfo.put("client", clientAddress);
-    responseInfo.put("class", serverInstance == null? "": serverInstance.getClass().getSimpleName());
+    responseInfo.put("class", server == null? "": server.getClass().getSimpleName());
     responseInfo.put("method", methodName);
-    if (params.length == 2 && serverInstance instanceof HRegionServer &&
+    if (params.length == 2 && server instanceof HRegionServer &&
         params[0] instanceof byte[] &&
         params[1] instanceof Operation) {
       // if the slow process is a query, we want to log its table as well
@@ -2099,7 +2081,7 @@ public class RpcServer implements RpcServerInterface {
       // report to the log file
       LOG.warn("(operation" + tag + "): " +
                MAPPER.writeValueAsString(responseInfo));
-    } else if (params.length == 1 && serverInstance instanceof HRegionServer &&
+    } else if (params.length == 1 && server instanceof HRegionServer &&
         params[0] instanceof Operation) {
       // annotate the response map with operation details
       responseInfo.putAll(((Operation) params[0]).toMap());
@@ -2181,7 +2163,6 @@ public class RpcServer implements RpcServerInterface {
    * @param addr InetAddress of incoming connection
    * @throws org.apache.hadoop.security.authorize.AuthorizationException when the client isn't authorized to talk the protocol
    */
-  @SuppressWarnings("static-access")
   public void authorize(UserGroupInformation user, ConnectionHeader connection, InetAddress addr)
   throws AuthorizationException {
     if (authorize) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServerInterface.java hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServerInterface.java
index 58106c6..cf2a8a7 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServerInterface.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/RpcServerInterface.java
@@ -44,8 +44,6 @@ import com.google.protobuf.ServiceException;
 @InterfaceAudience.Private
 public interface RpcServerInterface {
   void start();
-  void openServer();
-  void startThreads();
   boolean isStarted();
 
   void stop();
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java
index af7e6fb..6a526b5 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.java
@@ -30,8 +30,8 @@ import com.google.common.base.Strings;
 import com.google.common.collect.Lists;
 
 /**
- * A scheduler that maintains isolated handler pools for general, high-priority and replication
- * requests.
+ * A scheduler that maintains isolated handler pools for general,
+ * high-priority, and replication requests.
  */
 @InterfaceAudience.Private
 @InterfaceStability.Evolving
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
index 5eeaa82..d301127 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java
@@ -24,10 +24,10 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.ZNodeClearer;
-import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.ZNodeClearer;
+import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
@@ -65,6 +65,7 @@ public class ActiveMasterManager extends ZooKeeperListener {
    */
   ActiveMasterManager(ZooKeeperWatcher watcher, ServerName sn, Server master) {
     super(watcher);
+    watcher.registerListener(this);
     this.sn = sn;
     this.master = master;
   }
@@ -139,13 +140,15 @@ public class ActiveMasterManager extends ZooKeeperListener {
    *
    * This also makes sure that we are watching the master znode so will be
    * notified if another master dies.
-   * @param startupStatus
+   * @param checkInterval the interval to check if the master is stopped
+   * @param startupStatus the monitor status to track the progress
    * @return True if no issue becoming active master else false if another
    * master was running or if some other problem (zookeeper, stop flag has been
    * set on this Master)
    */
-  boolean blockUntilBecomingActiveMaster(MonitoredTask startupStatus) {
-    while (true) {
+  boolean blockUntilBecomingActiveMaster(
+      int checkInterval, MonitoredTask startupStatus) {
+    while (!(master.isAborted() || master.isStopped())) {
       startupStatus.setStatus("Trying to register in ZK as active master");
       // Try to become the active master, watch if there is another master.
       // Write out our ServerName as versioned bytes.
@@ -222,9 +225,9 @@ public class ActiveMasterManager extends ZooKeeperListener {
         return false;
       }
       synchronized (this.clusterHasActiveMaster) {
-        while (this.clusterHasActiveMaster.get() && !this.master.isStopped()) {
+        while (clusterHasActiveMaster.get() && !master.isStopped()) {
           try {
-            this.clusterHasActiveMaster.wait();
+            clusterHasActiveMaster.wait(checkInterval);
           } catch (InterruptedException e) {
             // We expect to be interrupted when a master dies,
             //  will fall out if so
@@ -235,18 +238,15 @@ public class ActiveMasterManager extends ZooKeeperListener {
           this.master.stop(
             "Cluster went down before this master became active");
         }
-        if (this.master.isStopped()) {
-          return false;
-        }
-        // there is no active master so we can try to become active master again
       }
     }
+    return false;
   }
 
   /**
    * @return True if cluster has an active master.
    */
-  public boolean isActiveMaster() {
+  boolean hasActiveMaster() {
     try {
       if (ZKUtil.checkExists(watcher, watcher.getMasterAddressZNode()) >= 0) {
         return true;
@@ -261,6 +261,11 @@ public class ActiveMasterManager extends ZooKeeperListener {
 
   public void stop() {
     try {
+      synchronized (clusterHasActiveMaster) {
+        // Master is already stopped, wake up the manager
+        // thread so that it can shutdown soon.
+        clusterHasActiveMaster.notifyAll();
+      }
       // If our address is in ZK, delete it on our way out
       ServerName activeMaster = null;
       try {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 924236d..c7be93a 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -26,30 +26,21 @@ import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
-import java.util.Collection;
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicReference;
 
-import javax.management.ObjectName;
+import javax.servlet.http.HttpServlet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.Abortable;
-import org.apache.hadoop.hbase.Chore;
-import org.apache.hadoop.hbase.ClusterId;
 import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.HBaseIOException;
@@ -57,7 +48,6 @@ import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.HealthCheckChore;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.NamespaceNotFoundException;
@@ -70,25 +60,18 @@ import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.UnknownRegionException;
-import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaReader;
-import org.apache.hadoop.hbase.client.ConnectionUtils;
 import org.apache.hadoop.hbase.client.MetaScanner;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitorBase;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
-import org.apache.hadoop.hbase.exceptions.MergeRegionException;
-import org.apache.hadoop.hbase.exceptions.UnknownProtocolException;
-import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.executor.ExecutorType;
-import org.apache.hadoop.hbase.ipc.FifoRpcScheduler;
 import org.apache.hadoop.hbase.ipc.RequestContext;
 import org.apache.hadoop.hbase.ipc.RpcServer;
-import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;
-import org.apache.hadoop.hbase.ipc.RpcServerInterface;
-import org.apache.hadoop.hbase.ipc.ServerRpcController;
+import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;
+import org.apache.hadoop.hbase.master.MasterRpcServices.BalanceSwitchMode;
 import org.apache.hadoop.hbase.master.RegionState.State;
 import org.apache.hadoop.hbase.master.balancer.BalancerChore;
 import org.apache.hadoop.hbase.master.balancer.ClusterStatusChore;
@@ -108,150 +91,33 @@ import org.apache.hadoop.hbase.master.snapshot.SnapshotManager;
 import org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
-import org.apache.hadoop.hbase.procedure.MasterProcedureManager;
 import org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost;
-import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
-import org.apache.hadoop.hbase.protobuf.RequestConverter;
-import org.apache.hadoop.hbase.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
-import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameStringPair;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ProcedureDescription;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionServerInfo;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AddColumnRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AddColumnResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AssignRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AssignRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.BalanceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.BalanceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateNamespaceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateNamespaceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateTableRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateTableResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteColumnRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteColumnResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteNamespaceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteNamespaceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteSnapshotRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteSnapshotResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteTableRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteTableResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DisableTableRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DisableTableResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DispatchMergingRegionsRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DispatchMergingRegionsResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableCatalogJanitorRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableCatalogJanitorResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableTableRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableTableResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ExecProcedureRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ExecProcedureResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetClusterStatusRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetClusterStatusResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetCompletedSnapshotsRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetCompletedSnapshotsResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetNamespaceDescriptorRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetNamespaceDescriptorResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetSchemaAlterStatusRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetSchemaAlterStatusResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableNamesRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableNamesResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsCatalogJanitorEnabledRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsCatalogJanitorEnabledResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsMasterRunningRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsMasterRunningResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsProcedureDoneRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsProcedureDoneResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsRestoreSnapshotDoneRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsRestoreSnapshotDoneResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListNamespaceDescriptorsRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListNamespaceDescriptorsResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableDescriptorsByNamespaceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableDescriptorsByNamespaceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableNamesByNamespaceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableNamesByNamespaceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyColumnRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyColumnResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyNamespaceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyNamespaceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyTableRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyTableResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MoveRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MoveRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.OfflineRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.OfflineRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RestoreSnapshotRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RestoreSnapshotResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RunCatalogScanRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RunCatalogScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SetBalancerRunningRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SetBalancerRunningResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ShutdownRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ShutdownResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SnapshotRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SnapshotResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.StopMasterRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.StopMasterResponse;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.UnassignRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.UnassignRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdResponse;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportResponse;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRSFatalErrorRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRSFatalErrorResponse;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.RSRpcServices;
 import org.apache.hadoop.hbase.regionserver.RegionSplitPolicy;
 import org.apache.hadoop.hbase.replication.regionserver.Replication;
-import org.apache.hadoop.hbase.security.UserProvider;
-import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
-import org.apache.hadoop.hbase.trace.SpanReceiverHost;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.CompressionTest;
-import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.HFileArchiveUtil;
-import org.apache.hadoop.hbase.util.HasThread;
-import org.apache.hadoop.hbase.util.InfoServer;
-import org.apache.hadoop.hbase.util.JvmPauseMonitor;
 import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.util.Sleeper;
-import org.apache.hadoop.hbase.util.Strings;
 import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hbase.util.VersionInfo;
-import org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker;
 import org.apache.hadoop.hbase.zookeeper.DrainingServerTracker;
 import org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker;
-import org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
 import org.apache.hadoop.hbase.zookeeper.RegionServerTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKClusterId;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
-import org.apache.hadoop.hbase.zookeeper.ZooKeeperListener;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
-import org.apache.hadoop.metrics.util.MBeanUtil;
-import org.apache.hadoop.net.DNS;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.Watcher;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.protobuf.Descriptors;
-import com.google.protobuf.Message;
-import com.google.protobuf.RpcCallback;
-import com.google.protobuf.RpcController;
 import com.google.protobuf.Service;
-import com.google.protobuf.ServiceException;
 
 /**
  * HMaster is the "master server" for HBase. An HBase cluster has one active
@@ -270,22 +136,13 @@ import com.google.protobuf.ServiceException;
  */
 @InterfaceAudience.Private
 @SuppressWarnings("deprecation")
-public class HMaster extends HasThread implements MasterProtos.MasterService.BlockingInterface,
-RegionServerStatusProtos.RegionServerStatusService.BlockingInterface,
-MasterServices, Server {
+public class HMaster extends HRegionServer implements MasterServices, Server {
   private static final Log LOG = LogFactory.getLog(HMaster.class.getName());
 
   // MASTER is name of the webapp and the attribute name used stuffing this
   //instance into web context.
   public static final String MASTER = "master";
 
-  // The configuration for the Master
-  private final Configuration conf;
-  // server for the web ui
-  private InfoServer infoServer;
-
-  // Our zk client.
-  private ZooKeeperWatcher zooKeeper;
   // Manager and zk listener for master election
   private ActiveMasterManager activeMasterManager;
   // Region server tracker
@@ -293,51 +150,28 @@ MasterServices, Server {
   // Draining region server tracker
   private DrainingServerTracker drainingServerTracker;
   // Tracker for load balancer state
-  private LoadBalancerTracker loadBalancerTracker;
-  // master address tracker
-  private MasterAddressTracker masterAddressTracker;
-
-  // RPC server for the HMaster
-  private final RpcServerInterface rpcServer;
-  private JvmPauseMonitor pauseMonitor;
-  // Set after we've called HBaseServer#openServer and ready to receive RPCs.
-  // Set back to false after we stop rpcServer.  Used by tests.
-  private volatile boolean rpcServerOpen = false;
+  LoadBalancerTracker loadBalancerTracker;
 
   /** Namespace stuff */
   private TableNamespaceManager tableNamespaceManager;
   private NamespaceJanitor namespaceJanitorChore;
 
-  /**
-   * This servers address.
-   */
-  private final InetSocketAddress isa;
-
   // Metrics for the HMaster
-  private final MetricsMaster metricsMaster;
+  final MetricsMaster metricsMaster;
   // file system manager for the master FS operations
   private MasterFileSystem fileSystemManager;
 
   // server manager to deal with region server info
-  ServerManager serverManager;
+  volatile ServerManager serverManager;
 
   // manager of assignment nodes in zookeeper
   AssignmentManager assignmentManager;
-  // manager of catalog regions
-  private CatalogTracker catalogTracker;
-  // Cluster status zk tracker and local setter
-  private ClusterStatusTracker clusterStatusTracker;
 
   // buffer for "fatal error" notices from region servers
   // in the cluster. This is only used for assisting
   // operations/debugging.
-  private MemoryBoundedLogMessageBuffer rsFatals;
+  MemoryBoundedLogMessageBuffer rsFatals;
 
-  // This flag is for stopping this Master instance.  Its set when we are
-  // stopping or aborting
-  private volatile boolean stopped = false;
-  // Set on abort -- usually failure of our zk session.
-  private volatile boolean abort = false;
   // flag set after we become the active master (used for testing)
   private volatile boolean isActiveMaster = false;
 
@@ -345,134 +179,65 @@ MasterServices, Server {
   // it is not private since it's used in unit tests
   volatile boolean initialized = false;
 
+  // flag set after master services are started,
+  // initialization may have not completed yet.
+  volatile boolean serviceStarted = false;
+
   // flag set after we complete assignMeta.
   private volatile boolean serverShutdownHandlerEnabled = false;
 
-  // Instance of the hbase executor service.
-  ExecutorService executorService;
-
-  private LoadBalancer balancer;
-  private Thread balancerChore;
-  private Thread clusterStatusChore;
+  LoadBalancer balancer;
+  private BalancerChore balancerChore;
+  private ClusterStatusChore clusterStatusChore;
   private ClusterStatusPublisher clusterStatusPublisherChore = null;
 
-  private CatalogJanitor catalogJanitorChore;
+  CatalogJanitor catalogJanitorChore;
   private LogCleaner logCleaner;
   private HFileCleaner hfileCleaner;
 
-  private MasterCoprocessorHost cpHost;
-  private final ServerName serverName;
-
-  private TableDescriptors tableDescriptors;
+  MasterCoprocessorHost cpHost;
 
-  // Table level lock manager for schema changes
-  private TableLockManager tableLockManager;
-
-  // Time stamps for when a hmaster was started and when it became active
-  private long masterStartTime;
+  // Time stamps for when a hmaster became active
   private long masterActiveTime;
 
-  /** time interval for emitting metrics values */
-  private final int msgInterval;
-  /**
-   * MX Bean for MasterInfo
-   */
-  private ObjectName mxBean = null;
-
   //should we check the compression codec type at master side, default true, HBASE-6370
   private final boolean masterCheckCompression;
 
-  private SpanReceiverHost spanReceiverHost;
-
-  private Map<String, Service> coprocessorServiceHandlers = Maps.newHashMap();
+  Map<String, Service> coprocessorServiceHandlers = Maps.newHashMap();
 
   // monitor for snapshot of hbase tables
-  private SnapshotManager snapshotManager;
+  SnapshotManager snapshotManager;
   // monitor for distributed procedures
-  private MasterProcedureManagerHost mpmHost;
-
-  /** The health check chore. */
-  private HealthCheckChore healthCheckChore;
-
-  /**
-   * is in distributedLogReplay mode. When true, SplitLogWorker directly replays WAL edits to newly
-   * assigned region servers instead of creating recovered.edits files.
-   */
-  private final boolean distributedLogReplay;
+  MasterProcedureManagerHost mpmHost;
 
   /** flag used in test cases in order to simulate RS failures during master initialization */
   private volatile boolean initializationBeforeMetaAssignment = false;
 
-  /** The following is used in master recovery scenario to re-register listeners */
-  private List<ZooKeeperListener> registeredZKListenersBeforeRecovery;
-
   /**
    * Initializes the HMaster. The steps are as follows:
    * <p>
    * <ol>
-   * <li>Initialize HMaster RPC and address
-   * <li>Connect to ZooKeeper.
+   * <li>Initialize the local HRegionServer
+   * <li>Start the ActiveMasterManager.
    * </ol>
    * <p>
-   * Remaining steps of initialization occur in {@link #run()} so that they
-   * run in their own thread rather than within the context of the constructor.
+   * Remaining steps of initialization occur in
+   * {@link #finishActiveMasterInitialization(MonitoredTask)} after
+   * the master becomes the active one.
+   *
    * @throws InterruptedException
+   * @throws KeeperException
+   * @throws IOException
    */
   public HMaster(final Configuration conf)
-  throws IOException, KeeperException, InterruptedException {
-    this.conf = new Configuration(conf);
-    // Disable the block cache on the master
-    this.conf.setFloat(HConstants.HFILE_BLOCK_CACHE_SIZE_KEY, 0.0f);
-    FSUtils.setupShortCircuitRead(conf);
-    // Server to handle client requests.
-    String hostname = Strings.domainNamePointerToHostName(DNS.getDefaultHost(
-      conf.get("hbase.master.dns.interface", "default"),
-      conf.get("hbase.master.dns.nameserver", "default")));
-    int port = conf.getInt(HConstants.MASTER_PORT, HConstants.DEFAULT_MASTER_PORT);
-    // Test that the hostname is reachable
-    InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
-    if (initialIsa.getAddress() == null) {
-      throw new IllegalArgumentException("Failed resolve of hostname " + initialIsa);
-    }
-    // Verify that the bind address is reachable if set
-    String bindAddress = conf.get("hbase.master.ipc.address");
-    if (bindAddress != null) {
-      initialIsa = new InetSocketAddress(bindAddress, port);
-      if (initialIsa.getAddress() == null) {
-        throw new IllegalArgumentException("Failed resolve of bind address " + initialIsa);
-      }
-    }
-    String name = "master/" + initialIsa.toString();
-    // Set how many times to retry talking to another server over HConnection.
-    ConnectionUtils.setServerSideHConnectionRetriesConfig(this.conf, name, LOG);
-    int numHandlers = conf.getInt(HConstants.MASTER_HANDLER_COUNT,
-      conf.getInt(HConstants.REGION_SERVER_HANDLER_COUNT, HConstants.DEFAULT_MASTER_HANLDER_COUNT));
-    this.rpcServer = new RpcServer(this, name, getServices(),
-      initialIsa, // BindAddress is IP we got for this server.
-      conf,
-      new FifoRpcScheduler(conf, numHandlers));
-    // Set our address.
-    this.isa = this.rpcServer.getListenerAddress();
-    // We don't want to pass isa's hostname here since it could be 0.0.0.0
-    this.serverName = ServerName.valueOf(hostname, this.isa.getPort(), System.currentTimeMillis());
+      throws IOException, KeeperException, InterruptedException {
+    super(conf);
     this.rsFatals = new MemoryBoundedLogMessageBuffer(
       conf.getLong("hbase.master.buffer.for.rs.fatals", 1*1024*1024));
 
-    // login the zookeeper client principal (if using security)
-    ZKUtil.loginClient(this.conf, "hbase.zookeeper.client.keytab.file",
-      "hbase.zookeeper.client.kerberos.principal", this.isa.getHostName());
-
-    // initialize server principal (if using secure Hadoop)
-    UserProvider provider = UserProvider.instantiate(conf);
-    provider.login("hbase.master.keytab.file",
-      "hbase.master.kerberos.principal", this.isa.getHostName());
-
     LOG.info("hbase.rootdir=" + FSUtils.getRootDir(this.conf) +
         ", hbase.cluster.distributed=" + this.conf.getBoolean("hbase.cluster.distributed", false));
 
-    // set the thread name now we have an address
-    setName(MASTER + ":" + this.serverName.toShortString());
-
     Replication.decorateMasterConfiguration(this.conf);
 
     // Hack! Maps DFSClient => Master for logs.  HDFS made this
@@ -481,26 +246,11 @@ MasterServices, Server {
       this.conf.set("mapred.task.id", "hb_m_" + this.serverName.toString());
     }
 
-    this.zooKeeper = new ZooKeeperWatcher(conf, MASTER + ":" + isa.getPort(), this, true);
-    this.rpcServer.startThreads();
-    this.pauseMonitor = new JvmPauseMonitor(conf);
-    this.pauseMonitor.start();
-
-    // metrics interval: using the same property as region server.
-    this.msgInterval = conf.getInt("hbase.regionserver.msginterval", 3 * 1000);
-
     //should we check the compression codec type at master side, default true, HBASE-6370
     this.masterCheckCompression = conf.getBoolean("hbase.master.check.compression", true);
 
     this.metricsMaster = new MetricsMaster( new MetricsMasterWrapperImpl(this));
 
-    // Health checker thread.
-    int sleepTime = this.conf.getInt(HConstants.HEALTH_CHORE_WAKE_FREQ,
-      HConstants.DEFAULT_THREAD_WAKE_FREQUENCY);
-    if (isHealthCheckerConfigured()) {
-      healthCheckChore = new HealthCheckChore(sleepTime, this, getConfiguration());
-    }
-
     // Do we publish the status?
     boolean shouldPublish = conf.getBoolean(HConstants.STATUS_PUBLISHED,
         HConstants.STATUS_PUBLISHED_DEFAULT);
@@ -519,161 +269,60 @@ MasterServices, Server {
         Threads.setDaemonThreadRunning(clusterStatusPublisherChore.getThread());
       }
     }
+    startActiveMasterManager();
+  }
 
-    distributedLogReplay = this.conf.getBoolean(HConstants.DISTRIBUTED_LOG_REPLAY_KEY,
-      HConstants.DEFAULT_DISTRIBUTED_LOG_REPLAY_CONFIG);
+  @VisibleForTesting
+  public MasterRpcServices getMasterRpcServices() {
+    return (MasterRpcServices)rpcServices;
   }
 
-  /**
-   * @return list of blocking services and their security info classes that this server supports
-   */
-  private List<BlockingServiceAndInterface> getServices() {
-    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(3);
-    bssi.add(new BlockingServiceAndInterface(
-        MasterProtos.MasterService.newReflectiveBlockingService(this),
-        MasterProtos.MasterService.BlockingInterface.class));
-    bssi.add(new BlockingServiceAndInterface(
-        RegionServerStatusProtos.RegionServerStatusService.newReflectiveBlockingService(this),
-        RegionServerStatusProtos.RegionServerStatusService.BlockingInterface.class));
-    return bssi;
+  public boolean balanceSwitch(final boolean b) throws IOException {
+    return getMasterRpcServices().switchBalancer(b, BalanceSwitchMode.ASYNC);
   }
 
-  /**
-   * Stall startup if we are designated a backup master; i.e. we want someone
-   * else to become the master before proceeding.
-   * @param c configuration
-   * @param amm
-   * @throws InterruptedException
-   */
-  private static void stallIfBackupMaster(final Configuration c,
-      final ActiveMasterManager amm)
-  throws InterruptedException {
-    // If we're a backup master, stall until a primary to writes his address
-    if (!c.getBoolean(HConstants.MASTER_TYPE_BACKUP,
-      HConstants.DEFAULT_MASTER_TYPE_BACKUP)) {
-      return;
-    }
-    LOG.debug("HMaster started in backup mode.  " +
-      "Stalling until master znode is written.");
-    // This will only be a minute or so while the cluster starts up,
-    // so don't worry about setting watches on the parent znode
-    while (!amm.isActiveMaster()) {
-      LOG.debug("Waiting for master address ZNode to be written " +
-        "(Also watching cluster state node)");
-      Thread.sleep(
-        c.getInt(HConstants.ZK_SESSION_TIMEOUT, HConstants.DEFAULT_ZK_SESSION_TIMEOUT));
-    }
+  protected String getProcessName() {
+    return MASTER;
+  }
 
+  protected boolean canCreateBaseZNode() {
+    return true;
   }
 
-  MetricsMaster getMetrics() {
-    return metricsMaster;
+  protected boolean canUpdateTableDescriptor() {
+    return true;
+  }
+
+  protected RSRpcServices createRpcServices() throws IOException {
+    return new MasterRpcServices(this);
+  }
+
+  protected void configureInfoServer() {
+    infoServer.addServlet("master-status", "/master-status", MasterStatusServlet.class);
+    infoServer.setAttribute(MASTER, this);
+    super.configureInfoServer();
+  }
+
+  protected Class<? extends HttpServlet> getDumpServlet() {
+    return MasterDumpServlet.class;
   }
 
   /**
-   * Main processing loop for the HMaster.
-   * <ol>
-   * <li>Block until becoming active master
-   * <li>Finish initialization via finishInitialization(MonitoredTask)
-   * <li>Enter loop until we are stopped
-   * <li>Stop services and perform cleanup once stopped
-   * </ol>
+   * Emit the HMaster metrics, such as region in transition metrics.
+   * Surrounding in a try block just to be sure metrics doesn't abort HMaster.
    */
-  @Override
-  public void run() {
-    MonitoredTask startupStatus =
-      TaskMonitor.get().createStatus("Master startup");
-    startupStatus.setDescription("Master startup");
-    masterStartTime = System.currentTimeMillis();
+  protected void doMetrics() {
     try {
-      this.masterAddressTracker = new MasterAddressTracker(getZooKeeperWatcher(), this);
-      this.masterAddressTracker.start();
-
-      // Put up info server.
-      int port = this.conf.getInt("hbase.master.info.port", HConstants.DEFAULT_MASTER_INFOPORT);
-      if (port >= 0) {
-        String a = this.conf.get("hbase.master.info.bindAddress", "0.0.0.0");
-        this.infoServer = new InfoServer(MASTER, a, port, false, this.conf);
-        this.infoServer.addServlet("status", "/master-status", MasterStatusServlet.class);
-        this.infoServer.addServlet("dump", "/dump", MasterDumpServlet.class);
-        this.infoServer.setAttribute(MASTER, this);
-        this.infoServer.start();
-      }
-
-      this.registeredZKListenersBeforeRecovery = this.zooKeeper.getListeners();
-      /*
-       * Block on becoming the active master.
-       *
-       * We race with other masters to write our address into ZooKeeper.  If we
-       * succeed, we are the primary/active master and finish initialization.
-       *
-       * If we do not succeed, there is another active master and we should
-       * now wait until it dies to try and become the next active master.  If we
-       * do not succeed on our first attempt, this is no longer a cluster startup.
-       */
-      becomeActiveMaster(startupStatus);
-
-      // We are either the active master or we were asked to shutdown
-      if (!this.stopped) {
-        finishInitialization(startupStatus, false);
-        loop();
-      }
-    } catch (Throwable t) {
-      // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility
-      if (t instanceof NoClassDefFoundError &&
-          t.getMessage().contains("org/apache/hadoop/hdfs/protocol/FSConstants$SafeModeAction")) {
-          // improved error message for this special case
-          abort("HBase is having a problem with its Hadoop jars.  You may need to "
-              + "recompile HBase against Hadoop version "
-              +  org.apache.hadoop.util.VersionInfo.getVersion()
-              + " or change your hadoop jars to start properly", t);
-      } else {
-        abort("Unhandled exception. Starting shutdown.", t);
+      if (assignmentManager != null) {
+        assignmentManager.updateRegionsInTransitionMetrics();
       }
-    } finally {
-      startupStatus.cleanup();
-
-      stopChores();
-      // Wait for all the remaining region servers to report in IFF we were
-      // running a cluster shutdown AND we were NOT aborting.
-      if (!this.abort && this.serverManager != null &&
-          this.serverManager.isClusterShutdown()) {
-        this.serverManager.letRegionServersShutdown();
-      }
-      stopServiceThreads();
-      // Stop services started for both backup and active masters
-      if (this.activeMasterManager != null) this.activeMasterManager.stop();
-      if (this.catalogTracker != null) this.catalogTracker.stop();
-      if (this.serverManager != null) this.serverManager.stop();
-      if (this.assignmentManager != null) this.assignmentManager.stop();
-      if (this.fileSystemManager != null) this.fileSystemManager.stop();
-      if (this.mpmHost != null) this.mpmHost.stop("server shutting down.");
-      this.zooKeeper.close();
+    } catch (Throwable e) {
+      LOG.error("Couldn't update metrics: " + e.getMessage());
     }
-    LOG.info("HMaster main thread exiting");
   }
 
-  /**
-   * Try becoming active master.
-   * @param startupStatus
-   * @return True if we could successfully become the active master.
-   * @throws InterruptedException
-   */
-  private boolean becomeActiveMaster(MonitoredTask startupStatus)
-  throws InterruptedException {
-    // TODO: This is wrong!!!! Should have new servername if we restart ourselves,
-    // if we come back to life.
-    this.activeMasterManager = new ActiveMasterManager(zooKeeper, this.serverName,
-        this);
-    this.zooKeeper.registerListener(activeMasterManager);
-    stallIfBackupMaster(this.conf, this.activeMasterManager);
-
-    // The ClusterStatusTracker is setup before the other
-    // ZKBasedSystemTrackers because it's needed by the activeMasterManager
-    // to check if the cluster should be shutdown.
-    this.clusterStatusTracker = new ClusterStatusTracker(getZooKeeper(), this);
-    this.clusterStatusTracker.start();
-    return this.activeMasterManager.blockUntilBecomingActiveMaster(startupStatus);
+  MetricsMaster getMasterMetrics() {
+    return metricsMaster;
   }
 
   /**
@@ -683,14 +332,11 @@ MasterServices, Server {
    */
   void initializeZKBasedSystemTrackers() throws IOException,
       InterruptedException, KeeperException {
-    this.catalogTracker = createCatalogTracker(this.zooKeeper, this.conf, this);
-    this.catalogTracker.start();
-
     this.balancer = LoadBalancerFactory.getLoadBalancer(conf);
     this.loadBalancerTracker = new LoadBalancerTracker(zooKeeper, this);
     this.loadBalancerTracker.start();
     this.assignmentManager = new AssignmentManager(this, serverManager,
-      this.catalogTracker, this.balancer, this.executorService, this.metricsMaster,
+      this.catalogTracker, this.balancer, this.service, this.metricsMaster,
       this.tableLockManager);
     zooKeeper.registerListenerFirst(assignmentManager);
 
@@ -721,74 +367,26 @@ MasterServices, Server {
   }
 
   /**
-   * Create CatalogTracker.
-   * In its own method so can intercept and mock it over in tests.
-   * @param zk If zk is null, we'll create an instance (and shut it down
-   * when {@link #stop(String)} is called) else we'll use what is passed.
-   * @param conf
-   * @param abortable If fatal exception we'll call abort on this.  May be null.
-   * If it is we'll use the Connection associated with the passed
-   * {@link Configuration} as our {@link Abortable}.
-   * ({@link Object#wait(long)} when passed a <code>0</code> waits for ever).
-   * @throws IOException
-   */
-  CatalogTracker createCatalogTracker(final ZooKeeperWatcher zk,
-      final Configuration conf, Abortable abortable)
-  throws IOException {
-    return new CatalogTracker(zk, conf, abortable);
-  }
-
-  // Check if we should stop every 100ms
-  private Sleeper stopSleeper = new Sleeper(100, this);
-
-  private void loop() {
-    long lastMsgTs = 0l;
-    long now = 0l;
-    while (!this.stopped) {
-      now = System.currentTimeMillis();
-      if ((now - lastMsgTs) >= this.msgInterval) {
-        doMetrics();
-        lastMsgTs = System.currentTimeMillis();
-      }
-      stopSleeper.sleep();
-    }
-  }
-
-  /**
-   * Emit the HMaster metrics, such as region in transition metrics.
-   * Surrounding in a try block just to be sure metrics doesn't abort HMaster.
-   */
-  private void doMetrics() {
-    try {
-      this.assignmentManager.updateRegionsInTransitionMetrics();
-    } catch (Throwable e) {
-      LOG.error("Couldn't update metrics: " + e.getMessage());
-    }
-  }
-
-  /**
    * Finish initialization of HMaster after becoming the primary master.
    *
    * <ol>
    * <li>Initialize master components - file system manager, server manager,
-   *     assignment manager, region server tracker, catalog tracker, etc</li>
-   * <li>Start necessary service threads - rpc server, info server,
+   *     assignment manager, region server tracker, etc</li>
+   * <li>Start necessary service threads - balancer, catalog janior,
    *     executor services, etc</li>
    * <li>Set cluster as UP in ZooKeeper</li>
    * <li>Wait for RegionServers to check-in</li>
    * <li>Split logs and perform data recovery, if necessary</li>
-   * <li>Ensure assignment of meta regions<li>
+   * <li>Ensure assignment of meta/namespace regions<li>
    * <li>Handle either fresh cluster start or master failover</li>
    * </ol>
    *
-   * @param masterRecovery
-   *
    * @throws IOException
    * @throws InterruptedException
    * @throws KeeperException
    */
-  private void finishInitialization(MonitoredTask status, boolean masterRecovery)
-  throws IOException, InterruptedException, KeeperException {
+  private void finishActiveMasterInitialization(MonitoredTask status)
+      throws IOException, InterruptedException, KeeperException {
 
     isActiveMaster = true;
 
@@ -802,44 +400,31 @@ MasterServices, Server {
 
     this.masterActiveTime = System.currentTimeMillis();
     // TODO: Do this using Dependency Injection, using PicoContainer, Guice or Spring.
-    this.fileSystemManager = new MasterFileSystem(this, this, masterRecovery);
-
-    this.tableDescriptors =
-      new FSTableDescriptors(this.fileSystemManager.getFileSystem(),
-      this.fileSystemManager.getRootDir());
+    this.fileSystemManager = new MasterFileSystem(this, this);
 
     // publish cluster ID
     status.setStatus("Publishing Cluster ID in ZooKeeper");
     ZKClusterId.setClusterId(this.zooKeeper, fileSystemManager.getClusterId());
+    this.serverManager = createServerManager(this, this);
 
-    if (!masterRecovery) {
-      this.executorService = new ExecutorService(getServerName().toShortString());
-      this.serverManager = createServerManager(this, this);
-    }
-
-    //Initialize table lock manager, and ensure that all write locks held previously
-    //are invalidated
-    this.tableLockManager = TableLockManager.createTableLockManager(conf, zooKeeper, serverName);
-    if (!masterRecovery) {
-      this.tableLockManager.reapWriteLocks();
-    }
+    // Invalidate all write locks held previously
+    this.tableLockManager.reapWriteLocks();
 
     status.setStatus("Initializing ZK system trackers");
     initializeZKBasedSystemTrackers();
 
-    if (!masterRecovery) {
-      // initialize master side coprocessors before we start handling requests
-      status.setStatus("Initializing master coprocessors");
-      this.cpHost = new MasterCoprocessorHost(this, this.conf);
+    // initialize master side coprocessors before we start handling requests
+    status.setStatus("Initializing master coprocessors");
+    this.cpHost = new MasterCoprocessorHost(this, this.conf);
 
-      spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());
+    // start up all service threads.
+    status.setStatus("Initializing master service threads");
+    startServiceThreads();
 
-      // start up all service threads.
-      status.setStatus("Initializing master service threads");
-      startServiceThreads();
-    }
+    // Wake up this server to check in
+    sleeper.skipSleepCycle();
 
-    // Wait for region servers to report in.
+    // Wait for region servers to report in
     this.serverManager.waitForRegionServers(status);
     // Check zk for region servers that are up but didn't register
     for (ServerName sn: this.regionServerTracker.getOnlineServers()) {
@@ -850,9 +435,7 @@ MasterServices, Server {
       }
     }
 
-    if (!masterRecovery) {
-      this.assignmentManager.startTimeOutMonitor();
-    }
+    this.assignmentManager.startTimeOutMonitor();
 
     // get a list for previously failed RS which need log splitting work
     // we recover hbase:meta region servers inside master initialization and
@@ -888,12 +471,19 @@ MasterServices, Server {
     this.balancer.setMasterServices(this);
     this.balancer.initialize();
 
+    // Wait for regionserver to finish initialization.
+    while (!isOnline()) {
+      synchronized (online) {
+        online.wait(100);
+      }
+    }
+
     // Make sure meta assigned before proceeding.
     status.setStatus("Assigning Meta Region");
     assignMeta(status, previouslyFailedMetaRSs);
     // check if master is shutting down because above assignMeta could return even hbase:meta isn't
     // assigned when master is shutting down
-    if(this.stopped) return;
+    if(isStopped()) return;
 
     status.setStatus("Submitting log splitting work for previously failed region servers");
     // Master has recovered hbase:meta region server and we put
@@ -915,15 +505,15 @@ MasterServices, Server {
     //set cluster status again after user regions are assigned
     this.balancer.setClusterStatus(getClusterStatus());
 
-    if (!masterRecovery) {
-      // Start balancer and meta catalog janitor after meta and regions have
-      // been assigned.
-      status.setStatus("Starting balancer and catalog janitor");
-      this.clusterStatusChore = getAndStartClusterStatusChore(this);
-      this.balancerChore = getAndStartBalancerChore(this);
-      this.catalogJanitorChore = new CatalogJanitor(this, this);
-      startCatalogJanitorChore();
-    }
+    // Start balancer and meta catalog janitor after meta and regions have
+    // been assigned.
+    status.setStatus("Starting balancer and catalog janitor");
+    this.clusterStatusChore = new ClusterStatusChore(this, balancer);
+    Threads.setDaemonThreadRunning(clusterStatusChore.getThread());
+    this.balancerChore = new BalancerChore(this);
+    Threads.setDaemonThreadRunning(balancerChore.getThread());
+    this.catalogJanitorChore = new CatalogJanitor(this, this);
+    Threads.setDaemonThreadRunning(catalogJanitorChore.getThread());
 
     status.setStatus("Starting namespace manager");
     initNamespace();
@@ -944,14 +534,12 @@ MasterServices, Server {
     // master initialization. See HBASE-5916.
     this.serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer();
 
-    if (!masterRecovery) {
-      if (this.cpHost != null) {
-        // don't let cp initialization errors kill the master
-        try {
-          this.cpHost.postStartMaster();
-        } catch (IOException ioe) {
-          LOG.error("Coprocessor postStartMaster() hook failed", ioe);
-        }
+    if (this.cpHost != null) {
+      // don't let cp initialization errors kill the master
+      try {
+        this.cpHost.postStartMaster();
+      } catch (IOException ioe) {
+        LOG.error("Coprocessor postStartMaster() hook failed", ioe);
       }
     }
   }
@@ -1127,47 +715,16 @@ MasterServices, Server {
     return this.tableDescriptors;
   }
 
-  /** @return InfoServer object. Maybe null.*/
-  public InfoServer getInfoServer() {
-    return this.infoServer;
-  }
-
-  @Override
-  public Configuration getConfiguration() {
-    return this.conf;
-  }
-
   @Override
   public ServerManager getServerManager() {
     return this.serverManager;
   }
 
   @Override
-  public ExecutorService getExecutorService() {
-    return this.executorService;
-  }
-
-  @Override
   public MasterFileSystem getMasterFileSystem() {
     return this.fileSystemManager;
   }
 
-  /**
-   * Get the ZK wrapper object - needed by master_jsp.java
-   * @return the zookeeper wrapper
-   */
-  public ZooKeeperWatcher getZooKeeperWatcher() {
-    return this.zooKeeper;
-  }
-
-  public ActiveMasterManager getActiveMasterManager() {
-    return this.activeMasterManager;
-  }
-
-  public MasterAddressTracker getMasterAddressTracker() {
-    return this.masterAddressTracker;
-  }
-
   /*
    * Start up all services. If any of these threads gets an unhandled exception
    * then they just die with a logged message.  This should be fine because
@@ -1175,99 +732,65 @@ MasterServices, Server {
    *  as OOMEs; it should be lightly loaded. See what HRegionServer does if
    *  need to install an unexpected exception handler.
    */
-  void startServiceThreads() throws IOException{
+  private void startServiceThreads() throws IOException{
    // Start the executor service pools
-   this.executorService.startExecutorService(ExecutorType.MASTER_OPEN_REGION,
+   this.service.startExecutorService(ExecutorType.MASTER_OPEN_REGION,
       conf.getInt("hbase.master.executor.openregion.threads", 5));
-   this.executorService.startExecutorService(ExecutorType.MASTER_CLOSE_REGION,
+   this.service.startExecutorService(ExecutorType.MASTER_CLOSE_REGION,
       conf.getInt("hbase.master.executor.closeregion.threads", 5));
-   this.executorService.startExecutorService(ExecutorType.MASTER_SERVER_OPERATIONS,
+   this.service.startExecutorService(ExecutorType.MASTER_SERVER_OPERATIONS,
       conf.getInt("hbase.master.executor.serverops.threads", 5));
-   this.executorService.startExecutorService(ExecutorType.MASTER_META_SERVER_OPERATIONS,
+   this.service.startExecutorService(ExecutorType.MASTER_META_SERVER_OPERATIONS,
       conf.getInt("hbase.master.executor.serverops.threads", 5));
-   this.executorService.startExecutorService(ExecutorType.M_LOG_REPLAY_OPS,
+   this.service.startExecutorService(ExecutorType.M_LOG_REPLAY_OPS,
       conf.getInt("hbase.master.executor.logreplayops.threads", 10));
 
    // We depend on there being only one instance of this executor running
    // at a time.  To do concurrency, would need fencing of enable/disable of
    // tables.
-   this.executorService.startExecutorService(ExecutorType.MASTER_TABLE_OPERATIONS, 1);
+   this.service.startExecutorService(ExecutorType.MASTER_TABLE_OPERATIONS, 1);
 
    // Start log cleaner thread
-   String n = Thread.currentThread().getName();
    int cleanerInterval = conf.getInt("hbase.master.cleaner.interval", 60 * 1000);
    this.logCleaner =
       new LogCleaner(cleanerInterval,
          this, conf, getMasterFileSystem().getFileSystem(),
          getMasterFileSystem().getOldLogDir());
-         Threads.setDaemonThreadRunning(logCleaner.getThread(), n + ".oldLogCleaner");
+         Threads.setDaemonThreadRunning(logCleaner.getThread(), getName() + ".oldLogCleaner");
 
    //start the hfile archive cleaner thread
     Path archiveDir = HFileArchiveUtil.getArchivePath(conf);
     this.hfileCleaner = new HFileCleaner(cleanerInterval, this, conf, getMasterFileSystem()
         .getFileSystem(), archiveDir);
-    Threads.setDaemonThreadRunning(hfileCleaner.getThread(), n + ".archivedHFileCleaner");
-
-    // Start the health checker
-    if (this.healthCheckChore != null) {
-      Threads.setDaemonThreadRunning(this.healthCheckChore.getThread(), n + ".healthChecker");
-    }
+    Threads.setDaemonThreadRunning(hfileCleaner.getThread(),
+      getName() + ".archivedHFileCleaner");
 
-    // Start allowing requests to happen.
-    this.rpcServer.openServer();
-    this.rpcServerOpen = true;
+    serviceStarted = true;
     if (LOG.isTraceEnabled()) {
       LOG.trace("Started service threads");
     }
   }
 
-  /**
-   * Use this when trying to figure when its ok to send in rpcs.  Used by tests.
-   * @return True if we have successfully run {@link RpcServer#openServer()}
-   */
-  boolean isRpcServerOpen() {
-    return this.rpcServerOpen;
-  }
-
-  private void stopServiceThreads() {
+  protected void stopServiceThreads() {
+    super.stopServiceThreads();
+    stopChores();
+    // Wait for all the remaining region servers to report in IFF we were
+    // running a cluster shutdown AND we were NOT aborting.
+    if (!isAborted() && this.serverManager != null &&
+        this.serverManager.isClusterShutdown()) {
+      this.serverManager.letRegionServersShutdown();
+    }
     if (LOG.isDebugEnabled()) {
       LOG.debug("Stopping service threads");
     }
-    if (this.rpcServer != null) this.rpcServer.stop();
-    this.rpcServerOpen = false;
     // Clean up and close up shop
     if (this.logCleaner!= null) this.logCleaner.interrupt();
     if (this.hfileCleaner != null) this.hfileCleaner.interrupt();
-
-    if (this.infoServer != null) {
-      LOG.info("Stopping infoServer");
-      try {
-        this.infoServer.stop();
-      } catch (Exception ex) {
-        ex.printStackTrace();
-      }
-    }
-    if (this.executorService != null) this.executorService.shutdown();
-    if (this.healthCheckChore != null) {
-      this.healthCheckChore.interrupt();
-    }
-    if (this.pauseMonitor != null) {
-      this.pauseMonitor.stop();
-    }
-  }
-
-  private static Thread getAndStartClusterStatusChore(HMaster master) {
-    if (master == null || master.balancer == null) {
-      return null;
-    }
-    Chore chore = new ClusterStatusChore(master, master.balancer);
-    return Threads.setDaemonThreadRunning(chore.getThread());
-  }
-
-  private static Thread getAndStartBalancerChore(final HMaster master) {
-    // Start up the load balancer chore
-    Chore chore = new BalancerChore(master);
-    return Threads.setDaemonThreadRunning(chore.getThread());
+    if (this.activeMasterManager != null) this.activeMasterManager.stop();
+    if (this.serverManager != null) this.serverManager.stop();
+    if (this.assignmentManager != null) this.assignmentManager.stop();
+    if (this.fileSystemManager != null) this.fileSystemManager.stop();
+    if (this.mpmHost != null) this.mpmHost.stop("server shutting down.");
   }
 
   private void stopChores() {
@@ -1288,131 +811,25 @@ MasterServices, Server {
     }
   }
 
-  @Override
-  public RegionServerStartupResponse regionServerStartup(
-      RpcController controller, RegionServerStartupRequest request) throws ServiceException {
-    // Register with server manager
-    try {
-      InetAddress ia = getRemoteInetAddress(request.getPort(), request.getServerStartCode());
-      ServerName rs = this.serverManager.regionServerStartup(ia, request.getPort(),
-        request.getServerStartCode(), request.getServerCurrentTime());
-
-      // Send back some config info
-      RegionServerStartupResponse.Builder resp = createConfigurationSubset();
-      NameStringPair.Builder entry = NameStringPair.newBuilder()
-        .setName(HConstants.KEY_FOR_HOSTNAME_SEEN_BY_MASTER)
-        .setValue(rs.getHostname());
-      resp.addMapEntries(entry.build());
-
-      return resp.build();
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-  }
-
   /**
    * @return Get remote side's InetAddress
    * @throws UnknownHostException
    */
-  InetAddress getRemoteInetAddress(final int port, final long serverStartCode)
-  throws UnknownHostException {
+  InetAddress getRemoteInetAddress(final int port,
+      final long serverStartCode) throws UnknownHostException {
     // Do it out here in its own little method so can fake an address when
     // mocking up in tests.
-    return RpcServer.getRemoteIp();
-  }
-
-  /**
-   * @return Subset of configuration to pass initializing regionservers: e.g.
-   * the filesystem to use and root directory to use.
-   */
-  protected RegionServerStartupResponse.Builder createConfigurationSubset() {
-    RegionServerStartupResponse.Builder resp = addConfig(
-      RegionServerStartupResponse.newBuilder(), HConstants.HBASE_DIR);
-    return addConfig(resp, "fs.default.name");
-  }
-
-  private RegionServerStartupResponse.Builder addConfig(
-      final RegionServerStartupResponse.Builder resp, final String key) {
-    NameStringPair.Builder entry = NameStringPair.newBuilder()
-      .setName(key)
-      .setValue(this.conf.get(key));
-    resp.addMapEntries(entry.build());
-    return resp;
-  }
-
-  @Override
-  public GetLastFlushedSequenceIdResponse getLastFlushedSequenceId(RpcController controller,
-      GetLastFlushedSequenceIdRequest request) throws ServiceException {
-    byte[] regionName = request.getRegionName().toByteArray();
-    long seqId = serverManager.getLastFlushedSequenceId(regionName);
-    return ResponseConverter.buildGetLastFlushedSequenceIdResponse(seqId);
-  }
+    InetAddress ia = RpcServer.getRemoteIp();
 
-  @Override
-  public RegionServerReportResponse regionServerReport(
-      RpcController controller, RegionServerReportRequest request) throws ServiceException {
-    try {
-      ClusterStatusProtos.ServerLoad sl = request.getLoad();
-      ServerName serverName = ProtobufUtil.toServerName(request.getServer());
-      ServerLoad oldLoad = serverManager.getLoad(serverName);
-      this.serverManager.regionServerReport(serverName, new ServerLoad(sl));
-      if (sl != null && this.metricsMaster != null) {
-        // Up our metrics.
-        this.metricsMaster.incrementRequests(sl.getTotalNumberOfRequests()
-          - (oldLoad != null ? oldLoad.getTotalNumberOfRequests() : 0));
+    // The call could be from the local regionserver,
+    // in which case, there is no remote address.
+    if (ia == null && serverStartCode == startcode) {
+      InetSocketAddress isa = rpcServices.getSocketAddress();
+      if (isa != null && isa.getPort() == port) {
+        ia = isa.getAddress();
       }
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-
-    return RegionServerReportResponse.newBuilder().build();
-  }
-
-  @Override
-  public ReportRSFatalErrorResponse reportRSFatalError(
-      RpcController controller, ReportRSFatalErrorRequest request) throws ServiceException {
-    String errorText = request.getErrorMessage();
-    ServerName sn = ProtobufUtil.toServerName(request.getServer());
-    String msg = "Region server " + sn +
-      " reported a fatal error:\n" + errorText;
-    LOG.error(msg);
-    rsFatals.add(msg);
-
-    return ReportRSFatalErrorResponse.newBuilder().build();
-  }
-
-  public boolean isMasterRunning() {
-    return !isStopped();
-  }
-
-  @Override
-  public IsMasterRunningResponse isMasterRunning(RpcController c, IsMasterRunningRequest req)
-  throws ServiceException {
-    return IsMasterRunningResponse.newBuilder().setIsMasterRunning(isMasterRunning()).build();
-  }
-
-  @Override
-  public RunCatalogScanResponse runCatalogScan(RpcController c,
-      RunCatalogScanRequest req) throws ServiceException {
-    try {
-      return ResponseConverter.buildRunCatalogScanResponse(catalogJanitorChore.scan());
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
     }
-  }
-
-  @Override
-  public EnableCatalogJanitorResponse enableCatalogJanitor(RpcController c,
-      EnableCatalogJanitorRequest req) throws ServiceException {
-    return EnableCatalogJanitorResponse.newBuilder().
-        setPrevValue(catalogJanitorChore.setEnabled(req.getEnable())).build();
-  }
-
-  @Override
-  public IsCatalogJanitorEnabledResponse isCatalogJanitorEnabled(RpcController c,
-      IsCatalogJanitorEnabledRequest req) throws ServiceException {
-    boolean isEnabled = catalogJanitorChore != null ? catalogJanitorChore.getEnabled() : false;
-    return IsCatalogJanitorEnabledResponse.newBuilder().setValue(isEnabled).build();
+    return ia;
   }
 
   /**
@@ -1515,54 +932,6 @@ MasterServices, Server {
     return balancerRan;
   }
 
-  @Override
-  public BalanceResponse balance(RpcController c, BalanceRequest request) throws ServiceException {
-    try {
-      return BalanceResponse.newBuilder().setBalancerRan(balance()).build();
-    } catch (IOException ex) {
-      throw new ServiceException(ex);
-    }
-  }
-
-  enum BalanceSwitchMode {
-    SYNC,
-    ASYNC
-  }
-
-  /**
-   * Assigns balancer switch according to BalanceSwitchMode
-   * @param b new balancer switch
-   * @param mode BalanceSwitchMode
-   * @return old balancer switch
-   */
-  public boolean switchBalancer(final boolean b, BalanceSwitchMode mode) throws IOException {
-    boolean oldValue = this.loadBalancerTracker.isBalancerOn();
-    boolean newValue = b;
-    try {
-      if (this.cpHost != null) {
-        newValue = this.cpHost.preBalanceSwitch(newValue);
-      }
-      try {
-        if (mode == BalanceSwitchMode.SYNC) {
-          synchronized (this.balancer) {
-            this.loadBalancerTracker.setBalancerOn(newValue);
-          }
-        } else {
-          this.loadBalancerTracker.setBalancerOn(newValue);
-        }
-      } catch (KeeperException ke) {
-        throw new IOException(ke);
-      }
-      LOG.info(getClientIdAuditPrefix() + " set balanceSwitch=" + newValue);
-      if (this.cpHost != null) {
-        this.cpHost.postBalanceSwitch(oldValue, newValue);
-      }
-    } catch (IOException ioe) {
-      LOG.warn("Error flipping balance switch", ioe);
-    }
-    return oldValue;
-  }
-
   /**
    * @return Client info for use as prefix on an audit log string; who did an action
    */
@@ -1571,26 +940,6 @@ MasterServices, Server {
       RequestContext.get().getRemoteAddress();
   }
 
-  public boolean synchronousBalanceSwitch(final boolean b) throws IOException {
-    return switchBalancer(b, BalanceSwitchMode.SYNC);
-  }
-
-  public boolean balanceSwitch(final boolean b) throws IOException {
-    return switchBalancer(b, BalanceSwitchMode.ASYNC);
-  }
-
-  @Override
-  public SetBalancerRunningResponse setBalancerRunning(
-      RpcController controller, SetBalancerRunningRequest req) throws ServiceException {
-    try {
-      boolean prevValue = (req.getSynchronous())?
-        synchronousBalanceSwitch(req.getOn()):balanceSwitch(req.getOn());
-      return SetBalancerRunningResponse.newBuilder().setPrevBalanceValue(prevValue).build();
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-  }
-
   /**
    * Switch for the background CatalogJanitor thread.
    * Used for testing.  The thread will continue to run.  It will just be a noop
@@ -1602,90 +951,13 @@ MasterServices, Server {
   }
 
   @Override
-  public DispatchMergingRegionsResponse dispatchMergingRegions(
-      RpcController controller, DispatchMergingRegionsRequest request)
-      throws ServiceException {
-    final byte[] encodedNameOfRegionA = request.getRegionA().getValue()
-        .toByteArray();
-    final byte[] encodedNameOfRegionB = request.getRegionB().getValue()
-        .toByteArray();
-    final boolean forcible = request.getForcible();
-    if (request.getRegionA().getType() != RegionSpecifierType.ENCODED_REGION_NAME
-        || request.getRegionB().getType() != RegionSpecifierType.ENCODED_REGION_NAME) {
-      LOG.warn("mergeRegions specifier type: expected: "
-          + RegionSpecifierType.ENCODED_REGION_NAME + " actual: region_a="
-          + request.getRegionA().getType() + ", region_b="
-          + request.getRegionB().getType());
-    }
-    RegionState regionStateA = assignmentManager.getRegionStates()
-        .getRegionState(Bytes.toString(encodedNameOfRegionA));
-    RegionState regionStateB = assignmentManager.getRegionStates()
-        .getRegionState(Bytes.toString(encodedNameOfRegionB));
-    if (regionStateA == null || regionStateB == null) {
-      throw new ServiceException(new UnknownRegionException(
-          Bytes.toStringBinary(regionStateA == null ? encodedNameOfRegionA
-              : encodedNameOfRegionB)));
-    }
-
-    if (!regionStateA.isOpened() || !regionStateB.isOpened()) {
-      throw new ServiceException(new MergeRegionException(
-        "Unable to merge regions not online " + regionStateA + ", " + regionStateB));
-    }
-
-    HRegionInfo regionInfoA = regionStateA.getRegion();
-    HRegionInfo regionInfoB = regionStateB.getRegion();
-    if (regionInfoA.compareTo(regionInfoB) == 0) {
-      throw new ServiceException(new MergeRegionException(
-        "Unable to merge a region to itself " + regionInfoA + ", " + regionInfoB));
-    }
-
-    if (!forcible && !HRegionInfo.areAdjacent(regionInfoA, regionInfoB)) {
-      throw new ServiceException(new MergeRegionException(
-        "Unable to merge not adjacent regions "
-          + regionInfoA.getRegionNameAsString() + ", "
-          + regionInfoB.getRegionNameAsString()
-          + " where forcible = " + forcible));
-    }
-
-    try {
-      dispatchMergingRegions(regionInfoA, regionInfoB, forcible);
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-
-    return DispatchMergingRegionsResponse.newBuilder().build();
-  }
-
-  @Override
   public void dispatchMergingRegions(final HRegionInfo region_a,
       final HRegionInfo region_b, final boolean forcible) throws IOException {
     checkInitialized();
-    this.executorService.submit(new DispatchMergingRegionHandler(this,
+    this.service.submit(new DispatchMergingRegionHandler(this,
         this.catalogJanitorChore, region_a, region_b, forcible));
   }
 
-  @Override
-  public MoveRegionResponse moveRegion(RpcController controller, MoveRegionRequest req)
-  throws ServiceException {
-    final byte [] encodedRegionName = req.getRegion().getValue().toByteArray();
-    RegionSpecifierType type = req.getRegion().getType();
-    final byte [] destServerName = (req.hasDestServerName())?
-      Bytes.toBytes(ProtobufUtil.toServerName(req.getDestServerName()).getServerName()):null;
-    MoveRegionResponse mrr = MoveRegionResponse.newBuilder().build();
-
-    if (type != RegionSpecifierType.ENCODED_REGION_NAME) {
-      LOG.warn("moveRegion specifier type: expected: " + RegionSpecifierType.ENCODED_REGION_NAME
-        + " actual: " + type);
-    }
-
-    try {
-      move(encodedRegionName, destServerName);
-    } catch (HBaseIOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return mrr;
-  }
-
   void move(final byte[] encodedRegionName,
       final byte[] destServerName) throws HBaseIOException {
     RegionState regionState = assignmentManager.getRegionStates().
@@ -1736,9 +1008,8 @@ MasterServices, Server {
 
   @Override
   public void createTable(HTableDescriptor hTableDescriptor,
-    byte [][] splitKeys)
-  throws IOException {
-    if (!isMasterRunning()) {
+      byte [][] splitKeys) throws IOException {
+    if (isStopped()) {
       throw new MasterNotRunningException();
     }
 
@@ -1752,7 +1023,7 @@ MasterServices, Server {
       cpHost.preCreateTable(hTableDescriptor, newRegions);
     }
     LOG.info(getClientIdAuditPrefix() + " create " + hTableDescriptor);
-    this.executorService.submit(new CreateTableHandler(this,
+    this.service.submit(new CreateTableHandler(this,
       this.fileSystemManager, hTableDescriptor, conf,
       newRegions, this).prepare());
     if (cpHost != null) {
@@ -1852,6 +1123,53 @@ MasterServices, Server {
     }
   }
 
+  private void startActiveMasterManager() {
+    activeMasterManager = new ActiveMasterManager(zooKeeper, serverName, this);
+    // Start a thread to try to become the active master, so we won't block here
+    Threads.setDaemonThreadRunning(new Thread(new Runnable() {
+      public void run() {
+        int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT,
+          HConstants.DEFAULT_ZK_SESSION_TIMEOUT);
+        // If we're a backup master, stall until a primary to writes his address
+        if (conf.getBoolean(HConstants.MASTER_TYPE_BACKUP,
+            HConstants.DEFAULT_MASTER_TYPE_BACKUP)) {
+          LOG.debug("HMaster started in backup mode. "
+            + "Stalling until master znode is written.");
+          // This will only be a minute or so while the cluster starts up,
+          // so don't worry about setting watches on the parent znode
+          while (!activeMasterManager.hasActiveMaster()) {
+            LOG.debug("Waiting for master address ZNode to be written "
+              + "(Also watching cluster state node)");
+            Threads.sleep(timeout);
+          }
+        }
+        MonitoredTask status = TaskMonitor.get().createStatus("Master startup");
+        status.setDescription("Master startup");
+        try {
+          if (activeMasterManager.blockUntilBecomingActiveMaster(timeout, status)) {
+            finishActiveMasterInitialization(status);
+          }
+        } catch (Throwable t) {
+          status.setStatus("Failed to become active: " + t.getMessage());
+          LOG.fatal("Failed to become active master", t);
+          // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility
+          if (t instanceof NoClassDefFoundError &&
+              t.getMessage().contains("org/apache/hadoop/hdfs/protocol/FSConstants$SafeModeAction")) {
+            // improved error message for this special case
+            abort("HBase is having a problem with its Hadoop jars.  You may need to "
+              + "recompile HBase against Hadoop version "
+              +  org.apache.hadoop.util.VersionInfo.getVersion()
+              + " or change your hadoop jars to start properly", t);
+          } else {
+            abort("Unhandled exception. Starting shutdown.", t);
+          }
+        } finally {
+          status.cleanup();
+        }
+      }
+    }, "ActiveMasterManager"));
+  }
+
   private void checkCompression(final HTableDescriptor htd)
   throws IOException {
     if (!this.masterCheckCompression) return;
@@ -1867,19 +1185,6 @@ MasterServices, Server {
     CompressionTest.testCompression(hcd.getCompactionCompression());
   }
 
-  @Override
-  public CreateTableResponse createTable(RpcController controller, CreateTableRequest req)
-  throws ServiceException {
-    HTableDescriptor hTableDescriptor = HTableDescriptor.convert(req.getTableSchema());
-    byte [][] splitKeys = ProtobufUtil.getSplitKeysArray(req);
-    try {
-      createTable(hTableDescriptor,splitKeys);
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return CreateTableResponse.newBuilder().build();
-  }
-
   private HRegionInfo[] getHRegionInfos(HTableDescriptor hTableDescriptor,
     byte[][] splitKeys) {
     HRegionInfo[] hRegionInfos = null;
@@ -1912,52 +1217,13 @@ MasterServices, Server {
       cpHost.preDeleteTable(tableName);
     }
     LOG.info(getClientIdAuditPrefix() + " delete " + tableName);
-    this.executorService.submit(new DeleteTableHandler(tableName, this, this).prepare());
+    this.service.submit(new DeleteTableHandler(tableName, this, this).prepare());
     if (cpHost != null) {
       cpHost.postDeleteTable(tableName);
     }
   }
 
   @Override
-  public DeleteTableResponse deleteTable(RpcController controller, DeleteTableRequest request)
-  throws ServiceException {
-    try {
-      deleteTable(ProtobufUtil.toTableName(request.getTableName()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return DeleteTableResponse.newBuilder().build();
-  }
-
-  /**
-   * Get the number of regions of the table that have been updated by the alter.
-   *
-   * @return Pair indicating the number of regions updated Pair.getFirst is the
-   *         regions that are yet to be updated Pair.getSecond is the total number
-   *         of regions of the table
-   * @throws IOException
-   */
-  @Override
-  public GetSchemaAlterStatusResponse getSchemaAlterStatus(
-      RpcController controller, GetSchemaAlterStatusRequest req) throws ServiceException {
-    // TODO: currently, we query using the table name on the client side. this
-    // may overlap with other table operations or the table operation may
-    // have completed before querying this API. We need to refactor to a
-    // transaction system in the future to avoid these ambiguities.
-    TableName tableName = ProtobufUtil.toTableName(req.getTableName());
-
-    try {
-      Pair<Integer,Integer> pair = this.assignmentManager.getReopenStatus(tableName);
-      GetSchemaAlterStatusResponse.Builder ret = GetSchemaAlterStatusResponse.newBuilder();
-      ret.setYetToUpdateRegions(pair.getFirst());
-      ret.setTotalRegions(pair.getSecond());
-      return ret.build();
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-  }
-
-  @Override
   public void addColumn(final TableName tableName, final HColumnDescriptor column)
       throws IOException {
     checkInitialized();
@@ -1974,18 +1240,6 @@ MasterServices, Server {
   }
 
   @Override
-  public AddColumnResponse addColumn(RpcController controller, AddColumnRequest req)
-  throws ServiceException {
-    try {
-      addColumn(ProtobufUtil.toTableName(req.getTableName()),
-        HColumnDescriptor.convert(req.getColumnFamilies()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return AddColumnResponse.newBuilder().build();
-  }
-
-  @Override
   public void modifyColumn(TableName tableName, HColumnDescriptor descriptor)
       throws IOException {
     checkInitialized();
@@ -2004,18 +1258,6 @@ MasterServices, Server {
   }
 
   @Override
-  public ModifyColumnResponse modifyColumn(RpcController controller, ModifyColumnRequest req)
-  throws ServiceException {
-    try {
-      modifyColumn(ProtobufUtil.toTableName(req.getTableName()),
-        HColumnDescriptor.convert(req.getColumnFamilies()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return ModifyColumnResponse.newBuilder().build();
-  }
-
-  @Override
   public void deleteColumn(final TableName tableName, final byte[] columnName)
       throws IOException {
     checkInitialized();
@@ -2032,25 +1274,13 @@ MasterServices, Server {
   }
 
   @Override
-  public DeleteColumnResponse deleteColumn(RpcController controller, DeleteColumnRequest req)
-  throws ServiceException {
-    try {
-      deleteColumn(ProtobufUtil.toTableName(req.getTableName()),
-          req.getColumnName().toByteArray());
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return DeleteColumnResponse.newBuilder().build();
-  }
-
-  @Override
   public void enableTable(final TableName tableName) throws IOException {
     checkInitialized();
     if (cpHost != null) {
       cpHost.preEnableTable(tableName);
     }
     LOG.info(getClientIdAuditPrefix() + " enable " + tableName);
-    this.executorService.submit(new EnableTableHandler(this, tableName,
+    this.service.submit(new EnableTableHandler(this, tableName,
       catalogTracker, assignmentManager, tableLockManager, false).prepare());
     if (cpHost != null) {
       cpHost.postEnableTable(tableName);
@@ -2058,41 +1288,19 @@ MasterServices, Server {
   }
 
   @Override
-  public EnableTableResponse enableTable(RpcController controller, EnableTableRequest request)
-  throws ServiceException {
-    try {
-      enableTable(ProtobufUtil.toTableName(request.getTableName()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return EnableTableResponse.newBuilder().build();
-  }
-
-  @Override
   public void disableTable(final TableName tableName) throws IOException {
     checkInitialized();
     if (cpHost != null) {
       cpHost.preDisableTable(tableName);
     }
     LOG.info(getClientIdAuditPrefix() + " disable " + tableName);
-    this.executorService.submit(new DisableTableHandler(this, tableName,
+    this.service.submit(new DisableTableHandler(this, tableName,
       catalogTracker, assignmentManager, tableLockManager, false).prepare());
     if (cpHost != null) {
       cpHost.postDisableTable(tableName);
     }
   }
 
-  @Override
-  public DisableTableResponse disableTable(RpcController controller, DisableTableRequest request)
-  throws ServiceException {
-    try {
-      disableTable(ProtobufUtil.toTableName(request.getTableName()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return DisableTableResponse.newBuilder().build();
-  }
-
   /**
    * Return the region and current deployment for the region containing
    * the given row. If the region cannot be found, returns null. If it
@@ -2144,18 +1352,6 @@ MasterServices, Server {
   }
 
   @Override
-  public ModifyTableResponse modifyTable(RpcController controller, ModifyTableRequest req)
-  throws ServiceException {
-    try {
-      modifyTable(ProtobufUtil.toTableName(req.getTableName()),
-        HTableDescriptor.convert(req.getTableSchema()));
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return ModifyTableResponse.newBuilder().build();
-  }
-
-  @Override
   public void checkTableModifiable(final TableName tableName)
       throws IOException, TableNotFoundException, TableNotDisabledException {
     if (isCatalogTable(tableName)) {
@@ -2170,19 +1366,6 @@ MasterServices, Server {
     }
   }
 
-  @Override
-  public GetClusterStatusResponse getClusterStatus(RpcController controller,
-      GetClusterStatusRequest req)
-  throws ServiceException {
-    GetClusterStatusResponse.Builder response = GetClusterStatusResponse.newBuilder();
-    try {
-      response.setClusterStatus(getClusterStatus().convert());
-    } catch (InterruptedIOException e) {
-      throw new ServiceException(e);
-    }
-    return response.build();
-  }
-
   /**
    * @return cluster status
    */
@@ -2235,18 +1418,7 @@ MasterServices, Server {
       this.serverName,
       backupMasters,
       this.assignmentManager.getRegionStates().getRegionsInTransition(),
-      this.getCoprocessors(), this.loadBalancerTracker.isBalancerOn());
-  }
-
-  public String getClusterId() {
-    if (fileSystemManager == null) {
-      return "";
-    }
-    ClusterId id = fileSystemManager.getClusterId();
-    if (id == null) {
-      return "";
-    }
-    return id.toString();
+      this.getMasterCoprocessors(), this.loadBalancerTracker.isBalancerOn());
   }
 
   /**
@@ -2264,7 +1436,7 @@ MasterServices, Server {
    * @return timestamp in millis when HMaster was started.
    */
   public long getMasterStartTime() {
-    return masterStartTime;
+    return startcode;
   }
 
   /**
@@ -2286,9 +1458,8 @@ MasterServices, Server {
   /**
    * @return array of coprocessor SimpleNames.
    */
-  public String[] getCoprocessors() {
-    Set<String> masterCoprocessors =
-        getCoprocessorHost().getCoprocessors();
+  public String[] getMasterCoprocessors() {
+    Set<String> masterCoprocessors = getMasterCoprocessorHost().getCoprocessors();
     return masterCoprocessors.toArray(new String[masterCoprocessors.size()]);
   }
 
@@ -2299,110 +1470,8 @@ MasterServices, Server {
       LOG.fatal("Master server abort: loaded coprocessors are: " +
           getLoadedCoprocessors());
     }
-
-    if (abortNow(msg, t)) {
-      if (t != null) LOG.fatal(msg, t);
-      else LOG.fatal(msg);
-      this.abort = true;
-      stop("Aborting");
-    }
-  }
-
-  /**
-   * We do the following in a different thread.  If it is not completed
-   * in time, we will time it out and assume it is not easy to recover.
-   *
-   * 1. Create a new ZK session. (since our current one is expired)
-   * 2. Try to become a primary master again
-   * 3. Initialize all ZK based system trackers.
-   * 4. Assign meta. (they are already assigned, but we need to update our
-   * internal memory state to reflect it)
-   * 5. Process any RIT if any during the process of our recovery.
-   *
-   * @return True if we could successfully recover from ZK session expiry.
-   * @throws InterruptedException
-   * @throws IOException
-   * @throws KeeperException
-   * @throws ExecutionException
-   */
-  private boolean tryRecoveringExpiredZKSession() throws InterruptedException,
-      IOException, KeeperException, ExecutionException {
-
-    this.zooKeeper.unregisterAllListeners();
-    // add back listeners which were registered before master initialization
-    // because they won't be added back in below Master re-initialization code
-    if (this.registeredZKListenersBeforeRecovery != null) {
-      for (ZooKeeperListener curListener : this.registeredZKListenersBeforeRecovery) {
-        this.zooKeeper.registerListener(curListener);
-      }
-    }
-
-    this.zooKeeper.reconnectAfterExpiration();
-
-    Callable<Boolean> callable = new Callable<Boolean> () {
-      @Override
-      public Boolean call() throws InterruptedException,
-          IOException, KeeperException {
-        MonitoredTask status =
-          TaskMonitor.get().createStatus("Recovering expired ZK session");
-        try {
-          if (!becomeActiveMaster(status)) {
-            return Boolean.FALSE;
-          }
-          serverShutdownHandlerEnabled = false;
-          initialized = false;
-          finishInitialization(status, true);
-          return !stopped;
-        } finally {
-          status.cleanup();
-        }
-      }
-    };
-
-    long timeout =
-      conf.getLong("hbase.master.zksession.recover.timeout", 300000);
-    java.util.concurrent.ExecutorService executor =
-      Executors.newSingleThreadExecutor();
-    Future<Boolean> result = executor.submit(callable);
-    executor.shutdown();
-    if (executor.awaitTermination(timeout, TimeUnit.MILLISECONDS)
-        && result.isDone()) {
-      Boolean recovered = result.get();
-      if (recovered != null) {
-        return recovered.booleanValue();
-      }
-    }
-    executor.shutdownNow();
-    return false;
-  }
-
-  /**
-   * Check to see if the current trigger for abort is due to ZooKeeper session
-   * expiry, and If yes, whether we can recover from ZK session expiry.
-   *
-   * @param msg Original abort message
-   * @param t   The cause for current abort request
-   * @return true if we should proceed with abort operation, false other wise.
-   */
-  private boolean abortNow(final String msg, final Throwable t) {
-    if (!this.isActiveMaster || this.stopped) {
-      return true;
-    }
-
-    boolean failFast = conf.getBoolean("fail.fast.expired.active.master", false);
-    if (t != null && t instanceof KeeperException.SessionExpiredException
-        && !failFast) {
-      try {
-        LOG.info("Primary Master trying to recover from ZooKeeper session " +
-            "expiry.");
-        return !tryRecoveringExpiredZKSession();
-      } catch (Throwable newT) {
-        LOG.error("Primary master encountered unexpected exception while " +
-            "trying to recover from ZooKeeper session" +
-            " expiry. Proceeding with server abort.", newT);
-      }
-    }
-    return true;
+    if (t != null) LOG.fatal(msg, t);
+    stop(msg);
   }
 
   @Override
@@ -2411,7 +1480,7 @@ MasterServices, Server {
   }
 
   @Override
-  public MasterCoprocessorHost getCoprocessorHost() {
+  public MasterCoprocessorHost getMasterCoprocessorHost() {
     return cpHost;
   }
 
@@ -2421,28 +1490,15 @@ MasterServices, Server {
   }
 
   @Override
-  public CatalogTracker getCatalogTracker() {
-    return catalogTracker;
-  }
-
-  @Override
   public AssignmentManager getAssignmentManager() {
     return this.assignmentManager;
   }
 
-  @Override
-  public TableLockManager getTableLockManager() {
-    return this.tableLockManager;
-  }
-
   public MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer() {
     return rsFatals;
   }
 
   public void shutdown() {
-    if (spanReceiverHost != null) {
-      spanReceiverHost.closeReceivers();
-    }
     if (cpHost != null) {
       try {
         cpHost.preShutdown();
@@ -2450,29 +1506,21 @@ MasterServices, Server {
         LOG.error("Error call master coprocessor preShutdown()", ioe);
       }
     }
-    if (mxBean != null) {
-      MBeanUtil.unregisterMBean(mxBean);
-      mxBean = null;
+    if (this.assignmentManager != null) {
+      this.assignmentManager.shutdown();
     }
-    if (this.assignmentManager != null) this.assignmentManager.shutdown();
-    if (this.serverManager != null) this.serverManager.shutdownCluster();
     try {
       if (this.clusterStatusTracker != null){
         this.clusterStatusTracker.setClusterDown();
+        if (this.serverManager != null) {
+          this.serverManager.shutdownCluster();
+        }
       }
     } catch (KeeperException e) {
       LOG.error("ZooKeeper exception trying to set cluster as down in ZK", e);
     }
   }
 
-  @Override
-  public ShutdownResponse shutdown(RpcController controller, ShutdownRequest request)
-  throws ServiceException {
-    LOG.info(getClientIdAuditPrefix() + " shutdown");
-    shutdown();
-    return ShutdownResponse.newBuilder().build();
-  }
-
   public void stopMaster() {
     if (cpHost != null) {
       try {
@@ -2484,49 +1532,26 @@ MasterServices, Server {
     stop("Stopped by " + Thread.currentThread().getName());
   }
 
-  @Override
-  public StopMasterResponse stopMaster(RpcController controller, StopMasterRequest request)
-  throws ServiceException {
-    LOG.info(getClientIdAuditPrefix() + " stop");
-    stopMaster();
-    return StopMasterResponse.newBuilder().build();
-  }
-
-  @Override
-  public void stop(final String why) {
-    LOG.info(why);
-    this.stopped = true;
-    // We wake up the stopSleeper to stop immediately
-    stopSleeper.skipSleepCycle();
-    // If we are a backup master, we need to interrupt wait
-    if (this.activeMasterManager != null) {
-      synchronized (this.activeMasterManager.clusterHasActiveMaster) {
-        this.activeMasterManager.clusterHasActiveMaster.notifyAll();
-      }
-    }
-    // If no region server is online then master may stuck waiting on hbase:meta to come on line.
-    // See HBASE-8422.
-    if (this.catalogTracker != null && this.serverManager.getOnlineServers().isEmpty()) {
-      this.catalogTracker.stop();
+  void checkServiceStarted() throws ServerNotRunningYetException {
+    if (!serviceStarted) {
+      throw new ServerNotRunningYetException("Server is not running yet");
     }
   }
 
-  @Override
-  public boolean isStopped() {
-    return this.stopped;
-  }
-
-  @Override
-  public boolean isAborted() {
-    return this.abort;
-  }
-
-  void checkInitialized() throws PleaseHoldException {
+  void checkInitialized() throws PleaseHoldException, ServerNotRunningYetException {
+    checkServiceStarted();
     if (!this.initialized) {
       throw new PleaseHoldException("Master is initializing");
     }
   }
 
+  void checkNamespaceManagerReady() throws IOException {
+    checkInitialized();
+    if (tableNamespaceManager == null ||
+        !tableNamespaceManager.isTableAvailableAndInitialized()) {
+      throw new IOException("Table Namespace Manager not ready yet, try again later");
+    }
+  }
   /**
    * Report whether this master is currently the active master or not.
    * If not active master, we are parked on ZK waiting to become active.
@@ -2571,178 +1596,10 @@ MasterServices, Server {
     return this.initializationBeforeMetaAssignment;
   }
 
-  @Override
-  public AssignRegionResponse assignRegion(RpcController controller, AssignRegionRequest req)
-  throws ServiceException {
-    try {
-      final byte [] regionName = req.getRegion().getValue().toByteArray();
-      RegionSpecifierType type = req.getRegion().getType();
-      AssignRegionResponse arr = AssignRegionResponse.newBuilder().build();
-
-      checkInitialized();
-      if (type != RegionSpecifierType.REGION_NAME) {
-        LOG.warn("assignRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
-          + " actual: " + type);
-      }
-      HRegionInfo regionInfo = assignmentManager.getRegionStates().getRegionInfo(regionName);
-      if (regionInfo == null) throw new UnknownRegionException(Bytes.toString(regionName));
-      if (cpHost != null) {
-        if (cpHost.preAssign(regionInfo)) {
-          return arr;
-        }
-      }
-      LOG.info(getClientIdAuditPrefix() + " assign " + regionInfo.getRegionNameAsString());
-      assignmentManager.assign(regionInfo, true, true);
-      if (cpHost != null) {
-        cpHost.postAssign(regionInfo);
-      }
-
-      return arr;
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-  }
-
   public void assignRegion(HRegionInfo hri) {
     assignmentManager.assign(hri, true);
   }
 
-  @Override
-  public UnassignRegionResponse unassignRegion(RpcController controller, UnassignRegionRequest req)
-  throws ServiceException {
-    try {
-      final byte [] regionName = req.getRegion().getValue().toByteArray();
-      RegionSpecifierType type = req.getRegion().getType();
-      final boolean force = req.getForce();
-      UnassignRegionResponse urr = UnassignRegionResponse.newBuilder().build();
-
-      checkInitialized();
-      if (type != RegionSpecifierType.REGION_NAME) {
-        LOG.warn("unassignRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
-          + " actual: " + type);
-      }
-      Pair<HRegionInfo, ServerName> pair =
-        MetaReader.getRegion(this.catalogTracker, regionName);
-      if (pair == null) throw new UnknownRegionException(Bytes.toString(regionName));
-      HRegionInfo hri = pair.getFirst();
-      if (cpHost != null) {
-        if (cpHost.preUnassign(hri, force)) {
-          return urr;
-        }
-      }
-      LOG.debug(getClientIdAuditPrefix() + " unassign " + hri.getRegionNameAsString()
-          + " in current location if it is online and reassign.force=" + force);
-      this.assignmentManager.unassign(hri, force);
-      if (this.assignmentManager.getRegionStates().isRegionOffline(hri)) {
-        LOG.debug("Region " + hri.getRegionNameAsString()
-            + " is not online on any region server, reassigning it.");
-        assignRegion(hri);
-      }
-      if (cpHost != null) {
-        cpHost.postUnassign(hri, force);
-      }
-
-      return urr;
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-  }
-
-  /**
-   * Get list of TableDescriptors for requested tables.
-   * @param controller Unused (set to null).
-   * @param req GetTableDescriptorsRequest that contains:
-   * - tableNames: requested tables, or if empty, all are requested
-   * @return GetTableDescriptorsResponse
-   * @throws ServiceException
-   */
-  @Override
-  public GetTableDescriptorsResponse getTableDescriptors(
-	      RpcController controller, GetTableDescriptorsRequest req) throws ServiceException {
-    List<HTableDescriptor> descriptors = new ArrayList<HTableDescriptor>();
-    List<TableName> tableNameList = new ArrayList<TableName>();
-    for(HBaseProtos.TableName tableNamePB: req.getTableNamesList()) {
-      tableNameList.add(ProtobufUtil.toTableName(tableNamePB));
-    }
-    boolean bypass = false;
-    if (this.cpHost != null) {
-      try {
-        bypass = this.cpHost.preGetTableDescriptors(tableNameList, descriptors);
-      } catch (IOException ioe) {
-        throw new ServiceException(ioe);
-      }
-    }
-
-    if (!bypass) {
-      if (req.getTableNamesCount() == 0) {
-        // request for all TableDescriptors
-        Map<String, HTableDescriptor> descriptorMap = null;
-        try {
-          descriptorMap = this.tableDescriptors.getAll();
-        } catch (IOException e) {
-          LOG.warn("Failed getting all descriptors", e);
-        }
-        if (descriptorMap != null) {
-          for(HTableDescriptor desc: descriptorMap.values()) {
-            if(!desc.getTableName().isSystemTable()) {
-              descriptors.add(desc);
-            }
-          }
-        }
-      } else {
-        for (TableName s: tableNameList) {
-          try {
-            HTableDescriptor desc = this.tableDescriptors.get(s);
-            if (desc != null) {
-              descriptors.add(desc);
-            }
-          } catch (IOException e) {
-            LOG.warn("Failed getting descriptor for " + s, e);
-          }
-        }
-      }
-
-      if (this.cpHost != null) {
-        try {
-          this.cpHost.postGetTableDescriptors(descriptors);
-        } catch (IOException ioe) {
-          throw new ServiceException(ioe);
-        }
-      }
-    }
-
-    GetTableDescriptorsResponse.Builder builder = GetTableDescriptorsResponse.newBuilder();
-    for (HTableDescriptor htd: descriptors) {
-      builder.addTableSchema(htd.convert());
-    }
-    return builder.build();
-  }
-
-  /**
-   * Get list of userspace table names
-   * @param controller Unused (set to null).
-   * @param req GetTableNamesRequest
-   * @return GetTableNamesResponse
-   * @throws ServiceException
-   */
-  @Override
-  public GetTableNamesResponse getTableNames(
-        RpcController controller, GetTableNamesRequest req) throws ServiceException {
-    try {
-      Collection<HTableDescriptor> descriptors = this.tableDescriptors.getAll().values();
-      GetTableNamesResponse.Builder builder = GetTableNamesResponse.newBuilder();
-      for (HTableDescriptor descriptor: descriptors) {
-        if (descriptor.getTableName().isSystemTable()) {
-          continue;
-        }
-        builder.addTableNames(ProtobufUtil.toProtoTableName(descriptor.getTableName()));
-      }
-      return builder.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
   /**
    * Compute the average load across all region servers.
    * Currently, this uses a very naive computation - just uses the number of
@@ -2761,42 +1618,6 @@ MasterServices, Server {
     return regionStates.getAverageLoad();
   }
 
-  /**
-   * Offline specified region from master's in-memory state. It will not attempt to
-   * reassign the region as in unassign.
-   *
-   * This is a special method that should be used by experts or hbck.
-   *
-   */
-  @Override
-  public OfflineRegionResponse offlineRegion(RpcController controller, OfflineRegionRequest request)
-  throws ServiceException {
-    final byte [] regionName = request.getRegion().getValue().toByteArray();
-    RegionSpecifierType type = request.getRegion().getType();
-    if (type != RegionSpecifierType.REGION_NAME) {
-      LOG.warn("moveRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
-        + " actual: " + type);
-    }
-
-    try {
-      Pair<HRegionInfo, ServerName> pair =
-        MetaReader.getRegion(this.catalogTracker, regionName);
-      if (pair == null) throw new UnknownRegionException(Bytes.toStringBinary(regionName));
-      HRegionInfo hri = pair.getFirst();
-      if (cpHost != null) {
-        cpHost.preRegionOffline(hri);
-      }
-      LOG.info(getClientIdAuditPrefix() + " offline " + hri.getRegionNameAsString());
-      this.assignmentManager.regionOffline(hri);
-      if (cpHost != null) {
-        cpHost.postRegionOffline(hri);
-      }
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-    return OfflineRegionResponse.newBuilder().build();
-  }
-
   @Override
   public boolean registerService(Service instance) {
     /*
@@ -2817,59 +1638,6 @@ MasterServices, Server {
     return true;
   }
 
-  @Override
-  public ClientProtos.CoprocessorServiceResponse execMasterService(final RpcController controller,
-      final ClientProtos.CoprocessorServiceRequest request) throws ServiceException {
-    try {
-      ServerRpcController execController = new ServerRpcController();
-
-      ClientProtos.CoprocessorServiceCall call = request.getCall();
-      String serviceName = call.getServiceName();
-      String methodName = call.getMethodName();
-      if (!coprocessorServiceHandlers.containsKey(serviceName)) {
-        throw new UnknownProtocolException(null,
-            "No registered master coprocessor service found for name "+serviceName);
-      }
-
-      Service service = coprocessorServiceHandlers.get(serviceName);
-      Descriptors.ServiceDescriptor serviceDesc = service.getDescriptorForType();
-      Descriptors.MethodDescriptor methodDesc = serviceDesc.findMethodByName(methodName);
-      if (methodDesc == null) {
-        throw new UnknownProtocolException(service.getClass(),
-            "Unknown method "+methodName+" called on master service "+serviceName);
-      }
-
-      //invoke the method
-      Message execRequest = service.getRequestPrototype(methodDesc).newBuilderForType()
-          .mergeFrom(call.getRequest()).build();
-      final Message.Builder responseBuilder =
-          service.getResponsePrototype(methodDesc).newBuilderForType();
-      service.callMethod(methodDesc, execController, execRequest, new RpcCallback<Message>() {
-        @Override
-        public void run(Message message) {
-          if (message != null) {
-            responseBuilder.mergeFrom(message);
-          }
-        }
-      });
-      Message execResult = responseBuilder.build();
-
-      if (execController.getFailedOn() != null) {
-        throw execController.getFailedOn();
-      }
-      ClientProtos.CoprocessorServiceResponse.Builder builder =
-          ClientProtos.CoprocessorServiceResponse.newBuilder();
-      builder.setRegion(RequestConverter.buildRegionSpecifier(
-          RegionSpecifierType.REGION_NAME, HConstants.EMPTY_BYTE_ARRAY));
-      builder.setValue(
-          builder.getValueBuilder().setName(execResult.getClass().getName())
-              .setValue(execResult.toByteString()));
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
   /**
    * Utility for constructing an instance of the passed HMaster class.
    * @param masterClass
@@ -2915,331 +1683,10 @@ MasterServices, Server {
     return this.snapshotManager;
   }
 
-  /**
-   * Triggers an asynchronous attempt to take a snapshot.
-   * {@inheritDoc}
-   */
-  @Override
-  public SnapshotResponse snapshot(RpcController controller, SnapshotRequest request)
-      throws ServiceException {
-    try {
-      this.snapshotManager.checkSnapshotSupport();
-    } catch (UnsupportedOperationException e) {
-      throw new ServiceException(e);
-    }
-
-    LOG.info(getClientIdAuditPrefix() + " snapshot request for:" +
-        ClientSnapshotDescriptionUtils.toString(request.getSnapshot()));
-    // get the snapshot information
-    SnapshotDescription snapshot = SnapshotDescriptionUtils.validate(request.getSnapshot(),
-      this.conf);
-    try {
-      snapshotManager.takeSnapshot(snapshot);
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-
-    // send back the max amount of time the client should wait for the snapshot to complete
-    long waitTime = SnapshotDescriptionUtils.getMaxMasterTimeout(conf, snapshot.getType(),
-      SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
-    return SnapshotResponse.newBuilder().setExpectedTimeout(waitTime).build();
-  }
-
-  /**
-   * List the currently available/stored snapshots. Any in-progress snapshots are ignored
-   */
-  @Override
-  public GetCompletedSnapshotsResponse getCompletedSnapshots(RpcController controller,
-      GetCompletedSnapshotsRequest request) throws ServiceException {
-    try {
-      GetCompletedSnapshotsResponse.Builder builder = GetCompletedSnapshotsResponse.newBuilder();
-      List<SnapshotDescription> snapshots = snapshotManager.getCompletedSnapshots();
-
-      // convert to protobuf
-      for (SnapshotDescription snapshot : snapshots) {
-        builder.addSnapshots(snapshot);
-      }
-      return builder.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  /**
-   * Execute Delete Snapshot operation.
-   * @return DeleteSnapshotResponse (a protobuf wrapped void) if the snapshot existed and was
-   *    deleted properly.
-   * @throws ServiceException wrapping SnapshotDoesNotExistException if specified snapshot did not
-   *    exist.
-   */
-  @Override
-  public DeleteSnapshotResponse deleteSnapshot(RpcController controller,
-      DeleteSnapshotRequest request) throws ServiceException {
-    try {
-      this.snapshotManager.checkSnapshotSupport();
-    } catch (UnsupportedOperationException e) {
-      throw new ServiceException(e);
-    }
-
-    try {
-      LOG.info(getClientIdAuditPrefix() + " delete " + request.getSnapshot());
-      snapshotManager.deleteSnapshot(request.getSnapshot());
-      return DeleteSnapshotResponse.newBuilder().build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  /**
-   * Checks if the specified snapshot is done.
-   * @return true if the snapshot is in file system ready to use,
-   *   false if the snapshot is in the process of completing
-   * @throws ServiceException wrapping UnknownSnapshotException if invalid snapshot, or
-   *  a wrapped HBaseSnapshotException with progress failure reason.
-   */
-  @Override
-  public IsSnapshotDoneResponse isSnapshotDone(RpcController controller,
-      IsSnapshotDoneRequest request) throws ServiceException {
-    LOG.debug("Checking to see if snapshot from request:" +
-        ClientSnapshotDescriptionUtils.toString(request.getSnapshot()) + " is done");
-    try {
-      IsSnapshotDoneResponse.Builder builder = IsSnapshotDoneResponse.newBuilder();
-      boolean done = snapshotManager.isSnapshotDone(request.getSnapshot());
-      builder.setDone(done);
-      return builder.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  /**
-   * Execute Restore/Clone snapshot operation.
-   *
-   * <p>If the specified table exists a "Restore" is executed, replacing the table
-   * schema and directory data with the content of the snapshot.
-   * The table must be disabled, or a UnsupportedOperationException will be thrown.
-   *
-   * <p>If the table doesn't exist a "Clone" is executed, a new table is created
-   * using the schema at the time of the snapshot, and the content of the snapshot.
-   *
-   * <p>The restore/clone operation does not require copying HFiles. Since HFiles
-   * are immutable the table can point to and use the same files as the original one.
-   */
-  @Override
-  public RestoreSnapshotResponse restoreSnapshot(RpcController controller,
-      RestoreSnapshotRequest request) throws ServiceException {
-    try {
-      this.snapshotManager.checkSnapshotSupport();
-    } catch (UnsupportedOperationException e) {
-      throw new ServiceException(e);
-    }
-
-    // ensure namespace exists
-    try {
-      TableName dstTable = TableName.valueOf(request.getSnapshot().getTable());
-      getNamespaceDescriptor(dstTable.getNamespaceAsString());
-    } catch (IOException ioe) {
-      throw new ServiceException(ioe);
-    }
-
-    try {
-      SnapshotDescription reqSnapshot = request.getSnapshot();
-      snapshotManager.restoreSnapshot(reqSnapshot);
-      return RestoreSnapshotResponse.newBuilder().build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  /**
-   * Returns the status of the requested snapshot restore/clone operation.
-   * This method is not exposed to the user, it is just used internally by HBaseAdmin
-   * to verify if the restore is completed.
-   *
-   * No exceptions are thrown if the restore is not running, the result will be "done".
-   *
-   * @return done <tt>true</tt> if the restore/clone operation is completed.
-   * @throws ServiceException if the operation failed.
-   */
-  @Override
-  public IsRestoreSnapshotDoneResponse isRestoreSnapshotDone(RpcController controller,
-      IsRestoreSnapshotDoneRequest request) throws ServiceException {
-    try {
-      SnapshotDescription snapshot = request.getSnapshot();
-      IsRestoreSnapshotDoneResponse.Builder builder = IsRestoreSnapshotDoneResponse.newBuilder();
-      boolean done = snapshotManager.isRestoreDone(snapshot);
-      builder.setDone(done);
-      return builder.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  /**
-   * Triggers an asynchronous attempt to run a distributed procedure.
-   * {@inheritDoc}
-   */
-  @Override
-  public ExecProcedureResponse execProcedure(RpcController controller,
-      ExecProcedureRequest request) throws ServiceException {
-    ProcedureDescription desc = request.getProcedure();
-    MasterProcedureManager mpm = this.mpmHost.getProcedureManager(desc
-        .getSignature());
-    if (mpm == null) {
-      throw new ServiceException("The procedure is not registered: "
-          + desc.getSignature());
-    }
-
-    LOG.info(getClientIdAuditPrefix() + " procedure request for: "
-        + desc.getSignature());
-
-    try {
-      mpm.execProcedure(desc);
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-
-    // send back the max amount of time the client should wait for the procedure
-    // to complete
-    long waitTime = SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME;
-    return ExecProcedureResponse.newBuilder().setExpectedTimeout(waitTime)
-        .build();
-  }
-
-  /**
-   * Checks if the specified procedure is done.
-   * @return true if the procedure is done,
-   *   false if the procedure is in the process of completing
-   * @throws ServiceException if invalid procedure, or
-   *  a failed procedure with progress failure reason.
-   */
-  @Override
-  public IsProcedureDoneResponse isProcedureDone(RpcController controller,
-      IsProcedureDoneRequest request) throws ServiceException {
-    ProcedureDescription desc = request.getProcedure();
-    MasterProcedureManager mpm = this.mpmHost.getProcedureManager(desc
-        .getSignature());
-    if (mpm == null) {
-      throw new ServiceException("The procedure is not registered: "
-          + desc.getSignature());
-    }
-    LOG.debug("Checking to see if procedure from request:"
-        + desc.getSignature() + " is done");
-
-    try {
-      IsProcedureDoneResponse.Builder builder = IsProcedureDoneResponse
-          .newBuilder();
-      boolean done = mpm.isProcedureDone(desc);
-      builder.setDone(done);
-      return builder.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public ModifyNamespaceResponse modifyNamespace(RpcController controller,
-      ModifyNamespaceRequest request) throws ServiceException {
-    try {
-      modifyNamespace(ProtobufUtil.toNamespaceDescriptor(request.getNamespaceDescriptor()));
-      return ModifyNamespaceResponse.getDefaultInstance();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public CreateNamespaceResponse createNamespace(RpcController controller,
-     CreateNamespaceRequest request) throws ServiceException {
-    try {
-      createNamespace(ProtobufUtil.toNamespaceDescriptor(request.getNamespaceDescriptor()));
-      return CreateNamespaceResponse.getDefaultInstance();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public DeleteNamespaceResponse deleteNamespace(RpcController controller,
-      DeleteNamespaceRequest request) throws ServiceException {
-    try {
-      deleteNamespace(request.getNamespaceName());
-      return DeleteNamespaceResponse.getDefaultInstance();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public GetNamespaceDescriptorResponse getNamespaceDescriptor(
-      RpcController controller, GetNamespaceDescriptorRequest request)
-      throws ServiceException {
-    try {
-      return GetNamespaceDescriptorResponse.newBuilder()
-          .setNamespaceDescriptor(
-              ProtobufUtil.toProtoNamespaceDescriptor(getNamespaceDescriptor(request.getNamespaceName())))
-          .build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public ListNamespaceDescriptorsResponse listNamespaceDescriptors(
-      RpcController controller, ListNamespaceDescriptorsRequest request)
-      throws ServiceException {
-    try {
-      ListNamespaceDescriptorsResponse.Builder response =
-          ListNamespaceDescriptorsResponse.newBuilder();
-      for(NamespaceDescriptor ns: listNamespaceDescriptors()) {
-        response.addNamespaceDescriptor(ProtobufUtil.toProtoNamespaceDescriptor(ns));
-      }
-      return response.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public ListTableDescriptorsByNamespaceResponse listTableDescriptorsByNamespace(
-      RpcController controller, ListTableDescriptorsByNamespaceRequest request)
-      throws ServiceException {
-    try {
-      ListTableDescriptorsByNamespaceResponse.Builder b =
-          ListTableDescriptorsByNamespaceResponse.newBuilder();
-      for(HTableDescriptor htd: listTableDescriptorsByNamespace(request.getNamespaceName())) {
-        b.addTableSchema(htd.convert());
-      }
-      return b.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  @Override
-  public ListTableNamesByNamespaceResponse listTableNamesByNamespace(
-      RpcController controller, ListTableNamesByNamespaceRequest request)
-      throws ServiceException {
-    try {
-      ListTableNamesByNamespaceResponse.Builder b =
-          ListTableNamesByNamespaceResponse.newBuilder();
-      for (TableName tableName: listTableNamesByNamespace(request.getNamespaceName())) {
-        b.addTableName(ProtobufUtil.toProtoTableName(tableName));
-      }
-      return b.build();
-    } catch (IOException e) {
-      throw new ServiceException(e);
-    }
-  }
-
-  private boolean isHealthCheckerConfigured() {
-    String healthScriptLocation = this.conf.get(HConstants.HEALTH_SCRIPT_LOC);
-    return org.apache.commons.lang.StringUtils.isNotBlank(healthScriptLocation);
-  }
-
   @Override
   public void createNamespace(NamespaceDescriptor descriptor) throws IOException {
     TableName.isLegalNamespaceName(Bytes.toBytes(descriptor.getName()));
+    checkNamespaceManagerReady();
     if (cpHost != null) {
       if (cpHost.preCreateNamespace(descriptor)) {
         return;
@@ -3255,6 +1702,7 @@ MasterServices, Server {
   @Override
   public void modifyNamespace(NamespaceDescriptor descriptor) throws IOException {
     TableName.isLegalNamespaceName(Bytes.toBytes(descriptor.getName()));
+    checkNamespaceManagerReady();
     if (cpHost != null) {
       if (cpHost.preModifyNamespace(descriptor)) {
         return;
@@ -3269,6 +1717,7 @@ MasterServices, Server {
 
   @Override
   public void deleteNamespace(String name) throws IOException {
+    checkNamespaceManagerReady();
     if (cpHost != null) {
       if (cpHost.preDeleteNamespace(name)) {
         return;
@@ -3283,11 +1732,7 @@ MasterServices, Server {
 
   @Override
   public NamespaceDescriptor getNamespaceDescriptor(String name) throws IOException {
-    boolean ready = tableNamespaceManager != null &&
-        tableNamespaceManager.isTableAvailableAndInitialized();
-    if (!ready) {
-      throw new IOException("Table Namespace Manager not ready yet, try again later");
-    }
+    checkNamespaceManagerReady();
     NamespaceDescriptor nsd = tableNamespaceManager.get(name);
     if (nsd == null) {
       throw new NamespaceNotFoundException(name);
@@ -3297,6 +1742,7 @@ MasterServices, Server {
 
   @Override
   public List<NamespaceDescriptor> listNamespaceDescriptors() throws IOException {
+    checkNamespaceManagerReady();
     return Lists.newArrayList(tableNamespaceManager.list());
   }
 
@@ -3315,5 +1761,4 @@ MasterServices, Server {
     }
     return tableNames;
   }
-
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
index 78078d8..16452f7 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterDumpServlet.java
@@ -34,6 +34,7 @@ import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.monitoring.LogMonitoring;
 import org.apache.hadoop.hbase.monitoring.StateDumpServlet;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
+import org.apache.hadoop.hbase.regionserver.RSDumpServlet;
 import org.apache.hadoop.util.ReflectionUtils;
 
 @InterfaceAudience.Private
@@ -94,7 +95,12 @@ public class MasterDumpServlet extends StateDumpServlet {
     out.println(LINE);
     long tailKb = getTailKbParam(request);
     LogMonitoring.dumpTailOfLogs(out, tailKb);
-    
+
+    out.println("\n\nRS Queue:");
+    out.println(LINE);
+    if(isShowQueueDump(conf)) {
+      RSDumpServlet.dumpQueue(master, out);
+    }
     out.flush();
   }
   
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
index 26da01e..59e93ad 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java
@@ -105,7 +105,7 @@ public class MasterFileSystem {
     }
   };
 
-  public MasterFileSystem(Server master, MasterServices services, boolean masterRecovery)
+  public MasterFileSystem(Server master, MasterServices services)
   throws IOException {
     this.conf = master.getConfiguration();
     this.master = master;
@@ -129,7 +129,7 @@ public class MasterFileSystem {
     HFileSystem.addLocationsOrderInterceptor(conf);
     this.splitLogManager = new SplitLogManager(master.getZooKeeper(),
       master.getConfiguration(), master, services,
-      master.getServerName(), masterRecovery);
+      master.getServerName());
   }
 
   /**
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
new file mode 100644
index 0000000..5162fa8
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java
@@ -0,0 +1,1194 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.ServerLoad;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.UnknownRegionException;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.exceptions.MergeRegionException;
+import org.apache.hadoop.hbase.exceptions.UnknownProtocolException;
+import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;
+import org.apache.hadoop.hbase.ipc.ServerRpcController;
+import org.apache.hadoop.hbase.procedure.MasterProcedureManager;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.RequestConverter;
+import org.apache.hadoop.hbase.protobuf.ResponseConverter;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
+import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameStringPair;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ProcedureDescription;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AddColumnRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AddColumnResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AssignRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.AssignRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.BalanceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.BalanceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateNamespaceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateNamespaceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.CreateTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteColumnRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteColumnResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteNamespaceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteNamespaceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteSnapshotRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteSnapshotResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DeleteTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DisableTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DisableTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DispatchMergingRegionsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.DispatchMergingRegionsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableCatalogJanitorRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableCatalogJanitorResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.EnableTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ExecProcedureRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ExecProcedureResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetClusterStatusRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetClusterStatusResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetCompletedSnapshotsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetCompletedSnapshotsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetNamespaceDescriptorRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetNamespaceDescriptorResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetSchemaAlterStatusRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetSchemaAlterStatusResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableNamesRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableNamesResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsCatalogJanitorEnabledRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsCatalogJanitorEnabledResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsMasterRunningRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsMasterRunningResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsProcedureDoneRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsProcedureDoneResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsRestoreSnapshotDoneRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsRestoreSnapshotDoneResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListNamespaceDescriptorsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListNamespaceDescriptorsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableDescriptorsByNamespaceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableDescriptorsByNamespaceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableNamesByNamespaceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ListTableNamesByNamespaceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MasterService;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyColumnRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyColumnResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyNamespaceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyNamespaceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ModifyTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MoveRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MoveRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.OfflineRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.OfflineRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RestoreSnapshotRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RestoreSnapshotResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RunCatalogScanRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.RunCatalogScanResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SetBalancerRunningRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SetBalancerRunningResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ShutdownRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.ShutdownResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SnapshotRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.SnapshotResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.StopMasterRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.StopMasterResponse;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.UnassignRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.UnassignRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdResponse;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportResponse;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStatusService;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRSFatalErrorRequest;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRSFatalErrorResponse;
+import org.apache.hadoop.hbase.regionserver.RSRpcServices;
+import org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.zookeeper.KeeperException;
+
+import com.google.protobuf.Descriptors;
+import com.google.protobuf.Message;
+import com.google.protobuf.RpcCallback;
+import com.google.protobuf.RpcController;
+import com.google.protobuf.Service;
+import com.google.protobuf.ServiceException;
+
+/**
+ * Implements the master RPC services.
+ */
+@InterfaceAudience.Private
+public class MasterRpcServices extends RSRpcServices
+    implements MasterService.BlockingInterface, RegionServerStatusService.BlockingInterface {
+  protected static final Log LOG = LogFactory.getLog(MasterRpcServices.class.getName());
+
+  private final HMaster master;
+
+  /**
+   * @return Subset of configuration to pass initializing regionservers: e.g.
+   * the filesystem to use and root directory to use.
+   */
+  private RegionServerStartupResponse.Builder createConfigurationSubset() {
+    RegionServerStartupResponse.Builder resp = addConfig(
+      RegionServerStartupResponse.newBuilder(), HConstants.HBASE_DIR);
+    return addConfig(resp, "fs.default.name");
+  }
+
+  private RegionServerStartupResponse.Builder addConfig(
+      final RegionServerStartupResponse.Builder resp, final String key) {
+    NameStringPair.Builder entry = NameStringPair.newBuilder()
+      .setName(key)
+      .setValue(master.getConfiguration().get(key));
+    resp.addMapEntries(entry.build());
+    return resp;
+  }
+
+  public MasterRpcServices(HMaster m) throws IOException {
+    super(m);
+    master = m;
+  }
+
+  enum BalanceSwitchMode {
+    SYNC,
+    ASYNC
+  }
+
+  /**
+   * Assigns balancer switch according to BalanceSwitchMode
+   * @param b new balancer switch
+   * @param mode BalanceSwitchMode
+   * @return old balancer switch
+   */
+  boolean switchBalancer(final boolean b, BalanceSwitchMode mode) throws IOException {
+    boolean oldValue = master.loadBalancerTracker.isBalancerOn();
+    boolean newValue = b;
+    try {
+      if (master.cpHost != null) {
+        newValue = master.cpHost.preBalanceSwitch(newValue);
+      }
+      try {
+        if (mode == BalanceSwitchMode.SYNC) {
+          synchronized (master.balancer) {
+            master.loadBalancerTracker.setBalancerOn(newValue);
+          }
+        } else {
+          master.loadBalancerTracker.setBalancerOn(newValue);
+        }
+      } catch (KeeperException ke) {
+        throw new IOException(ke);
+      }
+      LOG.info(master.getClientIdAuditPrefix() + " set balanceSwitch=" + newValue);
+      if (master.cpHost != null) {
+        master.cpHost.postBalanceSwitch(oldValue, newValue);
+      }
+    } catch (IOException ioe) {
+      LOG.warn("Error flipping balance switch", ioe);
+    }
+    return oldValue;
+  }
+
+  boolean synchronousBalanceSwitch(final boolean b) throws IOException {
+    return switchBalancer(b, BalanceSwitchMode.SYNC);
+  }
+
+  /**
+   * @return list of blocking services and their security info classes that this server supports
+   */
+  protected List<BlockingServiceAndInterface> getServices() {
+    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(4);
+    bssi.add(new BlockingServiceAndInterface(
+      MasterService.newReflectiveBlockingService(this),
+      MasterService.BlockingInterface.class));
+    bssi.add(new BlockingServiceAndInterface(
+      RegionServerStatusService.newReflectiveBlockingService(this),
+      RegionServerStatusService.BlockingInterface.class));
+    bssi.addAll(super.getServices());
+    return bssi;
+  }
+
+  @Override
+  public GetLastFlushedSequenceIdResponse getLastFlushedSequenceId(RpcController controller,
+      GetLastFlushedSequenceIdRequest request) throws ServiceException {
+    try {
+      master.checkServiceStarted();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    byte[] regionName = request.getRegionName().toByteArray();
+    long seqId = master.serverManager.getLastFlushedSequenceId(regionName);
+    return ResponseConverter.buildGetLastFlushedSequenceIdResponse(seqId);
+  }
+
+  @Override
+  public RegionServerReportResponse regionServerReport(
+      RpcController controller, RegionServerReportRequest request) throws ServiceException {
+    try {
+      master.checkServiceStarted();
+      ClusterStatusProtos.ServerLoad sl = request.getLoad();
+      ServerName serverName = ProtobufUtil.toServerName(request.getServer());
+      ServerLoad oldLoad = master.serverManager.getLoad(serverName);
+      master.serverManager.regionServerReport(serverName, new ServerLoad(sl));
+      if (sl != null && master.metricsMaster != null) {
+        // Up our metrics.
+        master.metricsMaster.incrementRequests(sl.getTotalNumberOfRequests()
+          - (oldLoad != null ? oldLoad.getTotalNumberOfRequests() : 0));
+      }
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return RegionServerReportResponse.newBuilder().build();
+  }
+
+  @Override
+  public RegionServerStartupResponse regionServerStartup(
+      RpcController controller, RegionServerStartupRequest request) throws ServiceException {
+    // Register with server manager
+    try {
+      master.checkServiceStarted();
+      InetAddress ia = master.getRemoteInetAddress(
+        request.getPort(), request.getServerStartCode());
+      ServerName rs = master.serverManager.regionServerStartup(ia, request.getPort(),
+        request.getServerStartCode(), request.getServerCurrentTime());
+
+      // Send back some config info
+      RegionServerStartupResponse.Builder resp = createConfigurationSubset();
+      NameStringPair.Builder entry = NameStringPair.newBuilder()
+        .setName(HConstants.KEY_FOR_HOSTNAME_SEEN_BY_MASTER)
+        .setValue(rs.getHostname());
+      resp.addMapEntries(entry.build());
+
+      return resp.build();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+
+  @Override
+  public ReportRSFatalErrorResponse reportRSFatalError(
+      RpcController controller, ReportRSFatalErrorRequest request) throws ServiceException {
+    String errorText = request.getErrorMessage();
+    ServerName sn = ProtobufUtil.toServerName(request.getServer());
+    String msg = "Region server " + sn
+      + " reported a fatal error:\n" + errorText;
+    LOG.error(msg);
+    master.rsFatals.add(msg);
+    return ReportRSFatalErrorResponse.newBuilder().build();
+  }
+
+  @Override
+  public AddColumnResponse addColumn(RpcController controller,
+      AddColumnRequest req) throws ServiceException {
+    try {
+      master.addColumn(ProtobufUtil.toTableName(req.getTableName()),
+        HColumnDescriptor.convert(req.getColumnFamilies()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return AddColumnResponse.newBuilder().build();
+  }
+
+  @Override
+  public AssignRegionResponse assignRegion(RpcController controller,
+      AssignRegionRequest req) throws ServiceException {
+    try {
+      final byte [] regionName = req.getRegion().getValue().toByteArray();
+      RegionSpecifierType type = req.getRegion().getType();
+      AssignRegionResponse arr = AssignRegionResponse.newBuilder().build();
+
+      master.checkInitialized();
+      if (type != RegionSpecifierType.REGION_NAME) {
+        LOG.warn("assignRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
+          + " actual: " + type);
+      }
+      RegionStates regionStates = master.assignmentManager.getRegionStates();
+      HRegionInfo regionInfo = regionStates.getRegionInfo(regionName);
+      if (regionInfo == null) throw new UnknownRegionException(Bytes.toString(regionName));
+      if (master.cpHost != null) {
+        if (master.cpHost.preAssign(regionInfo)) {
+          return arr;
+        }
+      }
+      LOG.info(master.getClientIdAuditPrefix()
+        + " assign " + regionInfo.getRegionNameAsString());
+      master.assignmentManager.assign(regionInfo, true, true);
+      if (master.cpHost != null) {
+        master.cpHost.postAssign(regionInfo);
+      }
+      return arr;
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+
+  @Override
+  public BalanceResponse balance(RpcController controller,
+      BalanceRequest request) throws ServiceException {
+    try {
+      return BalanceResponse.newBuilder().setBalancerRan(master.balance()).build();
+    } catch (IOException ex) {
+      throw new ServiceException(ex);
+    }
+  }
+
+  @Override
+  public CreateNamespaceResponse createNamespace(RpcController controller,
+     CreateNamespaceRequest request) throws ServiceException {
+    try {
+      master.createNamespace(ProtobufUtil.toNamespaceDescriptor(request.getNamespaceDescriptor()));
+      return CreateNamespaceResponse.getDefaultInstance();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public CreateTableResponse createTable(RpcController controller, CreateTableRequest req)
+  throws ServiceException {
+    HTableDescriptor hTableDescriptor = HTableDescriptor.convert(req.getTableSchema());
+    byte [][] splitKeys = ProtobufUtil.getSplitKeysArray(req);
+    try {
+      master.createTable(hTableDescriptor, splitKeys);
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return CreateTableResponse.newBuilder().build();
+  }
+
+  @Override
+  public DeleteColumnResponse deleteColumn(RpcController controller,
+      DeleteColumnRequest req) throws ServiceException {
+    try {
+      master.deleteColumn(ProtobufUtil.toTableName(req.getTableName()),
+        req.getColumnName().toByteArray());
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return DeleteColumnResponse.newBuilder().build();
+  }
+
+  @Override
+  public DeleteNamespaceResponse deleteNamespace(RpcController controller,
+      DeleteNamespaceRequest request) throws ServiceException {
+    try {
+      master.deleteNamespace(request.getNamespaceName());
+      return DeleteNamespaceResponse.getDefaultInstance();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  /**
+   * Execute Delete Snapshot operation.
+   * @return DeleteSnapshotResponse (a protobuf wrapped void) if the snapshot existed and was
+   *    deleted properly.
+   * @throws ServiceException wrapping SnapshotDoesNotExistException if specified snapshot did not
+   *    exist.
+   */
+  @Override
+  public DeleteSnapshotResponse deleteSnapshot(RpcController controller,
+      DeleteSnapshotRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      master.snapshotManager.checkSnapshotSupport();
+
+      LOG.info(master.getClientIdAuditPrefix() + " delete " + request.getSnapshot());
+      master.snapshotManager.deleteSnapshot(request.getSnapshot());
+      return DeleteSnapshotResponse.newBuilder().build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public DeleteTableResponse deleteTable(RpcController controller,
+      DeleteTableRequest request) throws ServiceException {
+    try {
+      master.deleteTable(ProtobufUtil.toTableName(request.getTableName()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return DeleteTableResponse.newBuilder().build();
+  }
+
+  @Override
+  public DisableTableResponse disableTable(RpcController controller,
+      DisableTableRequest request) throws ServiceException {
+    try {
+      master.disableTable(ProtobufUtil.toTableName(request.getTableName()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return DisableTableResponse.newBuilder().build();
+  }
+
+  @Override
+  public DispatchMergingRegionsResponse dispatchMergingRegions(RpcController c,
+      DispatchMergingRegionsRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+
+    final byte[] encodedNameOfRegionA = request.getRegionA().getValue()
+      .toByteArray();
+    final byte[] encodedNameOfRegionB = request.getRegionB().getValue()
+      .toByteArray();
+    final boolean forcible = request.getForcible();
+    if (request.getRegionA().getType() != RegionSpecifierType.ENCODED_REGION_NAME
+        || request.getRegionB().getType() != RegionSpecifierType.ENCODED_REGION_NAME) {
+      LOG.warn("mergeRegions specifier type: expected: "
+        + RegionSpecifierType.ENCODED_REGION_NAME + " actual: region_a="
+        + request.getRegionA().getType() + ", region_b="
+        + request.getRegionB().getType());
+    }
+    RegionStates regionStates = master.assignmentManager.getRegionStates();
+    RegionState regionStateA = regionStates.getRegionState(Bytes.toString(encodedNameOfRegionA));
+    RegionState regionStateB = regionStates.getRegionState(Bytes.toString(encodedNameOfRegionB));
+    if (regionStateA == null || regionStateB == null) {
+      throw new ServiceException(new UnknownRegionException(
+          Bytes.toStringBinary(regionStateA == null ? encodedNameOfRegionA
+              : encodedNameOfRegionB)));
+    }
+
+    if (!regionStateA.isOpened() || !regionStateB.isOpened()) {
+      throw new ServiceException(new MergeRegionException(
+        "Unable to merge regions not online " + regionStateA + ", " + regionStateB));
+    }
+
+    HRegionInfo regionInfoA = regionStateA.getRegion();
+    HRegionInfo regionInfoB = regionStateB.getRegion();
+    if (regionInfoA.compareTo(regionInfoB) == 0) {
+      throw new ServiceException(new MergeRegionException(
+        "Unable to merge a region to itself " + regionInfoA + ", " + regionInfoB));
+    }
+
+    if (!forcible && !HRegionInfo.areAdjacent(regionInfoA, regionInfoB)) {
+      throw new ServiceException(new MergeRegionException(
+        "Unable to merge not adjacent regions "
+          + regionInfoA.getRegionNameAsString() + ", "
+          + regionInfoB.getRegionNameAsString()
+          + " where forcible = " + forcible));
+    }
+
+    try {
+      master.dispatchMergingRegions(regionInfoA, regionInfoB, forcible);
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+
+    return DispatchMergingRegionsResponse.newBuilder().build();
+  }
+
+  @Override
+  public EnableCatalogJanitorResponse enableCatalogJanitor(RpcController c,
+      EnableCatalogJanitorRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return EnableCatalogJanitorResponse.newBuilder().setPrevValue(
+      master.catalogJanitorChore.setEnabled(req.getEnable())).build();
+  }
+
+  @Override
+  public EnableTableResponse enableTable(RpcController controller,
+      EnableTableRequest request) throws ServiceException {
+    try {
+      master.enableTable(ProtobufUtil.toTableName(request.getTableName()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return EnableTableResponse.newBuilder().build();
+  }
+
+  @Override
+  public ClientProtos.CoprocessorServiceResponse execMasterService(final RpcController controller,
+      final ClientProtos.CoprocessorServiceRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      ServerRpcController execController = new ServerRpcController();
+
+      ClientProtos.CoprocessorServiceCall call = request.getCall();
+      String serviceName = call.getServiceName();
+      String methodName = call.getMethodName();
+      if (!master.coprocessorServiceHandlers.containsKey(serviceName)) {
+        throw new UnknownProtocolException(null,
+          "No registered master coprocessor service found for name "+serviceName);
+      }
+
+      Service service = master.coprocessorServiceHandlers.get(serviceName);
+      Descriptors.ServiceDescriptor serviceDesc = service.getDescriptorForType();
+      Descriptors.MethodDescriptor methodDesc = serviceDesc.findMethodByName(methodName);
+      if (methodDesc == null) {
+        throw new UnknownProtocolException(service.getClass(),
+          "Unknown method "+methodName+" called on master service "+serviceName);
+      }
+
+      //invoke the method
+      Message execRequest = service.getRequestPrototype(methodDesc).newBuilderForType()
+          .mergeFrom(call.getRequest()).build();
+      final Message.Builder responseBuilder =
+          service.getResponsePrototype(methodDesc).newBuilderForType();
+      service.callMethod(methodDesc, execController, execRequest, new RpcCallback<Message>() {
+        @Override
+        public void run(Message message) {
+          if (message != null) {
+            responseBuilder.mergeFrom(message);
+          }
+        }
+      });
+      Message execResult = responseBuilder.build();
+
+      if (execController.getFailedOn() != null) {
+        throw execController.getFailedOn();
+      }
+      ClientProtos.CoprocessorServiceResponse.Builder builder =
+        ClientProtos.CoprocessorServiceResponse.newBuilder();
+      builder.setRegion(RequestConverter.buildRegionSpecifier(
+        RegionSpecifierType.REGION_NAME, HConstants.EMPTY_BYTE_ARRAY));
+      builder.setValue(
+        builder.getValueBuilder().setName(execResult.getClass().getName())
+          .setValue(execResult.toByteString()));
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Triggers an asynchronous attempt to run a distributed procedure.
+   * {@inheritDoc}
+   */
+  @Override
+  public ExecProcedureResponse execProcedure(RpcController controller,
+      ExecProcedureRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      ProcedureDescription desc = request.getProcedure();
+      MasterProcedureManager mpm = master.mpmHost.getProcedureManager(
+        desc.getSignature());
+      if (mpm == null) {
+        throw new ServiceException("The procedure is not registered: "
+          + desc.getSignature());
+      }
+  
+      LOG.info(master.getClientIdAuditPrefix() + " procedure request for: "
+        + desc.getSignature());
+
+      mpm.execProcedure(desc);
+
+      // send back the max amount of time the client should wait for the procedure
+      // to complete
+      long waitTime = SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME;
+      return ExecProcedureResponse.newBuilder().setExpectedTimeout(
+        waitTime).build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public GetClusterStatusResponse getClusterStatus(RpcController controller,
+      GetClusterStatusRequest req) throws ServiceException {
+    GetClusterStatusResponse.Builder response = GetClusterStatusResponse.newBuilder();
+    try {
+      master.checkInitialized();
+      response.setClusterStatus(master.getClusterStatus().convert());
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+    return response.build();
+  }
+
+  /**
+   * List the currently available/stored snapshots. Any in-progress snapshots are ignored
+   */
+  @Override
+  public GetCompletedSnapshotsResponse getCompletedSnapshots(RpcController controller,
+      GetCompletedSnapshotsRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      GetCompletedSnapshotsResponse.Builder builder = GetCompletedSnapshotsResponse.newBuilder();
+      List<SnapshotDescription> snapshots = master.snapshotManager.getCompletedSnapshots();
+
+      // convert to protobuf
+      for (SnapshotDescription snapshot : snapshots) {
+        builder.addSnapshots(snapshot);
+      }
+      return builder.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public GetNamespaceDescriptorResponse getNamespaceDescriptor(
+      RpcController controller, GetNamespaceDescriptorRequest request)
+      throws ServiceException {
+    try {
+      return GetNamespaceDescriptorResponse.newBuilder()
+        .setNamespaceDescriptor(ProtobufUtil.toProtoNamespaceDescriptor(
+          master.getNamespaceDescriptor(request.getNamespaceName())))
+        .build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  /**
+   * Get the number of regions of the table that have been updated by the alter.
+   *
+   * @return Pair indicating the number of regions updated Pair.getFirst is the
+   *         regions that are yet to be updated Pair.getSecond is the total number
+   *         of regions of the table
+   * @throws IOException
+   */
+  @Override
+  public GetSchemaAlterStatusResponse getSchemaAlterStatus(
+      RpcController controller, GetSchemaAlterStatusRequest req) throws ServiceException {
+    // TODO: currently, we query using the table name on the client side. this
+    // may overlap with other table operations or the table operation may
+    // have completed before querying this API. We need to refactor to a
+    // transaction system in the future to avoid these ambiguities.
+    TableName tableName = ProtobufUtil.toTableName(req.getTableName());
+
+    try {
+      master.checkInitialized();
+      Pair<Integer,Integer> pair = master.assignmentManager.getReopenStatus(tableName);
+      GetSchemaAlterStatusResponse.Builder ret = GetSchemaAlterStatusResponse.newBuilder();
+      ret.setYetToUpdateRegions(pair.getFirst());
+      ret.setTotalRegions(pair.getSecond());
+      return ret.build();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+
+  /**
+   * Get list of TableDescriptors for requested tables.
+   * @param controller Unused (set to null).
+   * @param req GetTableDescriptorsRequest that contains:
+   * - tableNames: requested tables, or if empty, all are requested
+   * @return GetTableDescriptorsResponse
+   * @throws ServiceException
+   */
+  @Override
+  public GetTableDescriptorsResponse getTableDescriptors(RpcController c,
+      GetTableDescriptorsRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+
+    List<HTableDescriptor> descriptors = new ArrayList<HTableDescriptor>();
+    List<TableName> tableNameList = new ArrayList<TableName>();
+    for(HBaseProtos.TableName tableNamePB: req.getTableNamesList()) {
+      tableNameList.add(ProtobufUtil.toTableName(tableNamePB));
+    }
+    boolean bypass = false;
+    if (master.cpHost != null) {
+      try {
+        bypass = master.cpHost.preGetTableDescriptors(tableNameList, descriptors);
+      } catch (IOException ioe) {
+        throw new ServiceException(ioe);
+      }
+    }
+
+    if (!bypass) {
+      if (req.getTableNamesCount() == 0) {
+        // request for all TableDescriptors
+        Map<String, HTableDescriptor> descriptorMap = null;
+        try {
+          descriptorMap = master.getTableDescriptors().getAll();
+        } catch (IOException e) {
+          LOG.warn("Failed getting all descriptors", e);
+        }
+        if (descriptorMap != null) {
+          for(HTableDescriptor desc: descriptorMap.values()) {
+            if(!desc.getTableName().isSystemTable()) {
+              descriptors.add(desc);
+            }
+          }
+        }
+      } else {
+        for (TableName s: tableNameList) {
+          try {
+            HTableDescriptor desc = master.getTableDescriptors().get(s);
+            if (desc != null) {
+              descriptors.add(desc);
+            }
+          } catch (IOException e) {
+            LOG.warn("Failed getting descriptor for " + s, e);
+          }
+        }
+      }
+
+      if (master.cpHost != null) {
+        try {
+          master.cpHost.postGetTableDescriptors(descriptors);
+        } catch (IOException ioe) {
+          throw new ServiceException(ioe);
+        }
+      }
+    }
+
+    GetTableDescriptorsResponse.Builder builder = GetTableDescriptorsResponse.newBuilder();
+    for (HTableDescriptor htd: descriptors) {
+      builder.addTableSchema(htd.convert());
+    }
+    return builder.build();
+  }
+
+  /**
+   * Get list of userspace table names
+   * @param controller Unused (set to null).
+   * @param req GetTableNamesRequest
+   * @return GetTableNamesResponse
+   * @throws ServiceException
+   */
+  @Override
+  public GetTableNamesResponse getTableNames(RpcController controller,
+      GetTableNamesRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+      Collection<HTableDescriptor> descriptors = master.getTableDescriptors().getAll().values();
+      GetTableNamesResponse.Builder builder = GetTableNamesResponse.newBuilder();
+      for (HTableDescriptor descriptor: descriptors) {
+        if (descriptor.getTableName().isSystemTable()) {
+          continue;
+        }
+        builder.addTableNames(ProtobufUtil.toProtoTableName(descriptor.getTableName()));
+      }
+      return builder.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public IsCatalogJanitorEnabledResponse isCatalogJanitorEnabled(RpcController c,
+      IsCatalogJanitorEnabledRequest req) throws ServiceException {
+    boolean isEnabled = master.catalogJanitorChore != null ?
+      master.catalogJanitorChore.getEnabled() : false;
+    return IsCatalogJanitorEnabledResponse.newBuilder().setValue(isEnabled).build();
+  }
+
+  @Override
+  public IsMasterRunningResponse isMasterRunning(RpcController c,
+      IsMasterRunningRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+      return IsMasterRunningResponse.newBuilder().setIsMasterRunning(
+        !master.isStopped()).build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  /**
+   * Checks if the specified procedure is done.
+   * @return true if the procedure is done,
+   *   false if the procedure is in the process of completing
+   * @throws ServiceException if invalid procedure, or
+   *  a failed procedure with progress failure reason.
+   */
+  @Override
+  public IsProcedureDoneResponse isProcedureDone(RpcController controller,
+      IsProcedureDoneRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      ProcedureDescription desc = request.getProcedure();
+      MasterProcedureManager mpm = master.mpmHost.getProcedureManager(
+        desc.getSignature());
+      if (mpm == null) {
+        throw new ServiceException("The procedure is not registered: "
+          + desc.getSignature());
+      }
+      LOG.debug("Checking to see if procedure from request:"
+        + desc.getSignature() + " is done");
+
+      IsProcedureDoneResponse.Builder builder =
+        IsProcedureDoneResponse.newBuilder();
+      boolean done = mpm.isProcedureDone(desc);
+      builder.setDone(done);
+      return builder.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  /**
+   * Returns the status of the requested snapshot restore/clone operation.
+   * This method is not exposed to the user, it is just used internally by HBaseAdmin
+   * to verify if the restore is completed.
+   *
+   * No exceptions are thrown if the restore is not running, the result will be "done".
+   *
+   * @return done <tt>true</tt> if the restore/clone operation is completed.
+   * @throws ServiceException if the operation failed.
+   */
+  @Override
+  public IsRestoreSnapshotDoneResponse isRestoreSnapshotDone(RpcController controller,
+      IsRestoreSnapshotDoneRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      SnapshotDescription snapshot = request.getSnapshot();
+      IsRestoreSnapshotDoneResponse.Builder builder = IsRestoreSnapshotDoneResponse.newBuilder();
+      boolean done = master.snapshotManager.isRestoreDone(snapshot);
+      builder.setDone(done);
+      return builder.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  /**
+   * Checks if the specified snapshot is done.
+   * @return true if the snapshot is in file system ready to use,
+   *   false if the snapshot is in the process of completing
+   * @throws ServiceException wrapping UnknownSnapshotException if invalid snapshot, or
+   *  a wrapped HBaseSnapshotException with progress failure reason.
+   */
+  @Override
+  public IsSnapshotDoneResponse isSnapshotDone(RpcController controller,
+      IsSnapshotDoneRequest request) throws ServiceException {
+    LOG.debug("Checking to see if snapshot from request:" +
+      ClientSnapshotDescriptionUtils.toString(request.getSnapshot()) + " is done");
+    try {
+      master.checkInitialized();
+      IsSnapshotDoneResponse.Builder builder = IsSnapshotDoneResponse.newBuilder();
+      boolean done = master.snapshotManager.isSnapshotDone(request.getSnapshot());
+      builder.setDone(done);
+      return builder.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public ListNamespaceDescriptorsResponse listNamespaceDescriptors(RpcController c,
+      ListNamespaceDescriptorsRequest request) throws ServiceException {
+    try {
+      ListNamespaceDescriptorsResponse.Builder response =
+        ListNamespaceDescriptorsResponse.newBuilder();
+      for(NamespaceDescriptor ns: master.listNamespaceDescriptors()) {
+        response.addNamespaceDescriptor(ProtobufUtil.toProtoNamespaceDescriptor(ns));
+      }
+      return response.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public ListTableDescriptorsByNamespaceResponse listTableDescriptorsByNamespace(RpcController c,
+      ListTableDescriptorsByNamespaceRequest request) throws ServiceException {
+    try {
+      ListTableDescriptorsByNamespaceResponse.Builder b =
+        ListTableDescriptorsByNamespaceResponse.newBuilder();
+      for(HTableDescriptor htd: master.listTableDescriptorsByNamespace(request.getNamespaceName())) {
+        b.addTableSchema(htd.convert());
+      }
+      return b.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public ListTableNamesByNamespaceResponse listTableNamesByNamespace(RpcController c,
+      ListTableNamesByNamespaceRequest request) throws ServiceException {
+    try {
+      ListTableNamesByNamespaceResponse.Builder b =
+        ListTableNamesByNamespaceResponse.newBuilder();
+      for (TableName tableName: master.listTableNamesByNamespace(request.getNamespaceName())) {
+        b.addTableName(ProtobufUtil.toProtoTableName(tableName));
+      }
+      return b.build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public ModifyColumnResponse modifyColumn(RpcController controller,
+      ModifyColumnRequest req) throws ServiceException {
+    try {
+      master.modifyColumn(ProtobufUtil.toTableName(req.getTableName()),
+        HColumnDescriptor.convert(req.getColumnFamilies()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return ModifyColumnResponse.newBuilder().build();
+  }
+
+  @Override
+  public ModifyNamespaceResponse modifyNamespace(RpcController controller,
+      ModifyNamespaceRequest request) throws ServiceException {
+    try {
+      master.modifyNamespace(
+        ProtobufUtil.toNamespaceDescriptor(request.getNamespaceDescriptor()));
+      return ModifyNamespaceResponse.getDefaultInstance();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public ModifyTableResponse modifyTable(RpcController controller,
+      ModifyTableRequest req) throws ServiceException {
+    try {
+      master.modifyTable(ProtobufUtil.toTableName(req.getTableName()),
+        HTableDescriptor.convert(req.getTableSchema()));
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return ModifyTableResponse.newBuilder().build();
+  }
+
+  @Override
+  public MoveRegionResponse moveRegion(RpcController controller,
+      MoveRegionRequest req) throws ServiceException {
+    final byte [] encodedRegionName = req.getRegion().getValue().toByteArray();
+    RegionSpecifierType type = req.getRegion().getType();
+    final byte [] destServerName = (req.hasDestServerName())?
+      Bytes.toBytes(ProtobufUtil.toServerName(req.getDestServerName()).getServerName()):null;
+    MoveRegionResponse mrr = MoveRegionResponse.newBuilder().build();
+
+    if (type != RegionSpecifierType.ENCODED_REGION_NAME) {
+      LOG.warn("moveRegion specifier type: expected: " + RegionSpecifierType.ENCODED_REGION_NAME
+        + " actual: " + type);
+    }
+
+    try {
+      master.checkInitialized();
+      master.move(encodedRegionName, destServerName);
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return mrr;
+  }
+
+  /**
+   * Offline specified region from master's in-memory state. It will not attempt to
+   * reassign the region as in unassign.
+   *
+   * This is a special method that should be used by experts or hbck.
+   *
+   */
+  @Override
+  public OfflineRegionResponse offlineRegion(RpcController controller,
+      OfflineRegionRequest request) throws ServiceException {
+    final byte [] regionName = request.getRegion().getValue().toByteArray();
+    RegionSpecifierType type = request.getRegion().getType();
+    if (type != RegionSpecifierType.REGION_NAME) {
+      LOG.warn("moveRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
+        + " actual: " + type);
+    }
+
+    try {
+      master.checkInitialized();
+      Pair<HRegionInfo, ServerName> pair =
+        MetaReader.getRegion(master.getCatalogTracker(), regionName);
+      if (pair == null) throw new UnknownRegionException(Bytes.toStringBinary(regionName));
+      HRegionInfo hri = pair.getFirst();
+      if (master.cpHost != null) {
+        master.cpHost.preRegionOffline(hri);
+      }
+      LOG.info(master.getClientIdAuditPrefix() + " offline " + hri.getRegionNameAsString());
+      master.assignmentManager.regionOffline(hri);
+      if (master.cpHost != null) {
+        master.cpHost.postRegionOffline(hri);
+      }
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+    return OfflineRegionResponse.newBuilder().build();
+  }
+
+  /**
+   * Execute Restore/Clone snapshot operation.
+   *
+   * <p>If the specified table exists a "Restore" is executed, replacing the table
+   * schema and directory data with the content of the snapshot.
+   * The table must be disabled, or a UnsupportedOperationException will be thrown.
+   *
+   * <p>If the table doesn't exist a "Clone" is executed, a new table is created
+   * using the schema at the time of the snapshot, and the content of the snapshot.
+   *
+   * <p>The restore/clone operation does not require copying HFiles. Since HFiles
+   * are immutable the table can point to and use the same files as the original one.
+   */
+  @Override
+  public RestoreSnapshotResponse restoreSnapshot(RpcController controller,
+      RestoreSnapshotRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      master.snapshotManager.checkSnapshotSupport();
+
+    // ensure namespace exists
+      TableName dstTable = TableName.valueOf(request.getSnapshot().getTable());
+      master.getNamespaceDescriptor(dstTable.getNamespaceAsString());
+
+      SnapshotDescription reqSnapshot = request.getSnapshot();
+      master.snapshotManager.restoreSnapshot(reqSnapshot);
+      return RestoreSnapshotResponse.newBuilder().build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public RunCatalogScanResponse runCatalogScan(RpcController c,
+      RunCatalogScanRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+      return ResponseConverter.buildRunCatalogScanResponse(master.catalogJanitorChore.scan());
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+
+  @Override
+  public SetBalancerRunningResponse setBalancerRunning(RpcController c,
+      SetBalancerRunningRequest req) throws ServiceException {
+    try {
+      master.checkInitialized();
+      boolean prevValue = (req.getSynchronous())?
+        synchronousBalanceSwitch(req.getOn()) : master.balanceSwitch(req.getOn());
+      return SetBalancerRunningResponse.newBuilder().setPrevBalanceValue(prevValue).build();
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+
+  @Override
+  public ShutdownResponse shutdown(RpcController controller,
+      ShutdownRequest request) throws ServiceException {
+    LOG.info(master.getClientIdAuditPrefix() + " shutdown");
+    master.shutdown();
+    return ShutdownResponse.newBuilder().build();
+  }
+
+  /**
+   * Triggers an asynchronous attempt to take a snapshot.
+   * {@inheritDoc}
+   */
+  @Override
+  public SnapshotResponse snapshot(RpcController controller,
+      SnapshotRequest request) throws ServiceException {
+    try {
+      master.checkInitialized();
+      master.snapshotManager.checkSnapshotSupport();
+
+      LOG.info(master.getClientIdAuditPrefix() + " snapshot request for:" +
+        ClientSnapshotDescriptionUtils.toString(request.getSnapshot()));
+      // get the snapshot information
+      SnapshotDescription snapshot = SnapshotDescriptionUtils.validate(
+        request.getSnapshot(), master.getConfiguration());
+      master.snapshotManager.takeSnapshot(snapshot);
+
+      // send back the max amount of time the client should wait for the snapshot to complete
+      long waitTime = SnapshotDescriptionUtils.getMaxMasterTimeout(master.getConfiguration(),
+        snapshot.getType(), SnapshotDescriptionUtils.DEFAULT_MAX_WAIT_TIME);
+      return SnapshotResponse.newBuilder().setExpectedTimeout(waitTime).build();
+    } catch (IOException e) {
+      throw new ServiceException(e);
+    }
+  }
+
+  @Override
+  public StopMasterResponse stopMaster(RpcController controller,
+      StopMasterRequest request) throws ServiceException {
+    LOG.info(master.getClientIdAuditPrefix() + " stop");
+    master.stopMaster();
+    return StopMasterResponse.newBuilder().build();
+  }
+
+  @Override
+  public UnassignRegionResponse unassignRegion(RpcController controller,
+      UnassignRegionRequest req) throws ServiceException {
+    try {
+      final byte [] regionName = req.getRegion().getValue().toByteArray();
+      RegionSpecifierType type = req.getRegion().getType();
+      final boolean force = req.getForce();
+      UnassignRegionResponse urr = UnassignRegionResponse.newBuilder().build();
+
+      master.checkInitialized();
+      if (type != RegionSpecifierType.REGION_NAME) {
+        LOG.warn("unassignRegion specifier type: expected: " + RegionSpecifierType.REGION_NAME
+          + " actual: " + type);
+      }
+      Pair<HRegionInfo, ServerName> pair =
+        MetaReader.getRegion(master.getCatalogTracker(), regionName);
+      if (pair == null) throw new UnknownRegionException(Bytes.toString(regionName));
+      HRegionInfo hri = pair.getFirst();
+      if (master.cpHost != null) {
+        if (master.cpHost.preUnassign(hri, force)) {
+          return urr;
+        }
+      }
+      LOG.debug(master.getClientIdAuditPrefix() + " unassign " + hri.getRegionNameAsString()
+          + " in current location if it is online and reassign.force=" + force);
+      master.assignmentManager.unassign(hri, force);
+      if (master.assignmentManager.getRegionStates().isRegionOffline(hri)) {
+        LOG.debug("Region " + hri.getRegionNameAsString()
+            + " is not online on any region server, reassigning it.");
+        master.assignRegion(hri);
+      }
+      if (master.cpHost != null) {
+        master.cpHost.postUnassign(hri, force);
+      }
+
+      return urr;
+    } catch (IOException ioe) {
+      throw new ServiceException(ioe);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
index b2ed89a..5e7cc75 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
@@ -22,13 +22,13 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.executor.ExecutorService;
@@ -68,7 +68,7 @@ public interface MasterServices extends Server {
   /**
    * @return Master's instance of {@link MasterCoprocessorHost}
    */
-  MasterCoprocessorHost getCoprocessorHost();
+  MasterCoprocessorHost getMasterCoprocessorHost();
 
   /**
    * Check table is modifiable; i.e. exists and is offline.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
index 49ab9d6..dde752b 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hbase.master;
 
 import java.io.IOException;
-
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -34,9 +33,10 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl;
+import org.apache.hadoop.hbase.util.FSUtils;
+
 import com.google.protobuf.ServiceException;
 
 /**
@@ -55,6 +55,14 @@ public class MasterStatusServlet extends HttpServlet {
     HMaster master = (HMaster) getServletContext().getAttribute(HMaster.MASTER);
     assert master != null : "No Master in context!";
 
+    response.setContentType("text/html");
+
+    if (!master.isOnline()) {
+      response.getWriter().write("The Master is initializing!");
+      response.getWriter().close();
+      return;
+    }
+
     Configuration conf = master.getConfiguration();
     HBaseAdmin admin = new HBaseAdmin(conf);
 
@@ -73,8 +81,7 @@ public class MasterStatusServlet extends HttpServlet {
       servers = master.getServerManager().getOnlineServersList();
       deadServers = master.getServerManager().getDeadServers().copyServerNames();
     }
-    
-    response.setContentType("text/html");
+
     MasterStatusTmpl tmpl;
     try {
        tmpl = new MasterStatusTmpl()
@@ -82,8 +89,8 @@ public class MasterStatusServlet extends HttpServlet {
       .setMetaLocation(metaLocation)
       .setServers(servers)
       .setDeadServers(deadServers)
-      .setCatalogJanitorEnabled(master.isCatalogJanitorEnabled(null,
-          RequestConverter.buildIsCatalogJanitorEnabledRequest()).getValue());
+      .setCatalogJanitorEnabled(master.getMasterRpcServices().isCatalogJanitorEnabled(
+          null, RequestConverter.buildIsCatalogJanitorEnabledRequest()).getValue());
     } catch (ServiceException s) {
       admin.close();
       throw new IOException(s);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java
index e19940b..f2d3009 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.java
@@ -46,7 +46,7 @@ public class MetricsMasterWrapperImpl implements MetricsMasterWrapper {
 
   @Override
   public String getZookeeperQuorum() {
-    ZooKeeperWatcher zk = master.getZooKeeperWatcher();
+    ZooKeeperWatcher zk = master.getZooKeeper();
     if (zk == null) {
       return "";
     }
@@ -55,7 +55,7 @@ public class MetricsMasterWrapperImpl implements MetricsMasterWrapper {
 
   @Override
   public String[] getCoprocessors() {
-    return master.getCoprocessors();
+    return master.getMasterCoprocessors();
   }
 
   @Override
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
index eaa57fc..5f96a22 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
@@ -671,6 +671,16 @@ public class RegionStates {
         numServers++;
       }
     }
+    if (numServers > 1) {
+      // The master region server holds only a couple regions.
+      // Don't consider this server in calculating the average load
+      // if there are other region servers to avoid possible confusion.
+      Set<HRegionInfo> hris = serverHoldings.get(server.getServerName());
+      if (hris != null) {
+        totalLoad -= hris.size();
+        numServers--;
+      }
+    }
     return numServers == 0 ? 0.0 :
       (double)totalLoad / (double)numServers;
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
index 09cfee1..193435e 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
@@ -58,9 +58,13 @@ import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.AdminService;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ServerInfo;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.RegionOpeningState;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Triple;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.protobuf.ServiceException;
@@ -182,7 +186,6 @@ public class ServerManager {
     this(master, services, true);
   }
 
-  @SuppressWarnings("deprecation")
   ServerManager(final Server master, final MasterServices services,
       final boolean connect) throws IOException {
     this.master = master;
@@ -441,12 +444,21 @@ public class ServerManager {
 
   void letRegionServersShutdown() {
     long previousLogTime = 0;
+    ServerName sn = master.getServerName();
+    ZooKeeperWatcher zkw = master.getZooKeeper();
     while (!onlineServers.isEmpty()) {
 
       if (System.currentTimeMillis() > (previousLogTime + 1000)) {
+        Set<ServerName> remainingServers = onlineServers.keySet();
+        synchronized (onlineServers) {
+          if (remainingServers.size() == 1 && remainingServers.contains(sn)) {
+            // Master will delete itself later.
+            return;
+          }
+        }
         StringBuilder sb = new StringBuilder();
         // It's ok here to not sync on onlineServers - merely logging
-        for (ServerName key : this.onlineServers.keySet()) {
+        for (ServerName key : remainingServers) {
           if (sb.length() > 0) {
             sb.append(", ");
           }
@@ -456,6 +468,17 @@ public class ServerManager {
         previousLogTime = System.currentTimeMillis();
       }
 
+      try {
+        List<String> servers = ZKUtil.listChildrenNoWatch(zkw, zkw.rsZNode);
+        if (servers == null || (servers.size() == 1
+            && servers.contains(sn.toString()))) {
+          LOG.info("ZK shows there is only the master self online, exiting now");
+          // Master could have lost some ZK events, no need to wait more.
+          break;
+        }
+      } catch (KeeperException ke) {
+        LOG.warn("Failed to list regionservers", ke);
+      }
       synchronized (onlineServers) {
         try {
           onlineServers.wait(100);
@@ -471,6 +494,12 @@ public class ServerManager {
    * shutdown processing.
    */
   public synchronized void expireServer(final ServerName serverName) {
+    if (serverName.equals(master.getServerName())) {
+      if (!(master.isAborted() || master.isStopped())) {
+        master.stop("We lost our znode?");
+      }
+      return;
+    }
     if (!services.isServerShutdownHandlerEnabled()) {
       LOG.info("Master doesn't enable ServerShutdownHandler during initialization, "
           + "delay expiring server " + serverName);
@@ -758,12 +787,18 @@ public class ServerManager {
     * @throws IOException
     * @throws RetriesExhaustedException wrapping a ConnectException if failed
     */
+  @SuppressWarnings("deprecation")
   private AdminService.BlockingInterface getRsAdmin(final ServerName sn)
   throws IOException {
     AdminService.BlockingInterface admin = this.rsAdmins.get(sn);
     if (admin == null) {
       LOG.debug("New admin connection to " + sn.toString());
-      admin = this.connection.getAdmin(sn);
+      if (sn.equals(master.getServerName()) && master instanceof HRegionServer) {
+        // A master is also a region server now, see HBASE-10569 for details
+        admin = ((HRegionServer)master).getRSRpcServices();
+      } else {
+        admin = this.connection.getAdmin(sn);
+      }
       this.rsAdmins.put(sn, admin);
     }
     return admin;
@@ -813,12 +848,10 @@ public class ServerManager {
     long lastCountChange = startTime;
     int count = countOfRegionServers();
     int oldCount = 0;
-    while (
-      !this.master.isStopped() &&
-        count < maxToStart &&
-        (lastCountChange+interval > now || timeout > slept || count < minToStart)
-      ){
-
+    ServerName masterSn = master.getServerName();
+    boolean selfCheckedIn = isServerOnline(masterSn);
+    while (!this.master.isStopped() && !selfCheckedIn && count < maxToStart
+        && (lastCountChange+interval > now || timeout > slept || count < minToStart)) {
       // Log some info at every interval time or if there is a change
       if (oldCount != count || lastLogTime+interval < now){
         lastLogTime = now;
@@ -837,6 +870,8 @@ public class ServerManager {
       now =  System.currentTimeMillis();
       slept = now - startTime;
 
+      selfCheckedIn = isServerOnline(masterSn);
+
       oldCount = count;
       count = countOfRegionServers();
       if (count != oldCount) {
@@ -942,7 +977,6 @@ public class ServerManager {
 
     // Remove the deadNotExpired servers from the server list.
     removeDeadNotExpiredServers(destServers);
-
     return destServers;
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
index 67b97a7..d345235 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java
@@ -46,7 +46,6 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hbase.Chore;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.SplitLogCounters;
@@ -151,25 +150,7 @@ public class SplitLogManager extends ZooKeeperListener {
 
   /**
    * Wrapper around {@link #SplitLogManager(ZooKeeperWatcher zkw, Configuration conf,
-   *   Stoppable stopper, MasterServices master, ServerName serverName,
-   *   boolean masterRecovery, TaskFinisher tf)}
-   * with masterRecovery = false, and tf = null.  Used in unit tests.
-   *
-   * @param zkw the ZK watcher
-   * @param conf the HBase configuration
-   * @param stopper the stoppable in case anything is wrong
-   * @param master the master services
-   * @param serverName the master server name
-   */
-  public SplitLogManager(ZooKeeperWatcher zkw, final Configuration conf,
-      Stoppable stopper, MasterServices master, ServerName serverName) {
-    this(zkw, conf, stopper, master, serverName, false, null);
-  }
-
-  /**
-   * Wrapper around {@link #SplitLogManager(ZooKeeperWatcher zkw, Configuration conf,
-   *   Stoppable stopper, MasterServices master, ServerName serverName,
-   *   boolean masterRecovery, TaskFinisher tf)}
+   *   Stoppable stopper, MasterServices master, ServerName serverName, TaskFinisher tf)}
    * that provides a task finisher for copying recovered edits to their final destination.
    * The task finisher has to be robust because it can be arbitrarily restarted or called
    * multiple times.
@@ -179,11 +160,10 @@ public class SplitLogManager extends ZooKeeperListener {
    * @param stopper the stoppable in case anything is wrong
    * @param master the master services
    * @param serverName the master server name
-   * @param masterRecovery an indication if the master is in recovery
    */
   public SplitLogManager(ZooKeeperWatcher zkw, final Configuration conf,
-      Stoppable stopper, MasterServices master, ServerName serverName, boolean masterRecovery) {
-    this(zkw, conf, stopper, master, serverName, masterRecovery, new TaskFinisher() {
+      Stoppable stopper, MasterServices master, ServerName serverName) {
+    this(zkw, conf, stopper, master, serverName, new TaskFinisher() {
       @Override
       public Status finish(ServerName workerName, String logfile) {
         try {
@@ -207,12 +187,11 @@ public class SplitLogManager extends ZooKeeperListener {
    * @param stopper the stoppable in case anything is wrong
    * @param master the master services
    * @param serverName the master server name
-   * @param masterRecovery an indication if the master is in recovery
    * @param tf task finisher
    */
   public SplitLogManager(ZooKeeperWatcher zkw, Configuration conf,
         Stoppable stopper, MasterServices master,
-        ServerName serverName, boolean masterRecovery, TaskFinisher tf) {
+        ServerName serverName, TaskFinisher tf) {
     super(zkw);
     this.taskFinisher = tf;
     this.conf = conf;
@@ -233,10 +212,8 @@ public class SplitLogManager extends ZooKeeperListener {
 
     this.failedDeletions = Collections.synchronizedSet(new HashSet<String>());
 
-    if (!masterRecovery) {
-      Threads.setDaemonThreadRunning(timeoutMonitor.getThread(), serverName
-          + ".splitLogManagerTimeoutMonitor");
-    }
+    Threads.setDaemonThreadRunning(timeoutMonitor.getThread(), serverName
+      + ".splitLogManagerTimeoutMonitor");
     // Watcher can be null during tests with Mock'd servers.
     if (this.watcher != null) {
       this.watcher.registerListener(this);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
index 3081811..b39f50f 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
@@ -22,6 +22,8 @@ import java.util.Arrays;
 import java.util.Comparator;
 import java.util.Deque;
 import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
@@ -38,9 +40,12 @@ import org.apache.hadoop.hbase.HBaseIOException;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.RegionLoad;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.master.AssignmentManager;
 import org.apache.hadoop.hbase.master.LoadBalancer;
 import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.security.access.AccessControlLists;
 
 import com.google.common.base.Joiner;
 import com.google.common.collect.ArrayListMultimap;
@@ -56,12 +61,32 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
   private static final int MIN_SERVER_BALANCE = 2;
   private volatile boolean stopped = false;
 
+  protected static final Set<String> TABLES_ON_MASTER = new HashSet<String>();
+
+  /**
+   * Regions of these tables will be put on the master regionserver by default.
+   */
+  static {
+    TABLES_ON_MASTER.add(AccessControlLists.ACL_TABLE_NAME.getNameAsString());
+    TABLES_ON_MASTER.add(TableName.NAMESPACE_TABLE_NAME.getNameAsString());
+    TABLES_ON_MASTER.add(TableName.META_TABLE_NAME.getNameAsString());
+  }
+
+  /**
+   * Check if a region belongs to some small system table.
+   * If so, it may be expected to be put on the master regionserver.
+   */
+  protected static boolean shouldBeOnMaster(HRegionInfo region) {
+    return TABLES_ON_MASTER.contains(region.getTable().getNameAsString());
+  }
+
   /**
    * An efficient array based implementation similar to ClusterState for keeping
    * the status of the cluster in terms of region assignment and distribution.
    * To be used by LoadBalancers.
    */
   protected static class Cluster {
+    ServerName masterServerName;
     ServerName[] servers;
     ArrayList<String> tables;
     HRegionInfo[] regions;
@@ -74,6 +99,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     int[]   regionIndexToTableIndex;     //regionIndex -> tableIndex
     int[][] numRegionsPerServerPerTable; //serverIndex -> tableIndex -> # regions
     int[]   numMaxRegionsPerTable;       //tableIndex -> max number of regions in a single RS
+    int     numUserRegionsOnMaster;      //number of user regions on the active master
 
     Integer[] serverIndicesSortedByRegionCount;
 
@@ -87,9 +113,13 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     int numMovedRegions = 0; //num moved regions from the initial configuration
     int numMovedMetaRegions = 0;       //num of moved regions that are META
 
-    protected Cluster(Map<ServerName, List<HRegionInfo>> clusterState,  Map<String, Deque<RegionLoad>> loads,
+    @SuppressWarnings("unchecked")
+    protected Cluster(ServerName masterServerName,
+        Map<ServerName, List<HRegionInfo>> clusterState,
+        Map<String, Deque<RegionLoad>> loads,
         RegionLocationFinder regionFinder) {
 
+      this.masterServerName = masterServerName;
       serversToIndex = new HashMap<String, Integer>();
       tablesToIndex = new HashMap<String, Integer>();
       //regionsToIndex = new HashMap<HRegionInfo, Integer>();
@@ -147,6 +177,14 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
           regionsPerServer[serverIndex] = new int[entry.getValue().size()];
         }
         serverIndicesSortedByRegionCount[serverIndex] = serverIndex;
+
+        if (servers[serverIndex].equals(masterServerName)) {
+          for (HRegionInfo hri: entry.getValue()) {
+            if (!shouldBeOnMaster(hri)) {
+              numUserRegionsOnMaster++;
+            }
+          }
+        }
       }
 
       for (Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {
@@ -218,6 +256,21 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     }
 
     public void moveOrSwapRegion(int lServer, int rServer, int lRegion, int rRegion) {
+      if (servers[lServer].equals(masterServerName)) {
+        if (lRegion >= 0 && !shouldBeOnMaster(regions[lRegion])) {
+          numUserRegionsOnMaster--;
+        }
+        if (rRegion >= 0 && !shouldBeOnMaster(regions[rRegion])) {
+          numUserRegionsOnMaster++;
+        }
+      } else if (servers[rServer].equals(masterServerName)) {
+        if (lRegion >= 0 && !shouldBeOnMaster(regions[lRegion])) {
+          numUserRegionsOnMaster++;
+        }
+        if (rRegion >= 0 && !shouldBeOnMaster(regions[rRegion])) {
+          numUserRegionsOnMaster--;
+        }
+      }
       //swap
       if (rRegion >= 0 && lRegion >= 0) {
         regionMoved(rRegion, rServer, lServer);
@@ -354,6 +407,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
   private static final Log LOG = LogFactory.getLog(BaseLoadBalancer.class);
 
   protected final MetricsBalancer metricsBalancer = new MetricsBalancer();
+  protected ServerName masterServerName;
   protected MasterServices services;
 
   @Override
@@ -369,6 +423,52 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     this.slop = conf.getFloat("hbase.regions.slop", (float) 0.2);
   }
 
+  /**
+   * Balance the regions that should be on master regionserver.
+   */
+  protected List<RegionPlan> balanceMasterRegions(
+      Map<ServerName, List<HRegionInfo>> clusterMap) {
+    if (services == null || clusterMap.size() <= 1) return null;
+    List<RegionPlan> plans = null;
+    List<HRegionInfo> regions = clusterMap.get(masterServerName);
+    if (regions != null) {
+      Iterator<ServerName> keyIt = null;
+      for (HRegionInfo region: regions) {
+        if (shouldBeOnMaster(region)) continue;
+
+        // Find a non-master regionserver to host the region
+        if (keyIt == null || !keyIt.hasNext()) {
+          keyIt = clusterMap.keySet().iterator();
+        }
+        ServerName dest = keyIt.next();
+        if (masterServerName.equals(dest)) {
+          dest = keyIt.next();
+        }
+
+        // Move this region away from the master regionserver
+        RegionPlan plan = new RegionPlan(region, masterServerName, dest);
+        if (plans == null) {
+          plans = new ArrayList<RegionPlan>();
+        }
+        plans.add(plan);
+      }
+    }
+    for (Map.Entry<ServerName, List<HRegionInfo>> server: clusterMap.entrySet()) {
+      if (masterServerName.equals(server.getKey())) continue;
+      for (HRegionInfo region: server.getValue()) {
+        if (!shouldBeOnMaster(region)) continue;
+
+        // Move this region to the master regionserver
+        RegionPlan plan = new RegionPlan(region, server.getKey(), masterServerName);
+        if (plans == null) {
+          plans = new ArrayList<RegionPlan>();
+        }
+        plans.add(plan);
+      }
+    }
+    return plans;
+  }
+
   @Override
   public Configuration getConf() {
     return this.config;
@@ -381,6 +481,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
 
   @Override
   public void setMasterServices(MasterServices masterServices) {
+    masterServerName = masterServices.getServerName();
     this.services = masterServices;
   }
 
@@ -438,19 +539,43 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       return null;
     }
     Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
-    int numRegions = regions.size();
     int numServers = servers.size();
-    int max = (int) Math.ceil((float) numRegions / numServers);
-    int serverIdx = 0;
-    if (numServers > 1) {
-      serverIdx = RANDOM.nextInt(numServers);
+    if (numServers == 1) { // Only one server, nothing fancy we can do here
+      assignments.put(servers.get(0), new ArrayList<HRegionInfo>(regions));
+      return assignments;
+    }
+
+    int numRegions = regions.size();
+    // Master regionserver is in the server list.
+    boolean masterIncluded = servers.contains(masterServerName);
+    int skipServers = numServers;
+    if (masterIncluded) {
+      skipServers--;
     }
+    int max = (int) Math.ceil((float) numRegions / skipServers);
+    int serverIdx = RANDOM.nextInt(numServers);
     int regionIdx = 0;
     for (int j = 0; j < numServers; j++) {
       ServerName server = servers.get((j + serverIdx) % numServers);
+      if (server.equals(masterServerName)) {
+        // Don't put non-special region on the master regionserver,
+        // So that it is not overloaded.
+        continue;
+      }
       List<HRegionInfo> serverRegions = new ArrayList<HRegionInfo>(max);
-      for (int i = regionIdx; i < numRegions; i += numServers) {
-        serverRegions.add(regions.get(i % numRegions));
+      for (int i = regionIdx; i < numRegions; i += skipServers) {
+        HRegionInfo region = regions.get(i % numRegions);
+        if (!(masterIncluded && shouldBeOnMaster(region))) {
+          serverRegions.add(region);
+          continue;
+        }
+        // Master is in the target list and this is a special region
+        List<HRegionInfo> masterRegions = assignments.get(masterServerName);
+        if (masterRegions == null) {
+          masterRegions = new ArrayList<HRegionInfo>(max);
+          assignments.put(masterServerName, masterRegions);
+        }
+        masterRegions.add(region);
       }
       assignments.put(server, serverRegions);
       regionIdx++;
@@ -498,7 +623,18 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       LOG.warn("Wanted to do random assignment but no servers to assign to");
       return null;
     }
-    return servers.get(RANDOM.nextInt(servers.size()));
+    int numServers = servers.size();
+    if (numServers == 1) return servers.get(0);
+    if (shouldBeOnMaster(regionInfo) && servers.contains(masterServerName)) {
+      return masterServerName;
+    }
+    int i = RANDOM.nextInt(numServers);
+    ServerName sn = servers.get(i);
+    if (sn.equals(masterServerName)) {
+      i = (i == 0 ? 1 : i - 1);
+      sn = servers.get(i);
+    }
+    return sn;
   }
 
   /**
@@ -524,6 +660,16 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     // Update metrics
     metricsBalancer.incrMiscInvocations();
 
+    if (regions.isEmpty() || servers.isEmpty()) {
+      return null;
+    }
+    Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+    int numServers = servers.size();
+    if (numServers == 1) { // Only one server, nothing fancy we can do here
+      assignments.put(servers.get(0), new ArrayList<HRegionInfo>(regions.keySet()));
+      return assignments;
+    }
+
     // Group all of the old assignments by their hostname.
     // We can't group directly by ServerName since the servers all have
     // new start-codes.
@@ -532,12 +678,11 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     // servers on the same host on different ports.
     ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();
     for (ServerName server : servers) {
-      serversByHostname.put(server.getHostname(), server);
+      if (!server.equals(masterServerName)) {
+        serversByHostname.put(server.getHostname(), server);
+      }
     }
 
-    // Now come up with new assignments
-    Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
-
     for (ServerName server : servers) {
       assignments.put(server, new ArrayList<HRegionInfo>());
     }
@@ -547,6 +692,9 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     // after the cluster restart.
     Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();
 
+    // Master regionserver is in the server list.
+    boolean masterIncluded = servers.contains(masterServerName);
+
     int numRandomAssignments = 0;
     int numRetainedAssigments = 0;
     for (Map.Entry<HRegionInfo, ServerName> entry : regions.entrySet()) {
@@ -556,10 +704,22 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
       if (oldServerName != null) {
         localServers = serversByHostname.get(oldServerName.getHostname());
       }
-      if (localServers.isEmpty()) {
+      if (masterIncluded && shouldBeOnMaster(region)) {
+        assignments.get(masterServerName).add(region);
+        if (localServers.contains(masterServerName)) {
+          numRetainedAssigments++;
+        } else {
+          numRandomAssignments++;
+        }
+      } else if (localServers.isEmpty()) {
         // No servers on the new cluster match up with this hostname,
         // assign randomly.
-        ServerName randomServer = servers.get(RANDOM.nextInt(servers.size()));
+        int i = RANDOM.nextInt(numServers);
+        ServerName randomServer = servers.get(i);
+        if (randomServer.equals(masterServerName)) {
+          i = (i == 0 ? 1 : i - 1);
+          randomServer = servers.get(i);
+        }
         assignments.get(randomServer).add(region);
         numRandomAssignments++;
         if (oldServerName != null) oldHostsNoLongerPresent.add(oldServerName.getHostname());
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
index 64c1921..acfebfb 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/ClusterLoadState.java
@@ -35,20 +35,29 @@ public class ClusterLoadState {
   private int numRegions = 0;
   private int numServers = 0;
 
-  public ClusterLoadState(Map<ServerName, List<HRegionInfo>> clusterState) {
-    super();
+  public ClusterLoadState(ServerName master,
+      Map<ServerName, List<HRegionInfo>> clusterState) {
     this.numRegions = 0;
     this.numServers = clusterState.size();
     this.clusterState = clusterState;
     serversByLoad = new TreeMap<ServerAndLoad, List<HRegionInfo>>();
     // Iterate so we can count regions as we build the map
     for (Map.Entry<ServerName, List<HRegionInfo>> server : clusterState.entrySet()) {
+      if (master != null && numServers > 1 && master.equals(server.getKey())) {
+        // Don't count the master regionserver since its
+        // load is meant to be low.
+        continue;
+      }
       List<HRegionInfo> regions = server.getValue();
       int sz = regions.size();
       if (sz == 0) emptyRegionServerPresent = true;
       numRegions += sz;
       serversByLoad.put(new ServerAndLoad(server.getKey(), sz), regions);
     }
+    if (master != null && numServers > 1
+        && clusterState.containsKey(master)) {
+      numServers--;
+    }
   }
 
   Map<ServerName, List<HRegionInfo>> getClusterState() {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
index da6b443..fc88212 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/SimpleLoadBalancer.java
@@ -180,10 +180,14 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
    */
   public List<RegionPlan> balanceCluster(
       Map<ServerName, List<HRegionInfo>> clusterMap) {
+    List<RegionPlan> regionsToReturn = balanceMasterRegions(clusterMap);
+    if (regionsToReturn != null) {
+      return regionsToReturn;
+    }
     boolean emptyRegionServerPresent = false;
     long startTime = System.currentTimeMillis();
 
-    ClusterLoadState cs = new ClusterLoadState(clusterMap);
+    ClusterLoadState cs = new ClusterLoadState(masterServerName, clusterMap);
 
     if (!this.needsBalance(cs)) return null;
     
@@ -204,7 +208,7 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
     // TODO: Look at data block locality or a more complex load to do this
     MinMaxPriorityQueue<RegionPlan> regionsToMove =
       MinMaxPriorityQueue.orderedBy(rpComparator).create();
-    List<RegionPlan> regionsToReturn = new ArrayList<RegionPlan>();
+    regionsToReturn = new ArrayList<RegionPlan>();
 
     // Walk down most loaded, pruning each to the max
     int serversOverloaded = 0;
@@ -233,8 +237,9 @@ public class SimpleLoadBalancer extends BaseLoadBalancer {
           hri = regions.get(regions.size() - 1 - i);
         }
         i++;
-        // Don't rebalance meta regions.
-        if (hri.isMetaRegion()) continue;
+        // Don't rebalance special regions.
+        if (shouldBeOnMaster(hri)
+            && masterServerName.equals(sal.getServerName())) continue;
         regionsToMove.add(new RegionPlan(hri, sal.getServerName(), null));
         numTaken++;
         if (numTaken >= numToOffload) break;
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
index c04e895..6eda66b 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
@@ -150,6 +150,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
 
     costFunctions = new CostFunction[]{
       new RegionCountSkewCostFunction(conf),
+      new RegionOnMasterCostFunction(conf),
       new MoveCostFunction(conf),
       localityCost,
       new TableSkewCostFunction(conf),
@@ -191,14 +192,18 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
    */
   @Override
   public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState) {
-    if (!needsBalance(new ClusterLoadState(clusterState))) {
+    List<RegionPlan> plans = balanceMasterRegions(clusterState);
+    if (plans != null) {
+      return plans;
+    }
+    if (!needsBalance(new ClusterLoadState(masterServerName, clusterState))) {
       return null;
     }
 
     long startTime = EnvironmentEdgeManager.currentTimeMillis();
 
     // Keep track of servers to iterate through them.
-    Cluster cluster = new Cluster(clusterState, loads, regionFinder);
+    Cluster cluster = new Cluster(masterServerName, clusterState, loads, regionFinder);
     double currentCost = computeCost(cluster, Double.MAX_VALUE);
 
     double initCost = currentCost;
@@ -257,7 +262,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
     metricsBalancer.balanceCluster(endTime - startTime);
 
     if (initCost > currentCost) {
-      List<RegionPlan> plans = createRegionPlans(cluster);
+      plans = createRegionPlans(cluster);
       if (LOG.isDebugEnabled()) {
         LOG.debug("Finished computing new load balance plan.  Computation took "
             + (endTime - startTime) + "ms to try " + step
@@ -563,8 +568,8 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
    */
   public abstract static class CostFunction {
 
-    private float multiplier = 0;
-    private Configuration conf;
+    protected float multiplier = 0;
+    protected Configuration conf;
 
     CostFunction(Configuration c) {
       this.conf = c;
@@ -742,6 +747,29 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
     }
   }
 
+  /**
+   * Compute the cost of a potential cluster configuration based upon if putting
+   * user regions on the master regionserver.
+   */
+  public static class RegionOnMasterCostFunction extends CostFunction {
+
+    private static final String REGION_ON_MASTER_COST_KEY =
+        "hbase.master.balancer.stochastic.regionOnMasterCost";
+    private static final float DEFAULT_REGION_ON_MASTER__COST = 1000;
+
+    RegionOnMasterCostFunction(Configuration conf) {
+      super(conf);
+      this.setMultiplier(conf.getFloat(
+        REGION_ON_MASTER_COST_KEY, DEFAULT_REGION_ON_MASTER__COST));
+    }
+
+    @Override
+    double cost(Cluster cluster) {
+      double max = cluster.numRegions;
+      double value = cluster.numUserRegionsOnMaster;
+      return scale(0, max, value);
+    }
+  }
 
   /**
    * Compute a cost of a potential cluster configuration based upon where
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
index d9a4a50..c767321 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
@@ -148,7 +148,7 @@ public class CreateTableHandler extends EventHandler {
     LOG.info("Create table " + tableName);
 
     try {
-      MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+      MasterCoprocessorHost cpHost = ((HMaster) this.server).getMasterCoprocessorHost();
       if (cpHost != null) {
         cpHost.preCreateTableHandler(this.hTableDescriptor, this.newRegions);
       }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
index 762b3f3..23c45b8 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DeleteTableHandler.java
@@ -61,7 +61,7 @@ public class DeleteTableHandler extends TableEventHandler {
   protected void handleTableOperation(List<HRegionInfo> regions)
   throws IOException, KeeperException {
     MasterCoprocessorHost cpHost = ((HMaster) this.server)
-        .getCoprocessorHost();
+        .getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preDeleteTableHandler(this.tableName);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
index 78a3c5b..d266c54 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/DisableTableHandler.java
@@ -129,7 +129,7 @@ public class DisableTableHandler extends EventHandler {
     try {
       LOG.info("Attempting to disable table " + this.tableName);
       MasterCoprocessorHost cpHost = ((HMaster) this.server)
-          .getCoprocessorHost();
+          .getMasterCoprocessorHost();
       if (cpHost != null) {
         cpHost.preDisableTableHandler(this.tableName);
       }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
index 0e21a25..11a5606 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java
@@ -22,7 +22,6 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.concurrent.ExecutorService;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -48,7 +47,6 @@ import org.apache.hadoop.hbase.master.TableLockManager;
 import org.apache.hadoop.hbase.master.TableLockManager.TableLock;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.zookeeper.KeeperException;
-import org.cloudera.htrace.Trace;
 
 /**
  * Handler to run enable of a table.
@@ -139,7 +137,7 @@ public class EnableTableHandler extends EventHandler {
     try {
       LOG.info("Attempting to enable the table " + this.tableName);
       MasterCoprocessorHost cpHost = ((HMaster) this.server)
-          .getCoprocessorHost();
+          .getMasterCoprocessorHost();
       if (cpHost != null) {
         cpHost.preEnableTableHandler(this.tableName);
       }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java
index 622a6ca..3c84af9 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/ModifyTableHandler.java
@@ -60,7 +60,7 @@ public class ModifyTableHandler extends TableEventHandler {
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris)
   throws IOException {
-    MasterCoprocessorHost cpHost = ((HMaster) this.server).getCoprocessorHost();
+    MasterCoprocessorHost cpHost = ((HMaster) this.server).getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preModifyTableHandler(this.tableName, this.htd);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java
index b377754..cd8fe9e 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java
@@ -61,7 +61,7 @@ public class TableAddFamilyHandler extends TableEventHandler {
   protected void handleTableOperation(List<HRegionInfo> hris)
   throws IOException {
     MasterCoprocessorHost cpHost = ((HMaster) this.server)
-        .getCoprocessorHost();
+        .getMasterCoprocessorHost();
     if(cpHost != null){
       cpHost.preAddColumnHandler(this.tableName, this.familyDesc);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java
index a4f16e3..330b9d8 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java
@@ -57,7 +57,7 @@ public class TableDeleteFamilyHandler extends TableEventHandler {
   @Override
   protected void handleTableOperation(List<HRegionInfo> hris) throws IOException {
     MasterCoprocessorHost cpHost = ((HMaster) this.server)
-        .getCoprocessorHost();
+        .getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preDeleteColumnHandler(this.tableName, this.familyName);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java
index b0dde7a..d07d0aa 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/TableModifyFamilyHandler.java
@@ -56,7 +56,7 @@ public class TableModifyFamilyHandler extends TableEventHandler {
   @Override
   protected void handleTableOperation(List<HRegionInfo> regions) throws IOException {
     MasterCoprocessorHost cpHost = ((HMaster) this.server)
-        .getCoprocessorHost();
+        .getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preModifyColumnHandler(this.tableName, this.familyDesc);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
index f45f660..50afb42 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/SnapshotManager.java
@@ -137,7 +137,6 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
 
   private boolean stopped;
   private MasterServices master;  // Needed by TableEventHandlers
-  private MetricsMaster metricsMaster;
   private ProcedureCoordinator coordinator;
 
   // Is snapshot feature enabled?
@@ -172,7 +171,6 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
       ProcedureCoordinator coordinator, ExecutorService pool)
       throws IOException, UnsupportedOperationException {
     this.master = master;
-    this.metricsMaster = metricsMaster;
 
     this.rootDir = master.getMasterFileSystem().getRootDir();
     checkSnapshotSupport(master.getConfiguration(), master.getMasterFileSystem());
@@ -260,7 +258,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
   public void deleteSnapshot(SnapshotDescription snapshot) throws SnapshotDoesNotExistException, IOException {
 
     // call coproc pre hook
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+    MasterCoprocessorHost cpHost = master.getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preDeleteSnapshot(snapshot);
     }
@@ -547,7 +545,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
         .build();
 
     // call pre coproc hook
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+    MasterCoprocessorHost cpHost = master.getMasterCoprocessorHost();
     if (cpHost != null) {
       cpHost.preSnapshot(snapshot, desc);
     }
@@ -668,7 +666,7 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
   public void restoreSnapshot(SnapshotDescription reqSnapshot) throws IOException {
     FileSystem fs = master.getMasterFileSystem().getFileSystem();
     Path snapshotDir = SnapshotDescriptionUtils.getCompletedSnapshotDir(reqSnapshot, rootDir);
-    MasterCoprocessorHost cpHost = master.getCoprocessorHost();
+    MasterCoprocessorHost cpHost = master.getMasterCoprocessorHost();
 
     // check if the snapshot exists
     if (!fs.exists(snapshotDir)) {
@@ -1005,7 +1003,6 @@ public class SnapshotManager extends MasterProcedureManager implements Stoppable
   public void initialize(MasterServices master, MetricsMaster metricsMaster) throws KeeperException,
       IOException, UnsupportedOperationException {
     this.master = master;
-    this.metricsMaster = metricsMaster;
 
     this.rootDir = master.getMasterFileSystem().getRootDir();
     checkSnapshotSupport(master.getConfiguration(), master.getMasterFileSystem());
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/StateDumpServlet.java hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/StateDumpServlet.java
index a254b33..f991e3b 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/StateDumpServlet.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/monitoring/StateDumpServlet.java
@@ -26,6 +26,7 @@ import javax.servlet.http.HttpServlet;
 import javax.servlet.http.HttpServletRequest;
 
 import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorStatus;
 import org.apache.hadoop.hbase.util.VersionInfo;
@@ -45,6 +46,10 @@ public abstract class StateDumpServlet extends HttpServlet {
         " on " + org.apache.hadoop.util.VersionInfo.getDate());
   }
 
+  protected boolean isShowQueueDump(Configuration conf){
+    return conf.getBoolean("hbase.regionserver.servlet.show.queuedump", true);
+  }
+
   protected long getTailKbParam(HttpServletRequest request) {
     String param = request.getParameter("tailkb");
     if (param == null) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
index 6051769..24fa862 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/AnnotationReadingPriorityFunction.java
@@ -24,7 +24,6 @@ import java.util.Map;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.ipc.PriorityFunction;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionRequest;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CompactRegionRequest;
@@ -38,7 +37,7 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
 import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RequestHeader;
-import org.apache.hadoop.hbase.regionserver.HRegionServer.QosPriority;
+import org.apache.hadoop.hbase.regionserver.RSRpcServices.QosPriority;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.protobuf.Message;
@@ -72,7 +71,7 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
   private final Map<String, Integer> annotatedQos;
   //We need to mock the regionserver instance for some unit tests (set via
   //setRegionServer method.
-  private HRegionServer hRegionServer;
+  private RSRpcServices rpcServices;
   @SuppressWarnings("unchecked")
   private final Class<? extends Message>[] knownArgumentClasses = new Class[]{
       GetRegionInfoRequest.class,
@@ -92,10 +91,9 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
   private final Map<String, Map<Class<? extends Message>, Method>> methodMap =
     new HashMap<String, Map<Class<? extends Message>, Method>>();
 
-  AnnotationReadingPriorityFunction(final HRegionServer hrs) {
-    this.hRegionServer = hrs;
+  AnnotationReadingPriorityFunction(final RSRpcServices rpcServices) {
     Map<String, Integer> qosMap = new HashMap<String, Integer>();
-    for (Method m : HRegionServer.class.getMethods()) {
+    for (Method m : RSRpcServices.class.getMethods()) {
       QosPriority p = m.getAnnotation(QosPriority.class);
       if (p != null) {
         // Since we protobuf'd, and then subsequently, when we went with pb style, method names
@@ -107,6 +105,7 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
         qosMap.put(capitalizedMethodName, p.priority());
       }
     }
+    this.rpcServices = rpcServices;
     this.annotatedQos = qosMap;
     if (methodMap.get("getRegion") == null) {
       methodMap.put("hasRegion", new HashMap<Class<? extends Message>, Method>());
@@ -129,16 +128,6 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
     return strBuilder.toString();
   }
 
-  public boolean isMetaRegion(byte[] regionName) {
-    HRegion region;
-    try {
-      region = hRegionServer.getRegion(regionName);
-    } catch (NotServingRegionException ignored) {
-      return false;
-    }
-    return region.getRegionInfo().isMetaTable();
-  }
-
   @Override
   public int getPriority(RequestHeader header, Message param) {
     String methodName = header.getMethodName();
@@ -167,7 +156,7 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
       if (hasRegion != null && (Boolean)hasRegion.invoke(param, (Object[])null)) {
         Method getRegion = methodMap.get("getRegion").get(rpcArgClass);
         regionSpecifier = (RegionSpecifier)getRegion.invoke(param, (Object[])null);
-        HRegion region = hRegionServer.getRegion(regionSpecifier);
+        HRegion region = rpcServices.getRegion(regionSpecifier);
         if (region.getRegionInfo().isMetaTable()) {
           if (LOG.isTraceEnabled()) {
             LOG.trace("High priority because region=" + region.getRegionNameAsString());
@@ -187,7 +176,7 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
       if (!request.hasScannerId()) {
         return HConstants.NORMAL_QOS;
       }
-      RegionScanner scanner = hRegionServer.getScanner(request.getScannerId());
+      RegionScanner scanner = rpcServices.getScanner(request.getScannerId());
       if (scanner != null && scanner.getRegionInfo().isMetaRegion()) {
         if (LOG.isTraceEnabled()) {
           // Scanner requests are small in size so TextFormat version should not overwhelm log.
@@ -201,6 +190,6 @@ class AnnotationReadingPriorityFunction implements PriorityFunction {
 
   @VisibleForTesting
   void setRegionServer(final HRegionServer hrs) {
-    this.hRegionServer = hrs;
+    this.rpcServices = hrs.getRSRpcServices();
   }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index d94f029..b7d7c5d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -21,8 +21,6 @@ package org.apache.hadoop.hbase.regionserver;
 import java.io.IOException;
 import java.io.InterruptedIOException;
 import java.lang.Thread.UncaughtExceptionHandler;
-import java.lang.annotation.Retention;
-import java.lang.annotation.RetentionPolicy;
 import java.lang.management.ManagementFactory;
 import java.lang.management.MemoryUsage;
 import java.lang.reflect.Constructor;
@@ -38,145 +36,61 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
-import java.util.Random;
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.TreeMap;
 import java.util.TreeSet;
-import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.ConcurrentSkipListMap;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 import javax.management.ObjectName;
+import javax.servlet.http.HttpServlet;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.protobuf.HBaseZeroCopyByteString;
+import org.apache.commons.lang.math.RandomUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.Cell;
-import org.apache.hadoop.hbase.CellScannable;
-import org.apache.hadoop.hbase.CellScanner;
-import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.Chore;
 import org.apache.hadoop.hbase.ClockOutOfSyncException;
-import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HBaseIOException;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.HealthCheckChore;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.Stoppable;
 import org.apache.hadoop.hbase.TableDescriptors;
 import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.YouAreDeadException;
 import org.apache.hadoop.hbase.ZNodeClearer;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
-import org.apache.hadoop.hbase.catalog.MetaReader;
-import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.ConnectionUtils;
-import org.apache.hadoop.hbase.client.Delete;
-import org.apache.hadoop.hbase.client.Get;
-import org.apache.hadoop.hbase.client.Increment;
-import org.apache.hadoop.hbase.client.Mutation;
-import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.client.Result;
-import org.apache.hadoop.hbase.client.RowMutations;
-import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.HConnection;
+import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
-import org.apache.hadoop.hbase.DroppedSnapshotException;
-import org.apache.hadoop.hbase.exceptions.FailedSanityCheckException;
-import org.apache.hadoop.hbase.exceptions.OperationConflictException;
-import org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException;
 import org.apache.hadoop.hbase.exceptions.RegionMovedException;
 import org.apache.hadoop.hbase.exceptions.RegionOpeningException;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.executor.ExecutorType;
-import org.apache.hadoop.hbase.filter.ByteArrayComparable;
-import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
-import org.apache.hadoop.hbase.io.hfile.HFile;
-import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
-import org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController;
-import org.apache.hadoop.hbase.ipc.PriorityFunction;
-import org.apache.hadoop.hbase.ipc.RpcCallContext;
 import org.apache.hadoop.hbase.ipc.RpcClient;
-import org.apache.hadoop.hbase.ipc.RpcServer;
-import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;
 import org.apache.hadoop.hbase.ipc.RpcServerInterface;
 import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;
-import org.apache.hadoop.hbase.ipc.ServerRpcController;
+import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.SplitLogManager;
 import org.apache.hadoop.hbase.master.TableLockManager;
 import org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
-import org.apache.hadoop.hbase.protobuf.ResponseConverter;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CompactRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CompactRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.FlushRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.FlushRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetOnlineRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetOnlineRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetStoreFileRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetStoreFileResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest.RegionOpenInfo;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse.RegionOpeningState;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.SplitRegionRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.SplitRegionResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UpdateFavoredNodesRequest;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UpdateFavoredNodesResponse;
-import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Condition;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.MutationType;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.RegionAction;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.RegionActionResult;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ResultOrException;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor;
@@ -184,40 +98,32 @@ import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameStringPair;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionServerInfo;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
-import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RequestHeader;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupRequest;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStatusService;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.ReportRSFatalErrorRequest;
-import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
-import org.apache.hadoop.hbase.regionserver.Leases.LeaseStillHeldException;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress;
 import org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler;
 import org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler;
-import org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler;
-import org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.wal.HLogFactory;
-import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtil;
 import org.apache.hadoop.hbase.regionserver.wal.WALActionsListener;
-import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
 import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.trace.SpanReceiverHost;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.CompressionTest;
-import org.apache.hadoop.hbase.util.Counter;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
 import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.util.HasThread;
 import org.apache.hadoop.hbase.util.InfoServer;
 import org.apache.hadoop.hbase.util.JvmPauseMonitor;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Sleeper;
-import org.apache.hadoop.hbase.util.Strings;
 import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hbase.util.VersionInfo;
 import org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker;
@@ -230,18 +136,15 @@ import org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hadoop.metrics.util.MBeanUtil;
-import org.apache.hadoop.net.DNS;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.data.Stat;
 
+import com.google.common.annotations.VisibleForTesting;
 import com.google.protobuf.BlockingRpcChannel;
-import com.google.protobuf.ByteString;
-import com.google.protobuf.Message;
-import com.google.protobuf.RpcController;
+import com.google.protobuf.HBaseZeroCopyByteString;
 import com.google.protobuf.ServiceException;
-import com.google.protobuf.TextFormat;
 
 /**
  * HRegionServer makes a set of HRegions available to clients. It checks in with
@@ -249,16 +152,11 @@ import com.google.protobuf.TextFormat;
  */
 @InterfaceAudience.Private
 @SuppressWarnings("deprecation")
-public class HRegionServer implements ClientProtos.ClientService.BlockingInterface,
-  AdminProtos.AdminService.BlockingInterface, Runnable, RegionServerServices,
-  HBaseRPCErrorHandler, LastSequenceId {
+public class HRegionServer extends HasThread implements
+    RegionServerServices, LastSequenceId {
 
   public static final Log LOG = LogFactory.getLog(HRegionServer.class);
 
-  private final Random rand;
-
-  private final AtomicLong scannerIdGen = new AtomicLong(0L);
-
   /*
    * Strings to be used in forming the exception message for
    * RegionsAlreadyInTransitionException.
@@ -272,12 +170,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   protected final ConcurrentMap<byte[], Boolean> regionsInTransitionInRS =
     new ConcurrentSkipListMap<byte[], Boolean>(Bytes.BYTES_COMPARATOR);
 
-  /** RPC scheduler to use for the region server. */
-  public static final String REGION_SERVER_RPC_SCHEDULER_FACTORY_CLASS =
-      "hbase.region.server.rpc.scheduler.factory.class";
-
-  protected long maxScannerResultSize;
-
   // Cache flushing
   protected MemStoreFlusher cacheFlusher;
 
@@ -302,9 +194,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   // Compactions
   public CompactSplitThread compactSplitThread;
 
-  final ConcurrentHashMap<String, RegionScannerHolder> scanners =
-      new ConcurrentHashMap<String, RegionScannerHolder>();
-
   /**
    * Map of regions currently being served by this region server. Key is the
    * encoded region name.  All access should be synchronized.
@@ -337,9 +226,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   // Instance of the hbase executor service.
   protected ExecutorService service;
 
-  // Request counter. (Includes requests that are not serviced by regions.)
-  final Counter requestCount = new Counter();
-
   // If false, the file system has become unavailable
   protected volatile boolean fsOk;
   protected HFileSystem fs;
@@ -347,14 +233,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   // Set when a report to the master comes back with a message asking us to
   // shutdown. Also set by call to stop when debugging or running unit tests
   // of HRegionServer in isolation.
-  protected volatile boolean stopped = false;
+  private volatile boolean stopped = false;
 
   // Go down hard. Used if file system becomes unavailable and also in
   // debugging and unit tests.
-  protected volatile boolean abortRequested;
-
-  // region server static info like info port
-  private RegionServerInfo.Builder rsInfo;
+  private volatile boolean abortRequested;
 
   ConcurrentMap<String, Integer> rowlocks = new ConcurrentHashMap<String, Integer>();
 
@@ -372,7 +255,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
   final int numRetries;
   protected final int threadWakeFrequency;
-  private final int msgInterval;
+  protected final int msgInterval;
 
   protected final int numRegionsToReport;
 
@@ -381,26 +264,18 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   // RPC client. Used to make the stub above that does region server status checking.
   RpcClient rpcClient;
 
-  // Server to handle client requests. Default access so can be accessed by
-  // unit tests.
-  RpcServerInterface rpcServer;
-
-  private final InetSocketAddress isa;
   private UncaughtExceptionHandler uncaughtExceptionHandler;
 
   // Info server. Default access so can be used by unit tests. REGIONSERVER
   // is name of the webapp and the attribute name used stuffing this instance
   // into web context.
-  InfoServer infoServer;
+  protected InfoServer infoServer;
   private JvmPauseMonitor pauseMonitor;
 
   /** region server process name */
   public static final String REGIONSERVER = "regionserver";
 
-  /** region server configuration name */
-  public static final String REGIONSERVER_CONF = "regionserver_conf";
-
-  private MetricsRegionServer metricsRegionServer;
+  MetricsRegionServer metricsRegionServer;
   private SpanReceiverHost spanReceiverHost;
 
   /*
@@ -423,23 +298,23 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   LogRoller hlogRoller;
   LogRoller metaHLogRoller;
 
-  // flag set after we're done setting up server threads (used for testing)
-  protected volatile boolean isOnline;
+  // flag set after we're done setting up server threads
+  protected AtomicBoolean online;
 
   // zookeeper connection and watcher
-  private ZooKeeperWatcher zooKeeper;
+  protected ZooKeeperWatcher zooKeeper;
 
   // master address tracker
   private MasterAddressTracker masterAddressTracker;
 
   // Cluster Status Tracker
-  private ClusterStatusTracker clusterStatusTracker;
+  protected ClusterStatusTracker clusterStatusTracker;
 
   // Log Splitting Worker
   private SplitLogWorker splitLogWorker;
 
   // A sleeper that sleeps for msgInterval.
-  private final Sleeper sleeper;
+  protected final Sleeper sleeper;
 
   private final int operationTimeout;
 
@@ -460,12 +335,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * against  Master.  The hostname can differ from the hostname in {@link #isa}
    * but usually doesn't if both servers resolve .
    */
-  private ServerName serverNameFromMasterPOV;
+  protected ServerName serverName;
 
   /**
    * This servers startcode.
    */
-  private final long startcode;
+  protected final long startcode;
 
   /**
    * Unique identifier for the cluster we are a part of.
@@ -482,25 +357,15 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   private MovedRegionsCleaner movedRegionsCleaner;
 
-  /**
-   * The lease timeout period for client scanners (milliseconds).
-   */
-  private final int scannerLeaseTimeoutPeriod;
-
-  /**
-   * The reference to the priority extraction function
-   */
-  private final PriorityFunction priority;
-
   private RegionServerCoprocessorHost rsHost;
 
   private RegionServerProcedureManagerHost rspmHost;
 
   // configuration setting on if replay WAL edits directly to another RS
-  private final boolean distributedLogReplay;
+  protected final boolean distributedLogReplay;
 
   // Table level lock manager for locking for region operations
-  private TableLockManager tableLockManager;
+  protected TableLockManager tableLockManager;
 
   /**
    * Nonce manager. Nonces are used to make operations like increment and append idempotent
@@ -520,10 +385,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * log (or both occasionally, which doesn't matter). Nonce log file can be deleted after the
    * latest nonce in it expired. It can also be recovered during move.
    */
-  private final ServerNonceManager nonceManager;
+  final ServerNonceManager nonceManager;
 
   private UserProvider userProvider;
 
+  protected final RSRpcServices rpcServices;
+
   /**
    * Starts a HRegionServer at the default location
    *
@@ -532,13 +399,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @throws InterruptedException
    */
   public HRegionServer(Configuration conf)
-  throws IOException, InterruptedException {
+      throws IOException, InterruptedException {
     this.fsOk = true;
     this.conf = conf;
-    this.isOnline = false;
     checkCodecs(this.conf);
+    this.online = new AtomicBoolean(false);
     this.userProvider = UserProvider.instantiate(conf);
-
     FSUtils.setupShortCircuitRead(this.conf);
 
     // Config'ed params
@@ -552,10 +418,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     boolean isNoncesEnabled = conf.getBoolean(HConstants.HBASE_RS_NONCES_ENABLED, true);
     this.nonceManager = isNoncesEnabled ? new ServerNonceManager(this.conf) : null;
 
-    this.maxScannerResultSize = conf.getLong(
-      HConstants.HBASE_CLIENT_SCANNER_MAX_RESULT_SIZE_KEY,
-      HConstants.DEFAULT_HBASE_CLIENT_SCANNER_MAX_RESULT_SIZE);
-
     this.numRegionsToReport = conf.getInt(
       "hbase.regionserver.numregionstoreport", 10);
 
@@ -566,58 +428,19 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     this.abortRequested = false;
     this.stopped = false;
 
-    this.scannerLeaseTimeoutPeriod = HBaseConfiguration.getInt(conf,
-      HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD,
-      HConstants.HBASE_REGIONSERVER_LEASE_PERIOD_KEY,
-      HConstants.DEFAULT_HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD);
-
-    // Server to handle client requests.
-    String hostname = conf.get("hbase.regionserver.ipc.address",
-      Strings.domainNamePointerToHostName(DNS.getDefaultHost(
-        conf.get("hbase.regionserver.dns.interface", "default"),
-        conf.get("hbase.regionserver.dns.nameserver", "default"))));
-    int port = conf.getInt(HConstants.REGIONSERVER_PORT,
-      HConstants.DEFAULT_REGIONSERVER_PORT);
-    // Creation of a HSA will force a resolve.
-    InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
-    if (initialIsa.getAddress() == null) {
-      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
-    }
-    this.rand = new Random(initialIsa.hashCode());
-    String name = "regionserver/" + initialIsa.toString();
-    // Set how many times to retry talking to another server over HConnection.
-    ConnectionUtils.setServerSideHConnectionRetriesConfig(this.conf, name, LOG);
-    this.priority = new AnnotationReadingPriorityFunction(this);
-    RpcSchedulerFactory rpcSchedulerFactory;
-    try {
-      Class<?> rpcSchedulerFactoryClass = conf.getClass(
-          REGION_SERVER_RPC_SCHEDULER_FACTORY_CLASS,
-          SimpleRpcSchedulerFactory.class);
-      rpcSchedulerFactory = ((RpcSchedulerFactory) rpcSchedulerFactoryClass.newInstance());
-    } catch (InstantiationException e) {
-      throw new IllegalArgumentException(e);
-    } catch (IllegalAccessException e) {
-      throw new IllegalArgumentException(e);
-    }
-    this.rpcServer = new RpcServer(this, name, getServices(),
-      /*HBaseRPCErrorHandler.class, OnlineRegions.class},*/
-      initialIsa, // BindAddress is IP we got for this server.
-      conf,
-      rpcSchedulerFactory.create(conf, this));
-
-    // Set our address.
-    this.isa = this.rpcServer.getListenerAddress();
-
-    this.rpcServer.setErrorHandler(this);
+    rpcServices = createRpcServices();
     this.startcode = System.currentTimeMillis();
+    String hostName = rpcServices.isa.getHostName();
+    serverName = ServerName.valueOf(hostName, rpcServices.isa.getPort(), startcode);
+    this.distributedLogReplay = HLogSplitter.isDistributedLogReplay(this.conf);
 
     // login the zookeeper client principal (if using security)
     ZKUtil.loginClient(this.conf, "hbase.zookeeper.client.keytab.file",
-      "hbase.zookeeper.client.kerberos.principal", this.isa.getHostName());
+      "hbase.zookeeper.client.kerberos.principal", hostName);
 
     // login the server principal (if using secure Hadoop)
     userProvider.login("hbase.regionserver.keytab.file",
-      "hbase.regionserver.kerberos.principal", this.isa.getHostName());
+      "hbase.regionserver.kerberos.principal", hostName);
     regionServerAccounting = new RegionServerAccounting();
     cacheConfig = new CacheConfig(conf);
     uncaughtExceptionHandler = new UncaughtExceptionHandler() {
@@ -627,25 +450,80 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       }
     };
 
-    this.rsInfo = RegionServerInfo.newBuilder();
-    // Put up the webui. Webui may come up on port other than configured if
-    // that port is occupied. Adjust serverInfo if this is the case.
-    this.rsInfo.setInfoPort(putUpWebUI());
-    this.distributedLogReplay = HLogSplitter.isDistributedLogReplay(this.conf);
+    // Set 'fs.defaultFS' to match the filesystem on hbase.rootdir else
+    // underlying hadoop hdfs accessors will be going against wrong filesystem
+    // (unless all is set to defaults).
+    FSUtils.setFsDefault(this.conf, FSUtils.getRootDir(this.conf));
+    // Get fs instance used by this RS.  Do we use checksum verification in the hbase? If hbase
+    // checksum verification enabled, then automatically switch off hdfs checksum verification.
+    boolean useHBaseChecksum = conf.getBoolean(HConstants.HBASE_CHECKSUM_VERIFICATION, true);
+    this.fs = new HFileSystem(this.conf, useHBaseChecksum);
+    this.rootDir = FSUtils.getRootDir(this.conf);
+    this.tableDescriptors = new FSTableDescriptors(
+      this.fs, this.rootDir, !canUpdateTableDescriptor());
+
+    service = new ExecutorService(getServerName().toShortString());
+    spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());
+
+    // Some unit tests don't need a cluster, so no zookeeper at all
+    if (!conf.getBoolean("hbase.testing.nocluster", false)) {
+      // Open connection to zookeeper and set primary watcher
+      zooKeeper = new ZooKeeperWatcher(conf, getProcessName() + ":" +
+        rpcServices.isa.getPort(), this, canCreateBaseZNode());
+  
+      tableLockManager = TableLockManager.createTableLockManager(
+        conf, zooKeeper, serverName);
+  
+      masterAddressTracker = new MasterAddressTracker(getZooKeeper(), this);
+      masterAddressTracker.start();
+  
+      clusterStatusTracker = new ClusterStatusTracker(zooKeeper, this);
+      clusterStatusTracker.start();
+  
+      catalogTracker = createCatalogTracker();
+      catalogTracker.start();
+    }
+
+    putUpWebUI();
+  }
+
+  protected String getProcessName() {
+    return REGIONSERVER;
+  }
+
+  protected boolean canCreateBaseZNode() {
+    return false;
+  }
+
+  protected boolean canUpdateTableDescriptor() {
+    return false;
+  }
+
+  protected RSRpcServices createRpcServices() throws IOException {
+    return new RSRpcServices(this);
+  }
+
+  protected void configureInfoServer() {
+    infoServer.addServlet("rs-status", "/rs-status", RSStatusServlet.class);
+    infoServer.setAttribute(REGIONSERVER, this);
+  }
+
+  protected Class<? extends HttpServlet> getDumpServlet() {
+    return RSDumpServlet.class;
+  }
+
+  protected void doMetrics() {
   }
 
   /**
-   * @return list of blocking services and their security info classes that this server supports
+   * Create CatalogTracker.
+   * In its own method so can intercept and mock it over in tests.
+   * @throws IOException
    */
-  private List<BlockingServiceAndInterface> getServices() {
-    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(2);
-    bssi.add(new BlockingServiceAndInterface(
-        ClientProtos.ClientService.newReflectiveBlockingService(this),
-        ClientProtos.ClientService.BlockingInterface.class));
-    bssi.add(new BlockingServiceAndInterface(
-        AdminProtos.AdminService.newReflectiveBlockingService(this),
-        AdminProtos.AdminService.BlockingInterface.class));
-    return bssi;
+  protected CatalogTracker createCatalogTracker() throws IOException {
+    HConnection conn = ConnectionUtils.createShortCircuitHConnection(
+      HConnectionManager.getConnection(conf), serverName, rpcServices, rpcServices);
+    return new CatalogTracker(zooKeeper, conf, conn, this);
   }
 
   /**
@@ -665,33 +543,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
   }
 
-  String getClusterId() {
+  public String getClusterId() {
     return this.clusterId;
   }
 
-  @Override
-  public int getPriority(RequestHeader header, Message param) {
-    return priority.getPriority(header, param);
-  }
-
-  @Retention(RetentionPolicy.RUNTIME)
-  protected @interface QosPriority {
-    int priority() default 0;
-  }
-
-  PriorityFunction getPriority() {
-    return priority;
-  }
-
-  RegionScanner getScanner(long scannerId) {
-    String scannerIdString = Long.toString(scannerId);
-    RegionScannerHolder scannerHolder = scanners.get(scannerIdString);
-    if (scannerHolder != null) {
-      return scannerHolder.s;
-    }
-    return null;
-  }
-
   /**
    * All initialization needed before we go register with Master.
    *
@@ -705,7 +560,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     } catch (Throwable t) {
       // Call stop if error or process will stick around for ever since server
       // puts up non-daemon threads.
-      this.rpcServer.stop();
+      this.rpcServices.stop();
       abort("Initialization of RS failed.  Hence aborting RS.", t);
     }
   }
@@ -719,10 +574,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @throws InterruptedException
    */
   private void initializeZooKeeper() throws IOException, InterruptedException {
-    // Open connection to zookeeper and set primary watcher
-    this.zooKeeper = new ZooKeeperWatcher(conf, REGIONSERVER + ":" +
-      this.isa.getPort(), this);
-
     // Create the master address tracker, register with zk, and start it.  Then
     // block until a master is available.  No point in starting up if no master
     // running.
@@ -732,14 +583,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     // Wait on cluster being up.  Master will set this flag up in zookeeper
     // when ready.
-    this.clusterStatusTracker = new ClusterStatusTracker(this.zooKeeper, this);
-    this.clusterStatusTracker.start();
     blockAndCheckIfStopped(this.clusterStatusTracker);
 
-    // Create the catalog tracker and start it;
-    this.catalogTracker = new CatalogTracker(this.zooKeeper, this.conf, this);
-    catalogTracker.start();
-
     // Retrieve clusterId
     // Since cluster status is now up
     // ID should have already been set by HMaster
@@ -761,9 +606,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     } catch (KeeperException e) {
       this.abort("Failed to reach zk cluster when creating procedure handler.", e);
     }
-    this.tableLockManager = TableLockManager.createTableLockManager(conf, zooKeeper,
-        ServerName.valueOf(isa.getHostName(), isa.getPort(), startcode));
-
     // register watcher for recovering regions
     if(this.distributedLogReplay) {
       this.recoveringRegionWatcher = new RecoveringRegionWatcher(this.zooKeeper, this);
@@ -823,7 +665,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     // Setup RPC client for master communication
     rpcClient = new RpcClient(conf, clusterId, new InetSocketAddress(
-        this.isa.getAddress(), 0));
+      rpcServices.isa.getAddress(), 0));
     this.pauseMonitor = new JvmPauseMonitor(conf);
     pauseMonitor.start();
   }
@@ -841,12 +683,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
 
     try {
-      // Set our ephemeral znode up in zookeeper now we have a name.
-      createMyEphemeralNode();
-
-      // Initialize the RegionServerCoprocessorHost now that our ephemeral
-      // node was created, in case any coprocessors want to use ZooKeeper
-      this.rsHost = new RegionServerCoprocessorHost(this, this.conf);
+      if (!isStopped() && !isAborted()) {
+        ShutdownHook.install(conf, fs, this, Thread.currentThread());
+        // Set our ephemeral znode up in zookeeper now we have a name.
+        createMyEphemeralNode();
+        // Initialize the RegionServerCoprocessorHost now that our ephemeral
+        // node was created, in case any coprocessors want to use ZooKeeper
+        this.rsHost = new RegionServerCoprocessorHost(this, this.conf);
+      }
 
       // Try and register with the Master; tell it we are here.  Break if
       // server is stopped or the clusterup flag is down or hdfs went wacky.
@@ -861,17 +705,17 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         }
       }
 
-      if (!this.stopped && isHealthy()){
+      if (!isStopped() && isHealthy()){
         // start the snapshot handler and other procedure handlers,
         // since the server is ready to run
         rspmHost.start();
       }
 
       // We registered with the Master.  Go into run mode.
-      long lastMsg = 0;
+      long lastMsg = System.currentTimeMillis();
       long oldRequestCount = -1;
       // The main run loop.
-      while (!this.stopped && isHealthy()) {
+      while (!isStopped() && isHealthy()) {
         if (!isClusterUp()) {
           if (isOnlineRegionsEmpty()) {
             stop("Exiting; cluster shutdown set and not carrying any regions");
@@ -903,11 +747,14 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         if ((now - lastMsg) >= msgInterval) {
           tryRegionServerReport(lastMsg, now);
           lastMsg = System.currentTimeMillis();
+          doMetrics();
+        }
+        if (!isStopped() && !isAborted()) {
+          this.sleeper.sleep();
         }
-        if (!this.stopped) this.sleeper.sleep();
       } // for
     } catch (Throwable t) {
-      if (!checkOOME(t)) {
+      if (!rpcServices.checkOOME(t)) {
         String prefix = t instanceof YouAreDeadException? "": "Unhandled: ";
         abort(prefix + t.getMessage(), t);
       }
@@ -918,7 +765,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       mxBean = null;
     }
     if (this.leases != null) this.leases.closeAfterLeasesExpire();
-    this.rpcServer.stop();
     if (this.splitLogWorker != null) {
       splitLogWorker.stop();
     }
@@ -942,8 +788,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     if(this.hMemManager != null) this.hMemManager.stop();
     if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();
     if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();
-    if (this.hlogRoller != null) this.hlogRoller.interruptIfNecessary();
-    if (this.metaHLogRoller != null) this.metaHLogRoller.interruptIfNecessary();
     if (this.compactionChecker != null)
       this.compactionChecker.interrupt();
     if (this.healthCheckChore != null) {
@@ -962,11 +806,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       if (this.fsOk) {
         closeUserRegions(abortRequested); // Don't leave any open file handles
       }
-      LOG.info("aborting server " + this.serverNameFromMasterPOV);
+      LOG.info("aborting server " + this.serverName);
     } else {
       closeUserRegions(abortRequested);
-      closeAllScanners();
-      LOG.info("stopping server " + this.serverNameFromMasterPOV);
+      LOG.info("stopping server " + this.serverName);
     }
     // Interrupt catalog tracker here in case any regions being opened out in
     // handlers are stuck waiting on meta.
@@ -985,7 +828,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     if (!this.killed && this.fsOk) {
       waitOnAllRegionsToClose(abortRequested);
-      LOG.info("stopping server " + this.serverNameFromMasterPOV +
+      LOG.info("stopping server " + this.serverName +
         "; all regions closed.");
     }
 
@@ -1005,9 +848,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
 
     if (!killed) {
-      join();
+      stopServiceThreads();
     }
 
+    this.rpcServices.stop();
+
     try {
       deleteMyEphemeralNode();
     } catch (KeeperException e) {
@@ -1016,8 +861,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     // We may have failed to delete the znode at the previous step, but
     //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.
     ZNodeClearer.deleteMyEphemeralNodeOnDisk();
+
     this.zooKeeper.close();
-    LOG.info("stopping server " + this.serverNameFromMasterPOV +
+    LOG.info("stopping server " + this.serverName +
       "; zookeeper connection closed.");
 
     LOG.info(Thread.currentThread().getName() + " exiting");
@@ -1061,7 +907,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     try {
       RegionServerReportRequest.Builder request = RegionServerReportRequest.newBuilder();
       ServerName sn = ServerName.parseVersionedServerName(
-        this.serverNameFromMasterPOV.getVersionedBytes());
+        this.serverName.getVersionedBytes());
       request.setServer(ProtobufUtil.toServerName(sn));
       request.setLoad(sl);
       this.rssStub.regionServerReport(null, request.build());
@@ -1210,18 +1056,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
   }
 
-  private void closeAllScanners() {
-    // Close any outstanding scanners. Means they'll get an UnknownScanner
-    // exception next time they come in.
-    for (Map.Entry<String, RegionScannerHolder> e : this.scanners.entrySet()) {
-      try {
-        e.getValue().s.close();
-      } catch (IOException ioe) {
-        LOG.warn("Closing scanner " + e.getKey(), ioe);
-      }
-    }
-  }
-
   /*
    * Run init. Sets up hlog and starts up all server threads.
    *
@@ -1235,11 +1069,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         // The hostname the master sees us as.
         if (key.equals(HConstants.KEY_FOR_HOSTNAME_SEEN_BY_MASTER)) {
           String hostnameFromMasterPOV = e.getValue();
-          this.serverNameFromMasterPOV = ServerName.valueOf(hostnameFromMasterPOV,
-              this.isa.getPort(), this.startcode);
-          if (!hostnameFromMasterPOV.equals(this.isa.getHostName())) {
+          this.serverName = ServerName.valueOf(hostnameFromMasterPOV,
+            rpcServices.isa.getPort(), this.startcode);
+          if (!hostnameFromMasterPOV.equals(rpcServices.isa.getHostName())) {
             LOG.info("Master passed us a different hostname to use; was=" +
-              this.isa.getHostName() + ", but now=" + hostnameFromMasterPOV);
+              rpcServices.isa.getHostName() + ", but now=" + hostnameFromMasterPOV);
           }
           continue;
         }
@@ -1254,39 +1088,27 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
       // config param for task trackers, but we can piggyback off of it.
       if (this.conf.get("mapred.task.id") == null) {
         this.conf.set("mapred.task.id", "hb_rs_" +
-          this.serverNameFromMasterPOV.toString());
+          this.serverName.toString());
       }
 
       // Save it in a file, this will allow to see if we crash
       ZNodeClearer.writeMyEphemeralNodeOnDisk(getMyEphemeralNodePath());
 
-      // Master sent us hbase.rootdir to use. Should be fully qualified
-      // path with file system specification included. Set 'fs.defaultFS'
-      // to match the filesystem on hbase.rootdir else underlying hadoop hdfs
-      // accessors will be going against wrong filesystem (unless all is set
-      // to defaults).
-      FSUtils.setFsDefault(this.conf, FSUtils.getRootDir(this.conf));
-      // Get fs instance used by this RS.  Do we use checksum verification in the hbase? If hbase
-      // checksum verification enabled, then automatically switch off hdfs checksum verification.
-      boolean useHBaseChecksum = conf.getBoolean(HConstants.HBASE_CHECKSUM_VERIFICATION, true);
-      this.fs = new HFileSystem(this.conf, useHBaseChecksum);
-      this.rootDir = FSUtils.getRootDir(this.conf);
-      this.tableDescriptors = new FSTableDescriptors(this.fs, this.rootDir, true);
       this.hlog = setupWALAndReplication();
       // Init in here rather than in constructor after thread name has been set
       this.metricsRegionServer = new MetricsRegionServer(new MetricsRegionServerWrapperImpl(this));
 
-      spanReceiverHost = SpanReceiverHost.getInstance(getConfiguration());
-
       startServiceThreads();
       startHeapMemoryManager();
-      LOG.info("Serving as " + this.serverNameFromMasterPOV +
-        ", RpcServer on " + this.isa +
+      LOG.info("Serving as " + this.serverName +
+        ", RpcServer on " + rpcServices.isa +
         ", sessionid=0x" +
         Long.toHexString(this.zooKeeper.getRecoverableZooKeeper().getSessionId()));
-      isOnline = true;
+      synchronized (online) {
+        online.set(true);
+        online.notifyAll();
+      }
     } catch (Throwable e) {
-      this.isOnline = false;
       stop("Failed initialization");
       throw convertThrowableToIOE(cleanup(e, "Failed init"),
           "Region server startup failed");
@@ -1303,6 +1125,8 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   private void createMyEphemeralNode() throws KeeperException, IOException {
+    RegionServerInfo.Builder rsInfo = RegionServerInfo.newBuilder();
+    rsInfo.setInfoPort(infoServer != null ? infoServer.getPort() : -1);
     byte[] data = ProtobufUtil.prependPBMagic(rsInfo.build().toByteArray());
     ZKUtil.createEphemeralNodeAndWatch(this.zooKeeper,
       getMyEphemeralNodePath(), data);
@@ -1473,7 +1297,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         if (r.shouldFlush()) {
           FlushRequester requester = server.getFlushRequester();
           if (requester != null) {
-            long randomDelay = rand.nextInt(RANGE_OF_DELAY) + MIN_DELAY_TIME;
+            long randomDelay = RandomUtils.nextInt(RANGE_OF_DELAY) + MIN_DELAY_TIME;
             LOG.info(getName() + " requesting flush for region " + r.getRegionNameAsString() +
                 " after a delay of " + randomDelay);
             //Throttle the flushes by putting a delay. If we don't throttle, and there
@@ -1494,7 +1318,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return true if online, false if not.
    */
   public boolean isOnline() {
-    return isOnline;
+    return online.get();
   }
 
   /**
@@ -1506,13 +1330,13 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   private HLog setupWALAndReplication() throws IOException {
     final Path oldLogDir = new Path(rootDir, HConstants.HREGION_OLDLOGDIR_NAME);
     final String logName
-      = HLogUtil.getHLogDirectoryName(this.serverNameFromMasterPOV.toString());
+      = HLogUtil.getHLogDirectoryName(this.serverName.toString());
 
     Path logdir = new Path(rootDir, logName);
     if (LOG.isDebugEnabled()) LOG.debug("logdir=" + logdir);
     if (this.fs.exists(logdir)) {
       throw new RegionServerRunningException("Region server has already " +
-        "created directory at " + this.serverNameFromMasterPOV.toString());
+        "created directory at " + this.serverName.toString());
     }
 
     // Instantiate replication manager if replication enabled.  Pass it the
@@ -1524,11 +1348,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
   private HLog getMetaWAL() throws IOException {
     if (this.hlogForMeta != null) return this.hlogForMeta;
-    final String logName = HLogUtil.getHLogDirectoryName(this.serverNameFromMasterPOV.toString());
+    final String logName = HLogUtil.getHLogDirectoryName(this.serverName.toString());
     Path logdir = new Path(rootDir, logName);
     if (LOG.isDebugEnabled()) LOG.debug("logdir=" + logdir);
     this.hlogForMeta = HLogFactory.createMetaHLog(this.fs.getBackingFs(), rootDir, logName,
-      this.conf, getMetaWALActionListeners(), this.serverNameFromMasterPOV.toString());
+      this.conf, getMetaWALActionListeners(), this.serverName.toString());
     return this.hlogForMeta;
   }
 
@@ -1541,7 +1365,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   protected HLog instantiateHLog(Path rootdir, String logName) throws IOException {
     return HLogFactory.createHLog(this.fs.getBackingFs(), rootdir, logName, this.conf,
-      getWALActionListeners(), this.serverNameFromMasterPOV.toString());
+      getWALActionListeners(), this.serverName.toString());
   }
 
   /**
@@ -1581,7 +1405,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     return hlogRoller;
   }
 
-  public MetricsRegionServer getMetrics() {
+  public MetricsRegionServer getRegionServerMetrics() {
     return this.metricsRegionServer;
   }
 
@@ -1605,9 +1429,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * hosting server. Worker logs the exception and exits.
    */
   private void startServiceThreads() throws IOException {
-    String n = Thread.currentThread().getName();
     // Start executor services
-    this.service = new ExecutorService(getServerName().toShortString());
     this.service.startExecutorService(ExecutorType.RS_OPEN_REGION,
       conf.getInt("hbase.regionserver.executor.openregion.threads", 3));
     this.service.startExecutorService(ExecutorType.RS_OPEN_META,
@@ -1623,25 +1445,25 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     this.service.startExecutorService(ExecutorType.RS_LOG_REPLAY_OPS,
       conf.getInt("hbase.regionserver.wal.max.splitters", SplitLogWorker.DEFAULT_MAX_SPLITTERS));
 
-    Threads.setDaemonThreadRunning(this.hlogRoller.getThread(), n + ".logRoller",
+    Threads.setDaemonThreadRunning(this.hlogRoller.getThread(), getName() + ".logRoller",
         uncaughtExceptionHandler);
     this.cacheFlusher.start(uncaughtExceptionHandler);
-    Threads.setDaemonThreadRunning(this.compactionChecker.getThread(), n +
+    Threads.setDaemonThreadRunning(this.compactionChecker.getThread(), getName() +
       ".compactionChecker", uncaughtExceptionHandler);
-    Threads.setDaemonThreadRunning(this.periodicFlusher.getThread(), n +
+    Threads.setDaemonThreadRunning(this.periodicFlusher.getThread(), getName() +
         ".periodicFlusher", uncaughtExceptionHandler);
     if (this.healthCheckChore != null) {
-      Threads.setDaemonThreadRunning(this.healthCheckChore.getThread(), n + ".healthChecker",
+      Threads.setDaemonThreadRunning(this.healthCheckChore.getThread(), getName() + ".healthChecker",
             uncaughtExceptionHandler);
     }
     if (this.nonceManagerChore != null) {
-      Threads.setDaemonThreadRunning(this.nonceManagerChore.getThread(), n + ".nonceCleaner",
+      Threads.setDaemonThreadRunning(this.nonceManagerChore.getThread(), getName() + ".nonceCleaner",
             uncaughtExceptionHandler);
     }
 
     // Leases is not a Thread. Internally it runs a daemon thread. If it gets
     // an unhandled exception, it will just exit.
-    this.leases.setName(n + ".leaseChecker");
+    this.leases.setName(getName() + ".leaseChecker");
     this.leases.start();
 
     if (this.replicationSourceHandler == this.replicationSinkHandler &&
@@ -1658,7 +1480,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     // Start Server.  This service is like leases in that it internally runs
     // a thread.
-    this.rpcServer.start();
+    rpcServices.rpcServer.start();
 
     // Create the log splitting worker and start it
     // set a smaller retries to fast fail otherwise splitlogworker could be blocked for
@@ -1681,7 +1503,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   private int putUpWebUI() throws IOException {
     int port = this.conf.getInt(HConstants.REGIONSERVER_INFO_PORT,
-				HConstants.DEFAULT_REGIONSERVER_INFOPORT);
+      HConstants.DEFAULT_REGIONSERVER_INFOPORT);
     // -1 is for disabling info server
     if (port < 0) return port;
     String addr = this.conf.get("hbase.regionserver.info.bindAddress", "0.0.0.0");
@@ -1690,11 +1512,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         false);
     while (true) {
       try {
-        this.infoServer = new InfoServer("regionserver", addr, port, false, this.conf);
-        this.infoServer.addServlet("status", "/rs-status", RSStatusServlet.class);
-        this.infoServer.addServlet("dump", "/dump", RSDumpServlet.class);
-        this.infoServer.setAttribute(REGIONSERVER, this);
-        this.infoServer.setAttribute(REGIONSERVER_CONF, conf);
+        this.infoServer = new InfoServer(getProcessName(), addr, port, false, this.conf);
+        infoServer.addServlet("dump", "/dump", getDumpServlet());
+        configureInfoServer();
         this.infoServer.start();
         break;
       } catch (BindException e) {
@@ -1708,7 +1528,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         port++;
       }
     }
-    return this.infoServer.getPort();
+    port = this.infoServer.getPort();
+    conf.setInt(HConstants.REGIONSERVER_INFO_PORT, port);
+    conf.setInt(HConstants.MASTER_INFO_PORT, port);
+    return port;
   }
 
   /*
@@ -1763,9 +1586,9 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   @Override
   public void stop(final String msg) {
     try {
-    	if (this.rsHost != null) {
-    		this.rsHost.preStop(msg);
-    	}
+      if (this.rsHost != null) {
+        this.rsHost.preStop(msg);
+      }
       this.stopped = true;
       LOG.info("STOPPED: " + msg);
       // Wakes run() if it is sleeping
@@ -1784,7 +1607,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   @Override
   public void postOpenDeployTasks(final HRegion r, final CatalogTracker ct)
   throws KeeperException, IOException {
-    checkOpen();
+    rpcServices.checkOpen();
     LOG.info("Post open deploy tasks for region=" + r.getRegionNameAsString());
     // Do checks to see if we need to compact (references or too many files)
     for (Store s : r.getStores().values()) {
@@ -1804,11 +1627,10 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
     // Update ZK, or META
     if (r.getRegionInfo().isMetaRegion()) {
-      MetaRegionTracker.setMetaLocation(getZooKeeper(),
-          this.serverNameFromMasterPOV);
+      MetaRegionTracker.setMetaLocation(getZooKeeper(), serverName);
     } else {
       MetaEditor.updateRegionLocation(ct, r.getRegionInfo(),
-        this.serverNameFromMasterPOV, openSeqNum);
+        this.serverName, openSeqNum);
     }
     LOG.info("Finished post open deploy task for " + r.getRegionNameAsString());
 
@@ -1816,7 +1638,12 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
   @Override
   public RpcServerInterface getRpcServer() {
-    return rpcServer;
+    return rpcServices.rpcServer;
+  }
+
+  @VisibleForTesting
+  public RSRpcServices getRSRpcServices() {
+    return rpcServices;
   }
 
   /**
@@ -1849,11 +1676,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         msg += "\nCause:\n" + StringUtils.stringifyException(cause);
       }
       // Report to the master but only if we have already registered with the master.
-      if (rssStub != null && this.serverNameFromMasterPOV != null) {
+      if (rssStub != null && this.serverName != null) {
         ReportRSFatalErrorRequest.Builder builder =
           ReportRSFatalErrorRequest.newBuilder();
         ServerName sn =
-          ServerName.parseVersionedServerName(this.serverNameFromMasterPOV.getVersionedBytes());
+          ServerName.parseVersionedServerName(this.serverName.getVersionedBytes());
         builder.setServer(ProtobufUtil.toServerName(sn));
         builder.setErrorMessage(msg);
         rssStub.reportRSFatalError(null, builder.build());
@@ -1890,7 +1717,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * Wait on all threads to finish. Presumption is that all closes and stops
    * have already been called.
    */
-  protected void join() {
+  protected void stopServiceThreads() {
     if (this.nonceManagerChore != null) {
       Threads.shutdown(this.nonceManagerChore.getThread());
     }
@@ -1951,7 +1778,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    * @return master + port, or null if server has been stopped
    */
   private Pair<ServerName, RegionServerStatusService.BlockingInterface>
-  createRegionServerStatusStub() {
+      createRegionServerStatusStub() {
     ServerName sn = null;
     long previousLogTime = 0;
     RegionServerStatusService.BlockingInterface master = null;
@@ -1974,7 +1801,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           continue;
         }
 
-        new InetSocketAddress(sn.getHostname(), sn.getPort());
+        // If we are on the active master, use the shortcut
+        if (this instanceof HMaster && sn.equals(getServerName())) {
+          intf = ((HMaster)this).getMasterRpcServices();
+          break;
+        }
         try {
           BlockingRpcChannel channel =
             this.rpcClient.createBlockingRpcChannel(sn, userProvider.getCurrent(), operationTimeout);
@@ -2033,11 +1864,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     ServerName masterServerName = p.getFirst();
     if (masterServerName == null) return result;
     try {
-      this.requestCount.set(0);
-      LOG.info("reportForDuty to master=" + masterServerName + " with port=" + this.isa.getPort() +
-        ", startcode=" + this.startcode);
+      rpcServices.requestCount.set(0);
+      LOG.info("reportForDuty to master=" + masterServerName + " with port="
+        + rpcServices.isa.getPort() + ", startcode=" + this.startcode);
       long now = EnvironmentEdgeManager.currentTimeMillis();
-      int port = this.isa.getPort();
+      int port = rpcServices.isa.getPort();
       RegionServerStartupRequest.Builder request = RegionServerStartupRequest.newBuilder();
       request.setPort(port);
       request.setServerStartCode(this.startcode);
@@ -2281,10 +2112,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
 
   @Override
   public ServerName getServerName() {
-    // Our servername could change after we talk to the master.
-    return this.serverNameFromMasterPOV == null?
-        ServerName.valueOf(this.isa.getHostName(), this.isa.getPort(), this.startcode) :
-        this.serverNameFromMasterPOV;
+    return serverName;
   }
 
   @Override
@@ -2292,11 +2120,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     return this.compactSplitThread;
   }
 
-  public ZooKeeperWatcher getZooKeeperWatcher() {
-    return this.zooKeeper;
-  }
-
-  public RegionServerCoprocessorHost getCoprocessorHost(){
+  public RegionServerCoprocessorHost getRegionServerCoprocessorHost(){
     return this.rsHost;
   }
 
@@ -2372,34 +2196,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   /**
-   * @param hrs
-   * @return Thread the RegionServer is running in correctly named.
-   * @throws IOException
-   */
-  public static Thread startRegionServer(final HRegionServer hrs)
-      throws IOException {
-    return startRegionServer(hrs, "regionserver" + hrs.isa.getPort());
-  }
-
-  /**
-   * @param hrs
-   * @param name
-   * @return Thread the RegionServer is running in correctly named.
-   * @throws IOException
-   */
-  public static Thread startRegionServer(final HRegionServer hrs,
-      final String name) throws IOException {
-    Thread t = new Thread(hrs);
-    t.setName(name);
-    t.start();
-    // Install shutdown hook that will catch signals and run an orderly shutdown
-    // of the hrs.
-    ShutdownHook.install(hrs.getConfiguration(), FileSystem.get(hrs
-        .getConfiguration()), hrs, t);
-    return t;
-  }
-
-  /**
    * Utility for constructing an instance of the passed HRegionServer class.
    *
    * @param regionServerClass
@@ -2457,7 +2253,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    }
 
   // used by org/apache/hbase/tmpl/regionserver/RSStatusTmpl.jamon (HBASE-4070).
-  public String[] getCoprocessors() {
+  public String[] getRegionServerCoprocessors() {
     TreeSet<String> coprocessors = new TreeSet<String>(
         this.hlog.getCoprocessorHost().getCoprocessors());
     Collection<HRegion> regions = getOnlineRegionsLocalContext();
@@ -2468,61 +2264,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
   }
 
   /**
-   * Instantiated as a scanner lease. If the lease times out, the scanner is
-   * closed
-   */
-  private class ScannerListener implements LeaseListener {
-    private final String scannerName;
-
-    ScannerListener(final String n) {
-      this.scannerName = n;
-    }
-
-    @Override
-    public void leaseExpired() {
-      RegionScannerHolder rsh = scanners.remove(this.scannerName);
-      if (rsh != null) {
-        RegionScanner s = rsh.s;
-        LOG.info("Scanner " + this.scannerName + " lease expired on region "
-            + s.getRegionInfo().getRegionNameAsString());
-        try {
-          HRegion region = getRegion(s.getRegionInfo().getRegionName());
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().preScannerClose(s);
-          }
-
-          s.close();
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().postScannerClose(s);
-          }
-        } catch (IOException e) {
-          LOG.error("Closing scanner for "
-              + s.getRegionInfo().getRegionNameAsString(), e);
-        }
-      } else {
-        LOG.warn("Scanner " + this.scannerName + " lease expired, but no related" +
-            " scanner found, hence no chance to close that related scanner!");
-      }
-    }
-  }
-
-  /**
-   * Called to verify that this server is up and running.
-   *
-   * @throws IOException
-   */
-  protected void checkOpen() throws IOException {
-    if (this.stopped || this.abortRequested) {
-      throw new RegionServerStoppedException("Server " + getServerName() +
-        " not running" + (this.abortRequested ? ", aborting" : ""));
-    }
-    if (!fsOk) {
-      throw new RegionServerStoppedException("File system not available");
-    }
-  }
-
-
-  /**
    * Try to close the region, logs a warning on failure but continues.
    * @param region Region to close
    */
@@ -2713,23 +2454,11 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    *
    * @param t Throwable
    *
-   * @return Throwable converted to an IOE; methods can only let out IOEs.
-   */
-  protected Throwable cleanup(final Throwable t) {
-    return cleanup(t, null);
-  }
-
-  /*
-   * Cleanup after Throwable caught invoking method. Converts <code>t</code> to
-   * IOE if it isn't already.
-   *
-   * @param t Throwable
-   *
    * @param msg Message to log in error. Can be null.
    *
    * @return Throwable converted to an IOE; methods can only let out IOEs.
    */
-  protected Throwable cleanup(final Throwable t, final String msg) {
+  private Throwable cleanup(final Throwable t, final String msg) {
     // Don't log as error if NSRE; NSRE is 'normal' operation.
     if (t instanceof NotServingRegionException) {
       LOG.debug("NotServingRegionException; " + t.getMessage());
@@ -2740,7 +2469,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     } else {
       LOG.error(msg, RemoteExceptionHandler.checkThrowable(t));
     }
-    if (!checkOOME(t)) {
+    if (!rpcServices.checkOOME(t)) {
       checkFileSystem();
     }
     return t;
@@ -2758,33 +2487,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         || msg.length() == 0 ? new IOException(t) : new IOException(msg, t));
   }
 
-  /*
-   * Check if an OOME and, if so, abort immediately to avoid creating more objects.
-   *
-   * @param e
-   *
-   * @return True if we OOME'd and are aborting.
-   */
-  @Override
-  public boolean checkOOME(final Throwable e) {
-    boolean stop = false;
-    try {
-      if (e instanceof OutOfMemoryError
-          || (e.getCause() != null && e.getCause() instanceof OutOfMemoryError)
-          || (e.getMessage() != null && e.getMessage().contains(
-              "java.lang.OutOfMemoryError"))) {
-        stop = true;
-        LOG.fatal(
-          "Run out of memory; HRegionServer will abort itself immediately", e);
-      }
-    } finally {
-      if (stop) {
-        Runtime.getRuntime().halt(1);
-      }
-    }
-    return stop;
-  }
-
   /**
    * Checks to see if the file system is still accessible. If not, sets
    * abortRequested and stopRequested
@@ -2803,1661 +2505,82 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     return this.fsOk;
   }
 
-  protected long addScanner(RegionScanner s, HRegion r) throws LeaseStillHeldException {
-    long scannerId = this.scannerIdGen.incrementAndGet();
-    String scannerName = String.valueOf(scannerId);
-
-    RegionScannerHolder existing =
-      scanners.putIfAbsent(scannerName, new RegionScannerHolder(s, r));
-    assert existing == null : "scannerId must be unique within regionserver's whole lifecycle!";
-
-    this.leases.createLease(scannerName, this.scannerLeaseTimeoutPeriod,
-        new ScannerListener(scannerName));
-
-    return scannerId;
+  @Override
+  public void updateRegionFavoredNodesMapping(String encodedRegionName,
+      List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> favoredNodes) {
+    InetSocketAddress[] addr = new InetSocketAddress[favoredNodes.size()];
+    // Refer to the comment on the declaration of regionFavoredNodesMap on why
+    // it is a map of region name to InetSocketAddress[]
+    for (int i = 0; i < favoredNodes.size(); i++) {
+      addr[i] = InetSocketAddress.createUnresolved(favoredNodes.get(i).getHostName(),
+          favoredNodes.get(i).getPort());
+    }
+    regionFavoredNodesMap.put(encodedRegionName, addr);
   }
 
-  // Start Client methods
-
   /**
-   * Get data from a table.
-   *
-   * @param controller the RPC controller
-   * @param request the get request
-   * @throws ServiceException
+   * Return the favored nodes for a region given its encoded name. Look at the
+   * comment around {@link #regionFavoredNodesMap} on why it is InetSocketAddress[]
+   * @param encodedRegionName
+   * @return array of favored locations
    */
   @Override
-  public GetResponse get(final RpcController controller,
-      final GetRequest request) throws ServiceException {
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-
-      GetResponse.Builder builder = GetResponse.newBuilder();
-      ClientProtos.Get get = request.getGet();
-      Boolean existence = null;
-      Result r = null;
-
-      if (get.hasClosestRowBefore() && get.getClosestRowBefore()) {
-        if (get.getColumnCount() != 1) {
-          throw new DoNotRetryIOException(
-            "get ClosestRowBefore supports one and only one family now, not "
-              + get.getColumnCount() + " families");
-        }
-        byte[] row = get.getRow().toByteArray();
-        byte[] family = get.getColumn(0).getFamily().toByteArray();
-        r = region.getClosestRowBefore(row, family);
-      } else {
-        Get clientGet = ProtobufUtil.toGet(get);
-        if (get.getExistenceOnly() && region.getCoprocessorHost() != null) {
-          existence = region.getCoprocessorHost().preExists(clientGet);
-        }
-        if (existence == null) {
-          r = region.get(clientGet);
-          if (get.getExistenceOnly()) {
-            boolean exists = r.getExists();
-            if (region.getCoprocessorHost() != null) {
-              exists = region.getCoprocessorHost().postExists(clientGet, exists);
-            }
-            existence = exists;
-          }
-        }
-      }
-      if (existence != null){
-        ClientProtos.Result pbr = ProtobufUtil.toResult(existence);
-        builder.setResult(pbr);
-      } else  if (r != null) {
-        ClientProtos.Result pbr = ProtobufUtil.toResult(r);
-        builder.setResult(pbr);
-      }
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    } finally {
-      metricsRegionServer.updateGet(EnvironmentEdgeManager.currentTimeMillis() - before);
-    }
+  public InetSocketAddress[] getFavoredNodesForRegion(String encodedRegionName) {
+    return regionFavoredNodesMap.get(encodedRegionName);
   }
 
-
-  /**
-   * Mutate data in a table.
-   *
-   * @param rpcc the RPC controller
-   * @param request the mutate request
-   * @throws ServiceException
-   */
   @Override
-  public MutateResponse mutate(final RpcController rpcc,
-      final MutateRequest request) throws ServiceException {
-    // rpc controller is how we bring in data via the back door;  it is unprotobuf'ed data.
-    // It is also the conduit via which we pass back data.
-    PayloadCarryingRpcController controller = (PayloadCarryingRpcController)rpcc;
-    CellScanner cellScanner = controller != null? controller.cellScanner(): null;
-    // Clear scanner so we are not holding on to reference across call.
-    if (controller != null) controller.setCellScanner(null);
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      MutateResponse.Builder builder = MutateResponse.newBuilder();
-      MutationProto mutation = request.getMutation();
-      if (!region.getRegionInfo().isMetaTable()) {
-        cacheFlusher.reclaimMemStoreMemory();
-      }
-      long nonceGroup = request.hasNonceGroup()
-          ? request.getNonceGroup() : HConstants.NO_NONCE;
-      Result r = null;
-      Boolean processed = null;
-      MutationType type = mutation.getMutateType();
-      switch (type) {
-      case APPEND:
-        // TODO: this doesn't actually check anything.
-        r = append(region, mutation, cellScanner, nonceGroup);
-        break;
-      case INCREMENT:
-        // TODO: this doesn't actually check anything.
-        r = increment(region, mutation, cellScanner, nonceGroup);
-        break;
-      case PUT:
-        Put put = ProtobufUtil.toPut(mutation, cellScanner);
-        if (request.hasCondition()) {
-          Condition condition = request.getCondition();
-          byte[] row = condition.getRow().toByteArray();
-          byte[] family = condition.getFamily().toByteArray();
-          byte[] qualifier = condition.getQualifier().toByteArray();
-          CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
-          ByteArrayComparable comparator =
-            ProtobufUtil.toComparator(condition.getComparator());
-          if (region.getCoprocessorHost() != null) {
-            processed = region.getCoprocessorHost().preCheckAndPut(
-              row, family, qualifier, compareOp, comparator, put);
-          }
-          if (processed == null) {
-            boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, put, true);
-            if (region.getCoprocessorHost() != null) {
-              result = region.getCoprocessorHost().postCheckAndPut(row, family,
-                qualifier, compareOp, comparator, put, result);
-            }
-            processed = result;
-          }
-        } else {
-          region.put(put);
-          processed = Boolean.TRUE;
-        }
-        break;
-      case DELETE:
-        Delete delete = ProtobufUtil.toDelete(mutation, cellScanner);
-        if (request.hasCondition()) {
-          Condition condition = request.getCondition();
-          byte[] row = condition.getRow().toByteArray();
-          byte[] family = condition.getFamily().toByteArray();
-          byte[] qualifier = condition.getQualifier().toByteArray();
-          CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
-          ByteArrayComparable comparator =
-            ProtobufUtil.toComparator(condition.getComparator());
-          if (region.getCoprocessorHost() != null) {
-            processed = region.getCoprocessorHost().preCheckAndDelete(
-              row, family, qualifier, compareOp, comparator, delete);
-          }
-          if (processed == null) {
-            boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, delete, true);
-            if (region.getCoprocessorHost() != null) {
-              result = region.getCoprocessorHost().postCheckAndDelete(row, family,
-                qualifier, compareOp, comparator, delete, result);
-            }
-            processed = result;
-          }
-        } else {
-          region.delete(delete);
-          processed = Boolean.TRUE;
-        }
-        break;
-        default:
-          throw new DoNotRetryIOException(
-            "Unsupported mutate type: " + type.name());
-      }
-      if (processed != null) builder.setProcessed(processed.booleanValue());
-      addResult(builder, r, controller);
-      return builder.build();
-    } catch (IOException ie) {
-      checkFileSystem();
-      throw new ServiceException(ie);
-    }
+  public ServerNonceManager getNonceManager() {
+    return this.nonceManager;
   }
 
+  private static class MovedRegionInfo {
+    private final ServerName serverName;
+    private final long seqNum;
+    private final long ts;
 
-  /**
-   * @return True if current call supports cellblocks
-   */
-  private boolean isClientCellBlockSupport() {
-    RpcCallContext context = RpcServer.getCurrentCall();
-    return context != null && context.isClientCellBlockSupport();
+    public MovedRegionInfo(ServerName serverName, long closeSeqNum) {
+      this.serverName = serverName;
+      this.seqNum = closeSeqNum;
+      ts = EnvironmentEdgeManager.currentTimeMillis();
+     }
+
+    public ServerName getServerName() {
+      return serverName;
+    }
+
+    public long getSeqNum() {
+      return seqNum;
+    }
+
+    public long getMoveTime() {
+      return ts;
+    }
   }
 
-  private void addResult(final MutateResponse.Builder builder,
-      final Result result, final PayloadCarryingRpcController rpcc) {
-    if (result == null) return;
-    if (isClientCellBlockSupport()) {
-      builder.setResult(ProtobufUtil.toResultNoData(result));
-      rpcc.setCellScanner(result.cellScanner());
-    } else {
-      ClientProtos.Result pbr = ProtobufUtil.toResult(result);
-      builder.setResult(pbr);
+  // This map will contains all the regions that we closed for a move.
+  //  We add the time it was moved as we don't want to keep too old information
+  protected Map<String, MovedRegionInfo> movedRegions =
+      new ConcurrentHashMap<String, MovedRegionInfo>(3000);
+
+  // We need a timeout. If not there is a risk of giving a wrong information: this would double
+  //  the number of network calls instead of reducing them.
+  private static final int TIMEOUT_REGION_MOVED = (2 * 60 * 1000);
+
+  protected void addToMovedRegions(String encodedName, ServerName destination, long closeSeqNum) {
+    if (ServerName.isSameHostnameAndPort(destination, this.getServerName())) {
+      LOG.warn("Not adding moved region record: " + encodedName + " to self.");
+      return;
     }
+    LOG.info("Adding moved region record: " + encodedName + " to "
+        + destination.getServerName() + ":" + destination.getPort()
+        + " as of " + closeSeqNum);
+    movedRegions.put(encodedName, new MovedRegionInfo(destination, closeSeqNum));
   }
 
-  //
-  // remote scanner interface
-  //
-
-  /**
-   * Scan data in a table.
-   *
-   * @param controller the RPC controller
-   * @param request the scan request
-   * @throws ServiceException
-   */
-  @Override
-  public ScanResponse scan(final RpcController controller, final ScanRequest request)
-  throws ServiceException {
-    Leases.Lease lease = null;
-    String scannerName = null;
-    try {
-      if (!request.hasScannerId() && !request.hasScan()) {
-        throw new DoNotRetryIOException(
-          "Missing required input: scannerId or scan");
-      }
-      long scannerId = -1;
-      if (request.hasScannerId()) {
-        scannerId = request.getScannerId();
-        scannerName = String.valueOf(scannerId);
-      }
-      try {
-        checkOpen();
-      } catch (IOException e) {
-        // If checkOpen failed, server not running or filesystem gone,
-        // cancel this lease; filesystem is gone or we're closing or something.
-        if (scannerName != null) {
-          try {
-            leases.cancelLease(scannerName);
-          } catch (LeaseException le) {
-            LOG.info("Server shutting down and client tried to access missing scanner " +
-              scannerName);
-          }
-        }
-        throw e;
-      }
-      requestCount.increment();
-
-      int ttl = 0;
-      HRegion region = null;
-      RegionScanner scanner = null;
-      RegionScannerHolder rsh = null;
-      boolean moreResults = true;
-      boolean closeScanner = false;
-      ScanResponse.Builder builder = ScanResponse.newBuilder();
-      if (request.hasCloseScanner()) {
-        closeScanner = request.getCloseScanner();
-      }
-      int rows = 1;
-      if (request.hasNumberOfRows()) {
-        rows = request.getNumberOfRows();
-      }
-      if (request.hasScannerId()) {
-        rsh = scanners.get(scannerName);
-        if (rsh == null) {
-          LOG.info("Client tried to access missing scanner " + scannerName);
-          throw new UnknownScannerException(
-            "Name: " + scannerName + ", already closed?");
-        }
-        scanner = rsh.s;
-        HRegionInfo hri = scanner.getRegionInfo();
-        region = getRegion(hri.getRegionName());
-        if (region != rsh.r) { // Yes, should be the same instance
-          throw new NotServingRegionException("Region was re-opened after the scanner"
-            + scannerName + " was created: " + hri.getRegionNameAsString());
-        }
-      } else {
-        region = getRegion(request.getRegion());
-        ClientProtos.Scan protoScan = request.getScan();
-        boolean isLoadingCfsOnDemandSet = protoScan.hasLoadColumnFamiliesOnDemand();
-        Scan scan = ProtobufUtil.toScan(protoScan);
-        // if the request doesn't set this, get the default region setting.
-        if (!isLoadingCfsOnDemandSet) {
-          scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());
-        }
-        scan.getAttribute(Scan.SCAN_ATTRIBUTES_METRICS_ENABLE);
-        region.prepareScanner(scan);
-        if (region.getCoprocessorHost() != null) {
-          scanner = region.getCoprocessorHost().preScannerOpen(scan);
-        }
-        if (scanner == null) {
-          scanner = region.getScanner(scan);
-        }
-        if (region.getCoprocessorHost() != null) {
-          scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);
-        }
-        scannerId = addScanner(scanner, region);
-        scannerName = String.valueOf(scannerId);
-        ttl = this.scannerLeaseTimeoutPeriod;
-      }
-
-      if (rows > 0) {
-        // if nextCallSeq does not match throw Exception straight away. This needs to be
-        // performed even before checking of Lease.
-        // See HBASE-5974
-        if (request.hasNextCallSeq()) {
-          if (rsh == null) {
-            rsh = scanners.get(scannerName);
-          }
-          if (rsh != null) {
-            if (request.getNextCallSeq() != rsh.nextCallSeq) {
-              throw new OutOfOrderScannerNextException("Expected nextCallSeq: " + rsh.nextCallSeq
-                + " But the nextCallSeq got from client: " + request.getNextCallSeq() +
-                "; request=" + TextFormat.shortDebugString(request));
-            }
-            // Increment the nextCallSeq value which is the next expected from client.
-            rsh.nextCallSeq++;
-          }
-        }
-        try {
-          // Remove lease while its being processed in server; protects against case
-          // where processing of request takes > lease expiration time.
-          lease = leases.removeLease(scannerName);
-          List<Result> results = new ArrayList<Result>(rows);
-          long currentScanResultSize = 0;
-
-          boolean done = false;
-          // Call coprocessor. Get region info from scanner.
-          if (region != null && region.getCoprocessorHost() != null) {
-            Boolean bypass = region.getCoprocessorHost().preScannerNext(
-              scanner, results, rows);
-            if (!results.isEmpty()) {
-              for (Result r : results) {
-                if (maxScannerResultSize < Long.MAX_VALUE){
-                  for (Cell kv : r.rawCells()) {
-                    // TODO
-                    currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
-                  }
-                }
-              }
-            }
-            if (bypass != null && bypass.booleanValue()) {
-              done = true;
-            }
-          }
-
-          if (!done) {
-            long maxResultSize = scanner.getMaxResultSize();
-            if (maxResultSize <= 0) {
-              maxResultSize = maxScannerResultSize;
-            }
-            List<Cell> values = new ArrayList<Cell>();
-            region.startRegionOperation(Operation.SCAN);
-            try {
-              int i = 0;
-              synchronized(scanner) {
-                for (; i < rows
-                    && currentScanResultSize < maxResultSize; ) {
-                  // Collect values to be returned here
-                  boolean moreRows = scanner.nextRaw(values);
-                  if (!values.isEmpty()) {
-                    if (maxScannerResultSize < Long.MAX_VALUE){
-                      for (Cell kv : values) {
-                        currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
-                      }
-                    }
-                    results.add(Result.create(values));
-                    i++;
-                  }
-                  if (!moreRows) {
-                    break;
-                  }
-                  values.clear();
-                }
-              }
-              region.readRequestsCount.add(i);
-            } finally {
-              region.closeRegionOperation();
-            }
-
-            // coprocessor postNext hook
-            if (region != null && region.getCoprocessorHost() != null) {
-              region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);
-            }
-          }
-
-          // If the scanner's filter - if any - is done with the scan
-          // and wants to tell the client to stop the scan. This is done by passing
-          // a null result, and setting moreResults to false.
-          if (scanner.isFilterDone() && results.isEmpty()) {
-            moreResults = false;
-            results = null;
-          } else {
-            addResults(builder, results, controller);
-          }
-        } finally {
-          // We're done. On way out re-add the above removed lease.
-          // Adding resets expiration time on lease.
-          if (scanners.containsKey(scannerName)) {
-            if (lease != null) leases.addLease(lease);
-            ttl = this.scannerLeaseTimeoutPeriod;
-          }
-        }
-      }
-
-      if (!moreResults || closeScanner) {
-        ttl = 0;
-        moreResults = false;
-        if (region != null && region.getCoprocessorHost() != null) {
-          if (region.getCoprocessorHost().preScannerClose(scanner)) {
-            return builder.build(); // bypass
-          }
-        }
-        rsh = scanners.remove(scannerName);
-        if (rsh != null) {
-          scanner = rsh.s;
-          scanner.close();
-          leases.cancelLease(scannerName);
-          if (region != null && region.getCoprocessorHost() != null) {
-            region.getCoprocessorHost().postScannerClose(scanner);
-          }
-        }
-      }
-
-      if (ttl > 0) {
-        builder.setTtl(ttl);
-      }
-      builder.setScannerId(scannerId);
-      builder.setMoreResults(moreResults);
-      return builder.build();
-    } catch (IOException ie) {
-      if (scannerName != null && ie instanceof NotServingRegionException) {
-        RegionScannerHolder rsh = scanners.remove(scannerName);
-        if (rsh != null) {
-          try {
-            RegionScanner scanner = rsh.s;
-            scanner.close();
-            leases.cancelLease(scannerName);
-          } catch (IOException e) {}
-        }
-      }
-      throw new ServiceException(ie);
-    }
-  }
-
-  private void addResults(final ScanResponse.Builder builder, final List<Result> results,
-      final RpcController controller) {
-    if (results == null || results.isEmpty()) return;
-    if (isClientCellBlockSupport()) {
-      for (Result res : results) {
-        builder.addCellsPerResult(res.size());
-      }
-      ((PayloadCarryingRpcController)controller).
-        setCellScanner(CellUtil.createCellScanner(results));
-    } else {
-      for (Result res: results) {
-        ClientProtos.Result pbr = ProtobufUtil.toResult(res);
-        builder.addResults(pbr);
-      }
-    }
-  }
-
-  /**
-   * Atomically bulk load several HFiles into an open region
-   * @return true if successful, false is failed but recoverably (no action)
-   * @throws IOException if failed unrecoverably
-   */
-  @Override
-  public BulkLoadHFileResponse bulkLoadHFile(final RpcController controller,
-      final BulkLoadHFileRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>();
-      for (FamilyPath familyPath: request.getFamilyPathList()) {
-        familyPaths.add(new Pair<byte[], String>(familyPath.getFamily().toByteArray(),
-          familyPath.getPath()));
-      }
-      boolean bypass = false;
-      if (region.getCoprocessorHost() != null) {
-        bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
-      }
-      boolean loaded = false;
-      if (!bypass) {
-        loaded = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum());
-      }
-      if (region.getCoprocessorHost() != null) {
-        loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
-      }
-      BulkLoadHFileResponse.Builder builder = BulkLoadHFileResponse.newBuilder();
-      builder.setLoaded(loaded);
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  @Override
-  public CoprocessorServiceResponse execService(final RpcController controller,
-      final CoprocessorServiceRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      Message result = execServiceOnRegion(region, request.getCall());
-      CoprocessorServiceResponse.Builder builder =
-          CoprocessorServiceResponse.newBuilder();
-      builder.setRegion(RequestConverter.buildRegionSpecifier(
-          RegionSpecifierType.REGION_NAME, region.getRegionName()));
-      builder.setValue(
-          builder.getValueBuilder().setName(result.getClass().getName())
-              .setValue(result.toByteString()));
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  private Message execServiceOnRegion(HRegion region,
-      final ClientProtos.CoprocessorServiceCall serviceCall) throws IOException {
-    // ignore the passed in controller (from the serialized call)
-    ServerRpcController execController = new ServerRpcController();
-    Message result = region.execService(execController, serviceCall);
-    if (execController.getFailedOn() != null) {
-      throw execController.getFailedOn();
-    }
-    return result;
-  }
-
-  /**
-   * Execute multiple actions on a table: get, mutate, and/or execCoprocessor
-   *
-   * @param rpcc the RPC controller
-   * @param request the multi request
-   * @throws ServiceException
-   */
-  @Override
-  public MultiResponse multi(final RpcController rpcc, final MultiRequest request)
-  throws ServiceException {
-    try {
-      checkOpen();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-
-    // rpc controller is how we bring in data via the back door;  it is unprotobuf'ed data.
-    // It is also the conduit via which we pass back data.
-    PayloadCarryingRpcController controller = (PayloadCarryingRpcController)rpcc;
-    CellScanner cellScanner = controller != null ? controller.cellScanner(): null;
-    if (controller != null) controller.setCellScanner(null);
-
-    long nonceGroup = request.hasNonceGroup() ? request.getNonceGroup() : HConstants.NO_NONCE;
-
-    // this will contain all the cells that we need to return. It's created later, if needed.
-    List<CellScannable> cellsToReturn = null;
-    MultiResponse.Builder responseBuilder = MultiResponse.newBuilder();
-    RegionActionResult.Builder regionActionResultBuilder = RegionActionResult.newBuilder();
-
-    for (RegionAction regionAction : request.getRegionActionList()) {
-      this.requestCount.add(regionAction.getActionCount());
-      HRegion region;
-      regionActionResultBuilder.clear();
-      try {
-        region = getRegion(regionAction.getRegion());
-      } catch (IOException e) {
-        regionActionResultBuilder.setException(ResponseConverter.buildException(e));
-        responseBuilder.addRegionActionResult(regionActionResultBuilder.build());
-        continue;  // For this region it's a failure.
-      }
-
-      if (regionAction.hasAtomic() && regionAction.getAtomic()) {
-        // How does this call happen?  It may need some work to play well w/ the surroundings.
-        // Need to return an item per Action along w/ Action index.  TODO.
-        try {
-          mutateRows(region, regionAction.getActionList(), cellScanner);
-        } catch (IOException e) {
-          // As it's atomic, we may expect it's a global failure.
-          regionActionResultBuilder.setException(ResponseConverter.buildException(e));
-        }
-      } else {
-        // doNonAtomicRegionMutation manages the exception internally
-        cellsToReturn = doNonAtomicRegionMutation(region, regionAction, cellScanner,
-            regionActionResultBuilder, cellsToReturn, nonceGroup);
-      }
-      responseBuilder.addRegionActionResult(regionActionResultBuilder.build());
-    }
-    // Load the controller with the Cells to return.
-    if (cellsToReturn != null && !cellsToReturn.isEmpty() && controller != null) {
-      controller.setCellScanner(CellUtil.createCellScanner(cellsToReturn));
-    }
-    return responseBuilder.build();
-  }
-
-  /**
-   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when
-   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.
-   * @param region
-   * @param actions
-   * @param cellScanner
-   * @param builder
-   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this
-   * method returns as a 'result'.
-   * @return Return the <code>cellScanner</code> passed
-   */
-  private List<CellScannable> doNonAtomicRegionMutation(final HRegion region,
-      final RegionAction actions, final CellScanner cellScanner,
-      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup) {
-    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do
-    // one at a time, we instead pass them in batch.  Be aware that the corresponding
-    // ResultOrException instance that matches each Put or Delete is then added down in the
-    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched
-    List<ClientProtos.Action> mutations = null;
-    for (ClientProtos.Action action: actions.getActionList()) {
-      ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = null;
-      try {
-        Result r = null;
-        if (action.hasGet()) {
-          Get get = ProtobufUtil.toGet(action.getGet());
-          r = region.get(get);
-        } else if (action.hasServiceCall()) {
-          resultOrExceptionBuilder = ResultOrException.newBuilder();
-          try {
-            Message result = execServiceOnRegion(region, action.getServiceCall());
-            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =
-                ClientProtos.CoprocessorServiceResult.newBuilder();
-            resultOrExceptionBuilder.setServiceResult(
-                serviceResultBuilder.setValue(
-                  serviceResultBuilder.getValueBuilder()
-                    .setName(result.getClass().getName())
-                    .setValue(result.toByteString())));
-          } catch (IOException ioe) {
-            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));
-          }
-        } else if (action.hasMutation()) {
-          MutationType type = action.getMutation().getMutateType();
-          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&
-              !mutations.isEmpty()) {
-            // Flush out any Puts or Deletes already collected.
-            doBatchOp(builder, region, mutations, cellScanner);
-            mutations.clear();
-          }
-          switch (type) {
-          case APPEND:
-            r = append(region, action.getMutation(), cellScanner, nonceGroup);
-            break;
-          case INCREMENT:
-            r = increment(region, action.getMutation(), cellScanner,  nonceGroup);
-            break;
-          case PUT:
-          case DELETE:
-            // Collect the individual mutations and apply in a batch
-            if (mutations == null) {
-              mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());
-            }
-            mutations.add(action);
-            break;
-          default:
-            throw new DoNotRetryIOException("Unsupported mutate type: " + type.name());
-          }
-        } else {
-          throw new HBaseIOException("Unexpected Action type");
-        }
-        if (r != null) {
-          ClientProtos.Result pbResult = null;
-          if (isClientCellBlockSupport()) {
-            pbResult = ProtobufUtil.toResultNoData(r);
-            //  Hard to guess the size here.  Just make a rough guess.
-            if (cellsToReturn == null) cellsToReturn = new ArrayList<CellScannable>();
-            cellsToReturn.add(r);
-          } else {
-            pbResult = ProtobufUtil.toResult(r);
-          }
-          resultOrExceptionBuilder =
-            ClientProtos.ResultOrException.newBuilder().setResult(pbResult);
-        }
-        // Could get to here and there was no result and no exception.  Presumes we added
-        // a Put or Delete to the collecting Mutations List for adding later.  In this
-        // case the corresponding ResultOrException instance for the Put or Delete will be added
-        // down in the doBatchOp method call rather than up here.
-      } catch (IOException ie) {
-        resultOrExceptionBuilder = ResultOrException.newBuilder().
-          setException(ResponseConverter.buildException(ie));
-      }
-      if (resultOrExceptionBuilder != null) {
-        // Propagate index.
-        resultOrExceptionBuilder.setIndex(action.getIndex());
-        builder.addResultOrException(resultOrExceptionBuilder.build());
-      }
-    }
-    // Finish up any outstanding mutations
-    if (mutations != null && !mutations.isEmpty()) {
-      doBatchOp(builder, region, mutations, cellScanner);
-    }
-    return cellsToReturn;
-  }
-
-// End Client methods
-// Start Admin methods
-
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public GetRegionInfoResponse getRegionInfo(final RpcController controller,
-      final GetRegionInfoRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      HRegionInfo info = region.getRegionInfo();
-      GetRegionInfoResponse.Builder builder = GetRegionInfoResponse.newBuilder();
-      builder.setRegionInfo(HRegionInfo.convert(info));
-      if (request.hasCompactionState() && request.getCompactionState()) {
-        builder.setCompactionState(region.getCompactionState());
-      }
-      builder.setIsRecovering(region.isRecovering());
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  @Override
-  public GetStoreFileResponse getStoreFile(final RpcController controller,
-      final GetStoreFileRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      HRegion region = getRegion(request.getRegion());
-      requestCount.increment();
-      Set<byte[]> columnFamilies;
-      if (request.getFamilyCount() == 0) {
-        columnFamilies = region.getStores().keySet();
-      } else {
-        columnFamilies = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
-        for (ByteString cf: request.getFamilyList()) {
-          columnFamilies.add(cf.toByteArray());
-        }
-      }
-      int nCF = columnFamilies.size();
-      List<String>  fileList = region.getStoreFileList(
-        columnFamilies.toArray(new byte[nCF][]));
-      GetStoreFileResponse.Builder builder = GetStoreFileResponse.newBuilder();
-      builder.addAllStoreFile(fileList);
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public GetOnlineRegionResponse getOnlineRegion(final RpcController controller,
-      final GetOnlineRegionRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      List<HRegionInfo> list = new ArrayList<HRegionInfo>(onlineRegions.size());
-      for (HRegion region: this.onlineRegions.values()) {
-        list.add(region.getRegionInfo());
-      }
-      Collections.sort(list);
-      return ResponseConverter.buildGetOnlineRegionResponse(list);
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  // Region open/close direct RPCs
-
-  /**
-   * Open asynchronously a region or a set of regions on the region server.
-   *
-   * The opening is coordinated by ZooKeeper, and this method requires the znode to be created
-   *  before being called. As a consequence, this method should be called only from the master.
-   * <p>
-   * Different manages states for the region are:<ul>
-   *  <li>region not opened: the region opening will start asynchronously.</li>
-   *  <li>a close is already in progress: this is considered as an error.</li>
-   *  <li>an open is already in progress: this new open request will be ignored. This is important
-   *  because the Master can do multiple requests if it crashes.</li>
-   *  <li>the region is already opened:  this new open request will be ignored./li>
-   *  </ul>
-   * </p>
-   * <p>
-   * Bulk assign: If there are more than 1 region to open, it will be considered as a bulk assign.
-   * For a single region opening, errors are sent through a ServiceException. For bulk assign,
-   * errors are put in the response as FAILED_OPENING.
-   * </p>
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public OpenRegionResponse openRegion(final RpcController controller,
-      final OpenRegionRequest request) throws ServiceException {
-    try {
-      checkOpen();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-    requestCount.increment();
-    if (request.hasServerStartCode() && this.serverNameFromMasterPOV != null) {
-      // check that we are the same server that this RPC is intended for.
-      long serverStartCode = request.getServerStartCode();
-      if (this.serverNameFromMasterPOV.getStartcode() !=  serverStartCode) {
-        throw new ServiceException(new DoNotRetryIOException("This RPC was intended for a " +
-            "different server with startCode: " + serverStartCode + ", this server is: "
-            + this.serverNameFromMasterPOV));
-      }
-    }
-    OpenRegionResponse.Builder builder = OpenRegionResponse.newBuilder();
-    final int regionCount = request.getOpenInfoCount();
-    final Map<TableName, HTableDescriptor> htds =
-        new HashMap<TableName, HTableDescriptor>(regionCount);
-    final boolean isBulkAssign = regionCount > 1;
-    for (RegionOpenInfo regionOpenInfo : request.getOpenInfoList()) {
-      final HRegionInfo region = HRegionInfo.convert(regionOpenInfo.getRegion());
-
-      int versionOfOfflineNode = -1;
-      if (regionOpenInfo.hasVersionOfOfflineNode()) {
-        versionOfOfflineNode = regionOpenInfo.getVersionOfOfflineNode();
-      }
-      HTableDescriptor htd;
-      try {
-        final HRegion onlineRegion = getFromOnlineRegions(region.getEncodedName());
-        if (onlineRegion != null) {
-          //Check if the region can actually be opened.
-          if (onlineRegion.getCoprocessorHost() != null) {
-            onlineRegion.getCoprocessorHost().preOpen();
-          }
-          // See HBASE-5094. Cross check with hbase:meta if still this RS is owning
-          // the region.
-          Pair<HRegionInfo, ServerName> p = MetaReader.getRegion(
-              this.catalogTracker, region.getRegionName());
-          if (this.getServerName().equals(p.getSecond())) {
-            Boolean closing = regionsInTransitionInRS.get(region.getEncodedNameAsBytes());
-            // Map regionsInTransitionInRSOnly has an entry for a region only if the region
-            // is in transition on this RS, so here closing can be null. If not null, it can
-            // be true or false. True means the region is opening on this RS; while false
-            // means the region is closing. Only return ALREADY_OPENED if not closing (i.e.
-            // not in transition any more, or still transition to open.
-            if (!Boolean.FALSE.equals(closing)
-                && getFromOnlineRegions(region.getEncodedName()) != null) {
-              LOG.warn("Attempted open of " + region.getEncodedName()
-                + " but already online on this server");
-              builder.addOpeningState(RegionOpeningState.ALREADY_OPENED);
-              continue;
-            }
-          } else {
-            LOG.warn("The region " + region.getEncodedName() + " is online on this server" +
-                " but hbase:meta does not have this server - continue opening.");
-            removeFromOnlineRegions(onlineRegion, null);
-          }
-        }
-        LOG.info("Open " + region.getRegionNameAsString());
-        htd = htds.get(region.getTable());
-        if (htd == null) {
-          htd = this.tableDescriptors.get(region.getTable());
-          htds.put(region.getTable(), htd);
-        }
-
-        final Boolean previous = this.regionsInTransitionInRS.putIfAbsent(
-            region.getEncodedNameAsBytes(), Boolean.TRUE);
-
-        if (Boolean.FALSE.equals(previous)) {
-          // There is a close in progress. We need to mark this open as failed in ZK.
-          OpenRegionHandler.
-              tryTransitionFromOfflineToFailedOpen(this, region, versionOfOfflineNode);
-
-          throw new RegionAlreadyInTransitionException("Received OPEN for the region:" +
-              region.getRegionNameAsString() + " , which we are already trying to CLOSE ");
-        }
-
-        if (Boolean.TRUE.equals(previous)) {
-          // An open is in progress. This is supported, but let's log this.
-          LOG.info("Receiving OPEN for the region:" +
-              region.getRegionNameAsString() + " , which we are already trying to OPEN" +
-              " - ignoring this new request for this region.");
-        }
-
-        // We are opening this region. If it moves back and forth for whatever reason, we don't
-        // want to keep returning the stale moved record while we are opening/if we close again.
-        removeFromMovedRegions(region.getEncodedName());
-
-        if (previous == null) {
-          // check if the region to be opened is marked in recovering state in ZK
-          if (this.distributedLogReplay
-              && SplitLogManager.isRegionMarkedRecoveringInZK(this.getZooKeeper(),
-            region.getEncodedName())) {
-            this.recoveringRegions.put(region.getEncodedName(), null);
-          }
-          // If there is no action in progress, we can submit a specific handler.
-          // Need to pass the expected version in the constructor.
-          if (region.isMetaRegion()) {
-            this.service.submit(new OpenMetaHandler(this, this, region, htd,
-                versionOfOfflineNode));
-          } else {
-            updateRegionFavoredNodesMapping(region.getEncodedName(),
-                regionOpenInfo.getFavoredNodesList());
-            this.service.submit(new OpenRegionHandler(this, this, region, htd,
-                versionOfOfflineNode));
-          }
-        }
-
-        builder.addOpeningState(RegionOpeningState.OPENED);
-
-      } catch (KeeperException zooKeeperEx) {
-        LOG.error("Can't retrieve recovering state from zookeeper", zooKeeperEx);
-        throw new ServiceException(zooKeeperEx);
-      } catch (IOException ie) {
-        LOG.warn("Failed opening region " + region.getRegionNameAsString(), ie);
-        if (isBulkAssign) {
-          builder.addOpeningState(RegionOpeningState.FAILED_OPENING);
-        } else {
-          throw new ServiceException(ie);
-        }
-      }
-    }
-
-    return builder.build();
-  }
-
-  @Override
-  public void updateRegionFavoredNodesMapping(String encodedRegionName,
-      List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> favoredNodes) {
-    InetSocketAddress[] addr = new InetSocketAddress[favoredNodes.size()];
-    // Refer to the comment on the declaration of regionFavoredNodesMap on why
-    // it is a map of region name to InetSocketAddress[]
-    for (int i = 0; i < favoredNodes.size(); i++) {
-      addr[i] = InetSocketAddress.createUnresolved(favoredNodes.get(i).getHostName(),
-          favoredNodes.get(i).getPort());
-    }
-    regionFavoredNodesMap.put(encodedRegionName, addr);
-  }
-
-  /**
-   * Return the favored nodes for a region given its encoded name. Look at the
-   * comment around {@link #regionFavoredNodesMap} on why it is InetSocketAddress[]
-   * @param encodedRegionName
-   * @return array of favored locations
-   */
-  @Override
-  public InetSocketAddress[] getFavoredNodesForRegion(String encodedRegionName) {
-    return regionFavoredNodesMap.get(encodedRegionName);
-  }
-
-  /**
-   * Close a region on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public CloseRegionResponse closeRegion(final RpcController controller,
-      final CloseRegionRequest request) throws ServiceException {
-    int versionOfClosingNode = -1;
-    if (request.hasVersionOfClosingNode()) {
-      versionOfClosingNode = request.getVersionOfClosingNode();
-    }
-    boolean zk = request.getTransitionInZK();
-    final ServerName sn = (request.hasDestinationServer() ?
-      ProtobufUtil.toServerName(request.getDestinationServer()) : null);
-
-    try {
-      checkOpen();
-      if (request.hasServerStartCode() && this.serverNameFromMasterPOV != null) {
-        // check that we are the same server that this RPC is intended for.
-        long serverStartCode = request.getServerStartCode();
-        if (this.serverNameFromMasterPOV.getStartcode() !=  serverStartCode) {
-          throw new ServiceException(new DoNotRetryIOException("This RPC was intended for a " +
-              "different server with startCode: " + serverStartCode + ", this server is: "
-              + this.serverNameFromMasterPOV));
-        }
-      }
-      final String encodedRegionName = ProtobufUtil.getRegionEncodedName(request.getRegion());
-
-      // Can be null if we're calling close on a region that's not online
-      final HRegion region = this.getFromOnlineRegions(encodedRegionName);
-      if ((region  != null) && (region .getCoprocessorHost() != null)) {
-        region.getCoprocessorHost().preClose(false);
-      }
-
-      requestCount.increment();
-      LOG.info("Close " + encodedRegionName + ", via zk=" + (zk ? "yes" : "no") +
-        ", znode version=" + versionOfClosingNode + ", on " + sn);
-
-      boolean closed = closeRegion(encodedRegionName, false, zk, versionOfClosingNode, sn);
-      CloseRegionResponse.Builder builder = CloseRegionResponse.newBuilder().setClosed(closed);
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Flush a region on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public FlushRegionResponse flushRegion(final RpcController controller,
-      final FlushRegionRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      LOG.info("Flushing " + region.getRegionNameAsString());
-      boolean shouldFlush = true;
-      if (request.hasIfOlderThanTs()) {
-        shouldFlush = region.getLastFlushTime() < request.getIfOlderThanTs();
-      }
-      FlushRegionResponse.Builder builder = FlushRegionResponse.newBuilder();
-      if (shouldFlush) {
-        boolean result = region.flushcache();
-        if (result) {
-          this.compactSplitThread.requestSystemCompaction(region,
-              "Compaction through user triggered flush");
-        }
-        builder.setFlushed(result);
-      }
-      builder.setLastFlushTime(region.getLastFlushTime());
-      return builder.build();
-    } catch (DroppedSnapshotException ex) {
-      // Cache flush can fail in a few places. If it fails in a critical
-      // section, we get a DroppedSnapshotException and a replay of hlog
-      // is required. Currently the only way to do this is a restart of
-      // the server.
-      abort("Replay of HLog required. Forcing server shutdown", ex);
-      throw new ServiceException(ex);
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Split a region on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public SplitRegionResponse splitRegion(final RpcController controller,
-      final SplitRegionRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      region.startRegionOperation(Operation.SPLIT_REGION);
-      LOG.info("Splitting " + region.getRegionNameAsString());
-      region.flushcache();
-      byte[] splitPoint = null;
-      if (request.hasSplitPoint()) {
-        splitPoint = request.getSplitPoint().toByteArray();
-      }
-      region.forceSplit(splitPoint);
-      compactSplitThread.requestSplit(region, region.checkSplit());
-      return SplitRegionResponse.newBuilder().build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Merge regions on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @return merge regions response
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority = HConstants.HIGH_QOS)
-  public MergeRegionsResponse mergeRegions(final RpcController controller,
-      final MergeRegionsRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion regionA = getRegion(request.getRegionA());
-      HRegion regionB = getRegion(request.getRegionB());
-      boolean forcible = request.getForcible();
-      regionA.startRegionOperation(Operation.MERGE_REGION);
-      regionB.startRegionOperation(Operation.MERGE_REGION);
-      LOG.info("Receiving merging request for  " + regionA + ", " + regionB
-          + ",forcible=" + forcible);
-      regionA.flushcache();
-      regionB.flushcache();
-      compactSplitThread.requestRegionsMerge(regionA, regionB, forcible);
-      return MergeRegionsResponse.newBuilder().build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Compact a region on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public CompactRegionResponse compactRegion(final RpcController controller,
-      final CompactRegionRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      region.startRegionOperation(Operation.COMPACT_REGION);
-      LOG.info("Compacting " + region.getRegionNameAsString());
-      boolean major = false;
-      byte [] family = null;
-      Store store = null;
-      if (request.hasFamily()) {
-        family = request.getFamily().toByteArray();
-        store = region.getStore(family);
-        if (store == null) {
-          throw new ServiceException(new IOException("column family " + Bytes.toString(family) +
-            " does not exist in region " + region.getRegionNameAsString()));
-        }
-      }
-      if (request.hasMajor()) {
-        major = request.getMajor();
-      }
-      if (major) {
-        if (family != null) {
-          store.triggerMajorCompaction();
-        } else {
-          region.triggerMajorCompaction();
-        }
-      }
-
-      String familyLogMsg = (family != null)?" for column family: " + Bytes.toString(family):"";
-      LOG.trace("User-triggered compaction requested for region " +
-        region.getRegionNameAsString() + familyLogMsg);
-      String log = "User-triggered " + (major ? "major " : "") + "compaction" + familyLogMsg;
-      if(family != null) {
-        compactSplitThread.requestCompaction(region, store, log,
-          Store.PRIORITY_USER, null);
-      } else {
-        compactSplitThread.requestCompaction(region, log,
-          Store.PRIORITY_USER, null);
-      }
-      return CompactRegionResponse.newBuilder().build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Replicate WAL entries on the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.REPLICATION_QOS)
-  public ReplicateWALEntryResponse replicateWALEntry(final RpcController controller,
-      final ReplicateWALEntryRequest request)
-  throws ServiceException {
-    try {
-      if (replicationSinkHandler != null) {
-        checkOpen();
-        requestCount.increment();
-        this.replicationSinkHandler.replicateLogEntries(request.getEntryList(),
-          ((PayloadCarryingRpcController)controller).cellScanner());
-      }
-      return ReplicateWALEntryResponse.newBuilder().build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Replay the given changes when distributedLogReplay WAL edits from a failed RS. The guarantee is
-   * that the given mutations will be durable on the receiving RS if this method returns without any
-   * exception.
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority = HConstants.REPLAY_QOS)
-  public ReplicateWALEntryResponse replay(final RpcController controller,
-      final ReplicateWALEntryRequest request) throws ServiceException {
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    CellScanner cells = ((PayloadCarryingRpcController) controller).cellScanner();
-    try {
-      checkOpen();
-      List<WALEntry> entries = request.getEntryList();
-      if (entries == null || entries.isEmpty()) {
-        // empty input
-        return ReplicateWALEntryResponse.newBuilder().build();
-      }
-      HRegion region = this.getRegionByEncodedName(
-        entries.get(0).getKey().getEncodedRegionName().toStringUtf8());
-      RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
-      List<Pair<HLogKey, WALEdit>> walEntries = new ArrayList<Pair<HLogKey, WALEdit>>();
-      List<HLogSplitter.MutationReplay> mutations = new ArrayList<HLogSplitter.MutationReplay>();
-      // when tag is enabled, we need tag replay edits with log sequence number
-      boolean needAddReplayTag = (HFile.getFormatVersion(this.conf) >= 3);
-      for (WALEntry entry : entries) {
-        if (nonceManager != null) {
-          long nonceGroup = entry.getKey().hasNonceGroup()
-              ? entry.getKey().getNonceGroup() : HConstants.NO_NONCE;
-          long nonce = entry.getKey().hasNonce() ? entry.getKey().getNonce() : HConstants.NO_NONCE;
-          nonceManager.reportOperationFromWal(nonceGroup, nonce, entry.getKey().getWriteTime());
-        }
-        Pair<HLogKey, WALEdit> walEntry = (coprocessorHost == null) ? null :
-          new Pair<HLogKey, WALEdit>();
-        List<HLogSplitter.MutationReplay> edits = HLogSplitter.getMutationsFromWALEntry(entry,
-          cells, walEntry, needAddReplayTag);
-        if (coprocessorHost != null) {
-          // Start coprocessor replay here. The coprocessor is for each WALEdit instead of a
-          // KeyValue.
-          if (coprocessorHost.preWALRestore(region.getRegionInfo(), walEntry.getFirst(),
-            walEntry.getSecond())) {
-            // if bypass this log entry, ignore it ...
-            continue;
-          }
-          walEntries.add(walEntry);
-        }
-        mutations.addAll(edits);
-      }
-
-      if (!mutations.isEmpty()) {
-        OperationStatus[] result = doReplayBatchOp(region, mutations);
-        // check if it's a partial success
-        for (int i = 0; result != null && i < result.length; i++) {
-          if (result[i] != OperationStatus.SUCCESS) {
-            throw new IOException(result[i].getExceptionMsg());
-          }
-        }
-      }
-      if (coprocessorHost != null) {
-        for (Pair<HLogKey, WALEdit> wal : walEntries) {
-          coprocessorHost.postWALRestore(region.getRegionInfo(), wal.getFirst(),
-            wal.getSecond());
-        }
-      }
-      return ReplicateWALEntryResponse.newBuilder().build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    } finally {
-      metricsRegionServer.updateReplay(EnvironmentEdgeManager.currentTimeMillis() - before);
-    }
-  }
-
-  /**
-   * Roll the WAL writer of the region server.
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  public RollWALWriterResponse rollWALWriter(final RpcController controller,
-      final RollWALWriterRequest request) throws ServiceException {
-    try {
-      checkOpen();
-      requestCount.increment();
-      HLog wal = this.getWAL();
-      byte[][] regionsToFlush = wal.rollWriter(true);
-      RollWALWriterResponse.Builder builder = RollWALWriterResponse.newBuilder();
-      if (regionsToFlush != null) {
-        for (byte[] region: regionsToFlush) {
-          builder.addRegionToFlush(HBaseZeroCopyByteString.wrap(region));
-        }
-      }
-      return builder.build();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Stop the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  public StopServerResponse stopServer(final RpcController controller,
-      final StopServerRequest request) throws ServiceException {
-    requestCount.increment();
-    String reason = request.getReason();
-    stop(reason);
-    return StopServerResponse.newBuilder().build();
-  }
-
-  /**
-   * Get some information of the region server.
-   *
-   * @param controller the RPC controller
-   * @param request the request
-   * @throws ServiceException
-   */
-  @Override
-  public GetServerInfoResponse getServerInfo(final RpcController controller,
-      final GetServerInfoRequest request) throws ServiceException {
-    try {
-      checkOpen();
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-    ServerName serverName = getServerName();
-    requestCount.increment();
-    return ResponseConverter.buildGetServerInfoResponse(serverName, rsInfo.getInfoPort());
-  }
-
-// End Admin methods
-
-  /**
-   * Find the HRegion based on a region specifier
-   *
-   * @param regionSpecifier the region specifier
-   * @return the corresponding region
-   * @throws IOException if the specifier is not null,
-   *    but failed to find the region
-   */
-  protected HRegion getRegion(
-      final RegionSpecifier regionSpecifier) throws IOException {
-    return getRegionByEncodedName(regionSpecifier.getValue().toByteArray(),
-        ProtobufUtil.getRegionEncodedName(regionSpecifier));
-  }
-
-  /**
-   * Execute an append mutation.
-   *
-   * @param region
-   * @param m
-   * @param cellScanner
-   * @return result to return to client if default operation should be
-   * bypassed as indicated by RegionObserver, null otherwise
-   * @throws IOException
-   */
-  protected Result append(final HRegion region,
-      final MutationProto m, final CellScanner cellScanner, long nonceGroup) throws IOException {
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    Append append = ProtobufUtil.toAppend(m, cellScanner);
-    Result r = null;
-    if (region.getCoprocessorHost() != null) {
-      r = region.getCoprocessorHost().preAppend(append);
-    }
-    if (r == null) {
-      long nonce = startNonceOperation(m, nonceGroup);
-      boolean success = false;
-      try {
-        r = region.append(append, nonceGroup, nonce);
-        success = true;
-      } finally {
-        endNonceOperation(m, nonceGroup, success);
-      }
-      if (region.getCoprocessorHost() != null) {
-        region.getCoprocessorHost().postAppend(append, r);
-      }
-    }
-    metricsRegionServer.updateAppend(EnvironmentEdgeManager.currentTimeMillis() - before);
-    return r;
-  }
-
-  /**
-   * Execute an increment mutation.
-   *
-   * @param region
-   * @param mutation
-   * @return the Result
-   * @throws IOException
-   */
-  protected Result increment(final HRegion region, final MutationProto mutation,
-      final CellScanner cells, long nonceGroup) throws IOException {
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    Increment increment = ProtobufUtil.toIncrement(mutation, cells);
-    Result r = null;
-    if (region.getCoprocessorHost() != null) {
-      r = region.getCoprocessorHost().preIncrement(increment);
-    }
-    if (r == null) {
-      long nonce = startNonceOperation(mutation, nonceGroup);
-      boolean success = false;
-      try {
-        r = region.increment(increment, nonceGroup, nonce);
-        success = true;
-      } finally {
-        endNonceOperation(mutation, nonceGroup, success);
-      }
-      if (region.getCoprocessorHost() != null) {
-        r = region.getCoprocessorHost().postIncrement(increment, r);
-      }
-    }
-    metricsRegionServer.updateIncrement(EnvironmentEdgeManager.currentTimeMillis() - before);
-    return r;
-  }
-
-  /**
-   * Starts the nonce operation for a mutation, if needed.
-   * @param mutation Mutation.
-   * @param nonceGroup Nonce group from the request.
-   * @returns Nonce used (can be NO_NONCE).
-   */
-  private long startNonceOperation(final MutationProto mutation, long nonceGroup)
-      throws IOException, OperationConflictException {
-    if (nonceManager == null || !mutation.hasNonce()) return HConstants.NO_NONCE;
-    boolean canProceed = false;
-    try {
-      canProceed = nonceManager.startOperation(nonceGroup, mutation.getNonce(), this);
-    } catch (InterruptedException ex) {
-      throw new InterruptedIOException("Nonce start operation interrupted");
-    }
-    if (!canProceed) {
-      // TODO: instead, we could convert append/increment to get w/mvcc
-      String message = "The operation with nonce {" + nonceGroup + ", " + mutation.getNonce()
-          + "} on row [" + Bytes.toString(mutation.getRow().toByteArray())
-          + "] may have already completed";
-      throw new OperationConflictException(message);
-    }
-    return mutation.getNonce();
-  }
-
-  /**
-   * Ends nonce operation for a mutation, if needed.
-   * @param mutation Mutation.
-   * @param nonceGroup Nonce group from the request. Always 0 in initial implementation.
-   * @param success Whether the operation for this nonce has succeeded.
-   */
-  private void endNonceOperation(final MutationProto mutation, long nonceGroup,
-      boolean success) {
-    if (nonceManager == null || !mutation.hasNonce()) return;
-    nonceManager.endOperation(nonceGroup, mutation.getNonce(), success);
-  }
-
-  @Override
-  public ServerNonceManager getNonceManager() {
-    return this.nonceManager;
-  }
-
-  /**
-   * Execute a list of Put/Delete mutations.
-   *
-   * @param builder
-   * @param region
-   * @param mutations
-   */
-  protected void doBatchOp(final RegionActionResult.Builder builder, final HRegion region,
-      final List<ClientProtos.Action> mutations, final CellScanner cells) {
-    Mutation[] mArray = new Mutation[mutations.size()];
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    boolean batchContainsPuts = false, batchContainsDelete = false;
-    try {
-      int i = 0;
-      for (ClientProtos.Action action: mutations) {
-        MutationProto m = action.getMutation();
-        Mutation mutation;
-        if (m.getMutateType() == MutationType.PUT) {
-          mutation = ProtobufUtil.toPut(m, cells);
-          batchContainsPuts = true;
-        } else {
-          mutation = ProtobufUtil.toDelete(m, cells);
-          batchContainsDelete = true;
-        }
-        mArray[i++] = mutation;
-      }
-
-      requestCount.add(mutations.size());
-      if (!region.getRegionInfo().isMetaTable()) {
-        cacheFlusher.reclaimMemStoreMemory();
-      }
-
-      OperationStatus codes[] = region.batchMutate(mArray);
-      for (i = 0; i < codes.length; i++) {
-        int index = mutations.get(i).getIndex();
-        Exception e = null;
-        switch (codes[i].getOperationStatusCode()) {
-          case BAD_FAMILY:
-            e = new NoSuchColumnFamilyException(codes[i].getExceptionMsg());
-            builder.addResultOrException(getResultOrException(e, index));
-            break;
-
-          case SANITY_CHECK_FAILURE:
-            e = new FailedSanityCheckException(codes[i].getExceptionMsg());
-            builder.addResultOrException(getResultOrException(e, index));
-            break;
-
-          default:
-            e = new DoNotRetryIOException(codes[i].getExceptionMsg());
-            builder.addResultOrException(getResultOrException(e, index));
-            break;
-
-          case SUCCESS:
-            builder.addResultOrException(getResultOrException(ClientProtos.Result.getDefaultInstance(), index));
-            break;
-        }
-      }
-    } catch (IOException ie) {
-      for (int i = 0; i < mutations.size(); i++) {
-        builder.addResultOrException(getResultOrException(ie, mutations.get(i).getIndex()));
-      }
-    }
-    long after = EnvironmentEdgeManager.currentTimeMillis();
-    if (batchContainsPuts) {
-      metricsRegionServer.updatePut(after - before);
-    }
-    if (batchContainsDelete) {
-      metricsRegionServer.updateDelete(after - before);
-    }
-  }
-  private static ResultOrException getResultOrException(final ClientProtos.Result r,
-      final int index) {
-    return getResultOrException(ResponseConverter.buildActionResult(r), index);
-  }
-  private static ResultOrException getResultOrException(final Exception e, final int index) {
-    return getResultOrException(ResponseConverter.buildActionResult(e), index);
-  }
-
-  private static ResultOrException getResultOrException(final ResultOrException.Builder builder,
-      final int index) {
-    return builder.setIndex(index).build();
-  }
-
-  /**
-   * Execute a list of Put/Delete mutations. The function returns OperationStatus instead of
-   * constructing MultiResponse to save a possible loop if caller doesn't need MultiResponse.
-   * @param region
-   * @param mutations
-   * @return an array of OperationStatus which internally contains the OperationStatusCode and the
-   *         exceptionMessage if any
-   * @throws IOException
-   */
-  protected OperationStatus [] doReplayBatchOp(final HRegion region,
-      final List<HLogSplitter.MutationReplay> mutations) throws IOException {
-    HLogSplitter.MutationReplay[] mArray = new HLogSplitter.MutationReplay[mutations.size()];
-
-    long before = EnvironmentEdgeManager.currentTimeMillis();
-    boolean batchContainsPuts = false, batchContainsDelete = false;
-    try {
-      int i = 0;
-      for (HLogSplitter.MutationReplay m : mutations) {
-        if (m.type == MutationType.PUT) {
-          batchContainsPuts = true;
-        } else {
-          batchContainsDelete = true;
-        }
-        mArray[i++] = m;
-      }
-      requestCount.add(mutations.size());
-      if (!region.getRegionInfo().isMetaTable()) {
-        cacheFlusher.reclaimMemStoreMemory();
-      }
-      return region.batchReplay(mArray);
-    } finally {
-      long after = EnvironmentEdgeManager.currentTimeMillis();
-      if (batchContainsPuts) {
-        metricsRegionServer.updatePut(after - before);
-      }
-      if (batchContainsDelete) {
-        metricsRegionServer.updateDelete(after - before);
-      }
-    }
-  }
-
-  /**
-   * Mutate a list of rows atomically.
-   *
-   * @param region
-   * @param actions
- * @param cellScanner if non-null, the mutation data -- the Cell content.
-   * @throws IOException
-   */
-  protected void mutateRows(final HRegion region, final List<ClientProtos.Action> actions,
-      final CellScanner cellScanner)
-  throws IOException {
-    if (!region.getRegionInfo().isMetaTable()) {
-      cacheFlusher.reclaimMemStoreMemory();
-    }
-    RowMutations rm = null;
-    for (ClientProtos.Action action: actions) {
-      if (action.hasGet()) {
-        throw new DoNotRetryIOException("Atomic put and/or delete only, not a Get=" +
-          action.getGet());
-      }
-      MutationType type = action.getMutation().getMutateType();
-      if (rm == null) {
-        rm = new RowMutations(action.getMutation().getRow().toByteArray());
-      }
-      switch (type) {
-      case PUT:
-        rm.add(ProtobufUtil.toPut(action.getMutation(), cellScanner));
-        break;
-      case DELETE:
-        rm.add(ProtobufUtil.toDelete(action.getMutation(), cellScanner));
-        break;
-      default:
-          throw new DoNotRetryIOException("Atomic put and/or delete only, not " + type.name());
-      }
-    }
-    region.mutateRow(rm);
-  }
-
-  private static class MovedRegionInfo {
-    private final ServerName serverName;
-    private final long seqNum;
-    private final long ts;
-
-    public MovedRegionInfo(ServerName serverName, long closeSeqNum) {
-      this.serverName = serverName;
-      this.seqNum = closeSeqNum;
-      ts = EnvironmentEdgeManager.currentTimeMillis();
-     }
-
-    public ServerName getServerName() {
-      return serverName;
-    }
-
-    public long getSeqNum() {
-      return seqNum;
-    }
-
-    public long getMoveTime() {
-      return ts;
-    }
-  }
-
-  // This map will contains all the regions that we closed for a move.
-  //  We add the time it was moved as we don't want to keep too old information
-  protected Map<String, MovedRegionInfo> movedRegions =
-      new ConcurrentHashMap<String, MovedRegionInfo>(3000);
-
-  // We need a timeout. If not there is a risk of giving a wrong information: this would double
-  //  the number of network calls instead of reducing them.
-  private static final int TIMEOUT_REGION_MOVED = (2 * 60 * 1000);
-
-  protected void addToMovedRegions(String encodedName, ServerName destination, long closeSeqNum) {
-    if (ServerName.isSameHostnameAndPort(destination, this.getServerName())) {
-      LOG.warn("Not adding moved region record: " + encodedName + " to self.");
-      return;
-    }
-    LOG.info("Adding moved region record: " + encodedName + " to "
-        + destination.getServerName() + ":" + destination.getPort()
-        + " as of " + closeSeqNum);
-    movedRegions.put(encodedName, new MovedRegionInfo(destination, closeSeqNum));
-  }
-
-  private void removeFromMovedRegions(String encodedName) {
-    movedRegions.remove(encodedName);
-  }
+  void removeFromMovedRegions(String encodedName) {
+    movedRegions.remove(encodedName);
+  }
 
   private MovedRegionInfo getMovedRegion(final String encodedRegionName) {
     MovedRegionInfo dest = movedRegions.get(encodedRegionName);
@@ -4533,20 +2656,6 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     return ZKUtil.joinZNode(this.zooKeeper.rsZNode, getServerName().toString());
   }
 
-  /**
-   * Holder class which holds the RegionScanner and nextCallSeq together.
-   */
-  private static class RegionScannerHolder {
-    private RegionScanner s;
-    private long nextCallSeq = 0L;
-    private HRegion r;
-
-    public RegionScannerHolder(RegionScanner s, HRegion r) {
-      this.s = s;
-      this.r = r;
-    }
-  }
-
   private boolean isHealthCheckerConfigured() {
     String healthScriptLocation = this.conf.get(HConstants.HEALTH_SCRIPT_LOC);
     return org.apache.commons.lang.StringUtils.isNotBlank(healthScriptLocation);
@@ -4636,18 +2745,4 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
     }
     return result;
   }
-
-  @Override
-  public UpdateFavoredNodesResponse updateFavoredNodes(RpcController controller,
-      UpdateFavoredNodesRequest request) throws ServiceException {
-    List<UpdateFavoredNodesRequest.RegionUpdateInfo> openInfoList = request.getUpdateInfoList();
-    UpdateFavoredNodesResponse.Builder respBuilder = UpdateFavoredNodesResponse.newBuilder();
-    for (UpdateFavoredNodesRequest.RegionUpdateInfo regionUpdateInfo : openInfoList) {
-      HRegionInfo hri = HRegionInfo.convert(regionUpdateInfo.getRegion());
-      updateRegionFavoredNodesMapping(hri.getEncodedName(),
-          regionUpdateInfo.getFavoredNodesList());
-    }
-    respBuilder.setResponse(openInfoList.size());
-    return respBuilder.build();
-  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java
index c51f453..8f08a31 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.java
@@ -59,9 +59,8 @@ public class HRegionServerCommandLine extends ServerCommandLine {
       } else {
         logProcessInfo(getConf());
         HRegionServer hrs = HRegionServer.constructRegionServer(regionServerClass, conf);
-        Thread rsThread = HRegionServer.startRegionServer(hrs);
-
-        rsThread.join();
+        hrs.start();
+        hrs.join();
         if (hrs.isAborted()) {
           throw new RuntimeException("HRegionServer Aborted");
         }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
index 5c7a041..c8d3b6a 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java
@@ -148,19 +148,6 @@ class LogRoller extends HasThread implements WALActionsListener {
     }
   }
 
-  /**
-   * Called by region server to wake up this thread if it sleeping.
-   * It is sleeping if rollLock is not held.
-   */
-  public void interruptIfNecessary() {
-    try {
-      rollLock.lock();
-      this.interrupt();
-    } finally {
-      rollLock.unlock();
-    }
-  }
-
   protected HLog getWAL() throws IOException {
     return this.services.getWAL(null);
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
index 9f88e70..55a5d80 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.java
@@ -118,7 +118,7 @@ class MetricsRegionServerWrapperImpl
 
   @Override
   public String getZookeeperQuorum() {
-    ZooKeeperWatcher zk = regionServer.getZooKeeperWatcher();
+    ZooKeeperWatcher zk = regionServer.getZooKeeper();
     if (zk == null) {
       return "";
     }
@@ -127,7 +127,7 @@ class MetricsRegionServerWrapperImpl
 
   @Override
   public String getCoprocessors() {
-    String[] coprocessors = regionServer.getCoprocessors();
+    String[] coprocessors = regionServer.getRegionServerCoprocessors();
     if (coprocessors == null || coprocessors.length == 0) {
       return "";
     }
@@ -154,7 +154,7 @@ class MetricsRegionServerWrapperImpl
 
   @Override
   public long getTotalRequestCount() {
-    return regionServer.requestCount.get();
+    return regionServer.rpcServices.requestCount.get();
   }
 
   @Override
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
index 8f8bd6f..5aa040d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSDumpServlet.java
@@ -45,10 +45,6 @@ public class RSDumpServlet extends StateDumpServlet {
     HRegionServer hrs = (HRegionServer)getServletContext().getAttribute(
         HRegionServer.REGIONSERVER);
     assert hrs != null : "No RS in context!";
-    
-    Configuration hrsconf = (Configuration)getServletContext().getAttribute(
-        HRegionServer.REGIONSERVER_CONF);
-    assert hrsconf != null : "No RS conf in context";
 
     response.setContentType("text/plain");
  
@@ -61,7 +57,7 @@ public class RSDumpServlet extends StateDumpServlet {
     OutputStream os = response.getOutputStream();
     PrintWriter out = new PrintWriter(os);
     
-    out.println("Master status for " + hrs.getServerName()
+    out.println("RegionServer status for " + hrs.getServerName()
         + " as of " + new Date());
     
     out.println("\n\nVersion Info:");
@@ -94,18 +90,14 @@ public class RSDumpServlet extends StateDumpServlet {
     
     out.println("\n\nRS Queue:");
     out.println(LINE);
-    if(isShowQueueDump(hrsconf)) {
+    if(isShowQueueDump(conf)) {
       dumpQueue(hrs, out);
     } 
     
     out.flush();
   }
-  
-  private boolean isShowQueueDump(Configuration conf){
-    return conf.getBoolean("hbase.regionserver.servlet.show.queuedump", true);
-  }
-    
-  private void dumpQueue(HRegionServer hrs, PrintWriter out)
+
+  public static void dumpQueue(HRegionServer hrs, PrintWriter out)
       throws IOException {
     // 1. Print out Compaction/Split Queue
     out.println("Compaction/Split Queue summary: " 
@@ -117,5 +109,4 @@ public class RSDumpServlet extends StateDumpServlet {
         + hrs.cacheFlusher.toString());
     out.println(hrs.cacheFlusher.dumpQueue());
   }
-  
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
new file mode 100644
index 0000000..54563d5
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -0,0 +1,1985 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.lang.annotation.Retention;
+import java.lang.annotation.RetentionPolicy;
+import java.net.InetSocketAddress;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellScannable;
+import org.apache.hadoop.hbase.CellScanner;
+import org.apache.hadoop.hbase.CellUtil;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.DroppedSnapshotException;
+import org.apache.hadoop.hbase.HBaseIOException;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.NotServingRegionException;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.UnknownScannerException;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.client.Append;
+import org.apache.hadoop.hbase.client.ConnectionUtils;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.Increment;
+import org.apache.hadoop.hbase.client.Mutation;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.RowMutations;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.exceptions.FailedSanityCheckException;
+import org.apache.hadoop.hbase.exceptions.OperationConflictException;
+import org.apache.hadoop.hbase.exceptions.OutOfOrderScannerNextException;
+import org.apache.hadoop.hbase.filter.ByteArrayComparable;
+import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
+import org.apache.hadoop.hbase.io.hfile.HFile;
+import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
+import org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController;
+import org.apache.hadoop.hbase.ipc.PriorityFunction;
+import org.apache.hadoop.hbase.ipc.RpcCallContext;
+import org.apache.hadoop.hbase.ipc.RpcServer;
+import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;
+import org.apache.hadoop.hbase.ipc.RpcServerInterface;
+import org.apache.hadoop.hbase.ipc.ServerNotRunningYetException;
+import org.apache.hadoop.hbase.ipc.ServerRpcController;
+import org.apache.hadoop.hbase.master.SplitLogManager;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.RequestConverter;
+import org.apache.hadoop.hbase.protobuf.ResponseConverter;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.AdminService;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CompactRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CompactRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.FlushRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.FlushRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetOnlineRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetOnlineRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetRegionInfoResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetServerInfoResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetStoreFileRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.GetStoreFileResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.MergeRegionsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest.RegionOpenInfo;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse.RegionOpeningState;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.ReplicateWALEntryResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.RollWALWriterResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.SplitRegionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.SplitRegionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.StopServerResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UpdateFavoredNodesRequest;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.UpdateFavoredNodesResponse;
+import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.WALEntry;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Condition;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.MutationType;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.RegionAction;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.RegionActionResult;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ResultOrException;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
+import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RequestHeader;
+import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
+import org.apache.hadoop.hbase.regionserver.Leases.LeaseStillHeldException;
+import org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler;
+import org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler;
+import org.apache.hadoop.hbase.regionserver.wal.HLog;
+import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
+import org.apache.hadoop.hbase.regionserver.wal.HLogSplitter;
+import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Counter;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Strings;
+import org.apache.hadoop.net.DNS;
+import org.apache.zookeeper.KeeperException;
+
+import com.google.protobuf.ByteString;
+import com.google.protobuf.HBaseZeroCopyByteString;
+import com.google.protobuf.Message;
+import com.google.protobuf.RpcController;
+import com.google.protobuf.ServiceException;
+import com.google.protobuf.TextFormat;
+
+/**
+ * Implements the regionserver RPC services.
+ */
+@InterfaceAudience.Private
+public class RSRpcServices implements HBaseRPCErrorHandler,
+    AdminService.BlockingInterface, ClientService.BlockingInterface, PriorityFunction {
+  protected static final Log LOG = LogFactory.getLog(RSRpcServices.class);
+
+  /** RPC scheduler to use for the region server. */
+  public static final String REGION_SERVER_RPC_SCHEDULER_FACTORY_CLASS =
+    "hbase.region.server.rpc.scheduler.factory.class";
+
+  // Request counter. (Includes requests that are not serviced by regions.)
+  final Counter requestCount = new Counter();
+  // Server to handle client requests.
+  final RpcServerInterface rpcServer;
+  final InetSocketAddress isa;
+
+  private final HRegionServer regionServer;
+  private final long maxScannerResultSize;
+
+  // The reference to the priority extraction function
+  private final PriorityFunction priority;
+
+  private final AtomicLong scannerIdGen = new AtomicLong(0L);
+  private final ConcurrentHashMap<String, RegionScannerHolder> scanners =
+    new ConcurrentHashMap<String, RegionScannerHolder>();
+
+  /**
+   * The lease timeout period for client scanners (milliseconds).
+   */
+  private final int scannerLeaseTimeoutPeriod;
+
+  /**
+   * Holder class which holds the RegionScanner and nextCallSeq together.
+   */
+  private static class RegionScannerHolder {
+    private RegionScanner s;
+    private long nextCallSeq = 0L;
+    private HRegion r;
+
+    public RegionScannerHolder(RegionScanner s, HRegion r) {
+      this.s = s;
+      this.r = r;
+    }
+  }
+
+  /**
+   * Instantiated as a scanner lease. If the lease times out, the scanner is
+   * closed
+   */
+  private class ScannerListener implements LeaseListener {
+    private final String scannerName;
+
+    ScannerListener(final String n) {
+      this.scannerName = n;
+    }
+
+    @Override
+    public void leaseExpired() {
+      RegionScannerHolder rsh = scanners.remove(this.scannerName);
+      if (rsh != null) {
+        RegionScanner s = rsh.s;
+        LOG.info("Scanner " + this.scannerName + " lease expired on region "
+          + s.getRegionInfo().getRegionNameAsString());
+        try {
+          HRegion region = regionServer.getRegion(s.getRegionInfo().getRegionName());
+          if (region != null && region.getCoprocessorHost() != null) {
+            region.getCoprocessorHost().preScannerClose(s);
+          }
+
+          s.close();
+          if (region != null && region.getCoprocessorHost() != null) {
+            region.getCoprocessorHost().postScannerClose(s);
+          }
+        } catch (IOException e) {
+          LOG.error("Closing scanner for "
+            + s.getRegionInfo().getRegionNameAsString(), e);
+        }
+      } else {
+        LOG.warn("Scanner " + this.scannerName + " lease expired, but no related" +
+          " scanner found, hence no chance to close that related scanner!");
+      }
+    }
+  }
+
+  private static ResultOrException getResultOrException(
+      final ClientProtos.Result r, final int index) {
+    return getResultOrException(ResponseConverter.buildActionResult(r), index);
+  }
+
+  private static ResultOrException getResultOrException(final Exception e, final int index) {
+    return getResultOrException(ResponseConverter.buildActionResult(e), index);
+  }
+
+  private static ResultOrException getResultOrException(
+      final ResultOrException.Builder builder, final int index) {
+    return builder.setIndex(index).build();
+  }
+
+  /**
+   * Starts the nonce operation for a mutation, if needed.
+   * @param mutation Mutation.
+   * @param nonceGroup Nonce group from the request.
+   * @returns Nonce used (can be NO_NONCE).
+   */
+  private long startNonceOperation(final MutationProto mutation, long nonceGroup)
+      throws IOException, OperationConflictException {
+    if (regionServer.nonceManager == null || !mutation.hasNonce()) return HConstants.NO_NONCE;
+    boolean canProceed = false;
+    try {
+      canProceed = regionServer.nonceManager.startOperation(
+        nonceGroup, mutation.getNonce(), regionServer);
+    } catch (InterruptedException ex) {
+      throw new InterruptedIOException("Nonce start operation interrupted");
+    }
+    if (!canProceed) {
+      // TODO: instead, we could convert append/increment to get w/mvcc
+      String message = "The operation with nonce {" + nonceGroup + ", " + mutation.getNonce()
+        + "} on row [" + Bytes.toString(mutation.getRow().toByteArray())
+        + "] may have already completed";
+      throw new OperationConflictException(message);
+    }
+    return mutation.getNonce();
+  }
+
+  /**
+   * Ends nonce operation for a mutation, if needed.
+   * @param mutation Mutation.
+   * @param nonceGroup Nonce group from the request. Always 0 in initial implementation.
+   * @param success Whether the operation for this nonce has succeeded.
+   */
+  private void endNonceOperation(final MutationProto mutation,
+      long nonceGroup, boolean success) {
+    if (regionServer.nonceManager != null && mutation.hasNonce()) {
+      regionServer.nonceManager.endOperation(nonceGroup, mutation.getNonce(), success);
+    }
+  }
+
+  /**
+   * @return True if current call supports cellblocks
+   */
+  private boolean isClientCellBlockSupport() {
+    RpcCallContext context = RpcServer.getCurrentCall();
+    return context != null && context.isClientCellBlockSupport();
+  }
+
+  private void addResult(final MutateResponse.Builder builder,
+      final Result result, final PayloadCarryingRpcController rpcc) {
+    if (result == null) return;
+    if (isClientCellBlockSupport()) {
+      builder.setResult(ProtobufUtil.toResultNoData(result));
+      rpcc.setCellScanner(result.cellScanner());
+    } else {
+      ClientProtos.Result pbr = ProtobufUtil.toResult(result);
+      builder.setResult(pbr);
+    }
+  }
+
+  private void addResults(final ScanResponse.Builder builder, final List<Result> results,
+      final RpcController controller) {
+    if (results == null || results.isEmpty()) return;
+    if (isClientCellBlockSupport()) {
+      for (Result res : results) {
+        builder.addCellsPerResult(res.size());
+      }
+      ((PayloadCarryingRpcController)controller).
+        setCellScanner(CellUtil.createCellScanner(results));
+    } else {
+      for (Result res: results) {
+        ClientProtos.Result pbr = ProtobufUtil.toResult(res);
+        builder.addResults(pbr);
+      }
+    }
+  }
+
+  /**
+   * Mutate a list of rows atomically.
+   *
+   * @param region
+   * @param actions
+   * @param cellScanner if non-null, the mutation data -- the Cell content.
+   * @throws IOException
+   */
+  private void mutateRows(final HRegion region, final List<ClientProtos.Action> actions,
+      final CellScanner cellScanner) throws IOException {
+    if (!region.getRegionInfo().isMetaTable()) {
+      regionServer.cacheFlusher.reclaimMemStoreMemory();
+    }
+    RowMutations rm = null;
+    for (ClientProtos.Action action: actions) {
+      if (action.hasGet()) {
+        throw new DoNotRetryIOException("Atomic put and/or delete only, not a Get=" +
+          action.getGet());
+      }
+      MutationType type = action.getMutation().getMutateType();
+      if (rm == null) {
+        rm = new RowMutations(action.getMutation().getRow().toByteArray());
+      }
+      switch (type) {
+      case PUT:
+        rm.add(ProtobufUtil.toPut(action.getMutation(), cellScanner));
+        break;
+      case DELETE:
+        rm.add(ProtobufUtil.toDelete(action.getMutation(), cellScanner));
+        break;
+      default:
+          throw new DoNotRetryIOException("Atomic put and/or delete only, not " + type.name());
+      }
+    }
+    region.mutateRow(rm);
+  }
+
+  /**
+   * Execute an append mutation.
+   *
+   * @param region
+   * @param m
+   * @param cellScanner
+   * @return result to return to client if default operation should be
+   * bypassed as indicated by RegionObserver, null otherwise
+   * @throws IOException
+   */
+  private Result append(final HRegion region, final MutationProto m,
+      final CellScanner cellScanner, long nonceGroup) throws IOException {
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    Append append = ProtobufUtil.toAppend(m, cellScanner);
+    Result r = null;
+    if (region.getCoprocessorHost() != null) {
+      r = region.getCoprocessorHost().preAppend(append);
+    }
+    if (r == null) {
+      long nonce = startNonceOperation(m, nonceGroup);
+      boolean success = false;
+      try {
+        r = region.append(append, nonceGroup, nonce);
+        success = true;
+      } finally {
+        endNonceOperation(m, nonceGroup, success);
+      }
+      if (region.getCoprocessorHost() != null) {
+        region.getCoprocessorHost().postAppend(append, r);
+      }
+    }
+    regionServer.metricsRegionServer.updateAppend(
+      EnvironmentEdgeManager.currentTimeMillis() - before);
+    return r;
+  }
+
+  /**
+   * Execute an increment mutation.
+   *
+   * @param region
+   * @param mutation
+   * @return the Result
+   * @throws IOException
+   */
+  private Result increment(final HRegion region, final MutationProto mutation,
+      final CellScanner cells, long nonceGroup) throws IOException {
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    Increment increment = ProtobufUtil.toIncrement(mutation, cells);
+    Result r = null;
+    if (region.getCoprocessorHost() != null) {
+      r = region.getCoprocessorHost().preIncrement(increment);
+    }
+    if (r == null) {
+      long nonce = startNonceOperation(mutation, nonceGroup);
+      boolean success = false;
+      try {
+        r = region.increment(increment, nonceGroup, nonce);
+        success = true;
+      } finally {
+        endNonceOperation(mutation, nonceGroup, success);
+      }
+      if (region.getCoprocessorHost() != null) {
+        r = region.getCoprocessorHost().postIncrement(increment, r);
+      }
+    }
+    regionServer.metricsRegionServer.updateIncrement(
+      EnvironmentEdgeManager.currentTimeMillis() - before);
+    return r;
+  }
+
+  /**
+   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when
+   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.
+   * @param region
+   * @param actions
+   * @param cellScanner
+   * @param builder
+   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this
+   * method returns as a 'result'.
+   * @return Return the <code>cellScanner</code> passed
+   */
+  private List<CellScannable> doNonAtomicRegionMutation(final HRegion region,
+      final RegionAction actions, final CellScanner cellScanner,
+      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup) {
+    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do
+    // one at a time, we instead pass them in batch.  Be aware that the corresponding
+    // ResultOrException instance that matches each Put or Delete is then added down in the
+    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched
+    List<ClientProtos.Action> mutations = null;
+    for (ClientProtos.Action action: actions.getActionList()) {
+      ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = null;
+      try {
+        Result r = null;
+        if (action.hasGet()) {
+          Get get = ProtobufUtil.toGet(action.getGet());
+          r = region.get(get);
+        } else if (action.hasServiceCall()) {
+          resultOrExceptionBuilder = ResultOrException.newBuilder();
+          try {
+            Message result = execServiceOnRegion(region, action.getServiceCall());
+            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =
+                ClientProtos.CoprocessorServiceResult.newBuilder();
+            resultOrExceptionBuilder.setServiceResult(
+                serviceResultBuilder.setValue(
+                  serviceResultBuilder.getValueBuilder()
+                    .setName(result.getClass().getName())
+                    .setValue(result.toByteString())));
+          } catch (IOException ioe) {
+            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));
+          }
+        } else if (action.hasMutation()) {
+          MutationType type = action.getMutation().getMutateType();
+          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&
+              !mutations.isEmpty()) {
+            // Flush out any Puts or Deletes already collected.
+            doBatchOp(builder, region, mutations, cellScanner);
+            mutations.clear();
+          }
+          switch (type) {
+          case APPEND:
+            r = append(region, action.getMutation(), cellScanner, nonceGroup);
+            break;
+          case INCREMENT:
+            r = increment(region, action.getMutation(), cellScanner,  nonceGroup);
+            break;
+          case PUT:
+          case DELETE:
+            // Collect the individual mutations and apply in a batch
+            if (mutations == null) {
+              mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());
+            }
+            mutations.add(action);
+            break;
+          default:
+            throw new DoNotRetryIOException("Unsupported mutate type: " + type.name());
+          }
+        } else {
+          throw new HBaseIOException("Unexpected Action type");
+        }
+        if (r != null) {
+          ClientProtos.Result pbResult = null;
+          if (isClientCellBlockSupport()) {
+            pbResult = ProtobufUtil.toResultNoData(r);
+            //  Hard to guess the size here.  Just make a rough guess.
+            if (cellsToReturn == null) cellsToReturn = new ArrayList<CellScannable>();
+            cellsToReturn.add(r);
+          } else {
+            pbResult = ProtobufUtil.toResult(r);
+          }
+          resultOrExceptionBuilder =
+            ClientProtos.ResultOrException.newBuilder().setResult(pbResult);
+        }
+        // Could get to here and there was no result and no exception.  Presumes we added
+        // a Put or Delete to the collecting Mutations List for adding later.  In this
+        // case the corresponding ResultOrException instance for the Put or Delete will be added
+        // down in the doBatchOp method call rather than up here.
+      } catch (IOException ie) {
+        resultOrExceptionBuilder = ResultOrException.newBuilder().
+          setException(ResponseConverter.buildException(ie));
+      }
+      if (resultOrExceptionBuilder != null) {
+        // Propagate index.
+        resultOrExceptionBuilder.setIndex(action.getIndex());
+        builder.addResultOrException(resultOrExceptionBuilder.build());
+      }
+    }
+    // Finish up any outstanding mutations
+    if (mutations != null && !mutations.isEmpty()) {
+      doBatchOp(builder, region, mutations, cellScanner);
+    }
+    return cellsToReturn;
+  }
+
+  /**
+   * Execute a list of Put/Delete mutations.
+   *
+   * @param builder
+   * @param region
+   * @param mutations
+   */
+  private void doBatchOp(final RegionActionResult.Builder builder, final HRegion region,
+      final List<ClientProtos.Action> mutations, final CellScanner cells) {
+    Mutation[] mArray = new Mutation[mutations.size()];
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    boolean batchContainsPuts = false, batchContainsDelete = false;
+    try {
+      int i = 0;
+      for (ClientProtos.Action action: mutations) {
+        MutationProto m = action.getMutation();
+        Mutation mutation;
+        if (m.getMutateType() == MutationType.PUT) {
+          mutation = ProtobufUtil.toPut(m, cells);
+          batchContainsPuts = true;
+        } else {
+          mutation = ProtobufUtil.toDelete(m, cells);
+          batchContainsDelete = true;
+        }
+        mArray[i++] = mutation;
+      }
+
+      requestCount.add(mutations.size());
+      if (!region.getRegionInfo().isMetaTable()) {
+        regionServer.cacheFlusher.reclaimMemStoreMemory();
+      }
+
+      OperationStatus codes[] = region.batchMutate(mArray);
+      for (i = 0; i < codes.length; i++) {
+        int index = mutations.get(i).getIndex();
+        Exception e = null;
+        switch (codes[i].getOperationStatusCode()) {
+          case BAD_FAMILY:
+            e = new NoSuchColumnFamilyException(codes[i].getExceptionMsg());
+            builder.addResultOrException(getResultOrException(e, index));
+            break;
+
+          case SANITY_CHECK_FAILURE:
+            e = new FailedSanityCheckException(codes[i].getExceptionMsg());
+            builder.addResultOrException(getResultOrException(e, index));
+            break;
+
+          default:
+            e = new DoNotRetryIOException(codes[i].getExceptionMsg());
+            builder.addResultOrException(getResultOrException(e, index));
+            break;
+
+          case SUCCESS:
+            builder.addResultOrException(getResultOrException(
+              ClientProtos.Result.getDefaultInstance(), index));
+            break;
+        }
+      }
+    } catch (IOException ie) {
+      for (int i = 0; i < mutations.size(); i++) {
+        builder.addResultOrException(getResultOrException(ie, mutations.get(i).getIndex()));
+      }
+    }
+    long after = EnvironmentEdgeManager.currentTimeMillis();
+    if (batchContainsPuts) {
+      regionServer.metricsRegionServer.updatePut(after - before);
+    }
+    if (batchContainsDelete) {
+      regionServer.metricsRegionServer.updateDelete(after - before);
+    }
+  }
+
+  /**
+   * Execute a list of Put/Delete mutations. The function returns OperationStatus instead of
+   * constructing MultiResponse to save a possible loop if caller doesn't need MultiResponse.
+   * @param region
+   * @param mutations
+   * @return an array of OperationStatus which internally contains the OperationStatusCode and the
+   *         exceptionMessage if any
+   * @throws IOException
+   */
+  private OperationStatus [] doReplayBatchOp(final HRegion region,
+      final List<HLogSplitter.MutationReplay> mutations) throws IOException {
+    HLogSplitter.MutationReplay[] mArray = new HLogSplitter.MutationReplay[mutations.size()];
+
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    boolean batchContainsPuts = false, batchContainsDelete = false;
+    try {
+      int i = 0;
+      for (HLogSplitter.MutationReplay m : mutations) {
+        if (m.type == MutationType.PUT) {
+          batchContainsPuts = true;
+        } else {
+          batchContainsDelete = true;
+        }
+        mArray[i++] = m;
+      }
+      requestCount.add(mutations.size());
+      if (!region.getRegionInfo().isMetaTable()) {
+        regionServer.cacheFlusher.reclaimMemStoreMemory();
+      }
+      return region.batchReplay(mArray);
+    } finally {
+      long after = EnvironmentEdgeManager.currentTimeMillis();
+      if (batchContainsPuts) {
+        regionServer.metricsRegionServer.updatePut(after - before);
+      }
+      if (batchContainsDelete) {
+        regionServer.metricsRegionServer.updateDelete(after - before);
+      }
+    }
+  }
+
+  private void closeAllScanners() {
+    // Close any outstanding scanners. Means they'll get an UnknownScanner
+    // exception next time they come in.
+    for (Map.Entry<String, RegionScannerHolder> e : scanners.entrySet()) {
+      try {
+        e.getValue().s.close();
+      } catch (IOException ioe) {
+        LOG.warn("Closing scanner " + e.getKey(), ioe);
+      }
+    }
+  }
+
+  public RSRpcServices(HRegionServer rs) throws IOException {
+    RpcSchedulerFactory rpcSchedulerFactory;
+    try {
+      Class<?> rpcSchedulerFactoryClass = rs.conf.getClass(
+          REGION_SERVER_RPC_SCHEDULER_FACTORY_CLASS,
+          SimpleRpcSchedulerFactory.class);
+      rpcSchedulerFactory = ((RpcSchedulerFactory) rpcSchedulerFactoryClass.newInstance());
+    } catch (InstantiationException e) {
+      throw new IllegalArgumentException(e);
+    } catch (IllegalAccessException e) {
+      throw new IllegalArgumentException(e);
+    }
+    // Server to handle client requests.
+    String hostname = rs.conf.get("hbase.regionserver.ipc.address",
+      Strings.domainNamePointerToHostName(DNS.getDefaultHost(
+        rs.conf.get("hbase.regionserver.dns.interface", "default"),
+        rs.conf.get("hbase.regionserver.dns.nameserver", "default"))));
+    int port = rs.conf.getInt(HConstants.REGIONSERVER_PORT,
+      HConstants.DEFAULT_REGIONSERVER_PORT);
+    // Creation of a HSA will force a resolve.
+    InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
+    if (initialIsa.getAddress() == null) {
+      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
+    }
+    priority = new AnnotationReadingPriorityFunction(this);
+    String name = rs.getProcessName() + "/" + initialIsa.toString();
+    // Set how many times to retry talking to another server over HConnection.
+    ConnectionUtils.setServerSideHConnectionRetriesConfig(rs.conf, name, LOG);
+    rpcServer = new RpcServer(rs, name, getServices(),
+      initialIsa, // BindAddress is IP we got for this server.
+      rs.conf,
+      rpcSchedulerFactory.create(rs.conf, this));
+    rpcServer.start();
+
+    scannerLeaseTimeoutPeriod = rs.conf.getInt(
+      HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD,
+      HConstants.DEFAULT_HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD);
+    maxScannerResultSize = rs.conf.getLong(
+      HConstants.HBASE_CLIENT_SCANNER_MAX_RESULT_SIZE_KEY,
+      HConstants.DEFAULT_HBASE_CLIENT_SCANNER_MAX_RESULT_SIZE);
+
+    // Set our address.
+    isa = rpcServer.getListenerAddress();
+    rpcServer.setErrorHandler(this);
+    regionServer = rs;
+    rs.setName(name);
+  }
+
+  RegionScanner getScanner(long scannerId) {
+    String scannerIdString = Long.toString(scannerId);
+    RegionScannerHolder scannerHolder = scanners.get(scannerIdString);
+    if (scannerHolder != null) {
+      return scannerHolder.s;
+    }
+    return null;
+  }
+
+  long addScanner(RegionScanner s, HRegion r) throws LeaseStillHeldException {
+    long scannerId = this.scannerIdGen.incrementAndGet();
+    String scannerName = String.valueOf(scannerId);
+
+    RegionScannerHolder existing =
+      scanners.putIfAbsent(scannerName, new RegionScannerHolder(s, r));
+    assert existing == null : "scannerId must be unique within regionserver's whole lifecycle!";
+
+    regionServer.leases.createLease(scannerName, this.scannerLeaseTimeoutPeriod,
+        new ScannerListener(scannerName));
+    return scannerId;
+  }
+
+  /**
+   * Find the HRegion based on a region specifier
+   *
+   * @param regionSpecifier the region specifier
+   * @return the corresponding region
+   * @throws IOException if the specifier is not null,
+   *    but failed to find the region
+   */
+  HRegion getRegion(
+      final RegionSpecifier regionSpecifier) throws IOException {
+    return regionServer.getRegionByEncodedName(regionSpecifier.getValue().toByteArray(),
+        ProtobufUtil.getRegionEncodedName(regionSpecifier));
+  }
+
+  PriorityFunction getPriority() {
+    return priority;
+  }
+
+  void stop() {
+    closeAllScanners();
+    rpcServer.stop();
+  }
+
+  /**
+   * Called to verify that this server is up and running.
+   *
+   * @throws IOException
+   */
+  protected void checkOpen() throws IOException {
+    if (regionServer.isStopped() || regionServer.isAborted()) {
+      throw new RegionServerStoppedException("Server " + regionServer.serverName
+        + " not running" + (regionServer.isAborted() ? ", aborting" : ""));
+    }
+    if (!regionServer.fsOk) {
+      throw new RegionServerStoppedException("File system not available");
+    }
+    if (!regionServer.isOnline()) {
+      throw new ServerNotRunningYetException("Server is not running yet");
+    }
+  }
+
+  /**
+   * @return list of blocking services and their security info classes that this server supports
+   */
+  protected List<BlockingServiceAndInterface> getServices() {
+    List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(2);
+    bssi.add(new BlockingServiceAndInterface(
+      ClientService.newReflectiveBlockingService(this),
+      ClientService.BlockingInterface.class));
+    bssi.add(new BlockingServiceAndInterface(
+      AdminService.newReflectiveBlockingService(this),
+      AdminService.BlockingInterface.class));
+    return bssi;
+  }
+
+  @Retention(RetentionPolicy.RUNTIME)
+  protected @interface QosPriority {
+    int priority() default 0;
+  }
+
+  public InetSocketAddress getSocketAddress() {
+    return isa;
+  }
+
+  @Override
+  public int getPriority(RequestHeader header, Message param) {
+    return priority.getPriority(header, param);
+  }
+
+  /*
+   * Check if an OOME and, if so, abort immediately to avoid creating more objects.
+   *
+   * @param e
+   *
+   * @return True if we OOME'd and are aborting.
+   */
+  @Override
+  public boolean checkOOME(final Throwable e) {
+    boolean stop = false;
+    try {
+      if (e instanceof OutOfMemoryError
+          || (e.getCause() != null && e.getCause() instanceof OutOfMemoryError)
+          || (e.getMessage() != null && e.getMessage().contains(
+              "java.lang.OutOfMemoryError"))) {
+        stop = true;
+        LOG.fatal("Run out of memory; " + getClass().getSimpleName()
+          + " will abort itself immediately", e);
+      }
+    } finally {
+      if (stop) {
+        Runtime.getRuntime().halt(1);
+      }
+    }
+    return stop;
+  }
+
+  /**
+   * Close a region on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public CloseRegionResponse closeRegion(final RpcController controller,
+      final CloseRegionRequest request) throws ServiceException {
+    int versionOfClosingNode = -1;
+    if (request.hasVersionOfClosingNode()) {
+      versionOfClosingNode = request.getVersionOfClosingNode();
+    }
+    boolean zk = request.getTransitionInZK();
+    final ServerName sn = (request.hasDestinationServer() ?
+      ProtobufUtil.toServerName(request.getDestinationServer()) : null);
+
+    try {
+      checkOpen();
+      if (request.hasServerStartCode()) {
+        // check that we are the same server that this RPC is intended for.
+        long serverStartCode = request.getServerStartCode();
+        if (regionServer.serverName.getStartcode() !=  serverStartCode) {
+          throw new ServiceException(new DoNotRetryIOException("This RPC was intended for a " +
+              "different server with startCode: " + serverStartCode + ", this server is: "
+              + regionServer.serverName));
+        }
+      }
+      final String encodedRegionName = ProtobufUtil.getRegionEncodedName(request.getRegion());
+
+      // Can be null if we're calling close on a region that's not online
+      final HRegion region = regionServer.getFromOnlineRegions(encodedRegionName);
+      if ((region  != null) && (region .getCoprocessorHost() != null)) {
+        region.getCoprocessorHost().preClose(false);
+      }
+
+      requestCount.increment();
+      LOG.info("Close " + encodedRegionName + ", via zk=" + (zk ? "yes" : "no")
+        + ", znode version=" + versionOfClosingNode + ", on " + sn);
+
+      boolean closed = regionServer.closeRegion(encodedRegionName, false, zk, versionOfClosingNode, sn);
+      CloseRegionResponse.Builder builder = CloseRegionResponse.newBuilder().setClosed(closed);
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Compact a region on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public CompactRegionResponse compactRegion(final RpcController controller,
+      final CompactRegionRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      region.startRegionOperation(Operation.COMPACT_REGION);
+      LOG.info("Compacting " + region.getRegionNameAsString());
+      boolean major = false;
+      byte [] family = null;
+      Store store = null;
+      if (request.hasFamily()) {
+        family = request.getFamily().toByteArray();
+        store = region.getStore(family);
+        if (store == null) {
+          throw new ServiceException(new IOException("column family " + Bytes.toString(family)
+            + " does not exist in region " + region.getRegionNameAsString()));
+        }
+      }
+      if (request.hasMajor()) {
+        major = request.getMajor();
+      }
+      if (major) {
+        if (family != null) {
+          store.triggerMajorCompaction();
+        } else {
+          region.triggerMajorCompaction();
+        }
+      }
+
+      String familyLogMsg = (family != null)?" for column family: " + Bytes.toString(family):"";
+      if (LOG.isTraceEnabled()) {
+        LOG.trace("User-triggered compaction requested for region "
+          + region.getRegionNameAsString() + familyLogMsg);
+      }
+      String log = "User-triggered " + (major ? "major " : "") + "compaction" + familyLogMsg;
+      if(family != null) {
+        regionServer.compactSplitThread.requestCompaction(region, store, log,
+          Store.PRIORITY_USER, null);
+      } else {
+        regionServer.compactSplitThread.requestCompaction(region, log,
+          Store.PRIORITY_USER, null);
+      }
+      return CompactRegionResponse.newBuilder().build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Flush a region on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public FlushRegionResponse flushRegion(final RpcController controller,
+      final FlushRegionRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      LOG.info("Flushing " + region.getRegionNameAsString());
+      boolean shouldFlush = true;
+      if (request.hasIfOlderThanTs()) {
+        shouldFlush = region.getLastFlushTime() < request.getIfOlderThanTs();
+      }
+      FlushRegionResponse.Builder builder = FlushRegionResponse.newBuilder();
+      if (shouldFlush) {
+        boolean result = region.flushcache();
+        if (result) {
+          regionServer.compactSplitThread.requestSystemCompaction(region,
+            "Compaction through user triggered flush");
+        }
+        builder.setFlushed(result);
+      }
+      builder.setLastFlushTime(region.getLastFlushTime());
+      return builder.build();
+    } catch (DroppedSnapshotException ex) {
+      // Cache flush can fail in a few places. If it fails in a critical
+      // section, we get a DroppedSnapshotException and a replay of hlog
+      // is required. Currently the only way to do this is a restart of
+      // the server.
+      regionServer.abort("Replay of HLog required. Forcing server shutdown", ex);
+      throw new ServiceException(ex);
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public GetOnlineRegionResponse getOnlineRegion(final RpcController controller,
+      final GetOnlineRegionRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      Map<String, HRegion> onlineRegions = regionServer.onlineRegions;
+      List<HRegionInfo> list = new ArrayList<HRegionInfo>(onlineRegions.size());
+      for (HRegion region: onlineRegions.values()) {
+        list.add(region.getRegionInfo());
+      }
+      Collections.sort(list);
+      return ResponseConverter.buildGetOnlineRegionResponse(list);
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public GetRegionInfoResponse getRegionInfo(final RpcController controller,
+      final GetRegionInfoRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      HRegionInfo info = region.getRegionInfo();
+      GetRegionInfoResponse.Builder builder = GetRegionInfoResponse.newBuilder();
+      builder.setRegionInfo(HRegionInfo.convert(info));
+      if (request.hasCompactionState() && request.getCompactionState()) {
+        builder.setCompactionState(region.getCompactionState());
+      }
+      builder.setIsRecovering(region.isRecovering());
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Get some information of the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @SuppressWarnings("deprecation")
+  public GetServerInfoResponse getServerInfo(final RpcController controller,
+      final GetServerInfoRequest request) throws ServiceException {
+    try {
+      checkOpen();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+    requestCount.increment();
+    int infoPort = regionServer.infoServer != null ? regionServer.infoServer.getPort() : -1;
+    return ResponseConverter.buildGetServerInfoResponse(regionServer.serverName, infoPort);
+  }
+
+  @Override
+  public GetStoreFileResponse getStoreFile(final RpcController controller,
+      final GetStoreFileRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      HRegion region = getRegion(request.getRegion());
+      requestCount.increment();
+      Set<byte[]> columnFamilies;
+      if (request.getFamilyCount() == 0) {
+        columnFamilies = region.getStores().keySet();
+      } else {
+        columnFamilies = new TreeSet<byte[]>(Bytes.BYTES_RAWCOMPARATOR);
+        for (ByteString cf: request.getFamilyList()) {
+          columnFamilies.add(cf.toByteArray());
+        }
+      }
+      int nCF = columnFamilies.size();
+      List<String>  fileList = region.getStoreFileList(
+        columnFamilies.toArray(new byte[nCF][]));
+      GetStoreFileResponse.Builder builder = GetStoreFileResponse.newBuilder();
+      builder.addAllStoreFile(fileList);
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Merge regions on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @return merge regions response
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority = HConstants.HIGH_QOS)
+  public MergeRegionsResponse mergeRegions(final RpcController controller,
+      final MergeRegionsRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion regionA = getRegion(request.getRegionA());
+      HRegion regionB = getRegion(request.getRegionB());
+      boolean forcible = request.getForcible();
+      regionA.startRegionOperation(Operation.MERGE_REGION);
+      regionB.startRegionOperation(Operation.MERGE_REGION);
+      LOG.info("Receiving merging request for  " + regionA + ", " + regionB
+          + ",forcible=" + forcible);
+      regionA.flushcache();
+      regionB.flushcache();
+      regionServer.compactSplitThread.requestRegionsMerge(regionA, regionB, forcible);
+      return MergeRegionsResponse.newBuilder().build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Open asynchronously a region or a set of regions on the region server.
+   *
+   * The opening is coordinated by ZooKeeper, and this method requires the znode to be created
+   *  before being called. As a consequence, this method should be called only from the master.
+   * <p>
+   * Different manages states for the region are:<ul>
+   *  <li>region not opened: the region opening will start asynchronously.</li>
+   *  <li>a close is already in progress: this is considered as an error.</li>
+   *  <li>an open is already in progress: this new open request will be ignored. This is important
+   *  because the Master can do multiple requests if it crashes.</li>
+   *  <li>the region is already opened:  this new open request will be ignored./li>
+   *  </ul>
+   * </p>
+   * <p>
+   * Bulk assign: If there are more than 1 region to open, it will be considered as a bulk assign.
+   * For a single region opening, errors are sent through a ServiceException. For bulk assign,
+   * errors are put in the response as FAILED_OPENING.
+   * </p>
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public OpenRegionResponse openRegion(final RpcController controller,
+      final OpenRegionRequest request) throws ServiceException {
+    try {
+      checkOpen();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+    requestCount.increment();
+    if (request.hasServerStartCode()) {
+      // check that we are the same server that this RPC is intended for.
+      long serverStartCode = request.getServerStartCode();
+      if (regionServer.serverName.getStartcode() !=  serverStartCode) {
+        throw new ServiceException(new DoNotRetryIOException("This RPC was intended for a " +
+            "different server with startCode: " + serverStartCode + ", this server is: "
+            + regionServer.serverName));
+      }
+    }
+    OpenRegionResponse.Builder builder = OpenRegionResponse.newBuilder();
+    final int regionCount = request.getOpenInfoCount();
+    final Map<TableName, HTableDescriptor> htds =
+        new HashMap<TableName, HTableDescriptor>(regionCount);
+    final boolean isBulkAssign = regionCount > 1;
+    for (RegionOpenInfo regionOpenInfo : request.getOpenInfoList()) {
+      final HRegionInfo region = HRegionInfo.convert(regionOpenInfo.getRegion());
+
+      int versionOfOfflineNode = -1;
+      if (regionOpenInfo.hasVersionOfOfflineNode()) {
+        versionOfOfflineNode = regionOpenInfo.getVersionOfOfflineNode();
+      }
+      HTableDescriptor htd;
+      try {
+        final HRegion onlineRegion = regionServer.getFromOnlineRegions(region.getEncodedName());
+        if (onlineRegion != null) {
+          //Check if the region can actually be opened.
+          if (onlineRegion.getCoprocessorHost() != null) {
+            onlineRegion.getCoprocessorHost().preOpen();
+          }
+          // See HBASE-5094. Cross check with hbase:meta if still this RS is owning
+          // the region.
+          Pair<HRegionInfo, ServerName> p = MetaReader.getRegion(
+            regionServer.catalogTracker, region.getRegionName());
+          if (regionServer.serverName.equals(p.getSecond())) {
+            Boolean closing = regionServer.regionsInTransitionInRS.get(region.getEncodedNameAsBytes());
+            // Map regionsInTransitionInRSOnly has an entry for a region only if the region
+            // is in transition on this RS, so here closing can be null. If not null, it can
+            // be true or false. True means the region is opening on this RS; while false
+            // means the region is closing. Only return ALREADY_OPENED if not closing (i.e.
+            // not in transition any more, or still transition to open.
+            if (!Boolean.FALSE.equals(closing)
+                && regionServer.getFromOnlineRegions(region.getEncodedName()) != null) {
+              LOG.warn("Attempted open of " + region.getEncodedName()
+                + " but already online on this server");
+              builder.addOpeningState(RegionOpeningState.ALREADY_OPENED);
+              continue;
+            }
+          } else {
+            LOG.warn("The region " + region.getEncodedName() + " is online on this server"
+              + " but hbase:meta does not have this server - continue opening.");
+            regionServer.removeFromOnlineRegions(onlineRegion, null);
+          }
+        }
+        LOG.info("Open " + region.getRegionNameAsString());
+        htd = htds.get(region.getTable());
+        if (htd == null) {
+          htd = regionServer.tableDescriptors.get(region.getTable());
+          htds.put(region.getTable(), htd);
+        }
+
+        final Boolean previous = regionServer.regionsInTransitionInRS.putIfAbsent(
+          region.getEncodedNameAsBytes(), Boolean.TRUE);
+
+        if (Boolean.FALSE.equals(previous)) {
+          // There is a close in progress. We need to mark this open as failed in ZK.
+          OpenRegionHandler.
+            tryTransitionFromOfflineToFailedOpen(regionServer, region, versionOfOfflineNode);
+
+          throw new RegionAlreadyInTransitionException("Received OPEN for the region:"
+            + region.getRegionNameAsString() + " , which we are already trying to CLOSE ");
+        }
+
+        if (Boolean.TRUE.equals(previous)) {
+          // An open is in progress. This is supported, but let's log this.
+          LOG.info("Receiving OPEN for the region:" +
+            region.getRegionNameAsString() + " , which we are already trying to OPEN"
+              + " - ignoring this new request for this region.");
+        }
+
+        // We are opening this region. If it moves back and forth for whatever reason, we don't
+        // want to keep returning the stale moved record while we are opening/if we close again.
+        regionServer.removeFromMovedRegions(region.getEncodedName());
+
+        if (previous == null) {
+          // check if the region to be opened is marked in recovering state in ZK
+          if (regionServer.distributedLogReplay
+              && SplitLogManager.isRegionMarkedRecoveringInZK(regionServer.getZooKeeper(),
+            region.getEncodedName())) {
+            regionServer.recoveringRegions.put(region.getEncodedName(), null);
+          }
+          // If there is no action in progress, we can submit a specific handler.
+          // Need to pass the expected version in the constructor.
+          if (region.isMetaRegion()) {
+            regionServer.service.submit(new OpenMetaHandler(
+              regionServer, regionServer, region, htd, versionOfOfflineNode));
+          } else {
+            regionServer.updateRegionFavoredNodesMapping(region.getEncodedName(),
+              regionOpenInfo.getFavoredNodesList());
+            regionServer.service.submit(new OpenRegionHandler(
+              regionServer, regionServer, region, htd, versionOfOfflineNode));
+          }
+        }
+
+        builder.addOpeningState(RegionOpeningState.OPENED);
+
+      } catch (KeeperException zooKeeperEx) {
+        LOG.error("Can't retrieve recovering state from zookeeper", zooKeeperEx);
+        throw new ServiceException(zooKeeperEx);
+      } catch (IOException ie) {
+        LOG.warn("Failed opening region " + region.getRegionNameAsString(), ie);
+        if (isBulkAssign) {
+          builder.addOpeningState(RegionOpeningState.FAILED_OPENING);
+        } else {
+          throw new ServiceException(ie);
+        }
+      }
+    }
+    return builder.build();
+  }
+
+  /**
+   * Replay the given changes when distributedLogReplay WAL edits from a failed RS. The guarantee is
+   * that the given mutations will be durable on the receiving RS if this method returns without any
+   * exception.
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority = HConstants.REPLAY_QOS)
+  public ReplicateWALEntryResponse replay(final RpcController controller,
+      final ReplicateWALEntryRequest request) throws ServiceException {
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    CellScanner cells = ((PayloadCarryingRpcController) controller).cellScanner();
+    try {
+      checkOpen();
+      List<WALEntry> entries = request.getEntryList();
+      if (entries == null || entries.isEmpty()) {
+        // empty input
+        return ReplicateWALEntryResponse.newBuilder().build();
+      }
+      HRegion region = regionServer.getRegionByEncodedName(
+        entries.get(0).getKey().getEncodedRegionName().toStringUtf8());
+      RegionCoprocessorHost coprocessorHost = region.getCoprocessorHost();
+      List<Pair<HLogKey, WALEdit>> walEntries = new ArrayList<Pair<HLogKey, WALEdit>>();
+      List<HLogSplitter.MutationReplay> mutations = new ArrayList<HLogSplitter.MutationReplay>();
+      // when tag is enabled, we need tag replay edits with log sequence number
+      boolean needAddReplayTag = (HFile.getFormatVersion(regionServer.conf) >= 3);
+      for (WALEntry entry : entries) {
+        if (regionServer.nonceManager != null) {
+          long nonceGroup = entry.getKey().hasNonceGroup()
+            ? entry.getKey().getNonceGroup() : HConstants.NO_NONCE;
+          long nonce = entry.getKey().hasNonce() ? entry.getKey().getNonce() : HConstants.NO_NONCE;
+          regionServer.nonceManager.reportOperationFromWal(nonceGroup, nonce, entry.getKey().getWriteTime());
+        }
+        Pair<HLogKey, WALEdit> walEntry = (coprocessorHost == null) ? null :
+          new Pair<HLogKey, WALEdit>();
+        List<HLogSplitter.MutationReplay> edits = HLogSplitter.getMutationsFromWALEntry(entry,
+          cells, walEntry, needAddReplayTag);
+        if (coprocessorHost != null) {
+          // Start coprocessor replay here. The coprocessor is for each WALEdit instead of a
+          // KeyValue.
+          if (coprocessorHost.preWALRestore(region.getRegionInfo(), walEntry.getFirst(),
+            walEntry.getSecond())) {
+            // if bypass this log entry, ignore it ...
+            continue;
+          }
+          walEntries.add(walEntry);
+        }
+        mutations.addAll(edits);
+      }
+
+      if (!mutations.isEmpty()) {
+        OperationStatus[] result = doReplayBatchOp(region, mutations);
+        // check if it's a partial success
+        for (int i = 0; result != null && i < result.length; i++) {
+          if (result[i] != OperationStatus.SUCCESS) {
+            throw new IOException(result[i].getExceptionMsg());
+          }
+        }
+      }
+      if (coprocessorHost != null) {
+        for (Pair<HLogKey, WALEdit> wal : walEntries) {
+          coprocessorHost.postWALRestore(region.getRegionInfo(), wal.getFirst(),
+            wal.getSecond());
+        }
+      }
+      return ReplicateWALEntryResponse.newBuilder().build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    } finally {
+      regionServer.metricsRegionServer.updateReplay(
+        EnvironmentEdgeManager.currentTimeMillis() - before);
+    }
+  }
+
+  /**
+   * Replicate WAL entries on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.REPLICATION_QOS)
+  public ReplicateWALEntryResponse replicateWALEntry(final RpcController controller,
+      final ReplicateWALEntryRequest request) throws ServiceException {
+    try {
+      if (regionServer.replicationSinkHandler != null) {
+        checkOpen();
+        requestCount.increment();
+        regionServer.replicationSinkHandler.replicateLogEntries(request.getEntryList(),
+          ((PayloadCarryingRpcController)controller).cellScanner());
+      }
+      return ReplicateWALEntryResponse.newBuilder().build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Roll the WAL writer of the region server.
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  public RollWALWriterResponse rollWALWriter(final RpcController controller,
+      final RollWALWriterRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HLog wal = regionServer.getWAL();
+      byte[][] regionsToFlush = wal.rollWriter(true);
+      RollWALWriterResponse.Builder builder = RollWALWriterResponse.newBuilder();
+      if (regionsToFlush != null) {
+        for (byte[] region: regionsToFlush) {
+          builder.addRegionToFlush(HBaseZeroCopyByteString.wrap(region));
+        }
+      }
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Split a region on the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  @QosPriority(priority=HConstants.HIGH_QOS)
+  public SplitRegionResponse splitRegion(final RpcController controller,
+      final SplitRegionRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      region.startRegionOperation(Operation.SPLIT_REGION);
+      LOG.info("Splitting " + region.getRegionNameAsString());
+      region.flushcache();
+      byte[] splitPoint = null;
+      if (request.hasSplitPoint()) {
+        splitPoint = request.getSplitPoint().toByteArray();
+      }
+      region.forceSplit(splitPoint);
+      regionServer.compactSplitThread.requestSplit(region, region.checkSplit());
+      return SplitRegionResponse.newBuilder().build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Stop the region server.
+   *
+   * @param controller the RPC controller
+   * @param request the request
+   * @throws ServiceException
+   */
+  @Override
+  public StopServerResponse stopServer(final RpcController controller,
+      final StopServerRequest request) throws ServiceException {
+    requestCount.increment();
+    String reason = request.getReason();
+    regionServer.stop(reason);
+    return StopServerResponse.newBuilder().build();
+  }
+
+  @Override
+  public UpdateFavoredNodesResponse updateFavoredNodes(RpcController controller,
+      UpdateFavoredNodesRequest request) throws ServiceException {
+    List<UpdateFavoredNodesRequest.RegionUpdateInfo> openInfoList = request.getUpdateInfoList();
+    UpdateFavoredNodesResponse.Builder respBuilder = UpdateFavoredNodesResponse.newBuilder();
+    for (UpdateFavoredNodesRequest.RegionUpdateInfo regionUpdateInfo : openInfoList) {
+      HRegionInfo hri = HRegionInfo.convert(regionUpdateInfo.getRegion());
+      regionServer.updateRegionFavoredNodesMapping(hri.getEncodedName(),
+        regionUpdateInfo.getFavoredNodesList());
+    }
+    respBuilder.setResponse(openInfoList.size());
+    return respBuilder.build();
+  }
+
+  /**
+   * Atomically bulk load several HFiles into an open region
+   * @return true if successful, false is failed but recoverably (no action)
+   * @throws IOException if failed unrecoverably
+   */
+  @Override
+  public BulkLoadHFileResponse bulkLoadHFile(final RpcController controller,
+      final BulkLoadHFileRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      List<Pair<byte[], String>> familyPaths = new ArrayList<Pair<byte[], String>>();
+      for (FamilyPath familyPath: request.getFamilyPathList()) {
+        familyPaths.add(new Pair<byte[], String>(familyPath.getFamily().toByteArray(),
+          familyPath.getPath()));
+      }
+      boolean bypass = false;
+      if (region.getCoprocessorHost() != null) {
+        bypass = region.getCoprocessorHost().preBulkLoadHFile(familyPaths);
+      }
+      boolean loaded = false;
+      if (!bypass) {
+        loaded = region.bulkLoadHFiles(familyPaths, request.getAssignSeqNum());
+      }
+      if (region.getCoprocessorHost() != null) {
+        loaded = region.getCoprocessorHost().postBulkLoadHFile(familyPaths, loaded);
+      }
+      BulkLoadHFileResponse.Builder builder = BulkLoadHFileResponse.newBuilder();
+      builder.setLoaded(loaded);
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  @Override
+  public CoprocessorServiceResponse execService(final RpcController controller,
+      final CoprocessorServiceRequest request) throws ServiceException {
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      Message result = execServiceOnRegion(region, request.getCall());
+      CoprocessorServiceResponse.Builder builder =
+        CoprocessorServiceResponse.newBuilder();
+      builder.setRegion(RequestConverter.buildRegionSpecifier(
+        RegionSpecifierType.REGION_NAME, region.getRegionName()));
+      builder.setValue(
+        builder.getValueBuilder().setName(result.getClass().getName())
+          .setValue(result.toByteString()));
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+  }
+
+  private Message execServiceOnRegion(HRegion region,
+      final ClientProtos.CoprocessorServiceCall serviceCall) throws IOException {
+    // ignore the passed in controller (from the serialized call)
+    ServerRpcController execController = new ServerRpcController();
+    Message result = region.execService(execController, serviceCall);
+    if (execController.getFailedOn() != null) {
+      throw execController.getFailedOn();
+    }
+    return result;
+  }
+
+  /**
+   * Get data from a table.
+   *
+   * @param controller the RPC controller
+   * @param request the get request
+   * @throws ServiceException
+   */
+  @Override
+  public GetResponse get(final RpcController controller,
+      final GetRequest request) throws ServiceException {
+    long before = EnvironmentEdgeManager.currentTimeMillis();
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+
+      GetResponse.Builder builder = GetResponse.newBuilder();
+      ClientProtos.Get get = request.getGet();
+      Boolean existence = null;
+      Result r = null;
+
+      if (get.hasClosestRowBefore() && get.getClosestRowBefore()) {
+        if (get.getColumnCount() != 1) {
+          throw new DoNotRetryIOException(
+            "get ClosestRowBefore supports one and only one family now, not "
+              + get.getColumnCount() + " families");
+        }
+        byte[] row = get.getRow().toByteArray();
+        byte[] family = get.getColumn(0).getFamily().toByteArray();
+        r = region.getClosestRowBefore(row, family);
+      } else {
+        Get clientGet = ProtobufUtil.toGet(get);
+        if (get.getExistenceOnly() && region.getCoprocessorHost() != null) {
+          existence = region.getCoprocessorHost().preExists(clientGet);
+        }
+        if (existence == null) {
+          r = region.get(clientGet);
+          if (get.getExistenceOnly()) {
+            boolean exists = r.getExists();
+            if (region.getCoprocessorHost() != null) {
+              exists = region.getCoprocessorHost().postExists(clientGet, exists);
+            }
+            existence = exists;
+          }
+        }
+      }
+      if (existence != null){
+        ClientProtos.Result pbr = ProtobufUtil.toResult(existence);
+        builder.setResult(pbr);
+      } else  if (r != null) {
+        ClientProtos.Result pbr = ProtobufUtil.toResult(r);
+        builder.setResult(pbr);
+      }
+      return builder.build();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    } finally {
+      regionServer.metricsRegionServer.updateGet(
+        EnvironmentEdgeManager.currentTimeMillis() - before);
+    }
+  }
+
+  /**
+   * Execute multiple actions on a table: get, mutate, and/or execCoprocessor
+   *
+   * @param rpcc the RPC controller
+   * @param request the multi request
+   * @throws ServiceException
+   */
+  @Override
+  public MultiResponse multi(final RpcController rpcc, final MultiRequest request)
+  throws ServiceException {
+    try {
+      checkOpen();
+    } catch (IOException ie) {
+      throw new ServiceException(ie);
+    }
+
+    // rpc controller is how we bring in data via the back door;  it is unprotobuf'ed data.
+    // It is also the conduit via which we pass back data.
+    PayloadCarryingRpcController controller = (PayloadCarryingRpcController)rpcc;
+    CellScanner cellScanner = controller != null ? controller.cellScanner(): null;
+    if (controller != null) controller.setCellScanner(null);
+
+    long nonceGroup = request.hasNonceGroup() ? request.getNonceGroup() : HConstants.NO_NONCE;
+
+    // this will contain all the cells that we need to return. It's created later, if needed.
+    List<CellScannable> cellsToReturn = null;
+    MultiResponse.Builder responseBuilder = MultiResponse.newBuilder();
+    RegionActionResult.Builder regionActionResultBuilder = RegionActionResult.newBuilder();
+
+    for (RegionAction regionAction : request.getRegionActionList()) {
+      this.requestCount.add(regionAction.getActionCount());
+      HRegion region;
+      regionActionResultBuilder.clear();
+      try {
+        region = getRegion(regionAction.getRegion());
+      } catch (IOException e) {
+        regionActionResultBuilder.setException(ResponseConverter.buildException(e));
+        responseBuilder.addRegionActionResult(regionActionResultBuilder.build());
+        continue;  // For this region it's a failure.
+      }
+
+      if (regionAction.hasAtomic() && regionAction.getAtomic()) {
+        // How does this call happen?  It may need some work to play well w/ the surroundings.
+        // Need to return an item per Action along w/ Action index.  TODO.
+        try {
+          mutateRows(region, regionAction.getActionList(), cellScanner);
+        } catch (IOException e) {
+          // As it's atomic, we may expect it's a global failure.
+          regionActionResultBuilder.setException(ResponseConverter.buildException(e));
+        }
+      } else {
+        // doNonAtomicRegionMutation manages the exception internally
+        cellsToReturn = doNonAtomicRegionMutation(region, regionAction, cellScanner,
+            regionActionResultBuilder, cellsToReturn, nonceGroup);
+      }
+      responseBuilder.addRegionActionResult(regionActionResultBuilder.build());
+    }
+    // Load the controller with the Cells to return.
+    if (cellsToReturn != null && !cellsToReturn.isEmpty() && controller != null) {
+      controller.setCellScanner(CellUtil.createCellScanner(cellsToReturn));
+    }
+    return responseBuilder.build();
+  }
+
+  /**
+   * Mutate data in a table.
+   *
+   * @param rpcc the RPC controller
+   * @param request the mutate request
+   * @throws ServiceException
+   */
+  @Override
+  public MutateResponse mutate(final RpcController rpcc,
+      final MutateRequest request) throws ServiceException {
+    // rpc controller is how we bring in data via the back door;  it is unprotobuf'ed data.
+    // It is also the conduit via which we pass back data.
+    PayloadCarryingRpcController controller = (PayloadCarryingRpcController)rpcc;
+    CellScanner cellScanner = controller != null? controller.cellScanner(): null;
+    // Clear scanner so we are not holding on to reference across call.
+    if (controller != null) controller.setCellScanner(null);
+    try {
+      checkOpen();
+      requestCount.increment();
+      HRegion region = getRegion(request.getRegion());
+      MutateResponse.Builder builder = MutateResponse.newBuilder();
+      MutationProto mutation = request.getMutation();
+      if (!region.getRegionInfo().isMetaTable()) {
+        regionServer.cacheFlusher.reclaimMemStoreMemory();
+      }
+      long nonceGroup = request.hasNonceGroup()
+          ? request.getNonceGroup() : HConstants.NO_NONCE;
+      Result r = null;
+      Boolean processed = null;
+      MutationType type = mutation.getMutateType();
+      switch (type) {
+      case APPEND:
+        // TODO: this doesn't actually check anything.
+        r = append(region, mutation, cellScanner, nonceGroup);
+        break;
+      case INCREMENT:
+        // TODO: this doesn't actually check anything.
+        r = increment(region, mutation, cellScanner, nonceGroup);
+        break;
+      case PUT:
+        Put put = ProtobufUtil.toPut(mutation, cellScanner);
+        if (request.hasCondition()) {
+          Condition condition = request.getCondition();
+          byte[] row = condition.getRow().toByteArray();
+          byte[] family = condition.getFamily().toByteArray();
+          byte[] qualifier = condition.getQualifier().toByteArray();
+          CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
+          ByteArrayComparable comparator =
+            ProtobufUtil.toComparator(condition.getComparator());
+          if (region.getCoprocessorHost() != null) {
+            processed = region.getCoprocessorHost().preCheckAndPut(
+              row, family, qualifier, compareOp, comparator, put);
+          }
+          if (processed == null) {
+            boolean result = region.checkAndMutate(row, family,
+              qualifier, compareOp, comparator, put, true);
+            if (region.getCoprocessorHost() != null) {
+              result = region.getCoprocessorHost().postCheckAndPut(row, family,
+                qualifier, compareOp, comparator, put, result);
+            }
+            processed = result;
+          }
+        } else {
+          region.put(put);
+          processed = Boolean.TRUE;
+        }
+        break;
+      case DELETE:
+        Delete delete = ProtobufUtil.toDelete(mutation, cellScanner);
+        if (request.hasCondition()) {
+          Condition condition = request.getCondition();
+          byte[] row = condition.getRow().toByteArray();
+          byte[] family = condition.getFamily().toByteArray();
+          byte[] qualifier = condition.getQualifier().toByteArray();
+          CompareOp compareOp = CompareOp.valueOf(condition.getCompareType().name());
+          ByteArrayComparable comparator =
+            ProtobufUtil.toComparator(condition.getComparator());
+          if (region.getCoprocessorHost() != null) {
+            processed = region.getCoprocessorHost().preCheckAndDelete(
+              row, family, qualifier, compareOp, comparator, delete);
+          }
+          if (processed == null) {
+            boolean result = region.checkAndMutate(row, family,
+              qualifier, compareOp, comparator, delete, true);
+            if (region.getCoprocessorHost() != null) {
+              result = region.getCoprocessorHost().postCheckAndDelete(row, family,
+                qualifier, compareOp, comparator, delete, result);
+            }
+            processed = result;
+          }
+        } else {
+          region.delete(delete);
+          processed = Boolean.TRUE;
+        }
+        break;
+      default:
+          throw new DoNotRetryIOException(
+            "Unsupported mutate type: " + type.name());
+      }
+      if (processed != null) builder.setProcessed(processed.booleanValue());
+      addResult(builder, r, controller);
+      return builder.build();
+    } catch (IOException ie) {
+      regionServer.checkFileSystem();
+      throw new ServiceException(ie);
+    }
+  }
+
+  /**
+   * Scan data in a table.
+   *
+   * @param controller the RPC controller
+   * @param request the scan request
+   * @throws ServiceException
+   */
+  @Override
+  public ScanResponse scan(final RpcController controller, final ScanRequest request)
+  throws ServiceException {
+    Leases.Lease lease = null;
+    String scannerName = null;
+    try {
+      if (!request.hasScannerId() && !request.hasScan()) {
+        throw new DoNotRetryIOException(
+          "Missing required input: scannerId or scan");
+      }
+      long scannerId = -1;
+      if (request.hasScannerId()) {
+        scannerId = request.getScannerId();
+        scannerName = String.valueOf(scannerId);
+      }
+      try {
+        checkOpen();
+      } catch (IOException e) {
+        // If checkOpen failed, server not running or filesystem gone,
+        // cancel this lease; filesystem is gone or we're closing or something.
+        if (scannerName != null) {
+          try {
+            regionServer.leases.cancelLease(scannerName);
+          } catch (LeaseException le) {
+            LOG.info("Server shutting down and client tried to access missing scanner " +
+              scannerName);
+          }
+        }
+        throw e;
+      }
+      requestCount.increment();
+
+      int ttl = 0;
+      HRegion region = null;
+      RegionScanner scanner = null;
+      RegionScannerHolder rsh = null;
+      boolean moreResults = true;
+      boolean closeScanner = false;
+      ScanResponse.Builder builder = ScanResponse.newBuilder();
+      if (request.hasCloseScanner()) {
+        closeScanner = request.getCloseScanner();
+      }
+      int rows = 1;
+      if (request.hasNumberOfRows()) {
+        rows = request.getNumberOfRows();
+      }
+      if (request.hasScannerId()) {
+        rsh = scanners.get(scannerName);
+        if (rsh == null) {
+          LOG.info("Client tried to access missing scanner " + scannerName);
+          throw new UnknownScannerException(
+            "Name: " + scannerName + ", already closed?");
+        }
+        scanner = rsh.s;
+        HRegionInfo hri = scanner.getRegionInfo();
+        region = regionServer.getRegion(hri.getRegionName());
+        if (region != rsh.r) { // Yes, should be the same instance
+          throw new NotServingRegionException("Region was re-opened after the scanner"
+            + scannerName + " was created: " + hri.getRegionNameAsString());
+        }
+      } else {
+        region = getRegion(request.getRegion());
+        ClientProtos.Scan protoScan = request.getScan();
+        boolean isLoadingCfsOnDemandSet = protoScan.hasLoadColumnFamiliesOnDemand();
+        Scan scan = ProtobufUtil.toScan(protoScan);
+        // if the request doesn't set this, get the default region setting.
+        if (!isLoadingCfsOnDemandSet) {
+          scan.setLoadColumnFamiliesOnDemand(region.isLoadingCfsOnDemandDefault());
+        }
+        scan.getAttribute(Scan.SCAN_ATTRIBUTES_METRICS_ENABLE);
+        region.prepareScanner(scan);
+        if (region.getCoprocessorHost() != null) {
+          scanner = region.getCoprocessorHost().preScannerOpen(scan);
+        }
+        if (scanner == null) {
+          scanner = region.getScanner(scan);
+        }
+        if (region.getCoprocessorHost() != null) {
+          scanner = region.getCoprocessorHost().postScannerOpen(scan, scanner);
+        }
+        scannerId = addScanner(scanner, region);
+        scannerName = String.valueOf(scannerId);
+        ttl = this.scannerLeaseTimeoutPeriod;
+      }
+
+      if (rows > 0) {
+        // if nextCallSeq does not match throw Exception straight away. This needs to be
+        // performed even before checking of Lease.
+        // See HBASE-5974
+        if (request.hasNextCallSeq()) {
+          if (rsh == null) {
+            rsh = scanners.get(scannerName);
+          }
+          if (rsh != null) {
+            if (request.getNextCallSeq() != rsh.nextCallSeq) {
+              throw new OutOfOrderScannerNextException("Expected nextCallSeq: " + rsh.nextCallSeq
+                + " But the nextCallSeq got from client: " + request.getNextCallSeq() +
+                "; request=" + TextFormat.shortDebugString(request));
+            }
+            // Increment the nextCallSeq value which is the next expected from client.
+            rsh.nextCallSeq++;
+          }
+        }
+        try {
+          // Remove lease while its being processed in server; protects against case
+          // where processing of request takes > lease expiration time.
+          lease = regionServer.leases.removeLease(scannerName);
+          List<Result> results = new ArrayList<Result>(rows);
+          long currentScanResultSize = 0;
+
+          boolean done = false;
+          // Call coprocessor. Get region info from scanner.
+          if (region != null && region.getCoprocessorHost() != null) {
+            Boolean bypass = region.getCoprocessorHost().preScannerNext(
+              scanner, results, rows);
+            if (!results.isEmpty()) {
+              for (Result r : results) {
+                if (maxScannerResultSize < Long.MAX_VALUE){
+                  for (Cell kv : r.rawCells()) {
+                    currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
+                  }
+                }
+              }
+            }
+            if (bypass != null && bypass.booleanValue()) {
+              done = true;
+            }
+          }
+
+          if (!done) {
+            long maxResultSize = scanner.getMaxResultSize();
+            if (maxResultSize <= 0) {
+              maxResultSize = maxScannerResultSize;
+            }
+            List<Cell> values = new ArrayList<Cell>();
+            region.startRegionOperation(Operation.SCAN);
+            try {
+              int i = 0;
+              synchronized(scanner) {
+                for (; i < rows
+                    && currentScanResultSize < maxResultSize; ) {
+                  // Collect values to be returned here
+                  boolean moreRows = scanner.nextRaw(values);
+                  if (!values.isEmpty()) {
+                    if (maxScannerResultSize < Long.MAX_VALUE){
+                      for (Cell kv : values) {
+                        currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
+                      }
+                    }
+                    results.add(Result.create(values));
+                    i++;
+                  }
+                  if (!moreRows) {
+                    break;
+                  }
+                  values.clear();
+                }
+              }
+              region.readRequestsCount.add(i);
+            } finally {
+              region.closeRegionOperation();
+            }
+
+            // coprocessor postNext hook
+            if (region != null && region.getCoprocessorHost() != null) {
+              region.getCoprocessorHost().postScannerNext(scanner, results, rows, true);
+            }
+          }
+
+          // If the scanner's filter - if any - is done with the scan
+          // and wants to tell the client to stop the scan. This is done by passing
+          // a null result, and setting moreResults to false.
+          if (scanner.isFilterDone() && results.isEmpty()) {
+            moreResults = false;
+            results = null;
+          } else {
+            addResults(builder, results, controller);
+          }
+        } finally {
+          // We're done. On way out re-add the above removed lease.
+          // Adding resets expiration time on lease.
+          if (scanners.containsKey(scannerName)) {
+            if (lease != null) regionServer.leases.addLease(lease);
+            ttl = this.scannerLeaseTimeoutPeriod;
+          }
+        }
+      }
+
+      if (!moreResults || closeScanner) {
+        ttl = 0;
+        moreResults = false;
+        if (region != null && region.getCoprocessorHost() != null) {
+          if (region.getCoprocessorHost().preScannerClose(scanner)) {
+            return builder.build(); // bypass
+          }
+        }
+        rsh = scanners.remove(scannerName);
+        if (rsh != null) {
+          scanner = rsh.s;
+          scanner.close();
+          regionServer.leases.cancelLease(scannerName);
+          if (region != null && region.getCoprocessorHost() != null) {
+            region.getCoprocessorHost().postScannerClose(scanner);
+          }
+        }
+      }
+
+      if (ttl > 0) {
+        builder.setTtl(ttl);
+      }
+      builder.setScannerId(scannerId);
+      builder.setMoreResults(moreResults);
+      return builder.build();
+    } catch (IOException ie) {
+      if (scannerName != null && ie instanceof NotServingRegionException) {
+        RegionScannerHolder rsh = scanners.remove(scannerName);
+        if (rsh != null) {
+          try {
+            RegionScanner scanner = rsh.s;
+            scanner.close();
+            regionServer.leases.cancelLease(scannerName);
+          } catch (IOException e) {}
+        }
+      }
+      throw new ServiceException(ie);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
index 287ffa1..4c02cfd 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionMergeTransaction.java
@@ -42,7 +42,6 @@ import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.client.Delete;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.executor.EventType;
@@ -240,7 +239,8 @@ public class RegionMergeTransaction {
   public HRegion execute(final Server server,
       final RegionServerServices services) throws IOException {
     if (rsCoprocessorHost == null) {
-      rsCoprocessorHost = server != null ? ((HRegionServer) server).getCoprocessorHost() : null;
+      rsCoprocessorHost = server != null ?
+        ((HRegionServer) server).getRegionServerCoprocessorHost() : null;
     }
     HRegion mergedRegion = createMergedRegion(server, services);
     if (rsCoprocessorHost != null) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
index 2be10cd..d50fad7 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java
@@ -18,27 +18,26 @@
  */
 package org.apache.hadoop.hbase.regionserver;
 
+import java.io.IOException;
+import java.util.Map;
+import java.util.concurrent.ConcurrentMap;
+
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.executor.ExecutorService;
-import org.apache.hadoop.hbase.ipc.PriorityFunction;
 import org.apache.hadoop.hbase.ipc.RpcServerInterface;
 import org.apache.hadoop.hbase.master.TableLockManager;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.zookeeper.KeeperException;
 
-import java.io.IOException;
-import java.util.Map;
-import java.util.concurrent.ConcurrentMap;
-
 /**
  * Services provided by {@link HRegionServer}
  */
 @InterfaceAudience.Private
 public interface RegionServerServices
-    extends OnlineRegions, FavoredNodesForRegion, PriorityFunction {
+    extends OnlineRegions, FavoredNodesForRegion {
   /**
    * @return True if this regionserver is stopping.
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java
index 58cbe12..25332ea 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.java
@@ -18,21 +18,16 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ipc.PriorityFunction;
 import org.apache.hadoop.hbase.ipc.RpcScheduler;
 
 /**
- * A factory class that constructs an {@link org.apache.hadoop.hbase.ipc.RpcScheduler} for
- * a region server.
+ * A factory class that constructs an {@link org.apache.hadoop.hbase.ipc.RpcScheduler}.
  */
 public interface RpcSchedulerFactory {
 
   /**
    * Constructs a {@link org.apache.hadoop.hbase.ipc.RpcScheduler}.
-   *
-   * Please note that this method is called in constructor of {@link HRegionServer}, so some
-   * fields may not be ready for access. The reason that {@code HRegionServer} is passed as
-   * parameter here is that an RPC scheduler may need to access data structure inside
-   * {@code HRegionServer} (see example in {@link SimpleRpcSchedulerFactory}).
    */
-  RpcScheduler create(Configuration conf, RegionServerServices server);
+  RpcScheduler create(Configuration conf, PriorityFunction priority);
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SimpleRpcSchedulerFactory.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SimpleRpcSchedulerFactory.java
index 03a75ba..87287bc 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SimpleRpcSchedulerFactory.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/SimpleRpcSchedulerFactory.java
@@ -19,14 +19,15 @@ package org.apache.hadoop.hbase.regionserver;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.ipc.PriorityFunction;
 import org.apache.hadoop.hbase.ipc.RpcScheduler;
 import org.apache.hadoop.hbase.ipc.SimpleRpcScheduler;
 
-/** Constructs a {@link SimpleRpcScheduler}. for the region server. */
+/** Constructs a {@link SimpleRpcScheduler}. */
 class SimpleRpcSchedulerFactory implements RpcSchedulerFactory {
 
   @Override
-  public RpcScheduler create(Configuration conf, RegionServerServices server) {
+  public RpcScheduler create(Configuration conf, PriorityFunction priority) {
     int handlerCount = conf.getInt(HConstants.REGION_SERVER_HANDLER_COUNT,
         HConstants.DEFAULT_REGION_SERVER_HANDLER_COUNT);
     return new SimpleRpcScheduler(
@@ -36,7 +37,7 @@ class SimpleRpcSchedulerFactory implements RpcSchedulerFactory {
             HConstants.DEFAULT_REGION_SERVER_META_HANDLER_COUNT),
         conf.getInt(HConstants.REGION_SERVER_REPLICATION_HANDLER_COUNT,
             HConstants.DEFAULT_REGION_SERVER_REPLICATION_HANDLER_COUNT),
-        server,
+        priority,
         HConstants.QOS_THRESHOLD);
   }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
index e37401d..a1d9fe0 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java
@@ -29,10 +29,8 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.regionserver.ShutdownHook;
 import org.apache.hadoop.util.ReflectionUtils;
 
 /**
@@ -195,9 +193,6 @@ public class JVMClusterUtil {
 
     if (regionservers != null) {
       for (JVMClusterUtil.RegionServerThread t: regionservers) {
-        HRegionServer hrs = t.getRegionServer();
-        ShutdownHook.install(hrs.getConfiguration(), FileSystem.get(hrs
-                .getConfiguration()), hrs, t);
         t.start();
       }
     }
diff --git hbase-server/src/main/resources/hbase-webapps/master/zk.jsp hbase-server/src/main/resources/hbase-webapps/master/zk.jsp
index 0c6d88e..2cabf16 100644
--- hbase-server/src/main/resources/hbase-webapps/master/zk.jsp
+++ hbase-server/src/main/resources/hbase-webapps/master/zk.jsp
@@ -23,7 +23,7 @@
   import="org.apache.hadoop.hbase.HBaseConfiguration"
   import="org.apache.hadoop.hbase.master.HMaster"%><%
   HMaster master = (HMaster)getServletContext().getAttribute(HMaster.MASTER);
-  ZooKeeperWatcher watcher = master.getZooKeeperWatcher();
+  ZooKeeperWatcher watcher = master.getZooKeeper();
 %>
 <!--[if IE]>
 <!DOCTYPE html>
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseCluster.java hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseCluster.java
index 1f6d666..d50052f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseCluster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseCluster.java
@@ -97,7 +97,7 @@ public abstract class HBaseCluster implements Closeable, Configurable {
   /**
    * Returns an {@link MasterAdminService.BlockingInterface} to the active master
    */
-  public abstract MasterService.BlockingInterface getMaster()
+  public abstract MasterService.BlockingInterface getMasterAdminService()
   throws IOException;
 
   /**
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index 6372ab1..266967f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -40,6 +40,7 @@ import java.util.Map;
 import java.util.NavigableSet;
 import java.util.Random;
 import java.util.Set;
+import java.util.TreeSet;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 
@@ -75,12 +76,14 @@ import org.apache.hadoop.hbase.mapreduce.MapreduceTestingShim;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.RegionStates;
 import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.BloomType;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HStore;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
+import org.apache.hadoop.hbase.regionserver.RegionServerStoppedException;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.security.User;
 import org.apache.hadoop.hbase.tool.Canary;
@@ -88,6 +91,7 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;
+import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
 import org.apache.hadoop.hbase.util.RegionSplitter;
 import org.apache.hadoop.hbase.util.RetryCounter;
 import org.apache.hadoop.hbase.util.Threads;
@@ -124,6 +128,7 @@ import org.apache.zookeeper.ZooKeeper.States;
  */
 @InterfaceAudience.Public
 @InterfaceStability.Evolving
+@SuppressWarnings("deprecation")
 public class HBaseTestingUtility extends HBaseCommonTestingUtility {
    private MiniZooKeeperCluster zkCluster = null;
 
@@ -1928,7 +1933,6 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
     return createMultiRegions(c, table, family, regionStartKeys);
   }
 
-  @SuppressWarnings("deprecation")
   public int createMultiRegions(final Configuration c, final HTable table,
       final byte[] columnFamily, byte [][] startKeys)
   throws IOException {
@@ -2740,6 +2744,32 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
     return user;
   }
 
+  public static NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)
+      throws IOException {
+    NavigableSet<String> online = new TreeSet<String>();
+    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
+      try {
+        for (HRegionInfo region :
+            ProtobufUtil.getOnlineRegions(rst.getRegionServer().getRSRpcServices())) {
+          online.add(region.getRegionNameAsString());
+        }
+      } catch (RegionServerStoppedException e) {
+        // That's fine.
+      }
+    }
+    for (MasterThread mt : cluster.getLiveMasterThreads()) {
+      try {
+        for (HRegionInfo region :
+            ProtobufUtil.getOnlineRegions(mt.getMaster().getRSRpcServices())) {
+          online.add(region.getRegionNameAsString());
+        }
+      } catch (RegionServerStoppedException e) {
+        // That's fine.
+      }
+    }
+    return online;
+  }
+
   /**
    * Set maxRecoveryErrorCount in DFSClient.  In 0.20 pre-append its hard-coded to 5 and
    * makes tests linger.  Here is the exception you'll see:
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
index 834fc51..f32d0ad 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/MiniHBaseCluster.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.AdminService;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ClientService;
+import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.MasterService;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
@@ -379,11 +380,32 @@ public class MiniHBaseCluster extends HBaseCluster {
    * Returns the current active master, if available.
    * @return the active HMaster, null if none is active.
    */
+  public MasterService.BlockingInterface getMasterAdminService() {
+    return this.hbaseCluster.getActiveMaster().getMasterRpcServices();
+  }
+
+  /**
+   * Returns the current active master, if available.
+   * @return the active HMaster, null if none is active.
+   */
   public HMaster getMaster() {
     return this.hbaseCluster.getActiveMaster();
   }
 
   /**
+   * Returns the current active master thread, if available.
+   * @return the active MasterThread, null if none is active.
+   */
+  public MasterThread getMasterThread() {
+    for (MasterThread mt: hbaseCluster.getLiveMasters()) {
+      if (mt.getMaster().isActiveMaster()) {
+        return mt;
+      }
+    }
+    return null;
+  }
+
+  /**
    * Returns the master at the specified index, if available.
    * @return the active HMaster, null if none is active.
    */
@@ -490,6 +512,7 @@ public class MiniHBaseCluster extends HBaseCluster {
    * Shut down the mini HBase cluster
    * @throws IOException
    */
+  @SuppressWarnings("deprecation")
   public void shutdown() throws IOException {
     if (this.hbaseCluster != null) {
       this.hbaseCluster.shutdown();
@@ -635,6 +658,15 @@ public class MiniHBaseCluster extends HBaseCluster {
 
   @Override
   public ServerName getServerHoldingRegion(byte[] regionName) throws IOException {
+    // Assume there is only one master thread which is the active master.
+    // If there are multiple master threads, the backup master threads
+    // should hold some regions. Please refer to #countServedRegions
+    // to see how we find out all regions.
+    HMaster master = getMaster();
+    HRegion region = master.getOnlineRegion(regionName);
+    if (region != null) {
+      return master.getServerName();
+    }
     int index = getServerWith(regionName);
     if (index < 0) {
       return null;
@@ -653,6 +685,9 @@ public class MiniHBaseCluster extends HBaseCluster {
     for (JVMClusterUtil.RegionServerThread rst : getLiveRegionServerThreads()) {
       count += rst.getRegionServer().getNumberOfOnlineRegions();
     }
+    for (JVMClusterUtil.MasterThread mt : getLiveMasterThreads()) {
+      count += mt.getMaster().getNumberOfOnlineRegions();
+    }
     return count;
   }
 
@@ -711,12 +746,12 @@ public class MiniHBaseCluster extends HBaseCluster {
 
   @Override
   public AdminService.BlockingInterface getAdminProtocol(ServerName serverName) throws IOException {
-    return getRegionServer(getRegionServerIndex(serverName));
+    return getRegionServer(getRegionServerIndex(serverName)).getRSRpcServices();
   }
 
   @Override
   public ClientService.BlockingInterface getClientProtocol(ServerName serverName)
   throws IOException {
-    return getRegionServer(getRegionServerIndex(serverName));
+    return getRegionServer(getRegionServerIndex(serverName)).getRSRpcServices();
   }
 }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
index 50ad030..1630b0d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/MockRegionServerServices.java
@@ -24,7 +24,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentSkipListMap;
 
-import com.google.protobuf.Message;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
@@ -33,7 +32,6 @@ import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.ipc.RpcServerInterface;
 import org.apache.hadoop.hbase.master.TableLockManager;
 import org.apache.hadoop.hbase.master.TableLockManager.NullTableLockManager;
-import org.apache.hadoop.hbase.protobuf.generated.RPCProtos;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -218,11 +216,6 @@ class MockRegionServerServices implements RegionServerServices {
   }
 
   @Override
-  public int getPriority(RPCProtos.RequestHeader header, Message param) {
-    return 0;
-  }
-
-  @Override
   public ServerNonceManager getNonceManager() {
     // TODO Auto-generated method stub
     return null;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestDrainingServer.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestDrainingServer.java
index 9a51729..85b833f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestDrainingServer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestDrainingServer.java
@@ -134,9 +134,8 @@ public class TestDrainingServer {
         balancer, startupMasterExecutor("mockExecutorService"), null, null);
 
     Mockito.when(master.getAssignmentManager()).thenReturn(am);
-    Mockito.when(master.getZooKeeperWatcher()).thenReturn(zkWatcher);
     Mockito.when(master.getZooKeeper()).thenReturn(zkWatcher);
-    
+
     am.addPlan(REGIONINFO.getEncodedName(), new RegionPlan(REGIONINFO, null, SERVERNAME_A));
 
     zkWatcher.registerListenerFirst(am);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
index 3efbdad..2052a3b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestGlobalMemStoreSize.java
@@ -81,7 +81,8 @@ public class TestGlobalMemStoreSize {
 
     for (HRegionServer server : getOnlineRegionServers()) {
       long globalMemStoreSize = 0;
-      for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
+      for (HRegionInfo regionInfo :
+          ProtobufUtil.getOnlineRegions(server.getRSRpcServices())) {
         globalMemStoreSize += 
           server.getFromOnlineRegions(regionInfo.getEncodedName()).
           getMemstoreSize().get();
@@ -96,7 +97,8 @@ public class TestGlobalMemStoreSize {
       LOG.info("Starting flushes on " + server.getServerName() +
         ", size=" + server.getRegionServerAccounting().getGlobalMemstoreSize());
 
-      for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
+      for (HRegionInfo regionInfo :
+          ProtobufUtil.getOnlineRegions(server.getRSRpcServices())) {
         HRegion r = server.getFromOnlineRegions(regionInfo.getEncodedName());
         flush(r, server);
       }
@@ -111,7 +113,8 @@ public class TestGlobalMemStoreSize {
       if (size > 0) {
         // If size > 0, see if its because the meta region got edits while
         // our test was running....
-        for (HRegionInfo regionInfo : ProtobufUtil.getOnlineRegions(server)) {
+        for (HRegionInfo regionInfo :
+            ProtobufUtil.getOnlineRegions(server.getRSRpcServices())) {
           HRegion r = server.getFromOnlineRegions(regionInfo.getEncodedName());
           long l = r.getMemstoreSize().longValue();
           if (l > 0) {
@@ -144,15 +147,6 @@ public class TestGlobalMemStoreSize {
       server.getRegionServerAccounting().getGlobalMemstoreSize());
   }
 
-  /** figure out how many regions are currently being served. */
-  private int getRegionCount() throws IOException {
-    int total = 0;
-    for (HRegionServer server : getOnlineRegionServers()) {
-      total += ProtobufUtil.getOnlineRegions(server).size();
-    }
-    return total;
-  }
-  
   private List<HRegionServer> getOnlineRegionServers() {
     List<HRegionServer> list = new ArrayList<HRegionServer>();
     for (JVMClusterUtil.RegionServerThread rst : 
@@ -168,12 +162,14 @@ public class TestGlobalMemStoreSize {
    * Wait until all the regions are assigned.
    */
   private void waitForAllRegionsAssigned() throws IOException {
-    while (getRegionCount() < totalRegionNum) {
-      LOG.debug("Waiting for there to be "+totalRegionNum+" regions, but there are " + getRegionCount() + " right now.");
+    while (true) {
+      int regionCount = HBaseTestingUtility.getAllOnlineRegions(cluster).size();
+      if (regionCount >= totalRegionNum) break;
+      LOG.debug("Waiting for there to be "+ totalRegionNum
+        +" regions, but there are " + regionCount + " right now.");
       try {
         Thread.sleep(100);
       } catch (InterruptedException e) {}
     }
   }
-
 }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestNamespace.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestNamespace.java
index 7eebf70..bdde696 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestNamespace.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestNamespace.java
@@ -37,13 +37,11 @@ import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.constraint.ConstraintException;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
-import com.google.common.collect.Sets;
 import org.junit.AfterClass;
 import org.junit.Assert;
 import org.junit.Before;
@@ -52,10 +50,7 @@ import org.junit.Ignore;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import java.io.IOException;
-import java.util.Set;
-
-import static org.junit.Assert.*;
+import com.google.common.collect.Sets;
 
 @Category(MediumTests.class)
 public class TestNamespace {
@@ -78,7 +73,7 @@ public class TestNamespace {
     cluster = TEST_UTIL.getHBaseCluster();
     master = ((MiniHBaseCluster)cluster).getMaster();
     zkNamespaceManager =
-        new ZKNamespaceManager(master.getZooKeeperWatcher());
+        new ZKNamespaceManager(master.getZooKeeper());
     zkNamespaceManager.start();
     LOG.info("Done initializing cluster");
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
index 0294b12..0b0e290 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestRegionRebalancing.java
@@ -149,9 +149,9 @@ public class TestRegionRebalancing {
 
   /** figure out how many regions are currently being served. */
   private int getRegionCount() throws IOException {
-    int total = 0;
+    int total = 0; // Regions on master are ignored since not counted for balancing
     for (HRegionServer server : getOnlineRegionServers()) {
-      total += ProtobufUtil.getOnlineRegions(server).size();
+      total += ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
     }
     return total;
   }
@@ -183,11 +183,13 @@ public class TestRegionRebalancing {
         + ", up border: " + avgLoadPlusSlop + "; attempt: " + i);
 
       for (HRegionServer server : servers) {
-        int serverLoad = ProtobufUtil.getOnlineRegions(server).size();
+        int serverLoad =
+          ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
         LOG.debug(server.getServerName() + " Avg: " + avg + " actual: " + serverLoad);
         if (!(avg > 2.0 && serverLoad <= avgLoadPlusSlop
             && serverLoad >= avgLoadMinusSlop)) {
-          for (HRegionInfo hri : ProtobufUtil.getOnlineRegions(server)) {
+          for (HRegionInfo hri :
+              ProtobufUtil.getOnlineRegions(server.getRSRpcServices())) {
             if (hri.isMetaRegion()) serverLoad--;
             // LOG.debug(hri.getRegionNameAsString());
           }
@@ -234,7 +236,7 @@ public class TestRegionRebalancing {
    * Wait until all the regions are assigned.
    */
   private void waitForAllRegionsAssigned() throws IOException {
-    int totalRegions = HBaseTestingUtility.KEYS.length+1;
+    int totalRegions = HBaseTestingUtility.KEYS.length;
     while (getRegionCount() < totalRegions) {
     // while (!cluster.getMaster().allRegionsAssigned()) {
       LOG.debug("Waiting for there to be "+ totalRegions +" regions, but there are " + getRegionCount() + " right now.");
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
index e2b4f0c..d179830 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestZooKeeper.java
@@ -106,7 +106,7 @@ public class TestZooKeeper {
    */
   @Before
   public void setUp() throws Exception {
-    TEST_UTIL.startMiniHBaseCluster(1, 2);
+    TEST_UTIL.startMiniHBaseCluster(2, 2);
   }
 
   @After
@@ -205,17 +205,14 @@ public class TestZooKeeper {
     connection.close();
   }
 
-  @Test (timeout = 60000)
+  @Test (timeout = 120000)
   public void testRegionServerSessionExpired() throws Exception {
     LOG.info("Starting testRegionServerSessionExpired");
-    int metaIndex = TEST_UTIL.getMiniHBaseCluster().getServerWithMeta();
-    TEST_UTIL.expireRegionServerSession(metaIndex);
+    TEST_UTIL.expireRegionServerSession(0);
     testSanity("testRegionServerSessionExpired");
   }
 
-  // @Test Disabled because seems to make no sense expiring master session
-  // and then trying to create table (down in testSanity); on master side
-  // it will fail because the master's session has expired -- St.Ack 07/24/2012
+  @Test(timeout = 300000)
   public void testMasterSessionExpired() throws Exception {
     LOG.info("Starting testMasterSessionExpired");
     TEST_UTIL.expireMasterSession();
@@ -227,14 +224,14 @@ public class TestZooKeeper {
    *  test differs from {@link #testMasterSessionExpired} because here
    *  the master znode will exist in ZK.
    */
-  @Test(timeout = 60000)
+  @Test(timeout = 300000)
   public void testMasterZKSessionRecoveryFailure() throws Exception {
     LOG.info("Starting testMasterZKSessionRecoveryFailure");
     MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
     HMaster m = cluster.getMaster();
     m.abort("Test recovery from zk session expired",
       new KeeperException.SessionExpiredException());
-    assertFalse(m.isStopped());
+    assertTrue(m.isStopped()); // Master doesn't recover any more
     testSanity("testMasterZKSessionRecoveryFailure");
   }
 
@@ -480,13 +477,13 @@ public class TestZooKeeper {
    * session. Without the HBASE-6046 fix master always tries to assign all the user regions by
    * calling retainAssignment.
    */
-  @Test
+  @Test(timeout = 300000)
   public void testRegionAssignmentAfterMasterRecoveryDueToZKExpiry() throws Exception {
     MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
     cluster.startRegionServer();
     cluster.waitForActiveAndReadyMaster(10000);
     HMaster m = cluster.getMaster();
-    ZooKeeperWatcher zkw = m.getZooKeeperWatcher();
+    ZooKeeperWatcher zkw = m.getZooKeeper();
     int expectedNumOfListeners = zkw.getNumberOfListeners();
     // now the cluster is up. So assign some regions.
     HBaseAdmin admin = new HBaseAdmin(TEST_UTIL.getConfiguration());
@@ -500,17 +497,17 @@ public class TestZooKeeper {
       admin.createTable(htd, SPLIT_KEYS);
       ZooKeeperWatcher zooKeeperWatcher = HBaseTestingUtility.getZooKeeperWatcher(TEST_UTIL);
       ZKAssign.blockUntilNoRIT(zooKeeperWatcher);
-      m.getZooKeeperWatcher().close();
+      m.getZooKeeper().close();
       MockLoadBalancer.retainAssignCalled = false;
       m.abort("Test recovery from zk session expired",
         new KeeperException.SessionExpiredException());
-      assertFalse(m.isStopped());
+      assertTrue(m.isStopped()); // Master doesn't recover any more
       // The recovered master should not call retainAssignment, as it is not a
       // clean startup.
       assertFalse("Retain assignment should not be called", MockLoadBalancer.retainAssignCalled);
       // number of listeners should be same as the value before master aborted
       // wait for new master is initialized
-      cluster.waitForActiveAndReadyMaster(10000);
+      cluster.waitForActiveAndReadyMaster(120000);
       assertEquals(expectedNumOfListeners, zkw.getNumberOfListeners());
     } finally {
       admin.close();
@@ -521,7 +518,7 @@ public class TestZooKeeper {
    * Tests whether the logs are split when master recovers from a expired zookeeper session and an
    * RS goes down.
    */
-  @Test(timeout = 240000)
+  @Test(timeout = 300000)
   public void testLogSplittingAfterMasterRecoveryDueToZKExpiry() throws IOException,
       KeeperException, InterruptedException {
     MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
@@ -549,10 +546,10 @@ public class TestZooKeeper {
         p.add(Bytes.toBytes("col"), Bytes.toBytes("ql"), Bytes.toBytes("value" + numberOfPuts));
         table.put(p);
       }
-      m.getZooKeeperWatcher().close();
+      m.getZooKeeper().close();
       m.abort("Test recovery from zk session expired",
         new KeeperException.SessionExpiredException());
-      assertFalse(m.isStopped());
+      assertTrue(m.isStopped()); // Master doesn't recover any more
       cluster.getRegionServer(0).abort("Aborting");
       // Without patch for HBASE-6046 this test case will always timeout
       // with patch the test case should pass.
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
index 6ed0445..8597916 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/catalog/TestMetaReaderEditor.java
@@ -25,23 +25,20 @@ import static org.junit.Assert.assertTrue;
 
 import java.io.IOException;
 import java.util.List;
-import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.Abortable;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
-import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -54,22 +51,7 @@ import org.junit.experimental.categories.Category;
 public class TestMetaReaderEditor {
   private static final Log LOG = LogFactory.getLog(TestMetaReaderEditor.class);
   private static final  HBaseTestingUtility UTIL = new HBaseTestingUtility();
-  private static ZooKeeperWatcher zkw;
   private static CatalogTracker CT;
-  private final static Abortable ABORTABLE = new Abortable() {
-    private final AtomicBoolean abort = new AtomicBoolean(false);
-
-    @Override
-    public void abort(String why, Throwable e) {
-      LOG.info(why, e);
-      abort.set(true);
-    }
-    
-    @Override
-    public boolean isAborted() {
-      return abort.get();
-    }
-  };
 
   @BeforeClass public static void beforeClass() throws Exception {
     UTIL.startMiniCluster(3);
@@ -79,13 +61,11 @@ public class TestMetaReaderEditor {
     // responsive.  1 second is default as is ten retries.
     c.setLong("hbase.client.pause", 1000);
     c.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 10);
-    zkw = new ZooKeeperWatcher(c, "TestMetaReaderEditor", ABORTABLE);
-    CT = new CatalogTracker(zkw, c, ABORTABLE);
+    CT = new CatalogTracker(c);
     CT.start();
   }
 
   @AfterClass public static void afterClass() throws Exception {
-    ABORTABLE.abort("test ending", null);
     CT.stop();
     UTIL.shutdownMiniCluster();
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/HConnectionTestingUtility.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/HConnectionTestingUtility.java
index 52fb407..778d659 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/HConnectionTestingUtility.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/HConnectionTestingUtility.java
@@ -98,7 +98,9 @@ public class HConnectionTestingUtility {
       final ClientProtos.ClientService.BlockingInterface client,
       final ServerName sn, final HRegionInfo hri)
   throws IOException {
-    ClusterConnection c = HConnectionTestingUtility.getMockedConnection(conf);
+    HConnectionImplementation c = Mockito.mock(HConnectionImplementation.class);
+    Mockito.when(c.getConfiguration()).thenReturn(conf);
+    ConnectionManager.CONNECTION_INSTANCES.put(new HConnectionKey(conf), c);
     Mockito.doNothing().when(c).close();
     // Make it so we return a particular location when asked.
     final HRegionLocation loc = new HRegionLocation(hri, sn);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
index ecacb50..82c2123 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestAdmin.java
@@ -18,7 +18,6 @@
  */
 package org.apache.hadoop.hbase.client;
 
-
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
@@ -68,7 +67,12 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.zookeeper.ZKTableReadOnly;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
-import org.junit.*;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
 import com.google.protobuf.ServiceException;
@@ -540,6 +544,11 @@ public class TestAdmin {
       }
       regs.add(entry.getKey());
     }
+    if (numRS >= 2) {
+      // Ignore the master region server,
+      // which contains less regions by intention.
+      numRS--;
+    }
     float average = (float) expectedRegions/numRS;
     int min = (int)Math.floor(average);
     int max = (int)Math.ceil(average);
@@ -834,7 +843,7 @@ public class TestAdmin {
    * @throws Exception
    * @throws IOException
    */
-  @Test (timeout=300000)
+  @Test (timeout=400000)
   public void testForceSplit() throws Exception {
     byte[][] familyNames = new byte[][] { Bytes.toBytes("cf") };
     int[] rowCounts = new int[] { 6000 };
@@ -886,7 +895,7 @@ public class TestAdmin {
    * @throws Exception
    * @throws IOException
    */
-  @Test (timeout=300000)
+  @Test (timeout=800000)
   public void testForceSplitMultiFamily() throws Exception {
     int numVersions = HColumnDescriptor.DEFAULT_VERSIONS;
 
@@ -973,7 +982,7 @@ public class TestAdmin {
     final AtomicInteger count = new AtomicInteger(0);
     Thread t = new Thread("CheckForSplit") {
       public void run() {
-        for (int i = 0; i < 20; i++) {
+        for (int i = 0; i < 45; i++) {
           try {
             sleep(1000);
           } catch (InterruptedException e) {
@@ -1299,7 +1308,7 @@ public class TestAdmin {
 
     HRegionInfo info = null;
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
-    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     for (HRegionInfo regionInfo : onlineRegions) {
       if (!regionInfo.getTable().isSystemTable()) {
         info = regionInfo;
@@ -1307,11 +1316,13 @@ public class TestAdmin {
             .getServerName().getServerName());
       }
     }
-    boolean isInList = ProtobufUtil.getOnlineRegions(rs).contains(info);
+    boolean isInList = ProtobufUtil.getOnlineRegions(
+      rs.getRSRpcServices()).contains(info);
     long timeout = System.currentTimeMillis() + 10000;
     while ((System.currentTimeMillis() < timeout) && (isInList)) {
       Thread.sleep(100);
-      isInList = ProtobufUtil.getOnlineRegions(rs).contains(info);
+      isInList = ProtobufUtil.getOnlineRegions(
+        rs.getRSRpcServices()).contains(info);
     }
 
     assertFalse("The region should not be present in online regions list.",
@@ -1325,7 +1336,7 @@ public class TestAdmin {
 
     HRegionInfo info = null;
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
-    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     for (HRegionInfo regionInfo : onlineRegions) {
       if (!regionInfo.isMetaTable()) {
         if (regionInfo.getRegionNameAsString().contains("TestHBACloseRegion1")) {
@@ -1339,7 +1350,7 @@ public class TestAdmin {
         }
       }
     }
-    onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     assertTrue("The region should be present in online regions list.",
         onlineRegions.contains(info));
   }
@@ -1352,7 +1363,7 @@ public class TestAdmin {
 
     HRegionInfo info = null;
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
-    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     for (HRegionInfo regionInfo : onlineRegions) {
       if (!regionInfo.isMetaTable()) {
 
@@ -1364,11 +1375,13 @@ public class TestAdmin {
       }
     }
 
-    boolean isInList = ProtobufUtil.getOnlineRegions(rs).contains(info);
+    boolean isInList = ProtobufUtil.getOnlineRegions(
+      rs.getRSRpcServices()).contains(info);
     long timeout = System.currentTimeMillis() + 10000;
     while ((System.currentTimeMillis() < timeout) && (isInList)) {
       Thread.sleep(100);
-      isInList = ProtobufUtil.getOnlineRegions(rs).contains(info);
+      isInList = ProtobufUtil.getOnlineRegions(
+        rs.getRSRpcServices()).contains(info);
     }
 
     assertFalse("The region should not be present in online regions list.",
@@ -1383,7 +1396,7 @@ public class TestAdmin {
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
 
     try {
-      List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+      List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
       for (HRegionInfo regionInfo : onlineRegions) {
         if (!regionInfo.isMetaTable()) {
           if (regionInfo.getRegionNameAsString()
@@ -1407,7 +1420,7 @@ public class TestAdmin {
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
 
     try {
-      List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+      List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
       for (HRegionInfo regionInfo : onlineRegions) {
         if (!regionInfo.isMetaTable()) {
           if (regionInfo.getRegionNameAsString()
@@ -1430,7 +1443,7 @@ public class TestAdmin {
     HRegionInfo info = null;
     HRegionServer rs = TEST_UTIL.getRSForFirstRegionInTable(TABLENAME);
 
-    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    List<HRegionInfo> onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     for (HRegionInfo regionInfo : onlineRegions) {
       if (!regionInfo.isMetaTable()) {
         if (regionInfo.getRegionNameAsString().contains("TestHBACloseRegion4")) {
@@ -1444,7 +1457,7 @@ public class TestAdmin {
         }
       }
     }
-    onlineRegions = ProtobufUtil.getOnlineRegions(rs);
+    onlineRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
     assertTrue("The region should be present in online regions list.",
         onlineRegions.contains(info));
   }
@@ -1683,7 +1696,7 @@ public class TestAdmin {
     TEST_UTIL.getHBaseAdmin().createTable(htd);
   }
 
-  @Test
+  @Test (timeout=300000)
   public void testIsEnabledOrDisabledOnUnknownTable() throws Exception {
     try {
       admin.isTableEnabled(Bytes.toBytes("unkownTable"));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientScannerRPCTimeout.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientScannerRPCTimeout.java
index 3f49590..f5b3f1e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientScannerRPCTimeout.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestClientScannerRPCTimeout.java
@@ -34,6 +34,8 @@ import org.apache.hadoop.hbase.ipc.RpcClient;
 import org.apache.hadoop.hbase.ipc.RpcServer;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.RSRpcServices;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.log4j.Level;
 import org.junit.AfterClass;
@@ -89,7 +91,7 @@ public class TestClientScannerRPCTimeout {
     putToTable(ht, r2);
     putToTable(ht, r3);
     LOG.info("Wrote our three values");
-    RegionServerWithScanTimeout.seqNoToSleepOn = 1;
+    RSRpcServicesWithScanTimeout.seqNoToSleepOn = 1;
     Scan scan = new Scan();
     scan.setCaching(1);
     ResultScanner scanner = ht.getScanner(scan);
@@ -100,15 +102,15 @@ public class TestClientScannerRPCTimeout {
     result = scanner.next();
     assertTrue((System.currentTimeMillis() - t1) > rpcTimeout);
     assertTrue("Expected row: row-2", Bytes.equals(r2, result.getRow()));
-    RegionServerWithScanTimeout.seqNoToSleepOn = -1;// No need of sleep
+    RSRpcServicesWithScanTimeout.seqNoToSleepOn = -1;// No need of sleep
     result = scanner.next();
     assertTrue("Expected row: row-3", Bytes.equals(r3, result.getRow()));
     scanner.close();
 
     // test the case that RPC is always timesout
     scanner = ht.getScanner(scan);
-    RegionServerWithScanTimeout.sleepAlways = true;
-    RegionServerWithScanTimeout.tryNumber = 0;
+    RSRpcServicesWithScanTimeout.sleepAlways = true;
+    RSRpcServicesWithScanTimeout.tryNumber = 0;
     try {
       result = scanner.next();
     } catch (IOException ioe) {
@@ -116,8 +118,8 @@ public class TestClientScannerRPCTimeout {
       LOG.info("Failed after maximal attempts=" + CLIENT_RETRIES_NUMBER, ioe);
     }
     assertTrue("Expected maximal try number=" + CLIENT_RETRIES_NUMBER
-        + ", actual =" + RegionServerWithScanTimeout.tryNumber,
-        RegionServerWithScanTimeout.tryNumber <= CLIENT_RETRIES_NUMBER);
+        + ", actual =" + RSRpcServicesWithScanTimeout.tryNumber,
+        RSRpcServicesWithScanTimeout.tryNumber <= CLIENT_RETRIES_NUMBER);
   }
 
   private void putToTable(HTable ht, byte[] rowkey) throws IOException {
@@ -127,15 +129,26 @@ public class TestClientScannerRPCTimeout {
   }
 
   private static class RegionServerWithScanTimeout extends MiniHBaseClusterRegionServer {
+    public RegionServerWithScanTimeout(Configuration conf)
+        throws IOException, InterruptedException {
+      super(conf);
+    }
+
+    protected RSRpcServices createRpcServices() throws IOException {
+      return new RSRpcServicesWithScanTimeout(this);
+    }
+  }
+
+  private static class RSRpcServicesWithScanTimeout extends RSRpcServices {
     private long tableScannerId;
     private boolean slept;
     private static long seqNoToSleepOn = -1;
     private static boolean sleepAlways = false;
     private static int tryNumber = 0;
 
-    public RegionServerWithScanTimeout(Configuration conf)
-    throws IOException, InterruptedException {
-      super(conf);
+    public RSRpcServicesWithScanTimeout(HRegionServer rs)
+        throws IOException {
+      super(rs);
     }
 
     @Override
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index a03e26d..428c733 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -4158,7 +4158,7 @@ public class TestFromClientSide {
     // HBaseAdmin and can connect to the new master;
     HBaseAdmin newAdmin = new HBaseAdmin(conn);
     assertTrue(newAdmin.tableExists(tableName));
-    assert(newAdmin.getClusterStatus().getServersSize() == SLAVES);
+    assertTrue(newAdmin.getClusterStatus().getServersSize() == SLAVES + 1);
   }
 
   @Test
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
index cc19937..98563d6 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
@@ -41,7 +41,6 @@ import org.apache.hadoop.hbase.Waiter;
 import org.apache.hadoop.hbase.exceptions.OperationConflictException;
 import org.apache.hadoop.hbase.ipc.RpcClient;
 import org.apache.hadoop.hbase.ipc.RpcServer;
-import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil;
 import org.apache.hadoop.hbase.util.Threads;
@@ -277,7 +276,7 @@ public class TestMultiParallel {
       // after writing successfully. It means the server we aborted is dead
       // and detected by matser
       while (liveRS.getRegionServer().getNumberOfOnlineRegions() != 0) {
-        Thread.sleep(10);
+        Thread.sleep(100);
       }
       // try putting more keys after the abort. same key/qual... just validating
       // no exceptions thrown
@@ -302,17 +301,14 @@ public class TestMultiParallel {
     LOG.info("Count=" + count);
     Assert.assertEquals("Server count=" + count + ", abort=" + doAbort,
         (doAbort ? (liveRScount - 1) : liveRScount), count);
-    for (JVMClusterUtil.RegionServerThread t: liveRSs) {
-      int regions = ProtobufUtil.getOnlineRegions(t.getRegionServer()).size();
-      // Assert.assertTrue("Count of regions=" + regions, regions > 10);
-    }
     if (doAbort) {
       UTIL.getMiniHBaseCluster().waitOnRegionServer(0);
       UTIL.waitFor(15 * 1000, new Waiter.Predicate<Exception>() {
         @Override
         public boolean evaluate() throws Exception {
+          // Master is also a regionserver, so the count is liveRScount
           return UTIL.getMiniHBaseCluster().getMaster()
-              .getClusterStatus().getServersSize() == (liveRScount - 1);
+              .getClusterStatus().getServersSize() == liveRScount;
         }
       });
       UTIL.waitFor(15 * 1000, UTIL.predicateNoRegionsInTransition());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
index 46b02a0..4e7fd23 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
@@ -472,7 +472,8 @@ public class TestScannersFromClientSide {
     byte[] regionName = hri.getRegionName();
     int i = cluster.getServerWith(regionName);
     HRegionServer rs = cluster.getRegionServer(i);
-    ProtobufUtil.closeRegion(rs, rs.getServerName(), regionName, false);
+    ProtobufUtil.closeRegion(
+      rs.getRSRpcServices(), rs.getServerName(), regionName, false);
     long startTime = EnvironmentEdgeManager.currentTimeMillis();
     long timeOut = 300000;
     while (true) {
@@ -492,7 +493,7 @@ public class TestScannersFromClientSide {
       states.regionOffline(hri);
       states.updateRegionState(hri, State.OPENING);
       ZKAssign.createNodeOffline(zkw, hri, loc.getServerName());
-      ProtobufUtil.openRegion(rs, rs.getServerName(), hri);
+      ProtobufUtil.openRegion(rs.getRSRpcServices(), rs.getServerName(), hri);
       startTime = EnvironmentEdgeManager.currentTimeMillis();
       while (true) {
         if (rs.getOnlineRegion(regionName) != null) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
index 759ccf3..5e94262 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestClassLoading.java
@@ -525,7 +525,7 @@ public class TestClassLoading {
         "[" + masterCoprocessor.getSimpleName() + "]";
     String loadedMasterCoprocessors =
         java.util.Arrays.toString(
-            TEST_UTIL.getHBaseCluster().getMaster().getCoprocessors());
+            TEST_UTIL.getHBaseCluster().getMaster().getMasterCoprocessors());
     assertEquals(loadedMasterCoprocessorsVerify, loadedMasterCoprocessors);
   }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
index dea64a0..b29bec4 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestHTableWrapper.java
@@ -129,7 +129,7 @@ public class TestHTableWrapper {
   @Test
   public void testHTableInterfaceMethods() throws Exception {
     Configuration conf = util.getConfiguration();
-    MasterCoprocessorHost cpHost = util.getMiniHBaseCluster().getMaster().getCoprocessorHost();
+    MasterCoprocessorHost cpHost = util.getMiniHBaseCluster().getMaster().getMasterCoprocessorHost();
     Class<?> implClazz = DummyRegionObserver.class;
     cpHost.load(implClazz, Coprocessor.PRIORITY_HIGHEST, conf);
     CoprocessorEnvironment env = cpHost.findCoprocessorEnvironment(implClazz.getName());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithAbort.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithAbort.java
index bd28ff0..513032a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithAbort.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithAbort.java
@@ -160,7 +160,7 @@ public class TestMasterCoprocessorExceptionWithAbort {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
 
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     BuggyMasterObserver cp = (BuggyMasterObserver)host.findCoprocessor(
         BuggyMasterObserver.class.getName());
     assertFalse("No table created yet", cp.wasCreateTableCalled());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithRemove.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithRemove.java
index 9c99650..a99dbcf 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithRemove.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterCoprocessorExceptionWithRemove.java
@@ -141,7 +141,7 @@ public class TestMasterCoprocessorExceptionWithRemove {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
 
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     BuggyMasterObserver cp = (BuggyMasterObserver)host.findCoprocessor(
         BuggyMasterObserver.class.getName());
     assertFalse("No table created yet", cp.wasCreateTableCalled());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
index 9e9bd80..4e25d8a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestMasterObserver.java
@@ -34,7 +34,16 @@ import java.util.concurrent.CountDownLatch;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.NamespaceDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.master.AssignmentManager;
@@ -42,10 +51,10 @@ import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.RegionState;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsRequest;
-import org.apache.hadoop.hbase.protobuf.RequestConverter;
-import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Threads;
@@ -968,7 +977,7 @@ public class TestMasterObserver {
 
     HMaster master = cluster.getMaster();
     assertTrue("Master should be active", master.isActiveMaster());
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     assertNotNull("CoprocessorHost should not be null", host);
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
@@ -987,7 +996,7 @@ public class TestMasterObserver {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
 
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
     cp.enableBypass(true);
@@ -1147,7 +1156,7 @@ public class TestMasterObserver {
   public void testSnapshotOperations() throws Exception {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
     cp.resetStates();
@@ -1204,7 +1213,7 @@ public class TestMasterObserver {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
     String testNamespace = "observed_ns";
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
 
@@ -1269,7 +1278,7 @@ public class TestMasterObserver {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
 
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
     cp.enableBypass(false);
@@ -1308,8 +1317,8 @@ public class TestMasterObserver {
       }
       assertTrue("Found server", found);
       LOG.info("Found " + destName);
-      master.moveRegion(null,RequestConverter.buildMoveRegionRequest(
-        firstGoodPair.getKey().getEncodedNameAsBytes(),Bytes.toBytes(destName)));
+      master.getMasterRpcServices().moveRegion(null, RequestConverter.buildMoveRegionRequest(
+          firstGoodPair.getKey().getEncodedNameAsBytes(),Bytes.toBytes(destName)));
       assertTrue("Coprocessor should have been called on region move",
         cp.wasMoveCalled());
   
@@ -1334,13 +1343,13 @@ public class TestMasterObserver {
       byte[] destRS = Bytes.toBytes(cluster.getRegionServer(1).getServerName().toString());
       //Make sure no regions are in transition now
       waitForRITtoBeZero(master);
-      List<HRegionInfo> openRegions = ProtobufUtil.getOnlineRegions(rs);
+      List<HRegionInfo> openRegions = ProtobufUtil.getOnlineRegions(rs.getRSRpcServices());
       int moveCnt = openRegions.size()/2;
       for (int i=0; i<moveCnt; i++) {
         HRegionInfo info = openRegions.get(i);
         if (!info.isMetaTable()) {
-          master.moveRegion(null,RequestConverter.buildMoveRegionRequest(
-            openRegions.get(i).getEncodedNameAsBytes(), destRS));
+          master.getMasterRpcServices().moveRegion(null, RequestConverter.buildMoveRegionRequest(
+              openRegions.get(i).getEncodedNameAsBytes(), destRS));
         }
       }
       //Make sure no regions are in transition now
@@ -1370,14 +1379,14 @@ public class TestMasterObserver {
     MiniHBaseCluster cluster = UTIL.getHBaseCluster();
 
     HMaster master = cluster.getMaster();
-    MasterCoprocessorHost host = master.getCoprocessorHost();
+    MasterCoprocessorHost host = master.getMasterCoprocessorHost();
     CPMasterObserver cp = (CPMasterObserver)host.findCoprocessor(
         CPMasterObserver.class.getName());
     cp.resetStates();
 
     GetTableDescriptorsRequest req =
         RequestConverter.buildGetTableDescriptorsRequest((List<TableName>)null);
-    master.getTableDescriptors(null, req);
+    master.getMasterRpcServices().getTableDescriptors(null, req);
 
     assertTrue("Coprocessor should be called on table descriptors request",
       cp.wasGetTableDescriptorsCalled());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
index f0eeeea..c8aa3ab 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverInterface.java
@@ -586,7 +586,7 @@ public class TestRegionObserverInterface {
         if (!t.isAlive() || t.getRegionServer().isAborted() || t.getRegionServer().isStopping()){
           continue;
         }
-        for (HRegionInfo r : ProtobufUtil.getOnlineRegions(t.getRegionServer())) {
+        for (HRegionInfo r : ProtobufUtil.getOnlineRegions(t.getRegionServer().getRSRpcServices())) {
           if (!r.getTable().equals(tableName)) {
             continue;
           }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithAbort.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithAbort.java
index 4029d55..d16bfd7 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithAbort.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerCoprocessorExceptionWithAbort.java
@@ -64,7 +64,7 @@ public class TestRegionServerCoprocessorExceptionWithAbort {
       final HRegionServer regionServer = cluster.getRegionServer(0);
       conf.set(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY,
         FailedInitializationObserver.class.getName());
-      regionServer.getCoprocessorHost().loadSystemCoprocessors(conf,
+      regionServer.getRegionServerCoprocessorHost().loadSystemCoprocessors(conf,
         CoprocessorHost.REGION_COPROCESSOR_CONF_KEY);
       TEST_UTIL.waitFor(10000, 1000, new Predicate<Exception>() {
         @Override
@@ -98,20 +98,16 @@ public class TestRegionServerCoprocessorExceptionWithAbort {
       // Note which regionServer will abort (after put is attempted).
       final HRegionServer regionServer = TEST_UTIL.getRSForFirstRegionInTable(TABLE_NAME);
 
-      boolean threwIOE = false;
       try {
         final byte[] ROW = Bytes.toBytes("aaa");
         Put put = new Put(ROW);
         put.add(TEST_FAMILY, ROW, ROW);
         table.put(put);
         table.flushCommits();
-        // We may need two puts to reliably get an exception
-        table.put(put);
-        table.flushCommits();
       } catch (IOException e) {
-        threwIOE = true;
-      } finally {
-        assertTrue("The regionserver should have thrown an exception", threwIOE);
+        // The region server is going to be aborted.
+        // We may get an exception if we retry,
+        // which is not guaranteed.
       }
 
       // Wait 10 seconds for the regionserver to abort: expected result is that
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
index 6fbae5d..ada3e6f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionServerObserver.java
@@ -31,17 +31,13 @@ import org.apache.hadoop.hbase.Coprocessor;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
-import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.Mutation;
-import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.RegionMergeTransaction;
@@ -82,7 +78,7 @@ public class TestRegionServerObserver {
     try {
       MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
       HRegionServer regionServer = cluster.getRegionServer(0);
-      RegionServerCoprocessorHost cpHost = regionServer.getCoprocessorHost();
+      RegionServerCoprocessorHost cpHost = regionServer.getRegionServerCoprocessorHost();
       Coprocessor coprocessor = cpHost.findCoprocessor(CPRegionServerObserver.class.getName());
       CPRegionServerObserver regionServerObserver = (CPRegionServerObserver) coprocessor;
       HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(TABLENAME));
@@ -115,7 +111,6 @@ public class TestRegionServerObserver {
     private RegionMergeTransaction rmt = null;
     private HRegion mergedRegion = null;
 
-    private boolean bypass = false;
     private boolean preMergeCalled;
     private boolean preMergeBeforePONRCalled;
     private boolean preMergeAfterPONRCalled;
@@ -123,10 +118,6 @@ public class TestRegionServerObserver {
     private boolean postRollBackMergeCalled;
     private boolean postMergeCalled;
 
-    public void enableBypass(boolean bypass) {
-      this.bypass = bypass;
-    }
-
     public void resetStates() {
       preMergeCalled = false;
       preMergeBeforePONRCalled = false;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java
index 325eb2d..73e0e65 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/fs/TestBlockReorder.java
@@ -44,7 +44,6 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtil;
 import org.apache.hadoop.hbase.util.FSUtils;
-import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hdfs.DFSClient;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
@@ -244,19 +243,18 @@ public class TestBlockReorder {
     byte[] sb = "sb".getBytes();
     htu.startMiniZKCluster();
 
-    MiniHBaseCluster hbm = htu.startMiniHBaseCluster(1, 1);
+    MiniHBaseCluster hbm = htu.startMiniHBaseCluster(1, 0);
     hbm.waitForActiveAndReadyMaster();
-    hbm.getRegionServer(0).waitForServerOnline();
+    HRegionServer targetRs = hbm.getMaster();
 
     // We want to have a datanode with the same name as the region server, so
     //  we're going to get the regionservername, and start a new datanode with this name.
-    String host4 = hbm.getRegionServer(0).getServerName().getHostname();
+    String host4 = targetRs.getServerName().getHostname();
     LOG.info("Starting a new datanode with the name=" + host4);
     cluster.startDataNodes(conf, 1, true, null, new String[]{"/r4"}, new String[]{host4}, null);
     cluster.waitClusterUp();
 
     final int repCount = 3;
-    HRegionServer targetRs = hbm.getRegionServer(0);
 
     // We use the regionserver file system & conf as we expect it to have the hook.
     conf = targetRs.getConfiguration();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
index 5828b8a..f302f10 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestLoadIncrementalHFilesSplitRecovery.java
@@ -183,10 +183,11 @@ public class TestLoadIncrementalHFilesSplitRecovery {
       HRegionServer hrs = util.getRSForFirstRegionInTable(Bytes
           .toBytes(table));
 
-      for (HRegionInfo hri : ProtobufUtil.getOnlineRegions(hrs)) {
+      for (HRegionInfo hri :
+          ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices())) {
         if (Bytes.equals(hri.getTable().getName(), Bytes.toBytes(table))) {
           // splitRegion doesn't work if startkey/endkey are null
-          ProtobufUtil.split(hrs, hri, rowkey(ROWCOUNT / 2)); // hard code split
+          ProtobufUtil.split(hrs.getRSRpcServices(), hri, rowkey(ROWCOUNT / 2)); // hard code split
         }
       }
 
@@ -194,7 +195,8 @@ public class TestLoadIncrementalHFilesSplitRecovery {
       int regions;
       do {
         regions = 0;
-        for (HRegionInfo hri : ProtobufUtil.getOnlineRegions(hrs)) {
+        for (HRegionInfo hri :
+            ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices())) {
           if (Bytes.equals(hri.getTable().getName(), Bytes.toBytes(table))) {
             regions++;
           }
@@ -298,6 +300,7 @@ public class TestLoadIncrementalHFilesSplitRecovery {
     fail("doBulkLoad should have thrown an exception");
   }
 
+  @SuppressWarnings("deprecation")
   private HConnection getMockedConnection(final Configuration conf)
   throws IOException, ServiceException {
     HConnection c = Mockito.mock(HConnection.class);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
index 45b9885..c98aad4 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
@@ -18,23 +18,22 @@
 package org.apache.hadoop.hbase.master;
 
 import java.io.IOException;
+import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.net.InetSocketAddress;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
 import java.util.TreeMap;
 import java.util.concurrent.ConcurrentSkipListMap;
 
-import com.google.protobuf.Message;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.CellScannable;
 import org.apache.hadoop.hbase.CellUtil;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.client.Get;
@@ -84,7 +83,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.RPCProtos;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -546,11 +544,6 @@ ClientProtos.ClientService.BlockingInterface, RegionServerServices {
   }
 
   @Override
-  public int getPriority(RPCProtos.RequestHeader header, Message param) {
-    return 0;
-  }
-
-  @Override
   public UpdateFavoredNodesResponse updateFavoredNodes(RpcController controller,
       UpdateFavoredNodesRequest request) throws ServiceException {
     return null;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/Mocking.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/Mocking.java
index d396867..10127c8 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/Mocking.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/Mocking.java
@@ -73,36 +73,6 @@ public class Mocking {
   }
 
   /**
-   * Fakes the regionserver-side zk transitions of a region open.
-   * @param w ZooKeeperWatcher to use.
-   * @param sn Name of the regionserver doing the 'opening'
-   * @param hri Region we're 'opening'.
-   * @throws KeeperException
-   * @throws DeserializationException
-   */
-  static void fakeRegionServerRegionOpenInZK(HMaster master,  final ZooKeeperWatcher w,
-      final ServerName sn, final HRegionInfo hri)
-    throws KeeperException, DeserializationException, InterruptedException {
-    // Wait till the we region is ready to be open in RIT.
-    waitForRegionPendingOpenInRIT(master.getAssignmentManager(), hri.getEncodedName());
-
-    // Get current versionid else will fail on transition from OFFLINE to OPENING below
-    int versionid = ZKAssign.getVersion(w, hri);
-    assertNotSame(-1, versionid);
-    // This uglyness below is what the openregionhandler on RS side does.  I
-    // looked at exposing the method over in openregionhandler but its just a
-    // one liner and its deep over in another package so just repeat it below.
-    versionid = ZKAssign.transitionNode(w, hri, sn,
-      EventType.M_ZK_REGION_OFFLINE, EventType.RS_ZK_REGION_OPENING, versionid);
-    assertNotSame(-1, versionid);
-    // Move znode from OPENING to OPENED as RS does on successful open.
-    versionid = ZKAssign.transitionNodeOpened(w, hri, sn, versionid);
-    assertNotSame(-1, versionid);
-    // We should be done now.  The master open handler will notice the
-    // transition and remove this regions znode.
-  }
-
-  /**
    * Verifies that the specified region is in the specified state in ZooKeeper.
    * <p>
    * Returns true if region is in transition and in the specified state in
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java
index bbb9337..994a26e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestActiveMasterManager.java
@@ -86,7 +86,7 @@ public class TestActiveMasterManager {
     MonitoredTask status = Mockito.mock(MonitoredTask.class);
     clusterStatusTracker.setClusterUp();
 
-    activeMasterManager.blockUntilBecomingActiveMaster(status);
+    activeMasterManager.blockUntilBecomingActiveMaster(100, status);
     assertTrue(activeMasterManager.clusterHasActiveMaster.get());
     assertMaster(zk, master);
 
@@ -95,7 +95,7 @@ public class TestActiveMasterManager {
     ActiveMasterManager secondActiveMasterManager =
       secondDummyMaster.getActiveMasterManager();
     assertFalse(secondActiveMasterManager.clusterHasActiveMaster.get());
-    activeMasterManager.blockUntilBecomingActiveMaster(status);
+    activeMasterManager.blockUntilBecomingActiveMaster(100, status);
     assertTrue(activeMasterManager.clusterHasActiveMaster.get());
     assertMaster(zk, master);
   }
@@ -130,7 +130,7 @@ public class TestActiveMasterManager {
     ClusterStatusTracker clusterStatusTracker =
       ms1.getClusterStatusTracker();
     clusterStatusTracker.setClusterUp();
-    activeMasterManager.blockUntilBecomingActiveMaster(
+    activeMasterManager.blockUntilBecomingActiveMaster(100, 
         Mockito.mock(MonitoredTask.class));
     assertTrue(activeMasterManager.clusterHasActiveMaster.get());
     assertMaster(zk, firstMasterAddress);
@@ -212,7 +212,7 @@ public class TestActiveMasterManager {
 
     @Override
     public void run() {
-      manager.blockUntilBecomingActiveMaster(
+      manager.blockUntilBecomingActiveMaster(100,
           Mockito.mock(MonitoredTask.class));
       LOG.info("Second master has become the active master!");
       isActiveMaster = true;
@@ -308,6 +308,5 @@ public class TestActiveMasterManager {
       return activeMasterManager;
     }
   }
-
 }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index 8f0ae98..2dc0eaf 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -197,7 +197,7 @@ public class TestCatalogJanitor {
     private final AssignmentManager asm;
 
     MockMasterServices(final Server server) throws IOException {
-      this.mfs = new MasterFileSystem(server, this, false);
+      this.mfs = new MasterFileSystem(server, this);
       this.asm = Mockito.mock(AssignmentManager.class);
     }
 
@@ -228,7 +228,7 @@ public class TestCatalogJanitor {
     }
 
     @Override
-    public MasterCoprocessorHost getCoprocessorHost() {
+    public MasterCoprocessorHost getMasterCoprocessorHost() {
       return null;
     }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
index 6fff11e..363d6cd 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestDistributedLogSplitting.java
@@ -25,7 +25,6 @@ import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_acquired;
 import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_done;
 import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_err;
 import static org.apache.hadoop.hbase.SplitLogCounters.tot_wkr_task_resigned;
-import static org.junit.Assert.*;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
@@ -40,7 +39,6 @@ import java.util.LinkedList;
 import java.util.List;
 import java.util.NavigableSet;
 import java.util.Set;
-import java.util.TreeSet;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
@@ -55,10 +53,9 @@ import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
@@ -68,10 +65,11 @@ import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.NamespaceDescriptor;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.SplitLogCounters;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.Waiter;
+import org.apache.hadoop.hbase.client.ConnectionUtils;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
-import org.apache.hadoop.hbase.client.ConnectionUtils;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Increment;
 import org.apache.hadoop.hbase.client.NonceGenerator;
@@ -113,6 +111,7 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
 @Category(LargeTests.class)
+@SuppressWarnings("deprecation")
 public class TestDistributedLogSplitting {
   private static final Log LOG = LogFactory.getLog(TestSplitLogManager.class);
   static {
@@ -148,6 +147,7 @@ public class TestDistributedLogSplitting {
   public static void tearDown() throws IOException {
     TEST_UTIL.shutdownMiniZKCluster();
     TEST_UTIL.shutdownMiniDFSCluster();
+    TEST_UTIL.shutdownMiniHBaseCluster();
   }
 
   private void startCluster(int num_rs) throws Exception {
@@ -159,6 +159,7 @@ public class TestDistributedLogSplitting {
     conf.setInt(HConstants.REGIONSERVER_INFO_PORT, -1);
     conf.setFloat(HConstants.LOAD_BALANCER_SLOP_KEY, (float) 100.0); // no load balancing
     conf.setInt("hbase.regionserver.wal.max.splitters", 3);
+    TEST_UTIL.shutdownMiniHBaseCluster();
     TEST_UTIL = new HBaseTestingUtility(conf);
     TEST_UTIL.setDFSCluster(dfsCluster);
     TEST_UTIL.setZkCluster(zkCluster);
@@ -219,7 +220,7 @@ public class TestDistributedLogSplitting {
     for (int i = 0; i < NUM_RS; i++) {
       boolean foundRs = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.getTable().getNameAsString().equalsIgnoreCase("table")) {
           foundRs = true;
@@ -248,7 +249,6 @@ public class TestDistributedLogSplitting {
     for (HRegionInfo hri : regions) {
 
       Path tdir = FSUtils.getTableDir(rootdir, table);
-      @SuppressWarnings("deprecation")
       Path editsdir =
         HLogUtil.getRegionDirRecoveredEditsDir(HRegion.getRegionDir(tdir, hri.getEncodedName()));
       LOG.debug("checking edits dir " + editsdir);
@@ -279,7 +279,7 @@ public class TestDistributedLogSplitting {
     HTable ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
 
     HRegionServer hrs = findRSToKill(false, "table");
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     makeHLog(hrs.getWAL(), regions, "table", "family", NUM_LOG_LINES, 100);
 
     // wait for abort completes
@@ -328,7 +328,7 @@ public class TestDistributedLogSplitting {
       List<Increment> reqs = new ArrayList<Increment>();
       for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
         HRegionServer hrs = rst.getRegionServer();
-        List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs);
+        List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
         for (HRegionInfo hri : hris) {
           if (TABLE_NAME.equalsIgnoreCase(hri.getTable().getNameAsString())) {
             byte[] key = hri.getStartKey();
@@ -377,7 +377,7 @@ public class TestDistributedLogSplitting {
     HTable ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
 
     HRegionServer hrs = findRSToKill(true, "table");
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     makeHLog(hrs.getWAL(), regions, "table", "family", NUM_LOG_LINES, 100);
 
     this.abortRSAndVerifyRecovery(hrs, ht, zkw, NUM_REGIONS_TO_CREATE, NUM_LOG_LINES);
@@ -412,7 +412,8 @@ public class TestDistributedLogSplitting {
     TEST_UTIL.waitFor(180000, 200, new Waiter.Predicate<Exception>() {
       @Override
       public boolean evaluate() throws Exception {
-        return (getAllOnlineRegions(tmpCluster).size() >= (numRegions + 1));
+        return (HBaseTestingUtility.getAllOnlineRegions(tmpCluster).size()
+            >= (numRegions + 1));
       }
     });
 
@@ -440,12 +441,11 @@ public class TestDistributedLogSplitting {
     // they will consume recovered.edits
     master.balanceSwitch(false);
 
-    List<RegionServerThread> rsts = cluster.getLiveRegionServerThreads();
     final ZooKeeperWatcher zkw = new ZooKeeperWatcher(conf, "table-creation", null);
     HTable ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
 
     HRegionServer hrs = findRSToKill(false, "table");
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     makeHLog(hrs.getWAL(), regions, "table", "family", NUM_LOG_LINES, 100);
 
     // abort master
@@ -464,20 +464,20 @@ public class TestDistributedLogSplitting {
     });
 
     Thread.sleep(2000);
-    LOG.info("Current Open Regions:" + getAllOnlineRegions(cluster).size());
-    
-    startMasterAndWaitUntilLogSplit(cluster);
-    
+    LOG.info("Current Open Regions:"
+        + HBaseTestingUtility.getAllOnlineRegions(cluster).size());
+
     // wait for abort completes
     TEST_UTIL.waitFor(120000, 200, new Waiter.Predicate<Exception>() {
       @Override
       public boolean evaluate() throws Exception {
-        return (getAllOnlineRegions(cluster).size() >= (NUM_REGIONS_TO_CREATE + 1));
+        return (HBaseTestingUtility.getAllOnlineRegions(cluster).size()
+          >= (NUM_REGIONS_TO_CREATE + 1));
       }
     });
 
     LOG.info("Current Open Regions After Master Node Starts Up:"
-        + getAllOnlineRegions(cluster).size());
+        + HBaseTestingUtility.getAllOnlineRegions(cluster).size());
 
     assertEquals(NUM_LOG_LINES, TEST_UTIL.countRows(ht));
 
@@ -498,12 +498,11 @@ public class TestDistributedLogSplitting {
     // they will consume recovered.edits
     master.balanceSwitch(false);
 
-    List<RegionServerThread> rsts = cluster.getLiveRegionServerThreads();
     final ZooKeeperWatcher zkw = new ZooKeeperWatcher(conf, "table-creation", null);
     HTable ht = installTable(zkw, "table", "family", NUM_REGIONS_TO_CREATE);
 
     HRegionServer hrs = findRSToKill(false, "table");
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     makeHLog(hrs.getWAL(), regions, "table", "family", NUM_LOG_LINES, 100);
 
     // abort master
@@ -522,9 +521,8 @@ public class TestDistributedLogSplitting {
     });
 
     Thread.sleep(2000);
-    LOG.info("Current Open Regions:" + getAllOnlineRegions(cluster).size());
-
-    startMasterAndWaitUntilLogSplit(cluster);
+    LOG.info("Current Open Regions:"
+        + HBaseTestingUtility.getAllOnlineRegions(cluster).size());
 
     // wait for all regions are fully recovered
     TEST_UTIL.waitFor(180000, 200, new Waiter.Predicate<Exception>() {
@@ -537,7 +535,7 @@ public class TestDistributedLogSplitting {
     });
 
     LOG.info("Current Open Regions After Master Node Starts Up:"
-        + getAllOnlineRegions(cluster).size());
+        + HBaseTestingUtility.getAllOnlineRegions(cluster).size());
 
     assertEquals(NUM_LOG_LINES, TEST_UTIL.countRows(ht));
 
@@ -563,7 +561,7 @@ public class TestDistributedLogSplitting {
 
     List<HRegionInfo> regions = null;
     HRegionServer hrs1 = findRSToKill(false, "table");
-    regions = ProtobufUtil.getOnlineRegions(hrs1);
+    regions = ProtobufUtil.getOnlineRegions(hrs1.getRSRpcServices());
 
     makeHLog(hrs1.getWAL(), regions, "table", "family", NUM_LOG_LINES, 100);
 
@@ -583,7 +581,8 @@ public class TestDistributedLogSplitting {
     TEST_UTIL.waitFor(180000, 200, new Waiter.Predicate<Exception>() {
       @Override
       public boolean evaluate() throws Exception {
-        return (getAllOnlineRegions(cluster).size() >= (NUM_REGIONS_TO_CREATE + 1));
+        return (HBaseTestingUtility.getAllOnlineRegions(cluster).size()
+            >= (NUM_REGIONS_TO_CREATE + 1));
       }
     });
 
@@ -607,7 +606,8 @@ public class TestDistributedLogSplitting {
     TEST_UTIL.waitFor(180000, 200, new Waiter.Predicate<Exception>() {
       @Override
       public boolean evaluate() throws Exception {
-        return (getAllOnlineRegions(cluster).size() >= (NUM_REGIONS_TO_CREATE + 1));
+        return (HBaseTestingUtility.getAllOnlineRegions(cluster).size()
+            >= (NUM_REGIONS_TO_CREATE + 1));
       }
     });
 
@@ -633,7 +633,7 @@ public class TestDistributedLogSplitting {
     startCluster(NUM_RS);
     master.balanceSwitch(false);
     List<RegionServerThread> rsts = cluster.getLiveRegionServerThreads();
-    final ZooKeeperWatcher zkw = master.getZooKeeperWatcher();
+    final ZooKeeperWatcher zkw = master.getZooKeeper();
     HTable ht = installTable(zkw, "table", "family", 40);
     final SplitLogManager slm = master.getMasterFileSystem().splitLogManager;
 
@@ -644,7 +644,7 @@ public class TestDistributedLogSplitting {
     ServerName secondFailedServer = null;
     for (int i = 0; i < NUM_RS; i++) {
       hrs = rsts.get(i).getRegionServer();
-      List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+      List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       if (regions.isEmpty()) continue;
       region = regions.get(0);
       regionSet.add(region);
@@ -670,7 +670,6 @@ public class TestDistributedLogSplitting {
       }
     });
     ht.close();
-    zkw.close();
   }
 
   @Test(timeout = 300000)
@@ -692,7 +691,7 @@ public class TestDistributedLogSplitting {
     for (int i = 0; i < NUM_RS; i++) {
       boolean isCarryingMeta = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.isMetaRegion()) {
           isCarryingMeta = true;
@@ -744,7 +743,7 @@ public class TestDistributedLogSplitting {
       hasRegionsForBothTables = false;
       boolean isCarryingSystem = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.getTable().isSystemTable()) {
           isCarryingSystem = true;
@@ -800,7 +799,8 @@ public class TestDistributedLogSplitting {
     TEST_UTIL.waitFor(180000, 200, new Waiter.Predicate<Exception>() {
       @Override
       public boolean evaluate() throws Exception {
-        return (getAllOnlineRegions(cluster).size() >= (NUM_REGIONS_TO_CREATE + 1));
+        return (HBaseTestingUtility.getAllOnlineRegions(cluster).size()
+            >= (NUM_REGIONS_TO_CREATE + 1));
       }
     });
 
@@ -821,7 +821,6 @@ public class TestDistributedLogSplitting {
     Path rootdir = FSUtils.getRootDir(conf);
     Path tdir = FSUtils.getTableDir(rootdir, TableName.valueOf("disableTable"));
     for (HRegionInfo hri : regions) {
-      @SuppressWarnings("deprecation")
       Path editsdir =
         HLogUtil.getRegionDirRecoveredEditsDir(HRegion.getRegionDir(tdir, hri.getEncodedName()));
       LOG.debug("checking edits dir " + editsdir);
@@ -843,7 +842,6 @@ public class TestDistributedLogSplitting {
     
     // clean up
     for (HRegionInfo hri : regions) {
-      @SuppressWarnings("deprecation")
       Path editsdir =
         HLogUtil.getRegionDirRecoveredEditsDir(HRegion.getRegionDir(tdir, hri.getEncodedName()));
       fs.delete(editsdir, true);
@@ -876,7 +874,7 @@ public class TestDistributedLogSplitting {
     HRegionServer dstRS = null;
     for (int i = 0; i < NUM_RS; i++) {
       hrs = rsts.get(i).getRegionServer();
-      List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+      List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       if (regions.isEmpty()) continue;
       region = regions.get(0);
       regionSet.add(region);
@@ -910,7 +908,6 @@ public class TestDistributedLogSplitting {
       Put put = new Put(key);
       put.add(Bytes.toBytes("family"), Bytes.toBytes("c1"), new byte[]{'b'});
       ht.put(put);
-      ht.close();
     } catch (IOException ioe) {
       Assert.assertTrue(ioe instanceof RetriesExhaustedWithDetailsException);
       RetriesExhaustedWithDetailsException re = (RetriesExhaustedWithDetailsException) ioe;
@@ -926,6 +923,7 @@ public class TestDistributedLogSplitting {
         foundRegionInRecoveryException);
     }
 
+    ht.close();
     zkw.close();
   }
 
@@ -955,8 +953,8 @@ public class TestDistributedLogSplitting {
     installTable(new ZooKeeperWatcher(conf, "table-creation", null),
         "table", "family", 40);
 
-    makeHLog(hrs.getWAL(), ProtobufUtil.getOnlineRegions(hrs), "table", "family", NUM_LOG_LINES,
-      100);
+    makeHLog(hrs.getWAL(), ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices()),
+      "table", "family", NUM_LOG_LINES, 100);
 
     new Thread() {
       public void run() {
@@ -1025,7 +1023,8 @@ public class TestDistributedLogSplitting {
     }
 
     start = EnvironmentEdgeManager.currentTimeMillis();
-    while (getAllOnlineRegions(cluster).size() < (NUM_REGIONS_TO_CREATE + 1)) {
+    while (HBaseTestingUtility.getAllOnlineRegions(cluster).size()
+        < (NUM_REGIONS_TO_CREATE + 1)) {
       if (EnvironmentEdgeManager.currentTimeMillis() - start > 60000) {
         assertTrue("Timedout", false);
       }
@@ -1124,11 +1123,10 @@ public class TestDistributedLogSplitting {
     // they will consume recovered.edits
     master.balanceSwitch(false);
     final ZooKeeperWatcher zkw = new ZooKeeperWatcher(conf, "table-creation", null);
-    List<RegionServerThread> rsts = cluster.getLiveRegionServerThreads();
 
     // only testing meta recovery in ZK operation
     HRegionServer hrs = findRSToKill(true, null);
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
 
     LOG.info("#regions = " + regions.size());
     Set<HRegionInfo> tmpRegions = new HashSet<HRegionInfo>();
@@ -1187,7 +1185,7 @@ public class TestDistributedLogSplitting {
     for (int i = 0; i < NUM_RS; i++) {
       boolean isCarryingMeta = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.isMetaRegion()) {
           isCarryingMeta = true;
@@ -1279,7 +1277,7 @@ public class TestDistributedLogSplitting {
     for (int i = 0; i < NUM_RS; i++) {
       boolean isCarryingMeta = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.isMetaRegion()) {
           isCarryingMeta = true;
@@ -1377,7 +1375,7 @@ public class TestDistributedLogSplitting {
     TEST_UTIL.getHBaseAdmin().disableTable(table);
     LOG.debug("Waiting for no more RIT\n");
     blockUntilNoRIT(zkw, master);
-    NavigableSet<String> regions = getAllOnlineRegions(cluster);
+    NavigableSet<String> regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
     LOG.debug("Verifying only catalog and namespace regions are assigned\n");
     if (regions.size() != 2) {
       for (String oregion : regions)
@@ -1389,7 +1387,7 @@ public class TestDistributedLogSplitting {
     LOG.debug("Waiting for no more RIT\n");
     blockUntilNoRIT(zkw, master);
     LOG.debug("Verifying there are " + numRegions + " assigned on cluster\n");
-    regions = getAllOnlineRegions(cluster);
+    regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
     assertEquals(numRegions + 2 + existingRegions, regions.size());
     return ht;
   }
@@ -1402,7 +1400,7 @@ public class TestDistributedLogSplitting {
 
     for (RegionServerThread rst : rsts) {
       HRegionServer hrs = rst.getRegionServer();
-      List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs);
+      List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo hri : hris) {
         if (hri.getTable().isSystemTable()) {
           continue;
@@ -1414,6 +1412,21 @@ public class TestDistributedLogSplitting {
         putData(region, hri.getStartKey(), nrows, Bytes.toBytes("q"), family);
       }
     }
+
+    for (MasterThread mt : cluster.getLiveMasterThreads()) {
+      HRegionServer hrs = mt.getMaster();
+      List<HRegionInfo> hris = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
+      for (HRegionInfo hri : hris) {
+        if (hri.getTable().isSystemTable()) {
+          continue;
+        }
+        LOG.debug("adding data to rs = " + mt.getName() +
+            " region = "+ hri.getRegionNameAsString());
+        HRegion region = hrs.getOnlineRegion(hri.getRegionName());
+        assertTrue(region != null);
+        putData(region, hri.getStartKey(), nrows, Bytes.toBytes("q"), family);
+      }
+    }
   }
 
   public void makeHLog(HLog log, List<HRegionInfo> regions, String tname, String fname,
@@ -1542,17 +1555,6 @@ public class TestDistributedLogSplitting {
     t.flushCommits();
   }
 
-  private NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)
-      throws IOException {
-    NavigableSet<String> online = new TreeSet<String>();
-    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
-      for (HRegionInfo region : ProtobufUtil.getOnlineRegions(rst.getRegionServer())) {
-        online.add(region.getRegionNameAsString());
-      }
-    }
-    return online;
-  }
-
   private void waitForCounter(AtomicLong ctr, long oldval, long newval,
       long timems) {
     long curt = System.currentTimeMillis();
@@ -1580,19 +1582,6 @@ public class TestDistributedLogSplitting {
     LOG.debug("Master is aborted");
   }
 
-  private void startMasterAndWaitUntilLogSplit(MiniHBaseCluster cluster)
-      throws IOException, InterruptedException {
-    cluster.startMaster();
-    HMaster master = cluster.getMaster();
-    while (!master.isInitialized()) {
-      Thread.sleep(100);
-    }
-    ServerManager serverManager = master.getServerManager();
-    while (serverManager.areDeadServersInProgress()) {
-      Thread.sleep(100);
-    }
-  }
-
   /**
    * Find a RS that has regions of a table.
    * @param hasMetaRegion when true, the returned RS has hbase:meta region as well
@@ -1610,7 +1599,7 @@ public class TestDistributedLogSplitting {
       boolean isCarryingMeta = false;
       boolean foundTableRegion = false;
       hrs = rsts.get(i).getRegionServer();
-      regions = ProtobufUtil.getOnlineRegions(hrs);
+      regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
       for (HRegionInfo region : regions) {
         if (region.isMetaRegion()) {
           isCarryingMeta = true;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
index 07be41d..af33ea7 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterFailover.java
@@ -61,6 +61,7 @@ import org.apache.hadoop.hbase.util.JVMClusterUtil;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
 import org.apache.hadoop.hbase.util.Threads;
+import org.apache.hadoop.hbase.zookeeper.MetaRegionTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.hadoop.hbase.zookeeper.ZKTable;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
@@ -284,6 +285,20 @@ public class TestMasterFailover {
      * javadoc.
      */
 
+    // Master is down, so is the meta. We need to assign it somewhere
+    // so that regions can be assigned during the mocking phase.
+    ZKAssign.createNodeOffline(
+      zkw, HRegionInfo.FIRST_META_REGIONINFO, hrs.getServerName());
+    ProtobufUtil.openRegion(hrs.getRSRpcServices(),
+      hrs.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
+    while (true) {
+      ServerName sn = MetaRegionTracker.getMetaRegionLocation(zkw);
+      if (sn != null && sn.equals(hrs.getServerName())) {
+        break;
+      }
+      Thread.sleep(100);
+    }
+
     List<HRegionInfo> regionsThatShouldBeOnline = new ArrayList<HRegionInfo>();
     List<HRegionInfo> regionsThatShouldBeOffline = new ArrayList<HRegionInfo>();
 
@@ -337,7 +352,7 @@ public class TestMasterFailover {
     region = enabledRegions.remove(0);
     regionsThatShouldBeOnline.add(region);
     ZKAssign.createNodeOffline(zkw, region, serverName);
-    ProtobufUtil.openRegion(hrs, hrs.getServerName(), region);
+    ProtobufUtil.openRegion(hrs.getRSRpcServices(), hrs.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -352,7 +367,7 @@ public class TestMasterFailover {
     region = disabledRegions.remove(0);
     regionsThatShouldBeOffline.add(region);
     ZKAssign.createNodeOffline(zkw, region, serverName);
-    ProtobufUtil.openRegion(hrs, hrs.getServerName(), region);
+    ProtobufUtil.openRegion(hrs.getRSRpcServices(), hrs.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -407,7 +422,8 @@ public class TestMasterFailover {
     Set<HRegionInfo> onlineRegions = new TreeSet<HRegionInfo>();
     for (JVMClusterUtil.RegionServerThread rst :
       cluster.getRegionServerThreads()) {
-      onlineRegions.addAll(ProtobufUtil.getOnlineRegions(rst.getRegionServer()));
+      onlineRegions.addAll(ProtobufUtil.getOnlineRegions(
+        rst.getRegionServer().getRSRpcServices()));
     }
 
     // Now, everything that should be online should be online
@@ -658,6 +674,20 @@ public class TestMasterFailover {
      * javadoc.
      */
 
+    // Master is down, so is the meta. We need to assign it somewhere
+    // so that regions can be assigned during the mocking phase.
+    ZKAssign.createNodeOffline(
+      zkw, HRegionInfo.FIRST_META_REGIONINFO, hrs.getServerName());
+    ProtobufUtil.openRegion(hrs.getRSRpcServices(),
+      hrs.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
+    while (true) {
+      ServerName sn = MetaRegionTracker.getMetaRegionLocation(zkw);
+      if (sn != null && sn.equals(hrs.getServerName())) {
+        break;
+      }
+      Thread.sleep(100);
+    }
+
     List<HRegionInfo> regionsThatShouldBeOnline = new ArrayList<HRegionInfo>();
     List<HRegionInfo> regionsThatShouldBeOffline = new ArrayList<HRegionInfo>();
 
@@ -736,7 +766,8 @@ public class TestMasterFailover {
     region = enabledRegions.remove(0);
     regionsThatShouldBeOnline.add(region);
     ZKAssign.createNodeOffline(zkw, region, deadServerName);
-    ProtobufUtil.openRegion(hrsDead, hrsDead.getServerName(), region);
+    ProtobufUtil.openRegion(hrsDead.getRSRpcServices(),
+      hrsDead.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -752,7 +783,8 @@ public class TestMasterFailover {
     region = disabledRegions.remove(0);
     regionsThatShouldBeOffline.add(region);
     ZKAssign.createNodeOffline(zkw, region, deadServerName);
-    ProtobufUtil.openRegion(hrsDead, hrsDead.getServerName(), region);
+    ProtobufUtil.openRegion(hrsDead.getRSRpcServices(),
+      hrsDead.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -772,7 +804,8 @@ public class TestMasterFailover {
     region = enabledRegions.remove(0);
     regionsThatShouldBeOnline.add(region);
     ZKAssign.createNodeOffline(zkw, region, deadServerName);
-    ProtobufUtil.openRegion(hrsDead, hrsDead.getServerName(), region);
+    ProtobufUtil.openRegion(hrsDead.getRSRpcServices(),
+      hrsDead.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -790,7 +823,8 @@ public class TestMasterFailover {
     region = disabledRegions.remove(0);
     regionsThatShouldBeOffline.add(region);
     ZKAssign.createNodeOffline(zkw, region, deadServerName);
-    ProtobufUtil.openRegion(hrsDead, hrsDead.getServerName(), region);
+    ProtobufUtil.openRegion(hrsDead.getRSRpcServices(),
+      hrsDead.getServerName(), region);
     while (true) {
       byte [] bytes = ZKAssign.getData(zkw, region.getEncodedName());
       RegionTransition rt = RegionTransition.parseFrom(bytes);
@@ -863,7 +897,7 @@ public class TestMasterFailover {
           }
           Thread.sleep(100);
         }
-        onlineRegions.addAll(ProtobufUtil.getOnlineRegions(rs));
+        onlineRegions.addAll(ProtobufUtil.getOnlineRegions(rs.getRSRpcServices()));
       } catch (RegionServerStoppedException e) {
         LOG.info("Got RegionServerStoppedException", e);
       }
@@ -891,7 +925,8 @@ public class TestMasterFailover {
    */
   private void verifyRegionLocation(HRegionServer hrs, List<HRegionInfo> regions)
       throws IOException {
-    List<HRegionInfo> tmpOnlineRegions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> tmpOnlineRegions =
+      ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     Iterator<HRegionInfo> itr = regions.iterator();
     while (itr.hasNext()) {
       HRegionInfo tmp = itr.next();
@@ -937,18 +972,9 @@ public class TestMasterFailover {
     MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
 
     // Find regionserver carrying meta.
-    List<RegionServerThread> regionServerThreads =
-      cluster.getRegionServerThreads();
-    int count = -1;
-    HRegion metaRegion = null;
-    for (RegionServerThread regionServerThread : regionServerThreads) {
-      HRegionServer regionServer = regionServerThread.getRegionServer();
-      metaRegion = regionServer.getOnlineRegion(HRegionInfo.FIRST_META_REGIONINFO.getRegionName());
-      count++;
-      regionServer.abort("");
-      if (null != metaRegion) break;
-    }
-    HRegionServer regionServer = cluster.getRegionServer(count);
+    HRegionServer regionServer = cluster.getMaster();
+    HRegion metaRegion = regionServer.getOnlineRegion(
+      HRegionInfo.FIRST_META_REGIONINFO.getRegionName());
 
     TEST_UTIL.shutdownMiniHBaseCluster();
 
@@ -1114,7 +1140,7 @@ public class TestMasterFailover {
     assertEquals(2, masterThreads.size());
     int rsCount = masterThreads.get(activeIndex).getMaster().getClusterStatus().getServersSize();
     LOG.info("Active master " + active.getServerName() + " managing " + rsCount +  " regions servers");
-    assertEquals(3, rsCount);
+    assertEquals(5, rsCount);
 
     // Check that ClusterStatus reports the correct active and backup masters
     assertNotNull(active);
@@ -1147,7 +1173,7 @@ public class TestMasterFailover {
     int rss = status.getServersSize();
     LOG.info("Active master " + mastername.getServerName() + " managing " +
       rss +  " region servers");
-    assertEquals(3, rss);
+    assertEquals(4, rss);
 
     // Stop the cluster
     TEST_UTIL.shutdownMiniCluster();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetrics.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetrics.java
index 07a3b48..5f9001b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetrics.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetrics.java
@@ -27,11 +27,11 @@ import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.MiniHBaseCluster.MiniHBaseClusterRegionServer;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos;
 import org.apache.hadoop.hbase.test.MetricsAssertHelper;
+import org.apache.zookeeper.KeeperException;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -48,11 +48,11 @@ public class TestMasterMetrics {
   private static HMaster master;
   private static HBaseTestingUtility TEST_UTIL;
 
-  private static class MyRegionServer extends MiniHBaseClusterRegionServer {
-    public MyRegionServer(Configuration conf) throws IOException,
-        InterruptedException {
+  public static class MyMaster extends HMaster {
+    public MyMaster(Configuration conf) throws IOException,
+        KeeperException, InterruptedException {
       super(conf);
-     }
+    }
 
     @Override
     protected void tryRegionServerReport(
@@ -65,7 +65,7 @@ public class TestMasterMetrics {
   public static void startCluster() throws Exception {
     LOG.info("Starting cluster");
     TEST_UTIL = new HBaseTestingUtility();
-    TEST_UTIL.startMiniCluster(1, 1, 1, null, HMaster.class, MyRegionServer.class);
+    TEST_UTIL.startMiniCluster(1, 0, 1, null, MyMaster.class, null);
     cluster = TEST_UTIL.getHBaseCluster();
     LOG.info("Waiting for active/ready master");
     cluster.waitForActiveAndReadyMaster();
@@ -85,16 +85,16 @@ public class TestMasterMetrics {
     // sending fake request to master to see how metric value has changed
     RegionServerStatusProtos.RegionServerReportRequest.Builder request =
         RegionServerStatusProtos.RegionServerReportRequest.newBuilder();
-    ServerName serverName = cluster.getRegionServer(0).getServerName();
+    ServerName serverName = cluster.getMaster(0).getServerName();
     request.setServer(ProtobufUtil.toServerName(serverName));
 
-    MetricsMasterSource masterSource = master.getMetrics().getMetricsSource();
+    MetricsMasterSource masterSource = master.getMasterMetrics().getMetricsSource();
     ClusterStatusProtos.ServerLoad sl = ClusterStatusProtos.ServerLoad.newBuilder()
                                            .setTotalNumberOfRequests(10000)
                                            .build();
     masterSource.init();
     request.setLoad(sl);
-    master.regionServerReport(null, request.build());
+    master.getMasterRpcServices().regionServerReport(null, request.build());
 
     metricsHelper.assertCounter("cluster_requests", 10000, masterSource);
 
@@ -102,11 +102,11 @@ public class TestMasterMetrics {
         .setTotalNumberOfRequests(15000)
         .build();
     request.setLoad(sl);
-    master.regionServerReport(null, request.build());
+    master.getMasterRpcServices().regionServerReport(null, request.build());
 
     metricsHelper.assertCounter("cluster_requests", 15000, masterSource);
 
-    master.regionServerReport(null, request.build());
+    master.getMasterRpcServices().regionServerReport(null, request.build());
 
     metricsHelper.assertCounter("cluster_requests", 15000, masterSource);
     master.stopMaster();
@@ -114,7 +114,7 @@ public class TestMasterMetrics {
 
   @Test
   public void testDefaultMasterMetrics() throws Exception {
-    MetricsMasterSource masterSource = master.getMetrics().getMetricsSource();
+    MetricsMasterSource masterSource = master.getMasterMetrics().getMetricsSource();
     metricsHelper.assertGauge( "numRegionServers", 1, masterSource);
     metricsHelper.assertGauge( "averageLoad", 2, masterSource);
     metricsHelper.assertGauge( "numDeadRegionServers", 0, masterSource);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetricsWrapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetricsWrapper.java
index 6d319f4..cb1ffcd 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetricsWrapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterMetricsWrapper.java
@@ -55,9 +55,9 @@ public class TestMasterMetricsWrapper {
     assertEquals(master.getClusterId(), info.getClusterId());
     assertEquals(master.getMasterActiveTime(), info.getActiveTime());
     assertEquals(master.getMasterStartTime(), info.getStartTime());
-    assertEquals(master.getCoprocessors().length, info.getCoprocessors().length);
+    assertEquals(master.getMasterCoprocessors().length, info.getCoprocessors().length);
     assertEquals(master.getServerManager().getOnlineServersList().size(), info.getNumRegionServers());
-    assertTrue(info.getNumRegionServers() == 4);
+    assertEquals(5, info.getNumRegionServers());
 
     String zkServers = info.getZookeeperQuorum();
     assertEquals(zkServers.split(",").length, TEST_UTIL.getZkCluster().getZooKeeperServerNum());
@@ -69,10 +69,10 @@ public class TestMasterMetricsWrapper {
     // We stopped the regionserver but could take a while for the master to notice it so hang here
     // until it does... then move forward to see if metrics wrapper notices.
     while (TEST_UTIL.getHBaseCluster().getMaster().getServerManager().getOnlineServers().size() !=
-        index) {
+        4) {
       Threads.sleep(10);
     }
-    assertTrue(info.getNumRegionServers() == 3);
-    assertTrue(info.getNumDeadRegionServers() == 1);
+    assertEquals(4, info.getNumRegionServers());
+    assertEquals(1, info.getNumDeadRegionServers());
   }
 }
\ No newline at end of file
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
index 50678da..e3b6982 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterNoCluster.java
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hbase.master;
 
 
-import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 
@@ -29,16 +28,18 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Set;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Abortable;
-import org.apache.hadoop.hbase.TableName;
-import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.ServerLoad;
+import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ZooKeeperConnectionException;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaMockingUtil;
@@ -46,28 +47,24 @@ import org.apache.hadoop.hbase.client.HConnection;
 import org.apache.hadoop.hbase.client.HConnectionTestingUtility;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
-import org.apache.hadoop.hbase.regionserver.RegionOpeningState;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hbase.zookeeper.MetaRegionTracker;
+import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.hadoop.hbase.zookeeper.ZKUtil;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
-import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
 import org.apache.zookeeper.KeeperException;
-import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
-import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameStringPair;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerReportRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupRequest;
-import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.RegionServerStartupResponse;
-import com.google.protobuf.ServiceException;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
-import org.mockito.Mockito;
 import org.junit.experimental.categories.Category;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
+import org.mockito.Mockito;
+
+import com.google.protobuf.ServiceException;
 
 /**
  * Standup the master and fake it to test various aspects of master function.
@@ -87,6 +84,7 @@ public class TestMasterNoCluster {
     Configuration c = TESTUTIL.getConfiguration();
     // We use local filesystem.  Set it so it writes into the testdir.
     FSUtils.setRootDir(c, TESTUTIL.getDataTestDir());
+    DefaultMetricsSystem.setMiniClusterMode(true);
     // Startup a mini zk cluster.
     TESTUTIL.startMiniZKCluster();
   }
@@ -179,6 +177,9 @@ public class TestMasterNoCluster {
       InetAddress getRemoteInetAddress(final int port, final long serverStartCode)
       throws UnknownHostException {
         // Return different address dependent on port passed.
+        if (port > sns.length) {
+          return super.getRemoteInetAddress(port, serverStartCode);
+        }
         ServerName sn = sns[port];
         return InetAddress.getByAddress(sn.getHostname(),
           new byte [] {10, 0, 0, (byte)sn.getPort()});
@@ -190,17 +191,15 @@ public class TestMasterNoCluster {
         ServerManager sm = super.createServerManager(master, services);
         // Spy on the created servermanager
         ServerManager spy = Mockito.spy(sm);
-        // Fake a successful open.
-        Mockito.doReturn(RegionOpeningState.OPENED).when(spy).
-          sendRegionOpen((ServerName)Mockito.any(), (HRegionInfo)Mockito.any(),
-            Mockito.anyInt(), Mockito.anyListOf(ServerName.class));
+        // Fake a successful close.
+        Mockito.doReturn(true).when(spy).
+          sendRegionClose((ServerName)Mockito.any(), (HRegionInfo)Mockito.any(),
+            Mockito.anyInt(), (ServerName)Mockito.any(), Mockito.anyBoolean());
         return spy;
       }
 
       @Override
-      CatalogTracker createCatalogTracker(ZooKeeperWatcher zk,
-          Configuration conf, Abortable abortable)
-      throws IOException {
+      protected CatalogTracker createCatalogTracker() throws IOException {
         // Insert a mock for the connection used by the CatalogTracker.  Any
         // regionserver should do.  Use TESTUTIL.getConfiguration rather than
         // the conf from the master; the conf will already have an HConnection
@@ -208,7 +207,7 @@ public class TestMasterNoCluster {
         HConnection connection =
           HConnectionTestingUtility.getMockedConnectionAndDecorate(TESTUTIL.getConfiguration(),
             rs0, rs0, rs0.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
-        return new CatalogTracker(zk, conf, connection, abortable);
+        return new CatalogTracker(getZooKeeper(), getConfiguration(), connection, this);
       }
 
       @Override
@@ -219,17 +218,24 @@ public class TestMasterNoCluster {
 
     try {
       // Wait till master is up ready for RPCs.
-      while (!master.isRpcServerOpen()) Threads.sleep(10);
+      while (!master.serviceStarted) Threads.sleep(10);
       // Fake master that there are regionservers out there.  Report in.
       for (int i = 0; i < sns.length; i++) {
         RegionServerReportRequest.Builder request = RegionServerReportRequest.newBuilder();;
         ServerName sn = ServerName.parseVersionedServerName(sns[i].getVersionedBytes());
         request.setServer(ProtobufUtil.toServerName(sn));
         request.setLoad(ServerLoad.EMPTY_SERVERLOAD.obtainServerLoadPB());
-        master.regionServerReport(null, request.build());
+        master.getMasterRpcServices().regionServerReport(null, request.build());
       }
+      ZooKeeperWatcher zkw = master.getZooKeeper();
       // Master should now come up.
-      while (!master.isInitialized()) {Threads.sleep(10);}
+      while (!master.isInitialized()) {
+        // Fake meta is closed on rs0, try several times in case the event is lost
+        // due to race with HMaster#assignMeta
+        ZKAssign.transitionNodeClosed(zkw,
+          HRegionInfo.FIRST_META_REGIONINFO, sn0, -1);
+        Threads.sleep(100);
+      }
       assertTrue(master.isInitialized());
     } finally {
       rs0.stop("Test is done");
@@ -240,120 +246,6 @@ public class TestMasterNoCluster {
     }
   }
 
-  /**
-   * Test starting master getting it up post initialized state using mocks.
-   * @throws IOException
-   * @throws KeeperException
-   * @throws InterruptedException
-   * @throws DeserializationException
-   * @throws ServiceException
-   */
-  @Test (timeout=60000)
-  public void testCatalogDeploys()
-      throws Exception {
-    final Configuration conf = TESTUTIL.getConfiguration();
-    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART, 1);
-    conf.setInt(ServerManager.WAIT_ON_REGIONSERVERS_MAXTOSTART, 1);
-
-    final long now = System.currentTimeMillis();
-    // Name for our single mocked up regionserver.
-    final ServerName sn = ServerName.valueOf("0.example.org", 0, now);
-    // Here is our mocked up regionserver.  Create it now.  Need it setting up
-    // master next.
-    final MockRegionServer rs0 = new MockRegionServer(conf, sn);
-
-    // Create master.  Subclass to override a few methods so we can insert mocks
-    // and get notification on transitions.  We need to fake out any rpcs the
-    // master does opening/closing regions.  Also need to fake out the address
-    // of the 'remote' mocked up regionservers.
-    HMaster master = new HMaster(conf) {
-      InetAddress getRemoteInetAddress(final int port, final long serverStartCode)
-      throws UnknownHostException {
-        // Interject an unchecked, nonsense InetAddress; i.e. no resolve.
-        return InetAddress.getByAddress(rs0.getServerName().getHostname(),
-          new byte [] {10, 0, 0, 0});
-      }
-
-      @Override
-      ServerManager createServerManager(Server master, MasterServices services)
-      throws IOException {
-        ServerManager sm = super.createServerManager(master, services);
-        // Spy on the created servermanager
-        ServerManager spy = Mockito.spy(sm);
-        // Fake a successful open.
-        Mockito.doReturn(RegionOpeningState.OPENED).when(spy).
-          sendRegionOpen((ServerName)Mockito.any(), (HRegionInfo)Mockito.any(),
-            Mockito.anyInt(), Mockito.anyListOf(ServerName.class));
-        return spy;
-      }
-
-      @Override
-      CatalogTracker createCatalogTracker(ZooKeeperWatcher zk,
-          Configuration conf, Abortable abortable)
-      throws IOException {
-        // Insert a mock for the connection used by the CatalogTracker.   Use
-        // TESTUTIL.getConfiguration rather than the conf from the master; the
-        // conf will already have an HConnection associate so the below mocking
-        // of a connection will fail.
-        HConnection connection =
-          HConnectionTestingUtility.getMockedConnectionAndDecorate(TESTUTIL.getConfiguration(),
-            rs0, rs0, rs0.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
-        return new CatalogTracker(zk, conf, connection, abortable);
-      }
-
-      @Override
-      void initNamespace() {
-      }
-    };
-    master.start();
-    LOG.info("Master has started");
-
-    try {
-      // Wait till master is up ready for RPCs.
-      while (!master.isRpcServerOpen()) Threads.sleep(10);
-      LOG.info("RpcServerOpen has started");
-
-      // Fake master that there is a regionserver out there.  Report in.
-      RegionServerStartupRequest.Builder request = RegionServerStartupRequest.newBuilder();
-      request.setPort(rs0.getServerName().getPort());
-      request.setServerStartCode(rs0.getServerName().getStartcode());
-      request.setServerCurrentTime(now);
-      RegionServerStartupResponse result =
-        master.regionServerStartup(null, request.build());
-      String rshostname = new String();
-      for (NameStringPair e : result.getMapEntriesList()) {
-        if (e.getName().toString().equals(HConstants.KEY_FOR_HOSTNAME_SEEN_BY_MASTER)) {
-          rshostname = e.getValue();
-        }
-      }
-      // Assert hostname is as expected.
-      assertEquals(rs0.getServerName().getHostname(), rshostname);
-      // Now master knows there is at least one regionserver checked in and so
-      // it'll wait a while to see if more and when none, will assign meta
-      // to this single server.  Will do an rpc open but we've
-      // mocked it above in our master override to return 'success'.  As part of
-      // region open, master will have set an unassigned znode for the region up
-      // into zk for the regionserver to transition.  Lets do that now to
-      // complete fake of a successful open.
-      Mocking.fakeRegionServerRegionOpenInZK(master, rs0.getZooKeeper(),
-        rs0.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
-      LOG.info("fakeRegionServerRegionOpenInZK has started");
-
-      // Need to set meta location as r0.  Usually the regionserver does this
-      // when its figured it just opened the meta region by setting the meta
-      // location up into zk.  Since we're mocking regionserver, need to do this
-      // ourselves.
-      MetaRegionTracker.setMetaLocation(rs0.getZooKeeper(), rs0.getServerName());
-      // Master should now come up.
-      while (!master.isInitialized()) {Threads.sleep(10);}
-      assertTrue(master.isInitialized());
-    } finally {
-      rs0.stop("Test is done");
-      master.stopMaster();
-      master.join();
-    }
-  }
-
   @Test
   public void testNotPullingDeadRegionServerFromZK()
       throws IOException, KeeperException, InterruptedException {
@@ -384,9 +276,7 @@ public class TestMasterNoCluster {
       }
 
       @Override
-      CatalogTracker createCatalogTracker(ZooKeeperWatcher zk,
-          Configuration conf, Abortable abortable)
-      throws IOException {
+      protected CatalogTracker createCatalogTracker() throws IOException {
         // Insert a mock for the connection used by the CatalogTracker.  Any
         // regionserver should do.  Use TESTUTIL.getConfiguration rather than
         // the conf from the master; the conf will already have an HConnection
@@ -394,7 +284,7 @@ public class TestMasterNoCluster {
         HConnection connection =
           HConnectionTestingUtility.getMockedConnectionAndDecorate(TESTUTIL.getConfiguration(),
             rs0, rs0, rs0.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
-        return new CatalogTracker(zk, conf, connection, abortable);
+        return new CatalogTracker(getZooKeeper(), getConfiguration(), connection, this);
       }
 
       @Override
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java
index ef7812f..0d956fa 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterRestartAfterDisablingTable.java
@@ -21,21 +21,21 @@ package org.apache.hadoop.hbase.master;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
-import java.io.IOException;
 import java.util.List;
 import java.util.NavigableSet;
-import java.util.TreeSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;
-import org.apache.hadoop.hbase.util.JVMClusterUtil.RegionServerThread;
 import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.apache.zookeeper.KeeperException;
@@ -80,7 +80,7 @@ public class TestMasterRestartAfterDisablingTable {
     log("Disabling table\n");
     TEST_UTIL.getHBaseAdmin().disableTable(table);
 
-    NavigableSet<String> regions = getAllOnlineRegions(cluster);
+    NavigableSet<String> regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
     assertEquals(
         "The number of regions for the table tableRestart should be 0 and only"
             + "the catalog and namespace tables should be present.", 2, regions.size());
@@ -108,10 +108,10 @@ public class TestMasterRestartAfterDisablingTable {
     log("Waiting for no more RIT\n");
     blockUntilNoRIT(zkw, master);
     log("Verifying there are " + numRegions + " assigned on cluster\n");
-    regions = getAllOnlineRegions(cluster);
-    assertEquals(
-        "The assigned regions were not onlined after master switch except for the catalog and namespace tables.",
-        6, regions.size());
+    regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
+    assertEquals("The assigned regions were not onlined after master"
+        + " switch except for the catalog and namespace tables.",
+          6, regions.size());
     assertTrue("The table should be in enabled state", cluster.getMaster()
         .getAssignmentManager().getZKTable()
         .isEnabledTable(TableName.valueOf("tableRestart")));
@@ -128,17 +128,5 @@ public class TestMasterRestartAfterDisablingTable {
     ZKAssign.blockUntilNoRIT(zkw);
     master.assignmentManager.waitUntilNoRegionsInTransition(60000);
   }
-
-  private NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)
-      throws IOException {
-    NavigableSet<String> online = new TreeSet<String>();
-    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
-      for (HRegionInfo region : ProtobufUtil.getOnlineRegions(rst.getRegionServer())) {
-        online.add(region.getRegionNameAsString());
-      }
-    }
-    return online;
-  }
-
 }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java
index 0731eef..e2bf0a0 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterShutdown.java
@@ -25,18 +25,19 @@ import static org.junit.Assert.assertTrue;
 
 import java.util.List;
 
-import org.apache.hadoop.hbase.*;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.LocalHBaseCluster;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.util.JVMClusterUtil.MasterThread;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
 @Category(LargeTests.class)
 public class TestMasterShutdown {
-  private static final Log LOG = LogFactory.getLog(TestMasterShutdown.class);
-
   /**
    * Simple test of shutdown.
    * <p>
@@ -100,6 +101,7 @@ public class TestMasterShutdown {
 
     // Create config to use for this cluster
     Configuration conf = HBaseConfiguration.create();
+    conf.setInt("hbase.ipc.client.failed.servers.expiry", 200);
 
     // Start the cluster
     final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility(conf);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
index b351d94..fbc2c1a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMasterStatusServlet.java
@@ -35,6 +35,8 @@ import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.hadoop.hbase.regionserver.MetricsRegionServer;
+import org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub;
 import org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl;
 import org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl;
 import org.junit.Before;
@@ -94,13 +96,17 @@ public class TestMasterStatusServlet {
     // Fake ZKW
     ZooKeeperWatcher zkw = Mockito.mock(ZooKeeperWatcher.class);
     Mockito.doReturn("fakequorum").when(zkw).getQuorum();
-    Mockito.doReturn(zkw).when(master).getZooKeeperWatcher();
+    Mockito.doReturn(zkw).when(master).getZooKeeper();
 
     // Fake MasterAddressTracker
     MasterAddressTracker tracker = Mockito.mock(MasterAddressTracker.class);
     Mockito.doReturn(tracker).when(master).getMasterAddressTracker();
     Mockito.doReturn(FAKE_HOST).when(tracker).getMasterAddress();
 
+    MetricsRegionServer rms = Mockito.mock(MetricsRegionServer.class);
+    Mockito.doReturn(new MetricsRegionServerWrapperStub()).when(rms).getRegionServerWrapper();
+    Mockito.doReturn(rms).when(master).getRegionServerMetrics();
+
     // Mock admin
     admin = Mockito.mock(HBaseAdmin.class); 
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
index af1dfc0..4a4c9e5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRollingRestart.java
@@ -18,7 +18,7 @@
  */
 package org.apache.hadoop.hbase.master;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
 
 import java.io.IOException;
 import java.util.List;
@@ -29,7 +29,12 @@ import java.util.TreeSet;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -86,7 +91,7 @@ public class  TestRollingRestart {
     TEST_UTIL.getHBaseAdmin().disableTable(table);
     log("Waiting for no more RIT\n");
     blockUntilNoRIT(zkw, master);
-    NavigableSet<String> regions = getAllOnlineRegions(cluster);
+    NavigableSet<String> regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
     log("Verifying only catalog and namespace regions are assigned\n");
     if (regions.size() != 2) {
       for (String oregion : regions) log("Region still online: " + oregion);
@@ -97,7 +102,7 @@ public class  TestRollingRestart {
     log("Waiting for no more RIT\n");
     blockUntilNoRIT(zkw, master);
     log("Verifying there are " + numRegions + " assigned on cluster\n");
-    regions = getAllOnlineRegions(cluster);
+    regions = HBaseTestingUtility.getAllOnlineRegions(cluster);
     assertRegionsAssigned(cluster, regions);
     assertEquals(expectedNumRS, cluster.getRegionServerThreads().size());
 
@@ -184,102 +189,6 @@ public class  TestRollingRestart {
     Thread.sleep(1000);
     assertRegionsAssigned(cluster, regions);
 
-    // Bring the RS hosting hbase:meta down
-    RegionServerThread metaServer = getServerHostingMeta(cluster);
-    log("Stopping server hosting hbase:meta #1");
-    metaServer.getRegionServer().stop("Stopping hbase:meta server");
-    cluster.hbaseCluster.waitOnRegionServer(metaServer);
-    log("Meta server down #1");
-    expectedNumRS--;
-    log("Waiting for meta server #1 RS shutdown to be handled by master");
-    waitForRSShutdownToStartAndFinish(activeMaster,
-        metaServer.getRegionServer().getServerName());
-    log("Waiting for no more RIT");
-    long start = System.currentTimeMillis();
-    do {
-      blockUntilNoRIT(zkw, master);
-    } while (getNumberOfOnlineRegions(cluster) < numRegions 
-        && System.currentTimeMillis()-start < 60000);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-    assertEquals(expectedNumRS, cluster.getRegionServerThreads().size());
-
-    // Kill off the server hosting hbase:meta again
-    metaServer = getServerHostingMeta(cluster);
-    log("Stopping server hosting hbase:meta #2");
-    metaServer.getRegionServer().stop("Stopping hbase:meta server");
-    cluster.hbaseCluster.waitOnRegionServer(metaServer);
-    log("Meta server down");
-    expectedNumRS--;
-    log("Waiting for RS shutdown to be handled by master");
-    waitForRSShutdownToStartAndFinish(activeMaster,
-        metaServer.getRegionServer().getServerName());
-    log("RS shutdown done, waiting for no more RIT");
-    blockUntilNoRIT(zkw, master);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-    assertEquals(expectedNumRS, cluster.getRegionServerThreads().size());
-
-    // Start 3 RS again
-    cluster.startRegionServer().waitForServerOnline();
-    cluster.startRegionServer().waitForServerOnline();
-    cluster.startRegionServer().waitForServerOnline();
-    Thread.sleep(1000);
-    log("Waiting for no more RIT");
-    blockUntilNoRIT(zkw, master);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-    // Shutdown server hosting META
-    metaServer = getServerHostingMeta(cluster);
-    log("Stopping server hosting hbase:meta (1 of 3)");
-    metaServer.getRegionServer().stop("Stopping hbase:meta server");
-    cluster.hbaseCluster.waitOnRegionServer(metaServer);
-    log("Meta server down (1 of 3)");
-    log("Waiting for RS shutdown to be handled by master");
-    waitForRSShutdownToStartAndFinish(activeMaster,
-        metaServer.getRegionServer().getServerName());
-    log("RS shutdown done, waiting for no more RIT");
-    blockUntilNoRIT(zkw, master);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-
-    // Shutdown server hosting hbase:meta again
-    metaServer = getServerHostingMeta(cluster);
-    log("Stopping server hosting hbase:meta (2 of 3)");
-    metaServer.getRegionServer().stop("Stopping hbase:meta server");
-    cluster.hbaseCluster.waitOnRegionServer(metaServer);
-    log("Meta server down (2 of 3)");
-    log("Waiting for RS shutdown to be handled by master");
-    waitForRSShutdownToStartAndFinish(activeMaster,
-        metaServer.getRegionServer().getServerName());
-    log("RS shutdown done, waiting for no more RIT");
-    blockUntilNoRIT(zkw, master);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-
-    // Shutdown server hosting hbase:meta again
-    metaServer = getServerHostingMeta(cluster);
-    log("Stopping server hosting hbase:meta (3 of 3)");
-    metaServer.getRegionServer().stop("Stopping hbase:meta server");
-    cluster.hbaseCluster.waitOnRegionServer(metaServer);
-    log("Meta server down (3 of 3)");
-    log("Waiting for RS shutdown to be handled by master");
-    waitForRSShutdownToStartAndFinish(activeMaster,
-        metaServer.getRegionServer().getServerName());
-    log("RS shutdown done, waiting for no more RIT");
-    blockUntilNoRIT(zkw, master);
-    log("Verifying there are " + numRegions + " assigned on cluster");
-    assertRegionsAssigned(cluster, regions);
-
-    if (cluster.getRegionServerThreads().size() != 1) {
-      log("Online regionservers:");
-      for (RegionServerThread rst : cluster.getRegionServerThreads()) {
-        log("RS: " + rst.getRegionServer().getServerName());
-      }
-    }
-    assertEquals(2, cluster.getRegionServerThreads().size());
-
-
     // TODO: Bring random 3 of 4 RS down at the same time
 
     ht.close();
@@ -314,26 +223,14 @@ public class  TestRollingRestart {
     LOG.debug("\n\nTRR: " + msg + "\n");
   }
 
-  private RegionServerThread getServerHostingMeta(MiniHBaseCluster cluster)
-      throws IOException {
-    return getServerHosting(cluster, HRegionInfo.FIRST_META_REGIONINFO);
-  }
-
-  private RegionServerThread getServerHosting(MiniHBaseCluster cluster,
-      HRegionInfo region) throws IOException {
-    for (RegionServerThread rst : cluster.getRegionServerThreads()) {
-      if (ProtobufUtil.getOnlineRegions(rst.getRegionServer()).contains(region)) {
-        return rst;
-      }
-    }
-    return null;
-  }
-
   private int getNumberOfOnlineRegions(MiniHBaseCluster cluster) {
     int numFound = 0;
     for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
       numFound += rst.getRegionServer().getNumberOfOnlineRegions();
     }
+    for (MasterThread mt : cluster.getMasterThreads()) {
+      numFound += mt.getMaster().getNumberOfOnlineRegions();
+    }
     return numFound;
   }
   
@@ -343,7 +240,8 @@ public class  TestRollingRestart {
     if (expectedRegions.size() > numFound) {
       log("Expected to find " + expectedRegions.size() + " but only found"
           + " " + numFound);
-      NavigableSet<String> foundRegions = getAllOnlineRegions(cluster);
+      NavigableSet<String> foundRegions =
+        HBaseTestingUtility.getAllOnlineRegions(cluster);
       for (String region : expectedRegions) {
         if (!foundRegions.contains(region)) {
           log("Missing region: " + region);
@@ -364,23 +262,13 @@ public class  TestRollingRestart {
     }
   }
 
-  private NavigableSet<String> getAllOnlineRegions(MiniHBaseCluster cluster)
-      throws IOException {
-    NavigableSet<String> online = new TreeSet<String>();
-    for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
-      for (HRegionInfo region : ProtobufUtil.getOnlineRegions(rst.getRegionServer())) {
-        online.add(region.getRegionNameAsString());
-      }
-    }
-    return online;
-  }
-
   private NavigableSet<String> getDoubleAssignedRegions(
       MiniHBaseCluster cluster) throws IOException {
     NavigableSet<String> online = new TreeSet<String>();
     NavigableSet<String> doubled = new TreeSet<String>();
     for (RegionServerThread rst : cluster.getLiveRegionServerThreads()) {
-      for (HRegionInfo region : ProtobufUtil.getOnlineRegions(rst.getRegionServer())) {
+      for (HRegionInfo region : ProtobufUtil.getOnlineRegions(
+          rst.getRegionServer().getRSRpcServices())) {
         if(!online.add(region.getRegionNameAsString())) {
           doubled.add(region.getRegionNameAsString());
         }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
index cfe5c51..61052bd 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestTableLockManager.java
@@ -107,7 +107,7 @@ public class TestTableLockManager {
     conf.setInt(TableLockManager.TABLE_WRITE_LOCK_TIMEOUT_MS, 3000);
     prepareMiniCluster();
     HMaster master = TEST_UTIL.getHBaseCluster().getMaster();
-    master.getCoprocessorHost().load(TestLockTimeoutExceptionMasterObserver.class,
+    master.getMasterCoprocessorHost().load(TestLockTimeoutExceptionMasterObserver.class,
         0, TEST_UTIL.getConfiguration());
 
     ExecutorService executor = Executors.newSingleThreadExecutor();
@@ -159,7 +159,7 @@ public class TestTableLockManager {
     // thread, send a request to disable, and then delete a table.
 
     HMaster master = TEST_UTIL.getHBaseCluster().getMaster();
-    master.getCoprocessorHost().load(TestAlterAndDisableMasterObserver.class,
+    master.getMasterCoprocessorHost().load(TestAlterAndDisableMasterObserver.class,
         0, TEST_UTIL.getConfiguration());
 
     ExecutorService executor = Executors.newFixedThreadPool(2);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestZKBasedOpenCloseRegion.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestZKBasedOpenCloseRegion.java
index c11cbc8..1913bf0 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestZKBasedOpenCloseRegion.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestZKBasedOpenCloseRegion.java
@@ -111,7 +111,8 @@ public class TestZKBasedOpenCloseRegion {
     int rsIdx = 0;
     HRegionServer regionServer =
       TEST_UTIL.getHBaseCluster().getRegionServer(rsIdx);
-    HRegionInfo hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(regionServer));
+    HRegionInfo hri = getNonMetaRegion(
+      ProtobufUtil.getOnlineRegions(regionServer.getRSRpcServices()));
     LOG.debug("Asking RS to close region " + hri.getRegionNameAsString());
 
     LOG.info("Unassign " + hri.getRegionNameAsString());
@@ -154,7 +155,7 @@ public class TestZKBasedOpenCloseRegion {
         cluster.getLiveRegionServerThreads().get(0).getRegionServer();
     HRegionServer hr1 =
         cluster.getLiveRegionServerThreads().get(1).getRegionServer();
-    HRegionInfo hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(hr0));
+    HRegionInfo hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(hr0.getRSRpcServices()));
 
     // fake that hr1 is processing the region
     hr1.getRegionsInTransitionInRS().putIfAbsent(hri.getEncodedNameAsBytes(), true);
@@ -170,7 +171,7 @@ public class TestZKBasedOpenCloseRegion {
     hr1.getRegionsInTransitionInRS().remove(hri.getEncodedNameAsBytes());
 
     // now try moving a region when there is no region in transition.
-    hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(hr1));
+    hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(hr1.getRSRpcServices()));
 
     TEST_UTIL.getHBaseAdmin().move(hri.getEncodedNameAsBytes(),
         Bytes.toBytes(hr0.getServerName().toString()));
@@ -192,7 +193,8 @@ public class TestZKBasedOpenCloseRegion {
 
     int rsIdx = 0;
     HRegionServer regionServer = TEST_UTIL.getHBaseCluster().getRegionServer(rsIdx);
-    HRegionInfo hri = getNonMetaRegion(ProtobufUtil.getOnlineRegions(regionServer));
+    HRegionInfo hri = getNonMetaRegion(
+      ProtobufUtil.getOnlineRegions(regionServer.getRSRpcServices()));
     LOG.debug("Asking RS to close region " + hri.getRegionNameAsString());
 
     cluster.getMaster().assignmentManager.unassign(hri);
@@ -230,7 +232,8 @@ public class TestZKBasedOpenCloseRegion {
     Whitebox.setInternalState(regionServer, "tableDescriptors", htd);
     Mockito.doThrow(new IOException()).when(htd).get((TableName) Mockito.any());
     try {
-      ProtobufUtil.openRegion(regionServer, regionServer.getServerName(), REGIONINFO);
+      ProtobufUtil.openRegion(regionServer.getRSRpcServices(),
+        regionServer.getServerName(), REGIONINFO);
       fail("It should throw IOException ");
     } catch (IOException e) {
     }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
index 310ae90..46783a4 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/BalancerTestBase.java
@@ -164,7 +164,7 @@ public class BalancerTestBase {
   }
 
   protected BaseLoadBalancer.Cluster mockCluster(int[] mockCluster) {
-    return new BaseLoadBalancer.Cluster(mockClusterServers(mockCluster, -1), null, null);
+    return new BaseLoadBalancer.Cluster(null, mockClusterServers(mockCluster, -1), null, null);
   }
 
   protected Map<ServerName, List<HRegionInfo>> mockClusterServers(int[] mockCluster, int numTables) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
index a98a65a..ad95287 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hbase.master.balancer;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotEquals;
 import static org.junit.Assert.assertTrue;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
@@ -39,11 +40,13 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.MasterServices;
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster;
 import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
 
 import com.google.common.collect.Lists;
 
@@ -52,6 +55,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
 
   private static LoadBalancer loadBalancer;
   private static final Log LOG = LogFactory.getLog(TestStochasticLoadBalancer.class);
+  private static final ServerName master = ServerName.valueOf("fake-master", 0, 1L);
 
   int[][] regionsAndServersMocks = new int[][] {
       // { num regions, num servers }
@@ -65,6 +69,9 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     Configuration conf = HBaseConfiguration.create();
     loadBalancer = new MockBalancer();
     loadBalancer.setConf(conf);
+    MasterServices st = Mockito.mock(MasterServices.class);
+    Mockito.when(st.getServerName()).thenReturn(master);
+    loadBalancer.setMasterServices(st);
   }
 
   public static class MockBalancer extends BaseLoadBalancer {
@@ -85,6 +92,17 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
    */
   @Test
   public void testImmediateAssignment() throws Exception {
+    List<ServerName> tmp = getListOfServerNames(randomServers(1, 0));
+    tmp.add(master);
+    ServerName sn = loadBalancer.randomAssignment(HRegionInfo.FIRST_META_REGIONINFO, tmp);
+    assertEquals(master, sn);
+    HRegionInfo hri = randomRegions(1, -1).get(0);
+    sn = loadBalancer.randomAssignment(hri, tmp);
+    assertNotEquals(master, sn);
+    tmp = new ArrayList<ServerName>();
+    tmp.add(master);
+    sn = loadBalancer.randomAssignment(hri, tmp);
+    assertEquals(master, sn);
     for (int[] mock : regionsAndServersMocks) {
       LOG.debug("testImmediateAssignment with " + mock[0] + " regions and " + mock[1] + " servers");
       List<HRegionInfo> regions = randomRegions(mock[0]);
@@ -120,6 +138,18 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
    */
   @Test
   public void testBulkAssignment() throws Exception {
+    List<ServerName> tmp = getListOfServerNames(randomServers(5, 0));
+    List<HRegionInfo> hris = randomRegions(20);
+    hris.add(HRegionInfo.FIRST_META_REGIONINFO);
+    tmp.add(master);
+    Map<ServerName, List<HRegionInfo>> plans = loadBalancer.roundRobinAssignment(hris, tmp);
+    assertTrue(plans.get(master).contains(HRegionInfo.FIRST_META_REGIONINFO));
+    assertEquals(1, plans.get(master).size());
+    int totalRegion = 0;
+    for (List<HRegionInfo> regions: plans.values()) {
+      totalRegion += regions.size();
+    }
+    assertEquals(hris.size(), totalRegion);
     for (int[] mock : regionsAndServersMocks) {
       LOG.debug("testBulkAssignment with " + mock[0] + " regions and " + mock[1] + " servers");
       List<HRegionInfo> regions = randomRegions(mock[0]);
@@ -256,7 +286,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     assignRegions(regions, oldServers, clusterState);
 
     // should not throw exception:
-    BaseLoadBalancer.Cluster cluster = new Cluster(clusterState, null, null);
+    BaseLoadBalancer.Cluster cluster = new Cluster(null, clusterState, null, null);
     assertEquals(101 + 9, cluster.numRegions);
     assertEquals(10, cluster.numServers); // only 10 servers because they share the same host + port
   }
@@ -298,7 +328,7 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
     when(locationFinder.getTopBlockLocations(regions.get(43))).thenReturn(
       Lists.newArrayList(ServerName.valueOf("foo", 0, 0))); // this server does not exists in clusterStatus
 
-    BaseLoadBalancer.Cluster cluster = new Cluster(clusterState, null, locationFinder);
+    BaseLoadBalancer.Cluster cluster = new Cluster(null, clusterState, null, locationFinder);
 
     int r0 = ArrayUtils.indexOf(cluster.regions, regions.get(0)); // this is ok, it is just a test
     int r1 = ArrayUtils.indexOf(cluster.regions, regions.get(1));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
index 74ec2cb..e62b788 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/cleaner/TestSnapshotFromMaster.java
@@ -30,7 +30,6 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
@@ -79,7 +78,6 @@ public class TestSnapshotFromMaster {
   private static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
   private static final int NUM_RS = 2;
   private static Path rootDir;
-  private static Path snapshots;
   private static FileSystem fs;
   private static HMaster master;
 
@@ -101,7 +99,6 @@ public class TestSnapshotFromMaster {
     fs = UTIL.getDFSCluster().getFileSystem();
     master = UTIL.getMiniHBaseCluster().getMaster();
     rootDir = master.getMasterFileSystem().getRootDir();
-    snapshots = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);
     archiveDir = new Path(rootDir, HConstants.HFILE_ARCHIVE_DIRECTORY);
   }
 
@@ -194,7 +191,8 @@ public class TestSnapshotFromMaster {
 
     // then do the lookup for the snapshot that it is done
     builder.setSnapshot(desc);
-    IsSnapshotDoneResponse response = master.isSnapshotDone(null, builder.build());
+    IsSnapshotDoneResponse response =
+      master.getMasterRpcServices().isSnapshotDone(null, builder.build());
     assertTrue("Snapshot didn't complete when it should have.", response.getDone());
 
     // now try the case where we are looking for a snapshot we didn't take
@@ -209,7 +207,7 @@ public class TestSnapshotFromMaster {
     SnapshotDescriptionUtils.writeSnapshotInfo(desc, snapshotDir, fs);
 
     builder.setSnapshot(desc);
-    response = master.isSnapshotDone(null, builder.build());
+    response = master.getMasterRpcServices().isSnapshotDone(null, builder.build());
     assertTrue("Completed, on-disk snapshot not found", response.getDone());
   }
 
@@ -217,7 +215,8 @@ public class TestSnapshotFromMaster {
   public void testGetCompletedSnapshots() throws Exception {
     // first check when there are no snapshots
     GetCompletedSnapshotsRequest request = GetCompletedSnapshotsRequest.newBuilder().build();
-    GetCompletedSnapshotsResponse response = master.getCompletedSnapshots(null, request);
+    GetCompletedSnapshotsResponse response =
+      master.getMasterRpcServices().getCompletedSnapshots(null, request);
     assertEquals("Found unexpected number of snapshots", 0, response.getSnapshotsCount());
 
     // write one snapshot to the fs
@@ -227,7 +226,7 @@ public class TestSnapshotFromMaster {
     SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, snapshotDir, fs);
 
     // check that we get one snapshot
-    response = master.getCompletedSnapshots(null, request);
+    response = master.getMasterRpcServices().getCompletedSnapshots(null, request);
     assertEquals("Found unexpected number of snapshots", 1, response.getSnapshotsCount());
     List<SnapshotDescription> snapshots = response.getSnapshotsList();
     List<SnapshotDescription> expected = Lists.newArrayList(snapshot);
@@ -241,7 +240,7 @@ public class TestSnapshotFromMaster {
     expected.add(snapshot);
 
     // check that we get one snapshot
-    response = master.getCompletedSnapshots(null, request);
+    response = master.getMasterRpcServices().getCompletedSnapshots(null, request);
     assertEquals("Found unexpected number of snapshots", 2, response.getSnapshotsCount());
     snapshots = response.getSnapshotsList();
     assertEquals("Returned snapshots don't match created snapshots", expected, snapshots);
@@ -256,7 +255,7 @@ public class TestSnapshotFromMaster {
     DeleteSnapshotRequest request = DeleteSnapshotRequest.newBuilder().setSnapshot(snapshot)
         .build();
     try {
-      master.deleteSnapshot(null, request);
+      master.getMasterRpcServices().deleteSnapshot(null, request);
       fail("Master didn't throw exception when attempting to delete snapshot that doesn't exist");
     } catch (ServiceException e) {
       LOG.debug("Correctly failed delete of non-existant snapshot:" + e.getMessage());
@@ -267,7 +266,7 @@ public class TestSnapshotFromMaster {
     SnapshotDescriptionUtils.writeSnapshotInfo(snapshot, snapshotDir, fs);
 
     // then delete the existing snapshot,which shouldn't cause an exception to be thrown
-    master.deleteSnapshot(null, request);
+    master.getMasterRpcServices().deleteSnapshot(null, request);
   }
 
   /**
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
index 321344b..307097a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
@@ -50,7 +50,7 @@ public class OOMERegionServer extends HRegionServer {
     try {
       MutateRequest request =
         RequestConverter.buildMutateRequest(regionName, put);
-      super.mutate(null, request);
+      rpcServices.mutate(null, request);
       for (int i = 0; i < 30; i++) {
         // Add the batch update 30 times to bring on the OOME faster.
         this.retainer.add(put);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestClusterId.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestClusterId.java
index b972a3f..def52fe 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestClusterId.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestClusterId.java
@@ -23,8 +23,6 @@ import static org.junit.Assert.assertNotNull;
 
 import java.util.UUID;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.FileSystem;
@@ -48,14 +46,10 @@ import org.junit.experimental.categories.Category;
 @Category(MediumTests.class)
 public class TestClusterId {
 
-  private static final Log LOG =
-      LogFactory.getLog(TestClusterId.class.getName());
-
   private final HBaseTestingUtility TEST_UTIL =
       new HBaseTestingUtility();
 
   private JVMClusterUtil.RegionServerThread rst;
-  private JVMClusterUtil.MasterThread mst;
 
   @Before
   public void setUp() throws Exception {
@@ -113,7 +107,7 @@ public class TestClusterId {
     }
     TEST_UTIL.startMiniHBaseCluster(1, 1);
     HMaster master = TEST_UTIL.getHBaseCluster().getMaster();
-    assertEquals(1, master.getServerManager().getOnlineServersList().size());
+    assertEquals(2, master.getServerManager().getOnlineServersList().size());
   }
   
 }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
index b1c6335..82f357f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestEndToEndSplitTransaction.java
@@ -158,11 +158,12 @@ public class TestEndToEndSplitTransaction {
       // get and scan should now succeed without exception
       ClientProtos.GetRequest request =
           RequestConverter.buildGetRequest(regionName, new Get(row));
-      server.get(null, request);
+      server.getRSRpcServices().get(null, request);
       ScanRequest scanRequest = RequestConverter.buildScanRequest(
         regionName, new Scan(row), 1, true);
       try {
-        server.scan(new PayloadCarryingRpcController(), scanRequest);
+        server.getRSRpcServices().scan(
+          new PayloadCarryingRpcController(), scanRequest);
       } catch (ServiceException se) {
         throw ProtobufUtil.getRemoteException(se);
       }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPriorityRpc.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPriorityRpc.java
index 2ce5298..e3c15d6 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPriorityRpc.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestPriorityRpc.java
@@ -24,7 +24,6 @@ import static org.junit.Assert.assertTrue;
 
 import java.io.IOException;
 
-import com.google.protobuf.HBaseZeroCopyByteString;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
@@ -43,6 +42,7 @@ import org.junit.experimental.categories.Category;
 import org.mockito.Mockito;
 
 import com.google.protobuf.ByteString;
+import com.google.protobuf.HBaseZeroCopyByteString;
 
 /**
  * Tests that verify certain RPCs get a higher QoS.
@@ -55,13 +55,14 @@ public class TestPriorityRpc {
   @Before
   public void setup() {
     Configuration conf = HBaseConfiguration.create();
+    conf.setBoolean("hbase.testing.nocluster", true); // No need to do ZK
     regionServer = HRegionServer.constructRegionServer(HRegionServer.class, conf);
-    priority = regionServer.getPriority();
+    priority = regionServer.rpcServices.getPriority();
   }
 
   @Test
   public void testQosFunctionForMeta() throws IOException {
-    priority = regionServer.getPriority();
+    priority = regionServer.rpcServices.getPriority();
     RequestHeader.Builder headerBuilder = RequestHeader.newBuilder();
     //create a rpc request that has references to hbase:meta region and also
     //uses one of the known argument classes (known argument classes are
@@ -82,8 +83,10 @@ public class TestPriorityRpc {
     RequestHeader header = headerBuilder.build();
     HRegion mockRegion = Mockito.mock(HRegion.class);
     HRegionServer mockRS = Mockito.mock(HRegionServer.class);
+    RSRpcServices mockRpc = Mockito.mock(RSRpcServices.class);
+    Mockito.when(mockRS.getRSRpcServices()).thenReturn(mockRpc);
     HRegionInfo mockRegionInfo = Mockito.mock(HRegionInfo.class);
-    Mockito.when(mockRS.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
+    Mockito.when(mockRpc.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
     Mockito.when(mockRegion.getRegionInfo()).thenReturn(mockRegionInfo);
     Mockito.when(mockRegionInfo.isMetaTable()).thenReturn(true);
     // Presume type.
@@ -100,7 +103,7 @@ public class TestPriorityRpc {
     RequestHeader.Builder headerBuilder = RequestHeader.newBuilder();
     headerBuilder.setMethodName("foo");
     RequestHeader header = headerBuilder.build();
-    PriorityFunction qosFunc = regionServer.getPriority();
+    PriorityFunction qosFunc = regionServer.rpcServices.getPriority();
     assertEquals(HConstants.NORMAL_QOS, qosFunc.getPriority(header, null));
   }
 
@@ -115,8 +118,10 @@ public class TestPriorityRpc {
     ScanRequest scanRequest = scanBuilder.build();
     HRegion mockRegion = Mockito.mock(HRegion.class);
     HRegionServer mockRS = Mockito.mock(HRegionServer.class);
+    RSRpcServices mockRpc = Mockito.mock(RSRpcServices.class);
+    Mockito.when(mockRS.getRSRpcServices()).thenReturn(mockRpc);
     HRegionInfo mockRegionInfo = Mockito.mock(HRegionInfo.class);
-    Mockito.when(mockRS.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
+    Mockito.when(mockRpc.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
     Mockito.when(mockRegion.getRegionInfo()).thenReturn(mockRegionInfo);
     Mockito.when(mockRegionInfo.isMetaRegion()).thenReturn(false);
     // Presume type.
@@ -130,9 +135,9 @@ public class TestPriorityRpc {
     scanRequest = scanBuilder.build();
     //mock out a high priority type handling and see the QoS returned
     RegionScanner mockRegionScanner = Mockito.mock(RegionScanner.class);
-    Mockito.when(mockRS.getScanner(12345)).thenReturn(mockRegionScanner);
+    Mockito.when(mockRpc.getScanner(12345)).thenReturn(mockRegionScanner);
     Mockito.when(mockRegionScanner.getRegionInfo()).thenReturn(mockRegionInfo);
-    Mockito.when(mockRS.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
+    Mockito.when(mockRpc.getRegion((RegionSpecifier)Mockito.any())).thenReturn(mockRegion);
     Mockito.when(mockRegion.getRegionInfo()).thenReturn(mockRegionInfo);
     Mockito.when(mockRegionInfo.isMetaRegion()).thenReturn(true);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
index 873e3ca..3bc9314 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestQosFunction.java
@@ -36,8 +36,9 @@ import com.google.protobuf.Message;
 public class TestQosFunction {
   @Test
   public void testPriority() {
-    HRegionServer hrs = Mockito.mock(HRegionServer.class);
-    AnnotationReadingPriorityFunction qosFunction = new AnnotationReadingPriorityFunction(hrs);
+    RSRpcServices rpcServices = Mockito.mock(RSRpcServices.class);
+    AnnotationReadingPriorityFunction qosFunction =
+      new AnnotationReadingPriorityFunction(rpcServices);
 
     // Set method name in pb style with the method name capitalized.
     checkMethod("ReplicateWALEntry", HConstants.REPLICATION_QOS, qosFunction);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSKilledWhenInitializing.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSKilledWhenInitializing.java
index 6ac8230..6c614b9 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSKilledWhenInitializing.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSKilledWhenInitializing.java
@@ -24,8 +24,6 @@ import java.io.IOException;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
@@ -47,8 +45,6 @@ import org.junit.experimental.categories.Category;
  */
 @Category(LargeTests.class)
 public class TestRSKilledWhenInitializing {
-  private static final Log LOG = LogFactory.getLog(TestRSKilledWhenInitializing.class);
-
   private static boolean masterActive = false;
   private static AtomicBoolean firstRS = new AtomicBoolean(true);
 
@@ -77,7 +73,7 @@ public class TestRSKilledWhenInitializing {
     master.start();
     try {
       long startTime = System.currentTimeMillis();
-      while (!master.getMaster().isActiveMaster()) {
+      while (!master.getMaster().isInitialized()) {
         try {
           Thread.sleep(100);
         } catch (InterruptedException ignored) {
@@ -92,11 +88,11 @@ public class TestRSKilledWhenInitializing {
       Thread.sleep(10000);
       List<ServerName> onlineServersList =
           master.getMaster().getServerManager().getOnlineServersList();
-      while (onlineServersList.size() != 1) {
+      while (onlineServersList.size() > 2) {
         Thread.sleep(100);
         onlineServersList = master.getMaster().getServerManager().getOnlineServersList();
       }
-      assertEquals(onlineServersList.size(), 1);
+      assertEquals(onlineServersList.size(), 2);
       cluster.shutdown();
     } finally {
       masterActive = false;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java
index 15e55bb..40161a9 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRSStatusServlet.java
@@ -51,10 +51,11 @@ import com.google.protobuf.ServiceException;
 @Category(SmallTests.class)
 public class TestRSStatusServlet {
   private HRegionServer rs;
-  
+  private RSRpcServices rpcServices;
+
   static final int FAKE_IPC_PORT = 1585;
   static final int FAKE_WEB_PORT = 1586;
-  
+
   private final ServerName fakeServerName =
       ServerName.valueOf("localhost", FAKE_IPC_PORT, 11111);
   private final GetServerInfoResponse fakeResponse =
@@ -66,9 +67,11 @@ public class TestRSStatusServlet {
   @Before
   public void setupBasicMocks() throws IOException, ServiceException {
     rs = Mockito.mock(HRegionServer.class);
+    rpcServices = Mockito.mock(RSRpcServices.class);
     Mockito.doReturn(HBaseConfiguration.create())
       .when(rs).getConfiguration();
-    Mockito.doReturn(fakeResponse).when(rs).getServerInfo(
+    Mockito.doReturn(rpcServices).when(rs).getRSRpcServices();
+    Mockito.doReturn(fakeResponse).when(rpcServices).getServerInfo(
       (RpcController)Mockito.any(), (GetServerInfoRequest)Mockito.any());
     // Fake ZKW
     ZooKeeperWatcher zkw = Mockito.mock(ZooKeeperWatcher.class);
@@ -82,7 +85,7 @@ public class TestRSStatusServlet {
 
     MetricsRegionServer rms = Mockito.mock(MetricsRegionServer.class);
     Mockito.doReturn(new MetricsRegionServerWrapperStub()).when(rms).getRegionServerWrapper();
-    Mockito.doReturn(rms).when(rs).getMetrics();
+    Mockito.doReturn(rms).when(rs).getRegionServerMetrics();
   }
   
   @Test
@@ -98,7 +101,7 @@ public class TestRSStatusServlet {
         new HRegionInfo(htd.getTableName(), Bytes.toBytes("d"), Bytes.toBytes("z"))
         );
     Mockito.doReturn(ResponseConverter.buildGetOnlineRegionResponse(
-      regions)).when(rs).getOnlineRegion((RpcController)Mockito.any(),
+      regions)).when(rpcServices).getOnlineRegion((RpcController)Mockito.any(),
         (GetOnlineRegionRequest)Mockito.any());
     
     new RSStatusTmpl().render(new StringWriter(), rs);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
index db6a4af..eacdecb 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerMetrics.java
@@ -17,8 +17,6 @@
  */
 package org.apache.hadoop.hbase.regionserver;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.*;
 import org.apache.hadoop.hbase.client.*;
@@ -38,7 +36,6 @@ import java.io.IOException;
 
 @Category(MediumTests.class)
 public class TestRegionServerMetrics {
-  private static final Log LOG = LogFactory.getLog(TestRegionServerMetrics.class);
   private static MetricsAssertHelper metricsHelper;
 
   static {
@@ -72,7 +69,7 @@ public class TestRegionServerMetrics {
     }
 
     rs = cluster.getRegionServer(0);
-    metricsRegionServer = rs.getMetrics();
+    metricsRegionServer = rs.getRegionServerMetrics();
     serverSource = metricsRegionServer.getMetricsSource();
   }
 
@@ -105,8 +102,6 @@ public class TestRegionServerMetrics {
     byte[] row = Bytes.toBytes("rk");
     byte[] qualifier = Bytes.toBytes("qual");
     byte[] initValue = Bytes.toBytes("Value");
-    byte[] nextValue = Bytes.toBytes("NEXT VAL");
-
 
     TEST_UTIL.createTable(tName, cfName);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
index 42e9e8f..b7cc51a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestRegionServerNoMaster.java
@@ -28,15 +28,19 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.catalog.MetaEditor;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.executor.EventType;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.CloseRegionRequest;
 import org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.zookeeper.MetaRegionTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKAssign;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Assert;
@@ -79,6 +83,22 @@ public class TestRegionServerNoMaster {
 
     // No master
     HTU.getHBaseCluster().getMaster().stopMaster();
+
+    // Master is down, so is the meta. We need to assign it somewhere
+    // so that regions can be assigned during the mocking phase.
+    HRegionServer hrs = HTU.getHBaseCluster().getRegionServer(0);
+    ZooKeeperWatcher zkw = hrs.getZooKeeper();
+    ZKAssign.createNodeOffline(
+      zkw, HRegionInfo.FIRST_META_REGIONINFO, hrs.getServerName());
+    ProtobufUtil.openRegion(hrs.getRSRpcServices(),
+      hrs.getServerName(), HRegionInfo.FIRST_META_REGIONINFO);
+    while (true) {
+      ServerName sn = MetaRegionTracker.getMetaRegionLocation(zkw);
+      if (sn != null && sn.equals(hrs.getServerName())) {
+        break;
+      }
+      Thread.sleep(100);
+    }
   }
 
   @AfterClass
@@ -108,8 +128,9 @@ public class TestRegionServerNoMaster {
     // We reopen. We need a ZK node here, as a open is always triggered by a master.
     ZKAssign.createNodeOffline(HTU.getZooKeeperWatcher(), hri, getRS().getServerName());
     // first version is '0'
-    AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
-    AdminProtos.OpenRegionResponse responseOpen = getRS().openRegion(null, orr);
+    AdminProtos.OpenRegionRequest orr =
+      RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
+    AdminProtos.OpenRegionResponse responseOpen = getRS().rpcServices.openRegion(null, orr);
     Assert.assertTrue(responseOpen.getOpeningStateCount() == 1);
     Assert.assertTrue(responseOpen.getOpeningState(0).
         equals(AdminProtos.OpenRegionResponse.RegionOpeningState.OPENED));
@@ -155,7 +176,7 @@ public class TestRegionServerNoMaster {
     // no transition in ZK
     AdminProtos.CloseRegionRequest crr =
         RequestConverter.buildCloseRegionRequest(getRS().getServerName(), regionName, false);
-    AdminProtos.CloseRegionResponse responseClose = getRS().closeRegion(null, crr);
+    AdminProtos.CloseRegionResponse responseClose = getRS().rpcServices.closeRegion(null, crr);
     Assert.assertTrue(responseClose.getClosed());
 
     // now waiting & checking. After a while, the transition should be done and the region closed
@@ -175,7 +196,7 @@ public class TestRegionServerNoMaster {
     // Transition in ZK on. This should fail, as there is no znode
     AdminProtos.CloseRegionRequest crr = RequestConverter.buildCloseRegionRequest(
       getRS().getServerName(), regionName, true);
-    AdminProtos.CloseRegionResponse responseClose = getRS().closeRegion(null, crr);
+    AdminProtos.CloseRegionResponse responseClose = getRS().rpcServices.closeRegion(null, crr);
     Assert.assertTrue(responseClose.getClosed());
 
     // now waiting. After a while, the transition should be done
@@ -194,7 +215,7 @@ public class TestRegionServerNoMaster {
 
     AdminProtos.CloseRegionRequest crr = RequestConverter.buildCloseRegionRequest(
       getRS().getServerName(), regionName, true);
-    AdminProtos.CloseRegionResponse responseClose = getRS().closeRegion(null, crr);
+    AdminProtos.CloseRegionResponse responseClose = getRS().rpcServices.closeRegion(null, crr);
     Assert.assertTrue(responseClose.getClosed());
 
     checkRegionIsClosed();
@@ -228,7 +249,7 @@ public class TestRegionServerNoMaster {
     // We're sending multiple requests in a row. The region server must handle this nicely.
     for (int i = 0; i < 10; i++) {
       AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
-      AdminProtos.OpenRegionResponse responseOpen = getRS().openRegion(null, orr);
+      AdminProtos.OpenRegionResponse responseOpen = getRS().rpcServices.openRegion(null, orr);
       Assert.assertTrue(responseOpen.getOpeningStateCount() == 1);
 
       AdminProtos.OpenRegionResponse.RegionOpeningState ors = responseOpen.getOpeningState(0);
@@ -246,10 +267,15 @@ public class TestRegionServerNoMaster {
     Assert.assertTrue(getRS().getRegion(regionName).isAvailable());
 
     try {
+      // we re-opened meta so some of its data is lost
+      ServerName sn = getRS().getServerName();
+      MetaEditor.updateRegionLocation(getRS().catalogTracker,
+        hri, sn, getRS().getRegion(regionName).getOpenSeqNum());
       // fake region to be closing now, need to clear state afterwards
       getRS().regionsInTransitionInRS.put(hri.getEncodedNameAsBytes(), Boolean.FALSE);
-      AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(getRS().getServerName(), hri, 0, null);
-      getRS().openRegion(null, orr);
+      AdminProtos.OpenRegionRequest orr =
+        RequestConverter.buildOpenRegionRequest(sn, hri, 0, null);
+      getRS().rpcServices.openRegion(null, orr);
       Assert.fail("The closing region should not be opened");
     } catch (ServiceException se) {
       Assert.assertTrue("The region should be already in transition",
@@ -268,7 +294,7 @@ public class TestRegionServerNoMaster {
       AdminProtos.CloseRegionRequest crr =
           RequestConverter.buildCloseRegionRequest(getRS().getServerName(), regionName, 0, null, true);
       try {
-        AdminProtos.CloseRegionResponse responseClose = getRS().closeRegion(null, crr);
+        AdminProtos.CloseRegionResponse responseClose = getRS().rpcServices.closeRegion(null, crr);
         Assert.assertEquals("The first request should succeeds", 0, i);
         Assert.assertTrue("request " + i + " failed",
             responseClose.getClosed() || responseClose.hasClosed());
@@ -304,7 +330,7 @@ public class TestRegionServerNoMaster {
     AdminProtos.CloseRegionRequest crr =
         RequestConverter.buildCloseRegionRequest(getRS().getServerName(), regionName, false);
     try {
-      getRS().closeRegion(null, crr);
+      getRS().rpcServices.closeRegion(null, crr);
       Assert.assertTrue(false);
     } catch (ServiceException expected) {
     }
@@ -322,7 +348,7 @@ public class TestRegionServerNoMaster {
 
     // The open handler should have updated the value in ZK.
     Assert.assertTrue(ZKAssign.deleteNode(
-        getRS().getZooKeeperWatcher(), hri.getEncodedName(),
+        getRS().getZooKeeper(), hri.getEncodedName(),
         EventType.RS_ZK_REGION_FAILED_OPEN, 1)
     );
 
@@ -347,7 +373,7 @@ public class TestRegionServerNoMaster {
     AdminProtos.CloseRegionRequest crr =
         RequestConverter.buildCloseRegionRequest(getRS().getServerName(), regionName, false);
     try {
-      getRS().closeRegion(null, crr);
+      getRS().rpcServices.closeRegion(null, crr);
       Assert.assertTrue(false);
     } catch (ServiceException expected) {
       Assert.assertTrue(expected.getCause() instanceof NotServingRegionException);
@@ -355,7 +381,7 @@ public class TestRegionServerNoMaster {
 
     // The close should have left the ZK state as it is: it's the job the AM to delete it
     Assert.assertTrue(ZKAssign.deleteNode(
-        getRS().getZooKeeperWatcher(), hri.getEncodedName(),
+        getRS().getZooKeeper(), hri.getEncodedName(),
         EventType.M_ZK_REGION_CLOSING, 0)
     );
 
@@ -393,7 +419,7 @@ public class TestRegionServerNoMaster {
 
     try {
       CloseRegionRequest request = RequestConverter.buildCloseRegionRequest(earlierServerName, regionName, true);
-      getRS().closeRegion(null, request);
+      getRS().getRSRpcServices().closeRegion(null, request);
       Assert.fail("The closeRegion should have been rejected");
     } catch (ServiceException se) {
       Assert.assertTrue(se.getCause() instanceof IOException);
@@ -404,7 +430,7 @@ public class TestRegionServerNoMaster {
     closeNoZK();
     try {
       AdminProtos.OpenRegionRequest orr = RequestConverter.buildOpenRegionRequest(earlierServerName, hri, 0, null);
-      getRS().openRegion(null, orr);
+      getRS().getRSRpcServices().openRegion(null, orr);
       Assert.fail("The openRegion should have been rejected");
     } catch (ServiceException se) {
       Assert.assertTrue(se.getCause() instanceof IOException);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
index 1ad65be..011c94f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
@@ -73,11 +73,10 @@ import org.apache.hadoop.hbase.executor.EventType;
 import org.apache.hadoop.hbase.master.AssignmentManager;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.RegionState;
-import org.apache.hadoop.hbase.master.RegionStates;
 import org.apache.hadoop.hbase.master.RegionState.State;
+import org.apache.hadoop.hbase.master.RegionStates;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
-import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.FSUtils;
@@ -302,6 +301,7 @@ public class TestSplitTransactionOnCluster {
     }
   }
   @Test
+  @SuppressWarnings("deprecation")
   public void testSplitFailedCompactionAndSplit() throws Exception {
     final byte[] tableName = Bytes.toBytes("testSplitFailedCompactionAndSplit");
     Configuration conf = TESTING_UTIL.getConfiguration();
@@ -393,7 +393,7 @@ public class TestSplitTransactionOnCluster {
       // Get region pre-split.
       HRegionServer server = cluster.getRegionServer(tableRegionIndex);
       printOutRegions(server, "Initial regions: ");
-      int regionCount = ProtobufUtil.getOnlineRegions(server).size();
+      int regionCount = ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
       // Now, before we split, set special flag in master, a flag that has
       // it FAIL the processing of split.
       AssignmentManager.TEST_SKIP_SPLIT_HANDLING = true;
@@ -466,7 +466,7 @@ public class TestSplitTransactionOnCluster {
       // Get region pre-split.
       HRegionServer server = cluster.getRegionServer(tableRegionIndex);
       printOutRegions(server, "Initial regions: ");
-      int regionCount = ProtobufUtil.getOnlineRegions(server).size();
+      int regionCount = ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
       // Insert into zk a blocking znode, a znode of same name as region
       // so it gets in way of our splitting.
       ServerName fakedServer = ServerName.valueOf("any.old.server", 1234, -1);
@@ -480,7 +480,8 @@ public class TestSplitTransactionOnCluster {
       // Wait around a while and assert count of regions remains constant.
       for (int i = 0; i < 10; i++) {
         Thread.sleep(100);
-        assertEquals(regionCount, ProtobufUtil.getOnlineRegions(server).size());
+        assertEquals(regionCount, ProtobufUtil.getOnlineRegions(
+          server.getRSRpcServices()).size());
       }
       // Now clear the zknode
       ZKAssign.deleteClosingNode(TESTING_UTIL.getZooKeeperWatcher(),
@@ -525,13 +526,13 @@ public class TestSplitTransactionOnCluster {
       // Get region pre-split.
       HRegionServer server = cluster.getRegionServer(tableRegionIndex);
       printOutRegions(server, "Initial regions: ");
-      int regionCount = ProtobufUtil.getOnlineRegions(server).size();
+      int regionCount = ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
       // Now split.
       split(hri, server, regionCount);
       // Get daughters
       List<HRegion> daughters = checkAndGetDaughters(tableName);
       // Now split one of the daughters.
-      regionCount = ProtobufUtil.getOnlineRegions(server).size();
+      regionCount = ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size();
       HRegionInfo daughter = daughters.get(0).getRegionInfo();
       LOG.info("Daughter we are going to split: " + daughter);
       // Compact first to ensure we have cleaned up references -- else the split
@@ -1169,13 +1170,14 @@ public class TestSplitTransactionOnCluster {
   private void split(final HRegionInfo hri, final HRegionServer server, final int regionCount)
       throws IOException, InterruptedException {
     this.admin.split(hri.getRegionNameAsString());
-    for (int i = 0; ProtobufUtil.getOnlineRegions(server).size() <= regionCount && i < 300; i++) {
+    for (int i = 0; ProtobufUtil.getOnlineRegions(
+        server.getRSRpcServices()).size() <= regionCount && i < 300; i++) {
       LOG.debug("Waiting on region to split");
       Thread.sleep(100);
     }
 
     assertFalse("Waited too long for split",
-        ProtobufUtil.getOnlineRegions(server).size() <= regionCount);
+      ProtobufUtil.getOnlineRegions(server.getRSRpcServices()).size() <= regionCount);
   }
 
   /**
@@ -1197,8 +1199,8 @@ public class TestSplitTransactionOnCluster {
     // hbase:meta  We don't want hbase:meta replay polluting our test when we later crash
     // the table region serving server.
     int metaServerIndex = cluster.getServerWithMeta();
-    assertTrue(metaServerIndex != -1);
-    HRegionServer metaRegionServer = cluster.getRegionServer(metaServerIndex);
+    assertTrue(metaServerIndex == -1); // meta is on master now
+    HRegionServer metaRegionServer = cluster.getMaster();
     int tableRegionIndex = cluster.getServerWith(hri.getRegionName());
     assertTrue(tableRegionIndex != -1);
     HRegionServer tableRegionServer = cluster.getRegionServer(tableRegionIndex);
@@ -1250,7 +1252,7 @@ public class TestSplitTransactionOnCluster {
 
   private void printOutRegions(final HRegionServer hrs, final String prefix)
       throws IOException {
-    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs);
+    List<HRegionInfo> regions = ProtobufUtil.getOnlineRegions(hrs.getRSRpcServices());
     for (HRegionInfo region: regions) {
       LOG.info(prefix + region.getRegionNameAsString());
     }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
index 0879a97..cc8c531 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestHLogFiltering.java
@@ -18,7 +18,7 @@
  */
 package org.apache.hadoop.hbase.regionserver.wal;
 
-import static junit.framework.Assert.assertEquals;
+import static org.junit.Assert.assertEquals;
 
 import java.io.IOException;
 import java.util.List;
@@ -26,11 +26,9 @@ import java.util.Random;
 import java.util.SortedMap;
 import java.util.TreeMap;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
@@ -51,8 +49,6 @@ import com.google.protobuf.ServiceException;
 
 @Category(MediumTests.class)
 public class TestHLogFiltering {
-  private static final Log LOG = LogFactory.getLog(TestHLogFiltering.class);
-
   private static final int NUM_MASTERS = 1;
   private static final int NUM_RS = 4;
 
@@ -124,7 +120,8 @@ public class TestHLogFiltering {
             RequestConverter.buildGetLastFlushedSequenceIdRequest(regionName);
 
           assertEquals((long)allFlushedSequenceIds.get(regionName),
-              master.getLastFlushedSequenceId(null, req).getLastFlushedSequenceId());
+            master.getMasterRpcServices().getLastFlushedSequenceId(
+              null, req).getLastFlushedSequenceId());
         }
       }
     }
@@ -149,7 +146,7 @@ public class TestHLogFiltering {
     for (byte[] regionName : getRegionsByServer(rsId)) {
       FlushRegionRequest request =
         RequestConverter.buildFlushRegionRequest(regionName);
-      hrs.flushRegion(null, request);
+      hrs.getRSRpcServices().flushRegion(null, request);
     }
   }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
index 2412582..0b3ba8c 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
@@ -187,13 +187,14 @@ public class TestAccessController extends SecureTestUtil {
     conf.setBoolean(AccessController.EXEC_PERMISSION_CHECKS_KEY, true);
 
     TEST_UTIL.startMiniCluster();
-    MasterCoprocessorHost cpHost = TEST_UTIL.getMiniHBaseCluster().getMaster().getCoprocessorHost();
+    MasterCoprocessorHost cpHost =
+      TEST_UTIL.getMiniHBaseCluster().getMaster().getMasterCoprocessorHost();
     cpHost.load(AccessController.class, Coprocessor.PRIORITY_HIGHEST, conf);
     ACCESS_CONTROLLER = (AccessController) cpHost.findCoprocessor(AccessController.class.getName());
     CP_ENV = cpHost.createEnvironment(AccessController.class, ACCESS_CONTROLLER,
       Coprocessor.PRIORITY_HIGHEST, 1, conf);
     RegionServerCoprocessorHost rsHost = TEST_UTIL.getMiniHBaseCluster().getRegionServer(0)
-        .getCoprocessorHost();
+      .getRegionServerCoprocessorHost();
     RSCP_ENV = rsHost.createEnvironment(AccessController.class, ACCESS_CONTROLLER,
       Coprocessor.PRIORITY_HIGHEST, 1, conf);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestNamespaceCommands.java hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestNamespaceCommands.java
index eeb57b1..ab7719c 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestNamespaceCommands.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestNamespaceCommands.java
@@ -78,7 +78,7 @@ public class TestNamespaceCommands extends SecureTestUtil {
     UTIL.waitTableAvailable(AccessControlLists.ACL_TABLE_NAME.getName(), 30 * 1000);
 
     ACCESS_CONTROLLER = (AccessController) UTIL.getMiniHBaseCluster().getMaster()
-      .getCoprocessorHost()
+      .getRegionServerCoprocessorHost()
         .findCoprocessor(AccessController.class.getName());
 
     UTIL.getHBaseAdmin().createNamespace(NamespaceDescriptor.create(TestNamespace).build());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
index 681fbba..ac9efff 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/snapshot/SnapshotTestingUtils.java
@@ -23,19 +23,15 @@ import static org.junit.Assert.assertTrue;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
-import java.util.HashSet;
 import java.util.TreeSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
@@ -57,10 +53,8 @@ import org.apache.hadoop.hbase.protobuf.generated.MasterProtos.IsSnapshotDoneRes
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSTableDescriptors;
-import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.FSVisitor;
 import org.apache.hadoop.hbase.util.MD5Hash;
 import org.junit.Assert;
@@ -270,7 +264,7 @@ public class SnapshotTestingUtils {
     IsSnapshotDoneResponse done = IsSnapshotDoneResponse.newBuilder()
         .buildPartial();
     while (!done.getDone()) {
-      done = master.isSnapshotDone(null, request);
+      done = master.getMasterRpcServices().isSnapshotDone(null, request);
       try {
         Thread.sleep(sleep);
       } catch (InterruptedException e) {
@@ -324,7 +318,7 @@ public class SnapshotTestingUtils {
       IsSnapshotDoneRequest snapshot,
       Class<? extends HBaseSnapshotException> clazz) {
     try {
-      master.isSnapshotDone(null, snapshot);
+      master.getMasterRpcServices().isSnapshotDone(null, snapshot);
       Assert.fail("didn't fail to lookup a snapshot");
     } catch (ServiceException se) {
       try {
