Index: src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java	(revision 11)
@@ -0,0 +1,151 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.NavigableSet;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.TimeRange;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+
+
+public class TestQueryMatcher extends HBaseTestCase
+implements HConstants {
+  private final boolean PRINT = false;
+  
+  private byte [] row1;
+  private byte [] row2;
+  private byte [] fam1;
+  private byte [] fam2;
+  private byte [] col1;
+  private byte [] col2;
+  private byte [] col3;
+  private byte [] col4;
+  private byte [] col5;
+  private byte [] col6;
+
+  private byte [] data;
+
+  private Get get;
+
+  long ttl = Long.MAX_VALUE;
+  KeyComparator rowComparator;
+
+  public void setUp(){
+    row1 = Bytes.toBytes("row1");
+    row2 = Bytes.toBytes("row2");
+    fam1 = Bytes.toBytes("fam1");
+    fam2 = Bytes.toBytes("fam2");
+    col1 = Bytes.toBytes("col1");
+    col2 = Bytes.toBytes("col2");
+    col3 = Bytes.toBytes("col3");
+    col4 = Bytes.toBytes("col4");
+    col5 = Bytes.toBytes("col5");
+    col6 = Bytes.toBytes("col6");
+
+    data = Bytes.toBytes("data");
+
+    //Create Get
+    get = new Get(row1);
+    get.addFamily(fam1);
+    get.addColumn(fam2, col2);
+    get.addColumn(fam2, col4);
+    get.addColumn(fam2, col5);
+
+    rowComparator = KeyValue.KEY_COMPARATOR;
+
+  }
+
+  public void testMatch_ExplicitColumns() 
+  throws IOException {
+    //Moving up from the Tracker by using Gets and List<KeyValue> instead
+    //of just byte [] 
+
+    //Expected result
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+
+    QueryMatcher qm = new QueryMatcher(get.getRow(), fam2,
+        get.getFamilyMap().get(fam2), get.getMaxVersions(), get.getTimeRange(),
+        ttl, rowComparator);
+
+    List<KeyValue> memCache = new ArrayList<KeyValue>();
+    memCache.add(new KeyValue(row1, fam2, col1, data));
+    memCache.add(new KeyValue(row1, fam2, col2, data));
+    memCache.add(new KeyValue(row1, fam2, col3, data));
+    memCache.add(new KeyValue(row1, fam2, col4, data));
+    memCache.add(new KeyValue(row1, fam2, col5, data));
+
+    memCache.add(new KeyValue(row2, fam1, col1, data));
+
+    List<MatchCode> actual = new ArrayList<MatchCode>();
+
+    for(KeyValue kv : memCache){
+      actual.add(qm.match(kv));
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected "+expected.get(i)+ 
+            ", actual " +actual.get(i));
+      }
+    }
+  }
+
+
+  public void testMatch_Wildcard() 
+  throws IOException {
+    //Moving up from the Tracker by using Gets and List<KeyValue> instead
+    //of just byte [] 
+
+    //Expected result
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.NEXT);
+
+    QueryMatcher qm = new QueryMatcher(get.getRow(), fam2, null,
+        get.getMaxVersions(), get.getTimeRange(), ttl, rowComparator);
+
+    List<KeyValue> memCache = new ArrayList<KeyValue>();
+    memCache.add(new KeyValue(row1, fam2, col1, data));
+    memCache.add(new KeyValue(row1, fam2, col2, data));
+    memCache.add(new KeyValue(row1, fam2, col3, data));
+    memCache.add(new KeyValue(row1, fam2, col4, data));
+    memCache.add(new KeyValue(row1, fam2, col5, data));
+    memCache.add(new KeyValue(row2, fam1, col1, data));
+
+    List<MatchCode> actual = new ArrayList<MatchCode>();
+
+    for(KeyValue kv : memCache){
+      actual.add(qm.match(kv));
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected "+expected.get(i)+ 
+            ", actual " +actual.get(i));
+      }
+    }
+  }
+
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(revision 1)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(revision 11)
@@ -218,7 +218,7 @@
     int i = 0;
     for (KeyValue kv: kvs) {
       String expectedColname = Bytes.toString(getColumnName(rowIndex, i++));
-      String colnameStr = kv.getColumnString();
+      String colnameStr = Bytes.toString(kv.getColumn());
       assertEquals("Column name", colnameStr, expectedColname);
       // Value is column name as bytes.  Usually result is
       // 100 bytes in size at least. This is the default size
Index: src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java	(revision 11)
@@ -0,0 +1,144 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestExplicitColumnTracker extends HBaseTestCase
+implements HConstants {
+  private boolean PRINT = false; 
+  
+  public void testGet_SingleVersion(){
+    if(PRINT){
+      System.out.println("SingleVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    //Looking for every other
+    columns.add(col2);
+    columns.add(col4);
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+    int maxVersions = 1;
+    
+    ColumnTracker exp = new ExplicitColumnTracker(columns, maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  public void testGet_MultiVersion(){
+    if(PRINT){
+      System.out.println("\nMultiVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    //Looking for every other
+    columns.add(col2);
+    columns.add(col4);
+    
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+
+    expected.add(MatchCode.DONE);
+    expected.add(MatchCode.DONE);
+    expected.add(MatchCode.DONE);
+    int maxVersions = 2;
+    
+    ColumnTracker exp = new ExplicitColumnTracker(columns, maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col5);
+    scanner.add(col5);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java	(revision 11)
@@ -0,0 +1,293 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.regionserver.GetDeleteTracker.Delete;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestGetDeleteTracker extends HBaseTestCase implements HConstants {
+  
+  private final boolean PRINT = true;
+  
+  private byte [] col1 = null;
+  private byte [] col2 = null;
+  
+  private int col1Len = 0;
+  private int col2Len = 0;
+
+  private byte [] empty = null;
+  
+  private long ts1 = 0L;
+  private long ts2 = 0L;
+  private long ts3 = 0L;
+  
+  
+  private Delete del10 = null;
+  private Delete del11 = null;
+  private Delete delQf10 = null;
+  private Delete delQf11 = null;
+  private Delete delFam10 = null;
+  
+  private Delete del20 = null;
+  private Delete del21 = null;
+  private Delete delQf20 = null;
+  private Delete delQf21 = null;
+  private Delete delFam20 = null;
+  
+  
+  private Delete del30 = null;
+  
+  GetDeleteTracker dt = null;
+  private byte del = KeyValue.Type.Delete.getCode();
+  private byte delCol = KeyValue.Type.DeleteColumn.getCode();
+  private byte delFam = KeyValue.Type.DeleteFamily.getCode();
+
+  protected void setUp() throws Exception {
+    dt = new GetDeleteTracker();
+    col1 = "col".getBytes();
+    col2 = "col2".getBytes();
+    col1Len = col1.length;
+    col2Len = col2.length;
+    
+    empty = new byte[0];
+
+    //ts1
+    ts1 = System.nanoTime();
+    del10 = dt.new Delete(col1, 0, col1Len, del, ts1);
+    del11 = dt.new Delete(col2, 0, col2Len, del, ts1);
+    delQf10 = dt.new Delete(col1, 0, col1Len, delCol, ts1);
+    delQf11 = dt.new Delete(col2, 0, col2Len, delCol, ts1);
+    delFam10 = dt.new Delete(empty, 0, 0, delFam, ts1);
+    
+    //ts2
+    ts2 = System.nanoTime();
+    del20 = dt.new Delete(col1, 0, col1Len, del, ts2);
+    del21 = dt.new Delete(col2, 0, col2Len, del, ts2);
+    delQf20 = dt.new Delete(col1, 0, col1Len, delCol, ts2);
+    delQf21 = dt.new Delete(col2, 0, col2Len, delCol, ts2);
+    delFam20 = dt.new Delete(empty, 0, 0, delFam, ts1);
+    
+    //ts3
+    ts3 = System.nanoTime();
+    del30 = dt.new Delete(col1, 0, col1Len, del, ts3);
+  }
+  
+  public void testUpdate_CompareDeletes() {
+    GetDeleteTracker.DeleteCompare res = null;
+    
+    
+    //Testing Delete and Delete
+    res = dt.compareDeletes(del10, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    //Testing Delete qf1 and Delete qf2 and <==> 
+    res = dt.compareDeletes(del10, del11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(del11, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+        
+    //Testing Delete ts1 and Delete ts2 and <==> 
+    res = dt.compareDeletes(del10, del20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    res = dt.compareDeletes(del20, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    
+    
+    
+    //Testing DeleteColumn and DeleteColumn
+    res = dt.compareDeletes(delQf10, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    //Testing DeleteColumn qf1 and DeleteColumn qf2 and <==> 
+    res = dt.compareDeletes(delQf10, delQf11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf11, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    
+    //Testing DeleteColumn ts1 and DeleteColumn ts2 and <==> 
+    res = dt.compareDeletes(delQf10, delQf20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_BOTH, res);
+    res = dt.compareDeletes(delQf20, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    
+    
+    //Testing Delete and DeleteColumn and <==> 
+    res = dt.compareDeletes(del10, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_OLD, res);
+    res = dt.compareDeletes(delQf10, del10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_NEW, res);
+
+    //Testing Delete qf1 and DeleteColumn qf2 and <==> 
+    res = dt.compareDeletes(del10, delQf11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf11, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    
+    //Testing Delete qf2 and DeleteColumn qf1 and <==> 
+    res = dt.compareDeletes(del11, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    res = dt.compareDeletes(delQf10, del11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    
+    //Testing Delete ts2 and DeleteColumn ts1 and <==> 
+    res = dt.compareDeletes(del20, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf10, del20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+ 
+    //Testing Delete ts1 and DeleteColumn ts2 and <==> 
+    res = dt.compareDeletes(del10, delQf20);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_OLD, res);
+    res = dt.compareDeletes(delQf20, del10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_NEW, res);
+    
+  }
+  
+  public void testUpdate(){
+    //Building lists
+    List<Delete> dels1 = new ArrayList<Delete>();
+    dels1.add(delQf10);
+    dels1.add(del21);
+    
+    List<Delete> dels2 = new ArrayList<Delete>();
+    dels2.add(delFam10);
+    dels2.add(del30);
+    dels2.add(delQf20);
+    
+    List<Delete> res = new ArrayList<Delete>();
+    res.add(del30);
+    res.add(delQf20);
+    res.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels1){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    //Check deleteList
+    List<Delete> delList = dt.deletes;
+    assertEquals(dels1.size(), delList.size());
+    for(int i=0; i<dels1.size(); i++){
+      assertEquals(0, Bytes.compareTo(dels1.get(i).buffer,
+          delList.get(i).buffer));
+      assertEquals(dels1.get(i).qualifierOffset, delList.get(i).qualifierOffset);
+      assertEquals(dels1.get(i).qualifierLength, delList.get(i).qualifierLength);
+      assertEquals(dels1.get(i).timestamp, delList.get(i).timestamp);
+      assertEquals(dels1.get(i).type, delList.get(i).type);
+    }
+    
+    //Add more entries
+    for(Delete del : dels2){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    //Update()
+    dt.update();
+    
+    //Check deleteList
+    delList = dt.deletes;
+
+    for(int i=0; i<res.size(); i++){
+      assertEquals(0, Bytes.compareTo(res.get(i).buffer,
+          delList.get(i).buffer));
+      assertEquals(res.get(i).qualifierOffset, delList.get(i).qualifierOffset);
+      assertEquals(res.get(i).qualifierLength, delList.get(i).qualifierLength);
+      assertEquals(res.get(i).timestamp, delList.get(i).timestamp);
+      assertEquals(res.get(i).type, delList.get(i).type);
+      if(PRINT){
+        System.out.println("Qf " +new String(delList.get(i).buffer) + 
+            ", timestamp, " +delList.get(i).timestamp+ 
+            ", type " +KeyValue.Type.codeToType(delList.get(i).type));
+      }
+    }
+    
+  }
+  
+  /**
+   * Test if a KeyValue is in the lists of deletes already. Cases that needs to
+   * be tested are:
+   * Not deleted
+   * Deleted by a Delete
+   * Deleted by a DeleteColumn
+   * Deleted by a DeleteFamily
+   */
+  public void testIsDeleted_NotDeleted(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delQf10);
+    dels.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(false, dt.isDeleted(col2, 0, col2Len, ts3));
+    assertEquals(false, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  public void testIsDeleted_Delete(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength, 
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts2));
+  }
+  
+  public void testIsDeleted_DeleteColumn(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delQf21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  
+  public void testIsDeleted_DeleteFamily(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delFam20);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java	(revision 11)
@@ -0,0 +1,207 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+//import java.util.Random;
+import java.util.Set;
+//import java.util.SortedSet;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestKeyValueHeap extends HBaseTestCase
+implements HConstants {
+  private final boolean PRINT = false;
+  
+  List<Scanner> scanners = new ArrayList<Scanner>();
+
+  private byte [] row1;
+  private byte [] fam1;
+  private byte [] col1;
+  private byte [] data;
+
+  private byte [] row2;
+  private byte [] fam2;
+  private byte [] col2;
+
+  private byte [] col3;
+  private byte [] col4;
+  private byte [] col5;
+
+  public void setUp(){
+    data = Bytes.toBytes("data");
+
+    row1 = Bytes.toBytes("row1");
+    fam1 = Bytes.toBytes("fam1");
+    col1 = Bytes.toBytes("col1");
+
+    row2 = Bytes.toBytes("row2");
+    fam2 = Bytes.toBytes("fam2");
+    col2 = Bytes.toBytes("col2");
+
+    col3 = Bytes.toBytes("col3");
+    col4 = Bytes.toBytes("col4");
+    col5 = Bytes.toBytes("col5");
+  }
+
+  public void testSorted(){
+    //Cases that need to be checked are:
+    //1. The "smallest" KeyValue is in the same scanners as current
+    //2. Current scanner gets empty
+
+    List<KeyValue> l1 = new ArrayList<KeyValue>();
+    l1.add(new KeyValue(row1, fam1, col5, data));
+    l1.add(new KeyValue(row2, fam1, col1, data));
+    l1.add(new KeyValue(row2, fam1, col2, data));
+    scanners.add(new Scanner(l1));
+
+    List<KeyValue> l2 = new ArrayList<KeyValue>();
+    l2.add(new KeyValue(row1, fam1, col1, data));
+    l2.add(new KeyValue(row1, fam1, col2, data));
+    scanners.add(new Scanner(l2));
+
+    List<KeyValue> l3 = new ArrayList<KeyValue>();
+    l3.add(new KeyValue(row1, fam1, col3, data));
+    l3.add(new KeyValue(row1, fam1, col4, data));
+    l3.add(new KeyValue(row1, fam2, col1, data));
+    l3.add(new KeyValue(row1, fam2, col2, data));
+    l3.add(new KeyValue(row2, fam1, col3, data));
+    scanners.add(new Scanner(l3));
+
+    List<KeyValue> expected = new ArrayList<KeyValue>();
+    expected.add(new KeyValue(row1, fam1, col1, data));
+    expected.add(new KeyValue(row1, fam1, col2, data));
+    expected.add(new KeyValue(row1, fam1, col3, data));
+    expected.add(new KeyValue(row1, fam1, col4, data));
+    expected.add(new KeyValue(row1, fam1, col5, data));
+    expected.add(new KeyValue(row1, fam2, col1, data));
+    expected.add(new KeyValue(row1, fam2, col2, data));
+    expected.add(new KeyValue(row2, fam1, col1, data));
+    expected.add(new KeyValue(row2, fam1, col2, data));
+    expected.add(new KeyValue(row2, fam1, col3, data));
+
+    //Creating KeyValueHeap
+    KeyValueHeap kvh =
+      new KeyValueHeap(scanners.toArray(new Scanner[0]), KeyValue.COMPARATOR);
+    
+    List<KeyValue> actual = new ArrayList<KeyValue>();
+    while(kvh.peek() != null){
+      actual.add(kvh.next());
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected " +expected.get(i)+
+            "\nactual   " +actual.get(i) +"\n");
+      }
+    }
+    
+    //Check if result is sorted according to Comparator
+    for(int i=0; i<actual.size()-1; i++){
+      int ret = KeyValue.COMPARATOR.compare(actual.get(i), actual.get(i+1));
+      assertTrue(ret < 0);
+    }
+    
+  }
+
+  public void testSeek(){
+    //Cases:
+    //1. Seek KeyValue that is not in scanner
+    //2. Check that smallest that is returned from a seek is correct
+    
+    List<KeyValue> l1 = new ArrayList<KeyValue>();
+    l1.add(new KeyValue(row1, fam1, col5, data));
+    l1.add(new KeyValue(row2, fam1, col1, data));
+    l1.add(new KeyValue(row2, fam1, col2, data));
+    scanners.add(new Scanner(l1));
+
+    List<KeyValue> l2 = new ArrayList<KeyValue>();
+    l2.add(new KeyValue(row1, fam1, col1, data));
+    l2.add(new KeyValue(row1, fam1, col2, data));
+    scanners.add(new Scanner(l2));
+
+    List<KeyValue> l3 = new ArrayList<KeyValue>();
+    l3.add(new KeyValue(row1, fam1, col3, data));
+    l3.add(new KeyValue(row1, fam1, col4, data));
+    l3.add(new KeyValue(row1, fam2, col1, data));
+    l3.add(new KeyValue(row1, fam2, col2, data));
+    l3.add(new KeyValue(row2, fam1, col3, data));
+    scanners.add(new Scanner(l3));
+
+    List<KeyValue> expected = new ArrayList<KeyValue>();
+    expected.add(new KeyValue(row2, fam1, col1, data));
+    
+    //Creating KeyValueHeap
+    KeyValueHeap kvh =
+      new KeyValueHeap(scanners.toArray(new Scanner[0]), KeyValue.COMPARATOR);
+    
+    KeyValue seekKv = new KeyValue(row2, fam1, null, null);
+    kvh.seek(seekKv);
+    
+    List<KeyValue> actual = new ArrayList<KeyValue>();
+    actual.add(kvh.peek());
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected " +expected.get(i)+
+            "\nactual   " +actual.get(i) +"\n");
+      }
+    }
+    
+  }
+
+  private class Scanner implements KeyValueScanner {
+    private Set<KeyValue> scan =
+      new TreeSet<KeyValue>((Comparator)KeyValue.COMPARATOR);
+    private Iterator<KeyValue> iter;
+    private KeyValue current;
+
+    public Scanner(List<KeyValue> list) {
+      Collections.sort(list, (Comparator)KeyValue.COMPARATOR);
+      iter = list.iterator();
+      if(iter.hasNext()){
+        current = iter.next();
+      } 
+    }
+    
+    public KeyValue peek() {
+      return current;
+    }
+
+    public KeyValue next() {
+      KeyValue oldCurrent = current;
+      if(iter.hasNext()){
+        current = iter.next();
+      } else {
+        current = null;
+      }
+      return oldCurrent;
+    }
+
+    public void close(){}
+    
+    public boolean seek(KeyValue seekKv) {
+      while(iter.hasNext()){
+        KeyValue next = iter.next();
+        int ret = KeyValue.COMPARATOR.compare(next, seekKv);
+        if(ret >= 0){
+          current = next;
+          return true;
+        }
+      }
+      return false;
+    }
+  }
+
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java	(revision 11)
@@ -0,0 +1,79 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestScanDeleteTracker extends HBaseTestCase implements HConstants {
+
+  private ScanDeleteTracker sdt;
+  private long timestamp = 10L;
+  private byte deleteType = 0;
+  
+  public void setUp(){
+    sdt = new ScanDeleteTracker();
+  }
+  
+  public void testDeletedBy_Delete() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.Delete.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDeletedBy_DeleteColumn() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDeletedBy_DeleteFamily() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteFamily.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDelete_DeleteColumn() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.Delete.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  
+  public void testDeleteColumn_Delete() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    qualifier = Bytes.toBytes("qualifier1");
+    deleteType = KeyValue.Type.Delete.getCode();
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java	(revision 11)
@@ -0,0 +1,336 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class TestWildcardColumnTracker extends HBaseTestCase
+implements HConstants {
+  private boolean PRINT = false; 
+  
+  public void testGet_SingleVersion() {
+    if(PRINT) {
+      System.out.println("SingleVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    int maxVersions = 1;
+    
+    ColumnTracker exp = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+
+  
+  public void testGet_MultiVersion() {
+    if(PRINT) {
+      System.out.println("\nMultiVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 5;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+      expected.add(MatchCode.INCLUDE);
+      expected.add(MatchCode.SKIP);
+    }
+    int maxVersions = 2;
+    
+    ColumnTracker exp = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col5);
+    scanner.add(col5);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  public void testUpdate_SameColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_SameColumns");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 10;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<5; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 2;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(int i=0; i<3; i++){
+      for(byte [] col : scanner){
+        result.add(wild.checkColumn(col, 0, col.length));
+      }
+      wild.update();
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  public void testUpdate_NewColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_NewColumns");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    byte [] col6 = Bytes.toBytes("col6");
+    byte [] col7 = Bytes.toBytes("col7");
+    byte [] col8 = Bytes.toBytes("col8");
+    byte [] col9 = Bytes.toBytes("col9");
+    byte [] col0 = Bytes.toBytes("col0");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 10;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<5; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 1;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col0);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Create "Scanner1"
+    List<byte[]> scanner1 = new ArrayList<byte[]>();
+    scanner1.add(col5);
+    scanner1.add(col6);
+    scanner1.add(col7);
+    scanner1.add(col8);
+    scanner1.add(col9);
+    for(byte [] col : scanner1){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Scanner again
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }  
+      
+    //"Match"
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  public void testUpdate_MixedColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_NewColumns");
+    }
+    byte [] col0 = Bytes.toBytes("col0");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    
+    byte [] col5 = Bytes.toBytes("col5");
+    byte [] col6 = Bytes.toBytes("col6");
+    byte [] col7 = Bytes.toBytes("col7");
+    byte [] col8 = Bytes.toBytes("col8");
+    byte [] col9 = Bytes.toBytes("col9");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 5;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 1;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col0);
+    scanner.add(col2);
+    scanner.add(col4);
+    scanner.add(col6);
+    scanner.add(col8);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    for(int i=0; i<2; i++){
+      for(byte [] col : scanner){
+        result.add(wild.checkColumn(col, 0, col.length));
+      }
+      wild.update();
+    }
+
+    //Create "Scanner1"
+    List<byte[]> scanner1 = new ArrayList<byte[]>();
+    scanner1.add(col1);
+    scanner1.add(col3);
+    scanner1.add(col5);
+    scanner1.add(col7);
+    scanner1.add(col9);
+    for(byte [] col : scanner1){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Scanner again
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }  
+      
+    //"Match"
+    assertEquals(expected.size(), result.size());
+    
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(revision 1)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(revision 11)
@@ -133,7 +133,7 @@
         assertTrue(Bytes.equals(regionName, key.getRegionName()));
         assertTrue(Bytes.equals(tableName, key.getTablename()));
         assertTrue(Bytes.equals(HLog.METAROW, val.getKeyValue().getRow()));
-        assertTrue(Bytes.equals(HLog.METACOLUMN, val.getKeyValue().getColumn()));
+        assertTrue(Bytes.equals(HLog.METAFAMILY, val.getKeyValue().getFamily()));
         assertEquals(0, Bytes.compareTo(HLogEdit.COMPLETE_CACHE_FLUSH,
           val.getKeyValue().getValue()));
         System.out.println(key + " " + val);
Index: src/test/org/apache/hadoop/hbase/TestKeyValue.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestKeyValue.java	(revision 1)
+++ src/test/org/apache/hadoop/hbase/TestKeyValue.java	(revision 11)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.util.Bytes;
 
 public class TestKeyValue extends TestCase {
@@ -39,13 +40,21 @@
     final byte [] a = Bytes.toBytes("aaa");
     byte [] column1 = Bytes.toBytes("abc:def");
     byte [] column2 = Bytes.toBytes("abcd:ef");
-    KeyValue aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column2, 0, column2.length, 4) == 0);
+    byte [] family2 = Bytes.toBytes("abcd");
+    byte [] qualifier2 = Bytes.toBytes("ef"); 
+    KeyValue aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column2));
+    assertTrue(aaa.matchingColumn(column1));
+    aaa = new KeyValue(a, column2, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column1));
+    assertTrue(aaa.matchingColumn(family2,qualifier2));
     column1 = Bytes.toBytes("abcd:");
-    aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column1, 0, column1.length, 4) == 0);
+    aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertTrue(aaa.matchingColumn(family2,null));
+    assertFalse(aaa.matchingColumn(family2,qualifier2));
+    // Previous test had an assertFalse that I don't understand
+    //    assertFalse(KeyValue.COMPARATOR.
+    //    compareColumns(aaa, column1, 0, column1.length, 4) == 0);
   }
 
   public void testBasics() throws Exception {
@@ -111,31 +120,31 @@
   public void testMoreComparisons() throws Exception {
     // Root compares
     long now = System.currentTimeMillis();
-    KeyValue a = new KeyValue(".META.,,99999999999999", now);
-    KeyValue b = new KeyValue(".META.,,1", now);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,,99999999999999"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,,1"), now);
     KVComparator c = new KeyValue.RootComparator();
     assertTrue(c.compare(b, a) < 0);
-    KeyValue aa = new KeyValue(".META.,,1", now);
-    KeyValue bb = new KeyValue(".META.,,1", "info:regioninfo",
-      1235943454602L);
+    KeyValue aa = new KeyValue(Bytes.toBytes(".META.,,1"), now);
+    KeyValue bb = new KeyValue(Bytes.toBytes(".META.,,1"), 
+        Bytes.toBytes("info:regioninfo"), 1235943454602L);
     assertTrue(c.compare(aa, bb) < 0);
     
     // Meta compares
-    KeyValue aaa =
-      new KeyValue("TestScanMultipleVersions,row_0500,1236020145502", now);
-    KeyValue bbb = new KeyValue("TestScanMultipleVersions,,99999999999999",
-      now);
+    KeyValue aaa = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,row_0500,1236020145502"), now);
+    KeyValue bbb = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,,99999999999999"), now);
     c = new KeyValue.MetaComparator();
     assertTrue(c.compare(bbb, aaa) < 0);
     
-    KeyValue aaaa = new KeyValue("TestScanMultipleVersions,,1236023996656",
-      "info:regioninfo", 1236024396271L);
+    KeyValue aaaa = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,,1236023996656"),
+        Bytes.toBytes("info:regioninfo"), 1236024396271L);
     assertTrue(c.compare(aaaa, bbb) < 0);
     
-    KeyValue x = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "", 9223372036854775807L);
-    KeyValue y = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "info:regioninfo", 1236034574912L);
+    KeyValue x = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes(""), 9223372036854775807L);
+    KeyValue y = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes("info:regioninfo"), 1236034574912L);
     assertTrue(c.compare(x, y) < 0);
     comparisons(new KeyValue.MetaComparator());
     comparisons(new KeyValue.KVComparator());
@@ -151,53 +160,53 @@
   public void testKeyValueBorderCases() throws IOException {
     // % sorts before , so if we don't do special comparator, rowB would
     // come before rowA.
-    KeyValue rowA = new KeyValue("testtable,www.hbase.org/,1234",
-      "", Long.MAX_VALUE);
-    KeyValue rowB = new KeyValue("testtable,www.hbase.org/%20,99999",
-      "", Long.MAX_VALUE);
+    KeyValue rowA = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/,1234"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
+    KeyValue rowB = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/%20,99999"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue("testtable,,1234", "", Long.MAX_VALUE);
-    rowB = new KeyValue("testtable,$www.hbase.org/,99999", "", Long.MAX_VALUE);
+    rowA = new KeyValue(Bytes.toBytes("testtable,,1234"), Bytes.toBytes(""), Long.MAX_VALUE);
+    rowB = new KeyValue(Bytes.toBytes("testtable,$www.hbase.org/,99999"), Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue(".META.,testtable,www.hbase.org/,1234,4321", "",
+    rowA = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/,1234,4321"), Bytes.toBytes(""),
       Long.MAX_VALUE);
-    rowB = new KeyValue(".META.,testtable,www.hbase.org/%20,99999,99999", "",
+    rowB = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/%20,99999,99999"), Bytes.toBytes(""),
       Long.MAX_VALUE);
     assertTrue(KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);
   }
 
   private void metacomparisons(final KeyValue.MetaComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,1", now),
-      new KeyValue(".META.,a,,0,1", now)) == 0);
-    KeyValue a = new KeyValue(".META.,a,,0,1", now);
-    KeyValue b = new KeyValue(".META.,a,,0,2", now);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) == 0);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now);
     assertTrue(c.compare(a, b) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,2", now),
-      new KeyValue(".META.,a,,0,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) > 0);
   }
 
   private void comparisons(final KeyValue.KVComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,1", now)) == 0);
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,2", now)) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,,2", now),
-      new KeyValue(".META.,,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) == 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,2"), now)) < 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) > 0);
   }
 
   public void testBinaryKeys() throws Exception {
     Set<KeyValue> set = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
-    String column = "col:umn";
-    KeyValue [] keys = {new KeyValue("aaaaa,\u0000\u0000,2", column, 2),
-      new KeyValue("aaaaa,\u0001,3", column, 3),
-      new KeyValue("aaaaa,,1", column, 1),
-      new KeyValue("aaaaa,\u1000,5", column, 5),
-      new KeyValue("aaaaa,a,4", column, 4),
-      new KeyValue("a,a,0", column, 0),
+    byte [] column = Bytes.toBytes("col:umn");
+    KeyValue [] keys = {new KeyValue(Bytes.toBytes("aaaaa,\u0000\u0000,2"), column, 2),
+      new KeyValue(Bytes.toBytes("aaaaa,\u0001,3"), column, 3),
+      new KeyValue(Bytes.toBytes("aaaaa,,1"), column, 1),
+      new KeyValue(Bytes.toBytes("aaaaa,\u1000,5"), column, 5),
+      new KeyValue(Bytes.toBytes("aaaaa,a,4"), column, 4),
+      new KeyValue(Bytes.toBytes("a,a,0"), column, 0),
     };
     // Add to set with bad comparator
     for (int i = 0; i < keys.length; i++) {
@@ -226,12 +235,12 @@
     }
     // Make up -ROOT- table keys.
     KeyValue [] rootKeys = {
-        new KeyValue(".META.,aaaaa,\u0000\u0000,0,2", column, 2),
-        new KeyValue(".META.,aaaaa,\u0001,0,3", column, 3),
-        new KeyValue(".META.,aaaaa,,0,1", column, 1),
-        new KeyValue(".META.,aaaaa,\u1000,0,5", column, 5),
-        new KeyValue(".META.,aaaaa,a,0,4", column, 4),
-        new KeyValue(".META.,,0", column, 0),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0000\u0000,0,2"), column, 2),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0001,0,3"), column, 3),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,,0,1"), column, 1),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u1000,0,5"), column, 5),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,a,0,4"), column, 4),
+        new KeyValue(Bytes.toBytes(".META.,,0"), column, 0),
       };
     // This will output the keys incorrectly.
     set = new TreeSet<KeyValue>(new KeyValue.MetaComparator());
Index: src/java/org/apache/hadoop/hbase/HConstants.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HConstants.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/HConstants.java	(revision 11)
@@ -162,8 +162,39 @@
   /** delimiter used between portions of a region name */
   public static final int META_ROW_DELIMITER = ',';
 
-  // Defines for the column names used in both ROOT and META HBase 'meta' tables.
+  /** The catalog family */
+  static final byte [] CATALOG_FAMILY = Bytes.toBytes("info");
   
+  /** Array of catalog family */
+  static final byte [][] CATALOG_FAMILY_ARRAY = new byte [][] {CATALOG_FAMILY};
+  
+  /** The catalog historian family */
+  static final byte [] CATALOG_HISTORIAN_FAMILY = Bytes.toBytes("historian");
+  
+  /** The regioninfo column qualifier */
+  static final byte [] REGIONINFO_QUALIFIER = Bytes.toBytes("regioninfo");
+  
+  /** Array of regioninfo column qualifier */ 
+  static final byte [][] REGIONINFO_QUALIFIER_ARRAY = new byte [][] {REGIONINFO_QUALIFIER};
+  
+  /** The server column qualifier */
+  static final byte [] SERVER_QUALIFIER = Bytes.toBytes("server");
+  
+  /** The startcode column qualifier */
+  static final byte [] STARTCODE_QUALIFIER = Bytes.toBytes("serverstartcode");
+  
+  /** The lower-half split region column qualifier */
+  static final byte [] SPLITA_QUALIFIER = Bytes.toBytes("splitA");
+  
+  /** The upper-half split region column qualifier */
+  static final byte [] SPLITB_QUALIFIER = Bytes.toBytes("splitB");
+  
+  /** All catalog column qualifiers */
+  static final byte [][] ALL_CATALOG_QUALIFIERS = {REGIONINFO_QUALIFIER, 
+    SERVER_QUALIFIER, STARTCODE_QUALIFIER, SPLITA_QUALIFIER, SPLITB_QUALIFIER};
+  
+  // Old style, still making the transition
+  
   /** The ROOT and META column family (string) */
   static final String COLUMN_FAMILY_STR = "info:";
   
@@ -204,6 +235,11 @@
   static final byte[][] ALL_META_COLUMNS = {COL_REGIONINFO, COL_SERVER,
     COL_STARTCODE, COL_SPLITA, COL_SPLITB};
 
+  //
+  // New stuff.  Making a slow transition.
+  //
+  
+
   // Other constants
 
   /**
Index: src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java	(revision 11)
@@ -0,0 +1,50 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.KeyValue;
+
+/**
+ * Scanner that returns the next KeyValue.
+ */
+public interface KeyValueScanner {
+  /**
+   * Look at the next KeyValue in this scanner, but do not iterate scanner.
+   * @return the next KeyValue
+   */
+  public KeyValue peek();
+  
+  /**
+   * Return the next KeyValue in this scanner, iterating the scanner 
+   * @return the next KeyValue
+   */
+  public KeyValue next();
+  
+  /**
+   * Seek the scanner at or after the specified KeyValue.
+   * @return true if scanner has values left, false if end of scanner
+   */
+  public boolean seek(KeyValue key);
+  
+  /**
+   * Close the KeyValue scanner.
+   */
+  public void close();
+}
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 11)
@@ -0,0 +1,72 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+/**
+ * Simple wrapper for a byte buffer and a counter.  Does not copy.
+ * <p>
+ * NOT thread-safe because it is not used in a multi-threaded context, yet.
+ */
+public class ColumnCount {
+  private byte [] bytes;
+  private int offset;
+  private int length;
+  private int count;
+  
+  public ColumnCount(byte [] column) {
+    this(column, 0);
+  }
+  
+  public ColumnCount(byte [] column, int count) {
+    this(column, 0, column.length, count);
+  }
+  
+  public ColumnCount(byte [] column, int offset, int length, int count) {
+    this.bytes = column;
+    this.offset = offset;
+    this.length = length;
+    this.count = count;
+  }
+  
+  public byte [] getBuffer(){
+    return this.bytes;
+  }
+  public int getOffset(){
+    return this.offset;
+  }
+  public int getLength(){
+    return this.length;
+  }  
+  
+  public int decrement() {
+    return --count;
+  }
+  
+  public int increment() {
+    return ++count;
+  }
+  
+  public boolean needMore(int max) {
+    if(this.count < max) {
+      return true;
+    }
+    return false;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 11)
@@ -0,0 +1,48 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+
+/**
+ * Implementing classes of this interface will be used for the tracking
+ * and enforcement of columns and numbers of versions during the course of a 
+ * Get or Scan operation.
+ * <p>
+ * Currently there are two different types of Store/Family-level queries.
+ * <ul><li>{@link ExplicitColumnTracker} is used when the query specifies
+ * one or more column qualifiers to return in the family.
+ * <li>{@link WildcardColumnTracker} is used when the query asks for all
+ * qualifiers within the family.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public interface ColumnTracker {
+  public MatchCode checkColumn(byte [] bytes, int offset, int length);
+  public void update();
+  public void reset();
+  public boolean done();
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -84,8 +84,11 @@
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
 import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
@@ -1741,40 +1744,7 @@
       throw convertThrowableToIOE(cleanup(t));
     }
   }
-  
-  public RowResult next(final long scannerId) throws IOException {
-    RowResult[] rrs = next(scannerId, 1);
-    return rrs.length == 0 ? null : rrs[0];
-  }
 
-  public RowResult [] next(final long scannerId, int nbRows) throws IOException {
-    checkOpen();
-    List<List<KeyValue>> results = new ArrayList<List<KeyValue>>();
-    try {
-      String scannerName = String.valueOf(scannerId);
-      InternalScanner s = scanners.get(scannerName);
-      if (s == null) {
-        throw new UnknownScannerException("Name: " + scannerName);
-      }
-      this.leases.renewLease(scannerName);
-      for (int i = 0; i < nbRows; i++) {
-        requestCount.incrementAndGet();
-        // Collect values to be returned here
-        List<KeyValue> values = new ArrayList<KeyValue>();
-        while (s.next(values)) {
-          if (!values.isEmpty()) {
-            // Row has something in it. Return the value.
-            results.add(values);
-            break;
-          }
-        }
-      }
-      return RowResult.createRowResultArray(results);
-    } catch (Throwable t) {
-      throw convertThrowableToIOE(cleanup(t));
-    }
-  }
-
   public void batchUpdate(final byte [] regionName, BatchUpdate b, long lockId)
   throws IOException {
     if (b.getRow() == null)
@@ -1835,17 +1805,14 @@
   // remote scanner interface
   //
 
-  public long openScanner(byte [] regionName, byte [][] cols, byte [] firstRow,
-    final long timestamp, final RowFilterInterface filter)
+  public long openScanner(byte [] regionName, byte [] startRow, Scan scan)
   throws IOException {
     checkOpen();
     NullPointerException npe = null;
     if (regionName == null) {
       npe = new NullPointerException("regionName is null");
-    } else if (cols == null) {
-      npe = new NullPointerException("columns to scan is null");
-    } else if (firstRow == null) {
-      npe = new NullPointerException("firstRow for scanner is null");
+    } else if (scan == null) {
+      npe = new NullPointerException("scan is null");
     }
     if (npe != null) {
       throw new IOException("Invalid arguments to openScanner", npe);
@@ -1853,8 +1820,7 @@
     requestCount.incrementAndGet();
     try {
       HRegion r = getRegion(regionName);
-      InternalScanner s =
-        r.getScanner(cols, firstRow, timestamp, filter);
+      InternalScanner s = r.getScanner(startRow, scan);
       long scannerId = addScanner(s);
       return scannerId;
     } catch (Throwable t) {
@@ -1873,7 +1839,35 @@
       createLease(scannerName, new ScannerListener(scannerName));
     return scannerId;
   }
-  
+
+  public Result [] next(final long scannerId, int nbRows) throws IOException {
+    checkOpen();
+    List<List<KeyValue>> results = new ArrayList<List<KeyValue>>();
+    try {
+      String scannerName = String.valueOf(scannerId);
+      InternalScanner s = scanners.get(scannerName);
+      if (s == null) {
+        throw new UnknownScannerException("Name: " + scannerName);
+      }
+      this.leases.renewLease(scannerName);
+      for (int i = 0; i < nbRows; i++) {
+        requestCount.incrementAndGet();
+        // Collect values to be returned here
+        List<KeyValue> values = new ArrayList<KeyValue>();
+        while (s.next(values)) {
+          if (!values.isEmpty()) {
+            // Row has something in it. Return the value.
+            results.add(values);
+            break;
+          }
+        }
+      }
+      return results.toArray(new Result[results.size()]);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
   public void close(final long scannerId) throws IOException {
     try {
       checkOpen();
@@ -2450,7 +2444,21 @@
       checkFileSystem();
       throw e;
     }
-    
-    
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  /** {@inheritDoc} */
+  public Result get(byte [] regionName, Get get) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.get(get, getLockFromId(get.getLockId()));
+    } catch(Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
 }
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(revision 11)
@@ -185,9 +185,9 @@
         // against a KeyValue.  Each invocation creates a new instance.  St.Ack.
 
         // Check this edit is for me.
-        byte[] column = val.getKeyValue().getColumn();
+        byte[] family = val.getKeyValue().getFamily();
         Long transactionId = val.getTransactionId();
-        if (!val.isTransactionEntry() || HLog.isMetaColumn(column)
+        if (!val.isTransactionEntry() || HLog.isMetaFamily(family)
             || !Bytes.equals(key.getRegionName(), regionInfo.getRegionName())) {
           continue;
         }
Index: src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java	(revision 11)
@@ -0,0 +1,165 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+import java.util.PriorityQueue;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+
+/**
+ * Implements a heap merge across any number of KeyValueScanners.
+ * <p>
+ * Implements KeyValueScanner itself.
+ * <p>
+ * This class is used at the Region level to merge across Stores
+ * and at the Store level to merge across the Memcache and StoreFiles.
+ * <p>
+ * In the Region case, we also need InternalScanner.next(List), so this class
+ * also implements InternalScanner.  WARNING: As is, if you try to use this
+ * as an InternalScanner at the Store level, you will get runtime exceptions. 
+ */
+public class KeyValueHeap implements KeyValueScanner, InternalScanner {
+  
+  private PriorityQueue<KeyValueScanner> heap;
+  
+  private KeyValueScanner current = null;
+  
+  private KVScannerComparator comparator;
+  
+  public KeyValueHeap(KeyValueScanner [] scanners, KVComparator comparator) {
+    this.comparator = new KVScannerComparator(comparator);
+    this.heap = new PriorityQueue<KeyValueScanner>(scanners.length, 
+        this.comparator);
+    for(KeyValueScanner scanner : scanners) {
+      this.heap.add(scanner);
+    }
+    this.current = heap.poll();
+  }
+  
+  public KeyValue peek() {
+    if(this.current == null) {
+      return null;
+    }
+    return this.current.peek();
+  }
+  
+  public KeyValue next()  {
+    if(this.current == null) {
+      return null;
+    }
+    KeyValue kvReturn = this.current.next();
+    KeyValue kvNext = this.current.peek();
+    if(kvNext == null) {
+      this.current.close();
+      this.current = this.heap.poll();
+    } else {
+      if(this.comparator.compare(kvNext, this.heap.peek().peek()) > 0) {
+        this.heap.add(this.current);
+        this.current = this.heap.poll();
+      }
+    }
+    return kvReturn;
+  }
+  
+  /**
+   * Gets the next row of keys from the top-most scanner.
+   * <p>
+   * This method takes care of updating the heap.
+   * <p>
+   * This can ONLY be called when you are using Scanners that implement
+   * InternalScanner as well as KeyValueScanner (a {@link StoreScanner}).
+   * @return true if there are more keys, false if all scanners are done 
+   */
+  public boolean next(List<KeyValue> result) throws IOException {
+    InternalScanner currentAsInternal = (InternalScanner)this.current;
+    if(!currentAsInternal.next(result)) {
+      this.current.close();
+    } else {
+      this.heap.add(this.current);
+    }
+    this.current = this.heap.poll();
+    return (this.current != null);
+  }
+  
+  private class KVScannerComparator implements Comparator<KeyValueScanner> {
+    private KVComparator kvComparator;
+    public KVScannerComparator(KVComparator kvComparator) {
+      this.kvComparator = kvComparator;
+    }
+    public int compare(KeyValueScanner left, KeyValueScanner right) {
+      return compare(left.peek(), right.peek());
+    }
+    public int compare(KeyValue left, KeyValue right) {
+      return this.kvComparator.compare(left, right);
+    }
+    public KVComparator getComparator() {
+      return this.kvComparator;
+    }
+  }
+
+  public void close() {
+    if(this.current != null) {
+      this.current.close();
+    }
+    KeyValueScanner scanner;
+    while((scanner = this.heap.poll()) != null) {
+      scanner.close();
+    }
+  }
+  
+  /**
+   * Seeks all scanners at or below the specified seek key.  If we earlied-out 
+   * of a row, we may end up skipping values that were never reached yet.
+   * Rather than iterating down, we want to give the opportunity to re-seek.
+   * <p>
+   * As individual scanners may run past their ends, those scanners are
+   * automatically closed and removed from the heap.
+   * @param key KeyValue to seek at or after
+   * @return true if KeyValues exist at or after specified key, false if not
+   */
+  public boolean seek(KeyValue seekKey) {
+    if(this.current == null) {
+      return false;
+    }
+    this.heap.add(this.current);
+    this.current = null;
+
+    KeyValueScanner scanner;
+    while((scanner = this.heap.poll()) != null) {
+      KeyValue topKey = scanner.peek();
+      if(comparator.getComparator().compare(seekKey, topKey) <= 0) { // Correct?
+        // Top KeyValue is at-or-after Seek KeyValue
+        this.current = scanner;
+        return true;
+      }
+      if(!scanner.seek(seekKey)) {
+        scanner.close();
+      } else {
+        this.heap.add(scanner);
+      }
+    }
+    // Heap is returning empty, scanner is done
+    return false;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -41,11 +41,9 @@
  */
 public interface InternalScanner extends Closeable {
   /**
-   * Grab the next row's worth of values. The scanner will return the most
-   * recent data value for each row that is not newer than the target time
-   * passed when the scanner was created.
+   * Grab the next row's worth of values.
    * @param results
-   * @return true if data was returned
+   * @return true if more rows exist after this one, false if scanner is done
    * @throws IOException
    */
   public boolean next(List<KeyValue> results)
@@ -55,11 +53,5 @@
    * Closes the scanner and releases any resources it has allocated
    * @throws IOException
    */
-  public void close() throws IOException;  
-  
-  /** @return true if the scanner is matching a column family or regex */
-  public boolean isWildcardScanner();
-  
-  /** @return true if the scanner is matching multiple column family members */
-  public boolean isMultipleMatchScanner();
+  public void close() throws IOException;
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/HLog.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HLog.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/HLog.java	(revision 11)
@@ -93,7 +93,7 @@
 public class HLog implements HConstants, Syncable {
   private static final Log LOG = LogFactory.getLog(HLog.class);
   private static final String HLOG_DATFILE = "hlog.dat.";
-  static final byte [] METACOLUMN = Bytes.toBytes("METACOLUMN:");
+  static final byte [] METAFAMILY = Bytes.toBytes("METAFAMILY");
   static final byte [] METAROW = Bytes.toBytes("METAROW");
   private final FileSystem fs;
   private final Path dir;
@@ -654,7 +654,7 @@
   }
 
   private HLogEdit completeCacheFlushLogEdit() {
-    return new HLogEdit(new KeyValue(METAROW, METACOLUMN,
+    return new HLogEdit(new KeyValue(METAROW, METAFAMILY, null,
       System.currentTimeMillis(), HLogEdit.COMPLETE_CACHE_FLUSH));
   }
 
@@ -672,8 +672,8 @@
    * @param column
    * @return true if the column is a meta column
    */
-  public static boolean isMetaColumn(byte [] column) {
-    return Bytes.equals(METACOLUMN, column);
+  public static boolean isMetaFamily(byte [] family) {
+    return Bytes.equals(METAFAMILY, family);
   }
   
   /**
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 11)
@@ -0,0 +1,89 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.hfile.HFileScanner;
+
+
+public class StoreFileScan {
+
+  private List<HFileScanner> scanners;
+  private QueryMatcher matcher;
+  
+  private byte [] startKey;
+  
+  public StoreFileScan(List<HFileScanner> scanners, QueryMatcher matcher) {
+    this.scanners = scanners;
+    this.matcher = matcher;
+    this.startKey = matcher.getStartKey().getBuffer();
+  }
+  
+  /**
+   * Performs a GET operation across multiple StoreFiles.
+   * <p>
+   * This style of StoreFile scanning goes through each
+   * StoreFile in its entirety, most recent first, before
+   * proceeding to the next StoreFile.
+   * <p>
+   * This strategy allows for optimal, stateless (no persisted Scanners)
+   * early-out scenarios.    
+   * @param result List to add results to
+   */
+  public void get(List<KeyValue> result) throws IOException {
+    for(HFileScanner scanner : this.scanners) {
+      this.matcher.update();
+      if(getStoreFile(scanner, result)) {
+        return;
+      }
+    }
+  }
+  
+  /**
+   * Performs a GET operation on a single StoreFile.
+   * @return true if done with this store, false if must continue to next 
+   */
+  public boolean getStoreFile(HFileScanner scanner, List<KeyValue> result) 
+  throws IOException {
+    if(scanner.seekTo(this.startKey) == -1) {
+      // No keys in StoreFile at or after specified startKey
+      return false;
+    }
+    do {
+      KeyValue kv = scanner.getKeyValue();
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    } while(scanner.next());
+    return false;
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/Memcache.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/Memcache.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/Memcache.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -32,7 +32,6 @@
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.SortedSet;
-import java.util.TreeMap;
 import java.util.TreeSet;
 import java.util.concurrent.ConcurrentSkipListSet;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
@@ -42,7 +41,6 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.regionserver.HRegion.Counter;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -621,93 +619,157 @@
   /**
    * @return a scanner over the keys in the Memcache
    */
-  InternalScanner getScanner(long timestamp,
-    final NavigableSet<byte []> targetCols, final byte [] firstRow)
+  KeyValueScanner getScanner()
   throws IOException {
     this.lock.readLock().lock();
     try {
-      return new MemcacheScanner(timestamp, targetCols, firstRow);
+      return new MemcacheScanner();
     } finally {
       this.lock.readLock().unlock();
     }
   }
 
+  //
+  // HBASE-880/1249/1304
+  //
+  
+  /**
+   * Perform a single-row Get on the memcache and snapshot, placing results
+   * into the specified KV list.
+   * <p>
+   * This will return true if it is determined that the query is complete
+   * and it is not necessary to check any storefiles after this.
+   * <p>
+   * Otherwise, it will return false and you should continue on.
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  public boolean get(QueryMatcher matcher, List<KeyValue> result)
+  throws IOException {
+    this.lock.readLock().lock();
+    try {
+      if(internalGet(this.memcache, matcher, result)) {
+        return true;
+      }
+      matcher.update();
+      if(internalGet(this.snapshot, matcher, result)) {
+        return true;
+      }
+      return false;
+    } finally {
+      this.lock.readLock().unlock();
+    }
+  }
+  
+  /**
+   * 
+   * @param set memcache or snapshot
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  private boolean internalGet(SortedSet<KeyValue> set, QueryMatcher matcher,
+      List<KeyValue> result) throws IOException {
+    if(set.isEmpty()) return false;
+    // Seek to startKey
+    set = set.tailSet(matcher.getStartKey());
+    for(KeyValue kv : set) {
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    }
+    return false;
+  }
+  
   //////////////////////////////////////////////////////////////////////////////
-  // MemcacheScanner implements the InternalScanner.
+  // MemcacheScanner implements the KeyValueScanner.
   // It lets the caller scan the contents of the Memcache.
+  // This behaves as if it were a real scanner but does not maintain position
+  // in the Memcache tree.
   //////////////////////////////////////////////////////////////////////////////
 
-  private class MemcacheScanner extends HAbstractScanner {
-    private KeyValue current;
-    private final NavigableSet<byte []> columns;
-    private final NavigableSet<KeyValue> deletes;
-    private final Map<KeyValue, Counter> versionCounter;
-    private final long now = System.currentTimeMillis();
-
-    MemcacheScanner(final long timestamp, final NavigableSet<byte []> columns,
-      final byte [] firstRow)
-    throws IOException {
-      // Call to super will create ColumnMatchers and whether this is a regex
-      // scanner or not.  Will also save away timestamp.  Also sorts rows.
-      super(timestamp, columns);
-      this.deletes = new TreeSet<KeyValue>(comparatorIgnoreType);
-      this.versionCounter =
-        new TreeMap<KeyValue, Counter>(comparatorIgnoreTimestamp);
-      this.current = KeyValue.createFirstOnRow(firstRow, timestamp);
-      // If we're being asked to scan explicit columns rather than all in 
-      // a family or columns that match regexes, cache the sorted array of
-      // columns.
-      this.columns = isWildcardScanner()? null: columns;
-    }
-
-    @Override
-    public boolean next(final List<KeyValue> keyvalues)
-    throws IOException {
-      if (this.scannerClosed) {
+  private class MemcacheScanner implements KeyValueScanner {
+    private KeyValue current = null;
+    private List<KeyValue> result = new ArrayList<KeyValue>();
+    private int idx = 0;
+    
+    MemcacheScanner() {}
+    
+    public boolean seek(KeyValue key) {
+      try {
+        current = memcache.tailSet(key).first();
+        if(current == null) {
+          close();
+          return false;
+        }
+        return cacheNextRow();
+      } catch(Exception e) {
+        close();
         return false;
       }
-      while (keyvalues.isEmpty() && this.current != null) {
-        // Deletes are per row.
-        if (!deletes.isEmpty()) {
-          deletes.clear();
+    }
+    
+    public KeyValue peek() {
+      if(idx >= result.size()) {
+        if(!cacheNextRow()) {
+          return null;
         }
-        if (!versionCounter.isEmpty()) {
-          versionCounter.clear();
+        return peek();
+      }
+      return result.get(idx);
+    }
+    
+    public KeyValue next() {
+      if(idx >= result.size()) {
+        if(!cacheNextRow()) {
+          return null;
         }
-        // The getFull will take care of expired and deletes inside memcache.
-        // The first getFull when row is the special empty bytes will return
-        // nothing so we go around again.  Alternative is calling a getNextRow
-        // if row is null but that looks like it would take same amount of work
-        // so leave it for now.
-        getFull(this.current, isWildcardScanner()? null: this.columns, null, 1,
-          versionCounter, deletes, keyvalues, this.now);
-        for (KeyValue bb: keyvalues) {
-          if (isWildcardScanner()) {
-            // Check the results match.  We only check columns, not timestamps.
-            // We presume that timestamps have been handled properly when we
-            // called getFull.
-            if (!columnMatch(bb)) {
-              keyvalues.remove(bb);
-            }
-          }
+        return next();
+      }
+      return result.get(idx++);
+    }
+    
+    public boolean cacheNextRow() {
+      NavigableSet<KeyValue> keys;
+      try {
+        keys = memcache.tailSet(current);
+      } catch(Exception e) {
+        close();
+        return false;
+      }
+      if(keys == null || keys.isEmpty()) {
+        close();
+        return false;
+      }
+      byte [] row = keys.first().getRow();
+      for(KeyValue key : keys) {
+        if(comparator.compareRows(key, row) != 0) {
+          current = key;
+          break;
         }
-        // Add any deletes found so they are available to the StoreScanner#next.
-        if (!this.deletes.isEmpty()) {
-          keyvalues.addAll(deletes);
-        }
-        this.current = getNextRow(this.current);
-        // Change current to be column-less and to have the scanners' now.  We
-        // do this because first item on 'next row' may not have the scanners'
-        // now time which will cause trouble down in getFull; same reason no
-        // column.
-        if (this.current != null) this.current = this.current.cloneRow(this.now);
+        result.add(key);
       }
-      return !keyvalues.isEmpty();
+      return true;
     }
 
     public void close() {
-      if (!scannerClosed) {
-        scannerClosed = true;
+      current = null;
+      idx = 0;
+      if(!result.isEmpty()) {
+        result.clear();
       }
     }
   }
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -21,306 +21,74 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
 
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 
 /**
- * A scanner that iterates through HStore files
+ * A KeyValue scanner that iterates over a single HFile
  */
-class StoreFileScanner extends HAbstractScanner
-implements ChangedReadersObserver {
-    // Keys retrieved from the sources
-  private volatile KeyValue keys[];
+class StoreFileScanner implements KeyValueScanner {
   
-  // Readers we go against.
-  private volatile HFileScanner [] scanners;
+  private HFileScanner hfs;
+  private KeyValue cur = null;
   
-  // Store this scanner came out of.
-  private final Store store;
-  
-  // Used around replacement of Readers if they change while we're scanning.
-  private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
-
-  private final long now = System.currentTimeMillis();
-
   /**
-   * @param store
-   * @param timestamp
-   * @param columns
-   * @param firstRow
-   * @param deletes Set of running deletes
-   * @throws IOException
+   * Implements a {@link KeyValueScanner} on top of the specified {@link HFileScanner}
+   * @param hfs HFile scanner
    */
-  public StoreFileScanner(final Store store, final long timestamp,
-    final NavigableSet<byte []> columns, final byte [] firstRow)
-  throws IOException {
-    super(timestamp, columns);
-    this.store = store;
-    this.store.addChangedReaderObserver(this);
-    try {
-      openScanner(firstRow);
-    } catch (Exception ex) {
-      close();
-      IOException e = new IOException("HStoreScanner failed construction");
-      e.initCause(ex);
-      throw e;
-    }
+  public StoreFileScanner(HFileScanner hfs) {
+    this.hfs = hfs;
   }
-
-  /*
-   * Go open new scanners and cue them at <code>firstRow</code>.
-   * Closes existing Readers if any.
-   * @param firstRow
-   * @throws IOException
-   */
-  private void openScanner(final byte [] firstRow) throws IOException {
-    List<HFileScanner> s =
-      new ArrayList<HFileScanner>(this.store.getStorefiles().size());
-    Map<Long, StoreFile> map = this.store.getStorefiles().descendingMap();
-    for (StoreFile f: map.values()) {
-       s.add(f.getReader().getScanner());
-    }
-    this.scanners = s.toArray(new HFileScanner [] {});
-    this.keys = new KeyValue[this.scanners.length];
-    // Advance the readers to the first pos.
-    KeyValue firstKey = (firstRow != null && firstRow.length > 0)?
-      new KeyValue(firstRow, HConstants.LATEST_TIMESTAMP): null;
-    for (int i = 0; i < this.scanners.length; i++) {
-      if (firstKey != null) {
-        if (seekTo(i, firstKey)) {
-          continue;
-        }
-      }
-      while (getNext(i)) {
-        if (columnMatch(i)) {
-          break;
-        }
-      }
-    }
+  
+  public KeyValue peek() {
+    return cur;
   }
-
-  /**
-   * For a particular column i, find all the matchers defined for the column.
-   * Compare the column family and column key using the matchers. The first one
-   * that matches returns true. If no matchers are successful, return false.
-   * 
-   * @param i index into the keys array
-   * @return true if any of the matchers for the column match the column family
-   * and the column key.
-   * @throws IOException
-   */
-  boolean columnMatch(int i) throws IOException {
-    return columnMatch(keys[i]);
-  }
-
-  /**
-   * Get the next set of values for this scanner.
-   * 
-   * @param key The key that matched
-   * @param results All the results for <code>key</code>
-   * @return true if a match was found
-   * @throws IOException
-   * 
-   * @see org.apache.hadoop.hbase.regionserver.InternalScanner#next(org.apache.hadoop.hbase.HStoreKey, java.util.SortedMap)
-   */
-  @Override
-  public boolean next(List<KeyValue> results)
-  throws IOException {
-    if (this.scannerClosed) {
-      return false;
+  
+  public KeyValue next() {
+    KeyValue retKey = cur;
+    cur = hfs.getKeyValue();
+    try {
+      hfs.next();
+    } catch(IOException e) {
+      // Only occurs if the scanner is not seeked, this is never the case
+      // as we seek immediately after construction in StoreScanner
     }
-    this.lock.readLock().lock();
+    return retKey;
+  }
+  
+  public boolean seek(KeyValue key) {
     try {
-      // Find the next viable row label (and timestamp).
-      KeyValue viable = getNextViableRow();
-      if (viable == null) {
+      if(!seekAtOrAfter(hfs, key)) {
+        close();
         return false;
       }
-
-      // Grab all the values that match this row/timestamp
-      boolean addedItem = false;
-      for (int i = 0; i < keys.length; i++) {
-        // Fetch the data
-        while ((keys[i] != null) &&
-            (this.store.comparator.compareRows(this.keys[i], viable) == 0)) {
-          // If we are doing a wild card match or there are multiple matchers
-          // per column, we need to scan all the older versions of this row
-          // to pick up the rest of the family members
-          if(!isWildcardScanner()
-              && !isMultipleMatchScanner()
-              && (keys[i].getTimestamp() != viable.getTimestamp())) {
-            break;
-          }
-          if (columnMatch(i)) {
-            // We only want the first result for any specific family member
-            // TODO: Do we have to keep a running list of column entries in
-            // the results across all of the StoreScanner?  Like we do
-            // doing getFull?
-            if (!results.contains(keys[i])) {
-              results.add(keys[i]);
-              addedItem = true;
-            }
-          }
-
-          if (!getNext(i)) {
-            closeSubScanner(i);
-          }
-        }
-        // Advance the current scanner beyond the chosen row, to
-        // a valid timestamp, so we're ready next time.
-        while ((keys[i] != null) &&
-            ((this.store.comparator.compareRows(this.keys[i], viable) <= 0) ||
-                (keys[i].getTimestamp() > this.timestamp) ||
-                !columnMatch(i))) {
-          getNext(i);
-        }
-      }
-      return addedItem;
-    } finally {
-      this.lock.readLock().unlock();
+      cur = hfs.getKeyValue();
+      hfs.next();
+      return true;
+    } catch(IOException ioe) {
+      close();
+      return false;
     }
   }
-
-  /*
-   * @return An instance of <code>ViableRow</code>
-   * @throws IOException
-   */
-  private KeyValue getNextViableRow() throws IOException {
-    // Find the next viable row label (and timestamp).
-    KeyValue viable = null;
-    long viableTimestamp = -1;
-    long ttl = store.ttl;
-    for (int i = 0; i < keys.length; i++) {
-      // The first key that we find that matches may have a timestamp greater
-      // than the one we're looking for. We have to advance to see if there
-      // is an older version present, since timestamps are sorted descending
-      while (keys[i] != null &&
-          keys[i].getTimestamp() > this.timestamp &&
-          columnMatch(i) &&
-          getNext(i)) {
-        if (columnMatch(i)) {
-          break;
-        }
-      }
-      if((keys[i] != null)
-          // If we get here and keys[i] is not null, we already know that the
-          // column matches and the timestamp of the row is less than or equal
-          // to this.timestamp, so we do not need to test that here
-          && ((viable == null) ||
-            (this.store.comparator.compareRows(this.keys[i], viable) < 0) ||
-            ((this.store.comparator.compareRows(this.keys[i], viable) == 0) &&
-              (keys[i].getTimestamp() > viableTimestamp)))) {
-        if (ttl == HConstants.FOREVER || now < keys[i].getTimestamp() + ttl) {
-          viable = keys[i];
-        } else {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("getNextViableRow :" + keys[i] + ": expired, skipped");
-          }
-        }
-      }
-    }
-    return viable;
-  }
-
-  /*
-   * The user didn't want to start scanning at the first row. This method
-   * seeks to the requested row.
-   *
-   * @param i which iterator to advance
-   * @param firstRow seek to this row
-   * @return true if we found the first row and so the scanner is properly
-   * primed or true if the row was not found and this scanner is exhausted.
-   */
-  private boolean seekTo(int i, final KeyValue firstKey)
-  throws IOException {
-    if (firstKey == null) {
-      if (!this.scanners[i].seekTo()) {
-        closeSubScanner(i);
-        return true;
-      }
-    } else {
-      // TODO: sort columns and pass in column as part of key so we get closer.
-      if (!Store.getClosest(this.scanners[i], firstKey)) {
-        closeSubScanner(i);
-        return true;
-      }
-    }
-    this.keys[i] = this.scanners[i].getKeyValue();
-    return isGoodKey(this.keys[i]);
-  }
-
-  /**
-   * Get the next value from the specified reader.
-   * 
-   * @param i which reader to fetch next value from
-   * @return true if there is more data available
-   */
-  private boolean getNext(int i) throws IOException {
-    boolean result = false;
-    while (true) {
-      if ((this.scanners[i].isSeeked() && !this.scanners[i].next()) ||
-          (!this.scanners[i].isSeeked() && !this.scanners[i].seekTo())) {
-        closeSubScanner(i);
-        break;
-      }
-      this.keys[i] = this.scanners[i].getKeyValue();
-      if (isGoodKey(this.keys[i])) {
-          result = true;
-          break;
-      }
-    }
-    return result;
-  }
-
-  /*
-   * @param kv
-   * @return True if good key candidate.
-   */
-  private boolean isGoodKey(final KeyValue kv) {
-    return !Store.isExpired(kv, this.store.ttl, this.now);
-  }
-
-  /** Close down the indicated reader. */
-  private void closeSubScanner(int i) {
-    this.scanners[i] = null;
-    this.keys[i] = null;
-  }
-
-  /** Shut it down! */
+  
   public void close() {
-    if (!this.scannerClosed) {
-      this.store.deleteChangedReaderObserver(this);
-      try {
-        for(int i = 0; i < this.scanners.length; i++) {
-          closeSubScanner(i);
-        }
-      } finally {
-        this.scannerClosed = true;
-      }
-    }
+    // Nothing to close on HFileScanner?
+    cur = null;
   }
-
-  // Implementation of ChangedReadersObserver
   
-  public void updateReaders() throws IOException {
-    this.lock.writeLock().lock();
-    try {
-      // The keys are currently lined up at the next row to fetch.  Pass in
-      // the current row as 'first' row and readers will be opened and cue'd
-      // up so future call to next will start here.
-      KeyValue viable = getNextViableRow();
-      openScanner(viable.getRow());
-      LOG.debug("Replaced Scanner Readers at row " +
-        viable.getRow().toString());
-    } finally {
-      this.lock.writeLock().unlock();
+  public static boolean seekAtOrAfter(HFileScanner s, KeyValue k)
+  throws IOException {
+    int result = s.seekTo(k.getBuffer(), k.getKeyOffset(), k.getKeyLength());
+    if(result < 0) {
+      // Passed KV is smaller than first KV in file, work from start of file
+      return s.seekTo();
+    } else if(result > 0) {
+      // Passed KV is larger than current KV in file, if there is a next
+      // it is the "after", if not then this scanner is done.
+      return s.next();
     }
+    // Seeked to the exact key
+    return true;
   }
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -25,288 +25,125 @@
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
-import java.util.TreeSet;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 
 /**
  * Scanner scans both the memcache and the HStore
  */
-class StoreScanner implements InternalScanner,  ChangedReadersObserver {
+class StoreScanner implements KeyValueScanner, ChangedReadersObserver {
   static final Log LOG = LogFactory.getLog(StoreScanner.class);
-
-  private InternalScanner [] scanners;
-  private List<KeyValue> [] resultSets;
-  private boolean wildcardMatch = false;
-  private boolean multipleMatchers = false;
-  private RowFilterInterface dataFilter;
+  
   private Store store;
-  private final long timestamp;
-  private final NavigableSet<byte []> columns;
   
-  // Indices for memcache scanner and hstorefile scanner.
-  private static final int MEMS_INDEX = 0;
-  private static final int HSFS_INDEX = MEMS_INDEX + 1;
+  private QueryMatcher matcher;
   
+  private KeyValueHeap heap;
+
   // Used around transition from no storefile to the first.
   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
-
+  
   // Used to indicate that the scanner has closed (see HBASE-1107)
   private final AtomicBoolean closing = new AtomicBoolean(false);
-
-  /** Create an Scanner with a handle on the memcache and HStore files. */
-  @SuppressWarnings("unchecked")
-  StoreScanner(Store store, final NavigableSet<byte []> targetCols,
-    byte [] firstRow, long timestamp, RowFilterInterface filter) 
-  throws IOException {
-    this.store = store;
-    this.dataFilter = filter;
-    if (null != dataFilter) {
-      dataFilter.reset();
-    }
-    this.scanners = new InternalScanner[2];
-    this.resultSets = new List[scanners.length];
-    // Save these args in case we need them later handling change in readers
-    // See updateReaders below.
-    this.timestamp = timestamp;
-    this.columns = targetCols;
-    try {
-      scanners[MEMS_INDEX] =
-        store.memcache.getScanner(timestamp, targetCols, firstRow);
-      scanners[HSFS_INDEX] =
-        new StoreFileScanner(store, timestamp, targetCols, firstRow);
-      for (int i = MEMS_INDEX; i < scanners.length; i++) {
-        checkScannerFlags(i);
-      }
-    } catch (IOException e) {
-      doClose();
-      throw e;
-    }
-    
-    // Advance to the first key in each scanner.
-    // All results will match the required column-set and scanTime.
-    for (int i = MEMS_INDEX; i < scanners.length; i++) {
-      setupScanner(i);
-    }
-    this.store.addChangedReaderObserver(this);
-  }
   
-  /*
-   * @param i Index.
+  /**
+   * Opens a scanner across memcache, snapshot, and all StoreFiles.
    */
-  private void checkScannerFlags(final int i) {
-    if (this.scanners[i].isWildcardScanner()) {
-      this.wildcardMatch = true;
-    }
-    if (this.scanners[i].isMultipleMatchScanner()) {
-      this.multipleMatchers = true;
-    }
-  }
-  
-  /*
-   * Do scanner setup.
-   * @param i
-   * @throws IOException
-   */
-  private void setupScanner(final int i) throws IOException {
-    this.resultSets[i] = new ArrayList<KeyValue>();
-    if (this.scanners[i] != null && !this.scanners[i].next(this.resultSets[i])) {
-      closeScanner(i);
-    }
-  }
-
-  /** @return true if the scanner is a wild card scanner */
-  public boolean isWildcardScanner() {
-    return this.wildcardMatch;
-  }
-
-  /** @return true if the scanner is a multiple match scanner */
-  public boolean isMultipleMatchScanner() {
-    return this.multipleMatchers;
-  }
-
-  public boolean next(List<KeyValue> results)
+  StoreScanner(Store store, Scan scan, final NavigableSet<byte[]> columns,
+      byte [] firstRow)
   throws IOException {
-    this.lock.readLock().lock();
-    try {
-    // Filtered flag is set by filters.  If a cell has been 'filtered out'
-    // -- i.e. it is not to be returned to the caller -- the flag is 'true'.
-    boolean filtered = true;
-    boolean moreToFollow = true;
-    while (filtered && moreToFollow) {
-      // Find the lowest-possible key.
-      KeyValue chosen = null;
-      long chosenTimestamp = -1;
-      for (int i = 0; i < this.scanners.length; i++) {
-        KeyValue kv = this.resultSets[i] == null || this.resultSets[i].isEmpty()?
-          null: this.resultSets[i].get(0);
-        if (kv == null) {
-          continue;
-        }
-        if (scanners[i] != null &&
-            (chosen == null ||
-              (this.store.comparator.compareRows(kv, chosen) < 0) ||
-              ((this.store.comparator.compareRows(kv, chosen) == 0) &&
-              (kv.getTimestamp() > chosenTimestamp)))) {
-          chosen = kv;
-          chosenTimestamp = chosen.getTimestamp();
-        }
-      }
+    this.store = store;
+    matcher = new QueryMatcher(firstRow, store.getFamily().getName(),
+        columns, scan.getMaxVersions(), scan.getTimeRange(), store.ttl,
+        store.comparator.getRawComparator());
 
-      // Filter whole row by row key?
-      filtered = dataFilter == null || chosen == null? false:
-        dataFilter.filterRowKey(chosen.getBuffer(), chosen.getRowOffset(),
-          chosen.getRowLength());
+    List<KeyValueScanner> scanners = getStoreFileScanners();
+    scanners.add(store.memcache.getScanner());
 
-      // Store results for each sub-scanner.
-      if (chosenTimestamp >= 0 && !filtered) {
-        NavigableSet<KeyValue> deletes =
-          new TreeSet<KeyValue>(this.store.comparatorIgnoringType);
-        for (int i = 0; i < scanners.length && !filtered; i++) {
-          if ((scanners[i] != null && !filtered && moreToFollow &&
-              this.resultSets[i] != null && !this.resultSets[i].isEmpty())) {
-            // Test this resultset is for the 'chosen' row.
-            KeyValue firstkv = resultSets[i].get(0);
-            if (!this.store.comparator.matchingRows(firstkv, chosen)) {
-              continue;
-            }
-            // Its for the 'chosen' row, work it.
-            for (KeyValue kv: resultSets[i]) {
-              if (kv.isDeleteType()) {
-                deletes.add(kv);
-              } else if ((deletes.isEmpty() || !deletes.contains(kv)) &&
-                  !filtered && moreToFollow && !results.contains(kv)) {
-                if (this.dataFilter != null) {
-                  // Filter whole row by column data?
-                  int rowlength = kv.getRowLength();
-                  int columnoffset = kv.getColumnOffset(rowlength);
-                  filtered = dataFilter.filterColumn(kv.getBuffer(),
-                      kv.getRowOffset(), rowlength,
-                    kv.getBuffer(), columnoffset, kv.getColumnLength(columnoffset),
-                    kv.getBuffer(), kv.getValueOffset(), kv.getValueLength());
-                  if (filtered) {
-                    results.clear();
-                    break;
-                  }
-                }
-                results.add(kv);
-                /* REMOVING BECAUSE COULD BE BUNCH OF DELETES IN RESULTS
-                   AND WE WANT TO INCLUDE THEM -- below short-circuit is
-                   probably not wanted.
-                // If we are doing a wild card match or there are multiple
-                // matchers per column, we need to scan all the older versions of 
-                // this row to pick up the rest of the family members
-                if (!wildcardMatch && !multipleMatchers &&
-                    (kv.getTimestamp() != chosenTimestamp)) {
-                  break;
-                }
-                */
-              }
-            }
-            // Move on to next row.
-            resultSets[i].clear();
-            if (!scanners[i].next(resultSets[i])) {
-              closeScanner(i);
-            }
-          }
-        }
-      }
-
-      moreToFollow = chosenTimestamp >= 0;
-      if (dataFilter != null) {
-        if (dataFilter.filterAllRemaining()) {
-          moreToFollow = false;
-        }
-      }
-
-      if (results.isEmpty() && !filtered) {
-        // There were no results found for this row.  Marked it as 
-        // 'filtered'-out otherwise we will not move on to the next row.
-        filtered = true;
-      }
-    }
+    // Seek all scanners to the initial key
+    seek(matcher.getStartKey());
     
-    // If we got no results, then there is no more to follow.
-    if (results == null || results.isEmpty()) {
-      moreToFollow = false;
-    }
+    // Combine all seeked scanners with a heap
+    heap = new KeyValueHeap(
+      scanners.toArray(new KeyValueScanner[scanners.size()]), store.comparator);
     
-    // Make sure scanners closed if no more results
-    if (!moreToFollow) {
-      for (int i = 0; i < scanners.length; i++) {
-        if (null != scanners[i]) {
-          closeScanner(i);
-        }
-      }
-    }
-    
-    return moreToFollow;
-    } finally {
-      this.lock.readLock().unlock();
-    }
   }
-
-  /** Shut down a single scanner */
-  void closeScanner(int i) {
-    try {
-      try {
-        scanners[i].close();
-      } catch (IOException e) {
-        LOG.warn(Bytes.toString(store.storeName) + " failed closing scanner " +
-          i, e);
-      }
-    } finally {
-      scanners[i] = null;
-      resultSets[i] = null;
-    }
+  
+  public KeyValue peek() {
+    return this.heap.peek();
   }
-
+  
+  public KeyValue next() {
+    return this.heap.next();
+  }
+  
   public void close() {
     this.closing.set(true);
     this.store.deleteChangedReaderObserver(this);
-    doClose();
+    this.heap.close();
   }
   
-  private void doClose() {
-    for (int i = MEMS_INDEX; i < scanners.length; i++) {
-      if (scanners[i] != null) {
-        closeScanner(i);
+  public boolean seek(KeyValue key) {
+    return this.heap.seek(key);
+  }
+  
+  /**
+   * Get the next row of values from this Store.
+   * @param result
+   * @return true if there are more rows, false if scanner is done
+   */
+  public boolean next(List<KeyValue> result) throws IOException {
+    matcher.setRow(this.heap.peek().getRow());
+    KeyValue kv;
+    while((kv = this.heap.peek()) != null) {
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(this.heap.next());
+          continue;
+        case SKIP:
+          continue;
+        case NEXT:
+        case DONE:
+          return true;
       }
     }
+    // No more keys
+    close();
+    return false;
   }
   
+  private List<KeyValueScanner> getStoreFileScanners() {
+    List<HFileScanner> s =
+      new ArrayList<HFileScanner>(this.store.getStorefilesCount());
+    Map<Long, StoreFile> map = this.store.getStorefiles().descendingMap();
+    for(StoreFile sf : map.values()) {
+      s.add(sf.getReader().getScanner());
+    }
+    List<KeyValueScanner> scanners =
+      new ArrayList<KeyValueScanner>(s.size()+1);
+    for(HFileScanner hfs : s) {
+      scanners.add(new StoreFileScanner(hfs));
+    }
+    return scanners;
+  }
+   
   // Implementation of ChangedReadersObserver
-  
   public void updateReaders() throws IOException {
     if (this.closing.get()) {
       return;
     }
     this.lock.writeLock().lock();
     try {
-      Map<Long, StoreFile> map = this.store.getStorefiles();
-      if (this.scanners[HSFS_INDEX] == null && map != null && map.size() > 0) {
-        // Presume that we went from no readers to at least one -- need to put
-        // a HStoreScanner in place.
-        try {
-          // I think its safe getting key from mem at this stage -- it shouldn't have
-          // been flushed yet
-          // TODO: MAKE SURE WE UPDATE FROM TRUNNK.
-          this.scanners[HSFS_INDEX] = new StoreFileScanner(this.store,
-              this.timestamp, this. columns, this.resultSets[MEMS_INDEX].get(0).getRow());
-          checkScannerFlags(HSFS_INDEX);
-          setupScanner(HSFS_INDEX);
-          LOG.debug("Added a StoreFileScanner to outstanding HStoreScanner");
-        } catch (IOException e) {
-          doClose();
-          throw e;
-        }
-      }
+      // Could do this pretty nicely with KeyValueHeap, but the existing
+      // implementation of this method only updated if no existing storefiles?
+      // Lets discuss.
+      return;
     } finally {
       this.lock.writeLock().unlock();
     }
Index: src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 11)
@@ -0,0 +1,283 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.NavigableSet;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.TimeRange;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This is the primary class used to process KeyValues during a Get or Scan
+ * operation.
+ * <p>
+ * It encapsulates the handling of the column and version input parameters to 
+ * the query through a {@link ColumnTracker}.
+ * <p>
+ * Deletes are handled using the {@link DeleteTracker}.
+ * <p>
+ * All other query parameters are accessed from the client-specified Get.
+ * <p>
+ * The primary method used is {@link match} with the current KeyValue.  It will
+ * return a {@link MatchCode} 
+ * 
+ * , deletes,
+ * versions, 
+ */
+public class QueryMatcher {
+  
+  /**
+   * {@link match} return codes.  These instruct the scanner moving through
+   * Memcaches and StoreFiles what to do with the current KeyValue.
+   * <p>
+   * Additionally, this contains "early-out" language to tell the scanner to
+   * move on to the next File (Memcache or Storefile), or to return immediately.
+   */
+  static enum MatchCode { 
+    /**
+     * Include KeyValue in the returned result
+     */
+    INCLUDE,
+    
+    /**
+     * Do not include KeyValue in the returned result
+     */
+    SKIP,
+    
+    /**
+     * Do not include, jump to next StoreFile or Memcache (in time order)
+     */
+    NEXT,
+    
+    /**
+     * Do not include, return current result
+     */
+    DONE
+  }
+  
+  /** Keeps track of deletes */
+  private DeleteTracker deletes;
+  
+  /** Keeps track of columns and versions */
+  private ColumnTracker columns;
+  
+  /** Key to seek to in Memcache and StoreFiles */
+  private KeyValue startKey;
+  
+  /** Row comparator for the region this query is for */
+  private KeyComparator rowComparator;
+  
+  /** Row the query is on */
+  private byte [] row;
+  
+  /** TimeRange the query is for */
+  private TimeRange tr;
+  
+  /** Oldest allowed version stamp for TTL enforcement */
+  private long oldestStamp;
+  
+  public QueryMatcher(byte [] row, byte [] family, NavigableSet<byte[]> columns,
+      int maxVersions, TimeRange tr, long ttl, KeyComparator rowComparator) {
+    this.row = row;
+    this.tr = tr;
+    this.oldestStamp = System.currentTimeMillis() - ttl;
+    this.rowComparator = rowComparator;
+    this.deletes =  new GetDeleteTracker();
+    // 
+    this.startKey = KeyValue.createFirstOnRow(row);
+    // Single branch to deal with two types of Gets (columns vs all in family)
+    if(columns == null) {
+      this.columns = new WildcardColumnTracker(maxVersions);
+    } else {
+      this.columns = new ExplicitColumnTracker(columns,maxVersions);
+      this.startKey = KeyValue.createFirstOnRow(row, family, columns.first());
+    }
+  }
+  
+  public QueryMatcher(QueryMatcher matcher, byte [] row) {
+    this.row = row;
+    this.tr = matcher.getTimeRange();
+    this.oldestStamp = matcher.getOldestStamp();
+    this.rowComparator = matcher.getRowComparator();
+    this.columns = matcher.getColumnTracker();
+    this.deletes = matcher.getDeleteTracker();
+    this.startKey = matcher.getStartKey();
+    reset();
+  }
+  
+  /**
+   * Main method for ColumnMatcher.
+   * <p>
+   * Determines whether the specified KeyValue should be included in the
+   * result or not.
+   * <p>
+   * Contains additional language to early-out of the current file or to
+   * return immediately.
+   * <p>
+   * Things to be checked:<ul>
+   * <li>Row
+   * <li>TTL
+   * <li>Type
+   * <li>TimeRange
+   * <li>Deletes
+   * <li>Column
+   * <li>Versions
+   * @param kv KeyValue to check
+   * @return MatchCode: include, skip, next, done
+   */
+  public MatchCode match(KeyValue kv) throws IOException {
+    if(this.columns.done()) {
+      return MatchCode.DONE;
+    }
+    
+    // Directly act on KV buffer
+    byte [] bytes = kv.getBuffer();
+    int offset = kv.getOffset();
+    
+    int keyLength = Bytes.toInt(bytes, offset);
+    offset += KeyValue.ROW_OFFSET;
+    
+    short rowLength = Bytes.toShort(bytes, offset);
+    offset += Bytes.SIZEOF_SHORT;
+    
+    /* Check ROW
+     * If past query's row, go to next StoreFile
+     * If not reached query's row, go to next KeyValue
+     */ 
+//  int ret = Bytes.compareTo(row, 0, row.length, bytes, offset, rowLength);
+    int ret = this.rowComparator.compareRows(row, 0, row.length,
+        bytes, offset, rowLength);
+    if(ret <= -1) {
+      // Have reached the next row
+      return MatchCode.NEXT;
+    } else if(ret >= 1) {
+      // At a previous row
+      return MatchCode.SKIP;
+    }
+    offset += rowLength;
+    
+    byte familyLength = bytes[offset];
+    offset += Bytes.SIZEOF_BYTE + familyLength;
+    
+    int columnLength = keyLength + KeyValue.ROW_OFFSET -
+      (offset - kv.getOffset()) - KeyValue.TIMESTAMP_TYPE_SIZE;
+    int columnOffset = offset;
+    offset += columnLength;
+    
+    /* Check TTL
+     * If expired, go to next KeyValue
+     */
+    long timestamp = Bytes.toLong(bytes, offset);
+    if(isExpired(timestamp)) {
+      return MatchCode.NEXT;
+    }
+    offset += Bytes.SIZEOF_LONG;
+    
+    /* Check TYPE
+     * If a delete within (or after) time range, add to deletes
+     * Move to next KeyValue
+     */
+    byte type = bytes[offset];
+    if(isDelete(type)) {
+      if(tr.withinOrAfterTimeRange(timestamp)) {
+        this.deletes.add(bytes, columnOffset, columnLength, timestamp, type);
+      }
+      return MatchCode.SKIP;
+    }
+    
+    /* Check TimeRange
+     * If outside of range, move to next KeyValue
+     */
+    if(!tr.withinTimeRange(timestamp)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Deletes
+     * If deleted, move to next KeyValue 
+     */
+    if(!deletes.isEmpty() && deletes.isDeleted(bytes, columnOffset,
+        columnLength, timestamp)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Column and Versions
+     * Returns a MatchCode directly, identical language
+     * If matched column without enough versions, include
+     * If enough versions of this column or does not match, skip
+     * If have moved past 
+     * If enough versions of everything, 
+     */
+    return columns.checkColumn(bytes, columnOffset, columnLength);
+  }
+  
+  private boolean isDelete(byte type) {
+    return (type != KeyValue.Type.Put.getCode());
+  }
+  
+  private boolean isExpired(long timestamp) {
+    return (timestamp < oldestStamp);
+  }
+
+  /**
+   * Called after reading each section (memcache, snapshot, storefiles).
+   * <p>
+   * This method will update the internal structures to be accurate for
+   * the next section. 
+   */
+  public void update() {
+    this.deletes.update();
+    this.columns.update();
+  }
+  
+  public void reset() {
+    this.deletes.reset();
+    this.columns.reset();
+  }
+  
+  public void setRow(byte [] row) {
+    this.row = row;
+  }
+  
+  public KeyValue getStartKey() {
+    return this.startKey;
+  }
+  
+  public TimeRange getTimeRange() {
+    return this.tr;
+  }
+  
+  public long getOldestStamp() {
+    return this.oldestStamp;
+  }
+  
+  public KeyComparator getRowComparator() {
+    return this.rowComparator;
+  }
+  
+  public ColumnTracker getColumnTracker() {
+    return this.columns;
+  }
+  
+  public DeleteTracker getDeleteTracker() {
+    return this.deletes;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 11)
@@ -0,0 +1,146 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.NavigableSet;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is used for the tracking and enforcement of columns and numbers 
+ * of versions during the course of a Get or Scan operation, when explicit
+ * column qualifiers have been asked for in the query.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class ExplicitColumnTracker implements ColumnTracker {
+
+  private int maxVersions;
+  private List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  private NavigableSet<byte[]> origColumns;
+  
+  /**
+   * Default constructor.
+   * @param columns columns specified user in query
+   * @param maxVersions maximum versions to return per column
+   */
+  public ExplicitColumnTracker(NavigableSet<byte[]> columns, int maxVersions) {
+    this.maxVersions = maxVersions;
+    this.origColumns = columns;
+    reset();
+  }
+  
+  /**
+   * Done when there are no more columns to match against.
+   */
+  public boolean done() {
+    return this.columns.size() == 0;
+  }
+  
+  /**
+   * Checks against the parameters of the query and the columns which have
+   * already been processed by this query.
+   * @param bytes KeyValue buffer
+   * @param offset offset to the start of the qualifier
+   * @param length length of the qualifier
+   * @return MatchCode telling QueryMatcher what action to take
+   */
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+    // No more columns left, we are done with this query
+    if(this.columns.size() == 0) {
+      return MatchCode.DONE;
+    }
+    
+    // No more columns to match against, done with storefile
+    if(this.column == null) {
+      return MatchCode.NEXT;
+    }
+    
+    // Compare specific column to current column
+    int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), 
+        column.getLength(), bytes, offset, length);
+    
+    // Matches, decrement versions left and include
+    if(ret == 0) {
+      if(this.column.decrement() == 0) {
+        // Done with versions for this column
+        this.columns.remove(this.index);
+        if(this.columns.size() == this.index) {
+          // Will not hit any more columns in this storefile
+          this.column = null;
+        } else {
+          this.column = this.columns.get(this.index);
+        }
+      }
+      return MatchCode.INCLUDE;
+    }
+
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++this.index == this.columns.size()) {
+        // No more to match, do not include, done with storefile
+        return MatchCode.NEXT;
+      }
+      this.column = this.columns.get(this.index);
+      return checkColumn(bytes, offset, length);
+    }
+
+    // Specified column is smaller than current column
+    // Skip
+    return MatchCode.SKIP;
+  }
+  
+  /**
+   * Called at the end of every StoreFile or Memcache.
+   */
+  public void update() {
+    if(this.columns.size() != 0) {
+      this.index = 0;
+      this.column = this.columns.get(this.index);
+    } else {
+      this.index = -1;
+      this.column = null;
+    }
+  }
+  
+  public void reset() {
+    buildColumnList(this.origColumns);
+    this.index = 0;
+    this.column = this.columns.get(this.index);
+  }
+
+  private void buildColumnList(NavigableSet<byte[]> columns) {
+    this.columns = new ArrayList<ColumnCount>(columns.size());
+    for(byte [] column : columns) {
+      this.columns.add(new ColumnCount(column,maxVersions));
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 11)
@@ -0,0 +1,101 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This interface is used for the tracking and enforcement of Deletes
+ * during the course of a Get or Scan operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link update} when reaching the end of a StoreFile 
+ */
+public interface DeleteTracker {
+  
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @param type delete type as byte
+   */
+  public void add(byte [] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type);
+  
+  /**
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+  public boolean isDeleted(byte [] buffer, int qualifierOffset,
+      int qualifierLength, long timestamp);
+  
+  public boolean isEmpty();
+  
+  /**
+   * Called at the end of every StoreFile.
+   * <p>
+   * Many optimized implementations of Trackers will require an update at
+   * when the end of each StoreFile is reached.
+   */
+  public void update();
+  
+  /**
+   * Called between rows.
+   * <p>
+   * This clears everything as if a new DeleteTracker was instantiated.
+   */
+  public void reset();
+  
+
+  /**
+   * Return codes for comparison of two Deletes.
+   * <p>
+   * The codes tell the merging function what to do.
+   * <p>
+   * INCLUDE means add the specified Delete to the merged list.
+   * NEXT means move to the next element in the specified list(s).
+   */
+  enum DeleteCompare { 
+    INCLUDE_OLD_NEXT_OLD,
+    INCLUDE_OLD_NEXT_BOTH,
+    INCLUDE_NEW_NEXT_NEW,
+    INCLUDE_NEW_NEXT_BOTH,
+    NEXT_OLD,
+    NEXT_NEW
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java	(revision 11)
@@ -0,0 +1,375 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is responsible for the tracking and enforcement of Deletes
+ * during the course of a Get or Scan operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link update} when reaching the end of a StoreFile
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class GetDeleteTracker implements DeleteTracker {
+
+  private long familyStamp = -1L;
+  protected List<Delete> deletes = null;
+  private List<Delete> newDeletes = new ArrayList<Delete>();
+  private Iterator<Delete> iterator;
+  private Delete delete = null;
+
+  public GetDeleteTracker() {}
+
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param kv
+   * @param type
+   * @param timestamp
+   */
+  @Override
+  public void add(byte [] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type) {
+    if(type == KeyValue.Type.DeleteFamily.getCode()) {
+      if(timestamp > familyStamp) {
+        familyStamp = timestamp;
+      }
+      return;
+    }
+    if(timestamp > familyStamp) {
+      this.newDeletes.add(new Delete(buffer, qualifierOffset, qualifierLength,
+          type, timestamp));
+    }
+  }
+
+  /** 
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+  @Override
+  public boolean isDeleted(byte [] buffer, int qualifierOffset,
+      int qualifierLength, long timestamp) {
+
+    // Check against DeleteFamily
+    if(timestamp <= familyStamp) {
+      return true;
+    }
+
+    // Check if there are other deletes
+    if(this.delete == null) {
+      return false;
+    }
+
+    // Check column
+    int ret = Bytes.compareTo(buffer, qualifierOffset, qualifierLength, 
+        this.delete.buffer, this.delete.qualifierOffset, 
+        this.delete.qualifierLength);
+    if(ret <= -1) {
+      // Have not reached the next delete yet
+      return false;
+    } else if(ret >= 1) {
+      // Deletes an earlier column, need to move down deletes
+      if(this.iterator.hasNext()) {
+        this.delete = this.iterator.next();
+      } else {
+        this.delete = null;
+        return false;
+      }
+      return isDeleted(buffer, qualifierOffset, qualifierLength, timestamp);
+    }
+
+    // Check Timestamp
+    if(timestamp > this.delete.timestamp) {
+      return false;
+    }
+
+    // Check Type
+    switch(KeyValue.Type.codeToType(this.delete.type)) {
+    case Delete:
+      boolean equal = timestamp == this.delete.timestamp;
+
+      if(this.iterator.hasNext()) {
+        this.delete = this.iterator.next();
+      } else {
+        this.delete = null;
+      }
+
+      if(equal){
+        return true;
+      }
+      // timestamp < this.delete.timestamp
+      // Delete of an explicit column newer than current
+      return isDeleted(buffer, qualifierOffset, qualifierLength, timestamp);
+    case DeleteColumn:
+      return true;
+    }
+    // should never reach this
+    return false;
+  }
+
+  @Override
+  public boolean isEmpty() {
+    if(this.familyStamp == 0L && this.delete == null) {
+      return true;
+    }
+    return false;
+  }
+
+  @Override
+  public void reset() {
+    this.deletes = null;
+    this.delete = null;
+    this.newDeletes = new ArrayList<Delete>();
+    this.familyStamp = 0L;
+    this.iterator = null;
+  }
+
+  /**
+   * Called at the end of every StoreFile.
+   * <p>
+   * Many optimized implementations of Trackers will require an update at
+   * when the end of each StoreFile is reached.
+   */
+  @Override
+  public void update() {
+    // If no previous deletes, use new deletes and return
+    if(this.deletes == null || this.deletes.size() == 0) {
+      finalize(this.newDeletes);
+      return;
+    }
+
+    // If no new delete, retain previous deletes and return
+    if(this.newDeletes.size() == 0) {
+      return;
+    }
+
+    // Merge previous deletes with new deletes
+    List<Delete> mergeDeletes = 
+      new ArrayList<Delete>(this.newDeletes.size());
+    int oldIndex = 0;
+    int newIndex = 0;
+
+    Delete newDelete = newDeletes.get(oldIndex);
+    Delete oldDelete = deletes.get(oldIndex);
+    while(true) {
+      switch(compareDeletes(oldDelete,newDelete)) {
+      case NEXT_NEW: {
+        if(++newIndex == newDeletes.size()) {
+          // Done with new, add the rest of old to merged and return
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_NEW_NEXT_NEW: {
+        mergeDeletes.add(newDelete);
+        if(++newIndex == newDeletes.size()) {
+          // Done with new, add the rest of old to merged and return
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_NEW_NEXT_BOTH: {
+        mergeDeletes.add(newDelete);
+        ++oldIndex;
+        ++newIndex;
+        if(oldIndex == deletes.size()) {
+          if(newIndex == newDeletes.size()) {
+            finalize(mergeDeletes);
+            return;
+          }
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        } else if(newIndex == newDeletes.size()) {
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_OLD_NEXT_BOTH: {
+        mergeDeletes.add(oldDelete);
+        ++oldIndex;
+        ++newIndex;
+        if(oldIndex == deletes.size()) {
+          if(newIndex == newDeletes.size()) {
+            finalize(mergeDeletes);
+            return;
+          }
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        } else if(newIndex == newDeletes.size()) {
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_OLD_NEXT_OLD: {
+        mergeDeletes.add(oldDelete);
+        if(++oldIndex == deletes.size()) {
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+      }
+
+      case NEXT_OLD: {
+        if(++oldIndex == deletes.size()) {
+          // Done with old, add the rest of new to merged and return
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+      }
+      }
+    }
+  }
+
+  private void finalize(List<Delete> mergeDeletes) {
+    this.deletes = mergeDeletes;
+    this.newDeletes = new ArrayList<Delete>();
+    if(this.deletes.size() > 0){
+      this.iterator = deletes.iterator();
+      this.delete = iterator.next();
+    }
+  }
+
+  private void mergeDown(List<Delete> mergeDeletes, List<Delete> srcDeletes, 
+      int srcIndex) {
+    while(srcIndex < srcDeletes.size()) {
+      mergeDeletes.add(srcDeletes.get(srcIndex++));
+    }
+  }
+
+
+  protected DeleteCompare compareDeletes(Delete oldDelete, Delete newDelete) {
+
+    // Compare columns
+    int ret = Bytes.compareTo(oldDelete.buffer, oldDelete.qualifierOffset,
+        oldDelete.qualifierLength, newDelete.buffer, newDelete.qualifierOffset,
+        newDelete.qualifierLength);
+
+    if(ret <= -1) {
+      return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+    } else if(ret >= 1) {
+      return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+    }
+
+    // Same column
+
+    // Branches below can be optimized.  Keeping like this until testing
+    // is complete.
+    if(oldDelete.type == newDelete.type) {
+      if(oldDelete.type == KeyValue.Type.Delete.getCode()){
+        if(oldDelete.timestamp > newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+        } else if(oldDelete.timestamp < newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+        } else {
+          return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+        }
+      }
+      if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_BOTH;
+      } 
+      return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+    }
+
+    if(oldDelete.type < newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.NEXT_OLD;
+      } else {
+        return DeleteCompare.NEXT_OLD;
+      }
+    }
+
+    if(oldDelete.type > newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.NEXT_NEW;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+      } else {
+        return DeleteCompare.NEXT_NEW;
+      }
+    }
+
+    // Should never reach
+    return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+  }
+
+  /**
+   * Internal class used to store the necessary information for a Delete.
+   * <p>
+   * Rather than reparsing the KeyValue, or copying fields, this class points
+   * to the underlying KeyValue buffer with pointers to the qualifier and fields
+   * for type and timestamp.  No parsing work is done in DeleteTracker now.
+   * <p>
+   * Fields are public because they are accessed often, directly, and only
+   * within this class.
+   */
+  protected class Delete {
+    public byte [] buffer;
+    public int qualifierOffset;
+    public int qualifierLength;
+    public byte type;
+    public long timestamp;
+    public Delete(byte [] buffer, int qualifierOffset, int qualifierLength,
+        byte type, long timestamp) {
+      this.buffer = buffer;
+      this.qualifierOffset = qualifierOffset;
+      this.qualifierLength = qualifierLength;
+      this.type = type;
+      this.timestamp = timestamp;
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 11)
@@ -0,0 +1,222 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is responsible for the tracking and enforcement of Deletes
+ * during the course of a Scan operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link reset} when reaching the end of a Row
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class ScanDeleteTracker implements DeleteTracker {
+  
+  private long familyStamp = 0L;
+  private List<Delete> deletes = new ArrayList<Delete>();
+  int idxDeletes = 0;
+  private List<Delete> columnDeletes = new ArrayList<Delete>();
+  int idxColumnDeletes = 0;
+  
+  public ScanDeleteTracker() {}
+  
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param kv
+   * @param type
+   * @param timestamp
+   */
+  public void add(byte [] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type) {
+    if(timestamp <= familyStamp) {
+      return;
+    }
+    switch(KeyValue.Type.codeToType(type)) {
+      case DeleteFamily: {
+        familyStamp = timestamp;
+      }
+      case DeleteColumn: {
+        this.deletes.add(new Delete(buffer, qualifierOffset, qualifierLength,
+          type, timestamp));
+      }
+      case Delete: {
+        this.columnDeletes.add(new Delete(buffer, qualifierOffset, qualifierLength,
+          type, timestamp));
+      }
+    }
+  }
+  
+  /**
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param bytes KeyValue buffer
+   * @param offset offset to start of KeyValue
+   * @param rowLength row length
+   * @param familyLength family length
+   * @param qualifierLength column qualifier length
+   * @param qualifierOffset column qualifier offset
+   * @param timestamp timestamp
+   * @param type key type
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+    public boolean isDeleted(byte [] bytes, int qualifierOffset,
+        int qualifierLength, long timestamp) {
+    
+      return false;
+      
+  }
+  
+  public boolean isEmpty() {
+    if(this.familyStamp == 0L && this.deletes.size() == 0
+        && this.columnDeletes.size() == 0) {
+      return true;
+    }
+    return false;
+  }
+  
+  public void reset() {
+    this.deletes = new ArrayList<Delete>();
+    this.columnDeletes = new ArrayList<Delete>();
+    this.idxDeletes = -1;
+    this.idxColumnDeletes = -1;
+    this.familyStamp = 0L;
+  }
+  
+  /**
+   * Called at the end of every StoreFile.
+   * <p>
+   * Does nothing because we only reset during a scan, we do not update.
+   */
+  public void update() {}
+  
+
+  /**
+   * Return codes for comparison of two Deletes.
+   * <p>
+   * The codes tell the merging function what to do.
+   * <p>
+   * INCLUDE means add the specified Delete to the merged list.
+   * NEXT means move to the next element in the specified list(s).
+   */
+  protected enum DeleteCompare { 
+    INCLUDE_OLD_NEXT_OLD,
+    INCLUDE_OLD_NEXT_BOTH,
+    INCLUDE_NEW_NEXT_NEW,
+    INCLUDE_NEW_NEXT_BOTH,
+    NEXT_OLD,
+    NEXT_NEW
+  }
+  
+  protected DeleteCompare compareDeletes(Delete oldDelete, Delete newDelete) {
+    
+    // Compare columns
+    int ret = Bytes.compareTo(oldDelete.buffer, oldDelete.qualifierOffset,
+        oldDelete.qualifierLength, newDelete.buffer, newDelete.qualifierOffset,
+        newDelete.qualifierLength);
+    
+    if(ret <= -1) {
+      return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+    } else if(ret >= 1) {
+      return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+    }
+    
+    // Same column
+    
+    // Branches below can be optimized.  Keeping like this until testing
+    // is complete.
+    if(oldDelete.type == newDelete.type) {
+      if(oldDelete.type == KeyValue.Type.Delete.getCode()){
+        if(oldDelete.timestamp > newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+        } else if(oldDelete.timestamp < newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+        } else {
+          return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+        }
+      }
+      if(oldDelete.timestamp < newDelete.timestamp) {
+         return DeleteCompare.INCLUDE_NEW_NEXT_BOTH;
+      } 
+      return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+    }
+
+    if(oldDelete.type < newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.NEXT_OLD;
+      } else {
+        return DeleteCompare.NEXT_OLD;
+      }
+    }
+    
+    if(oldDelete.type > newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.NEXT_NEW;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+      } else {
+        return DeleteCompare.NEXT_NEW;
+      }
+    }
+    
+    // Should never reach
+    return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+  }
+
+  /**
+   * Internal class used to store the necessary information for a Delete.
+   * <p>
+   * Rather than reparsing the KeyValue, or copying fields, this class points
+   * to the underlying KeyValue buffer with pointers to the qualifier and fields
+   * for type and timestamp.  No parsing work is done in DeleteTracker now.
+   * <p>
+   * Fields are public because they are accessed often, directly, and only
+   * within this class.
+   */
+  protected class Delete {
+    public byte [] buffer;
+    public int qualifierOffset;
+    public int qualifierLength;
+    public byte type;
+    public long timestamp;
+    public Delete(byte [] buffer, int qualifierOffset, int qualifierLength,
+        byte type, long timestamp) {
+      this.buffer = buffer;
+      this.qualifierOffset = qualifierOffset;
+      this.qualifierLength = qualifierLength;
+      this.type = type;
+      this.timestamp = timestamp;
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 11)
@@ -0,0 +1,307 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is used for the tracking and enforcement of columns and numbers 
+ * of versions during the course of a Get or Scan operation, when all available
+ * column qualifiers have been asked for in the query.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class WildcardColumnTracker implements ColumnTracker {
+  
+  private int maxVersions;
+  
+  protected List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  
+  private List<ColumnCount> newColumns; 
+  private int newIndex;
+  private ColumnCount newColumn;
+  
+  /**
+   * Default constructor.
+   * @param maxVersions maximum versions to return per columns
+   */
+  public WildcardColumnTracker(int maxVersions) {
+    this.maxVersions = maxVersions;
+    reset();
+  }
+  
+  public void reset() {
+    this.index = 0;
+    this.column = null;
+    this.columns = null;
+    this.newColumns = new ArrayList<ColumnCount>();
+    this.newIndex = 0;
+    this.newColumn = null;
+  }
+  
+  /**
+   * Can never early-out from reading more storefiles in Wildcard case.
+   */
+  public boolean done() {
+    return false;
+  }
+  
+  /**
+   * Checks against the parameters of the query and the columns which have
+   * already been processed by this query.
+   * @param bytes KeyValue buffer
+   * @param offset offset to the start of the qualifier
+   * @param length length of the qualifier
+   * @return MatchCode telling QueryMatcher what action to take
+   */
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+
+    // Nothing to match against, add to new and include
+    if(this.column == null && this.newColumn == null) {
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing old, compare against new
+    if(this.column == null && this.newColumn != null) {
+      int ret = Bytes.compareTo(newColumn.getBuffer(), newColumn.getOffset(), 
+          newColumn.getLength(), bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(newColumn.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++newIndex == newColumns.size()) {
+          // No more, add to end and include
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          return MatchCode.INCLUDE;
+        }
+        this.newColumn = newColumns.get(newIndex);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(++newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing new, compare against old
+    if(this.newColumn == null && this.column != null) {
+      int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(),
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          // No more, add to new and include (new was empty prior to this)
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          this.column = null;
+          return MatchCode.INCLUDE;
+        }
+        this.column = columns.get(index);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    
+    // There are new and old, figure which to check first
+    int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(), 
+        newColumn.getBuffer(), newColumn.getOffset(), newColumn.getLength());
+        
+    // Old is smaller than new, compare against old
+    if(ret <= -1) {
+      ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(),
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          this.column = null;
+        } else {
+          this.column = columns.get(index);
+        }
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      return MatchCode.INCLUDE;
+    }
+    
+    // Cannot be equal, so ret >= 1
+    // New is smaller than old, compare against new
+    
+    ret = Bytes.compareTo(newColumn.getBuffer(), newColumn.getOffset(), 
+        newColumn.getLength(), bytes, offset, length);
+    
+    // Same column
+    if(ret == 0) {
+      if(newColumn.increment() > this.maxVersions) {
+        return MatchCode.SKIP;
+      }
+      return MatchCode.INCLUDE;
+    }
+    
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++newIndex == newColumns.size()) {
+        this.newColumn = null;
+      } else {
+        this.newColumn = newColumns.get(newIndex);
+      }
+      return checkColumn(bytes, offset, length);
+    }
+    
+    // ret >= 1
+    // Specified column is smaller than current column
+    // Nothing to match against, add to new and include
+    newColumns.add(new ColumnCount(bytes, offset, length, 1));
+    return MatchCode.INCLUDE;
+  }
+  
+  /**
+   * Called at the end of every StoreFile or Memcache.
+   */
+  public void update() {
+    // If no previous columns, use new columns and return
+    if(this.columns == null || this.columns.size() == 0) {
+      if(this.newColumns.size() > 0){
+        finalize(newColumns);
+      }
+      return;
+    }
+    
+    // If no new columns, retain previous columns and return
+    if(this.newColumns.size() == 0) {
+      this.index = 0;
+      this.column = this.columns.get(index);
+      return;
+    }
+    
+    // Merge previous columns with new columns
+    // There will be no overlapping
+    List<ColumnCount> mergeColumns = new ArrayList<ColumnCount>(
+        columns.size() + newColumns.size());
+    index = 0;
+    newIndex = 0;
+    column = columns.get(0);
+    newColumn = newColumns.get(0);
+    while(true) {
+      int ret = Bytes.compareTo(
+          column.getBuffer(), column.getOffset(),column.getLength(), 
+          newColumn.getBuffer(), newColumn.getOffset(), newColumn.getLength());
+      
+      // Existing is smaller than new, add existing and iterate it
+      if(ret <= -1) {
+        mergeColumns.add(column);
+        if(++index == columns.size()) {
+          // No more existing left, merge down rest of new and return 
+          mergeDown(mergeColumns, newColumns, newIndex);
+          finalize(mergeColumns);
+          return;
+        }
+        column = columns.get(index);
+        continue;
+      }
+      
+      // New is smaller than existing, add new and iterate it
+      mergeColumns.add(newColumn);
+      if(++newIndex == newColumns.size()) {
+        // No more new left, merge down rest of existing and return
+        mergeDown(mergeColumns, columns, index);
+        finalize(mergeColumns);
+        return;
+      }
+      newColumn = newColumns.get(newIndex);
+      continue;
+    }
+  }
+  
+  private void mergeDown(List<ColumnCount> mergeColumns, 
+      List<ColumnCount> srcColumns, int srcIndex) {
+    while(srcIndex < srcColumns.size()) {
+      mergeColumns.add(srcColumns.get(srcIndex++));
+    }
+  }
+  
+  private void finalize(List<ColumnCount> mergeColumns) {
+    this.columns = mergeColumns;
+    this.index = 0;
+    this.column = this.columns.size() > 0? columns.get(index) : null;
+    
+    this.newColumns = new ArrayList<ColumnCount>();
+    this.newIndex = 0;
+    this.newColumn = null;
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(revision 11)
@@ -167,14 +167,14 @@
      */
     boolean matches(final KeyValue kv) throws IOException {
       if (this.matchType == MATCH_TYPE.SIMPLE) {
-        return kv.matchingColumnNoDelimiter(this.col, this.familylength);
+        return kv.matchingColumnNoDelimiter(this.col);
       } else if(this.matchType == MATCH_TYPE.FAMILY_ONLY) {
         return kv.matchingFamily(this.family);
       } else if (this.matchType == MATCH_TYPE.REGEX) {
         // Pass a column without the delimiter since thats whats we're
         // expected to match.
-        int o = kv.getColumnOffset();
-        int l = kv.getColumnLength(o);
+        int o = kv.getFamilyOffset();
+        int l = kv.getTotalColumnLength();
         String columnMinusQualifier = Bytes.toString(kv.getBuffer(), o, l);
         return this.columnMatcher.matcher(columnMinusQualifier).matches();
       } else {
Index: src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 11)
@@ -1,5 +1,5 @@
  /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -55,18 +55,19 @@
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.RegionHistorian;
 import org.apache.hadoop.hbase.ValueOverMaxLengthException;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchOperation;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.Reference.Range;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Writables;
-import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
 
@@ -1212,42 +1213,44 @@
 
   /**
    * Return an iterator that scans over the HRegion, returning the indicated 
-   * columns for only the rows that match the data filter.  This Iterator must
-   * be closed by the caller.
+   * columns and rows specified by the {@link Scan}.
+   * <p>
+   * This Iterator must be closed by the caller.
    *
-   * @param cols columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
    * @param firstRow row which is the starting point of the scan
-   * @param timestamp only return rows whose timestamp is <= this value
-   * @param filter row filter
+   * @param scan configured {@link Scan}
    * @return InternalScanner
    * @throws IOException
    */
-  public InternalScanner getScanner(byte[][] cols, byte [] firstRow,
-    long timestamp, RowFilterInterface filter) 
+  public InternalScanner getScanner(byte [] firstRow, Scan scan)
   throws IOException {
     newScannerLock.readLock().lock();
     try {
       if (this.closed.get()) {
         throw new IOException("Region " + this + " closed");
       }
-      HashSet<Store> storeSet = new HashSet<Store>();
-      NavigableSet<byte []> columns =
-        new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
-      // Below we make up set of stores we want scanners on and we fill out the
-      // list of columns.
-      for (int i = 0; i < cols.length; i++) {
-        columns.add(cols[i]);
-        Store s = stores.get(cols[i]);
-        if (s != null) {
-          storeSet.add(s);
+      List<Store> storeList;
+      if(scan.hasFamilies()) {
+        storeList = new ArrayList<Store>(scan.numFamilies());
+        for(byte [] family : scan.getFamilies()) {
+          Store s = stores.get(family);
+          if(s != null) {
+            storeList.add(s);
+          }
         }
+      } else {
+        // stores.size() is slow because of ConcurrentSkipListMap
+        // might change this to an array or just initialized to 0
+        storeList = new ArrayList<Store>(stores.size());
+        for(Map.Entry<byte[], Store> entry : stores.entrySet()) {
+          storeList.add(entry.getValue());
+        }
       }
-      return new HScanner(columns, firstRow, timestamp,
-        storeSet.toArray(new Store [storeSet.size()]), filter);
+      
+      Store [] stores = storeList.toArray(new Store[storeList.size()]);
+      
+      return new RegionScanner(firstRow, scan, stores);
+      
     } finally {
       newScannerLock.readLock().unlock();
     }
@@ -1815,7 +1818,6 @@
   }
   
   // Do any reconstruction needed from the log
-  @SuppressWarnings("unused")
   protected void doReconstructionLog(Path oldLogFile, long minSeqId, long maxSeqId,
     Progressable reporter)
   throws UnsupportedEncodingException, IOException {
@@ -2008,160 +2010,92 @@
   }
 
   /**
-   * HScanner is an iterator through a bunch of rows in an HRegion.
+   * RegionScanner is an iterator through a bunch of rows in an HRegion.
+   * <p>
+   * It is used to combine scanners from multiple Stores.
    */
-  private class HScanner implements InternalScanner {
-    private InternalScanner[] scanners;
-    private List<KeyValue> [] resultSets;
-    private RowFilterInterface filter;
-
-    /** Create an HScanner with a handle on many HStores. */
-    @SuppressWarnings("unchecked")
-    HScanner(final NavigableSet<byte []> columns, byte [] firstRow,
-      long timestamp, final Store [] stores, final RowFilterInterface filter)
+  private class RegionScanner implements InternalScanner {
+    
+    private KeyValueHeap storeHeap;
+    private byte [] stopRow;
+    
+    RegionScanner(byte [] firstRow, Scan scan, final Store [] stores)
     throws IOException {
-      this.filter = filter;
-      this.scanners = new InternalScanner[stores.length];
+      this.stopRow = scan.getStopRow();
+      KeyValueScanner [] scanners = new KeyValueScanner[stores.length];
       try {
+        Map<byte[], NavigableSet<byte[]>> familyMap = scan.getFamilyMap();
         for (int i = 0; i < stores.length; i++) {
-          // Only pass relevant columns to each store
-          NavigableSet<byte[]> columnSubset =
-            new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-          for (byte [] c: columns) {
-            if (KeyValue.FAMILY_COMPARATOR.compare(stores[i].storeName, c) == 0) {
-              columnSubset.add(c);
-            }
+          NavigableSet<byte[]> columns;
+          if(familyMap == null) {
+            columns = null;
+          } else {
+            columns = familyMap.get(stores[i].storeName);
           }
-          RowFilterInterface f = filter;
-          if (f != null) {
-            // Need to replicate filters.
-            // At least WhileMatchRowFilter will mess up the scan if only
-            // one shared across many rows. See HADOOP-2467.
-            f = WritableUtils.clone(filter, conf);
-          }
-          scanners[i] = stores[i].getScanner(timestamp, columnSubset, firstRow, f);
+          scanners[i] = stores[i].getScanner(scan, columns, firstRow);
         }
       } catch (IOException e) {
-        for (int i = 0; i < this.scanners.length; i++) {
+        for (int i = 0; i < scanners.length; i++) {
           if (scanners[i] != null) {
-            closeScanner(i);
+            close(scanners[i]);
           }
         }
         throw e;
       }
-
-      // Advance to the first key in each store.
-      // All results will match the required column-set and scanTime.
-      this.resultSets = new List[scanners.length];
-      for (int i = 0; i < scanners.length; i++) {
-        resultSets[i] = new ArrayList<KeyValue>();
-        if(scanners[i] != null && !scanners[i].next(resultSets[i])) {
-          closeScanner(i);
-        }
-      }
-
+      
+      this.storeHeap = new KeyValueHeap(scanners, comparator);
+      
       // As we have now successfully completed initialization, increment the
       // activeScanner count.
       activeScannerCount.incrementAndGet();
     }
 
+    /**
+     * Get the next row of results from this region.
+     * @param results list to append results to
+     * @return true if there are more rows, false if scanner is done
+     */
     public boolean next(List<KeyValue> results)
     throws IOException {
-      boolean moreToFollow = false;
-      boolean filtered = false;
-      do {
-        // Find the lowest key across all stores.
-        KeyValue chosen = null;
-        long chosenTimestamp = -1;
-        for (int i = 0; i < this.scanners.length; i++) {
-          if (this.resultSets[i] == null || this.resultSets[i].isEmpty()) {
-            continue;
-          }
-          KeyValue kv = this.resultSets[i].get(0);
-          if (chosen == null ||
-               (comparator.compareRows(kv, chosen) < 0) ||
-               ((comparator.compareRows(kv, chosen) == 0) &&
-                 (kv.getTimestamp() > chosenTimestamp))) {
-            chosen = kv;
-            chosenTimestamp = chosen.getTimestamp();
-          }
-        }
-
-        // Store results from each sub-scanner.
-        if (chosenTimestamp >= 0) {
-          for (int i = 0; i < scanners.length; i++) {
-            if (this.resultSets[i] == null || this.resultSets[i].isEmpty()) {
-              continue;
-            }
-            KeyValue kv = this.resultSets[i].get(0);
-            if (comparator.compareRows(kv, chosen) == 0) {
-              results.addAll(this.resultSets[i]);
-              resultSets[i].clear();
-              if (!scanners[i].next(resultSets[i])) {
-                closeScanner(i);
-              }
-            }
-          }
-        }
-
-        moreToFollow = chosenTimestamp >= 0;
-        if (results == null || results.size() <= 0) {
-          // If we got no results, then there is no more to follow.
-          moreToFollow = false;
-        }
-
-        filtered = filter == null ? false : filter.filterRow(results);
-        if (filter != null && filter.filterAllRemaining()) {
-          moreToFollow = false;
-        }
-        
-        if (moreToFollow) {
-          if (filter != null) {
-            filter.rowProcessed(filtered, chosen.getBuffer(), chosen.getRowOffset(),
-              chosen.getRowLength());
-          }
-          if (filtered) {
-            results.clear();
-          }
-        }
-      } while(filtered && moreToFollow);
-
-      // Make sure scanners closed if no more results
-      if (!moreToFollow) {
-        for (int i = 0; i < scanners.length; i++) {
-          if (null != scanners[i]) {
-            closeScanner(i);
-          }
-        }
+      // This method should probably be reorganized a bit... has gotten messy
+      KeyValue kv = this.storeHeap.peek();
+      if(kv == null) {
+        close();
+        return false;
       }
+      byte [] row = kv.getRow();
+      // See if we passed stopRow
+      if(comparator.compareRows(row, 0, row.length, stopRow, 0, stopRow.length)
+          <= 0){
+        close();
+        return false;
+      }
+      if(!this.storeHeap.next(results)) {
+        // May or may not have received results, but our heap is now empty
+        close();
+        return false;
+      }
       
-      return moreToFollow;
-    }
-
-    /** Shut down a single scanner */
-    void closeScanner(int i) {
-      try {
-        try {
-          scanners[i].close();
-        } catch (IOException e) {
-          LOG.warn("Failed closing scanner " + i, e);
+      while((kv = this.storeHeap.peek()) != null) {
+        // See if we jumped rows
+        if(comparator.compareRows(kv, row) != 0) {
+          // Jumped rows, return now (in the future, do a filter check?)
+          return true;
         }
-      } finally {
-        scanners[i] = null;
-        // These data members can be null if exception in constructor
-        if (resultSets != null) {
-          resultSets[i] = null;
+        if(!this.storeHeap.next(results)) {
+          close();
+          return false;
         }
       }
+      
+      // Heap is empty, scanners are done.
+      close();
+      return false;
     }
 
     public void close() {
       try {
-        for(int i = 0; i < scanners.length; i++) {
-          if(scanners[i] != null) {
-            closeScanner(i);
-          }
-        }
+        storeHeap.close();
       } finally {
         synchronized (activeScannerCount) {
           int count = activeScannerCount.decrementAndGet();
@@ -2177,14 +2111,12 @@
         }
       }
     }
-
-    public boolean isWildcardScanner() {
-      throw new UnsupportedOperationException("Unimplemented on HScanner");
+    
+    public void close(KeyValueScanner scanner) {
+      try {
+        scanner.close();
+      } catch(NullPointerException npe) {}
     }
-
-    public boolean isMultipleMatchScanner() {
-      throw new UnsupportedOperationException("Unimplemented on HScanner");
-    }  
   }
   
   // Utility methods
@@ -2690,4 +2622,47 @@
       releaseRowLock(lid);
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  public Result get(final Get get, final Integer lockid) throws IOException {
+    // Verify families are all valid
+    if(get.hasFamilies()) {
+      for(byte [] family : get.familySet()) {
+        checkFamily(family);
+      }
+    }
+    // Lock row
+    Integer lid = getLock(lockid, get.getRow()); 
+    List<KeyValue> result = new ArrayList<KeyValue>();
+    try {
+      for(Map.Entry<byte[],TreeSet<byte[]>> entry : get.getFamilyMap().entrySet()) {
+        byte [] family = entry.getKey();
+        Store store = stores.get(family);
+        store.get(get, entry.getValue(), result, 
+            this.comparator.getRawComparator());
+      }
+    } finally {
+      if(lockid == null) releaseRowLock(lid);
+    }
+    return new Result(result);
+  }
+  
+  //
+  // New HBASE-880 Helpers
+  //
+  
+  private void checkFamily(final byte [] family) 
+  throws NoSuchColumnFamilyException {
+    if(family == null) {
+      throw new NoSuchColumnFamilyException("Empty family is invalid");
+    }
+    if(!regionInfo.getTableDesc().hasFamily(family)) {
+      throw new NoSuchColumnFamilyException("Column family " +
+          Bytes.toString(family) + " does not exist in region " + this
+            + " in table " + regionInfo.getTableDesc());
+    }
+  }
 }
Index: src/java/org/apache/hadoop/hbase/regionserver/Store.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/Store.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/regionserver/Store.java	(revision 11)
@@ -51,7 +51,10 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.SequenceFile;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.io.hfile.HFile;
@@ -306,8 +309,7 @@
         // METACOLUMN info such as HBASE::CACHEFLUSH entries
         KeyValue kv = val.getKeyValue();
         if (val.isTransactionEntry() ||
-            kv.matchingColumnNoDelimiter(HLog.METACOLUMN,
-              HLog.METACOLUMN.length - 1) ||
+            kv.matchingFamily(HLog.METAFAMILY) ||
           !Bytes.equals(key.getRegionName(), regioninfo.getRegionName()) ||
           !kv.matchingFamily(family.getName())) {
           continue;
@@ -1118,7 +1120,7 @@
     // if the column pattern is not null, we use it for column matching.
     // we will skip the keys whose column doesn't match the pattern.
     if (columnPattern != null) {
-      if (!(columnPattern.matcher(candidate.getColumnString()).matches())) {
+      if (!(columnPattern.matcher(Bytes.toString(candidate.getColumn())).matches())) {
         return false;
       }
     }
@@ -1686,13 +1688,12 @@
   /**
    * Return a scanner for both the memcache and the HStore files
    */
-  protected InternalScanner getScanner(long timestamp,
-      final NavigableSet<byte []> targetCols,
-      byte [] firstRow, RowFilterInterface filter)
+  protected KeyValueScanner getScanner(Scan scan,
+      final NavigableSet<byte []> targetCols, byte [] firstRow)
   throws IOException {
     lock.readLock().lock();
     try {
-      return new StoreScanner(this, targetCols, firstRow, timestamp, filter);
+      return new StoreScanner(this, scan, targetCols, firstRow);
     } finally {
       lock.readLock().unlock();
     }
@@ -1798,4 +1799,47 @@
     }
     return false;
   }
+  
+  //
+  // HBASE-880/1249/1304
+  //
+  
+  /**
+   * Retrieve results from this store given the specified Get parameters.
+   * @param get Get operation
+   * @param columns List of columns to match, can be empty (not null)
+   * @param result List to add results to 
+   */
+  public void get(Get get, TreeSet<byte[]> columns, List<KeyValue> result,
+      KeyComparator keyComparator) 
+  throws IOException {
+    
+    // Column matching and version enforcement
+    QueryMatcher matcher = new QueryMatcher(get.getRow(), this.family.getName(),
+        columns, get.getMaxVersions(), get.getTimeRange(), this.ttl, 
+        keyComparator);
+    
+    // Read from Memcache
+    if(this.memcache.get(matcher, result)) {
+      // Received early-out from memcache
+      return;
+    }
+    
+    // Check if we even have storefiles
+    if(this.storefiles.isEmpty()) {
+      return;
+    }
+    
+    // Get storefiles for this store
+    List<HFileScanner> storefileScanners = new ArrayList<HFileScanner>();
+    for(StoreFile sf : this.storefiles.descendingMap().values()) {
+      storefileScanners.add(sf.getReader().getScanner());
+    }
+    
+    // StoreFileScan will handle reading this store's storefiles
+    StoreFileScan scanner = new StoreFileScan(storefileScanners, matcher);
+    
+    // Run a GET scan and put results into the specified list 
+    scanner.get(result);
+  }
 }
Index: src/java/org/apache/hadoop/hbase/HTableDescriptor.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HTableDescriptor.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/HTableDescriptor.java	(revision 11)
@@ -102,7 +102,7 @@
   // Key is hash of the family name.
   private final Map<byte [], HColumnDescriptor> families =
     new TreeMap<byte [], HColumnDescriptor>(KeyValue.FAMILY_COMPARATOR);
-
+  
   // Key is indexId
   private final Map<String, IndexSpecification> indexes =
     new HashMap<String, IndexSpecification>();
Index: src/java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- src/java/org/apache/hadoop/hbase/KeyValue.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/KeyValue.java	(revision 11)
@@ -19,13 +19,17 @@
  */
 package org.apache.hadoop.hbase;
 
+import java.io.DataInput;
+import java.io.DataOutput;
 import java.io.IOException;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Writable;
 
 /**
  * An HBase Key/Value.  Instances of this class are immutable.  They are not
@@ -45,10 +49,10 @@
  * Byte.MAX_SIZE, and column qualifier + key length must be < Integer.MAX_SIZE.
  * The column does not contain the family/qualifier delimiter.
  * 
- * <p>TODO: Group Key-only compartors and operations into a Key class, just
+ * <p>TODO: Group Key-only comparators and operations into a Key class, just
  * for neatness sake, if can figure what to call it.
  */
-public class KeyValue {
+public class KeyValue implements Writable, HeapSize {
   static final Log LOG = LogFactory.getLog(KeyValue.class);
 
   /**
@@ -116,24 +120,24 @@
     };
 
   // Size of the timestamp and type byte on end of a key -- a long + a byte.
-  private static final int TIMESTAMP_TYPE_SIZE =
+  public static final int TIMESTAMP_TYPE_SIZE =
     Bytes.SIZEOF_LONG /* timestamp */ +
     Bytes.SIZEOF_BYTE /*keytype*/;
 
   // Size of the length shorts and bytes in key.
-  private static final int KEY_INFRASTRUCTURE_SIZE =
+  public static final int KEY_INFRASTRUCTURE_SIZE =
     Bytes.SIZEOF_SHORT /*rowlength*/ +
     Bytes.SIZEOF_BYTE /*columnfamilylength*/ +
     TIMESTAMP_TYPE_SIZE;
 
   // How far into the key the row starts at. First thing to read is the short
   // that says how long the row is.
-  private static final int ROW_OFFSET =
+  public static final int ROW_OFFSET =
     Bytes.SIZEOF_INT /*keylength*/ +
     Bytes.SIZEOF_INT /*valuelength*/;
 
   // Size of the length ints in a KeyValue datastructure.
-  private static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
+  public static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
 
   /**
    * Key type.
@@ -190,10 +194,21 @@
   public static final KeyValue LOWESTKEY = 
     new KeyValue(HConstants.EMPTY_BYTE_ARRAY, HConstants.LATEST_TIMESTAMP);
   
-  private final byte [] bytes;
-  private final int offset;
-  private final int length;
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue fields and constructors 
+  //
+  //---------------------------------------------------------------------------
+  
+  private byte [] bytes;
+  private int offset;
+  private int length;
 
+  /** Writable Constructor */
+  public KeyValue() {}
+  
+  /** Constructors using existing KeyValue formatted byte arrays */
+  
   /**
    * Creates a KeyValue from the start of the specified byte array.
    * Presumes <code>bytes</code> content is formatted as a KeyValue blob.
@@ -226,194 +241,199 @@
     this.offset = offset;
     this.length = length;
   }
+
+  /** Temporary constructors until 880/1249 is committed to remove deps */
   
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param timestamp
+   * Temporary.
    */
-  public KeyValue(final String row, final long timestamp) {
-    this(Bytes.toBytes(row), timestamp);
+  public KeyValue(final byte [] row, final byte [] column) {
+    this(row, column, HConstants.LATEST_TIMESTAMP, null);
   }
-
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts) {
+    this(row, column, ts, null);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts,
+      byte [] value) {
+    this(row, column, ts, Type.Put, value);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts, Type type,
+      byte [] value) {
+    int rlength = row == null ? 0 : row.length;
+    int vlength = value == null ? 0 : value.length;
+    int clength = column == null ? 0 : column.length;
+    this.bytes = createByteArray(row, 0, rlength, column, 0, clength,
+        ts, type, value, 0, vlength);
+    this.length = this.bytes.length;
+    this.offset = 0;
+  }
+  
+  /** Constructors that build a new backing byte array from fields */
+  
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
    * @param timestamp
    */
   public KeyValue(final byte [] row, final long timestamp) {
-    this(row, null, timestamp, Type.Put, null);
+    this(row, timestamp, Type.Put);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param timestamp
    */
-  public KeyValue(final String row, final String column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final long timestamp, Type type) {
+    this(row, null, null, timestamp, type, null);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final byte [] row, final byte [] column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put);
   }
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final String row, final String column, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), value);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column, final byte [] value) {
-    this(row, column, HConstants.LATEST_TIMESTAMP, value);
-  }
-
-
-  /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final String row, final String column, final long ts) {
-    this(row, column, ts, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier, final byte [] value) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put, value);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column, final long ts) {
-    this(row, column, ts, Type.Put);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type) {
+    this(row, family, qualifier, timestamp, type, null);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-    final long timestamp, final byte [] value) {
-    this(Bytes.toBytes(row),
-      column == null? HConstants.EMPTY_BYTE_ARRAY: Bytes.toBytes(column),
-      timestamp, value);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, final byte[] value) {
+    this(row, family, qualifier, timestamp, Type.Put, value);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column,
-     final long timestamp, final byte [] value) {
-    this(row, column, timestamp, Type.Put, value);
-  }
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type,
+      final byte[] value) {
+    this(row, family, qualifier, 0, qualifier==null ? 0 : qualifier.length, 
+        timestamp, type, value, 0, value==null ? 0 : value.length);
+  } 
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-     final long timestamp, final Type type, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), timestamp, type,
-      value);
+  public KeyValue(byte [] row, byte [] family, 
+      byte [] qualifier, int qoffset, int qlength, long timestamp, Type type, 
+      byte [] value, int voffset, int vlength) {
+    this(row, 0, row==null ? 0 : row.length, 
+        family, 0, family==null ? 0 : family.length,
+        qualifier, qoffset, qlength, timestamp, type, 
+        value, voffset, vlength);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, null, 0, -1);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type, final byte [] value) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, value, 0, value == null? 0: value.length);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column Column with delimiter between family and qualifier
-   * @param coffset Where to start reading the column.
-   * @param clength How long column is (including the family/qualifier delimiter.
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * Constructs KeyValue structure filled with specified values.
+   * <p>
+   * Column is split into two fields, family and qualifier.
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @throws IllegalArgumentException
    */
   public KeyValue(final byte [] row, final int roffset, final int rlength,
-      final byte [] column, final int coffset, int clength,
+      final byte [] family, final int foffset, final int flength,
+      final byte [] qualifier, final int qoffset, final int qlength,
       final long timestamp, final Type type,
-      final byte [] value, final int voffset, int vlength) {
-    this.bytes = createByteArray(row, roffset, rlength, column, coffset,
-      clength, timestamp, type, value, voffset, vlength);
+      final byte [] value, final int voffset, final int vlength) {
+    this.bytes = createByteArray(row, roffset, rlength, 
+        family, foffset, flength, qualifier, qoffset, qlength,
+        timestamp, type, value, voffset, vlength);
     this.length = bytes.length;
     this.offset = 0;
   }
 
   /**
    * Write KeyValue format into a byte array.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column
-   * @param coffset
-   * @param clength
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @return
    */
-  static byte [] createByteArray(final byte [] row, final int roffset,
-        final int rlength,
-      final byte [] column, final int coffset, int clength,
+  static byte [] createByteArray(final byte [] row, final int roffset, final int rlength,
+      final byte [] family, final int foffset, int flength,
+      final byte [] qualifier, final int qoffset, int qlength,
       final long timestamp, final Type type,
       final byte [] value, final int voffset, int vlength) {
     if (rlength > Short.MAX_VALUE) {
@@ -422,19 +442,14 @@
     if (row == null) {
       throw new IllegalArgumentException("Row is null");
     }
-    // If column is non-null, figure where the delimiter is at.
-    int delimiteroffset = 0;
-    if (column != null && column.length > 0) {
-      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
-      if (delimiteroffset > Byte.MAX_VALUE) {
-        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
-      }
-    }
     // Value length
-    vlength = value == null? 0: vlength;
-    // Column length - minus delimiter
-    clength = column == null || column.length == 0? 0: clength - 1;
-    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + clength;
+    vlength = value == null? 0 : vlength;
+    // Family length
+    flength = family == null ? 0 : flength;
+    // Qualifier length
+    qlength = qualifier == null ? 0 : qlength;
+    // Key length
+    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + flength + qlength;
     if (longkeylength > Integer.MAX_VALUE) {
       throw new IllegalArgumentException("keylength " + longkeylength + " > " +
         Integer.MAX_VALUE);
@@ -448,16 +463,13 @@
     pos = Bytes.putInt(bytes, pos, vlength);
     pos = Bytes.putShort(bytes, pos, (short)(rlength & 0x0000ffff));
     pos = Bytes.putBytes(bytes, pos, row, roffset, rlength);
-    // Write out column family length.
-    pos = Bytes.putByte(bytes, pos, (byte)(delimiteroffset & 0x0000ff));
-    if (column != null && column.length != 0) {
-      // Write family.
-      pos = Bytes.putBytes(bytes, pos, column, coffset, delimiteroffset);
-      // Write qualifier.
-      delimiteroffset++;
-      pos = Bytes.putBytes(bytes, pos, column, coffset + delimiteroffset,
-        column.length - delimiteroffset);
+    pos = Bytes.putByte(bytes, pos, (byte)(flength & 0x0000ff));
+    if(flength != 0) {
+      pos = Bytes.putBytes(bytes, pos, family, foffset, flength);
     }
+    if(qlength != 0) {
+      pos = Bytes.putBytes(bytes, pos, qualifier, qoffset, qlength);
+    }
     pos = Bytes.putLong(bytes, pos, timestamp);
     pos = Bytes.putByte(bytes, pos, type.getCode());
     if (value != null && value.length > 0) {
@@ -465,6 +477,46 @@
     }
     return bytes;
   }
+  
+  /**
+   * Write KeyValue format into a byte array.
+   * <p>
+   * Takes column in the form <code>family:qualifier</code>
+   * @param row - row key (arbitrary byte array)
+   * @param roffset
+   * @param rlength
+   * @param column
+   * @param coffset
+   * @param clength
+   * @param timestamp
+   * @param type
+   * @param value
+   * @param voffset
+   * @param vlength
+   * @return
+   */
+  static byte [] createByteArray(final byte [] row, final int roffset,
+        final int rlength,
+      final byte [] column, final int coffset, int clength,
+      final long timestamp, final Type type,
+      final byte [] value, final int voffset, int vlength) {
+    // If column is non-null, figure where the delimiter is at.
+    int delimiteroffset = 0;
+    if (column != null && column.length > 0) {
+      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
+      if (delimiteroffset > Byte.MAX_VALUE) {
+        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
+      }
+    } else {
+      return createByteArray(row,roffset,rlength,null,0,0,null,0,0,timestamp,
+          type,value,voffset,vlength);
+    }
+    int flength = delimiteroffset-coffset;
+    int qlength = clength - flength - 1;
+    return createByteArray(row, roffset, rlength, column, coffset,
+        flength, column, delimiteroffset+1, qlength, timestamp, type,
+        value, voffset, vlength);
+  }
 
   // Needed doing 'contains' on List.  Only compares the key portion, not the
   // value.
@@ -478,6 +530,12 @@
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue cloning
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @param timestamp
    * @return Clone of bb's key portion with only the row and timestamp filled in.
@@ -485,9 +543,10 @@
    */
   public KeyValue cloneRow(final long timestamp) {
     return new KeyValue(getBuffer(), getRowOffset(), getRowLength(),
-      null, 0, 0, timestamp, Type.codeToType(getType()), null, 0, 0);
+        null, 0, 0, null, 0, 0, 
+        timestamp, Type.codeToType(getType()), null, 0, 0);
   }
-
+  
   /**
    * @return Clone of bb's key portion with type set to Type.Delete.
    * @throws IOException
@@ -524,6 +583,12 @@
     return new KeyValue(other, 0, other.length);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  String representation
+  //
+  //---------------------------------------------------------------------------
+  
   public String toString() {
     return keyToString(this.bytes, this.offset + ROW_OFFSET, getKeyLength()) +
       "/vlen=" + getValueLength();
@@ -561,6 +626,12 @@
       qualifier + "/" + timestamp + "/" + Type.codeToType(type);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  Public Member Accessors
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @return The byte array backing this KeyValue.
    */
@@ -582,7 +653,13 @@
     return length;
   }
 
-  /*
+  //---------------------------------------------------------------------------
+  //
+  //  Length and Offset Calculators
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
    * Determines the total length of the KeyValue stored in the specified
    * byte array and offset.  Includes all headers.
    * @param bytes byte array
@@ -596,41 +673,167 @@
   }
 
   /**
-   * @return Copy of the key portion only.  Used compacting and testing.
+   * @return Key offset in backing buffer..
    */
-  public byte [] getKey() {
-    int keylength = getKeyLength();
-    byte [] key = new byte[keylength];
-    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
-    return key;
+  public int getKeyOffset() {
+    return this.offset + ROW_OFFSET;
   }
 
-  public String getKeyString() {
-    return Bytes.toString(getBuffer(), getKeyOffset(), getKeyLength());
+  /**
+   * @return Length of key portion.
+   */
+  public int getKeyLength() {
+    return Bytes.toInt(this.bytes, this.offset);
   }
 
   /**
-   * @return Key offset in backing buffer..
+   * @return Value offset
    */
-  public int getKeyOffset() {
-    return this.offset + ROW_OFFSET;
+  public int getValueOffset() {
+    return getKeyOffset() + getKeyLength();
   }
+  
+  /**
+   * @return Value length
+   */
+  public int getValueLength() {
+    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  }
 
   /**
-   * @return Row length.
+   * @return Row offset
    */
+  public int getRowOffset() {
+    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  }
+  
+  /**
+   * @return Row length
+   */
   public short getRowLength() {
     return Bytes.toShort(this.bytes, getKeyOffset());
   }
 
   /**
-   * @return Offset into backing buffer at which row starts.
+   * @return Family offset
    */
-  public int getRowOffset() {
-    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  public int getFamilyOffset() {
+    return getFamilyOffset(getRowLength());
   }
+  
+  /**
+   * @return Family offset
+   */
+  public int getFamilyOffset(int rlength) {
+    return ROW_OFFSET + Bytes.SIZEOF_SHORT + rlength + Bytes.SIZEOF_BYTE;
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength() {
+    return getFamilyLength(getFamilyOffset());
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength(int foffset) {
+    return this.bytes[foffset-1];
+  }
 
   /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset() {
+    return getQualifierOffset(getFamilyOffset());
+  }
+  
+  /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset(int foffset) {
+    return foffset + getFamilyLength(foffset);
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength() {
+    return getQualifierLength(getRowLength(),getFamilyLength());
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength(int rlength, int flength) {
+    return getKeyLength() - 
+      (KEY_INFRASTRUCTURE_SIZE + rlength + flength);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength() {
+    int rlength = getRowLength();
+    int foffset = getFamilyOffset(rlength);
+    return getTotalColumnLength(rlength,foffset);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength(int rlength, int foffset) {
+    int flength = getFamilyLength(foffset);
+    int qlength = getQualifierLength(rlength,flength);
+    return flength + qlength;
+  }
+  
+  /**
+   * @return Timestamp offset
+   */
+  int getTimestampOffset() {
+    return getTimestampOffset(getKeyLength());
+  }
+  
+  /**
+   * @param keylength Pass if you have it to save on a int creation.
+   * @return Timestamp offset
+   */
+  int getTimestampOffset(final int keylength) {
+    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
+  }
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Methods that return copies of fields
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
+   * @return Copy of the key portion only.  Used compacting and testing.
+   */
+  public byte [] getKey() {
+    int keylength = getKeyLength();
+    byte [] key = new byte[keylength];
+    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
+    return key;
+  }
+  
+  /**
+   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
+   * offset and lengths instead.
+   * @return Value in a new byte array.
+   */
+  public byte [] getValue() {
+    int o = getValueOffset();
+    int l = getValueLength();
+    byte [] result = new byte[l];
+    System.arraycopy(getBuffer(), o, result, 0, l);
+    return result;
+  }
+  
+  /**
    * Do not use this unless you have to.
    * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
    * @return Row in a new byte array.
@@ -660,14 +863,6 @@
   }
 
   /**
-   * @param keylength Pass if you have it to save on a int creation.
-   * @return Offset into backing buffer at which timestamp starts.
-   */
-  int getTimestampOffset(final int keylength) {
-    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
-  }
-
-  /**
    * @return True if a {@link Type#Delete}.
    */
   public boolean isDeleteType() {
@@ -690,99 +885,156 @@
   }
 
   /**
-   * @return Length of key portion.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns column. Makes a copy.  Inserts delimiter.
    */
-  public int getKeyLength() {
-    return Bytes.toInt(this.bytes, this.offset);
+  public byte [] getColumn() {
+    int fo = getFamilyOffset();
+    int fl = getFamilyLength(fo);
+    int ql = getQualifierLength();
+    byte [] result = new byte[fl + 1 + ql];
+    System.arraycopy(this.bytes, fo, result, 0, fl);
+    result[fl] = COLUMN_FAMILY_DELIMITER;
+    System.arraycopy(this.bytes, fo + fl, result,
+      fl + 1, ql);
+    return result;
   }
 
   /**
-   * @return Value length
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns family. Makes a copy.
    */
-  public int getValueLength() {
-    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  public byte [] getFamily() {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    byte [] result = new byte[l];
+    System.arraycopy(this.bytes, o, result, 0, l);
+    return result;
   }
 
   /**
-   * @return Offset into backing buffer at which value starts.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns qualifier. Makes a copy.
    */
-  public int getValueOffset() {
-    return getKeyOffset() + getKeyLength();
-  }
-
-  /**
-   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
-   * offset and lengths instead.
-   * @return Value in a new byte array.
-   */
-  public byte [] getValue() {
-    int o = getValueOffset();
-    int l = getValueLength();
+  public byte [] getQualifier() {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
     byte [] result = new byte[l];
-    System.arraycopy(getBuffer(), o, result, 0, l);
+    System.arraycopy(this.bytes, o, result, 0, l);
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue splitter
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @return Offset into backing buffer at which the column begins
+   * Utility class that splits a KeyValue buffer into separate byte arrays.
+   * <p>
+   * Should get rid of this if we can, but is very useful for debugging.
    */
-  public int getColumnOffset() {
-    return getColumnOffset(getRowLength());
+  public static class SplitKeyValue {
+    private byte [][] split;
+    SplitKeyValue() {
+      this.split = new byte[6][];
+    }
+    public void setRow(byte [] value) { this.split[0] = value; }
+    public void setFamily(byte [] value) { this.split[1] = value; }
+    public void setQualifier(byte [] value) { this.split[2] = value; }
+    public void setTimestamp(byte [] value) { this.split[3] = value; }
+    public void setType(byte [] value) { this.split[4] = value; }
+    public void setValue(byte [] value) { this.split[5] = value; }
+    public byte [] getRow() { return this.split[0]; }
+    public byte [] getFamily() { return this.split[1]; }
+    public byte [] getQualifier() { return this.split[2]; }
+    public byte [] getTimestamp() { return this.split[3]; }
+    public byte [] getType() { return this.split[4]; }
+    public byte [] getValue() { return this.split[5]; }
   }
-
-  /**
-   * @param rowlength - length of row.
-   * @return Offset into backing buffer at which the column begins
-   */
-  public int getColumnOffset(final int rowlength) {
-    return getRowOffset() + rowlength + 1;
+  
+  public SplitKeyValue split() {
+    SplitKeyValue split = new SplitKeyValue();
+    int splitOffset = this.offset;
+    int keyLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    int valLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    short rowLen = Bytes.toShort(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_SHORT;
+    byte [] row = new byte[rowLen];
+    System.arraycopy(bytes, splitOffset, row, 0, rowLen);
+    splitOffset += rowLen;
+    split.setRow(row);
+    byte famLen = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    byte [] family = new byte[famLen];
+    System.arraycopy(bytes, splitOffset, family, 0, famLen);
+    splitOffset += famLen;
+    split.setFamily(family);
+    int colLen = keyLen -
+      (rowLen + famLen + Bytes.SIZEOF_SHORT + Bytes.SIZEOF_BYTE +
+      Bytes.SIZEOF_LONG + Bytes.SIZEOF_BYTE);
+    byte [] qualifier = new byte[colLen];
+    System.arraycopy(bytes, splitOffset, qualifier, 0, colLen);
+    splitOffset += colLen;
+    split.setQualifier(qualifier);
+    byte [] timestamp = new byte[Bytes.SIZEOF_LONG];
+    System.arraycopy(bytes, splitOffset, timestamp, 0, Bytes.SIZEOF_LONG);
+    splitOffset += Bytes.SIZEOF_LONG;
+    split.setTimestamp(timestamp);
+    byte [] type = new byte[1];
+    type[0] = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    split.setType(type);
+    byte [] value = new byte[valLen];
+    System.arraycopy(bytes, splitOffset, value, 0, valLen);
+    split.setValue(value);
+    return split;
   }
-
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Compare specified fields against those contained in this KeyValue 
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of family portion of column.
+   * @param family
+   * @return True if matching families.
    */
-  int getFamilyLength(final int columnoffset) {
-    return this.bytes[columnoffset - 1];
+  public boolean matchingFamily(final byte [] family) {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
   }
 
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of column.
+   * @param qualifier
+   * @return True if matching qualifiers.
    */
-  public int getColumnLength(final int columnoffset) {
-    return getColumnLength(columnoffset, getKeyLength());
+  public boolean matchingQualifier(final byte [] qualifier) {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
+    return Bytes.compareTo(qualifier, 0, qualifier.length, 
+        this.bytes, o, l) == 0;
   }
 
-  int getColumnLength(final int columnoffset, final int keylength) {
-    return (keylength + ROW_OFFSET) - (columnoffset - this.offset) -
-      TIMESTAMP_TYPE_SIZE;
-  }
-
   /**
-   * @param family
-   * @return True if matching families.
-   */
-  public boolean matchingFamily(final byte [] family) {
-    int o = getColumnOffset();
-    // Family length byte is just before the column starts.
-    int l = this.bytes[o - 1];
-    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
-  }
-
-  /**
    * @param column Column minus its delimiter
-   * @param familylength Length of family in passed <code>column</code>
    * @return True if column matches.
    * @see #matchingColumn(byte[])
    */
-  public boolean matchingColumnNoDelimiter(final byte [] column,
-      final int familylength) {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int f = getFamilyLength(o);
-    return compareColumns(getBuffer(), o, l, f,
-      column, 0, column.length, familylength) == 0;
+  public boolean matchingColumnNoDelimiter(final byte [] column) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int l = fl + getQualifierLength(rl,fl);
+    return Bytes.compareTo(column, 0, column.length, this.bytes, o, l) == 0;
   }
 
   /**
@@ -791,17 +1043,41 @@
    */
   public boolean matchingColumn(final byte [] column) {
     int index = getFamilyDelimiterIndex(column, 0, column.length);
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int result = Bytes.compareTo(getBuffer(), o, index, column, 0, index);
-    if (result != 0) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(column, 0, index, this.bytes, o, fl) != 0) {
       return false;
     }
-    return Bytes.compareTo(getBuffer(), o + index, l - index,
-      column, index + 1, column.length - (index + 1)) == 0;
+    return Bytes.compareTo(column, index + 1, column.length - (index + 1),
+        this.bytes, o + fl, ql) == 0;
   }
 
   /**
+   * @param column Column with delimiter
+   * @return True if column matches.
+   */
+  public boolean matchingColumn(final byte[] family, final byte[] qualifier) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(family, 0, family.length, this.bytes, o, family.length)
+        != 0) {
+      return false;
+    }
+    if(qualifier == null || qualifier.length == 0) {
+      if(ql == 0) {
+        return true;
+      }
+      return false;
+    }
+    return Bytes.compareTo(qualifier, 0, qualifier.length,
+        this.bytes, o + fl, ql) == 0;
+  }
+
+  /**
    * @param left
    * @param loffset
    * @param llength
@@ -836,42 +1112,35 @@
   }
 
   /**
-   * @return Returns column String with delimiter added back. Expensive!
+   * @return True if column is empty.
    */
-  public String getColumnString() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    return Bytes.toString(this.bytes, o, familylength) +
-      COLUMN_FAMILY_DELIMITER + Bytes.toString(this.bytes,
-       o + familylength, l - familylength);
+  public boolean isEmptyColumn() {
+    return getQualifierLength() == 0;
   }
 
   /**
-   * Do not use this unless you have to.
-   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
-   * @return Returns column. Makes a copy.  Inserts delimiter.
+   * Splits a column in family:qualifier form into separate byte arrays.
+   * <p>
+   * Catches 
+   * @param column
+   * @return
    */
-  public byte [] getColumn() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    byte [] result = new byte[l + 1];
-    System.arraycopy(getBuffer(), o, result, 0, familylength);
-    result[familylength] = COLUMN_FAMILY_DELIMITER;
-    System.arraycopy(getBuffer(), o + familylength, result,
-      familylength + 1, l - familylength);
+  public static byte [][] parseColumn(byte [] c) {
+    final byte [][] result = new byte [2][];
+    final int index = getFamilyDelimiterIndex(c, 0, c.length);
+    if (index == -1) {
+      throw new IllegalArgumentException("Impossible column name: " + c);
+    }
+    result[0] = new byte [index];
+    System.arraycopy(c, 0, result[0], 0, index);
+    final int len = c.length - (index + 1);
+    result[1] = new byte[len];
+    System.arraycopy(c, index + 1 /*Skip delimiter*/, result[1], 0,
+      len);
     return result;
   }
-
+  
   /**
-   * @return True if column is empty.
-   */
-  public boolean isEmptyColumn() {
-    return getColumnLength(getColumnOffset()) == 0;
-  }
-
-  /**
    * @param b
    * @return Index of the family-qualifier colon delimiter character in passed
    * buffer.
@@ -1019,7 +1288,8 @@
      * @return Result comparing rows.
      */
     public int compareRows(final KeyValue left, final KeyValue right) {
-      return compareRows(left, left.getRowLength(), right, right.getRowLength());
+      return compareRows(left, left.getRowLength(), right, 
+          right.getRowLength());
     }
 
     /**
@@ -1051,11 +1321,11 @@
       return getRawComparator().compareRows(left, loffset, llength,
         right, roffset, rlength);
     }
-
+    
     public int compareColumns(final KeyValue left, final byte [] right,
         final int roffset, final int rlength, final int rfamilyoffset) {
-      int offset = left.getColumnOffset();
-      int length = left.getColumnLength(offset);
+      int offset = left.getFamilyOffset();
+      int length = left.getFamilyLength() + left.getQualifierLength();
       return getRawComparator().compareColumns(left.getBuffer(), offset, length,
         left.getFamilyLength(offset),
         right, roffset, rlength, rfamilyoffset);
@@ -1064,15 +1334,15 @@
     int compareColumns(final KeyValue left, final short lrowlength,
         final int lkeylength, final KeyValue right, final short rrowlength,
         final int rkeylength) {
-      int loffset = left.getColumnOffset(lrowlength);
-      int roffset = right.getColumnOffset(rrowlength);
-      int llength = left.getColumnLength(loffset, lkeylength);
-      int rlength = right.getColumnLength(roffset, rkeylength);
-      int lfamilylength = left.getFamilyLength(loffset);
-      int rfamilylength = right.getFamilyLength(roffset);
-      return getRawComparator().compareColumns(left.getBuffer(), loffset,
-          llength, lfamilylength,
-        right.getBuffer(), roffset, rlength, rfamilylength);
+      int lfoffset = left.getFamilyOffset(lrowlength);
+      int rfoffset = right.getFamilyOffset(rrowlength);
+      int lclength = left.getTotalColumnLength(lrowlength,lfoffset);
+      int rclength = right.getTotalColumnLength(rrowlength, rfoffset);
+      int lfamilylength = left.getFamilyLength(lfoffset);
+      int rfamilylength = right.getFamilyLength(rfoffset);
+      return getRawComparator().compareColumns(left.getBuffer(), lfoffset,
+          lclength, lfamilylength,
+        right.getBuffer(), rfoffset, rclength, rfamilylength);
     }
 
     /**
@@ -1133,7 +1403,8 @@
     public boolean matchingRows(final byte [] left, final int loffset,
         final int llength,
         final byte [] right, final int roffset, final int rlength) {
-      int compare = compareRows(left, loffset, llength, right, roffset, rlength);
+      int compare = compareRows(left, loffset, llength, 
+          right, roffset, rlength);
       if (compare != 0) {
         return false;
       }
@@ -1207,20 +1478,44 @@
    */
   public static KeyValue createFirstOnRow(final byte [] row,
       final long ts) {
-    return createFirstOnRow(row, null, ts);
+    return new KeyValue(row, null, null, ts, Type.Maximum);
   }
 
   /**
    * @param row - row key (arbitrary byte array)
    * @param ts - timestamp
-   * @return First possible key on passed <code>row</code>, column and timestamp.
+   * @return First possible key on passed <code>row</code>, column and timestamp
    */
   public static KeyValue createFirstOnRow(final byte [] row, final byte [] c,
       final long ts) {
-    return new KeyValue(row, c, ts, Type.Maximum);
+    byte [][] split = parseColumn(c);
+    return new KeyValue(row, split[0], split[1], ts, Type.Maximum);
   }
 
   /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @return First possible key on passed <code>row</code>, and column.
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q) {
+    return new KeyValue(row, f, q, HConstants.LATEST_TIMESTAMP, Type.Maximum);
+  }
+
+  /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @param ts - timestamp
+   * @return First possible key on passed <code>row</code>, column and timestamp
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q, final long ts) {
+    return new KeyValue(row, f, q, ts, Type.Maximum);
+  }
+  
+  /**
    * @param b
    * @param o
    * @param l
@@ -1248,7 +1543,8 @@
       //          "---" + Bytes.toString(right, roffset, rlength));
       final int metalength = 7; // '.META.' length
       int lmetaOffsetPlusDelimiter = loffset + metalength;
-      int leftFarDelimiter = getDelimiterInReverse(left, lmetaOffsetPlusDelimiter,
+      int leftFarDelimiter = getDelimiterInReverse(left, 
+          lmetaOffsetPlusDelimiter,
           llength - metalength, HRegionInfo.DELIMITER);
       int rmetaOffsetPlusDelimiter = roffset + metalength;
       int rightFarDelimiter = getDelimiterInReverse(right,
@@ -1395,7 +1691,7 @@
       return compare(left, 0, left.length, right, 0, right.length);
     }
 
-    protected int compareRows(byte [] left, int loffset, int llength,
+    public int compareRows(byte [] left, int loffset, int llength,
         byte [] right, int roffset, int rlength) {
       return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);
     }
@@ -1420,4 +1716,22 @@
       return 0;
     }
   }
+  
+  // HeapSize
+  public long heapSize() {
+    return this.length;
+  }
+  
+  // Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.length = in.readInt();
+    this.offset = 0;
+    this.bytes = new byte[this.length];
+    in.readFully(this.bytes, 0, this.length);
+  }
+  
+  public void write(final DataOutput out) throws IOException {
+    out.writeInt(this.length);
+    out.write(this.bytes, this.offset, this.length);
+  }
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/io/Delete.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Delete.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Delete.java	(revision 11)
@@ -0,0 +1,293 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Used to perform Delete operations on a single row.
+ * <p>
+ * To delete an entire row, instantiate a Delete object with the row 
+ * to delete.  To further define the scope of what to delete, perform additional
+ * methods as outlined below.
+ * <p>
+ * To delete specific families, execute {@link #deleteFamily(byte []) deleteFamily}
+ * for each family to delete.
+ * <p>
+ * To delete multiple versions of specific columns, execute {@link #deleteColumns(byte [],byte []) deleteColumns}
+ * for each column to delete.  
+ * <p>
+ * To delete specific versions of specific columns, execute {@link #deleteColumn(byte [],byte [],long) deleteColumn}
+ * for each column version to delete.
+ * <p>
+ * Specifying timestamps to the constructor, deleteFamily, and deleteColumns
+ * will delete all versions with a timestamp less than or equal to that
+ * specified.  Specifying a timestamp to deleteColumn will delete versions
+ * only with a timestamp equal to that specified.
+ */
+public class Delete implements Writable {
+  private byte [] row = null;
+  private long timestamp;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap = 
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Delete() {}
+  
+  /**
+   * Create a Delete operation for the specified row.
+   * <p>
+   * If no further operations are done, this will delete everything
+   * associated with the specified row (all versions of all columns in all
+   * families).
+   * @param row row key
+   */
+  public Delete(byte [] row) {
+    this(row, HConstants.LATEST_TIMESTAMP, null);
+  }
+  
+  /**
+   * Create a Delete operation for the specified row and timestamp, using
+   * an optional row lock.
+   * <p>
+   * If no further operations are done, this will delete all columns in all
+   * families of the specified row with a timestamp less than or equal to the 
+   * specified timestamp.
+   * @param row row key
+   * @param timestamp maximum version timestamp
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Delete(byte [] row, long timestamp, RowLock rowLock) {
+    this.row = row;
+    this.timestamp = timestamp;
+    if(rowLock != null) {
+    	this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Delete all versions of all columns of the specified family.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   */
+  public void deleteFamily(byte [] family) {
+	this.deleteFamily(family, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all columns of the specified family with a timestamp less than
+   * or equal to the specified timestamp.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteFamily(byte [] family, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    } else if(!list.isEmpty()) {
+      list.clear();
+    }
+    list.add(new KeyValue(row, family, null, timestamp, KeyValue.Type.DeleteFamily));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete all versions of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier) {
+    this.deleteColumns(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all versions of the specified column with a timestamp less than
+   * or equal to the specified timestamp.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier, 
+	  long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.DeleteColumn));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier) {
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete the specified version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.Delete));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column, given in
+   * <code>family:qualifier</code> notation.
+   * @param column colon-delimited family and qualifier 
+   */
+  public void deleteColumn(byte [] column) {
+    int len = column.length;
+    int i=0;
+    byte b = 0;
+    for(; i<len; i++) {
+      b = column[i];
+      if(b == ':') {
+        break;
+      }
+    }
+    byte [] family = new byte[i];
+    System.arraycopy(column, 0, family, 0, i);
+    i++;
+    int qLen = len - i;
+    byte [] qualifier = new byte[qLen];
+    System.arraycopy(column, i, qualifier, 0, qLen);
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+	if(this.lockId == -1L) {
+      return null;
+	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+	return this.lockId;
+  }
+  
+  public long getTimeStamp() {
+    return this.timestamp;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", families={");
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", keyvalues=(");
+      boolean moreThanOneB = false;
+      for(KeyValue kv : entry.getValue()) {
+        if(moreThanOneB) {
+          sb.append(", ");
+        } else {
+          moreThanOneB = true;
+        }
+        sb.append(kv.toString());
+      }
+      sb.append(")");
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    this.familyMap = 
+        new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    int numFamilies = in.readInt();
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      List<KeyValue> list = new ArrayList<KeyValue>(numColumns);
+      for(int j=0;j<numColumns;j++) {
+    	KeyValue kv = new KeyValue();
+    	kv.readFields(in);
+    	list.add(kv);
+      }
+      this.familyMap.put(family, list);
+    }
+  }  
+  
+  public void write(final DataOutput out) throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> list = entry.getValue();
+      out.writeInt(list.size());
+      for(KeyValue kv : list) {
+        kv.write(out);
+      }
+    }
+  }
+}
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/io/hfile/HFile.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/hfile/HFile.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/io/hfile/HFile.java	(revision 11)
@@ -1035,6 +1035,9 @@
       }
       
       public KeyValue getKeyValue() {
+        if(this.block == null) {
+          return null;
+        }
         return new KeyValue(this.block.array(),
             this.block.arrayOffset() + this.block.position() - 8);
       }
Index: src/java/org/apache/hadoop/hbase/io/HeapSize.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/HeapSize.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/io/HeapSize.java	(revision 11)
@@ -21,11 +21,24 @@
 
 /**
  * Implementations can be asked for an estimate of their size in bytes.
+ * <p>
  * Useful for sizing caches.  Its a given that implementation approximations
- * probably do not account for 32 vs 64 bit nor for different VM implemenations.
+ * do not account for 32 vs 64 bit nor for different VM implementations.
+ * <p>
+ * An Object's size is determined by the non-static data members in it,
+ * as well as the fixed {@link OBJECT} overhead.
+ * <p>
+ * For example:
+ * <pre>
+ * public class SampleObject implements HeapSize {
+ *   
+ *   int [] numbers;
+ *   int x;
+ * }
+ * </pre>
  */
 public interface HeapSize {
-  
+
   /** Reference size is 8 bytes on 64-bit, 4 bytes on 32-bit */
   static final int REFERENCE = 8;
   
@@ -49,10 +62,12 @@
   static final int LONG = 8;
   
   /** Array overhead */
-  static final int BYTE_ARRAY = REFERENCE;
   static final int ARRAY = 3 * REFERENCE;
   static final int MULTI_ARRAY = (4 * REFERENCE) + ARRAY;
   
+  /** Byte arrays are fixed size below plus its length, 8 byte aligned */
+  static final int BYTE_ARRAY = 3 * REFERENCE;
+  
   static final int BLOCK_SIZE_TAX = 8;
 
   
Index: src/java/org/apache/hadoop/hbase/io/Put.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Put.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Put.java	(revision 11)
@@ -0,0 +1,215 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+/** 
+ * Used to perform Put operations for a single row.
+ * <p>
+ * To perform a Put, instantiate a Put object with the row to insert to and
+ * for each column to be inserted, execute {@link #add(byte[], byte[], byte[]) add} or
+ * {@link #add(byte[], byte[], long, byte[]) add} if setting the timestamp.
+ */
+public class Put implements HeapSize, Writable {
+  private byte [] row = null;
+  private long timestamp = HConstants.LATEST_TIMESTAMP;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap =
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Put() {}
+  
+  /**
+   * Create a Put operation for the specified row.
+   * @param row row key
+   */
+  public Put(byte [] row) {
+    this(row,null);
+  }
+  
+  /**
+   * Create a Put operation for the specified row, using an existing row lock.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Put(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Copy constructor.  Creates a Put operation cloned from the specified Put.
+   * @param putToCopy put to copy
+   */
+  public Put(Put putToCopy) {
+    this(putToCopy.getRow(), putToCopy.getRowLock());
+    this.familyMap = 
+      new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte [], List<KeyValue>> entry :
+      putToCopy.getFamilyMap().entrySet()) {
+      this.familyMap.put(entry.getKey(), entry.getValue());
+    }
+  }
+  
+  /**
+   * Add the specified column and value to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, byte [] value) {
+    add(family, qualifier, this.timestamp, value);
+  }
+
+  /**
+   * Add the specified column and value, with the specified timestamp as 
+   * its version to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, long timestamp, byte [] value) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    KeyValue kv = new KeyValue(this.row, family, qualifier, timestamp, 
+      KeyValue.Type.Put, value); 
+    list.add(kv);
+    familyMap.put(family, list);
+  }
+  
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+  	if(this.lockId == -1L) {
+      return null;
+  	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+  	return this.lockId;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", families={");
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", keyvalues=(");
+      boolean moreThanOneB = false;
+      for(KeyValue kv : entry.getValue()) {
+        if(moreThanOneB) {
+          sb.append(", ");
+        } else {
+          moreThanOneB = true;
+        }
+        sb.append(kv.toString());
+      }
+      sb.append(")");
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //HeapSize
+  public long heapSize() {
+  	long totalSize = 0;
+  	for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+  	  for(KeyValue kv : entry.getValue()) {
+  		totalSize += kv.heapSize();
+  	  }
+  	}
+    return totalSize;
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numKeys = in.readInt();
+      List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
+      for(int j=0;j<numKeys;j++) {
+        KeyValue kv = new KeyValue();
+        kv.readFields(in);
+        keys.add(kv);
+      }
+      this.familyMap.put(family, keys);
+    }
+  }
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> keys = entry.getValue();
+      out.writeInt(keys.size());
+      for(KeyValue kv : keys) {
+        kv.write(out);
+      }
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/TimeRange.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 11)
@@ -0,0 +1,163 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Represents an interval of version timestamps.
+ * <p>
+ * Evaluated according to minStamp <= timestamp < maxStamp
+ * or [minStamp,maxStamp) in interval notation.
+ * <p>
+ * Only used internally; should not be accessed directly by clients.
+ */
+public class TimeRange implements Writable {
+  private long minStamp = 0L;
+  private long maxStamp = Long.MAX_VALUE;
+  private boolean allTime = true;
+
+  /**
+   * Default constructor.
+   * Represents interval [0, Long.MAX_VALUE) (allTime)
+   */
+  public TimeRange() {
+    allTime = true;
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(long minStamp) {
+    this.minStamp = minStamp;
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(byte [] minStamp) {
+  	this.minStamp = Bytes.toLong(minStamp);
+  }
+  
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   */
+  public TimeRange(long minStamp, long maxStamp)
+  throws IOException {
+    if(minStamp > maxStamp) {
+      throw new IOException("maxStamp is smaller than minStamp");
+    }
+    this.minStamp = minStamp;
+    this.maxStamp = maxStamp;
+  }
+
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   */
+  public TimeRange(byte [] minStamp, byte [] maxStamp)
+  throws IOException {
+    this(Bytes.toLong(minStamp), Bytes.toLong(maxStamp));
+  }
+  
+  public long getMin() {
+    return minStamp;
+  }
+
+  public long getMax() {
+    return maxStamp;
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param bytes timestamp to check
+   * @param offset offset into the bytes
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(byte [] bytes, int offset) {
+  	if(allTime) return true;
+  	return withinTimeRange(Bytes.toLong(bytes, offset));
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(long timestamp) {
+  	if(allTime) return true;
+  	// check if >= minStamp
+  	return (minStamp <= timestamp && timestamp < maxStamp);
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinOrAfterTimeRange(long timestamp) {
+    if(allTime) return true;
+    // check if >= minStamp
+    return (timestamp >= minStamp);
+  }
+  
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("maxStamp=");
+    sb.append(this.maxStamp);
+    sb.append(", minStamp=");
+    sb.append(this.minStamp);
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.minStamp = in.readLong();
+    this.maxStamp = in.readLong();
+    this.allTime = in.readBoolean();
+  }
+  
+  public void write(final DataOutput out) throws IOException {
+    out.writeLong(minStamp);
+    out.writeLong(maxStamp);
+    out.writeBoolean(this.allTime);
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Result.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Result.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Result.java	(revision 11)
@@ -0,0 +1,255 @@
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.SplitKeyValue;
+import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Single row result of a {@link Get} or {@link Scan} query.
+ */
+public class Result implements Writable {
+  private byte [] row = null;
+  private KeyValue [] kvs = null;
+  private SortedMap<byte[], 
+     SortedMap<byte[], SortedMap<Long, byte[]>>> familyMap = null;
+
+  /**
+   * Constructor used for Writable.
+   */
+  public Result() {}
+  
+  /**
+   * Instantiate a Result with the specified array of KeyValues.
+   * @param kvs array of KeyValues
+   */
+  public Result(KeyValue [] kvs) {
+    if(kvs != null) {
+      this.row = kvs[0].getRow();
+      this.kvs = kvs;
+    }
+  }
+  
+  /**
+   * Instantiate a Result with the specified List of KeyValues.
+   * @param kvs List of KeyValues
+   */
+  public Result(List<KeyValue> kvs) {
+    this(kvs.toArray(new KeyValue[0]));
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  /**
+   * Directly return the unsorted array of KeyValues in this Result.
+   * @return unsorted array of KeyValues
+   */
+  public KeyValue[] raw() {
+    return kvs;
+  }
+  
+  /**
+   * Returns a sorted array of KeyValues in this Result.
+   * <p>
+   * Note: Sorting is done in place, so the backing array will be sorted
+   * after calling this method.
+   * @return sorted array of KeyValues
+   */
+  public KeyValue[] sorted() {
+    if(isEmpty()) {
+      return null;
+    }
+    Arrays.sort(kvs, (Comparator<KeyValue>)KeyValue.COMPARATOR);
+    return kvs;
+  }
+  
+  /**
+   * Map of families to all versions of its qualifiers and values.
+   * <p>
+   * Returns a three level Map of the form: 
+   * <code>Map<family,Map<qualifier,Map<timestamp,value>>></code>
+   * <p>
+   * Note: All other map returning methods make use of this map internally. 
+   * @return map from families to qualifiers to versions
+   */
+  public SortedMap<byte[], SortedMap<byte[], SortedMap<Long, byte[]>>> 
+  getMap() {
+    if(this.familyMap != null) {
+      return this.familyMap;
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    this.familyMap =
+      new TreeMap<byte[], SortedMap<byte[], SortedMap<Long, byte[]>>>
+      (Bytes.BYTES_COMPARATOR);
+    for(KeyValue kv : this.kvs) {
+      SplitKeyValue splitKV = kv.split();
+      byte [] family = splitKV.getFamily();
+      SortedMap<byte[], SortedMap<Long, byte[]>> columnMap = 
+        familyMap.get(family);
+      if(columnMap == null) {
+        columnMap = new TreeMap<byte[], SortedMap<Long, byte[]>>
+          (Bytes.BYTES_COMPARATOR);
+        familyMap.put(family, columnMap);
+      }
+      byte [] qualifier = splitKV.getQualifier();
+      SortedMap<Long, byte[]> versionMap = columnMap.get(qualifier);
+      if(versionMap == null) {
+        versionMap = new TreeMap<Long, byte[]>(new Comparator<Long>() {
+          public int compare(Long l1, Long l2) {
+            return l2.compareTo(l1);
+          }
+        });
+        columnMap.put(qualifier, versionMap);
+      }
+      Long timestamp = Bytes.toLong(splitKV.getTimestamp());
+      byte [] value = splitKV.getValue();
+      versionMap.put(timestamp, value);
+    }
+    return this.familyMap;
+  }
+  
+  /**
+   * Map of families to their most recent qualifiers and values.
+   * <p>
+   * Returns a two level Map of the form: <code>Map<family,Map<qualifier,value>></code>
+   * <p>
+   * The most recent version of each qualifier will be used.
+   * @return map from families to qualifiers and value
+   */
+  public SortedMap<byte[], SortedMap<byte[], byte[]>> getNoVersionMap() {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    SortedMap<byte[], SortedMap<byte[], byte[]>> returnMap = 
+      new TreeMap<byte[], SortedMap<byte[], byte[]>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte[], SortedMap<byte[], SortedMap<Long, byte[]>>> 
+      familyEntry : familyMap.entrySet()) {
+      SortedMap<byte[], byte[]> qualifierMap = 
+        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+      for(Map.Entry<byte[], SortedMap<Long, byte[]>> qualifierEntry :
+        familyEntry.getValue().entrySet()) {
+        byte [] value = 
+          qualifierEntry.getValue().get(qualifierEntry.getValue().firstKey());
+        qualifierMap.put(qualifierEntry.getKey(), value);
+      }
+      returnMap.put(familyEntry.getKey(), qualifierMap);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Map of qualifiers to values.
+   * <p>
+   * Returns a Map of the form: <code>Map<qualifier,value></code>
+   * @return map of qualifiers to values
+   */
+  public SortedMap<byte[], byte[]> getFamilyMap(byte[] family) {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    SortedMap<byte[], byte[]> returnMap = 
+      new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    SortedMap<byte[], SortedMap<Long, byte[]>> qualifierMap = 
+      familyMap.get(family);
+    if(qualifierMap == null) {
+      return returnMap;
+    }
+    for(Map.Entry<byte[], SortedMap<Long, byte[]>> entry : 
+      qualifierMap.entrySet()) {
+      byte [] value = 
+        entry.getValue().get(entry.getValue().firstKey());
+      returnMap.put(entry.getKey(), value);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Returns this Result in the old return format, {@link RowResult}.
+   * @return a RowResult
+   */
+  public RowResult rowResult() {
+    return RowResult.createRowResult(Arrays.asList(kvs));
+  }
+  
+  /**
+   * Method that returns the value of the first column in the Result.
+   * @return value of the first column
+   */
+  public byte [] value() {
+    if(isEmpty()) {
+      return null;
+    }
+    return kvs[0].getValue();
+  }
+  
+  public boolean isEmpty() {
+    return (this.kvs == null || this.kvs.length == 0);
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", keyvalues=");
+    if(isEmpty()) {
+      sb.append("NONE");
+      return sb.toString();
+    }
+    sb.append("{");
+    boolean moreThanOne = false;
+    for(KeyValue kv : this.kvs) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append(kv.toString());
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    int length = in.readInt();
+    this.kvs = new KeyValue[length];
+    for(int i=0; i<length; i++) {
+      KeyValue kv = new KeyValue();
+      kv.readFields(in);
+      this.kvs[i] = kv;
+    }
+  }  
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    int len = this.kvs.length;
+    out.writeInt(len);
+    for(int i=0; i<len; i++) {
+      this.kvs[i].write(out);
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Get.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Get.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Get.java	(revision 11)
@@ -0,0 +1,305 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Get operations on a single row.
+ * <p>
+ * To get everything for a row, instantiate a Get object with the row to get.
+ * To further define the scope of what to get, perform additional methods as 
+ * outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Get implements Writable {
+  private byte [] row = null;
+  private long lockId = -1L;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], TreeSet<byte []>> familyMap = 
+	new TreeMap<byte [], TreeSet<byte []>>(Bytes.BYTES_COMPARATOR);
+	  
+  /** Constructor for Writable.  DO NOT USE */
+  public Get() {}
+  
+  /**
+   * Create a Get operation for the specified row.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   */
+  public Get(byte [] row) {
+    this(row, null);
+  }
+  
+  /**
+   * Create a Get operation for the specified row, using an existing row lock.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Get(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+  
+  /**
+   * Get the column from the specific family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    TreeSet<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+  
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+  
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+  
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+  	this.maxVersions = Integer.MAX_VALUE;
+  }
+  
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) throws IOException {
+  	if(maxVersions <= 0) {
+  	  throw new IOException("maxVersions must be positive");
+  	}
+    this.maxVersions = maxVersions;
+  }
+  
+  /**
+   * Apply the specified server-side filter when performing the Get.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+  
+  /** Accessors */
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+  	if(this.lockId == -1L) {
+        return null;
+  	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+  	return this.lockId;
+  }
+
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  public TimeRange getTimeRange() {
+    return this.tr;
+  }
+
+  public Set<byte[]> familySet() {
+    return this.familyMap.keySet();
+  }
+  
+  public int numFamilies() {
+    return this.familyMap.size();
+  }
+  
+  public boolean hasFamilies() {
+    return !this.familyMap.isEmpty();
+  }
+  
+  public Map<byte[],TreeSet<byte[]>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", maxVersions=");
+    sb.append("" + this.maxVersions);
+    sb.append(", timeRange=");
+    sb.append("[" + this.tr.getMin() + "," + this.tr.getMax() + ")");
+    sb.append(", families=");
+    if(this.familyMap.size() == 0) {
+      sb.append("ALL");
+      return sb.toString();
+    }
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], TreeSet<byte[]>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append("), ");
+      } else {
+        moreThanOne = true;
+        sb.append("{");
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", columns=");
+      if(entry.getValue() == null) {
+        sb.append("ALL");
+      } else {
+        sb.append("{");
+        boolean moreThanOneB = false;
+        for(byte [] column : entry.getValue()) {
+          if(moreThanOneB) {
+            sb.append(", ");
+          } else {
+            moreThanOneB = true;
+          }
+          sb.append(Bytes.toString(column));
+        }
+        sb.append("}");
+      }
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+  	this.row = Bytes.readByteArray(in);
+  	this.lockId = in.readLong();
+  	this.maxVersions = in.readInt();
+  	boolean hasFilter = in.readBoolean();
+  	if(hasFilter) {
+  	  this.filter = (RowFilterInterface)HbaseObjectWritable.readObject(in, null);
+  	}
+  	this.tr = new TimeRange();
+  	tr.readFields(in);
+  	int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],TreeSet<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      TreeSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.lockId);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], TreeSet<byte []>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      TreeSet<byte []> columnSet = entry.getValue();
+      out.writeInt(columnSet.size());
+      for(byte [] qualifier : columnSet) {
+        Bytes.writeByteArray(out, qualifier);
+      }
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Scan.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Scan.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Scan.java	(revision 11)
@@ -0,0 +1,317 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Scan operations.
+ * <p>
+ * All operations are identical to {@link Get} with the exception of
+ * instantiation.  Rather than specifying a single row, an optional startRow
+ * and stopRow may be defined.  If rows are not specified, the Scanner will
+ * iterate over all rows.
+ * <p>
+ * To scan everything for each row, instantiate a Scan object.
+ * To further define the scope of what to get when scanning, perform additional 
+ * methods as outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Scan implements Writable {
+  private byte [] startRow = HConstants.EMPTY_START_ROW;
+  private byte [] stopRow  = HConstants.EMPTY_END_ROW;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], NavigableSet<byte []>> familyMap =
+    new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+  
+  /**
+   * Create a Scan operation across all rows.
+   */
+  public Scan() {}
+  
+  /**
+   * Create a Scan operation starting at the specified row.
+   * <p>
+   * If the specified row does not exist, the Scanner will start from the
+   * next closest row after the specified row.
+   * @param startRow row to start scanner at or after
+   */
+  public Scan(byte [] startRow) {
+    this.startRow = startRow;
+  }
+  
+  /**
+   * Create a Scan operation for the range of rows specified.
+   * @param startRow row to start scanner at or after (inclusive)
+   * @param stopRow row to stop scanner before (exclusive)
+   */
+  public Scan(byte [] startRow, byte [] stopRow) {
+    this.startRow = startRow;
+    this.stopRow = stopRow;
+  }
+  
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+  
+  /**
+   * Get the column from the specified family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    NavigableSet<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+  
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+  
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+  	this.maxVersions = Integer.MAX_VALUE;
+  }
+
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) {
+    this.maxVersions = maxVersions;
+  }
+  
+  /**
+   * Apply the specified server-side filter when performing the Scan.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+  
+  public void setFamilyMap(Map<byte [], NavigableSet<byte []>> familyMap) {
+    this.familyMap = familyMap;
+  }
+  
+  public Map<byte [], NavigableSet<byte []>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public int numFamilies() {
+    if(hasFamilies()) {
+      return this.familyMap.size();
+    }
+    return 0;
+  }
+  
+  public boolean hasFamilies() {
+    return !this.familyMap.isEmpty();
+  }
+  
+  public byte[][] getFamilies() {
+    if(hasFamilies()) {
+      return this.familyMap.keySet().toArray(new byte[0][0]);
+    }
+    return null;
+  }
+  
+  public byte [] getStartRow() {
+    return this.startRow;
+  }
+
+  public byte [] getStopRow() {
+    return this.stopRow;
+  }
+  
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  public TimeRange getTimeRange() {
+    return this.tr;
+  } 
+  
+  public RowFilterInterface getFilter() {
+    return filter;
+  }
+  
+  public boolean hasFilter() {
+    return filter != null;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("startRow=");
+    sb.append(Bytes.toString(this.startRow));
+    sb.append(", stopRow=");
+    sb.append(Bytes.toString(this.stopRow));
+    sb.append(", maxVersions=");
+    sb.append("" + this.maxVersions);
+    sb.append(", timeRange=");
+    sb.append("[" + this.tr.getMin() + "," + this.tr.getMax() + ")");
+    sb.append(", families=");
+    if(this.familyMap.size() == 0) {
+      sb.append("ALL");
+      return sb.toString();
+    }
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], NavigableSet<byte[]>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append("), ");
+      } else {
+        moreThanOne = true;
+        sb.append("{");
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", columns=");
+      if(entry.getValue() == null) {
+        sb.append("ALL");
+      } else {
+        sb.append("{");
+        boolean moreThanOneB = false;
+        for(byte [] column : entry.getValue()) {
+          if(moreThanOneB) {
+            sb.append(", ");
+          } else {
+            moreThanOneB = true;
+          }
+          sb.append(Bytes.toString(column));
+        }
+        sb.append("}");
+      }
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.startRow = Bytes.readByteArray(in);
+    this.stopRow = Bytes.readByteArray(in);
+  	this.maxVersions = in.readInt();
+  	boolean hasFilter = in.readBoolean();
+  	if(hasFilter) {
+  	  this.filter = (RowFilterInterface)HbaseObjectWritable.readObject(in, null);
+  	}
+  	this.tr = new TimeRange();
+  	tr.readFields(in);
+  	int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      TreeSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }  
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.startRow);
+    Bytes.writeByteArray(out, this.stopRow);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], NavigableSet<byte []>> entry : familyMap.entrySet()) {
+  	  Bytes.writeByteArray(out, entry.getKey());
+  	  Set<byte []> columnSet = entry.getValue();
+  	  out.writeInt(columnSet.size());
+  	  for(byte [] qualifier : columnSet) {
+  	    Bytes.writeByteArray(out, qualifier);
+  	  }
+  	}
+  }
+}
Index: src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
===================================================================
--- src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 11)
@@ -24,8 +24,11 @@
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Scan;
 
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.NotServingRegionException;
@@ -231,20 +234,12 @@
    * Opens a remote scanner with a RowFilter.
    * 
    * @param regionName name of region to scan
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex for column family name. A column name is judged to be
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row to scan
-   * @param timestamp only return values whose timestamp is <= this value
-   * @param filter RowFilter for filtering results at the row-level.
-   *
+   * @param scan configured scan object
    * @return scannerId scanner identifier used in other calls
    * @throws IOException
    */
-  public long openScanner(final byte [] regionName, final byte [][] columns,
-      final byte [] startRow, long timestamp, RowFilterInterface filter)
+  public long openScanner(final byte [] regionName, final byte [] startRow, 
+      final Scan scan)
   throws IOException;
   
   /**
@@ -253,7 +248,8 @@
    * @return map of values
    * @throws IOException
    */
-  public RowResult next(long scannerId) throws IOException;
+ // Is this used anymore?
+  //public Result next(long scannerId) throws IOException;
   
   /**
    * Get the next set of values
@@ -262,7 +258,7 @@
    * @return map of values
    * @throws IOException
    */
-  public RowResult[] next(long scannerId, int numberOfRows) throws IOException;
+  public Result [] next(long scannerId, int numberOfRows) throws IOException;
   
   /**
    * Close a scanner
@@ -271,7 +267,7 @@
    * @throws IOException
    */
   public void close(long scannerId) throws IOException;
-  
+
   /**
    * Opens a remote row lock.
    *
@@ -306,4 +302,17 @@
    */
   public long incrementColumnValue(byte [] regionName, byte [] row,
       byte [] column, long amount) throws IOException;
+  
+  
+  //
+  // HBASE-880
+  //
+  
+  /**
+   * Perform Get operation.
+   * 
+   * @param get Get operation
+   * @return Result
+   */
+  public Result get(byte [] regionName, Get get) throws IOException;
 }
Index: src/java/org/apache/hadoop/hbase/util/MetaUtils.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/MetaUtils.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/util/MetaUtils.java	(revision 11)
@@ -250,9 +250,8 @@
       while (metaScanner.next(results)) {
         HRegionInfo info = null;
         for (KeyValue kv: results) {
-          if (KeyValue.META_COMPARATOR.compareColumns(kv,
-            HConstants.COL_REGIONINFO, 0, HConstants.COL_REGIONINFO.length,
-              HConstants.COLUMN_FAMILY_STR.length()) == 0) {
+          if(kv.matchingColumn(HConstants.CATALOG_FAMILY,
+              HConstants.REGIONINFO_QUALIFIER)) {
             info = Writables.getHRegionInfoOrNull(kv.getValue());
             if (info == null) {
               LOG.warn("region info is null for row " +
Index: src/java/org/apache/hadoop/hbase/util/Bytes.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/Bytes.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/util/Bytes.java	(revision 11)
@@ -27,9 +27,12 @@
 import java.util.Comparator;
 import java.math.BigInteger;
 
+import org.apache.hadoop.hbase.ColumnNameParseException;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.io.WritableUtils;
 
@@ -38,8 +41,93 @@
  * comparisons, hash code generation, manufacturing keys for HashMaps or
  * HashSets, etc.
  */
-public class Bytes {
+public class Bytes implements Writable, Comparable<Bytes> {
+  
+  //
+  // Instantiable Bytes
+  //
+  
+  private byte [] bytes;
+  private int offset;
+  private int length;
+  
+  /** Writable Constructor, DO NOT USE. */
+  public Bytes() {}
+  
   /**
+   * Constructor that only takes the byte [] and sets the offset and length
+   * internally.
+   * @param bytes
+   */
+  public Bytes(byte [] bytes){
+    this(bytes, 0, bytes.length);
+  }
+  
+  /**
+   * Constructor to create a wrapper for a byte [], so that it can be used as a
+   * pointer into bigger byte [] without having to do any copying of the byte []
+   * @param bytes
+   * @param offset
+   * @param length
+   */
+  public Bytes(byte [] bytes, int offset, int length){
+    this.bytes = bytes;
+    this.offset = offset;
+    this.length = length;
+  }
+  
+  /**
+   * Compare method to compare this Bytes object with a byte [] and it's offset
+   * and length.
+   */
+  public int compareTo(byte [] bytes, int offset, int length){
+    return Bytes.compareTo(this.bytes, this.offset, this.length, 
+        bytes, offset, length);
+  }
+  
+  //Comparable
+  @Override
+  public int compareTo(Bytes bytes){
+    return Bytes.compareTo(this.bytes, this.offset, this.length, 
+        bytes.bytes, bytes.offset, bytes.length);
+  }
+  
+  //Writable
+  @Override
+  public void readFields(final DataInput in) throws IOException {
+    this.length = in.readInt();
+    this.offset = in.readInt();
+    this.bytes = new byte[this.length];
+    in.readFully(this.bytes, this.offset, this.length);
+  }
+  @Override
+  public void write(final DataOutput out) throws IOException {
+    out.writeInt(this.length);
+    out.writeInt(this.offset);
+    out.write(this.bytes, this.offset, this.bytes.length);
+  }
+  
+  //toString
+  @Override
+  public String toString(){
+    return new String(this.bytes, this.offset, this.length);
+  }
+  
+  /**
+   * Return as a byte array.  If this Bytes represents the entire buffer,
+   * the backing array will be returned.  Otherwise, a copy is returned.
+   * @return backing array if complete, otherwise a copy
+   */
+  public byte [] get() {
+    if(this.offset == 0 && this.length == this.bytes.length) {
+      return this.bytes;
+    }
+    byte [] ret = new byte[this.length];
+    System.arraycopy(this.bytes, this.offset, ret, 0, this.length);
+    return ret;
+  }
+  
+  /**
    * Size of long in bytes
    */
   public static final int SIZEOF_LONG = Long.SIZE/Byte.SIZE;
Index: src/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HTable.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/client/HTable.java	(revision 11)
@@ -44,11 +44,15 @@
 import org.apache.hadoop.hbase.io.BatchOperation;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.ipc.Server;
 
 /**
  * Used to communicate with a single HBase table
@@ -657,232 +661,15 @@
     }
 
   /** 
-   * Get a scanner on the current table starting at first row.
-   * Return the specified columns.
+   * Get a scanner on the current table as specified by the {@link Scan} object
    *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
+   * @param scan a configured {@link Scan} object
    * @return scanner
    * @throws IOException
    */
-  public Scanner getScanner(final String [] columns)
+  public Scanner getScanner(final Scan scan)
   throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), HConstants.EMPTY_START_ROW);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final String [] columns, final String startRow)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow));
-  }
-
-  /** 
-   * Get a scanner on the current table starting at first row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns)
-  throws IOException {
-    return getScanner(columns, HConstants.EMPTY_START_ROW,
-      HConstants.LATEST_TIMESTAMP, null);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow)
-  throws IOException {
-    return getScanner(columns, startRow, HConstants.LATEST_TIMESTAMP, null);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow,
-    long timestamp)
-  throws IOException {
-    return getScanner(columns, startRow, timestamp, null);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow,
-    RowFilterInterface filter)
-  throws IOException { 
-    return getScanner(columns, startRow, HConstants.LATEST_TIMESTAMP, filter);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, final byte [] stopRow)
-  throws IOException {
-    return getScanner(columns, startRow, stopRow,
-      HConstants.LATEST_TIMESTAMP);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final String [] columns,
-    final String startRow, final String stopRow, final long timestamp)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow),
-      Bytes.toBytes(stopRow), timestamp);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, final byte [] stopRow, final long timestamp)
-  throws IOException {
-    return getScanner(columns, startRow, timestamp,
-      new WhileMatchRowFilter(new StopRowFilter(stopRow)));
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(String[] columns,
-    String startRow, long timestamp, RowFilterInterface filter)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow),
-      timestamp, filter);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, long timestamp, RowFilterInterface filter)
-  throws IOException {
-    ClientScanner s = new ClientScanner(columns, startRow,
-        timestamp, filter);
+    ClientScanner s = new ClientScanner(scan);
     s.initialize();
     return s;
   }
@@ -1525,37 +1312,30 @@
    */
   protected class ClientScanner implements Scanner {
     private final Log CLIENT_LOG = LogFactory.getLog(this.getClass());
-    private byte[][] columns;
-    private byte [] startRow;
-    protected long scanTime;
+    private Scan scan;
     private boolean closed = false;
     private HRegionInfo currentRegion = null;
     private ScannerCallable callable = null;
-    protected RowFilterInterface filter;
-    private final LinkedList<RowResult> cache = new LinkedList<RowResult>();
+    private final LinkedList<Result> cache = new LinkedList<Result>();
     @SuppressWarnings("hiding")
     private final int scannerCaching = HTable.this.scannerCaching;
     private long lastNext;
-
-    protected ClientScanner(final byte[][] columns, final byte [] startRow,
-        final long timestamp, final RowFilterInterface filter) {
+    
+    protected ClientScanner(final Scan scan) {
       if (CLIENT_LOG.isDebugEnabled()) {
         CLIENT_LOG.debug("Creating scanner over " 
             + Bytes.toString(getTableName()) 
-            + " starting at key '" + Bytes.toString(startRow) + "'");
+            + " starting at key '" + Bytes.toString(scan.getStartRow()) + "'");
       }
-      // save off the simple parameters
-      this.columns = columns;
-      this.startRow = startRow;
-      this.scanTime = timestamp;
-      
-      // save the filter, and make sure that the filter applies to the data
-      // we're expecting to pull back
-      this.filter = filter;
-      if (filter != null) {
-        filter.validate(columns);
-      }
+      this.scan = scan;
       this.lastNext = System.currentTimeMillis();
+      
+      // Removed filter validation.  We have a new format now, only one of all
+      // the current filters has a validate() method.  We can add it back,
+      // need to decide on what we're going to do re: filter redesign.
+      // Need, at the least, to break up family from qualifier as separate
+      // checks, I think it's important server-side filters are optimal in that
+      // respect.
     }
 
     //TODO: change visibility to protected
@@ -1563,19 +1343,15 @@
     public void initialize() throws IOException {
       nextScanner(this.scannerCaching);
     }
-    
-    protected byte[][] getColumns() {
-      return columns;
+
+    protected Scan getScan() {
+      return scan;
     }
     
     protected long getTimestamp() {
-      return scanTime;
+      return lastNext;
     }
     
-    protected RowFilterInterface getFilter() {
-      return filter;
-    }
-        
     /*
      * Gets a scanner for the next region.
      * Returns false if there are no more scanners.
@@ -1603,9 +1379,10 @@
           return false;
         }
       } 
-      
+
       HRegionInfo oldRegion = this.currentRegion;
-      byte [] localStartKey = oldRegion == null? startRow: oldRegion.getEndKey();
+      byte [] localStartKey = 
+        oldRegion == null ? scan.getStartRow() : oldRegion.getEndKey();
 
       if (CLIENT_LOG.isDebugEnabled()) {
         CLIENT_LOG.debug("Advancing internal scanner to startKey at '" +
@@ -1628,8 +1405,7 @@
     protected ScannerCallable getScannerCallable(byte [] localStartKey,
         int nbRows) {
       ScannerCallable s = new ScannerCallable(getConnection(), 
-          getTableName(), columns, 
-          localStartKey, scanTime, filter);
+          getTableName(), localStartKey, scan);
       s.setCaching(nbRows);
       return s;
     }
@@ -1640,22 +1416,22 @@
      * filter.
      */
     private boolean filterSaysStop(final byte [] endKey) {
-      if (this.filter == null) {
+      if(!scan.hasFilter()) {
         return false;
       }
       // Let the filter see current row.
-      this.filter.filterRowKey(endKey, 0, endKey.length);
-      return this.filter.filterAllRemaining();
+      scan.getFilter().filterRowKey(endKey, 0, endKey.length);
+      return scan.getFilter().filterAllRemaining();
     }
 
-    public RowResult next() throws IOException {
+    public Result next() throws IOException {
       // If the scanner is closed but there is some rows left in the cache,
       // it will first empty it before returning null
       if (cache.size() == 0 && this.closed) {
         return null;
       }
       if (cache.size() == 0) {
-        RowResult[] values = null;
+        Result [] values = null;
         int countdown = this.scannerCaching;
         // We need to reset it if it's a new callable that was created 
         // with a countdown in nextScanner
@@ -1674,7 +1450,7 @@
           }
           lastNext = System.currentTimeMillis();
           if (values != null && values.length > 0) {
-            for (RowResult rs : values) {
+            for (Result rs : values) {
               cache.add(rs);
               countdown--;
             }
@@ -1693,18 +1469,18 @@
      * @return Between zero and <param>nbRows</param> RowResults
      * @throws IOException
      */
-    public RowResult[] next(int nbRows) throws IOException {
+    public Result [] next(int nbRows) throws IOException {
       // Collect values to be returned here
-      ArrayList<RowResult> resultSets = new ArrayList<RowResult>(nbRows);
+      ArrayList<Result> resultSets = new ArrayList<Result>(nbRows);
       for(int i = 0; i < nbRows; i++) {
-        RowResult next = next();
+        Result next = next();
         if (next != null) {
           resultSets.add(next);
         } else {
           break;
         }
       }
-      return resultSets.toArray(new RowResult[resultSets.size()]);
+      return resultSets.toArray(new Result[resultSets.size()]);
     }
     
     public void close() {
@@ -1723,10 +1499,10 @@
       closed = true;
     }
 
-    public Iterator<RowResult> iterator() {
-      return new Iterator<RowResult>() {
+    public Iterator<Result> iterator() {
+      return new Iterator<Result>() {
         // The next RowResult, possibly pre-read
-        RowResult next = null;
+        Result next = null;
         
         // return true if there is another item pending, false if there isn't.
         // this method is where the actual advancing takes place, but you need
@@ -1746,7 +1522,7 @@
 
         // get the pending next item and advance the iterator. returns null if
         // there is no next item.
-        public RowResult next() {
+        public Result next() {
           // since hasNext() does the real advancing, we call this to determine
           // if there is a next before proceeding.
           if (!hasNext()) {
@@ -1756,7 +1532,7 @@
           // if we get to here, then hasNext() has given us an item to return.
           // we want to return the item and then null out the next pointer, so
           // we use a temporary variable.
-          RowResult temp = next;
+          Result temp = next;
           next = null;
           return temp;
         }
@@ -1767,4 +1543,21 @@
       };
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  public Result get(final Get get) throws IOException {
+    return connection.getRegionServerWithRetries(
+        new ServerCallable<Result>(connection, tableName, get.getRow()) {
+          public Result call() throws IOException {
+            return server.get(location.getRegionInfo().getRegionName(), get);
+          }
+        }
+    );
+  }
+  
+  
+  
 }
Index: src/java/org/apache/hadoop/hbase/client/ScannerCallable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/ScannerCallable.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/client/ScannerCallable.java	(revision 11)
@@ -24,20 +24,20 @@
 
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 
 
 /**
  * Retries scanner operations such as create, next, etc.
  * Used by {@link Scanner}s made by {@link HTable}.
  */
-public class ScannerCallable extends ServerCallable<RowResult[]> {
+public class ScannerCallable extends ServerCallable<Result[]> {
   private long scannerId = -1L;
   private boolean instantiated = false;
   private boolean closed = false;
-  private final byte [][] columns;
-  private final long timestamp;
-  private final RowFilterInterface filter;
+  private Scan scan;
+  private byte [] startRow;
   private int caching = 1;
 
   /**
@@ -48,12 +48,11 @@
    * @param timestamp
    * @param filter
    */
-  public ScannerCallable (HConnection connection, byte [] tableName, byte [][] columns,
-      byte [] startRow, long timestamp, RowFilterInterface filter) {
+  public ScannerCallable (HConnection connection, byte [] tableName,
+      byte [] startRow, Scan scan) {
     super(connection, tableName, startRow);
-    this.columns = columns;
-    this.timestamp = timestamp;
-    this.filter = filter;
+    this.scan = scan;
+    this.startRow = startRow;
   }
   
   /**
@@ -71,7 +70,7 @@
   /**
    * @see java.util.concurrent.Callable#call()
    */
-  public RowResult[] call() throws IOException {
+  public Result [] call() throws IOException {
     if (scannerId != -1L && closed) {
       server.close(scannerId);
       scannerId = -1L;
@@ -79,7 +78,7 @@
       // open the scanner
       scannerId = openScanner();
     } else {
-      RowResult [] rrs = server.next(scannerId, caching);
+      Result [] rrs = server.next(scannerId, caching);
       return rrs.length == 0 ? null : rrs;
     }
     return null;
@@ -87,22 +86,13 @@
   
   protected long openScanner() throws IOException {
     return server.openScanner(
-        this.location.getRegionInfo().getRegionName(), columns, row,
-        timestamp, filter);
+        this.location.getRegionInfo().getRegionName(), startRow, scan);
   }
   
-  protected byte [][] getColumns() {
-    return columns;
+  protected Scan getScan() {
+    return scan;
   }
   
-  protected long getTimestamp() {
-    return timestamp;
-  }
-  
-  protected RowFilterInterface getFilter() {
-    return filter;
-  }
-  
   /**
    * Call this when the next invocation of call should close the scanner
    */
Index: src/java/org/apache/hadoop/hbase/client/Scanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/Scanner.java	(revision 1)
+++ src/java/org/apache/hadoop/hbase/client/Scanner.java	(revision 11)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -21,32 +21,30 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import org.apache.hadoop.hbase.io.RowResult;
 
+import org.apache.hadoop.hbase.io.Result;
+
 /**
  * Interface for client-side scanning.
  * Go to {@link HTable} to obtain instances.
  */
-public interface Scanner extends Closeable, Iterable<RowResult> {
+public interface Scanner extends Closeable, Iterable<Result> {
+
   /**
-   * Grab the next row's worth of values. The scanner will return a RowResult
-   * that contains both the row's key and a map of byte[] column names to Cell 
-   * value objects. The data returned will only contain the most recent data 
-   * value for each row that is not newer than the target time passed when the
-   * scanner was created.
-   * @return RowResult object if there is another row, null if the scanner is
+   * Grab the next row's worth of values. The scanner will return a Result.
+   * @return Result object if there is another row, null if the scanner is
    * exhausted.
    * @throws IOException
    */  
-  public RowResult next() throws IOException;
-  
+  public Result next() throws IOException;
+ 
   /**
    * @param nbRows number of rows to return
-   * @return Between zero and <param>nbRows</param> RowResults
+   * @return Between zero and <param>nbRows</param> Results
    * @throws IOException
    */
-  public RowResult [] next(int nbRows) throws IOException;
-  
+  public Result [] next(int nbRows) throws IOException;
+ 
   /**
    * Closes the scanner and releases any resources it has allocated
    */
