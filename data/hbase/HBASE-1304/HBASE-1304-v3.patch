Index: java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- java/org/apache/hadoop/hbase/client/HTable.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/client/HTable.java	(working copy)
@@ -44,11 +44,14 @@
 import org.apache.hadoop.hbase.io.BatchOperation;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.ipc.Server;
 
 /**
  * Used to communicate with a single HBase table
@@ -1767,4 +1770,21 @@
       };
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  public Result get(final Get get) throws IOException {
+    return connection.getRegionServerWithRetries(
+        new ServerCallable<Result>(connection, tableName, get.getRow()) {
+          public Result call() throws IOException {
+            return server.get(location.getRegionInfo().getRegionName(), get);
+          }
+        }
+    );
+  }
+  
+  
+  
 }
Index: java/org/apache/hadoop/hbase/HConstants.java
===================================================================
--- java/org/apache/hadoop/hbase/HConstants.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/HConstants.java	(working copy)
@@ -162,8 +162,39 @@
   /** delimiter used between portions of a region name */
   public static final int META_ROW_DELIMITER = ',';
 
-  // Defines for the column names used in both ROOT and META HBase 'meta' tables.
+  /** The catalog family */
+  static final byte [] CATALOG_FAMILY = Bytes.toBytes("info");
   
+  /** Array of catalog family */
+  static final byte [][] CATALOG_FAMILY_ARRAY = new byte [][] {CATALOG_FAMILY};
+  
+  /** The catalog historian family */
+  static final byte [] CATALOG_HISTORIAN_FAMILY = Bytes.toBytes("historian");
+  
+  /** The regioninfo column qualifier */
+  static final byte [] REGIONINFO_QUALIFIER = Bytes.toBytes("regioninfo");
+  
+  /** Array of regioninfo column qualifier */ 
+  static final byte [][] REGIONINFO_QUALIFIER_ARRAY = new byte [][] {REGIONINFO_QUALIFIER};
+  
+  /** The server column qualifier */
+  static final byte [] SERVER_QUALIFIER = Bytes.toBytes("server");
+  
+  /** The startcode column qualifier */
+  static final byte [] STARTCODE_QUALIFIER = Bytes.toBytes("serverstartcode");
+  
+  /** The lower-half split region column qualifier */
+  static final byte [] SPLITA_QUALIFIER = Bytes.toBytes("splitA");
+  
+  /** The upper-half split region column qualifier */
+  static final byte [] SPLITB_QUALIFIER = Bytes.toBytes("splitB");
+  
+  /** All catalog column qualifiers */
+  static final byte [][] ALL_CATALOG_QUALIFIERS = {REGIONINFO_QUALIFIER, 
+    SERVER_QUALIFIER, STARTCODE_QUALIFIER, SPLITA_QUALIFIER, SPLITB_QUALIFIER};
+  
+  // Old style, still making the transition
+  
   /** The ROOT and META column family (string) */
   static final String COLUMN_FAMILY_STR = "info:";
   
@@ -204,6 +235,11 @@
   static final byte[][] ALL_META_COLUMNS = {COL_REGIONINFO, COL_SERVER,
     COL_STARTCODE, COL_SPLITA, COL_SPLITB};
 
+  //
+  // New stuff.  Making a slow transition.
+  //
+  
+
   // Other constants
 
   /**
Index: java/org/apache/hadoop/hbase/HTableDescriptor.java
===================================================================
--- java/org/apache/hadoop/hbase/HTableDescriptor.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/HTableDescriptor.java	(working copy)
@@ -102,7 +102,7 @@
   // Key is hash of the family name.
   private final Map<byte [], HColumnDescriptor> families =
     new TreeMap<byte [], HColumnDescriptor>(KeyValue.FAMILY_COMPARATOR);
-
+  
   // Key is indexId
   private final Map<String, IndexSpecification> indexes =
     new HashMap<String, IndexSpecification>();
Index: java/org/apache/hadoop/hbase/io/Delete.java
===================================================================
--- java/org/apache/hadoop/hbase/io/Delete.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/Delete.java	(revision 0)
@@ -0,0 +1,282 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Used to perform Delete operations on a single row.
+ * <p>
+ * To delete an entire row, instantiate a Delete object with the row 
+ * to delete.  To further define the scope of what to delete, perform additional
+ * methods as outlined below.
+ * <p>
+ * To delete specific families, execute {@link #deleteFamily(byte []) deleteFamily}
+ * for each family to delete.
+ * <p>
+ * To delete multiple versions of specific columns, execute {@link #deleteColumns(byte [],byte []) deleteColumns}
+ * for each column to delete.  
+ * <p>
+ * To delete specific versions of specific columns, execute {@link #deleteColumn(byte [],byte [],long) deleteColumn}
+ * for each column version to delete.
+ * <p>
+ * Specifying timestamps to the constructor, deleteFamily, and deleteColumns
+ * will delete all versions with a timestamp less than or equal to that
+ * specified.  Specifying a timestamp to deleteColumn will delete versions
+ * only with a timestamp equal to that specified.
+ */
+public class Delete implements Writable {
+  private byte [] row = null;
+  private long timestamp;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap = 
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Delete() {}
+  
+  /**
+   * Create a Delete operation for the specified row.
+   * <p>
+   * If no further operations are done, this will delete everything
+   * associated with the specified row (all versions of all columns in all
+   * families).
+   * @param row row key
+   */
+  public Delete(byte [] row) {
+    this(row, HConstants.LATEST_TIMESTAMP, null);
+  }
+  
+  /**
+   * Create a Delete operation for the specified row and timestamp, using
+   * an optional row lock.
+   * <p>
+   * If no further operations are done, this will delete all columns in all
+   * families of the specified row with a timestamp less than or equal to the 
+   * specified timestamp.
+   * @param row row key
+   * @param timestamp maximum version timestamp
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Delete(byte [] row, long timestamp, RowLock rowLock) {
+    this.row = row;
+    this.timestamp = timestamp;
+    if(rowLock != null) {
+    	this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Delete all versions of all columns of the specified family.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   */
+  public void deleteFamily(byte [] family) {
+	this.deleteFamily(family, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all columns of the specified family with a timestamp less than
+   * or equal to the specified timestamp.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteFamily(byte [] family, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    } else if(!list.isEmpty()) {
+      list.clear();
+    }
+    list.add(new KeyValue(row, family, null, timestamp, KeyValue.Type.DeleteFamily));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete all versions of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier) {
+    this.deleteColumns(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all versions of the specified column with a timestamp less than
+   * or equal to the specified timestamp.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier, 
+	  long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.DeleteColumn));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier) {
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete the specified version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.Delete));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column, given in
+   * <code>family:qualifier</code> notation.
+   * @param column colon-delimited family and qualifier 
+   */
+  public void deleteColumn(byte [] column) {
+    int len = column.length;
+    int i=0;
+    byte b = 0;
+    for(; i<len; i++) {
+      b = column[i];
+      if(b == ':') {
+        break;
+      }
+    }
+    byte [] family = new byte[i];
+    System.arraycopy(column, 0, family, 0, i);
+    i++;
+    int qLen = len - i;
+    byte [] qualifier = new byte[qLen];
+    System.arraycopy(column, i, qualifier, 0, qLen);
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+	if(this.lockId == -1L) {
+      return null;
+	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+	return this.lockId;
+  }
+  
+  public long getTimeStamp() {
+    return this.timestamp;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("Row ");
+    sb.append(new String(this.row));
+    sb.append(", Families[ ");
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      sb.append(new String(entry.getKey()));
+      sb.append(", (");
+      for(KeyValue kv : entry.getValue()) {
+        sb.append(kv.toString());
+        sb.append(", ");
+      }
+      sb.append("), ");
+    }
+    sb.append("]");    
+    
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    this.familyMap = 
+        new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    int numFamilies = in.readInt();
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      List<KeyValue> list = new ArrayList<KeyValue>(numColumns);
+      for(int j=0;j<numColumns;j++) {
+    	KeyValue kv = new KeyValue();
+    	kv.readFields(in);
+    	list.add(kv);
+      }
+      this.familyMap.put(family, list);
+    }
+  }  
+  
+  public void write(final DataOutput out) throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> list = entry.getValue();
+      out.writeInt(list.size());
+      for(KeyValue kv : list) {
+        kv.write(out);
+      }
+    }
+  }
+}
\ No newline at end of file
Index: java/org/apache/hadoop/hbase/io/Get.java
===================================================================
--- java/org/apache/hadoop/hbase/io/Get.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/Get.java	(revision 0)
@@ -0,0 +1,257 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Get operations on a single row.
+ * <p>
+ * To get everything for a row, instantiate a Get object with the row to get.
+ * To further define the scope of what to get, perform additional methods as 
+ * outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Get implements Writable, ScanOperation {
+  private byte [] row = null;
+  private long lockId = -1L;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], TreeSet<byte []>> familyMap = 
+	new TreeMap<byte [], TreeSet<byte []>>(Bytes.BYTES_COMPARATOR);
+	  
+  /** Constructor for Writable.  DO NOT USE */
+  public Get() {}
+  
+  /**
+   * Create a Get operation for the specified row.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   */
+  public Get(byte [] row) {
+    this(row, null);
+  }
+  
+  /**
+   * Create a Get operation for the specified row, using an existing row lock.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Get(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+  
+  /**
+   * Get the column from the specific family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    TreeSet<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+  
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+  
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+  
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+  	this.maxVersions = Integer.MAX_VALUE;
+  }
+  
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) throws IOException {
+  	if(maxVersions <= 0) {
+  	  throw new IOException("maxVersions must be positive");
+  	}
+    this.maxVersions = maxVersions;
+  }
+  
+  /**
+   * Apply the specified server-side filter when performing the Get.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+  
+  /** Accessors */
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+  	if(this.lockId == -1L) {
+        return null;
+  	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+  	return this.lockId;
+  }
+
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  public TimeRange getTimeRange() {
+    return this.tr;
+  }
+
+  public Set<byte[]> familySet() {
+    return this.familyMap.keySet();
+  }
+  
+  public int numFamilies() {
+    return this.familyMap.size();
+  }
+  
+  public Set<Map.Entry<byte[],TreeSet<byte[]>>> entrySet() {
+    return this.familyMap.entrySet();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+  	this.row = Bytes.readByteArray(in);
+  	this.lockId = in.readLong();
+  	this.maxVersions = in.readInt();
+  	boolean hasFilter = in.readBoolean();
+  	if(hasFilter) {
+  	  this.filter = (RowFilterInterface)HbaseObjectWritable.readObject(in, null);
+  	}
+  	this.tr = new TimeRange();
+  	tr.readFields(in);
+  	int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],TreeSet<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      TreeSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.lockId);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], TreeSet<byte []>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      TreeSet<byte []> columnSet = entry.getValue();
+      out.writeInt(columnSet.size());
+      for(byte [] qualifier : columnSet) {
+        Bytes.writeByteArray(out, qualifier);
+      }
+    }
+  }
+}
Index: java/org/apache/hadoop/hbase/io/HeapSize.java
===================================================================
--- java/org/apache/hadoop/hbase/io/HeapSize.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/io/HeapSize.java	(working copy)
@@ -49,7 +49,7 @@
   static final int LONG = 8;
   
   /** Array overhead */
-  static final int BYTE_ARRAY = REFERENCE;
+  static final int BYTE_ARRAY = 3 * REFERENCE;
   static final int ARRAY = 3 * REFERENCE;
   static final int MULTI_ARRAY = (4 * REFERENCE) + ARRAY;
   
Index: java/org/apache/hadoop/hbase/io/Put.java
===================================================================
--- java/org/apache/hadoop/hbase/io/Put.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/Put.java	(revision 0)
@@ -0,0 +1,203 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+/** 
+ * Used to perform Put operations for a single row.
+ * <p>
+ * To perform a Put, instantiate a Put object with the row to insert to and
+ * for each column to be inserted, execute {@link #add(byte[], byte[], byte[]) add} or
+ * {@link #add(byte[], byte[], long, byte[]) add} if setting the timestamp.
+ */
+public class Put implements HeapSize, Writable {
+  private byte [] row = null;
+  private long timestamp = HConstants.LATEST_TIMESTAMP;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap =
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Put() {}
+  
+  /**
+   * Create a Put operation for the specified row.
+   * @param row row key
+   */
+  public Put(byte [] row) {
+    this(row,null);
+  }
+  
+  /**
+   * Create a Put operation for the specified row, using an existing row lock.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Put(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Copy constructor.  Creates a Put operation cloned from the specified Put.
+   * @param putToCopy put to copy
+   */
+  public Put(Put putToCopy) {
+    this(putToCopy.getRow(), putToCopy.getRowLock());
+    this.familyMap = 
+      new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte [], List<KeyValue>> entry :
+      putToCopy.getFamilyMap().entrySet()) {
+      this.familyMap.put(entry.getKey(), entry.getValue());
+    }
+  }
+  
+  /**
+   * Add the specified column and value to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, byte [] value) {
+    add(family, qualifier, this.timestamp, value);
+  }
+
+  /**
+   * Add the specified column and value, with the specified timestamp as 
+   * its version to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, long timestamp, byte [] value) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    KeyValue kv = new KeyValue(this.row, family, qualifier, timestamp, 
+      KeyValue.Type.Put, value); 
+    list.add(kv);
+    familyMap.put(family, list);
+  }
+  
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  public RowLock getRowLock() {
+  	if(this.lockId == -1L) {
+      return null;
+  	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  public long getLockId() {
+  	return this.lockId;
+  }
+  
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("Row ");
+    sb.append(new String(this.row));
+    sb.append(", Families[ ");
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      sb.append(new String(entry.getKey()));
+      sb.append(", (");
+      for(KeyValue kv : entry.getValue()) {
+        sb.append(kv.toString());
+        sb.append(", ");
+      }
+      sb.append("), ");
+    }
+    sb.append("]");
+    return sb.toString();
+  }
+  
+  //HeapSize
+  public long heapSize() {
+  	long totalSize = 0;
+  	for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+  	  for(KeyValue kv : entry.getValue()) {
+  		totalSize += kv.heapSize();
+  	  }
+  	}
+    return totalSize;
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numKeys = in.readInt();
+      List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
+      for(int j=0;j<numKeys;j++) {
+        KeyValue kv = new KeyValue();
+        kv.readFields(in);
+        keys.add(kv);
+      }
+      this.familyMap.put(family, keys);
+    }
+  }
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> keys = entry.getValue();
+      out.writeInt(keys.size());
+      for(KeyValue kv : keys) {
+        kv.write(out);
+      }
+    }
+  }
+}
Index: java/org/apache/hadoop/hbase/io/Result.java
===================================================================
--- java/org/apache/hadoop/hbase/io/Result.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/Result.java	(revision 0)
@@ -0,0 +1,210 @@
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.SplitKeyValue;
+import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+public class Result implements Writable {
+  private byte [] row = null;
+  private KeyValue [] kvs = null;
+  private SortedMap<byte[],SortedMap<byte[],SortedMap<Long,byte[]>>> familyMap = null;
+
+  /**
+   * Constructor used for Writable.
+   */
+  public Result() {}
+  
+  /**
+   * Instantiate a Result with the specified array of KeyValues.
+   * @param kvs array of KeyValues
+   */
+  public Result(KeyValue [] kvs) {
+    this.row = kvs[0].getRow();
+    this.kvs = kvs;
+  }
+  
+  /**
+   * Instantiate a Result with the specified List of KeyValues.
+   * @param kvs List of KeyValues
+   */
+  public Result(List<KeyValue> kvs) {
+    this(kvs.toArray(new KeyValue[0]));
+  }
+  
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  /**
+   * Directly return the unsorted array of KeyValues in this Result.
+   * @return unsorted array of KeyValues
+   */
+  public KeyValue[] raw() {
+    return kvs;
+  }
+  
+  /**
+   * Returns a sorted array of KeyValues in this Result.
+   * <p>
+   * Note: Sorting is done in place, so the backing array will be sorted
+   * after calling this method.
+   * @return sorted array of KeyValues
+   */
+  public KeyValue[] sorted() {
+    Arrays.sort(kvs, (Comparator<KeyValue>)KeyValue.COMPARATOR);
+    return kvs;
+  }
+  
+  /**
+   * Map of families to all versions of its qualifiers and values.
+   * <p>
+   * Returns a three level Map of the form: <code>Map<family,Map<qualifier,Map<timestamp,value>>></code>
+   * <p>
+   * Note: All other map returning methods make use of this map internally. 
+   * @return map from families to qualifiers to versions
+   */
+  public SortedMap<byte[],SortedMap<byte[],SortedMap<Long,byte[]>>> getMap() {
+    if(this.familyMap != null) {
+      return this.familyMap;
+    }
+    this.familyMap =
+      new TreeMap<byte[],SortedMap<byte[],SortedMap<Long,byte[]>>>(Bytes.BYTES_COMPARATOR);
+    for(KeyValue kv : this.kvs) {
+      SplitKeyValue splitKV = kv.split();
+      byte [] family = splitKV.getFamily();
+      SortedMap<byte[],SortedMap<Long,byte[]>> columnMap = familyMap.get(family);
+      if(columnMap == null) {
+        columnMap = new TreeMap<byte[],SortedMap<Long,byte[]>>(Bytes.BYTES_COMPARATOR);
+        familyMap.put(family, columnMap);
+      }
+      byte [] qualifier = splitKV.getQualifier();
+      SortedMap<Long,byte[]> versionMap = columnMap.get(qualifier);
+      if(versionMap == null) {
+        versionMap = new TreeMap<Long,byte[]>(new Comparator<Long>() {
+          public int compare(Long l1, Long l2) {
+            return l2.compareTo(l1);
+          }
+        });
+        columnMap.put(qualifier, versionMap);
+      }
+      Long timestamp = Bytes.toLong(splitKV.getTimestamp());
+      byte [] value = splitKV.getValue();
+      versionMap.put(timestamp, value);
+    }
+    return this.familyMap;
+  }
+  
+  /**
+   * Map of families to their most recent qualifiers and values.
+   * <p>
+   * Returns a two level Map of the form: <code>Map<family,Map<qualifier,value>></code>
+   * <p>
+   * The most recent version of each qualifier will be used.
+   * @return map from families to qualifiers and value
+   */
+  public SortedMap<byte[],SortedMap<byte[],byte[]>> getNoVersionMap() {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    SortedMap<byte[],SortedMap<byte[],byte[]>> returnMap = 
+      new TreeMap<byte[],SortedMap<byte[],byte[]>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte[], SortedMap<byte[], SortedMap<Long,byte[]>>> familyEntry : 
+      familyMap.entrySet()) {
+      SortedMap<byte[],byte[]> qualifierMap = 
+        new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
+      for(Map.Entry<byte[],SortedMap<Long,byte[]>> qualifierEntry :
+        familyEntry.getValue().entrySet()) {
+        byte [] value = 
+          qualifierEntry.getValue().get(qualifierEntry.getValue().firstKey());
+        qualifierMap.put(qualifierEntry.getKey(), value);
+      }
+      returnMap.put(familyEntry.getKey(), qualifierMap);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Map of qualifiers to values.
+   * <p>
+   * Returns a Map of the form: <code>Map<qualifier,value></code>
+   * @return map of qualifiers to values
+   */
+  public SortedMap<byte[],byte[]> getFamilyMap(byte [] family) {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    SortedMap<byte[],byte[]> returnMap = 
+      new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
+    SortedMap<byte[],SortedMap<Long,byte[]>> qualifierMap = familyMap.get(family);
+    if(qualifierMap == null) {
+      return returnMap;
+    }
+    for(Map.Entry<byte[],SortedMap<Long,byte[]>> entry : 
+      qualifierMap.entrySet()) {
+      byte [] value = 
+        entry.getValue().get(entry.getValue().firstKey());
+      returnMap.put(entry.getKey(), value);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Returns a {@link RowResult}.
+   * @return a RowResult
+   */
+  public RowResult rowResult() {
+    return new RowResult();
+  //  return RowResult.createRowResult(kvs);
+  }
+  
+  /**
+   * Method that returns the value of the first column in the Result.
+   * @return value of the first column
+   */
+  public byte [] value() {
+    return kvs[0].getValue();
+  }
+  
+  public boolean isEmpty() {
+    if(this.kvs == null || this.kvs.length == 0) {
+      return true;
+    }
+    return false;
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    int length = in.readInt();
+    this.kvs = new KeyValue[length];
+    for(int i=0; i<length; i++) {
+      KeyValue kv = new KeyValue();
+      kv.readFields(in);
+      this.kvs[i] = kv;
+    }
+  }  
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    int len = this.kvs.length;
+    out.writeInt(len);
+    for(int i=0; i<len; i++) {
+      this.kvs[i].write(out);
+    }
+  }
+}
Index: java/org/apache/hadoop/hbase/io/Scan.java
===================================================================
--- java/org/apache/hadoop/hbase/io/Scan.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/Scan.java	(revision 0)
@@ -0,0 +1,250 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Scan operations.
+ * <p>
+ * All operations are identical to {@link Get} with the exception of
+ * instantiation.  Rather than specifying a single row, an optional startRow
+ * and stopRow may be defined.  If rows are not specified, the Scanner will
+ * iterate over all rows.
+ * <p>
+ * To scan everything for each row, instantiate a Scan object.
+ * To further define the scope of what to get when scanning, perform additional 
+ * methods as outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Scan implements Writable {
+  private byte [] startRow = HConstants.EMPTY_START_ROW;
+  private byte [] stopRow  = HConstants.EMPTY_END_ROW;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], Set<byte []>> familyMap =
+    new TreeMap<byte [], Set<byte []>>(Bytes.BYTES_COMPARATOR);
+  
+  /**
+   * Create a Scan operation across all rows.
+   */
+  public Scan() {}
+  
+  /**
+   * Create a Scan operation starting at the specified row.
+   * <p>
+   * If the specified row does not exist, the Scanner will start from the
+   * next closest row after the specified row.
+   * @param startRow row to start scanner at or after
+   */
+  public Scan(byte [] startRow) {
+    this.startRow = startRow;
+  }
+  
+  /**
+   * Create a Scan operation for the range of rows specified.
+   * @param startRow row to start scanner at or after (inclusive)
+   * @param stopRow row to stop scanner before (exclusive)
+   */
+  public Scan(byte [] startRow, byte [] stopRow) {
+    this.startRow = startRow;
+    this.stopRow = stopRow;
+  }
+  
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+  
+  /**
+   * Get the column from the specified family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    Set<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+  
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+  
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+  	this.maxVersions = Integer.MAX_VALUE;
+  }
+
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) {
+    this.maxVersions = maxVersions;
+  }
+  
+  /**
+   * Apply the specified server-side filter when performing the Scan.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+  
+  public void setFamilyMap(Map<byte [], Set<byte []>> familyMap) {
+    this.familyMap = familyMap;
+  }
+  
+  public Map<byte [], Set<byte []>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  public byte [] getStartRow() {
+    return this.startRow;
+  }
+
+  public byte [] getStopRow() {
+    return this.stopRow;
+  }
+  
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  public TimeRange getTimeRange() {
+    return this.tr;
+  } 
+  
+  public RowFilterInterface getFilter() {
+    return filter;
+  }
+  
+  
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.startRow = Bytes.readByteArray(in);
+    this.stopRow = Bytes.readByteArray(in);
+  	this.maxVersions = in.readInt();
+  	boolean hasFilter = in.readBoolean();
+  	if(hasFilter) {
+  	  this.filter = (RowFilterInterface)HbaseObjectWritable.readObject(in, null);
+  	}
+  	this.tr = new TimeRange();
+  	tr.readFields(in);
+  	int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [], Set<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      Set<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }  
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.startRow);
+    Bytes.writeByteArray(out, this.stopRow);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], Set<byte []>> entry : familyMap.entrySet()) {
+  	  Bytes.writeByteArray(out, entry.getKey());
+  	  Set<byte []> columnSet = entry.getValue();
+  	  out.writeInt(columnSet.size());
+  	  for(byte [] qualifier : columnSet) {
+  	    Bytes.writeByteArray(out, qualifier);
+  	  }
+  	}
+  }
+}
Index: java/org/apache/hadoop/hbase/io/TimeRange.java
===================================================================
--- java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 0)
+++ java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 0)
@@ -0,0 +1,197 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Represents an interval of version timestamps.
+ * <p>
+ * Evaluated according to minStamp <= timestamp < maxStamp
+ * or [minStamp,maxStamp) in interval notation.
+ * <p>
+ * Only used internally; should not be accessed directly by clients.
+ */
+public class TimeRange implements Writable {
+  private byte [] minStamp = null;
+  private byte [] maxStamp = null;
+  private boolean allTime = false;
+
+  /**
+   * Default constructor.
+   * Represents interval [0, Long.MAX_VALUE)
+   */
+  public TimeRange() {
+    // Doesn't use another constructor to prevent needing to throw IOException
+    this.minStamp = Bytes.toBytes(0L);
+    this.maxStamp = Bytes.toBytes(Long.MAX_VALUE);
+    this.allTime = true;
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(long minStamp) {
+    this(Bytes.toBytes(minStamp));
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(byte [] minStamp) {
+  	this.minStamp = minStamp;
+    this.maxStamp = Bytes.toBytes(Long.MAX_VALUE);
+  }
+  
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   */
+  public TimeRange(long minStamp, long maxStamp)
+  throws IOException {
+    this(Bytes.toBytes(minStamp), Bytes.toBytes(maxStamp));
+  }
+
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   */
+  public TimeRange(byte [] minStamp, byte [] maxStamp)
+  throws IOException {
+    int ret = Bytes.compareTo(maxStamp, minStamp);
+    if(ret <= -1) {
+      throw new IOException("maxStamp is smallar than minStamp");
+    }
+    this.minStamp = minStamp;
+    this.maxStamp = maxStamp;
+  }
+  
+  public byte [] getMin() {
+    return minStamp;
+  }
+
+  public byte [] getMax() {
+    return maxStamp;
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param bytes timestamp to check
+   * @param offset offset into the bytes
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(byte [] bytes, int offset) {
+  	if(allTime) return true;
+  	// check if >= minStamp
+    int ret = Bytes.compareTo(minStamp, 0, Bytes.SIZEOF_LONG, bytes,
+      offset, Bytes.SIZEOF_LONG);
+    if(ret == 0) {
+      return true;
+    } else if(ret >= 1) {
+      return false;
+    }
+    // check if < maxStamp
+    ret = Bytes.compareTo(maxStamp, 0, Bytes.SIZEOF_LONG, bytes,
+      offset, Bytes.SIZEOF_LONG);
+    if(ret >= 1) {
+      return true;
+    }
+    return false;
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(long timestamp) {
+  	if(allTime) return true;
+  	// check if >= minStamp
+    long min = Bytes.toLong(minStamp);
+    if(timestamp == min) {
+      return true;
+    } else if(timestamp < min) {
+      return false;
+    }
+    // check if < maxStamp
+    long max = Bytes.toLong(maxStamp);
+    if(timestamp < max) {
+      return true;
+    }
+    return false;
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinOrAfterTimeRange(long timestamp) {
+    if(allTime) return true;
+    // check if >= minStamp
+    long min = Bytes.toLong(minStamp);
+    if(timestamp < min) {
+      return false;
+    }
+    return true;
+  }
+  
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("maxStamp ");
+    sb.append(Bytes.toLong(maxStamp));
+    sb.append(", minStamp");
+    sb.append(Bytes.toLong(minStamp));
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.minStamp = Bytes.readByteArray(in);
+    this.maxStamp = Bytes.readByteArray(in);
+    this.allTime = in.readBoolean();
+  }
+  
+  public void write(final DataOutput out) throws IOException {
+    Bytes.writeByteArray(out, this.minStamp);
+    Bytes.writeByteArray(out, this.maxStamp);
+    out.writeBoolean(this.allTime);
+  }
+}
Index: java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
===================================================================
--- java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(working copy)
@@ -24,6 +24,8 @@
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
 
@@ -306,4 +308,17 @@
    */
   public long incrementColumnValue(byte [] regionName, byte [] row,
       byte [] column, long amount) throws IOException;
+  
+  
+  //
+  // HBASE-880
+  //
+  
+  /**
+   * Perform Get operation.
+   * 
+   * @param get Get operation
+   * @return Result
+   */
+  public Result get(byte [] regionName, Get get) throws IOException;
 }
Index: java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- java/org/apache/hadoop/hbase/KeyValue.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/KeyValue.java	(working copy)
@@ -19,13 +19,17 @@
  */
 package org.apache.hadoop.hbase;
 
+import java.io.DataInput;
+import java.io.DataOutput;
 import java.io.IOException;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Writable;
 
 /**
  * An HBase Key/Value.  Instances of this class are immutable.  They are not
@@ -45,10 +49,10 @@
  * Byte.MAX_SIZE, and column qualifier + key length must be < Integer.MAX_SIZE.
  * The column does not contain the family/qualifier delimiter.
  * 
- * <p>TODO: Group Key-only compartors and operations into a Key class, just
+ * <p>TODO: Group Key-only comparators and operations into a Key class, just
  * for neatness sake, if can figure what to call it.
  */
-public class KeyValue {
+public class KeyValue implements Writable, HeapSize {
   static final Log LOG = LogFactory.getLog(KeyValue.class);
 
   /**
@@ -116,24 +120,24 @@
     };
 
   // Size of the timestamp and type byte on end of a key -- a long + a byte.
-  private static final int TIMESTAMP_TYPE_SIZE =
+  public static final int TIMESTAMP_TYPE_SIZE =
     Bytes.SIZEOF_LONG /* timestamp */ +
     Bytes.SIZEOF_BYTE /*keytype*/;
 
   // Size of the length shorts and bytes in key.
-  private static final int KEY_INFRASTRUCTURE_SIZE =
+  public static final int KEY_INFRASTRUCTURE_SIZE =
     Bytes.SIZEOF_SHORT /*rowlength*/ +
     Bytes.SIZEOF_BYTE /*columnfamilylength*/ +
     TIMESTAMP_TYPE_SIZE;
 
   // How far into the key the row starts at. First thing to read is the short
   // that says how long the row is.
-  private static final int ROW_OFFSET =
+  public static final int ROW_OFFSET =
     Bytes.SIZEOF_INT /*keylength*/ +
     Bytes.SIZEOF_INT /*valuelength*/;
 
   // Size of the length ints in a KeyValue datastructure.
-  private static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
+  public static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
 
   /**
    * Key type.
@@ -190,10 +194,21 @@
   public static final KeyValue LOWESTKEY = 
     new KeyValue(HConstants.EMPTY_BYTE_ARRAY, HConstants.LATEST_TIMESTAMP);
   
-  private final byte [] bytes;
-  private final int offset;
-  private final int length;
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue fields and constructors 
+  //
+  //---------------------------------------------------------------------------
+  
+  private byte [] bytes;
+  private int offset;
+  private int length;
 
+  /** Writable Constructor */
+  public KeyValue() {}
+  
+  /** Constructors using existing KeyValue formatted byte arrays */
+  
   /**
    * Creates a KeyValue from the start of the specified byte array.
    * Presumes <code>bytes</code> content is formatted as a KeyValue blob.
@@ -226,194 +241,199 @@
     this.offset = offset;
     this.length = length;
   }
+
+  /** Temporary constructors until 880/1249 is committed to remove deps */
   
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param timestamp
+   * Temporary.
    */
-  public KeyValue(final String row, final long timestamp) {
-    this(Bytes.toBytes(row), timestamp);
+  public KeyValue(final byte [] row, final byte [] column) {
+    this(row, column, HConstants.LATEST_TIMESTAMP, null);
   }
-
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts) {
+    this(row, column, ts, null);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts,
+      byte [] value) {
+    this(row, column, ts, Type.Put, value);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts, Type type,
+      byte [] value) {
+    int rlength = row == null ? 0 : row.length;
+    int vlength = value == null ? 0 : value.length;
+    int clength = column == null ? 0 : column.length;
+    this.bytes = createByteArray(row, 0, rlength, column, 0, clength,
+        ts, type, value, 0, vlength);
+    this.length = this.bytes.length;
+    this.offset = 0;
+  }
+  
+  /** Constructors that build a new backing byte array from fields */
+  
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
    * @param timestamp
    */
   public KeyValue(final byte [] row, final long timestamp) {
-    this(row, null, timestamp, Type.Put, null);
+    this(row, timestamp, Type.Put);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param timestamp
    */
-  public KeyValue(final String row, final String column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final long timestamp, Type type) {
+    this(row, null, null, timestamp, type, null);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final byte [] row, final byte [] column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put);
   }
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final String row, final String column, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), value);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column, final byte [] value) {
-    this(row, column, HConstants.LATEST_TIMESTAMP, value);
-  }
-
-
-  /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final String row, final String column, final long ts) {
-    this(row, column, ts, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier, final byte [] value) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put, value);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column, final long ts) {
-    this(row, column, ts, Type.Put);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type) {
+    this(row, family, qualifier, timestamp, type, null);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-    final long timestamp, final byte [] value) {
-    this(Bytes.toBytes(row),
-      column == null? HConstants.EMPTY_BYTE_ARRAY: Bytes.toBytes(column),
-      timestamp, value);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, final byte[] value) {
+    this(row, family, qualifier, timestamp, Type.Put, value);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column,
-     final long timestamp, final byte [] value) {
-    this(row, column, timestamp, Type.Put, value);
-  }
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type,
+      final byte[] value) {
+    this(row, family, qualifier, 0, qualifier==null ? 0 : qualifier.length, 
+        timestamp, type, value, 0, value==null ? 0 : value.length);
+  } 
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-     final long timestamp, final Type type, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), timestamp, type,
-      value);
+  public KeyValue(byte [] row, byte [] family, 
+      byte [] qualifier, int qoffset, int qlength, long timestamp, Type type, 
+      byte [] value, int voffset, int vlength) {
+    this(row, 0, row==null ? 0 : row.length, 
+        family, 0, family==null ? 0 : family.length,
+        qualifier, qoffset, qlength, timestamp, type, 
+        value, voffset, vlength);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, null, 0, -1);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type, final byte [] value) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, value, 0, value == null? 0: value.length);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column Column with delimiter between family and qualifier
-   * @param coffset Where to start reading the column.
-   * @param clength How long column is (including the family/qualifier delimiter.
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * Constructs KeyValue structure filled with specified values.
+   * <p>
+   * Column is split into two fields, family and qualifier.
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @throws IllegalArgumentException
    */
   public KeyValue(final byte [] row, final int roffset, final int rlength,
-      final byte [] column, final int coffset, int clength,
+      final byte [] family, final int foffset, final int flength,
+      final byte [] qualifier, final int qoffset, final int qlength,
       final long timestamp, final Type type,
-      final byte [] value, final int voffset, int vlength) {
-    this.bytes = createByteArray(row, roffset, rlength, column, coffset,
-      clength, timestamp, type, value, voffset, vlength);
+      final byte [] value, final int voffset, final int vlength) {
+    this.bytes = createByteArray(row, roffset, rlength, 
+        family, foffset, flength, qualifier, qoffset, qlength,
+        timestamp, type, value, voffset, vlength);
     this.length = bytes.length;
     this.offset = 0;
   }
 
   /**
    * Write KeyValue format into a byte array.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column
-   * @param coffset
-   * @param clength
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @return
    */
-  static byte [] createByteArray(final byte [] row, final int roffset,
-        final int rlength,
-      final byte [] column, final int coffset, int clength,
+  static byte [] createByteArray(final byte [] row, final int roffset, final int rlength,
+      final byte [] family, final int foffset, int flength,
+      final byte [] qualifier, final int qoffset, int qlength,
       final long timestamp, final Type type,
       final byte [] value, final int voffset, int vlength) {
     if (rlength > Short.MAX_VALUE) {
@@ -422,19 +442,14 @@
     if (row == null) {
       throw new IllegalArgumentException("Row is null");
     }
-    // If column is non-null, figure where the delimiter is at.
-    int delimiteroffset = 0;
-    if (column != null && column.length > 0) {
-      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
-      if (delimiteroffset > Byte.MAX_VALUE) {
-        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
-      }
-    }
     // Value length
-    vlength = value == null? 0: vlength;
-    // Column length - minus delimiter
-    clength = column == null || column.length == 0? 0: clength - 1;
-    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + clength;
+    vlength = value == null? 0 : vlength;
+    // Family length
+    flength = family == null ? 0 : flength;
+    // Qualifier length
+    qlength = qualifier == null ? 0 : qlength;
+    // Key length
+    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + flength + qlength;
     if (longkeylength > Integer.MAX_VALUE) {
       throw new IllegalArgumentException("keylength " + longkeylength + " > " +
         Integer.MAX_VALUE);
@@ -448,16 +463,13 @@
     pos = Bytes.putInt(bytes, pos, vlength);
     pos = Bytes.putShort(bytes, pos, (short)(rlength & 0x0000ffff));
     pos = Bytes.putBytes(bytes, pos, row, roffset, rlength);
-    // Write out column family length.
-    pos = Bytes.putByte(bytes, pos, (byte)(delimiteroffset & 0x0000ff));
-    if (column != null && column.length != 0) {
-      // Write family.
-      pos = Bytes.putBytes(bytes, pos, column, coffset, delimiteroffset);
-      // Write qualifier.
-      delimiteroffset++;
-      pos = Bytes.putBytes(bytes, pos, column, coffset + delimiteroffset,
-        column.length - delimiteroffset);
+    pos = Bytes.putByte(bytes, pos, (byte)(flength & 0x0000ff));
+    if(flength != 0) {
+      pos = Bytes.putBytes(bytes, pos, family, foffset, flength);
     }
+    if(qlength != 0) {
+      pos = Bytes.putBytes(bytes, pos, qualifier, qoffset, qlength);
+    }
     pos = Bytes.putLong(bytes, pos, timestamp);
     pos = Bytes.putByte(bytes, pos, type.getCode());
     if (value != null && value.length > 0) {
@@ -465,6 +477,46 @@
     }
     return bytes;
   }
+  
+  /**
+   * Write KeyValue format into a byte array.
+   * <p>
+   * Takes column in the form <code>family:qualifier</code>
+   * @param row - row key (arbitrary byte array)
+   * @param roffset
+   * @param rlength
+   * @param column
+   * @param coffset
+   * @param clength
+   * @param timestamp
+   * @param type
+   * @param value
+   * @param voffset
+   * @param vlength
+   * @return
+   */
+  static byte [] createByteArray(final byte [] row, final int roffset,
+        final int rlength,
+      final byte [] column, final int coffset, int clength,
+      final long timestamp, final Type type,
+      final byte [] value, final int voffset, int vlength) {
+    // If column is non-null, figure where the delimiter is at.
+    int delimiteroffset = 0;
+    if (column != null && column.length > 0) {
+      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
+      if (delimiteroffset > Byte.MAX_VALUE) {
+        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
+      }
+    } else {
+      return createByteArray(row,roffset,rlength,null,0,0,null,0,0,timestamp,
+          type,value,voffset,vlength);
+    }
+    int flength = delimiteroffset-coffset;
+    int qlength = clength - flength - 1;
+    return createByteArray(row, roffset, rlength, column, coffset,
+        flength, column, delimiteroffset+1, qlength, timestamp, type,
+        value, voffset, vlength);
+  }
 
   // Needed doing 'contains' on List.  Only compares the key portion, not the
   // value.
@@ -478,6 +530,12 @@
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue cloning
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @param timestamp
    * @return Clone of bb's key portion with only the row and timestamp filled in.
@@ -485,9 +543,10 @@
    */
   public KeyValue cloneRow(final long timestamp) {
     return new KeyValue(getBuffer(), getRowOffset(), getRowLength(),
-      null, 0, 0, timestamp, Type.codeToType(getType()), null, 0, 0);
+        null, 0, 0, null, 0, 0, 
+        timestamp, Type.codeToType(getType()), null, 0, 0);
   }
-
+  
   /**
    * @return Clone of bb's key portion with type set to Type.Delete.
    * @throws IOException
@@ -524,6 +583,12 @@
     return new KeyValue(other, 0, other.length);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  String representation
+  //
+  //---------------------------------------------------------------------------
+  
   public String toString() {
     return keyToString(this.bytes, this.offset + ROW_OFFSET, getKeyLength()) +
       "/vlen=" + getValueLength();
@@ -561,6 +626,12 @@
       qualifier + "/" + timestamp + "/" + Type.codeToType(type);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  Public Member Accessors
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @return The byte array backing this KeyValue.
    */
@@ -582,7 +653,13 @@
     return length;
   }
 
-  /*
+  //---------------------------------------------------------------------------
+  //
+  //  Length and Offset Calculators
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
    * Determines the total length of the KeyValue stored in the specified
    * byte array and offset.  Includes all headers.
    * @param bytes byte array
@@ -596,41 +673,167 @@
   }
 
   /**
-   * @return Copy of the key portion only.  Used compacting and testing.
+   * @return Key offset in backing buffer..
    */
-  public byte [] getKey() {
-    int keylength = getKeyLength();
-    byte [] key = new byte[keylength];
-    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
-    return key;
+  public int getKeyOffset() {
+    return this.offset + ROW_OFFSET;
   }
 
-  public String getKeyString() {
-    return Bytes.toString(getBuffer(), getKeyOffset(), getKeyLength());
+  /**
+   * @return Length of key portion.
+   */
+  public int getKeyLength() {
+    return Bytes.toInt(this.bytes, this.offset);
   }
 
   /**
-   * @return Key offset in backing buffer..
+   * @return Value offset
    */
-  public int getKeyOffset() {
-    return this.offset + ROW_OFFSET;
+  public int getValueOffset() {
+    return getKeyOffset() + getKeyLength();
   }
+  
+  /**
+   * @return Value length
+   */
+  public int getValueLength() {
+    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  }
 
   /**
-   * @return Row length.
+   * @return Row offset
    */
+  public int getRowOffset() {
+    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  }
+  
+  /**
+   * @return Row length
+   */
   public short getRowLength() {
     return Bytes.toShort(this.bytes, getKeyOffset());
   }
 
   /**
-   * @return Offset into backing buffer at which row starts.
+   * @return Family offset
    */
-  public int getRowOffset() {
-    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  public int getFamilyOffset() {
+    return getFamilyOffset(getRowLength());
   }
+  
+  /**
+   * @return Family offset
+   */
+  public int getFamilyOffset(int rlength) {
+    return ROW_OFFSET + Bytes.SIZEOF_SHORT + rlength + Bytes.SIZEOF_BYTE;
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength() {
+    return getFamilyLength(getFamilyOffset());
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength(int foffset) {
+    return this.bytes[foffset-1];
+  }
 
   /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset() {
+    return getQualifierOffset(getFamilyOffset());
+  }
+  
+  /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset(int foffset) {
+    return foffset + getFamilyLength(foffset);
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength() {
+    return getQualifierLength(getRowLength(),getFamilyLength());
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength(int rlength, int flength) {
+    return getKeyLength() - 
+      (KEY_INFRASTRUCTURE_SIZE + rlength + flength);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength() {
+    int rlength = getRowLength();
+    int foffset = getFamilyOffset(rlength);
+    return getTotalColumnLength(rlength,foffset);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength(int rlength, int foffset) {
+    int flength = getFamilyLength(foffset);
+    int qlength = getQualifierLength(rlength,flength);
+    return flength + qlength;
+  }
+  
+  /**
+   * @return Timestamp offset
+   */
+  int getTimestampOffset() {
+    return getTimestampOffset(getKeyLength());
+  }
+  
+  /**
+   * @param keylength Pass if you have it to save on a int creation.
+   * @return Timestamp offset
+   */
+  int getTimestampOffset(final int keylength) {
+    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
+  }
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Methods that return copies of fields
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
+   * @return Copy of the key portion only.  Used compacting and testing.
+   */
+  public byte [] getKey() {
+    int keylength = getKeyLength();
+    byte [] key = new byte[keylength];
+    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
+    return key;
+  }
+  
+  /**
+   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
+   * offset and lengths instead.
+   * @return Value in a new byte array.
+   */
+  public byte [] getValue() {
+    int o = getValueOffset();
+    int l = getValueLength();
+    byte [] result = new byte[l];
+    System.arraycopy(getBuffer(), o, result, 0, l);
+    return result;
+  }
+  
+  /**
    * Do not use this unless you have to.
    * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
    * @return Row in a new byte array.
@@ -660,14 +863,6 @@
   }
 
   /**
-   * @param keylength Pass if you have it to save on a int creation.
-   * @return Offset into backing buffer at which timestamp starts.
-   */
-  int getTimestampOffset(final int keylength) {
-    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
-  }
-
-  /**
    * @return True if a {@link Type#Delete}.
    */
   public boolean isDeleteType() {
@@ -690,99 +885,156 @@
   }
 
   /**
-   * @return Length of key portion.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns column. Makes a copy.  Inserts delimiter.
    */
-  public int getKeyLength() {
-    return Bytes.toInt(this.bytes, this.offset);
+  public byte [] getColumn() {
+    int fo = getFamilyOffset();
+    int fl = getFamilyLength(fo);
+    int ql = getQualifierLength();
+    byte [] result = new byte[fl + 1 + ql];
+    System.arraycopy(this.bytes, fo, result, 0, fl);
+    result[fl] = COLUMN_FAMILY_DELIMITER;
+    System.arraycopy(this.bytes, fo + fl, result,
+      fl + 1, ql);
+    return result;
   }
 
   /**
-   * @return Value length
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns family. Makes a copy.
    */
-  public int getValueLength() {
-    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  public byte [] getFamily() {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    byte [] result = new byte[l];
+    System.arraycopy(this.bytes, o, result, 0, l);
+    return result;
   }
 
   /**
-   * @return Offset into backing buffer at which value starts.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns qualifier. Makes a copy.
    */
-  public int getValueOffset() {
-    return getKeyOffset() + getKeyLength();
-  }
-
-  /**
-   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
-   * offset and lengths instead.
-   * @return Value in a new byte array.
-   */
-  public byte [] getValue() {
-    int o = getValueOffset();
-    int l = getValueLength();
+  public byte [] getQualifier() {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
     byte [] result = new byte[l];
-    System.arraycopy(getBuffer(), o, result, 0, l);
+    System.arraycopy(this.bytes, o, result, 0, l);
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue splitter
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @return Offset into backing buffer at which the column begins
+   * Utility class that splits a KeyValue buffer into separate byte arrays.
+   * <p>
+   * Should get rid of this if we can, but is very useful for debugging.
    */
-  public int getColumnOffset() {
-    return getColumnOffset(getRowLength());
+  public static class SplitKeyValue {
+    private byte [][] split;
+    SplitKeyValue() {
+      this.split = new byte[6][];
+    }
+    public void setRow(byte [] value) { this.split[0] = value; }
+    public void setFamily(byte [] value) { this.split[1] = value; }
+    public void setQualifier(byte [] value) { this.split[2] = value; }
+    public void setTimestamp(byte [] value) { this.split[3] = value; }
+    public void setType(byte [] value) { this.split[4] = value; }
+    public void setValue(byte [] value) { this.split[5] = value; }
+    public byte [] getRow() { return this.split[0]; }
+    public byte [] getFamily() { return this.split[1]; }
+    public byte [] getQualifier() { return this.split[2]; }
+    public byte [] getTimestamp() { return this.split[3]; }
+    public byte [] getType() { return this.split[4]; }
+    public byte [] getValue() { return this.split[5]; }
   }
-
-  /**
-   * @param rowlength - length of row.
-   * @return Offset into backing buffer at which the column begins
-   */
-  public int getColumnOffset(final int rowlength) {
-    return getRowOffset() + rowlength + 1;
+  
+  public SplitKeyValue split() {
+    SplitKeyValue split = new SplitKeyValue();
+    int splitOffset = this.offset;
+    int keyLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    int valLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    short rowLen = Bytes.toShort(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_SHORT;
+    byte [] row = new byte[rowLen];
+    System.arraycopy(bytes, splitOffset, row, 0, rowLen);
+    splitOffset += rowLen;
+    split.setRow(row);
+    byte famLen = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    byte [] family = new byte[famLen];
+    System.arraycopy(bytes, splitOffset, family, 0, famLen);
+    splitOffset += famLen;
+    split.setFamily(family);
+    int colLen = keyLen -
+      (rowLen + famLen + Bytes.SIZEOF_SHORT + Bytes.SIZEOF_BYTE +
+      Bytes.SIZEOF_LONG + Bytes.SIZEOF_BYTE);
+    byte [] qualifier = new byte[colLen];
+    System.arraycopy(bytes, splitOffset, qualifier, 0, colLen);
+    splitOffset += colLen;
+    split.setQualifier(qualifier);
+    byte [] timestamp = new byte[Bytes.SIZEOF_LONG];
+    System.arraycopy(bytes, splitOffset, timestamp, 0, Bytes.SIZEOF_LONG);
+    splitOffset += Bytes.SIZEOF_LONG;
+    split.setTimestamp(timestamp);
+    byte [] type = new byte[1];
+    type[0] = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    split.setType(type);
+    byte [] value = new byte[valLen];
+    System.arraycopy(bytes, splitOffset, value, 0, valLen);
+    split.setValue(value);
+    return split;
   }
-
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Compare specified fields against those contained in this KeyValue 
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of family portion of column.
+   * @param family
+   * @return True if matching families.
    */
-  int getFamilyLength(final int columnoffset) {
-    return this.bytes[columnoffset - 1];
+  public boolean matchingFamily(final byte [] family) {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
   }
 
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of column.
+   * @param qualifier
+   * @return True if matching qualifiers.
    */
-  public int getColumnLength(final int columnoffset) {
-    return getColumnLength(columnoffset, getKeyLength());
+  public boolean matchingQualifier(final byte [] qualifier) {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
+    return Bytes.compareTo(qualifier, 0, qualifier.length, 
+        this.bytes, o, l) == 0;
   }
 
-  int getColumnLength(final int columnoffset, final int keylength) {
-    return (keylength + ROW_OFFSET) - (columnoffset - this.offset) -
-      TIMESTAMP_TYPE_SIZE;
-  }
-
   /**
-   * @param family
-   * @return True if matching families.
-   */
-  public boolean matchingFamily(final byte [] family) {
-    int o = getColumnOffset();
-    // Family length byte is just before the column starts.
-    int l = this.bytes[o - 1];
-    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
-  }
-
-  /**
    * @param column Column minus its delimiter
-   * @param familylength Length of family in passed <code>column</code>
    * @return True if column matches.
    * @see #matchingColumn(byte[])
    */
-  public boolean matchingColumnNoDelimiter(final byte [] column,
-      final int familylength) {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int f = getFamilyLength(o);
-    return compareColumns(getBuffer(), o, l, f,
-      column, 0, column.length, familylength) == 0;
+  public boolean matchingColumnNoDelimiter(final byte [] column) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int l = fl + getQualifierLength(rl,fl);
+    return Bytes.compareTo(column, 0, column.length, this.bytes, o, l) == 0;
   }
 
   /**
@@ -791,17 +1043,41 @@
    */
   public boolean matchingColumn(final byte [] column) {
     int index = getFamilyDelimiterIndex(column, 0, column.length);
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int result = Bytes.compareTo(getBuffer(), o, index, column, 0, index);
-    if (result != 0) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(column, 0, index, this.bytes, o, fl) != 0) {
       return false;
     }
-    return Bytes.compareTo(getBuffer(), o + index, l - index,
-      column, index + 1, column.length - (index + 1)) == 0;
+    return Bytes.compareTo(column, index + 1, column.length - (index + 1),
+        this.bytes, o + fl, ql) == 0;
   }
 
   /**
+   * @param column Column with delimiter
+   * @return True if column matches.
+   */
+  public boolean matchingColumn(final byte[] family, final byte[] qualifier) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(family, 0, family.length, this.bytes, o, family.length)
+        != 0) {
+      return false;
+    }
+    if(qualifier == null || qualifier.length == 0) {
+      if(ql == 0) {
+        return true;
+      }
+      return false;
+    }
+    return Bytes.compareTo(qualifier, 0, qualifier.length,
+        this.bytes, o + fl, ql) == 0;
+  }
+
+  /**
    * @param left
    * @param loffset
    * @param llength
@@ -836,42 +1112,35 @@
   }
 
   /**
-   * @return Returns column String with delimiter added back. Expensive!
+   * @return True if column is empty.
    */
-  public String getColumnString() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    return Bytes.toString(this.bytes, o, familylength) +
-      COLUMN_FAMILY_DELIMITER + Bytes.toString(this.bytes,
-       o + familylength, l - familylength);
+  public boolean isEmptyColumn() {
+    return getQualifierLength() == 0;
   }
 
   /**
-   * Do not use this unless you have to.
-   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
-   * @return Returns column. Makes a copy.  Inserts delimiter.
+   * Splits a column in family:qualifier form into separate byte arrays.
+   * <p>
+   * Catches 
+   * @param column
+   * @return
    */
-  public byte [] getColumn() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    byte [] result = new byte[l + 1];
-    System.arraycopy(getBuffer(), o, result, 0, familylength);
-    result[familylength] = COLUMN_FAMILY_DELIMITER;
-    System.arraycopy(getBuffer(), o + familylength, result,
-      familylength + 1, l - familylength);
+  public static byte [][] parseColumn(byte [] c) {
+    final byte [][] result = new byte [2][];
+    final int index = getFamilyDelimiterIndex(c, 0, c.length);
+    if (index == -1) {
+      throw new IllegalArgumentException("Impossible column name: " + c);
+    }
+    result[0] = new byte [index];
+    System.arraycopy(c, 0, result[0], 0, index);
+    final int len = c.length - (index + 1);
+    result[1] = new byte[len];
+    System.arraycopy(c, index + 1 /*Skip delimiter*/, result[1], 0,
+      len);
     return result;
   }
-
+  
   /**
-   * @return True if column is empty.
-   */
-  public boolean isEmptyColumn() {
-    return getColumnLength(getColumnOffset()) == 0;
-  }
-
-  /**
    * @param b
    * @return Index of the family-qualifier colon delimiter character in passed
    * buffer.
@@ -1019,7 +1288,8 @@
      * @return Result comparing rows.
      */
     public int compareRows(final KeyValue left, final KeyValue right) {
-      return compareRows(left, left.getRowLength(), right, right.getRowLength());
+      return compareRows(left, left.getRowLength(), right, 
+          right.getRowLength());
     }
 
     /**
@@ -1051,11 +1321,11 @@
       return getRawComparator().compareRows(left, loffset, llength,
         right, roffset, rlength);
     }
-
+    
     public int compareColumns(final KeyValue left, final byte [] right,
         final int roffset, final int rlength, final int rfamilyoffset) {
-      int offset = left.getColumnOffset();
-      int length = left.getColumnLength(offset);
+      int offset = left.getFamilyOffset();
+      int length = left.getFamilyLength() + left.getQualifierLength();
       return getRawComparator().compareColumns(left.getBuffer(), offset, length,
         left.getFamilyLength(offset),
         right, roffset, rlength, rfamilyoffset);
@@ -1064,15 +1334,15 @@
     int compareColumns(final KeyValue left, final short lrowlength,
         final int lkeylength, final KeyValue right, final short rrowlength,
         final int rkeylength) {
-      int loffset = left.getColumnOffset(lrowlength);
-      int roffset = right.getColumnOffset(rrowlength);
-      int llength = left.getColumnLength(loffset, lkeylength);
-      int rlength = right.getColumnLength(roffset, rkeylength);
-      int lfamilylength = left.getFamilyLength(loffset);
-      int rfamilylength = right.getFamilyLength(roffset);
-      return getRawComparator().compareColumns(left.getBuffer(), loffset,
-          llength, lfamilylength,
-        right.getBuffer(), roffset, rlength, rfamilylength);
+      int lfoffset = left.getFamilyOffset(lrowlength);
+      int rfoffset = right.getFamilyOffset(rrowlength);
+      int lclength = left.getTotalColumnLength(lrowlength,lfoffset);
+      int rclength = right.getTotalColumnLength(rrowlength, rfoffset);
+      int lfamilylength = left.getFamilyLength(lfoffset);
+      int rfamilylength = right.getFamilyLength(rfoffset);
+      return getRawComparator().compareColumns(left.getBuffer(), lfoffset,
+          lclength, lfamilylength,
+        right.getBuffer(), rfoffset, rclength, rfamilylength);
     }
 
     /**
@@ -1133,7 +1403,8 @@
     public boolean matchingRows(final byte [] left, final int loffset,
         final int llength,
         final byte [] right, final int roffset, final int rlength) {
-      int compare = compareRows(left, loffset, llength, right, roffset, rlength);
+      int compare = compareRows(left, loffset, llength, 
+          right, roffset, rlength);
       if (compare != 0) {
         return false;
       }
@@ -1207,20 +1478,44 @@
    */
   public static KeyValue createFirstOnRow(final byte [] row,
       final long ts) {
-    return createFirstOnRow(row, null, ts);
+    return new KeyValue(row, null, null, ts, Type.Maximum);
   }
 
   /**
    * @param row - row key (arbitrary byte array)
    * @param ts - timestamp
-   * @return First possible key on passed <code>row</code>, column and timestamp.
+   * @return First possible key on passed <code>row</code>, column and timestamp
    */
   public static KeyValue createFirstOnRow(final byte [] row, final byte [] c,
       final long ts) {
-    return new KeyValue(row, c, ts, Type.Maximum);
+    byte [][] split = parseColumn(c);
+    return new KeyValue(row, split[0], split[1], ts, Type.Maximum);
   }
 
   /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @return First possible key on passed <code>row</code>, and column.
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q) {
+    return new KeyValue(row, f, q, HConstants.LATEST_TIMESTAMP, Type.Maximum);
+  }
+
+  /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @param ts - timestamp
+   * @return First possible key on passed <code>row</code>, column and timestamp
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q, final long ts) {
+    return new KeyValue(row, f, q, ts, Type.Maximum);
+  }
+  
+  /**
    * @param b
    * @param o
    * @param l
@@ -1248,7 +1543,8 @@
       //          "---" + Bytes.toString(right, roffset, rlength));
       final int metalength = 7; // '.META.' length
       int lmetaOffsetPlusDelimiter = loffset + metalength;
-      int leftFarDelimiter = getDelimiterInReverse(left, lmetaOffsetPlusDelimiter,
+      int leftFarDelimiter = getDelimiterInReverse(left, 
+          lmetaOffsetPlusDelimiter,
           llength - metalength, HRegionInfo.DELIMITER);
       int rmetaOffsetPlusDelimiter = roffset + metalength;
       int rightFarDelimiter = getDelimiterInReverse(right,
@@ -1395,7 +1691,7 @@
       return compare(left, 0, left.length, right, 0, right.length);
     }
 
-    protected int compareRows(byte [] left, int loffset, int llength,
+    public int compareRows(byte [] left, int loffset, int llength,
         byte [] right, int roffset, int rlength) {
       return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);
     }
@@ -1420,4 +1716,22 @@
       return 0;
     }
   }
+  
+  // HeapSize
+  public long heapSize() {
+    return this.length;
+  }
+  
+  // Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.length = in.readInt();
+    this.offset = 0;
+    this.bytes = new byte[this.length];
+    in.readFully(this.bytes, 0, this.length);
+  }
+  
+  public void write(final DataOutput out) throws IOException {
+    out.writeInt(this.length);
+    out.write(this.bytes, this.offset, this.length);
+  }
 }
\ No newline at end of file
Index: java/org/apache/hadoop/hbase/regionserver/ColumnCount.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 0)
@@ -0,0 +1,60 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+/**
+ * Simple wrapper for a byte buffer and a counter.
+ */
+public class ColumnCount {
+  public byte [] bytes;
+  public int offset;
+  public int length;
+  public int count;
+  
+  public ColumnCount(byte [] column) {
+    this(column, 0);
+  }
+  
+  public ColumnCount(byte [] column, int count) {
+    this(column, 0, column.length, count);
+  }
+  
+  public ColumnCount(byte [] column, int offset, int length, int count) {
+    this.bytes = column;
+    this.offset = offset;
+    this.length = length;
+    this.count = count;
+  }
+  
+  public int decrement() {
+    return --count;
+  }
+  
+  public int increment() {
+    return ++count;
+  }
+  
+  public boolean needMore(int max) {
+    if(this.count < max) {
+      return true;
+    }
+    return false;
+  }
+}
Index: java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 0)
@@ -0,0 +1,27 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+
+public interface ColumnTracker {
+  public MatchCode checkColumn(byte [] bytes, int offset, int length);
+  public void update();
+}
Index: java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 0)
@@ -0,0 +1,378 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.Type;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class DeleteTracker {
+  
+  private long familyStamp = 0L;
+  private List<KeyValue> deletes;
+  private List<KeyValue> newDeletes;
+  private Iterator<KeyValue> iterator;
+  private KeyValue delete;
+  
+  public DeleteTracker() {
+    this.deletes = null;
+    this.delete = null;
+    this.newDeletes = new ArrayList<KeyValue>();
+  }
+  
+  public void add(KeyValue kv) {
+    this.newDeletes.add(kv);
+  }
+  
+  public boolean isDeleted(byte [] bytes, int offset, int rowLength, 
+      int familyLength, int columnLength, int columnOffset, 
+      long timestamp, byte type) {
+    
+    // Check against DeleteFamily
+    if(timestamp <= familyStamp) {
+      return true;
+    }
+    
+    // Check if there are other deletes
+    if(this.delete == null) {
+      return false;
+    }
+
+    // Deconstruct the delete to compare input KeyValue to
+    byte [] dbytes = delete.getBuffer();
+    int doffset = delete.getOffset();
+    
+    int dkeyLength = Bytes.toInt(dbytes,doffset);
+    doffset += Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+    
+    short drowLength = Bytes.toShort(dbytes, doffset);
+    doffset += Bytes.SIZEOF_SHORT + drowLength;
+    
+    byte dfamilyLength = dbytes[doffset];
+    doffset += dfamilyLength;
+    
+    int dcolumnLength = dkeyLength - (doffset - delete.getOffset()) -
+      KeyValue.TIMESTAMP_TYPE_SIZE;
+    int dcolumnOffset = doffset;
+    doffset += dcolumnLength;
+    
+    // Check column
+    int ret = Bytes.compareTo(bytes, columnOffset, columnLength, dbytes, 
+        dcolumnOffset, dcolumnLength);
+    if(ret <= -1) {
+      // Have not reached the next delete yet
+      return false;
+    } else if(ret >= 1) {
+      // Deletes an earlier column, need to move down deletes
+      if(this.iterator.hasNext()) {
+        this.delete = this.iterator.next();
+      } else {
+        this.delete = null;
+        return false;
+      }
+      return isDeleted(bytes, offset, rowLength, familyLength, columnLength,
+          columnOffset, timestamp, type);
+    }
+    
+    // Check Timestamp
+    long dtimestamp = Bytes.toLong(dbytes, doffset);
+    doffset += Bytes.SIZEOF_LONG;
+    if(timestamp > dtimestamp) {
+      return false;
+    }
+    
+    // Check Type
+    Type dtype = Type.codeToType(dbytes[doffset]);
+    switch(dtype) {
+      case Delete:
+        if(this.iterator.hasNext()) {
+          this.delete = this.iterator.next();
+        } else {
+          this.delete = null;
+        }
+        if(timestamp == dtimestamp) {
+          // This is a delete of this specific version
+          return true;
+        } else { // timestamp < dtimestamp
+          // Delete of an explicit column newer than current
+          return false;
+        }
+      case DeleteColumn:
+        return true;
+    }
+    // should never reach this
+    return false;
+  }
+  
+  public boolean isEmpty() {
+    if(this.familyStamp == 0L && this.delete == null) {
+      return true;
+    }
+    return false;
+  }
+  
+  /**
+   * Merge existing deletes with new deletes.
+   */
+  public void update() {
+    // If no previous deletes, use new deletes and return
+    if(this.deletes == null || this.deletes.size() == 0) {
+      this.deletes = this.newDeletes;
+      this.newDeletes = new ArrayList<KeyValue>();
+      return;
+    }
+    
+    // If no new delete, retain previous deletes and return
+    if(this.newDeletes.size() == 0) {
+      return;
+    }
+    
+    // Merge previous deletes with new deletes
+    List<KeyValue> mergeDeletes = 
+      new ArrayList<KeyValue>(this.newDeletes.size());
+    int oldIndex = 0;
+    int newIndex = 0;
+    
+    // Check if first new delete is a DeleteFamily
+    KeyValue newKV = this.newDeletes.get(0);
+    byte [] nbytes = newKV.getBuffer();
+    int noffset = newKV.getOffset();
+    int nkeyLength = Bytes.toInt(nbytes, noffset);
+    int ntypeOffset = noffset + KeyValue.ROW_OFFSET + nkeyLength - 
+      Bytes.SIZEOF_BYTE;
+    Type ntype = KeyValue.Type.codeToType(nbytes[ntypeOffset]);
+    if(ntype == Type.DeleteFamily) {
+      int ntimestampOffset = ntypeOffset - Bytes.SIZEOF_LONG;
+      long ntimestamp = Bytes.toLong(nbytes, ntimestampOffset);
+      if(ntimestamp > this.familyStamp) {
+        this.familyStamp = ntimestamp;
+      }
+      if(this.newDeletes.size() == 1) {
+        // Just one delete family
+        this.newDeletes = new ArrayList<KeyValue>();
+        return;
+      }
+    }
+    
+    KeyValue oldKV = deletes.get(oldIndex);
+    while(true) {
+      switch(compareDeletes(oldKV,newKV)) {
+      
+        case INCLUDE_NEW_NEXT_NEW: {
+          mergeDeletes.add(newKV);
+          if(++newIndex == newDeletes.size()) {
+            // Done with new, add the rest of old to merged and return
+            mergeDown(mergeDeletes, deletes, oldIndex);
+            finalize(mergeDeletes);
+            return;
+          }
+          newKV = this.newDeletes.get(newIndex);
+        }
+          
+        case INCLUDE_NEW_NEXT_OLD: {
+          mergeDeletes.add(newKV);
+          if(++oldIndex == deletes.size()) {
+            mergeDown(mergeDeletes, newDeletes, newIndex);
+            finalize(mergeDeletes);
+            return;
+          }
+          oldKV = this.deletes.get(oldIndex);
+        }
+          
+        case INCLUDE_OLD_NEXT_NEW: {
+          mergeDeletes.add(oldKV);
+          if(++newIndex == newDeletes.size()) {
+            mergeDown(mergeDeletes, deletes, oldIndex);
+            finalize(mergeDeletes);
+            return;
+          }
+          newKV = this.newDeletes.get(newIndex);
+        }
+          
+        case INCLUDE_OLD_NEXT_OLD: {
+          mergeDeletes.add(oldKV);
+          if(++oldIndex == deletes.size()) {
+            mergeDown(mergeDeletes, newDeletes, newIndex);
+            finalize(mergeDeletes);
+            return;
+          }
+          oldKV = this.deletes.get(oldIndex);
+        }
+          
+        case INCLUDE_OLD_NEXT_BOTH: {
+          mergeDeletes.add(oldKV);
+          ++oldIndex;
+          ++newIndex;
+          if(oldIndex == deletes.size()) {
+            if(newIndex == newDeletes.size()) {
+              finalize(mergeDeletes);
+              return;
+            }
+            mergeDown(mergeDeletes, newDeletes, newIndex);
+            finalize(mergeDeletes);
+            return;
+          } else if(newIndex == newDeletes.size()) {
+            mergeDown(mergeDeletes, deletes, oldIndex);
+            finalize(mergeDeletes);
+            return;
+          }
+          oldKV = this.deletes.get(oldIndex);
+          newKV = this.newDeletes.get(newIndex);
+        }
+      }
+    }
+  }
+  
+  private void finalize(List<KeyValue> mergeDeletes) {
+    this.deletes = mergeDeletes;
+    this.newDeletes = new ArrayList<KeyValue>();
+  }
+  
+  private void mergeDown(List<KeyValue> mergeDeletes, List<KeyValue> srcDeletes, 
+      int srcIndex) {
+    while(srcIndex < srcDeletes.size()) {
+      mergeDeletes.add(srcDeletes.get(srcIndex));
+    }
+  }
+  
+  private enum DeleteCompare { 
+    INCLUDE_NEW_NEXT_NEW,
+    INCLUDE_NEW_NEXT_OLD,
+    INCLUDE_OLD_NEXT_NEW,
+    INCLUDE_OLD_NEXT_OLD,
+    INCLUDE_OLD_NEXT_BOTH
+  }
+  
+  private DeleteCompare compareDeletes(KeyValue oldKV, KeyValue newKV) {
+    byte [] obytes = oldKV.getBuffer();
+    int ooffset = oldKV.getOffset();
+    
+    byte [] nbytes = newKV.getBuffer();
+    int noffset = newKV.getOffset();
+    
+    // This code can be replaced with KeyValue.getOffset methods
+    // but it's optimal like this and we don't have to increase the number
+    // of methods in KeyValue to support less wasteful accessors.
+    
+    int okeyLength = Bytes.toInt(obytes, ooffset);
+    ooffset += Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+    
+    int nkeyLength = Bytes.toInt(nbytes, noffset);
+    noffset += Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+    
+    short orowLength = Bytes.toShort(obytes, ooffset);
+    ooffset += Bytes.SIZEOF_SHORT + orowLength;
+    
+    short nrowLength = Bytes.toShort(nbytes, noffset);
+    noffset += Bytes.SIZEOF_SHORT + nrowLength;
+    
+    byte ofamilyLength = obytes[ooffset];
+    ooffset += Bytes.SIZEOF_BYTE + ofamilyLength;
+    
+    byte nfamilyLength = nbytes[noffset];
+    noffset += Bytes.SIZEOF_BYTE + nfamilyLength;
+    
+    int ocolumnLength = okeyLength - orowLength - ofamilyLength - 
+      KeyValue.KEY_INFRASTRUCTURE_SIZE;
+    
+    int ncolumnLength = nkeyLength - nrowLength - nfamilyLength -
+      KeyValue.KEY_INFRASTRUCTURE_SIZE;
+    
+    int ret = Bytes.compareTo(obytes, ooffset, ocolumnLength,
+        nbytes, noffset, ncolumnLength);
+    
+    if(ret <= -1) {
+      return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+    } else if(ret >= 1) {
+      return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+    }
+    
+    // Same column
+    
+    ooffset += ocolumnLength;
+    byte otype = obytes[ooffset];
+    ooffset += Bytes.SIZEOF_BYTE;
+    long otimestamp = Bytes.toLong(obytes, ooffset);
+    
+    noffset += ncolumnLength;
+    byte ntype = nbytes[noffset];
+    noffset += Bytes.SIZEOF_BYTE;
+    long ntimestamp = Bytes.toLong(nbytes, noffset);
+    
+    // Branches below can be optimized.  Keeping like this until testing
+    // is complete.
+    
+    if(otype == ntype) {
+      if(otimestamp > ntimestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_NEW;
+      } else if(otimestamp < ntimestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_OLD;
+      } else {
+        return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+      }
+    }
+
+    if(otype < ntype) {
+      if(otimestamp > ntimestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+      } else if(otimestamp < ntimestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_OLD;
+      } else {
+        return DeleteCompare.INCLUDE_NEW_NEXT_OLD;
+      }
+    }
+    
+    if(otype > ntype) {
+      if(otimestamp > ntimestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_NEW;
+      } else if(otimestamp < ntimestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+      } else {
+        return DeleteCompare.INCLUDE_OLD_NEXT_NEW;
+      }
+    }
+    
+    // Should never reach
+    return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+  }
+  
+  /*
+   * Condition Chart (ToInclude, ToNext)
+   * 
+   *   otype = ntype | otype < ntype | otype > ntype
+   * .---------------.---------------.---------------.
+   * |               |               |               |
+   * |   (OLD,BOTH)  |   (NEW,OLD)   |   (OLD,NEW)   | ots = nts
+   * |               |               |               |
+   * +---------------+---------------+---------------+-
+   * |               |               |               |
+   * |   (OLD,NEW)   |   (OLD,OLD)   |   (OLD,NEW)   | ots > nts
+   * |               |               |               |
+   * +---------------+---------------+---------------+-
+   * |               |               |               |
+   * |   (NEW,OLD)   |   (NEW,OLD)   |   (NEW,NEW)   | ots < nts
+   * |               |               |               |
+   * .---------------.---------------.---------------.
+   */
+}
Index: java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 0)
@@ -0,0 +1,105 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class ExplicitColumnTracker implements ColumnTracker {
+
+  private int maxVersions;
+  private List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  
+  public ExplicitColumnTracker(TreeSet<byte[]> columns, int maxVersions) {
+    this.maxVersions = maxVersions;
+    buildColumnList(columns);
+    this.index = 0;
+    this.column = this.columns.get(this.index);
+  }
+  
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+    // No more columns left, we are done with this query
+    if(this.columns.size() == 0) {
+      return MatchCode.DONE;
+    }
+    
+    // No more columns to match against, done with storefile
+    if(this.column == null) {
+      return MatchCode.NEXT;
+    }
+    
+    // Compare specific column to current column
+    int ret = Bytes.compareTo(column.bytes, column.offset, column.length, 
+        bytes, offset, length);
+    
+    // Matches, decrement versions left and include
+    if(ret == 0) {
+      if(this.column.decrement() == 0) {
+        // Done with versions for this column
+        this.columns.remove(this.index);
+        if(this.columns.size() == this.index) {
+          // Will not hit any more columns in this storefile
+          this.column = null;
+        } else {
+          this.column = this.columns.get(this.index);
+        }
+      }
+      return MatchCode.INCLUDE;
+    }
+
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++this.index == this.columns.size()) {
+        // No more to match, do not include, done with storefile
+        return MatchCode.NEXT;
+      }
+      this.column = this.columns.get(this.index);
+      return checkColumn(bytes, offset, length);
+    }
+
+    // Specified column is smaller than current column
+    // Skip
+    return MatchCode.SKIP;
+  }
+  
+  public void update() {
+    if(this.columns.size() != 0) {
+      this.index = 0;
+      this.column = this.columns.get(this.index);
+    } else {
+      this.index = -1;
+      this.column = null;
+    }
+  }
+
+  private void buildColumnList(TreeSet<byte[]> columns) {
+    this.columns = new ArrayList<ColumnCount>(columns.size());
+    for(byte [] column : columns) {
+      this.columns.add(new ColumnCount(column,maxVersions));
+    }
+  }
+}
Index: java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(working copy)
@@ -167,14 +167,14 @@
      */
     boolean matches(final KeyValue kv) throws IOException {
       if (this.matchType == MATCH_TYPE.SIMPLE) {
-        return kv.matchingColumnNoDelimiter(this.col, this.familylength);
+        return kv.matchingColumnNoDelimiter(this.col);
       } else if(this.matchType == MATCH_TYPE.FAMILY_ONLY) {
         return kv.matchingFamily(this.family);
       } else if (this.matchType == MATCH_TYPE.REGEX) {
         // Pass a column without the delimiter since thats whats we're
         // expected to match.
-        int o = kv.getColumnOffset();
-        int l = kv.getColumnLength(o);
+        int o = kv.getFamilyOffset();
+        int l = kv.getTotalColumnLength();
         String columnMinusQualifier = Bytes.toString(kv.getBuffer(), o, l);
         return this.columnMatcher.matcher(columnMinusQualifier).matches();
       } else {
Index: java/org/apache/hadoop/hbase/regionserver/HLog.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/HLog.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/HLog.java	(working copy)
@@ -90,7 +90,7 @@
 public class HLog implements HConstants, Syncable {
   private static final Log LOG = LogFactory.getLog(HLog.class);
   private static final String HLOG_DATFILE = "hlog.dat.";
-  static final byte [] METACOLUMN = Bytes.toBytes("METACOLUMN:");
+  static final byte [] METAFAMILY = Bytes.toBytes("METAFAMILY");
   static final byte [] METAROW = Bytes.toBytes("METAROW");
   final FileSystem fs;
   final Path dir;
@@ -665,7 +665,7 @@
 
   private HLogEdit completeCacheFlushLogEdit() {
     // TODO Profligacy!!! Fix all this creation.
-    return new HLogEdit(new KeyValue(METAROW, METACOLUMN,
+    return new HLogEdit(new KeyValue(METAROW, METAFAMILY, null,
       System.currentTimeMillis(), HLogEdit.COMPLETE_CACHE_FLUSH));
   }
 
@@ -683,8 +683,8 @@
    * @param column
    * @return true if the column is a meta column
    */
-  public static boolean isMetaColumn(byte [] column) {
-    return Bytes.equals(METACOLUMN, column);
+  public static boolean isMetaFamily(byte [] family) {
+    return Bytes.equals(METAFAMILY, family);
   }
   
   /**
Index: java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/HRegion.java	(working copy)
@@ -59,7 +59,9 @@
 import org.apache.hadoop.hbase.io.BatchOperation;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.Reference.Range;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
@@ -2704,4 +2706,47 @@
       releaseRowLock(lid);
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  public Result get(final Get get, final Integer lockid) throws IOException {
+    // Verify families are all valid
+    if(get.numFamilies() > 0) {
+      for(byte [] family : get.familySet()) {
+        checkFamily(family);
+      }
+    }
+    // Lock row
+    Integer lid = getLock(lockid, get.getRow()); 
+    List<KeyValue> result = new ArrayList<KeyValue>();
+    try {
+      for(Map.Entry<byte[],TreeSet<byte[]>> entry : get.entrySet()) {
+        byte [] family = entry.getKey();
+        Store store = stores.get(family);
+        store.get(get, entry.getValue(), result, 
+            this.comparator.getRawComparator());
+      }
+    } finally {
+      if(lockid == null) releaseRowLock(lid);
+    }
+    return new Result(result);
+  }
+  
+  //
+  // New HBASE-880 Helpers
+  //
+  
+  private void checkFamily(final byte [] family) 
+  throws NoSuchColumnFamilyException {
+    if(family == null) {
+      throw new NoSuchColumnFamilyException("Empty family is invalid");
+    }
+    if(!regionInfo.getTableDesc().hasFamily(family)) {
+      throw new NoSuchColumnFamilyException("Column family " +
+          Bytes.toString(family) + " does not exist in region " + this
+            + " in table " + regionInfo.getTableDesc());
+    }
+  }
 }
Index: java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(working copy)
@@ -84,7 +84,9 @@
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
@@ -2361,7 +2363,21 @@
       checkFileSystem();
       throw e;
     }
-    
-    
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  /** {@inheritDoc} */
+  public Result get(byte [] regionName, Get get) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.get(get, getLockFromId(get.getLockId()));
+    } catch(Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
 }
Index: java/org/apache/hadoop/hbase/regionserver/Memcache.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/Memcache.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/Memcache.java	(working copy)
@@ -632,6 +632,71 @@
     }
   }
 
+  //
+  // HBASE-880/1249/1304
+  //
+  
+  /**
+   * Perform a single-row Get on the memcache and snapshot, placing results
+   * into the specified KV list.
+   * <p>
+   * This will return true if it is determined that the query is complete
+   * and it is not necessary to check any storefiles after this.
+   * <p>
+   * Otherwise, it will return false and you should continue on.
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  public boolean get(QueryMatcher matcher, List<KeyValue> result)
+  throws IOException {
+    this.lock.readLock().lock();
+    try {
+      if(internalGet(this.memcache, matcher, result)) {
+        return true;
+      }
+      matcher.update();
+      if(internalGet(this.snapshot, matcher, result)) {
+        return true;
+      }
+      return false;
+    } finally {
+      this.lock.readLock().unlock();
+    }
+  }
+  
+  /**
+   * 
+   * @param set memcache or snapshot
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  private boolean internalGet(SortedSet<KeyValue> set, QueryMatcher matcher,
+      List<KeyValue> result) throws IOException {
+    if(set.isEmpty()) return false;
+    // Seek to startKey
+    set = set.tailSet(matcher.getStartKey());
+    for(KeyValue kv : set) {
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    }
+    return false;
+  }
+  
   //////////////////////////////////////////////////////////////////////////////
   // MemcacheScanner implements the InternalScanner.
   // It lets the caller scan the contents of the Memcache.
Index: java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 0)
@@ -0,0 +1,201 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class QueryMatcher {
+  
+  // ColumnMatcher.match() return codes
+  public static enum MatchCode { 
+    INCLUDE, // Include in result 
+    SKIP,    // Do not include in result
+    NEXT,    // Move to next StoreFile
+    DONE     // Query done, return
+  }
+  
+  // Stateless
+  private Get get;
+  private KeyValue startKey;
+  private KeyComparator rowComparator;
+  
+  // Stateful
+  private ColumnTracker columns;
+  private DeleteTracker deletes;
+  
+  // Extracted Fields
+  private byte [] row;
+  private long oldestStamp;
+  
+  public QueryMatcher(Get get, byte [] family, TreeSet<byte[]> columns, 
+      long ttl, KeyComparator rowComparator) {
+    this.get = get;
+    this.deletes = new DeleteTracker();
+    this.row = get.getRow();
+    this.oldestStamp = System.currentTimeMillis() - ttl;
+    this.rowComparator = rowComparator;
+    // Single branch to deal with two types of Gets (columns vs all in family)
+    if(columns == null) {
+      this.columns = new WildcardColumnTracker(get.getMaxVersions());
+      this.startKey = KeyValue.createFirstOnRow(get.getRow());
+    } else {
+      this.columns = new ExplicitColumnTracker(columns,get.getMaxVersions());
+      this.startKey = KeyValue.createFirstOnRow(get.getRow(), family, 
+          columns.first());
+    }
+  }
+  
+  /**
+   * Main method for ColumnMatcher.
+   * <p>
+   * Determines whether the specified KeyValue should be included in the
+   * result or not.
+   * <p>
+   * Contains additional language to early-out of the current file or to
+   * return immediately.
+   * <p>
+   * Things to be checked:<ul>
+   * <li>Row
+   * <li>TTL
+   * <li>Type
+   * <li>TimeRange
+   * <li>Deletes
+   * <li>Column
+   * <li>Versions
+   * @param kv KeyValue to check
+   * @return MatchCode: include, skip, next, done
+   */
+  public MatchCode match(KeyValue kv) throws IOException {
+    // Directly act on KV buffer
+    byte [] bytes = kv.getBuffer();
+    int offset = kv.getOffset();
+    
+    int keyLength = Bytes.toInt(bytes, offset);
+    offset += Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+    
+    short rowLength = Bytes.toShort(bytes, offset);
+    offset += Bytes.SIZEOF_SHORT;
+    
+    /* Check ROW
+     * If past query's row, go to next StoreFile
+     * If not reached query's row, go to next KeyValue
+     */ 
+//  int ret = Bytes.compareTo(row, 0, row.length, bytes, offset, rowLength);
+    int ret = this.rowComparator.compareRows(row, 0, row.length, bytes, offset, rowLength);
+    if(ret <= -1) {
+      // Have reached the next row
+      return MatchCode.NEXT;
+    } else if(ret >= 1) {
+      // At a previous row
+      return MatchCode.SKIP;
+    }
+    offset += rowLength;
+    
+    byte familyLength = bytes[offset];
+    offset += Bytes.SIZEOF_BYTE + familyLength;
+    
+    int columnLength = keyLength - (offset - kv.getOffset()) - 
+      KeyValue.TIMESTAMP_TYPE_SIZE;
+    int columnOffset = offset;
+    offset += columnLength;
+    
+    /* Check TTL
+     * If expired, go to next KeyValue
+     */
+    long timestamp = Bytes.toLong(bytes, offset);
+    if(isExpired(timestamp)) {
+      return MatchCode.NEXT;
+    }
+    offset += Bytes.SIZEOF_LONG;
+    
+    /* Check TYPE
+     * If a delete within (or after) time range, add to deletes
+     * Move to next KeyValue
+     */
+    byte type = bytes[offset];
+    if(isDelete(type)) {
+      if(get.getTimeRange().withinOrAfterTimeRange(timestamp)) {
+        this.deletes.add(kv);
+      }
+      return MatchCode.SKIP;
+    }
+    
+    /* Check TimeRange
+     * If outside of range, move to next KeyValue
+     */
+    if(!get.getTimeRange().withinTimeRange(timestamp)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Deletes
+     * If deleted, move to next KeyValue 
+     */
+    if(!deletes.isEmpty() && deletes.isDeleted(bytes, kv.getOffset(), 
+        rowLength, familyLength, columnLength, columnOffset, 
+        timestamp, type)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Column and Versions
+     * Returns a MatchCode directly, identical language
+     * If matched column without enough versions, include
+     * If enough versions of this column or does not match, skip
+     * If have moved past 
+     * If enough versions of everything, 
+     */
+    return columns.checkColumn(bytes, columnOffset, columnLength);
+  }
+  
+  private boolean isDelete(byte type) {
+    return (type != KeyValue.Type.Put.getCode());
+  }
+  
+  private boolean isExpired(long timestamp) {
+    if(timestamp < oldestStamp) {
+      return true;
+    }
+    return false;
+  }
+
+  /**
+   * Called after reading each section (memcache, snapshot, storefiles).
+   * <p>
+   * This method will update the internal structures to be accurate for
+   * the next section. 
+   */
+  public void update() {
+    this.deletes.update();
+    this.columns.update();
+  }
+  
+  public int maxVersions() {
+    return this.get.getMaxVersions();
+  }
+  
+  public KeyValue getStartKey() {
+    return this.startKey;
+  }
+}
Index: java/org/apache/hadoop/hbase/regionserver/Store.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/Store.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/Store.java	(working copy)
@@ -51,7 +51,9 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.SequenceFile;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.io.hfile.HFile;
@@ -310,8 +312,7 @@
         // METACOLUMN info such as HBASE::CACHEFLUSH entries
         KeyValue kv = val.getKeyValue();
         if (val.isTransactionEntry() ||
-            kv.matchingColumnNoDelimiter(HLog.METACOLUMN,
-              HLog.METACOLUMN.length - 1) ||
+            kv.matchingFamily(HLog.METAFAMILY) ||
           !Bytes.equals(key.getRegionName(), regioninfo.getRegionName()) ||
           !kv.matchingFamily(family.getName())) {
           continue;
@@ -1005,7 +1006,46 @@
   // (This is the only section that is directly useful!)
   //////////////////////////////////////////////////////////////////////////////
   
+  //
+  // HBASE-880/1249/1304
+  //
+  
   /**
+   * Retrieve results from this store given the specified Get parameters.
+   * @param get Get operation
+   * @param columns List of columns to match, can be empty (not null)
+   * @param result List to add results to 
+   */
+  public void get(Get get, TreeSet<byte[]> columns, List<KeyValue> result,
+      KeyComparator keyComparator) 
+  throws IOException {
+    
+    // Column matching and version enforcement
+    QueryMatcher matcher = new QueryMatcher(get, this.family.getName(), columns, 
+        this.ttl, keyComparator);
+    
+    // Read from Memcache
+    this.memcache.get(matcher, result);
+    
+    // Check if we even have storefiles
+    if(this.storefiles.size() == 0) {
+      return;
+    }
+    
+    // Get storefiles for this store
+    List<HFileScanner> storefileScanners = new ArrayList<HFileScanner>();
+    for(StoreFile sf : this.storefiles.descendingMap().values()) {
+      storefileScanners.add(sf.getReader().getScanner());
+    }
+    
+    // StoreFileScan will handle reading this store's storefiles
+    StoreFileScan scanner = new StoreFileScan(storefileScanners, matcher);
+    
+    // Run a GET scan and put results into the specified list 
+    scanner.get(result);
+  }
+  
+  /**
    * Return all the available columns for the given key.  The key indicates a 
    * row and timestamp, but not a column name.
    *
@@ -1122,7 +1162,7 @@
     // if the column pattern is not null, we use it for column matching.
     // we will skip the keys whose column doesn't match the pattern.
     if (columnPattern != null) {
-      if (!(columnPattern.matcher(candidate.getColumnString()).matches())) {
+      if (!(columnPattern.matcher(Bytes.toString(candidate.getColumn())).matches())) {
         return false;
       }
     }
Index: java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 0)
@@ -0,0 +1,88 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.hfile.HFileScanner;
+
+public class StoreFileScan {
+
+  private List<HFileScanner> scanners;
+  private QueryMatcher matcher;
+  
+  private byte [] startKey;
+  
+  public StoreFileScan(List<HFileScanner> scanners, QueryMatcher matcher) {
+    this.scanners = scanners;
+    this.matcher = matcher;
+    this.startKey = matcher.getStartKey().getBuffer();
+  }
+  
+  /**
+   * Performs a GET operation across multiple StoreFiles.
+   * <p>
+   * This style of StoreFile scanning goes through each
+   * StoreFile in its entirety, most recent first, before
+   * proceeding to the next StoreFile.
+   * <p>
+   * This strategy allows for optimal, stateless (no persisted Scanners)
+   * early-out scenarios.    
+   * @param result List to add results to
+   */
+  public void get(List<KeyValue> result) throws IOException {
+    for(HFileScanner scanner : this.scanners) {
+      this.matcher.update();
+      if(getStoreFile(scanner, result)) {
+        return;
+      }
+    }
+  }
+  
+  /**
+   * Performs a GET operation on a single StoreFile.
+   * @return true if done with this store, false if must continue to next 
+   */
+  public boolean getStoreFile(HFileScanner scanner, List<KeyValue> result) 
+  throws IOException {
+    if(scanner.seekTo(this.startKey) == -1) {
+      // No keys in StoreFile at or after specified startKey
+      return false;
+    }
+    do {
+      KeyValue kv = scanner.getKeyValue();
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    } while(scanner.next());
+    return false;
+  }
+  
+}
Index: java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(working copy)
@@ -185,10 +185,11 @@
                 if (this.dataFilter != null) {
                   // Filter whole row by column data?
                   int rowlength = kv.getRowLength();
-                  int columnoffset = kv.getColumnOffset(rowlength);
+                  int columnoffset = kv.getFamilyOffset(rowlength);
                   filtered = dataFilter.filterColumn(kv.getBuffer(),
-                      kv.getRowOffset(), rowlength,
-                    kv.getBuffer(), columnoffset, kv.getColumnLength(columnoffset),
+                    kv.getRowOffset(), rowlength,
+                    kv.getBuffer(), columnoffset, kv.getTotalColumnLength(
+                        rowlength,columnoffset),
                     kv.getBuffer(), kv.getValueOffset(), kv.getValueLength());
                   if (filtered) {
                     results.clear();
Index: java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(working copy)
@@ -185,9 +185,9 @@
         // against a KeyValue.  Each invocation creates a new instance.  St.Ack.
 
         // Check this edit is for me.
-        byte[] column = val.getKeyValue().getColumn();
+        byte[] family = val.getKeyValue().getFamily();
         Long transactionId = val.getTransactionId();
-        if (!val.isTransactionEntry() || HLog.isMetaColumn(column)
+        if (!val.isTransactionEntry() || HLog.isMetaFamily(family)
             || !Bytes.equals(key.getRegionName(), regionInfo.getRegionName())) {
           continue;
         }
Index: java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java
===================================================================
--- java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 0)
+++ java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 0)
@@ -0,0 +1,250 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class WildcardColumnTracker implements ColumnTracker {
+  
+  private int maxVersions;
+  
+  private List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  
+  private List<ColumnCount> newColumns; 
+  private int newIndex;
+  private ColumnCount newColumn;
+  
+  public WildcardColumnTracker(int maxVersions) {
+    this.maxVersions = maxVersions;
+    this.index = 0;
+    this.column = null;
+    this.columns = null;
+    this.newColumns = new ArrayList<ColumnCount>();
+    this.newIndex = 0;
+    this.newColumn = null;
+  }
+  
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+
+    // Nothing to match against, add to new and include
+    if(this.column == null && this.newColumn == null) {
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing old, compare against new
+    if(this.column == null && this.newColumn != null) {
+      int ret = Bytes.compareTo(newColumn.bytes, newColumn.offset, 
+          newColumn.length, bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(newColumn.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++newIndex == newColumns.size()) {
+          // No more, add to end and include
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          return MatchCode.INCLUDE;
+        }
+        this.newColumn = newColumns.get(newIndex);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(++newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing new, compare against old
+    if(this.newColumn == null && this.column != null) {
+      int ret = Bytes.compareTo(column.bytes, column.offset, column.length,
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          // No more, add to new and include (new was empty prior to this)
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          return MatchCode.INCLUDE;
+        }
+        this.column = columns.get(index);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    
+    // There are new and old, figure which to check first
+    int ret = Bytes.compareTo(column.bytes, column.offset, column.length, 
+        newColumn.bytes, newColumn.offset, newColumn.length);
+        
+    // Old is smaller than new, compare against old
+    if(ret <= -1) {
+      ret = Bytes.compareTo(column.bytes, column.offset, column.length,
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          this.column = null;
+        } else {
+          this.column = columns.get(index);
+        }
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      return MatchCode.INCLUDE;
+    }
+    
+    // Cannot be equal, so ret >= 1
+    // New is smaller than old, compare against new
+    
+    ret = Bytes.compareTo(newColumn.bytes, newColumn.offset, newColumn.length,
+        bytes, offset, length);
+    
+    // Same column
+    if(ret == 0) {
+      if(newColumn.increment() > this.maxVersions) {
+        return MatchCode.SKIP;
+      }
+      return MatchCode.INCLUDE;
+    }
+    
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++newIndex == newColumns.size()) {
+        this.newColumn = null;
+      } else {
+        this.newColumn = newColumns.get(newIndex);
+      }
+      return checkColumn(bytes, offset, length);
+    }
+    
+    // ret >= 1
+    // Specified column is smaller than current column
+    // Nothing to match against, add to new and include
+    newColumns.add(new ColumnCount(bytes, offset, length, 1));
+    return MatchCode.INCLUDE;
+  }
+  
+  public void update() {
+    // If no previous columns, use new columns and return
+    if(this.columns == null || this.columns.size() == 0) {
+      this.columns = this.newColumns;
+      this.newColumns = new ArrayList<ColumnCount>();
+      return;
+    }
+    
+    // If no new columns, retain previous columns and reutrn
+    if(this.newColumns.size() == 0) {
+      return;
+    }
+    
+    // Merge previous columns with new columns
+    // There will be no overlapping
+    List<ColumnCount> mergeColumns = new ArrayList<ColumnCount>(
+        columns.size() + newColumns.size());
+    index = 0;
+    newIndex = 0;
+    column = columns.get(0);
+    newColumn = newColumns.get(0);
+    while(true) {
+      int ret = Bytes.compareTo(column.bytes, column.offset, column.length,
+          newColumn.bytes, newColumn.offset, newColumn.length);
+      
+      // Existing is smaller than new, add existing and iterate it
+      if(ret <= -1) {
+        mergeColumns.add(column);
+        if(++index == columns.size()) {
+          // No more existing left, merge down rest of new and return 
+          mergeDown(mergeColumns, newColumns, newIndex);
+          return;
+        }
+        column = columns.get(index);
+        continue;
+      }
+      
+      // New is smaller than existing, add new and iterate it
+      mergeColumns.add(newColumn);
+      if(++newIndex == newColumns.size()) {
+        // No more new left, merge down rest of existing and return
+        mergeDown(mergeColumns, columns, index);
+        return;
+      }
+      newColumn = newColumns.get(newIndex);
+      continue;
+    }
+  }
+  
+  private void mergeDown(List<ColumnCount> mergeColumns, 
+      List<ColumnCount> srcColumns, int srcIndex) {
+    while(srcIndex < srcColumns.size()) {
+      mergeColumns.add(srcColumns.get(srcIndex));
+    }
+  }
+}
Index: java/org/apache/hadoop/hbase/util/Bytes.java
===================================================================
--- java/org/apache/hadoop/hbase/util/Bytes.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/util/Bytes.java	(working copy)
@@ -27,9 +27,12 @@
 import java.util.Comparator;
 import java.math.BigInteger;
 
+import org.apache.hadoop.hbase.ColumnNameParseException;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.io.RawComparator;
+import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.io.WritableComparator;
 import org.apache.hadoop.io.WritableUtils;
 
@@ -38,8 +41,93 @@
  * comparisons, hash code generation, manufacturing keys for HashMaps or
  * HashSets, etc.
  */
-public class Bytes {
+public class Bytes implements Writable, Comparable<Bytes> {
+  
+  //
+  // Instantiable Bytes
+  //
+  
+  private byte [] bytes;
+  private int offset;
+  private int length;
+  
+  /** Writable Constructor, DO NOT USE. */
+  public Bytes() {}
+  
   /**
+   * Constructor that only takes the byte [] and sets the offset and length
+   * internally.
+   * @param bytes
+   */
+  public Bytes(byte [] bytes){
+    this(bytes, 0, bytes.length);
+  }
+  
+  /**
+   * Constructor to create a wrapper for a byte [], so that it can be used as a
+   * pointer into bigger byte [] without having to do any copying of the byte []
+   * @param bytes
+   * @param offset
+   * @param length
+   */
+  public Bytes(byte [] bytes, int offset, int length){
+    this.bytes = bytes;
+    this.offset = offset;
+    this.length = length;
+  }
+  
+  /**
+   * Compare method to compare this Bytes object with a byte [] and it's offset
+   * and length.
+   */
+  public int compareTo(byte [] bytes, int offset, int length){
+    return Bytes.compareTo(this.bytes, this.offset, this.length, 
+        bytes, offset, length);
+  }
+  
+  //Comparable
+  @Override
+  public int compareTo(Bytes bytes){
+    return Bytes.compareTo(this.bytes, this.offset, this.length, 
+        bytes.bytes, bytes.offset, bytes.length);
+  }
+  
+  //Writable
+  @Override
+  public void readFields(final DataInput in) throws IOException {
+    this.length = in.readInt();
+    this.offset = in.readInt();
+    this.bytes = new byte[this.length];
+    in.readFully(this.bytes, this.offset, this.length);
+  }
+  @Override
+  public void write(final DataOutput out) throws IOException {
+    out.writeInt(this.length);
+    out.writeInt(this.offset);
+    out.write(this.bytes, this.offset, this.bytes.length);
+  }
+  
+  //toString
+  @Override
+  public String toString(){
+    return new String(this.bytes, this.offset, this.length);
+  }
+  
+  /**
+   * Return as a byte array.  If this Bytes represents the entire buffer,
+   * the backing array will be returned.  Otherwise, a copy is returned.
+   * @return backing array if complete, otherwise a copy
+   */
+  public byte [] get() {
+    if(this.offset == 0 && this.length == this.bytes.length) {
+      return this.bytes;
+    }
+    byte [] ret = new byte[this.length];
+    System.arraycopy(this.bytes, this.offset, ret, 0, this.length);
+    return ret;
+  }
+  
+  /**
    * Size of long in bytes
    */
   public static final int SIZEOF_LONG = Long.SIZE/Byte.SIZE;
@@ -995,4 +1083,30 @@
     }
     return value;
   }
+  
+
+  /**
+   * @param c Full column in family:qualifier form
+   * @return Return array of size two whose first element has the family
+   * prefix of passed column <code>c</code> and whose second element is the
+   * column qualifier.
+   * @throws ColumnNameParseException 
+   */
+  public static byte [][] parseColumn(final byte [] c)
+  throws ColumnNameParseException {
+    final byte [][] result = new byte [2][];
+    // TODO: Change this so don't do parse but instead use the comparator
+    // inside in KeyValue which just looks at column family.
+    final int index = KeyValue.getFamilyDelimiterIndex(c, 0, c.length);
+    if (index == -1) {
+      throw new ColumnNameParseException("Impossible column name: " + c);
+    }
+    result[0] = new byte [index];
+    System.arraycopy(c, 0, result[0], 0, index);
+    final int len = c.length - (index + 1);
+    result[1] = new byte[len];
+    System.arraycopy(c, index + 1 /*Skip delimiter*/, result[1], 0,
+      len);
+    return result;
+  }
 }
Index: java/org/apache/hadoop/hbase/util/MetaUtils.java
===================================================================
--- java/org/apache/hadoop/hbase/util/MetaUtils.java	(revision 772412)
+++ java/org/apache/hadoop/hbase/util/MetaUtils.java	(working copy)
@@ -250,9 +250,8 @@
       while (metaScanner.next(results)) {
         HRegionInfo info = null;
         for (KeyValue kv: results) {
-          if (KeyValue.META_COMPARATOR.compareColumns(kv,
-            HConstants.COL_REGIONINFO, 0, HConstants.COL_REGIONINFO.length,
-              HConstants.COLUMN_FAMILY_STR.length()) == 0) {
+          if(kv.matchingColumn(HConstants.CATALOG_FAMILY,
+              HConstants.REGIONINFO_QUALIFIER)) {
             info = Writables.getHRegionInfoOrNull(kv.getValue());
             if (info == null) {
               LOG.warn("region info is null for row " +
Index: test/org/apache/hadoop/hbase/regionserver/TestHLog.java
===================================================================
--- test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(revision 772412)
+++ test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(working copy)
@@ -133,7 +133,7 @@
         assertTrue(Bytes.equals(regionName, key.getRegionName()));
         assertTrue(Bytes.equals(tableName, key.getTablename()));
         assertTrue(Bytes.equals(HLog.METAROW, val.getKeyValue().getRow()));
-        assertTrue(Bytes.equals(HLog.METACOLUMN, val.getKeyValue().getColumn()));
+        assertTrue(Bytes.equals(HLog.METAFAMILY, val.getKeyValue().getFamily()));
         assertEquals(0, Bytes.compareTo(HLogEdit.COMPLETE_CACHE_FLUSH,
           val.getKeyValue().getValue()));
         System.out.println(key + " " + val);
Index: test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java
===================================================================
--- test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(revision 772412)
+++ test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(working copy)
@@ -218,7 +218,7 @@
     int i = 0;
     for (KeyValue kv: kvs) {
       String expectedColname = Bytes.toString(getColumnName(rowIndex, i++));
-      String colnameStr = kv.getColumnString();
+      String colnameStr = Bytes.toString(kv.getColumn());
       assertEquals("Column name", colnameStr, expectedColname);
       // Value is column name as bytes.  Usually result is
       // 100 bytes in size at least. This is the default size
Index: test/org/apache/hadoop/hbase/TestKeyValue.java
===================================================================
--- test/org/apache/hadoop/hbase/TestKeyValue.java	(revision 772412)
+++ test/org/apache/hadoop/hbase/TestKeyValue.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.util.Bytes;
 
 public class TestKeyValue extends TestCase {
@@ -39,13 +40,21 @@
     final byte [] a = Bytes.toBytes("aaa");
     byte [] column1 = Bytes.toBytes("abc:def");
     byte [] column2 = Bytes.toBytes("abcd:ef");
-    KeyValue aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column2, 0, column2.length, 4) == 0);
+    byte [] family2 = Bytes.toBytes("abcd");
+    byte [] qualifier2 = Bytes.toBytes("ef"); 
+    KeyValue aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column2));
+    assertTrue(aaa.matchingColumn(column1));
+    aaa = new KeyValue(a, column2, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column1));
+    assertTrue(aaa.matchingColumn(family2,qualifier2));
     column1 = Bytes.toBytes("abcd:");
-    aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column1, 0, column1.length, 4) == 0);
+    aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertTrue(aaa.matchingColumn(family2,null));
+    assertFalse(aaa.matchingColumn(family2,qualifier2));
+    // Previous test had an assertFalse that I don't understand
+    //    assertFalse(KeyValue.COMPARATOR.
+    //    compareColumns(aaa, column1, 0, column1.length, 4) == 0);
   }
 
   public void testBasics() throws Exception {
@@ -111,31 +120,31 @@
   public void testMoreComparisons() throws Exception {
     // Root compares
     long now = System.currentTimeMillis();
-    KeyValue a = new KeyValue(".META.,,99999999999999", now);
-    KeyValue b = new KeyValue(".META.,,1", now);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,,99999999999999"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,,1"), now);
     KVComparator c = new KeyValue.RootComparator();
     assertTrue(c.compare(b, a) < 0);
-    KeyValue aa = new KeyValue(".META.,,1", now);
-    KeyValue bb = new KeyValue(".META.,,1", "info:regioninfo",
-      1235943454602L);
+    KeyValue aa = new KeyValue(Bytes.toBytes(".META.,,1"), now);
+    KeyValue bb = new KeyValue(Bytes.toBytes(".META.,,1"), 
+        Bytes.toBytes("info:regioninfo"), 1235943454602L);
     assertTrue(c.compare(aa, bb) < 0);
     
     // Meta compares
-    KeyValue aaa =
-      new KeyValue("TestScanMultipleVersions,row_0500,1236020145502", now);
-    KeyValue bbb = new KeyValue("TestScanMultipleVersions,,99999999999999",
-      now);
+    KeyValue aaa = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,row_0500,1236020145502"), now);
+    KeyValue bbb = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,,99999999999999"), now);
     c = new KeyValue.MetaComparator();
     assertTrue(c.compare(bbb, aaa) < 0);
     
-    KeyValue aaaa = new KeyValue("TestScanMultipleVersions,,1236023996656",
-      "info:regioninfo", 1236024396271L);
+    KeyValue aaaa = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,,1236023996656"),
+        Bytes.toBytes("info:regioninfo"), 1236024396271L);
     assertTrue(c.compare(aaaa, bbb) < 0);
     
-    KeyValue x = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "", 9223372036854775807L);
-    KeyValue y = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "info:regioninfo", 1236034574912L);
+    KeyValue x = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes(""), 9223372036854775807L);
+    KeyValue y = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes("info:regioninfo"), 1236034574912L);
     assertTrue(c.compare(x, y) < 0);
     comparisons(new KeyValue.MetaComparator());
     comparisons(new KeyValue.KVComparator());
@@ -151,53 +160,53 @@
   public void testKeyValueBorderCases() throws IOException {
     // % sorts before , so if we don't do special comparator, rowB would
     // come before rowA.
-    KeyValue rowA = new KeyValue("testtable,www.hbase.org/,1234",
-      "", Long.MAX_VALUE);
-    KeyValue rowB = new KeyValue("testtable,www.hbase.org/%20,99999",
-      "", Long.MAX_VALUE);
+    KeyValue rowA = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/,1234"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
+    KeyValue rowB = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/%20,99999"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue("testtable,,1234", "", Long.MAX_VALUE);
-    rowB = new KeyValue("testtable,$www.hbase.org/,99999", "", Long.MAX_VALUE);
+    rowA = new KeyValue(Bytes.toBytes("testtable,,1234"), Bytes.toBytes(""), Long.MAX_VALUE);
+    rowB = new KeyValue(Bytes.toBytes("testtable,$www.hbase.org/,99999"), Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue(".META.,testtable,www.hbase.org/,1234,4321", "",
+    rowA = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/,1234,4321"), Bytes.toBytes(""),
       Long.MAX_VALUE);
-    rowB = new KeyValue(".META.,testtable,www.hbase.org/%20,99999,99999", "",
+    rowB = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/%20,99999,99999"), Bytes.toBytes(""),
       Long.MAX_VALUE);
     assertTrue(KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);
   }
 
   private void metacomparisons(final KeyValue.MetaComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,1", now),
-      new KeyValue(".META.,a,,0,1", now)) == 0);
-    KeyValue a = new KeyValue(".META.,a,,0,1", now);
-    KeyValue b = new KeyValue(".META.,a,,0,2", now);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) == 0);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now);
     assertTrue(c.compare(a, b) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,2", now),
-      new KeyValue(".META.,a,,0,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) > 0);
   }
 
   private void comparisons(final KeyValue.KVComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,1", now)) == 0);
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,2", now)) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,,2", now),
-      new KeyValue(".META.,,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) == 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,2"), now)) < 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) > 0);
   }
 
   public void testBinaryKeys() throws Exception {
     Set<KeyValue> set = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
-    String column = "col:umn";
-    KeyValue [] keys = {new KeyValue("aaaaa,\u0000\u0000,2", column, 2),
-      new KeyValue("aaaaa,\u0001,3", column, 3),
-      new KeyValue("aaaaa,,1", column, 1),
-      new KeyValue("aaaaa,\u1000,5", column, 5),
-      new KeyValue("aaaaa,a,4", column, 4),
-      new KeyValue("a,a,0", column, 0),
+    byte [] column = Bytes.toBytes("col:umn");
+    KeyValue [] keys = {new KeyValue(Bytes.toBytes("aaaaa,\u0000\u0000,2"), column, 2),
+      new KeyValue(Bytes.toBytes("aaaaa,\u0001,3"), column, 3),
+      new KeyValue(Bytes.toBytes("aaaaa,,1"), column, 1),
+      new KeyValue(Bytes.toBytes("aaaaa,\u1000,5"), column, 5),
+      new KeyValue(Bytes.toBytes("aaaaa,a,4"), column, 4),
+      new KeyValue(Bytes.toBytes("a,a,0"), column, 0),
     };
     // Add to set with bad comparator
     for (int i = 0; i < keys.length; i++) {
@@ -226,12 +235,12 @@
     }
     // Make up -ROOT- table keys.
     KeyValue [] rootKeys = {
-        new KeyValue(".META.,aaaaa,\u0000\u0000,0,2", column, 2),
-        new KeyValue(".META.,aaaaa,\u0001,0,3", column, 3),
-        new KeyValue(".META.,aaaaa,,0,1", column, 1),
-        new KeyValue(".META.,aaaaa,\u1000,0,5", column, 5),
-        new KeyValue(".META.,aaaaa,a,0,4", column, 4),
-        new KeyValue(".META.,,0", column, 0),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0000\u0000,0,2"), column, 2),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0001,0,3"), column, 3),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,,0,1"), column, 1),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u1000,0,5"), column, 5),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,a,0,4"), column, 4),
+        new KeyValue(Bytes.toBytes(".META.,,0"), column, 0),
       };
     // This will output the keys incorrectly.
     set = new TreeSet<KeyValue>(new KeyValue.MetaComparator());
