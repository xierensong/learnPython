Index: src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java	(working copy)
@@ -27,6 +27,7 @@
 import org.apache.commons.logging.LogFactory;
 
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.client.HTable;
 
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
@@ -209,9 +210,9 @@
   throws IOException {
     HRegion region = createNewHRegion(desc, startKey, endKey);
     byte [] keyToWrite = startKey == null ? Bytes.toBytes("row_000") : startKey;
-    BatchUpdate bu = new BatchUpdate(keyToWrite);
-    bu.put(COLUMN_NAME, "test".getBytes());
-    region.batchUpdate(bu, null);
+    Put put = new Put(keyToWrite);
+    put.add(COLUMN_NAME, "test".getBytes());
+    region.put(put);
     region.close();
     region.getLog().closeAndDelete();
     return region;
Index: src/test/org/apache/hadoop/hbase/TestSerialization.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestSerialization.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestSerialization.java	(working copy)
@@ -20,11 +20,25 @@
 package org.apache.hadoop.hbase;
 
 
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+//import java.util.SortedSet;
+import java.util.TreeSet;
+import java.util.NavigableSet;
+
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.filter.InclusiveStopRowFilter;
 import org.apache.hadoop.hbase.io.BatchOperation;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
 
@@ -171,4 +185,203 @@
     }
     assertEquals(firstCount, secondCount);
   }
+  
+  
+  //
+  // HBASE-880
+  //
+  
+  public void testPut() throws Exception{
+    byte[] row = "row".getBytes();
+    byte[] fam = "fam".getBytes();
+    byte[] qf1 = "qf1".getBytes();
+    byte[] qf2 = "qf2".getBytes();
+    byte[] qf3 = "qf3".getBytes();
+    byte[] qf4 = "qf4".getBytes();
+    byte[] qf5 = "qf5".getBytes();
+    byte[] qf6 = "qf6".getBytes();
+    byte[] qf7 = "qf7".getBytes();
+    byte[] qf8 = "qf8".getBytes();
+    
+    long ts = System.currentTimeMillis();
+    byte[] val = "val".getBytes();
+    
+    Put put = new Put(row);
+    put.add(fam, qf1, ts, val);
+    put.add(fam, qf2, ts, val);
+    put.add(fam, qf3, ts, val);
+    put.add(fam, qf4, ts, val);
+    put.add(fam, qf5, ts, val);
+    put.add(fam, qf6, ts, val);
+    put.add(fam, qf7, ts, val);
+    put.add(fam, qf8, ts, val);
+    
+    byte[] sb = Writables.getBytes(put);
+    Put desPut = (Put)Writables.getWritable(sb, new Put());
+
+    //Timing test
+//    long start = System.nanoTime();
+//    desPut = (Put)Writables.getWritable(sb, new Put());
+//    long stop = System.nanoTime();
+//    System.out.println("timer " +(stop-start));
+    
+    assertTrue(Bytes.equals(put.getRow(), desPut.getRow()));
+    List<KeyValue> list = null;
+    List<KeyValue> desList = null;
+    for(Map.Entry<byte[], List<KeyValue>> entry : put.getFamilyMap().entrySet()){
+      assertTrue(desPut.getFamilyMap().containsKey(entry.getKey()));
+      list = entry.getValue();
+      desList = desPut.getFamilyMap().get(entry.getKey());
+      for(int i=0; i<list.size(); i++){
+        assertTrue(list.get(i).equals(desList.get(i)));
+      }
+    }
+  }
+
+  public void testDelete() throws Exception{
+    byte[] row = "row".getBytes();
+    byte[] fam = "fam".getBytes();
+    byte[] qf1 = "qf1".getBytes();
+    
+    long ts = System.currentTimeMillis();
+    byte[] val = "val".getBytes();
+    
+    Delete delete = new Delete(row);
+    delete.deleteColumn(fam, qf1, ts);
+    
+    byte[] sb = Writables.getBytes(delete);
+    Delete desDelete = (Delete)Writables.getWritable(sb, new Delete());
+
+    assertTrue(Bytes.equals(delete.getRow(), desDelete.getRow()));
+    List<KeyValue> list = null;
+    List<KeyValue> desList = null;
+    for(Map.Entry<byte[], List<KeyValue>> entry :
+        delete.getFamilyMap().entrySet()){
+      assertTrue(desDelete.getFamilyMap().containsKey(entry.getKey()));
+      list = entry.getValue();
+      desList = desDelete.getFamilyMap().get(entry.getKey());
+      for(int i=0; i<list.size(); i++){
+        assertTrue(list.get(i).equals(desList.get(i)));
+      }
+    }
+  }
+ 
+  public void testGet() throws Exception{
+    byte[] row = "row".getBytes();
+    byte[] fam = "fam".getBytes();
+    byte[] qf1 = "qf1".getBytes();
+    
+    long ts = System.currentTimeMillis();
+    byte[] val = "val".getBytes();
+    int maxVersions = 2;
+    long lockid = 5;
+    RowLock rowLock = new RowLock(lockid);
+    
+    Get get = new Get(row, rowLock);
+    get.addColumn(fam, qf1);
+    get.setTimeRange(ts, ts);
+    get.setMaxVersions(maxVersions);
+    
+    byte[] sb = Writables.getBytes(get);
+    Get desGet = (Get)Writables.getWritable(sb, new Get());
+
+    assertTrue(Bytes.equals(get.getRow(), desGet.getRow()));
+    Set<byte[]> set = null;
+    Set<byte[]> desSet = null;
+    
+    for(Map.Entry<byte[], NavigableSet<byte[]>> entry :
+        get.getFamilyMap().entrySet()){
+      assertTrue(desGet.getFamilyMap().containsKey(entry.getKey()));
+      set = entry.getValue();
+      desSet = desGet.getFamilyMap().get(entry.getKey());
+      for(byte [] qualifier : set){
+        assertTrue(desSet.contains(qualifier));
+      }
+    }
+    
+    assertEquals(get.getRowLock().getLockId(), desGet.getRowLock().getLockId());
+    assertEquals(get.getMaxVersions(), desGet.getMaxVersions());
+    TimeRange tr = get.getTimeRange();
+    TimeRange desTr = desGet.getTimeRange();
+    assertEquals(tr.getMax(), desTr.getMax());
+    assertEquals(tr.getMin(), desTr.getMin());
+  }
+  
+
+  public void testScan() throws Exception{
+    byte[] startRow = "startRow".getBytes();
+    byte[] stopRow  = "stopRow".getBytes();
+    byte[] fam = "fam".getBytes();
+    byte[] qf1 = "qf1".getBytes();
+    
+    long ts = System.currentTimeMillis();
+    byte[] val = "val".getBytes();
+    int maxVersions = 2;
+    long rowLock = 5;
+    
+    Scan scan = new Scan(startRow, stopRow);
+    scan.addColumn(fam, qf1);
+    scan.setTimeRange(ts, ts);
+    scan.setMaxVersions(maxVersions);
+    scan.setFilter(new InclusiveStopRowFilter(stopRow));
+    
+    byte[] sb = Writables.getBytes(scan);
+    Scan desScan = (Scan)Writables.getWritable(sb, new Scan());
+
+    assertTrue(Bytes.equals(scan.getStartRow(), desScan.getStartRow()));
+    assertTrue(Bytes.equals(scan.getStopRow(), desScan.getStopRow()));
+    Set<byte[]> set = null;
+    Set<byte[]> desSet = null;
+    
+    for(Map.Entry<byte[], NavigableSet<byte[]>> entry :
+        scan.getFamilyMap().entrySet()){
+      assertTrue(desScan.getFamilyMap().containsKey(entry.getKey()));
+      set = entry.getValue();
+      desSet = desScan.getFamilyMap().get(entry.getKey());
+      for(byte[] column : set){
+        assertTrue(desSet.contains(column));
+      }
+    }
+    
+    assertEquals(scan.getMaxVersions(), desScan.getMaxVersions());
+    TimeRange tr = scan.getTimeRange();
+    TimeRange desTr = desScan.getTimeRange();
+    assertEquals(tr.getMax(), desTr.getMax());
+    assertEquals(tr.getMin(), desTr.getMin());
+    
+    assertTrue(desScan.getFilter() instanceof InclusiveStopRowFilter);
+  }
+  
+  public void testTimeRange(String[] args) throws Exception{
+    TimeRange tr = new TimeRange(5,0);
+    byte [] mb = Writables.getBytes(tr);
+    TimeRange deserializedTr =
+      (TimeRange)Writables.getWritable(mb, new TimeRange());
+    
+    assertEquals(tr.getMax(), deserializedTr.getMax());
+    assertEquals(tr.getMin(), deserializedTr.getMin());
+    
+  }
+  
+  public void testKeyValue() throws Exception {
+    byte[] row = getName().getBytes();
+    byte [] col = (getName()+":col").getBytes();
+    byte[] fam = "fam".getBytes();
+    byte[] qf = "qf".getBytes();
+    long ts = System.currentTimeMillis();
+    byte[] val = "val".getBytes();
+    
+    KeyValue kv = new KeyValue(row, fam, qf, ts, val);
+//    KeyValue kv = new KeyValue(row, col);
+    
+    byte [] mb = Writables.getBytes(kv);
+    KeyValue deserializedKv =
+      (KeyValue)Writables.getWritable(mb, new KeyValue());
+    assertTrue(Bytes.equals(kv.getBuffer(), deserializedKv.getBuffer()));
+    assertEquals(kv.getOffset(), deserializedKv.getOffset());
+    assertEquals(kv.getLength(), deserializedKv.getLength());
+  }
+  
+  
+  
 }
\ No newline at end of file
Index: src/test/org/apache/hadoop/hbase/HBaseTestCase.java
===================================================================
--- src/test/org/apache/hadoop/hbase/HBaseTestCase.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/HBaseTestCase.java	(working copy)
@@ -37,7 +37,11 @@
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -284,11 +288,13 @@
             break EXIT;
           }
           try {
-            BatchUpdate batchUpdate = ts == -1 ? 
-              new BatchUpdate(t) : new BatchUpdate(t, ts);
+            Put put = new Put(t);
+            if(ts != -1) {
+              put.setTimeStamp(ts);
+            }
             try {
-              batchUpdate.put(column, t);
-              updater.commit(batchUpdate);
+              put.add(Bytes.toBytes(column), t);
+              updater.put(put);
               count++;
             } catch (RuntimeException ex) {
               ex.printStackTrace();
@@ -356,18 +362,20 @@
     public Cell[] get(byte [] row, byte [] column, long ts, int versions)
     throws IOException;
     /**
-     * @param row
-     * @param column
-     * @param ts
+     * 
+     * @param delete
+     * @param lockid
+     * @param writeToWAL
      * @throws IOException
      */
-    public void deleteAll(byte [] row, byte [] column, long ts) throws IOException;
+    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    throws IOException;
 
     /**
-     * @param batchUpdate
+     * @param put
      * @throws IOException
      */
-    public void commit(BatchUpdate batchUpdate) throws IOException;
+    public void put(Put put) throws IOException;
 
     /**
      * @param columns
@@ -393,20 +401,23 @@
       this.region = HRegion;
     }
     
-    public void commit(BatchUpdate batchUpdate) throws IOException {
-      region.batchUpdate(batchUpdate, null);
+    public void put(Put put) throws IOException {
+      region.put(put);
     }
     
-    public void deleteAll(byte [] row, byte [] column, long ts)
+    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
     throws IOException {
-      this.region.deleteAll(row, column, ts, null);
+      this.region.delete(delete, lockid, writeToWAL);
     }
-
+    
     public ScannerIncommon getScanner(byte [][] columns, byte [] firstRow,
       long ts) 
     throws IOException {
+      Scan scan = new Scan(firstRow);
+      scan.addColumns(columns);
+      scan.setTimeRange(ts, 0);
       return new 
-        InternalScannerIncommon(region.getScanner(columns, firstRow, ts, null));
+        InternalScannerIncommon(region.getScanner(firstRow, scan));
     }
 
     public Cell get(byte [] row, byte [] column) throws IOException {
@@ -455,19 +466,23 @@
       this.table = table;
     }
     
-    public void commit(BatchUpdate batchUpdate) throws IOException {
-      table.commit(batchUpdate);
+    public void put(Put put) throws IOException {
+      table.put(put);
     }
     
-    public void deleteAll(byte [] row, byte [] column, long ts)
+    
+    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
     throws IOException {
-      this.table.deleteAll(row, column, ts);
+      this.table.delete(delete);
     }
     
     public ScannerIncommon getScanner(byte [][] columns, byte [] firstRow, long ts) 
     throws IOException {
+      Scan scan = new Scan(firstRow);
+      scan.addColumns(columns);
+      scan.setTimeRange(ts, 0);
       return new 
-        ClientScannerIncommon(table.getScanner(columns, firstRow, ts, null));
+        ClientScannerIncommon(table.getScanner(scan));
     }
     
     public Cell get(byte [] row, byte [] column) throws IOException {
@@ -501,15 +516,12 @@
     
     public boolean next(List<KeyValue> values)
     throws IOException {
-      RowResult results = scanner.next();
+      Result results = scanner.next();
       if (results == null) {
         return false;
       }
       values.clear();
-      for (Map.Entry<byte [], Cell> entry : results.entrySet()) {
-        values.add(new KeyValue(results.getRow(), entry.getKey(),
-          entry.getValue().getTimestamp(), entry.getValue().getValue()));
-      }
+      values.addAll(results.list());
       return true;
     }
     
Index: src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java
===================================================================
--- src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java	(working copy)
@@ -25,6 +25,7 @@
 
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.commons.logging.Log;
@@ -126,11 +127,10 @@
 
     HRegionIncommon r = new HRegionIncommon(region);
     for(int i = firstRow; i < firstRow + nrows; i++) {
-      BatchUpdate batchUpdate = new BatchUpdate(Bytes.toBytes("row_"
+      Put put = new Put(Bytes.toBytes("row_"
           + String.format("%1$05d", i)));
-
-      batchUpdate.put(COLUMN_NAME, value.get());
-      region.batchUpdate(batchUpdate, null);
+      put.add(COLUMN_NAME, value.get());
+      region.put(put);
       if(i % 10000 == 0) {
         System.out.println("Flushing write #" + i);
         r.flushcache();
Index: src/test/org/apache/hadoop/hbase/TestEmptyMetaInfo.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestEmptyMetaInfo.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestEmptyMetaInfo.java	(working copy)
@@ -25,7 +25,10 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -44,9 +47,10 @@
       byte [] regionName = HRegionInfo.createRegionName(tableName,
         Bytes.toBytes(i == 0? "": Integer.toString(i)),
         Long.toString(System.currentTimeMillis()));
-      BatchUpdate b = new BatchUpdate(regionName);
-      b.put(HConstants.COL_SERVER, Bytes.toBytes("localhost:1234"));
-      t.commit(b);
+      Put put = new Put(regionName);
+      put.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
+          Bytes.toBytes("localhost:1234"));
+      t.put(put);
     }
     long sleepTime =
       conf.getLong("hbase.master.meta.thread.rescanfrequency", 10000);
@@ -59,11 +63,14 @@
       } catch (InterruptedException e) {
         // ignore
       }
-      Scanner scanner = t.getScanner(HConstants.ALL_META_COLUMNS, tableName);
+      Scan scan = new Scan();
+      scan.addColumns(HConstants.ALL_META_COLUMNS);
+      Scanner scanner = t.getScanner(scan);
       try {
         count = 0;
-        for (RowResult r: scanner) {
-          if (r.size() > 0) {
+        Result r;
+        while((r = scanner.next()) != null) {
+          if (!r.isEmpty()) {
             count += 1;
           }
         }
Index: src/test/org/apache/hadoop/hbase/TestHBaseCluster.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestHBaseCluster.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestHBaseCluster.java	(working copy)
@@ -29,7 +29,10 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -100,10 +103,10 @@
     // Write out a bunch of values
 
     for (int k = FIRST_ROW; k <= NUM_VALS; k++) {
-      BatchUpdate b = new BatchUpdate("row_" + k);
-      b.put(CONTENTS_BASIC, Bytes.toBytes(CONTENTSTR + k));
-      b.put(ANCHORNUM + k, Bytes.toBytes(ANCHORSTR + k));
-      table.commit(b);
+      Put put = new Put(Bytes.toBytes("row_" + k));
+      put.add(CONTENTS_BASIC, Bytes.toBytes(CONTENTSTR + k));
+      put.add(Bytes.toBytes(ANCHORNUM + k), Bytes.toBytes(ANCHORSTR + k));
+      table.put(put);
     }
     LOG.info("Write " + NUM_VALS + " rows. Elapsed time: "
         + ((System.currentTimeMillis() - startTime) / 1000.0));
@@ -147,16 +150,19 @@
     
     long startTime = System.currentTimeMillis();
     
-    Scanner s = table.getScanner(cols, HConstants.EMPTY_BYTE_ARRAY);
+    Scan scan = new Scan();
+    scan.addColumns(cols);
+    Scanner s = table.getScanner(scan);
     try {
 
       int contentsFetched = 0;
       int anchorFetched = 0;
       int k = 0;
-      for (RowResult curVals : s) {
-        for (Iterator<byte []> it = curVals.keySet().iterator(); it.hasNext(); ) {
+      for (Result curVals : s) {
+        for (Iterator<byte []> it = curVals.rowResult().keySet().iterator(); 
+        it.hasNext(); ) {
           byte [] col = it.next();
-          byte val[] = curVals.get(col).getValue();
+          byte [] val = curVals.getValue(col);
           String curval = Bytes.toString(val);
           if (Bytes.compareTo(col, CONTENTS_BASIC) == 0) {
             assertTrue("Error at:" + Bytes.toString(curVals.getRow()) 
Index: src/test/org/apache/hadoop/hbase/TestTable.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestTable.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestTable.java	(working copy)
@@ -24,6 +24,7 @@
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /** Tests table creation restrictions*/
@@ -140,10 +141,11 @@
     HTable table = new HTable(conf, getName());
     try {
       byte[] value = Bytes.toBytes("somedata");
-      BatchUpdate update = new BatchUpdate();
-      update.put(colName, value);
-      table.commit(update);
-      fail("BatchUpdate on read only table succeeded");  
+      // This used to use an empty row... That must have been a bug
+      Put put = new Put(value);
+      put.add(colName, value);
+      table.put(put);
+      fail("Put on read only table succeeded");  
     } catch (Exception e) {
       // expected
     }
Index: src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java	(working copy)
@@ -25,7 +25,10 @@
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -70,9 +73,11 @@
         HRegion.createHRegion(this.INFOS[i], this.testDir, this.conf);
       // Insert data
       for (int j = 0; j < TIMESTAMPS.length; j++) {
-        BatchUpdate b = new BatchUpdate(ROWS[i], TIMESTAMPS[j]);
-        b.put(HConstants.COLUMN_FAMILY, Bytes.toBytes(TIMESTAMPS[j]));
-        REGIONS[i].batchUpdate(b, null);
+        Put put = new Put(ROWS[i]);
+        put.setTimeStamp(TIMESTAMPS[j]);
+        put.add(HConstants.CATALOG_FAMILY, null, TIMESTAMPS[j], 
+            Bytes.toBytes(TIMESTAMPS[j]));
+        REGIONS[i].put(put);
       }
       // Insert the region we created into the meta
       HRegion.addRegionToMETA(meta, REGIONS[i]);
@@ -103,9 +108,11 @@
     
     // Case 1: scan with LATEST_TIMESTAMP. Should get two rows
     int count = 0;
-    Scanner s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY);
+    Scan scan = new Scan();
+    scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+    Scanner s = t.getScanner(scan);
     try {
-      for (RowResult rr = null; (rr = s.next()) != null;) {
+      for (Result rr = null; (rr = s.next()) != null;) {
         System.out.println(rr.toString());
         count += 1;
       }
@@ -118,8 +125,10 @@
     // (in this case > 1000 and < LATEST_TIMESTAMP. Should get 2 rows.
     
     count = 0;
-    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,
-        10000L);
+    scan = new Scan();
+    scan.setTimeRange(Long.MAX_VALUE, 1000L);
+    scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+    s = t.getScanner(scan);
     try {
       while (s.next() != null) {
         count += 1;
@@ -133,8 +142,10 @@
     // (in this case == 1000. Should get 2 rows.
     
     count = 0;
-    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,
-        1000L);
+    scan = new Scan();
+    scan.setTimeStamp(1000L);
+    scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+    s = t.getScanner(scan);
     try {
       while (s.next() != null) {
         count += 1;
@@ -148,8 +159,10 @@
     // second timestamp (100 < timestamp < 1000). Should get 2 rows.
     
     count = 0;
-    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,
-        500L);
+    scan = new Scan();
+    scan.setTimeRange(1000L, 100L);
+    scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+    s = t.getScanner(scan);
     try {
       while (s.next() != null) {
         count += 1;
@@ -163,8 +176,10 @@
     // Should get 2 rows.
     
     count = 0;
-    s = t.getScanner(HConstants.COLUMN_FAMILY_ARRAY, HConstants.EMPTY_START_ROW,
-        100L);
+    scan = new Scan();
+    scan.setTimeStamp(100L);
+    scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+    s = t.getScanner(scan);
     try {
       while (s.next() != null) {
         count += 1;
Index: src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestBloomFilters.java	(working copy)
@@ -31,6 +31,7 @@
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 
@@ -191,9 +192,9 @@
     for(int i = 0; i < 100; i++) {
       byte [] row = rows[i];
       String value = Bytes.toString(row);
-      BatchUpdate b = new BatchUpdate(row);
-      b.put(CONTENTS, value.getBytes(HConstants.UTF8_ENCODING));
-      table.commit(b);
+      Put put = new Put(row);
+      put.add(CONTENTS, value.getBytes(HConstants.UTF8_ENCODING));
+      table.put(put);
     }
     
     // Get HRegionInfo for our table
Index: src/test/org/apache/hadoop/hbase/regionserver/TestGet2.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestGet2.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestGet2.java	(working copy)
@@ -33,8 +33,11 @@
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.filter.StopRowFilter;
 import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 
@@ -45,13 +48,13 @@
 public class TestGet2 extends HBaseTestCase implements HConstants {
   private MiniDFSCluster miniHdfs;
   
-  private static final String T00 = "000";
-  private static final String T10 = "010";
-  private static final String T11 = "011";
-  private static final String T12 = "012";
-  private static final String T20 = "020";
-  private static final String T30 = "030";
-  private static final String T31 = "031";
+  private static final byte [] T00 = Bytes.toBytes("000");
+  private static final byte [] T10 = Bytes.toBytes("010");
+  private static final byte [] T11 = Bytes.toBytes("011");
+  private static final byte [] T12 = Bytes.toBytes("012");
+  private static final byte [] T20 = Bytes.toBytes("020");
+  private static final byte [] T30 = Bytes.toBytes("030");
+  private static final byte [] T31 = Bytes.toBytes("031");
 
   @Override
   protected void setUp() throws Exception {
@@ -65,7 +68,7 @@
 
   public void testGetFullMultiMapfile() throws IOException {
     HRegion region = null;
-    BatchUpdate batchUpdate = null;
+    Put put = null;
     Map<byte [], Cell> results = null;
     
     try {
@@ -77,9 +80,9 @@
       byte [] row = Bytes.toBytes("row1");
      
       // write some data
-      batchUpdate = new BatchUpdate(row);
-      batchUpdate.put(COLUMNS[0], "olderValue".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(row);
+      put.add(COLUMNS[0], "olderValue".getBytes());
+      region.put(put);
 
       // flush
       region.flushcache();
@@ -91,9 +94,9 @@
         new String(results.get(COLUMNS[0]).getValue()));
       
       // write a new value for the cell
-      batchUpdate = new BatchUpdate(row);
-      batchUpdate.put(COLUMNS[0], "newerValue".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(row);
+      put.add(COLUMNS[0], "newerValue".getBytes());
+      region.put(put);
 
       // flush
       region.flushcache();
@@ -114,10 +117,11 @@
       long now = System.currentTimeMillis();
       
       // write some data at two columns
-      batchUpdate = new BatchUpdate(row2, now);
-      batchUpdate.put(cell1, "column0 value".getBytes());
-      batchUpdate.put(cell2, "column1 value".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(row2);
+      put.setTimeStamp(now);
+      put.add(cell1, "column0 value".getBytes());
+      put.add(cell2, "column1 value".getBytes());
+      region.put(put);
       
       // flush
       region.flushcache();
@@ -130,11 +134,21 @@
       assertEquals("column1 value", new String(results.get(cell2).getValue()));
       
       // write a delete for the first column
-      batchUpdate = new BatchUpdate(row2, now);
-      batchUpdate.delete(cell1);
-      batchUpdate.put(cell2, "column1 new value".getBytes());
-      region.batchUpdate(batchUpdate, null);
-            
+
+      put = new Put(row2);
+      put.setTimeStamp(now);
+      put.add(cell2, "column1 new value".getBytes());
+      region.put(put);
+
+      Delete delete = new Delete(row2, now, null);
+      byte [][] famAndQf = KeyValue.parseColumn(cell1);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
+      
       // flush
       region.flushcache(); 
       
@@ -149,11 +163,20 @@
       //
       // Include a delete and value from the memcache in the mix
       //
-      batchUpdate = new BatchUpdate(row2, now);
-      batchUpdate.delete(cell2);
-      batchUpdate.put(cell3, "column3 value!".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(row2);
+      put.setTimeStamp(now);
+      put.add(cell3, "column3 value!".getBytes());
+      region.put(put);
 
+      delete = new Delete(row2, now, null);
+      famAndQf = KeyValue.parseColumn(cell2);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
+      
       // assert i get the third column only
       results = region.getFull(row2, (NavigableSet<byte []>)null, LATEST_TIMESTAMP, 1, null);
       assertEquals("Should have one column in the results map", 1, results.size());
@@ -185,7 +208,7 @@
       
       byte [] column = COLUMNS[0];
       for (int i = 0; i < 100; i++) {
-        addToRow(region, T00, column, i, T00.getBytes());
+        addToRow(region, T00, column, i, T00);
       }
       checkVersions(region, T00, column);
       // Flush and retry.
@@ -193,18 +216,21 @@
       checkVersions(region, T00, column);
       
       // Now delete all then retry
-      region.deleteAll(Bytes.toBytes(T00), System.currentTimeMillis(), null);
-      Cell [] cells = Cell.createSingleCellArray(region.get(Bytes.toBytes(T00), column, -1,
+      Delete delete = new Delete(T00, System.currentTimeMillis(),
+          null);
+      region.delete(delete, null, true);
+      
+      Cell [] cells = Cell.createSingleCellArray(region.get(T00, column, -1,
         HColumnDescriptor.DEFAULT_VERSIONS));
       assertTrue(cells == null);
       region.flushcache();
-      cells = Cell.createSingleCellArray(region.get(Bytes.toBytes(T00), column, -1,
+      cells = Cell.createSingleCellArray(region.get(T00, column, -1,
           HColumnDescriptor.DEFAULT_VERSIONS));
       assertTrue(cells == null);
       
       // Now add back the rows
       for (int i = 0; i < 100; i++) {
-        addToRow(region, T00, column, i, T00.getBytes());
+        addToRow(region, T00, column, i, T00);
       }
       // Run same verifications.
       checkVersions(region, T00, column);
@@ -229,54 +255,58 @@
   public void testGetClosestRowBefore2() throws IOException {
 
     HRegion region = null;
-    BatchUpdate batchUpdate = null;
+    Put put = null;
     
     try {
       HTableDescriptor htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
      
       // set up some test data
-      String t10 = "010";
-      String t20 = "020";
-      String t30 = "030";
-      String t40 = "040";
+      byte [] t10 = Bytes.toBytes("010");
+      byte [] t20 = Bytes.toBytes("020");
+      byte [] t30 = Bytes.toBytes("030");
+      byte [] t40 = Bytes.toBytes("040");
       
-      batchUpdate = new BatchUpdate(t10);
-      batchUpdate.put(COLUMNS[0], "t10 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t10);
+      put.add(COLUMNS[0], "t10 bytes".getBytes());
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t30);
-      batchUpdate.put(COLUMNS[0], "t30 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t30);
+      put.add(COLUMNS[0], "t30 bytes".getBytes());
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t40);
-      batchUpdate.put(COLUMNS[0], "t40 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t40);
+      put.add(COLUMNS[0], "t40 bytes".getBytes());
+      region.put(put);
 
       // try finding "035"
-      String t35 = "035";
-      Map<byte [], Cell> results = 
-        region.getClosestRowBefore(Bytes.toBytes(t35), COLUMNS[0]);
+      byte [] t35 = Bytes.toBytes("035");
+      Result res = region.getClosestRowBefore(t35, COLUMNS[0]); 
+      Map<byte [], Cell> results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
 
       region.flushcache();
 
       // try finding "035"
-      results = region.getClosestRowBefore(Bytes.toBytes(t35), COLUMNS[0]);
+      res = region.getClosestRowBefore(t35, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
 
-      batchUpdate = new BatchUpdate(t20);
-      batchUpdate.put(COLUMNS[0], "t20 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t20);
+      put.add(COLUMNS[0], "t20 bytes".getBytes());
+      region.put(put);
 
       // try finding "035"
-      results = region.getClosestRowBefore(Bytes.toBytes(t35), COLUMNS[0]);
+      res = region.getClosestRowBefore(t35, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
 
       region.flushcache();
 
       // try finding "035"
-      results = region.getClosestRowBefore(Bytes.toBytes(t35), COLUMNS[0]);
+      res = region.getClosestRowBefore(t35, COLUMNS[0]);
+      results = res.rowResult();
+
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
     } finally {
       if (region != null) {
@@ -290,18 +320,18 @@
     }
   }
 
-  private void addToRow(final HRegion r, final String row, final byte [] column,
-      final long ts, final byte [] bytes)
+  private void addToRow(final HRegion r, final byte [] row, 
+      final byte [] column, final long ts, final byte [] bytes)
   throws IOException {
-    BatchUpdate batchUpdate = new BatchUpdate(row, ts);
-    batchUpdate.put(column, bytes);
-    r.batchUpdate(batchUpdate, null);
+    Put put = new Put(row);
+    put.setTimeStamp(ts);
+    put.add(column, bytes);
+    r.put(put, null);
   }
 
-  private void checkVersions(final HRegion region, final String row,
+  private void checkVersions(final HRegion region, final byte [] r,
       final byte [] column)
   throws IOException {
-    byte [] r = Bytes.toBytes(row);
     Cell [] cells = Cell.createSingleCellArray(region.get(r, column, -1, 100));
     assertTrue(cells.length == HColumnDescriptor.DEFAULT_VERSIONS);
     cells = Cell.createSingleCellArray(region.get(r, column, -1, 1));
@@ -317,97 +347,145 @@
    */
   public void testGetClosestRowBefore3() throws IOException {
     HRegion region = null;
-    BatchUpdate batchUpdate = null;
+    Put put = null;
     try {
       HTableDescriptor htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
       
-      batchUpdate = new BatchUpdate(T00);
-      batchUpdate.put(COLUMNS[0], T00.getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(T00);
+      put.add(COLUMNS[0], T00);
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(T10);
-      batchUpdate.put(COLUMNS[0], T10.getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(T10);
+      put.add(COLUMNS[0], T10);
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(T20);
-      batchUpdate.put(COLUMNS[0], T20.getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(T20);
+      put.add(COLUMNS[0], T20);
+      region.put(put);
       
-      Map<byte [], Cell> results =
-        region.getClosestRowBefore(Bytes.toBytes(T20), COLUMNS[0]);
+      Result res = region.getClosestRowBefore(T20, COLUMNS[0]); 
+      Map<byte [], Cell> results = res.rowResult();
+
       assertEquals(T20, new String(results.get(COLUMNS[0]).getValue()));
       
-      batchUpdate = new BatchUpdate(T20);
-      batchUpdate.delete(COLUMNS[0]);
-      region.batchUpdate(batchUpdate, null);
+      Delete delete = new Delete(T20);
+      byte [][] famAndQf = KeyValue.parseColumn(COLUMNS[0]);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
       
-      results = region.getClosestRowBefore(Bytes.toBytes(T20), COLUMNS[0]);
+      res = region.getClosestRowBefore(T20, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
       
-      batchUpdate = new BatchUpdate(T30);
-      batchUpdate.put(COLUMNS[0], T30.getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(T30);
+      put.add(COLUMNS[0], T30);
+      region.put(put);
       
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T30, new String(results.get(COLUMNS[0]).getValue()));
       
-      batchUpdate = new BatchUpdate(T30);
-      batchUpdate.delete(COLUMNS[0]);
-      region.batchUpdate(batchUpdate, null);
+      delete = new Delete(T30);
+      famAndQf = KeyValue.parseColumn(COLUMNS[0]);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
+      
 
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
 
       region.flushcache();
 
       // try finding "010" after flush
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
       
       // Put into a different column family.  Should make it so I still get t10
-      batchUpdate = new BatchUpdate(T20);
-      batchUpdate.put(COLUMNS[1], T20.getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(T20);
+      put.add(COLUMNS[1], T20);
+      region.put(put);
       
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
+      
       region.flushcache();
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
       
       // Now try combo of memcache and mapfiles.  Delete the t20 COLUMS[1]
       // in memory; make sure we get back t10 again.
-      batchUpdate = new BatchUpdate(T20);
-      batchUpdate.delete(COLUMNS[1]);
-      region.batchUpdate(batchUpdate, null);
-      results = region.getClosestRowBefore(Bytes.toBytes(T30), COLUMNS[0]);
+      delete = new Delete(T20);
+      famAndQf = KeyValue.parseColumn(COLUMNS[1]);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
+      
+      
+      res = region.getClosestRowBefore(T30, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
       
       // Ask for a value off the end of the file.  Should return t10.
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
+      
       region.flushcache();
-      results = region.getClosestRowBefore(Bytes.toBytes(T31), COLUMNS[0]);
+      
+      res = region.getClosestRowBefore(T31, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T10, new String(results.get(COLUMNS[0]).getValue()));
       
       // Ok.  Let the candidate come out of mapfiles but have delete of
       // the candidate be in memory.
-      batchUpdate = new BatchUpdate(T11);
-      batchUpdate.put(COLUMNS[0], T11.getBytes());
-      region.batchUpdate(batchUpdate, null);
-      batchUpdate = new BatchUpdate(T10);
-      batchUpdate.delete(COLUMNS[0]);
-      region.batchUpdate(batchUpdate, null);
-      results = region.getClosestRowBefore(Bytes.toBytes(T12), COLUMNS[0]);
+      put = new Put(T11);
+      put.add(COLUMNS[0], T11);
+      region.put(put);
+      
+      delete = new Delete(T10);
+      famAndQf = KeyValue.parseColumn(COLUMNS[0]);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
+      
+      res = region.getClosestRowBefore(T12, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(T11, new String(results.get(COLUMNS[0]).getValue()));
     } finally {
       if (region != null) {
@@ -440,9 +518,9 @@
       final byte [] arbitraryStopRow = Bytes.toBytes("c");
       byte [] actualStopRow = null;
       byte [][] columns = {COLFAMILY_NAME1};
-      scanner = region.getScanner(columns,
-          arbitraryStartRow, HConstants.LATEST_TIMESTAMP,
-          new WhileMatchRowFilter(new StopRowFilter(arbitraryStopRow)));
+      Scan scan = new Scan(arbitraryStartRow, arbitraryStopRow);
+      scan.addColumns(columns);
+      scanner = region.getScanner(arbitraryStartRow, scan);
       List<KeyValue> value = new ArrayList<KeyValue>();
       while (scanner.next(value)) { 
         if (actualStartRow == null) {
@@ -488,25 +566,27 @@
       long right_now = System.currentTimeMillis();
       long one_second_ago = right_now - 1000;
       
-      String t = "test_row";
-      BatchUpdate batchUpdate = new BatchUpdate(t, one_second_ago);
-      batchUpdate.put(COLUMNS[0], "old text".getBytes());
-      region_incommon.commit(batchUpdate);
+      byte [] t = Bytes.toBytes("test_row");
+      Put put = new Put(t);
+      put.setTimeStamp(one_second_ago);
+      put.add(COLUMNS[0], Bytes.toBytes("old text"));
+      region_incommon.put(put);
  
-      batchUpdate = new BatchUpdate(t, right_now);
-      batchUpdate.put(COLUMNS[0], "new text".getBytes());
-      region_incommon.commit(batchUpdate);
+      put = new Put(t);
+      put.setTimeStamp(right_now);
+      put.add(COLUMNS[0], Bytes.toBytes("new text"));
+      region_incommon.put(put);
 
-      assertCellEquals(region, Bytes.toBytes(t), COLUMNS[0],
+      assertCellEquals(region, t, COLUMNS[0],
         right_now, "new text");
-      assertCellEquals(region, Bytes.toBytes(t), COLUMNS[0],
+      assertCellEquals(region, t, COLUMNS[0],
         one_second_ago, "old text");
       
       // Force a flush so store files come into play.
       region_incommon.flushcache();
 
-      assertCellEquals(region, Bytes.toBytes(t), COLUMNS[0], right_now, "new text");
-      assertCellEquals(region, Bytes.toBytes(t), COLUMNS[0], one_second_ago, "old text");
+      assertCellEquals(region, t, COLUMNS[0], right_now, "new text");
+      assertCellEquals(region, t, COLUMNS[0], one_second_ago, "old text");
 
     } finally {
       if (region != null) {
@@ -527,80 +607,94 @@
   public void testGetClosestRowBefore() throws IOException{
 
     HRegion region = null;
-    BatchUpdate batchUpdate = null;
+    Put put = null;
     
     try {
       HTableDescriptor htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
      
       // set up some test data
-      String t10 = "010";
-      String t20 = "020";
-      String t30 = "030";
-      String t35 = "035";
-      String t40 = "040";
+      byte [] t10 = Bytes.toBytes("010");
+      byte [] t20 = Bytes.toBytes("020");
+      byte [] t30 = Bytes.toBytes("030");
+      byte [] t35 = Bytes.toBytes("035");
+      byte [] t40 = Bytes.toBytes("040");
       
-      batchUpdate = new BatchUpdate(t10);
-      batchUpdate.put(COLUMNS[0], "t10 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t10);
+      put.add(COLUMNS[0], Bytes.toBytes("t10 bytes"));
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t20);
-      batchUpdate.put(COLUMNS[0], "t20 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t20);
+      put.add(COLUMNS[0], Bytes.toBytes("t20 bytes"));
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t30);
-      batchUpdate.put(COLUMNS[0], "t30 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t30);
+      put.add(COLUMNS[0], Bytes.toBytes("t30 bytes"));
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t35);
-      batchUpdate.put(COLUMNS[0], "t35 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
+      put = new Put(t35);
+      put.add(COLUMNS[0], Bytes.toBytes("t35 bytes"));
+      region.put(put);
       
-      batchUpdate = new BatchUpdate(t35);
-      batchUpdate.delete(COLUMNS[0]);
-      region.batchUpdate(batchUpdate, null);
+      Delete delete = new Delete(t35);
+      byte [][] famAndQf = KeyValue.parseColumn(COLUMNS[0]);
+      if(famAndQf[1].length == 0){
+        delete.deleteFamily(famAndQf[0]);
+      } else {
+        delete.deleteColumn(famAndQf[0], famAndQf[1]);
+      }
+      region.delete(delete, null, true);
       
-      batchUpdate = new BatchUpdate(t40);
-      batchUpdate.put(COLUMNS[0], "t40 bytes".getBytes());
-      region.batchUpdate(batchUpdate, null);
       
+      put = new Put(t40);
+      put.add(COLUMNS[0], "t40 bytes".getBytes());
+      region.put(put);
+      
       // try finding "015"
-      String t15 = "015";
-      Map<byte [], Cell> results = 
-        region.getClosestRowBefore(Bytes.toBytes(t15), COLUMNS[0]);
+      byte [] t15 = Bytes.toBytes("015");
+      
+      Result res = region.getClosestRowBefore(t15, COLUMNS[0]);
+      Map<byte [], Cell>  results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t10 bytes");
 
       // try "020", we should get that row exactly
-      results = region.getClosestRowBefore(Bytes.toBytes(t20), COLUMNS[0]);
+      res = region.getClosestRowBefore(t20, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t20 bytes");
       
       // try "038", should skip deleted "035" and get "030"
-      String t38 = "038";
-      results = region.getClosestRowBefore(Bytes.toBytes(t38), COLUMNS[0]);
+      byte [] t38 = Bytes.toBytes("038");
+      res = region.getClosestRowBefore(t38, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
       
       // try "050", should get stuff from "040"
-      String t50 = "050";
-      results = region.getClosestRowBefore(Bytes.toBytes(t50), COLUMNS[0]);
+      byte [] t50 = Bytes.toBytes("050");
+      res = region.getClosestRowBefore(t50, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t40 bytes");
 
       // force a flush
       region.flushcache();
 
       // try finding "015"
-      results = region.getClosestRowBefore(Bytes.toBytes(t15), COLUMNS[0]);
+      res = region.getClosestRowBefore(t15, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t10 bytes");
 
       // try "020", we should get that row exactly
-      results = region.getClosestRowBefore(Bytes.toBytes(t20), COLUMNS[0]);
+      res = region.getClosestRowBefore(t20, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t20 bytes");
 
       // try "038", should skip deleted "035" and get "030"
-      results = region.getClosestRowBefore(Bytes.toBytes(t38), COLUMNS[0]);
+      res = region.getClosestRowBefore(t38, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t30 bytes");
 
       // try "050", should get stuff from "040"
-      results = region.getClosestRowBefore(Bytes.toBytes(t50), COLUMNS[0]);
+      res = region.getClosestRowBefore(t50, COLUMNS[0]);
+      results = res.rowResult();
       assertEquals(new String(results.get(COLUMNS[0]).getValue()), "t40 bytes");
     } finally {
       if (region != null) {
@@ -628,11 +722,11 @@
       
       // write a row with a bunch of columns
       byte [] row = Bytes.toBytes("some_row");
-      BatchUpdate bu = new BatchUpdate(row);
-      bu.put(COLUMNS[0], "column 0".getBytes());
-      bu.put(COLUMNS[1], "column 1".getBytes());
-      bu.put(COLUMNS[2], "column 2".getBytes());
-      region.batchUpdate(bu, null);
+      Put put = new Put(row);
+      put.add(COLUMNS[0], Bytes.toBytes("column 0"));
+      put.add(COLUMNS[1], Bytes.toBytes("column 1"));
+      put.add(COLUMNS[2], Bytes.toBytes("column 2"));
+      region.put(put);
       
       assertSpecifiedColumns(region, row);
       // try it again with a cache flush to involve the store, not just the 
Index: src/test/org/apache/hadoop/hbase/regionserver/TestDeleteFamily.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestDeleteFamily.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestDeleteFamily.java	(working copy)
@@ -24,7 +24,10 @@
 import org.apache.hadoop.hbase.HBaseTestCase;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 
@@ -63,10 +66,6 @@
       makeSureItWorks(region, region_incommon, false);
       // test hstore
       makeSureItWorks(region, region_incommon, true);
-      // family regex test memcache
-      makeSureRegexWorks(region, region_incommon, false);
-      // family regex test hstore
-      makeSureRegexWorks(region, region_incommon, true);
       
     } finally {
       if (region != null) {
@@ -93,30 +92,37 @@
     byte [] colB = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "b");
     byte [] colC = Bytes.toBytes(Bytes.toString(COLUMNS[1]) + "c");
 
-    BatchUpdate batchUpdate = null;
-    batchUpdate = new BatchUpdate(row, t0);
-    batchUpdate.put(colA, cellData(0, flush).getBytes());
-    batchUpdate.put(colB, cellData(0, flush).getBytes());
-    batchUpdate.put(colC, cellData(0, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
+    Put put = new Put(row);
+    put.setTimeStamp(t0);
+    put.add(colA, cellData(0, flush).getBytes());
+    put.add(colB, cellData(0, flush).getBytes());
+    put.add(colC, cellData(0, flush).getBytes());      
+    region_incommon.put(put);
 
-    batchUpdate = new BatchUpdate(row, t1);
-    batchUpdate.put(colA, cellData(1, flush).getBytes());
-    batchUpdate.put(colB, cellData(1, flush).getBytes());
-    batchUpdate.put(colC, cellData(1, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
+    put = new Put(row);
+    put.setTimeStamp(t1);
+    put.add(colA, cellData(1, flush).getBytes());
+    put.add(colB, cellData(1, flush).getBytes());
+    put.add(colC, cellData(1, flush).getBytes());      
+    region_incommon.put(put);
     
-    batchUpdate = new BatchUpdate(row, t2);
-    batchUpdate.put(colA, cellData(2, flush).getBytes());
-    batchUpdate.put(colB, cellData(2, flush).getBytes());
-    batchUpdate.put(colC, cellData(2, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
+    put = new Put(row);
+    put.setTimeStamp(t2);
+    put.add(colA, cellData(2, flush).getBytes());
+    put.add(colB, cellData(2, flush).getBytes());
+    put.add(colC, cellData(2, flush).getBytes());      
+    region_incommon.put(put);
 
     if (flush) {region_incommon.flushcache();}
 
     // call delete family at a timestamp, make sure only the most recent stuff
     // for column c is left behind
-    region.deleteFamily(row, COLUMNS[0], t1, null);
+    Delete delete = new Delete(row, t1, null);
+    byte [][] famAndQf = KeyValue.parseColumn(COLUMNS[0]);
+    delete.deleteFamily(famAndQf[0]);
+    
+    region.delete(delete, null, true);
+    
     if (flush) {region_incommon.flushcache();}
     // most recent for A,B,C should be fine
     // A,B at older timestamps should be gone
@@ -133,7 +139,11 @@
 
     // call delete family w/o a timestamp, make sure nothing is left except for
     // column C.
-    region.deleteFamily(row, COLUMNS[0], HConstants.LATEST_TIMESTAMP, null);
+    delete = new Delete(row, HConstants.LATEST_TIMESTAMP, null);
+    delete.deleteFamily(famAndQf[0]);
+    
+    region.delete(delete, null, true);
+    
     if (flush) {region_incommon.flushcache();}
     // A,B for latest timestamp should be gone
     // C should still be fine
@@ -145,71 +155,6 @@
     
   }
   
-  private void makeSureRegexWorks(HRegion region, HRegionIncommon region_incommon, 
-      boolean flush)
-    throws Exception{
-      // insert a few versions worth of data for a row
-      byte [] row = Bytes.toBytes("test_row");
-      long t0 = System.currentTimeMillis();
-      long t1 = t0 - 15000;
-      long t2 = t1 - 15000;
-
-      byte [] colA = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "a");
-      byte [] colB = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "b");
-      byte [] colC = Bytes.toBytes(Bytes.toString(COLUMNS[1]) + "c");
-
-      BatchUpdate batchUpdate = null;
-      batchUpdate = new BatchUpdate(row, t0);
-      batchUpdate.put(colA, cellData(0, flush).getBytes());
-      batchUpdate.put(colB, cellData(0, flush).getBytes());
-      batchUpdate.put(colC, cellData(0, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-
-      batchUpdate = new BatchUpdate(row, t1);
-      batchUpdate.put(colA, cellData(1, flush).getBytes());
-      batchUpdate.put(colB, cellData(1, flush).getBytes());
-      batchUpdate.put(colC, cellData(1, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-      
-      batchUpdate = new BatchUpdate(row, t2);
-      batchUpdate.put(colA, cellData(2, flush).getBytes());
-      batchUpdate.put(colB, cellData(2, flush).getBytes());
-      batchUpdate.put(colC, cellData(2, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-
-      if (flush) {region_incommon.flushcache();}
-
-      // call delete family at a timestamp, make sure only the most recent stuff
-      // for column c is left behind
-      region.deleteFamilyByRegex(row, COLFAMILY_REGEX, t1, null);
-      if (flush) {region_incommon.flushcache();}
-      // most recent for A,B,C should be fine
-      // A,B at older timestamps should be gone
-      // C should be fine for older timestamps
-      assertCellEquals(region, row, colA, t0, cellData(0, flush));
-      assertCellEquals(region, row, colA, t1, null);    
-      assertCellEquals(region, row, colA, t2, null);
-      assertCellEquals(region, row, colB, t0, cellData(0, flush));
-      assertCellEquals(region, row, colB, t1, null);
-      assertCellEquals(region, row, colB, t2, null);    
-      assertCellEquals(region, row, colC, t0, cellData(0, flush));
-      assertCellEquals(region, row, colC, t1, cellData(1, flush));
-      assertCellEquals(region, row, colC, t2, cellData(2, flush));        
-
-      // call delete family w/o a timestamp, make sure nothing is left except for
-      // column C.
-      region.deleteFamilyByRegex(row, COLFAMILY_REGEX, HConstants.LATEST_TIMESTAMP, null);
-      if (flush) {region_incommon.flushcache();}
-      // A,B for latest timestamp should be gone
-      // C should still be fine
-      assertCellEquals(region, row, colA, t0, null);
-      assertCellEquals(region, row, colB, t0, null);
-      assertCellEquals(region, row, colC, t0, cellData(0, flush));
-      assertCellEquals(region, row, colC, t1, cellData(1, flush));
-      assertCellEquals(region, row, colC, t2, cellData(2, flush));        
-      
-    }
-  
   private String cellData(int tsNum, boolean flush){
     return "t" + tsNum + " data" + (flush ? " - with flush" : "");
   }
Index: src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestLogRolling.java	(working copy)
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -112,10 +113,9 @@
     HTable table = new HTable(conf, tableName);
 
     for (int i = 1; i <= 256; i++) {    // 256 writes should cause 8 log rolls
-      BatchUpdate b =
-        new BatchUpdate("row" + String.format("%1$04d", i));
-      b.put(HConstants.COLUMN_FAMILY, value);
-      table.commit(b);
+      Put put = new Put(Bytes.toBytes("row" + String.format("%1$04d", i)));
+      put.add(HConstants.COLUMN_FAMILY, value);
+      table.put(put);
 
       if (i % 32 == 0) {
         // After every 32 writes sleep to let the log roller run
Index: src/test/org/apache/hadoop/hbase/regionserver/TestDeleteCompare.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestDeleteCompare.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestDeleteCompare.java	(revision 0)
@@ -0,0 +1,255 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.regionserver.DeleteCompare.DeleteCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import junit.framework.TestCase;
+
+public class TestDeleteCompare extends TestCase {
+
+  private final boolean PRINT = true;
+  
+
+  
+  
+  //Cases to compare:
+  //1. DeleteFamily and whatever of the same row
+  //2. DeleteColumn and whatever of the same row + qualifier
+  //3. Delete and the matching put
+  //4. Big test that include starting on the wrong row and qualifier
+  public void testDeleteCompare_DeleteFamily() {
+    byte [] row1 = Bytes.toBytes("row11");
+    byte [] row2 = Bytes.toBytes("row21");
+    byte [] fam1 = Bytes.toBytes("fam");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] data = Bytes.toBytes("data");
+    
+    long ts1 = System.nanoTime();
+    long ts2 = System.nanoTime();
+    long ts3 = System.nanoTime();
+    
+    //Creating memcache
+    Set<KeyValue> memcache = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
+    memcache.add(new KeyValue(row1, fam1, col1, ts3, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts2, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts1, data));
+    memcache.add(new KeyValue(row1, fam1, col2, ts1, data));
+    memcache.add(new KeyValue(row1, fam1, col3, ts3, data));
+    memcache.add(new KeyValue(row1, fam1, col3, ts2, data));
+    memcache.add(new KeyValue(row1, fam1, col3, ts1, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts1, data));
+    
+    //Creating expected result
+    List<DeleteCode> expected = new ArrayList<DeleteCode>();
+    expected.add(DeleteCode.SKIP);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.SKIP);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DONE);
+    
+    KeyValue delete = new KeyValue(row1, null, col1, ts2,
+        KeyValue.Type.DeleteFamily, null);
+    byte [] deleteBuffer = delete.getBuffer();
+    int deleteRowOffset = delete.getRowOffset();
+    short deleteRowLen = delete.getRowLength();
+    int deleteQualifierOffset = delete.getQualifierOffset();
+    int deleteQualifierLen = delete.getQualifierLength();
+    int deleteTimestampOffset = deleteQualifierOffset + deleteQualifierLen;
+    byte deleteType = deleteBuffer[deleteTimestampOffset +Bytes.SIZEOF_LONG];
+    
+    List<DeleteCode> actual = new ArrayList<DeleteCode>();
+    for(KeyValue mem : memcache){
+    actual.add(DeleteCompare.deleteCompare(mem, deleteBuffer, deleteRowOffset,
+        deleteRowLen, deleteQualifierOffset, deleteQualifierLen,
+        deleteTimestampOffset, deleteType));
+      
+    }
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT) {
+        System.out.println("expected " + expected.get(i) +
+            ", actual " + actual.get(i));
+      }
+    }
+  }
+  
+  public void testDeleteCompare_DeleteColumn() {
+    byte [] row1 = Bytes.toBytes("row11");
+    byte [] row2 = Bytes.toBytes("row21");
+    byte [] fam1 = Bytes.toBytes("fam");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] data = Bytes.toBytes("data");
+    
+    long ts1 = System.nanoTime();
+    long ts2 = System.nanoTime();
+    long ts3 = System.nanoTime();
+    
+    //Creating memcache
+    Set<KeyValue> memcache = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
+    memcache.add(new KeyValue(row1, fam1, col1, ts3, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts2, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts1, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts1, data));
+    
+    //Creating expected result
+    List<DeleteCode> expected = new ArrayList<DeleteCode>();
+    expected.add(DeleteCode.SKIP);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DONE);
+    
+    KeyValue delete = new KeyValue(row1, null, col1, ts2,
+        KeyValue.Type.DeleteColumn, null);
+    byte [] deleteBuffer = delete.getBuffer();
+    int deleteRowOffset = delete.getRowOffset();
+    short deleteRowLen = delete.getRowLength();
+    int deleteQualifierOffset = delete.getQualifierOffset();
+    int deleteQualifierLen = delete.getQualifierLength();
+    int deleteTimestampOffset = deleteQualifierOffset + deleteQualifierLen;
+    byte deleteType = deleteBuffer[deleteTimestampOffset +Bytes.SIZEOF_LONG];
+    
+    List<DeleteCode> actual = new ArrayList<DeleteCode>();
+    for(KeyValue mem : memcache){
+    actual.add(DeleteCompare.deleteCompare(mem, deleteBuffer, deleteRowOffset,
+        deleteRowLen, deleteQualifierOffset, deleteQualifierLen,
+        deleteTimestampOffset, deleteType));
+      
+    }
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT) {
+        System.out.println("expected " + expected.get(i) +
+            ", actual " + actual.get(i));
+      }
+    }
+  }
+  
+  
+  public void testDeleteCompare_Delete() {
+    byte [] row1 = Bytes.toBytes("row11");
+    byte [] fam1 = Bytes.toBytes("fam");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] data = Bytes.toBytes("data");
+    
+    long ts1 = System.nanoTime();
+    long ts2 = System.nanoTime();
+    long ts3 = System.nanoTime();
+    
+    //Creating memcache
+    Set<KeyValue> memcache = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
+    memcache.add(new KeyValue(row1, fam1, col1, ts3, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts2, data));
+    memcache.add(new KeyValue(row1, fam1, col1, ts1, data));
+    
+    //Creating expected result
+    List<DeleteCode> expected = new ArrayList<DeleteCode>();
+    expected.add(DeleteCode.SKIP);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DONE);
+    
+    KeyValue delete = new KeyValue(row1, null, col1, ts2,
+        KeyValue.Type.Delete, null);
+    byte [] deleteBuffer = delete.getBuffer();
+    int deleteRowOffset = delete.getRowOffset();
+    short deleteRowLen = delete.getRowLength();
+    int deleteQualifierOffset = delete.getQualifierOffset();
+    int deleteQualifierLen = delete.getQualifierLength();
+    int deleteTimestampOffset = deleteQualifierOffset + deleteQualifierLen;
+    byte deleteType = deleteBuffer[deleteTimestampOffset +Bytes.SIZEOF_LONG];
+    
+    List<DeleteCode> actual = new ArrayList<DeleteCode>();
+    for(KeyValue mem : memcache){
+    actual.add(DeleteCompare.deleteCompare(mem, deleteBuffer, deleteRowOffset,
+        deleteRowLen, deleteQualifierOffset, deleteQualifierLen,
+        deleteTimestampOffset, deleteType));
+      
+    }
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT) {
+        System.out.println("expected " + expected.get(i) +
+            ", actual " + actual.get(i));
+      }
+    }
+  }
+  
+  public void testDeleteCompare_Multiple() {
+    byte [] row1 = Bytes.toBytes("row11");
+    byte [] row2 = Bytes.toBytes("row21");
+    byte [] row3 = Bytes.toBytes("row31");
+    byte [] fam1 = Bytes.toBytes("fam");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] data = Bytes.toBytes("data");
+    
+    long ts1 = System.nanoTime();
+    long ts2 = System.nanoTime();
+    long ts3 = System.nanoTime();
+    long ts4 = System.nanoTime();
+    long ts5 = System.nanoTime();
+    
+    //Creating memcache
+    Set<KeyValue> memcache = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
+    memcache.add(new KeyValue(row1, fam1, col1, ts1, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts4, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts3, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts2, data));
+    memcache.add(new KeyValue(row2, fam1, col1, ts1,
+        KeyValue.Type.Delete, data));
+    memcache.add(new KeyValue(row3, fam1, col1, ts1, data));
+    
+    //Creating expected result
+    List<DeleteCode> expected = new ArrayList<DeleteCode>();
+    expected.add(DeleteCode.SKIP);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DELETE);
+    expected.add(DeleteCode.DONE);
+    
+    KeyValue delete = new KeyValue(row2, null, col1, ts5,
+        KeyValue.Type.DeleteColumn, null);
+    byte [] deleteBuffer = delete.getBuffer();
+    int deleteRowOffset = delete.getRowOffset();
+    short deleteRowLen = delete.getRowLength();
+    int deleteQualifierOffset = delete.getQualifierOffset();
+    int deleteQualifierLen = delete.getQualifierLength();
+    int deleteTimestampOffset = deleteQualifierOffset + deleteQualifierLen;
+    byte deleteType = deleteBuffer[deleteTimestampOffset +Bytes.SIZEOF_LONG];
+    
+    List<DeleteCode> actual = new ArrayList<DeleteCode>();
+    for(KeyValue mem : memcache){
+    actual.add(DeleteCompare.deleteCompare(mem, deleteBuffer, deleteRowOffset,
+        deleteRowLen, deleteQualifierOffset, deleteQualifierLen,
+        deleteTimestampOffset, deleteType));
+      
+    }
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT) {
+        System.out.println("expected " + expected.get(i) +
+            ", actual " + actual.get(i));
+      }
+    }
+  }
+  
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestQueryMatcher.java	(revision 0)
@@ -0,0 +1,150 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.NavigableSet;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.TimeRange;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+
+
+public class TestQueryMatcher extends HBaseTestCase
+implements HConstants {
+  private final boolean PRINT = false;
+  
+  private byte [] row1;
+  private byte [] row2;
+  private byte [] fam1;
+  private byte [] fam2;
+  private byte [] col1;
+  private byte [] col2;
+  private byte [] col3;
+  private byte [] col4;
+  private byte [] col5;
+  private byte [] col6;
+
+  private byte [] data;
+
+  private Get get;
+
+  long ttl = Long.MAX_VALUE;
+  KeyComparator rowComparator;
+
+  public void setUp(){
+    row1 = Bytes.toBytes("row1");
+    row2 = Bytes.toBytes("row2");
+    fam1 = Bytes.toBytes("fam1");
+    fam2 = Bytes.toBytes("fam2");
+    col1 = Bytes.toBytes("col1");
+    col2 = Bytes.toBytes("col2");
+    col3 = Bytes.toBytes("col3");
+    col4 = Bytes.toBytes("col4");
+    col5 = Bytes.toBytes("col5");
+    col6 = Bytes.toBytes("col6");
+
+    data = Bytes.toBytes("data");
+
+    //Create Get
+    get = new Get(row1);
+    get.addFamily(fam1);
+    get.addColumn(fam2, col2);
+    get.addColumn(fam2, col4);
+    get.addColumn(fam2, col5);
+
+    rowComparator = KeyValue.KEY_COMPARATOR;
+
+  }
+
+  public void testMatch_ExplicitColumns() 
+  throws IOException {
+    //Moving up from the Tracker by using Gets and List<KeyValue> instead
+    //of just byte [] 
+
+    //Expected result
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+
+    QueryMatcher qm = new QueryMatcher(get, get.getRow(), fam2,
+        get.getFamilyMap().get(fam2), ttl, rowComparator);
+
+    List<KeyValue> memCache = new ArrayList<KeyValue>();
+    memCache.add(new KeyValue(row1, fam2, col1, data));
+    memCache.add(new KeyValue(row1, fam2, col2, data));
+    memCache.add(new KeyValue(row1, fam2, col3, data));
+    memCache.add(new KeyValue(row1, fam2, col4, data));
+    memCache.add(new KeyValue(row1, fam2, col5, data));
+
+    memCache.add(new KeyValue(row2, fam1, col1, data));
+
+    List<MatchCode> actual = new ArrayList<MatchCode>();
+
+    for(KeyValue kv : memCache){
+      actual.add(qm.match(kv));
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected "+expected.get(i)+ 
+            ", actual " +actual.get(i));
+      }
+    }
+  }
+
+
+  public void testMatch_Wildcard() 
+  throws IOException {
+    //Moving up from the Tracker by using Gets and List<KeyValue> instead
+    //of just byte [] 
+
+    //Expected result
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.NEXT);
+
+    QueryMatcher qm = new QueryMatcher(get, get.getRow(), fam2, null,
+        ttl, rowComparator);
+
+    List<KeyValue> memCache = new ArrayList<KeyValue>();
+    memCache.add(new KeyValue(row1, fam2, col1, data));
+    memCache.add(new KeyValue(row1, fam2, col2, data));
+    memCache.add(new KeyValue(row1, fam2, col3, data));
+    memCache.add(new KeyValue(row1, fam2, col4, data));
+    memCache.add(new KeyValue(row1, fam2, col5, data));
+    memCache.add(new KeyValue(row2, fam1, col1, data));
+
+    List<MatchCode> actual = new ArrayList<MatchCode>();
+
+    for(KeyValue kv : memCache){
+      actual.add(qm.match(kv));
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected "+expected.get(i)+ 
+            ", actual " +actual.get(i));
+      }
+    }
+  }
+
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHMemcache.java	(working copy)
@@ -34,10 +34,12 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.regionserver.HRegion.Counter;
+import org.apache.hadoop.hbase.regionserver.Memcache.MemcacheScanner;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /** memcache test case */
 public class TestHMemcache extends TestCase {
+  
   private Memcache hmemcache;
 
   private static final int ROW_COUNT = 10;
@@ -218,7 +220,7 @@
     int i = 0;
     for (KeyValue kv: kvs) {
       String expectedColname = Bytes.toString(getColumnName(rowIndex, i++));
-      String colnameStr = kv.getColumnString();
+      String colnameStr = Bytes.toString(kv.getColumn());
       assertEquals("Column name", colnameStr, expectedColname);
       // Value is column name as bytes.  Usually result is
       // 100 bytes in size at least. This is the default size
@@ -340,10 +342,10 @@
         columns.add(getColumnName(i, ii));
       }
     }
-    InternalScanner scanner =
-      this.hmemcache.getScanner(timestamp, columns, HConstants.EMPTY_START_ROW);
+    MemcacheScanner scanner = (MemcacheScanner)this.hmemcache.getScanner();
+    scanner.seek(KeyValue.LOWESTKEY);
     List<KeyValue> results = new ArrayList<KeyValue>();
-    for (int i = 0; scanner.next(results); i++) {
+    for (int i = 0; scanner.getNextRow(results); i++) {
       KeyValue.COMPARATOR.compareRows(results.get(0), getRowName(i));
       assertEquals("Count of columns", COLUMNS_COUNT, results.size());
       isExpectedRowWithoutTimestamps(i, results);
@@ -436,10 +438,11 @@
     }
     //starting from each row, validate results should contain the starting row
     for (int startRowId = 0; startRowId < ROW_COUNT; startRowId++) {
-      InternalScanner scanner = this.hmemcache.getScanner(timestamp,
-          cols, getRowName(startRowId));
+      
+      MemcacheScanner scanner = (MemcacheScanner)this.hmemcache.getScanner();
+      scanner.seek(new KeyValue(getRowName(startRowId), Long.MAX_VALUE));
       List<KeyValue> results = new ArrayList<KeyValue>();
-      for (int i = 0; scanner.next(results); i++) {
+      for (int i = 0; scanner.getNextRow(results); i++) {
         int rowId = startRowId + i;
         assertTrue("Row name",
           KeyValue.COMPARATOR.compareRows(results.get(0),
Index: src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestExplicitColumnTracker.java	(revision 0)
@@ -0,0 +1,144 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestExplicitColumnTracker extends HBaseTestCase
+implements HConstants {
+  private boolean PRINT = false; 
+  
+  public void testGet_SingleVersion(){
+    if(PRINT){
+      System.out.println("SingleVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    //Looking for every other
+    columns.add(col2);
+    columns.add(col4);
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+    int maxVersions = 1;
+    
+    ColumnTracker exp = new ExplicitColumnTracker(columns, maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  public void testGet_MultiVersion(){
+    if(PRINT){
+      System.out.println("\nMultiVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    TreeSet<byte[]> columns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
+    //Looking for every other
+    columns.add(col2);
+    columns.add(col4);
+    
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+    expected.add(MatchCode.SKIP);
+
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.DONE);
+
+    expected.add(MatchCode.DONE);
+    expected.add(MatchCode.DONE);
+    expected.add(MatchCode.DONE);
+    int maxVersions = 2;
+    
+    ColumnTracker exp = new ExplicitColumnTracker(columns, maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col5);
+    scanner.add(col5);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestGetDeleteTracker.java	(revision 0)
@@ -0,0 +1,293 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.regionserver.GetDeleteTracker.Delete;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestGetDeleteTracker extends HBaseTestCase implements HConstants {
+  
+  private final boolean PRINT = true;
+  
+  private byte [] col1 = null;
+  private byte [] col2 = null;
+  
+  private int col1Len = 0;
+  private int col2Len = 0;
+
+  private byte [] empty = null;
+  
+  private long ts1 = 0L;
+  private long ts2 = 0L;
+  private long ts3 = 0L;
+  
+  
+  private Delete del10 = null;
+  private Delete del11 = null;
+  private Delete delQf10 = null;
+  private Delete delQf11 = null;
+  private Delete delFam10 = null;
+  
+  private Delete del20 = null;
+  private Delete del21 = null;
+  private Delete delQf20 = null;
+  private Delete delQf21 = null;
+  private Delete delFam20 = null;
+  
+  
+  private Delete del30 = null;
+  
+  GetDeleteTracker dt = null;
+  private byte del = KeyValue.Type.Delete.getCode();
+  private byte delCol = KeyValue.Type.DeleteColumn.getCode();
+  private byte delFam = KeyValue.Type.DeleteFamily.getCode();
+
+  protected void setUp() throws Exception {
+    dt = new GetDeleteTracker();
+    col1 = "col".getBytes();
+    col2 = "col2".getBytes();
+    col1Len = col1.length;
+    col2Len = col2.length;
+    
+    empty = new byte[0];
+
+    //ts1
+    ts1 = System.nanoTime();
+    del10 = dt.new Delete(col1, 0, col1Len, del, ts1);
+    del11 = dt.new Delete(col2, 0, col2Len, del, ts1);
+    delQf10 = dt.new Delete(col1, 0, col1Len, delCol, ts1);
+    delQf11 = dt.new Delete(col2, 0, col2Len, delCol, ts1);
+    delFam10 = dt.new Delete(empty, 0, 0, delFam, ts1);
+    
+    //ts2
+    ts2 = System.nanoTime();
+    del20 = dt.new Delete(col1, 0, col1Len, del, ts2);
+    del21 = dt.new Delete(col2, 0, col2Len, del, ts2);
+    delQf20 = dt.new Delete(col1, 0, col1Len, delCol, ts2);
+    delQf21 = dt.new Delete(col2, 0, col2Len, delCol, ts2);
+    delFam20 = dt.new Delete(empty, 0, 0, delFam, ts1);
+    
+    //ts3
+    ts3 = System.nanoTime();
+    del30 = dt.new Delete(col1, 0, col1Len, del, ts3);
+  }
+  
+  public void testUpdate_CompareDeletes() {
+    GetDeleteTracker.DeleteCompare res = null;
+    
+    
+    //Testing Delete and Delete
+    res = dt.compareDeletes(del10, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    //Testing Delete qf1 and Delete qf2 and <==> 
+    res = dt.compareDeletes(del10, del11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(del11, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+        
+    //Testing Delete ts1 and Delete ts2 and <==> 
+    res = dt.compareDeletes(del10, del20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    res = dt.compareDeletes(del20, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    
+    
+    
+    //Testing DeleteColumn and DeleteColumn
+    res = dt.compareDeletes(delQf10, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    //Testing DeleteColumn qf1 and DeleteColumn qf2 and <==> 
+    res = dt.compareDeletes(delQf10, delQf11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf11, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    
+    //Testing DeleteColumn ts1 and DeleteColumn ts2 and <==> 
+    res = dt.compareDeletes(delQf10, delQf20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_BOTH, res);
+    res = dt.compareDeletes(delQf20, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_BOTH, res);
+    
+    
+    
+    //Testing Delete and DeleteColumn and <==> 
+    res = dt.compareDeletes(del10, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_OLD, res);
+    res = dt.compareDeletes(delQf10, del10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_NEW, res);
+
+    //Testing Delete qf1 and DeleteColumn qf2 and <==> 
+    res = dt.compareDeletes(del10, delQf11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf11, del10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    
+    //Testing Delete qf2 and DeleteColumn qf1 and <==> 
+    res = dt.compareDeletes(del11, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+    res = dt.compareDeletes(delQf10, del11);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    
+    //Testing Delete ts2 and DeleteColumn ts1 and <==> 
+    res = dt.compareDeletes(del20, delQf10);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_OLD_NEXT_OLD, res);
+    res = dt.compareDeletes(delQf10, del20);
+    assertEquals(DeleteTracker.DeleteCompare.INCLUDE_NEW_NEXT_NEW, res);
+ 
+    //Testing Delete ts1 and DeleteColumn ts2 and <==> 
+    res = dt.compareDeletes(del10, delQf20);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_OLD, res);
+    res = dt.compareDeletes(delQf20, del10);
+    assertEquals(DeleteTracker.DeleteCompare.NEXT_NEW, res);
+    
+  }
+  
+  public void testUpdate(){
+    //Building lists
+    List<Delete> dels1 = new ArrayList<Delete>();
+    dels1.add(delQf10);
+    dels1.add(del21);
+    
+    List<Delete> dels2 = new ArrayList<Delete>();
+    dels2.add(delFam10);
+    dels2.add(del30);
+    dels2.add(delQf20);
+    
+    List<Delete> res = new ArrayList<Delete>();
+    res.add(del30);
+    res.add(delQf20);
+    res.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels1){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    //Check deleteList
+    List<Delete> delList = dt.deletes;
+    assertEquals(dels1.size(), delList.size());
+    for(int i=0; i<dels1.size(); i++){
+      assertEquals(0, Bytes.compareTo(dels1.get(i).buffer,
+          delList.get(i).buffer));
+      assertEquals(dels1.get(i).qualifierOffset, delList.get(i).qualifierOffset);
+      assertEquals(dels1.get(i).qualifierLength, delList.get(i).qualifierLength);
+      assertEquals(dels1.get(i).timestamp, delList.get(i).timestamp);
+      assertEquals(dels1.get(i).type, delList.get(i).type);
+    }
+    
+    //Add more entries
+    for(Delete del : dels2){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    //Update()
+    dt.update();
+    
+    //Check deleteList
+    delList = dt.deletes;
+
+    for(int i=0; i<res.size(); i++){
+      assertEquals(0, Bytes.compareTo(res.get(i).buffer,
+          delList.get(i).buffer));
+      assertEquals(res.get(i).qualifierOffset, delList.get(i).qualifierOffset);
+      assertEquals(res.get(i).qualifierLength, delList.get(i).qualifierLength);
+      assertEquals(res.get(i).timestamp, delList.get(i).timestamp);
+      assertEquals(res.get(i).type, delList.get(i).type);
+      if(PRINT){
+        System.out.println("Qf " +new String(delList.get(i).buffer) + 
+            ", timestamp, " +delList.get(i).timestamp+ 
+            ", type " +KeyValue.Type.codeToType(delList.get(i).type));
+      }
+    }
+    
+  }
+  
+  /**
+   * Test if a KeyValue is in the lists of deletes already. Cases that needs to
+   * be tested are:
+   * Not deleted
+   * Deleted by a Delete
+   * Deleted by a DeleteColumn
+   * Deleted by a DeleteFamily
+   */
+  public void testIsDeleted_NotDeleted(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delQf10);
+    dels.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(false, dt.isDeleted(col2, 0, col2Len, ts3));
+    assertEquals(false, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  public void testIsDeleted_Delete(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(del21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength, 
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts2));
+  }
+  
+  public void testIsDeleted_DeleteColumn(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delQf21);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  
+  public void testIsDeleted_DeleteFamily(){
+    //Building lists
+    List<Delete> dels = new ArrayList<Delete>();
+    dels.add(delFam20);
+    
+    //Adding entries
+    for(Delete del : dels){
+      dt.add(del.buffer, del.qualifierOffset, del.qualifierLength,
+          del.timestamp, del.type);
+    }
+    
+    //update()
+    dt.update();
+    
+    assertEquals(true, dt.isDeleted(col2, 0, col2Len, ts1));
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestGet.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestGet.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestGet.java	(working copy)
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.io.Cell;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 
 /** Test case for get */
 public class TestGet extends HBaseTestCase {
@@ -100,20 +101,22 @@
       
       // Write information to the table
 
-      BatchUpdate batchUpdate = null;
-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());
-      batchUpdate.put(CONTENTS, CONTENTS);
-      batchUpdate.put(HConstants.COL_REGIONINFO, 
+      Put put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
+      put.add(CONTENTS, CONTENTS);
+      put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
           Writables.getBytes(HRegionInfo.ROOT_REGIONINFO));
-      r.commit(batchUpdate);
+      r.put(put);
       
-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());
-      batchUpdate.put(HConstants.COL_SERVER, 
-        Bytes.toBytes(new HServerAddress(SERVER_ADDRESS).toString()));
-      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(12345));
-      batchUpdate.put(Bytes.toString(HConstants.COLUMN_FAMILY) +
-        "region", Bytes.toBytes("region"));
-      r.commit(batchUpdate);
+      put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
+      put.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
+          Bytes.toBytes(new HServerAddress(SERVER_ADDRESS).toString()));
+      put.add(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER,
+          Bytes.toBytes(12345));
+      put.add(HConstants.CATALOG_FAMILY, Bytes.toBytes("region"),
+          Bytes.toBytes("region"));
+      r.put(put);
       
       // Verify that get works the same from memcache as when reading from disk
       // NOTE dumpRegion won't work here because it only reads from disk.
@@ -132,15 +135,17 @@
       
       // Update one family member and add a new one
       
-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());
-      batchUpdate.put(Bytes.toString(HConstants.COLUMN_FAMILY) + "region",
-        "region2".getBytes(HConstants.UTF8_ENCODING));
       String otherServerName = "bar.foo.com:4321";
-      batchUpdate.put(HConstants.COL_SERVER, 
-        Bytes.toBytes(new HServerAddress(otherServerName).toString()));
-      batchUpdate.put(Bytes.toString(HConstants.COLUMN_FAMILY) + "junk",
-        "junk".getBytes(HConstants.UTF8_ENCODING));
-      r.commit(batchUpdate);
+      
+      put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
+      put.add(HConstants.CATALOG_FAMILY, Bytes.toBytes("region"),
+          Bytes.toBytes("region2"));
+      put.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
+          Bytes.toBytes(new HServerAddress(otherServerName).toString()));
+      put.add(HConstants.CATALOG_FAMILY, Bytes.toBytes("junk"),
+          Bytes.toBytes("junk"));
+      r.put(put);
 
       verifyGet(r, otherServerName);
       
Index: src/test/org/apache/hadoop/hbase/regionserver/TestAtomicIncrement.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestAtomicIncrement.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestAtomicIncrement.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 public class TestAtomicIncrement extends HBaseClusterTestCase {
@@ -81,9 +82,9 @@
       // set a weird value, then increment:
       row = Bytes.toBytes("foo2");
       byte [] value = {0,0,2};
-      BatchUpdate bu = new BatchUpdate(row);
-      bu.put(column, value);
-      table.commit(bu);
+      Put put = new Put(row);
+      put.add(column, value);
+      table.put(put);
       
       assertEquals(3L, table.incrementColumnValue(row, column, 1));
 
@@ -94,9 +95,9 @@
 
       row = Bytes.toBytes("foo3");
       byte[] value2 = {1,2,3,4,5,6,7,8,9};
-      bu = new BatchUpdate(row);
-      bu.put(column, value2);
-      table.commit(bu);
+      put = new Put(row);
+      put.add(column, value2);
+      table.put(put);
       
       try {
         table.incrementColumnValue(row, column, 1);
Index: src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestTransactionalHLogManager.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestTransactionalHLogManager.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestTransactionalHLogManager.java	(working copy)
@@ -1,308 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.regionserver.HLog;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/** JUnit test case for HLog */
-public class DisabledTestTransactionalHLogManager extends HBaseTestCase implements
-    HConstants {
-  private Path dir;
-  private MiniDFSCluster cluster;
-
-  final byte[] tableName = Bytes.toBytes("tablename");
-  final HTableDescriptor tableDesc = new HTableDescriptor(tableName);
-  final HRegionInfo regionInfo = new HRegionInfo(tableDesc,
-      HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
-  final byte[] row1 = Bytes.toBytes("row1");
-  final byte[] val1 = Bytes.toBytes("val1");
-  final byte[] row2 = Bytes.toBytes("row2");
-  final byte[] val2 = Bytes.toBytes("val2");
-  final byte[] row3 = Bytes.toBytes("row3");
-  final byte[] val3 = Bytes.toBytes("val3");
-  final byte[] col = Bytes.toBytes("col:A");
-
-  @Override
-  public void setUp() throws Exception {
-    cluster = new MiniDFSCluster(conf, 2, true, (String[]) null);
-    // Set the hbase.rootdir to be the home directory in mini dfs.
-    this.conf.set(HConstants.HBASE_DIR, this.cluster.getFileSystem()
-        .getHomeDirectory().toString());
-    super.setUp();
-    this.dir = new Path("/hbase", getName());
-    if (fs.exists(dir)) {
-      fs.delete(dir, true);
-    }
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    if (this.fs.exists(this.dir)) {
-      this.fs.delete(this.dir, true);
-    }
-    shutdownDfs(cluster);
-    super.tearDown();
-  }
-
-  /**
-   * @throws IOException
-   */
-  public void testSingleCommit() throws IOException {
-
-    HLog log = new HLog(fs, dir, this.conf, null);
-    TransactionalHLogManager logMangaer = new TransactionalHLogManager(log, fs,
-        regionInfo, conf);
-
-    // Write columns named 1, 2, 3, etc. and then values of single byte
-    // 1, 2, 3...
-    long transactionId = 1;
-    logMangaer.writeStartToLog(transactionId);
-
-    BatchUpdate update1 = new BatchUpdate(row1);
-    update1.put(col, val1);
-    logMangaer.writeUpdateToLog(transactionId, update1);
-
-    BatchUpdate update2 = new BatchUpdate(row2);
-    update2.put(col, val2);
-    logMangaer.writeUpdateToLog(transactionId, update2);
-
-    BatchUpdate update3 = new BatchUpdate(row3);
-    update3.put(col, val3);
-    logMangaer.writeUpdateToLog(transactionId, update3);
-
-    logMangaer.writeCommitToLog(transactionId);
-
-    // log.completeCacheFlush(regionName, tableName, logSeqId);
-
-    log.close();
-    Path filename = log.computeFilename(log.getFilenum());
-
-    Map<Long, List<BatchUpdate>> commits = logMangaer.getCommitsFromLog(
-        filename, -1, null);
-
-    assertEquals(1, commits.size());
-    assertTrue(commits.containsKey(transactionId));
-    assertEquals(3, commits.get(transactionId).size());
-
-    List<BatchUpdate> updates = commits.get(transactionId);
-
-    update1 = updates.get(0);
-    assertTrue(Bytes.equals(row1, update1.getRow()));
-    assertTrue(Bytes.equals(val1, update1.iterator().next().getValue()));
-
-    update2 = updates.get(1);
-    assertTrue(Bytes.equals(row2, update2.getRow()));
-    assertTrue(Bytes.equals(val2, update2.iterator().next().getValue()));
-
-    update3 = updates.get(2);
-    assertTrue(Bytes.equals(row3, update3.getRow()));
-    assertTrue(Bytes.equals(val3, update3.iterator().next().getValue()));
-
-  }
-  
-  /**
-   * @throws IOException
-   */
-  public void testSingleAbort() throws IOException {
-
-    HLog log = new HLog(fs, dir, this.conf, null);
-    TransactionalHLogManager logMangaer = new TransactionalHLogManager(log, fs,
-        regionInfo, conf);
-
-    long transactionId = 1;
-    logMangaer.writeStartToLog(transactionId);
-
-    BatchUpdate update1 = new BatchUpdate(row1);
-    update1.put(col, val1);
-    logMangaer.writeUpdateToLog(transactionId, update1);
-
-    BatchUpdate update2 = new BatchUpdate(row2);
-    update2.put(col, val2);
-    logMangaer.writeUpdateToLog(transactionId, update2);
-
-    BatchUpdate update3 = new BatchUpdate(row3);
-    update3.put(col, val3);
-    logMangaer.writeUpdateToLog(transactionId, update3);
-
-    logMangaer.writeAbortToLog(transactionId);
-
-    // log.completeCacheFlush(regionName, tableName, logSeqId);
-
-    log.close();
-    Path filename = log.computeFilename(log.getFilenum());
-
-    Map<Long, List<BatchUpdate>> commits = logMangaer.getCommitsFromLog(
-        filename, -1, null);
-
-    assertEquals(0, commits.size());
-  }
-  
-  /**
-   * @throws IOException
-   */
-  public void testInterlievedCommits() throws IOException {
-
-    HLog log = new HLog(fs, dir, this.conf, null);
-    TransactionalHLogManager logMangaer = new TransactionalHLogManager(log, fs,
-        regionInfo, conf);
-
-    long transaction1Id = 1;
-    long transaction2Id = 2;
-    logMangaer.writeStartToLog(transaction1Id);
-
-    BatchUpdate update1 = new BatchUpdate(row1);
-    update1.put(col, val1);
-    logMangaer.writeUpdateToLog(transaction1Id, update1);
-
-    logMangaer.writeStartToLog(transaction2Id);
-    
-    BatchUpdate update2 = new BatchUpdate(row2);
-    update2.put(col, val2);
-    logMangaer.writeUpdateToLog(transaction2Id, update2);
-
-    BatchUpdate update3 = new BatchUpdate(row3);
-    update3.put(col, val3);
-    logMangaer.writeUpdateToLog(transaction1Id, update3);
-
-    logMangaer.writeCommitToLog(transaction2Id);
-    logMangaer.writeCommitToLog(transaction1Id);
-
-    // log.completeCacheFlush(regionName, tableName, logSeqId);
-
-    log.close();
-    Path filename = log.computeFilename(log.getFilenum());
-
-    Map<Long, List<BatchUpdate>> commits = logMangaer.getCommitsFromLog(
-        filename, -1, null);
-
-    assertEquals(2, commits.size());
-    assertEquals(2, commits.get(transaction1Id).size());
-    assertEquals(1, commits.get(transaction2Id).size());
-  }
-  
-  /**
-   * @throws IOException
-   */
-  public void testInterlievedAbortCommit() throws IOException {
-
-    HLog log = new HLog(fs, dir, this.conf, null);
-    TransactionalHLogManager logMangaer = new TransactionalHLogManager(log, fs,
-        regionInfo, conf);
-
-    long transaction1Id = 1;
-    long transaction2Id = 2;
-    logMangaer.writeStartToLog(transaction1Id);
-
-    BatchUpdate update1 = new BatchUpdate(row1);
-    update1.put(col, val1);
-    logMangaer.writeUpdateToLog(transaction1Id, update1);
-
-    logMangaer.writeStartToLog(transaction2Id);
-    
-    BatchUpdate update2 = new BatchUpdate(row2);
-    update2.put(col, val2);
-    logMangaer.writeUpdateToLog(transaction2Id, update2);
-
-    logMangaer.writeAbortToLog(transaction2Id);
-    
-    BatchUpdate update3 = new BatchUpdate(row3);
-    update3.put(col, val3);
-    logMangaer.writeUpdateToLog(transaction1Id, update3);
-
-    logMangaer.writeCommitToLog(transaction1Id);
-
-    // log.completeCacheFlush(regionName, tableName, logSeqId);
-
-    log.close();
-    Path filename = log.computeFilename(log.getFilenum());
-
-    Map<Long, List<BatchUpdate>> commits = logMangaer.getCommitsFromLog(
-        filename, -1, null);
-
-    assertEquals(1, commits.size());
-    assertEquals(2, commits.get(transaction1Id).size());
-  }
-  
-  /**
-   * @throws IOException
-   */
-  public void testInterlievedCommitAbort() throws IOException {
-
-    HLog log = new HLog(fs, dir, this.conf, null);
-    TransactionalHLogManager logMangaer = new TransactionalHLogManager(log, fs,
-        regionInfo, conf);
-
-    long transaction1Id = 1;
-    long transaction2Id = 2;
-    logMangaer.writeStartToLog(transaction1Id);
-
-    BatchUpdate update1 = new BatchUpdate(row1);
-    update1.put(col, val1);
-    logMangaer.writeUpdateToLog(transaction1Id, update1);
-
-    logMangaer.writeStartToLog(transaction2Id);
-    
-    BatchUpdate update2 = new BatchUpdate(row2);
-    update2.put(col, val2);
-    logMangaer.writeUpdateToLog(transaction2Id, update2);
-
-    logMangaer.writeCommitToLog(transaction2Id);
-    
-    BatchUpdate update3 = new BatchUpdate(row3);
-    update3.put(col, val3);
-    logMangaer.writeUpdateToLog(transaction1Id, update3);
-
-    logMangaer.writeAbortToLog(transaction1Id);
-
-    // log.completeCacheFlush(regionName, tableName, logSeqId);
-
-    log.close();
-    Path filename = log.computeFilename(log.getFilenum());
-
-    Map<Long, List<BatchUpdate>> commits = logMangaer.getCommitsFromLog(
-        filename, -1, null);
-
-    assertEquals(1, commits.size());
-    assertEquals(1, commits.get(transaction2Id).size());
-  }
-  
-  // FIXME Cannot do this test without a global transacton manager
-  // public void testMissingCommit() {
-  // fail();
-  // }
-
-  // FIXME Cannot do this test without a global transacton manager
-  // public void testMissingAbort() {
-  // fail();
-  // }
-
-}
Index: src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestHLogRecovery.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestHLogRecovery.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/transactional/DisabledTestHLogRecovery.java	(working copy)
@@ -1,279 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.LocalHBaseCluster;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.Scanner;
-import org.apache.hadoop.hbase.client.transactional.CommitUnsuccessfulException;
-import org.apache.hadoop.hbase.client.transactional.TransactionManager;
-import org.apache.hadoop.hbase.client.transactional.TransactionState;
-import org.apache.hadoop.hbase.client.transactional.TransactionalTable;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.util.Bytes;
-
-public class DisabledTestHLogRecovery extends HBaseClusterTestCase {
-  protected static final Log LOG = LogFactory.getLog(DisabledTestHLogRecovery.class);
-
-  private static final String TABLE_NAME = "table1";
-
-  private static final byte[] FAMILY = Bytes.toBytes("family:");
-  static final byte[] COL_A = Bytes.toBytes("family:a");
-
-  private static final byte[] ROW1 = Bytes.toBytes("row1");
-  private static final byte[] ROW2 = Bytes.toBytes("row2");
-  private static final byte[] ROW3 = Bytes.toBytes("row3");
-  private static final int TOTAL_VALUE = 10;
-
-  private HBaseAdmin admin;
-  private TransactionManager transactionManager;
-  private TransactionalTable table;
-
-  /** constructor */
-  public DisabledTestHLogRecovery() {
-    super(2, false);
-
-    conf.set(HConstants.REGION_SERVER_CLASS, TransactionalRegionInterface.class
-        .getName());
-    conf.set(HConstants.REGION_SERVER_IMPL, TransactionalRegionServer.class
-        .getName());
-
-    // Set flush params so we don't get any
-    // FIXME (defaults are probably fine)
-
-    // Copied from TestRegionServerExit
-    conf.setInt("ipc.client.connect.max.retries", 5); // reduce ipc retries
-    conf.setInt("ipc.client.timeout", 10000); // and ipc timeout
-    conf.setInt("hbase.client.pause", 10000); // increase client timeout
-    conf.setInt("hbase.client.retries.number", 10); // increase HBase retries
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    FileSystem.getLocal(conf).delete(new Path(conf.get(HConstants.HBASE_DIR)), true);
-    super.setUp();
-
-    HTableDescriptor desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(FAMILY));
-    admin = new HBaseAdmin(conf);
-    admin.createTable(desc);
-    table = new TransactionalTable(conf, desc.getName());
-
-    transactionManager = new TransactionManager(conf);
-    writeInitalRows();
-  }
-
-  private void writeInitalRows() throws IOException {
-    BatchUpdate update = new BatchUpdate(ROW1);
-    update.put(COL_A, Bytes.toBytes(TOTAL_VALUE));
-    table.commit(update);
-    update = new BatchUpdate(ROW2);
-    update.put(COL_A, Bytes.toBytes(0));
-    table.commit(update);
-    update = new BatchUpdate(ROW3);
-    update.put(COL_A, Bytes.toBytes(0));
-    table.commit(update);
-  }
-
-  public void testWithoutFlush() throws IOException,
-      CommitUnsuccessfulException {
-    writeInitalRows();
-    TransactionState state1 = makeTransaction(false);
-    transactionManager.tryCommit(state1);
-    stopOrAbortRegionServer(true);
-
-    Thread t = startVerificationThread(1);
-    t.start();
-    threadDumpingJoin(t);
-  }
-
-  public void testWithFlushBeforeCommit() throws IOException,
-      CommitUnsuccessfulException {
-    writeInitalRows();
-    TransactionState state1 = makeTransaction(false);
-    flushRegionServer();
-    transactionManager.tryCommit(state1);
-    stopOrAbortRegionServer(true);
-
-    Thread t = startVerificationThread(1);
-    t.start();
-    threadDumpingJoin(t);
-  }
-
-  // FIXME, TODO
-  // public void testWithFlushBetweenTransactionWrites() {
-  // fail();
-  // }
-
-  private void flushRegionServer() {
-    List<LocalHBaseCluster.RegionServerThread> regionThreads = cluster
-        .getRegionThreads();
-
-    HRegion region = null;
-    int server = -1;
-    for (int i = 0; i < regionThreads.size() && server == -1; i++) {
-      HRegionServer s = regionThreads.get(i).getRegionServer();
-      Collection<HRegion> regions = s.getOnlineRegions();
-      for (HRegion r : regions) {
-        if (Bytes.equals(r.getTableDesc().getName(), Bytes.toBytes(TABLE_NAME))) {
-          server = i;
-          region = r;
-        }
-      }
-    }
-    if (server == -1) {
-      LOG.fatal("could not find region server serving table region");
-      fail();
-    }
-    ((TransactionalRegionServer) regionThreads.get(server).getRegionServer())
-        .getFlushRequester().request(region);
-  }
-
-  /**
-   * Stop the region server serving TABLE_NAME.
-   * 
-   * @param abort set to true if region server should be aborted, if false it is
-   * just shut down.
-   */
-  private void stopOrAbortRegionServer(final boolean abort) {
-    List<LocalHBaseCluster.RegionServerThread> regionThreads = cluster
-        .getRegionThreads();
-
-    int server = -1;
-    for (int i = 0; i < regionThreads.size(); i++) {
-      HRegionServer s = regionThreads.get(i).getRegionServer();
-      Collection<HRegion> regions = s.getOnlineRegions();
-      LOG.info("server: " + regionThreads.get(i).getName());
-      for (HRegion r : regions) {
-        LOG.info("region: " + r.getRegionInfo().getRegionNameAsString());
-        if (Bytes.equals(r.getTableDesc().getName(), Bytes.toBytes(TABLE_NAME))) {
-          server = i;
-        }
-      }
-    }
-    if (server == -1) {
-      LOG.fatal("could not find region server serving table region");
-      fail();
-    }
-    if (abort) {
-      this.cluster.abortRegionServer(server);
-
-    } else {
-      this.cluster.stopRegionServer(server, false);
-    }
-    LOG.info(this.cluster.waitOnRegionServer(server) + " has been "
-        + (abort ? "aborted" : "shut down"));
-  }
-
-  protected void verify(final int numRuns) throws IOException {
-    // Reads
-    int row1 = Bytes.toInt(table.get(ROW1, COL_A).getValue());
-    int row2 = Bytes.toInt(table.get(ROW2, COL_A).getValue());
-    int row3 = Bytes.toInt(table.get(ROW3, COL_A).getValue());
-
-    assertEquals(TOTAL_VALUE - 2 * numRuns, row1);
-    assertEquals(numRuns, row2);
-    assertEquals(numRuns, row3);
-  }
-
-  // Move 2 out of ROW1 and 1 into ROW2 and 1 into ROW3
-  private TransactionState makeTransaction(final boolean flushMidWay)
-      throws IOException {
-    TransactionState transactionState = transactionManager.beginTransaction();
-
-    // Reads
-    int row1 = Bytes.toInt(table.get(transactionState, ROW1, COL_A).getValue());
-    int row2 = Bytes.toInt(table.get(transactionState, ROW2, COL_A).getValue());
-    int row3 = Bytes.toInt(table.get(transactionState, ROW3, COL_A).getValue());
-
-    row1 -= 2;
-    row2 += 1;
-    row3 += 1;
-
-    if (flushMidWay) {
-      flushRegionServer();
-    }
-
-    // Writes
-    BatchUpdate write = new BatchUpdate(ROW1);
-    write.put(COL_A, Bytes.toBytes(row1));
-    table.commit(transactionState, write);
-
-    write = new BatchUpdate(ROW2);
-    write.put(COL_A, Bytes.toBytes(row2));
-    table.commit(transactionState, write);
-
-    write = new BatchUpdate(ROW3);
-    write.put(COL_A, Bytes.toBytes(row3));
-    table.commit(transactionState, write);
-
-    return transactionState;
-  }
-
-  /*
-   * Run verification in a thread so I can concurrently run a thread-dumper
-   * while we're waiting (because in this test sometimes the meta scanner looks
-   * to be be stuck). @param tableName Name of table to find. @param row Row we
-   * expect to find. @return Verification thread. Caller needs to calls start on
-   * it.
-   */
-  private Thread startVerificationThread(final int numRuns) {
-    Runnable runnable = new Runnable() {
-      public void run() {
-        try {
-          // Now try to open a scanner on the meta table. Should stall until
-          // meta server comes back up.
-          HTable t = new HTable(conf, TABLE_NAME);
-          Scanner s = t.getScanner(new byte[][] { COL_A },
-              HConstants.EMPTY_START_ROW);
-          s.close();
-
-        } catch (IOException e) {
-          LOG.fatal("could not re-open meta table because", e);
-          fail();
-        }
-        try {
-          verify(numRuns);
-          LOG.info("Success!");
-        } catch (Exception e) {
-          e.printStackTrace();
-          fail();
-        }
-      }
-    };
-    return new Thread(runnable);
-  }
-}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestKeyValueHeap.java	(revision 0)
@@ -0,0 +1,207 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+//import java.util.Random;
+import java.util.Set;
+//import java.util.SortedSet;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestKeyValueHeap extends HBaseTestCase
+implements HConstants {
+  private final boolean PRINT = false;
+  
+  List<Scanner> scanners = new ArrayList<Scanner>();
+
+  private byte [] row1;
+  private byte [] fam1;
+  private byte [] col1;
+  private byte [] data;
+
+  private byte [] row2;
+  private byte [] fam2;
+  private byte [] col2;
+
+  private byte [] col3;
+  private byte [] col4;
+  private byte [] col5;
+
+  public void setUp(){
+    data = Bytes.toBytes("data");
+
+    row1 = Bytes.toBytes("row1");
+    fam1 = Bytes.toBytes("fam1");
+    col1 = Bytes.toBytes("col1");
+
+    row2 = Bytes.toBytes("row2");
+    fam2 = Bytes.toBytes("fam2");
+    col2 = Bytes.toBytes("col2");
+
+    col3 = Bytes.toBytes("col3");
+    col4 = Bytes.toBytes("col4");
+    col5 = Bytes.toBytes("col5");
+  }
+
+  public void testSorted(){
+    //Cases that need to be checked are:
+    //1. The "smallest" KeyValue is in the same scanners as current
+    //2. Current scanner gets empty
+
+    List<KeyValue> l1 = new ArrayList<KeyValue>();
+    l1.add(new KeyValue(row1, fam1, col5, data));
+    l1.add(new KeyValue(row2, fam1, col1, data));
+    l1.add(new KeyValue(row2, fam1, col2, data));
+    scanners.add(new Scanner(l1));
+
+    List<KeyValue> l2 = new ArrayList<KeyValue>();
+    l2.add(new KeyValue(row1, fam1, col1, data));
+    l2.add(new KeyValue(row1, fam1, col2, data));
+    scanners.add(new Scanner(l2));
+
+    List<KeyValue> l3 = new ArrayList<KeyValue>();
+    l3.add(new KeyValue(row1, fam1, col3, data));
+    l3.add(new KeyValue(row1, fam1, col4, data));
+    l3.add(new KeyValue(row1, fam2, col1, data));
+    l3.add(new KeyValue(row1, fam2, col2, data));
+    l3.add(new KeyValue(row2, fam1, col3, data));
+    scanners.add(new Scanner(l3));
+
+    List<KeyValue> expected = new ArrayList<KeyValue>();
+    expected.add(new KeyValue(row1, fam1, col1, data));
+    expected.add(new KeyValue(row1, fam1, col2, data));
+    expected.add(new KeyValue(row1, fam1, col3, data));
+    expected.add(new KeyValue(row1, fam1, col4, data));
+    expected.add(new KeyValue(row1, fam1, col5, data));
+    expected.add(new KeyValue(row1, fam2, col1, data));
+    expected.add(new KeyValue(row1, fam2, col2, data));
+    expected.add(new KeyValue(row2, fam1, col1, data));
+    expected.add(new KeyValue(row2, fam1, col2, data));
+    expected.add(new KeyValue(row2, fam1, col3, data));
+
+    //Creating KeyValueHeap
+    KeyValueHeap kvh =
+      new KeyValueHeap(scanners.toArray(new Scanner[0]), KeyValue.COMPARATOR);
+    
+    List<KeyValue> actual = new ArrayList<KeyValue>();
+    while(kvh.peek() != null){
+      actual.add(kvh.next());
+    }
+
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected " +expected.get(i)+
+            "\nactual   " +actual.get(i) +"\n");
+      }
+    }
+    
+    //Check if result is sorted according to Comparator
+    for(int i=0; i<actual.size()-1; i++){
+      int ret = KeyValue.COMPARATOR.compare(actual.get(i), actual.get(i+1));
+      assertTrue(ret < 0);
+    }
+    
+  }
+
+  public void testSeek(){
+    //Cases:
+    //1. Seek KeyValue that is not in scanner
+    //2. Check that smallest that is returned from a seek is correct
+    
+    List<KeyValue> l1 = new ArrayList<KeyValue>();
+    l1.add(new KeyValue(row1, fam1, col5, data));
+    l1.add(new KeyValue(row2, fam1, col1, data));
+    l1.add(new KeyValue(row2, fam1, col2, data));
+    scanners.add(new Scanner(l1));
+
+    List<KeyValue> l2 = new ArrayList<KeyValue>();
+    l2.add(new KeyValue(row1, fam1, col1, data));
+    l2.add(new KeyValue(row1, fam1, col2, data));
+    scanners.add(new Scanner(l2));
+
+    List<KeyValue> l3 = new ArrayList<KeyValue>();
+    l3.add(new KeyValue(row1, fam1, col3, data));
+    l3.add(new KeyValue(row1, fam1, col4, data));
+    l3.add(new KeyValue(row1, fam2, col1, data));
+    l3.add(new KeyValue(row1, fam2, col2, data));
+    l3.add(new KeyValue(row2, fam1, col3, data));
+    scanners.add(new Scanner(l3));
+
+    List<KeyValue> expected = new ArrayList<KeyValue>();
+    expected.add(new KeyValue(row2, fam1, col1, data));
+    
+    //Creating KeyValueHeap
+    KeyValueHeap kvh =
+      new KeyValueHeap(scanners.toArray(new Scanner[0]), KeyValue.COMPARATOR);
+    
+    KeyValue seekKv = new KeyValue(row2, fam1, null, null);
+    kvh.seek(seekKv);
+    
+    List<KeyValue> actual = new ArrayList<KeyValue>();
+    actual.add(kvh.peek());
+    
+    assertEquals(expected.size(), actual.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), actual.get(i));
+      if(PRINT){
+        System.out.println("expected " +expected.get(i)+
+            "\nactual   " +actual.get(i) +"\n");
+      }
+    }
+    
+  }
+
+  private class Scanner implements KeyValueScanner {
+    private Set<KeyValue> scan =
+      new TreeSet<KeyValue>((Comparator)KeyValue.COMPARATOR);
+    private Iterator<KeyValue> iter;
+    private KeyValue current;
+
+    public Scanner(List<KeyValue> list) {
+      Collections.sort(list, (Comparator)KeyValue.COMPARATOR);
+      iter = list.iterator();
+      if(iter.hasNext()){
+        current = iter.next();
+      } 
+    }
+    
+    public KeyValue peek() {
+      return current;
+    }
+
+    public KeyValue next() {
+      KeyValue oldCurrent = current;
+      if(iter.hasNext()){
+        current = iter.next();
+      } else {
+        current = null;
+      }
+      return oldCurrent;
+    }
+
+    public void close(){}
+    
+    public boolean seek(KeyValue seekKv) {
+      while(iter.hasNext()){
+        KeyValue next = iter.next();
+        int ret = KeyValue.COMPARATOR.compare(next, seekKv);
+        if(ret >= 0){
+          current = next;
+          return true;
+        }
+      }
+      return false;
+    }
+  }
+
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestScanDeleteTracker.java	(revision 0)
@@ -0,0 +1,79 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+public class TestScanDeleteTracker extends HBaseTestCase implements HConstants {
+
+  private ScanDeleteTracker sdt;
+  private long timestamp = 10L;
+  private byte deleteType = 0;
+  
+  public void setUp(){
+    sdt = new ScanDeleteTracker();
+  }
+  
+  public void testDeletedBy_Delete() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.Delete.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDeletedBy_DeleteColumn() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDeletedBy_DeleteFamily() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteFamily.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  public void testDelete_DeleteColumn() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.Delete.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    timestamp -= 5;
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+  
+  public void testDeleteColumn_Delete() {
+    byte [] qualifier = Bytes.toBytes("qualifier");
+    deleteType = KeyValue.Type.DeleteColumn.getCode();
+    
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    qualifier = Bytes.toBytes("qualifier1");
+    deleteType = KeyValue.Type.Delete.getCode();
+    sdt.add(qualifier, 0, qualifier.length, timestamp, deleteType);
+    
+    boolean ret = sdt.isDeleted(qualifier, 0, qualifier.length, timestamp);
+    assertEquals(true, ret);
+  }
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestWildcardColumnTracker.java	(revision 0)
@@ -0,0 +1,336 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestCase;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class TestWildcardColumnTracker extends HBaseTestCase
+implements HConstants {
+  private boolean PRINT = false; 
+  
+  public void testGet_SingleVersion() {
+    if(PRINT) {
+      System.out.println("SingleVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    expected.add(MatchCode.INCLUDE);
+    int maxVersions = 1;
+    
+    ColumnTracker exp = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+
+  
+  public void testGet_MultiVersion() {
+    if(PRINT) {
+      System.out.println("\nMultiVersion");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 5;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+      expected.add(MatchCode.INCLUDE);
+      expected.add(MatchCode.SKIP);
+    }
+    int maxVersions = 2;
+    
+    ColumnTracker exp = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col4);
+    scanner.add(col5);
+    scanner.add(col5);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(byte [] col : scanner){
+      result.add(exp.checkColumn(col, 0, col.length));
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i< expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  public void testUpdate_SameColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_SameColumns");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 10;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<5; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 2;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    scanner.add(col5);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    //"Match"
+    for(int i=0; i<3; i++){
+      for(byte [] col : scanner){
+        result.add(wild.checkColumn(col, 0, col.length));
+      }
+      wild.update();
+    }
+    
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  public void testUpdate_NewColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_NewColumns");
+    }
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    byte [] col5 = Bytes.toBytes("col5");
+    
+    byte [] col6 = Bytes.toBytes("col6");
+    byte [] col7 = Bytes.toBytes("col7");
+    byte [] col8 = Bytes.toBytes("col8");
+    byte [] col9 = Bytes.toBytes("col9");
+    byte [] col0 = Bytes.toBytes("col0");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 10;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<5; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 1;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col0);
+    scanner.add(col1);
+    scanner.add(col2);
+    scanner.add(col3);
+    scanner.add(col4);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Create "Scanner1"
+    List<byte[]> scanner1 = new ArrayList<byte[]>();
+    scanner1.add(col5);
+    scanner1.add(col6);
+    scanner1.add(col7);
+    scanner1.add(col8);
+    scanner1.add(col9);
+    for(byte [] col : scanner1){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Scanner again
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }  
+      
+    //"Match"
+    assertEquals(expected.size(), result.size());
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  public void testUpdate_MixedColumns(){
+    if(PRINT) {
+      System.out.println("\nUpdate_NewColumns");
+    }
+    byte [] col0 = Bytes.toBytes("col0");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    byte [] col4 = Bytes.toBytes("col4");
+    
+    byte [] col5 = Bytes.toBytes("col5");
+    byte [] col6 = Bytes.toBytes("col6");
+    byte [] col7 = Bytes.toBytes("col7");
+    byte [] col8 = Bytes.toBytes("col8");
+    byte [] col9 = Bytes.toBytes("col9");
+    
+    //Create tracker
+    List<MatchCode> expected = new ArrayList<MatchCode>();
+    int size = 5;
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.INCLUDE);
+    }
+    for(int i=0; i<size; i++){
+      expected.add(MatchCode.SKIP);
+    }
+    
+    int maxVersions = 1;
+    
+    ColumnTracker wild = new WildcardColumnTracker(maxVersions);
+        
+    //Create "Scanner"
+    List<byte[]> scanner = new ArrayList<byte[]>();
+    scanner.add(col0);
+    scanner.add(col2);
+    scanner.add(col4);
+    scanner.add(col6);
+    scanner.add(col8);
+    
+    //Initialize result
+    List<MatchCode> result = new ArrayList<MatchCode>(); 
+    
+    for(int i=0; i<2; i++){
+      for(byte [] col : scanner){
+        result.add(wild.checkColumn(col, 0, col.length));
+      }
+      wild.update();
+    }
+
+    //Create "Scanner1"
+    List<byte[]> scanner1 = new ArrayList<byte[]>();
+    scanner1.add(col1);
+    scanner1.add(col3);
+    scanner1.add(col5);
+    scanner1.add(col7);
+    scanner1.add(col9);
+    for(byte [] col : scanner1){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }
+    wild.update();
+
+    //Scanner again
+    for(byte [] col : scanner){
+      result.add(wild.checkColumn(col, 0, col.length));
+    }  
+      
+    //"Match"
+    assertEquals(expected.size(), result.size());
+    
+    for(int i=0; i<expected.size(); i++){
+      assertEquals(expected.get(i), result.get(i));
+      if(PRINT){
+        System.out.println("Expected " +expected.get(i) + ", actual " +
+            result.get(i));
+      }
+    }
+  }
+  
+  
+  
+}
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHLog.java	(working copy)
@@ -133,7 +133,7 @@
         assertTrue(Bytes.equals(regionName, key.getRegionName()));
         assertTrue(Bytes.equals(tableName, key.getTablename()));
         assertTrue(Bytes.equals(HLog.METAROW, val.getRow()));
-        assertTrue(Bytes.equals(HLog.METACOLUMN, val.getColumn()));
+        assertTrue(Bytes.equals(HLog.METAFAMILY, val.getFamily()));
         assertEquals(0, Bytes.compareTo(HLog.COMPLETE_CACHE_FLUSH,
           val.getValue()));
         System.out.println(key + " " + val);
Index: src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java	(working copy)
@@ -37,6 +37,8 @@
 import org.apache.hadoop.hbase.filter.StopRowFilter;
 import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
@@ -102,9 +104,9 @@
       addContent(this.r, HConstants.COLUMN_FAMILY);
       List<KeyValue> results = new ArrayList<KeyValue>();
       // Do simple test of getting one row only first.
-      InternalScanner s = r.getScanner(HConstants.COLUMN_FAMILY_ARRAY,
-          Bytes.toBytes("abc"), HConstants.LATEST_TIMESTAMP,
-          new WhileMatchRowFilter(new StopRowFilter(Bytes.toBytes("abd"))));
+      Scan scan = new Scan(Bytes.toBytes("abc"), Bytes.toBytes("abd"));
+      scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+      InternalScanner s = r.getScanner(Bytes.toBytes("abc"),scan);
       int count = 0;
       while (s.next(results)) {
         count++;
@@ -112,9 +114,9 @@
       s.close();
       assertEquals(1, count);
       // Now do something a bit more imvolved.
-      s = r.getScanner(HConstants.COLUMN_FAMILY_ARRAY,
-        startrow, HConstants.LATEST_TIMESTAMP,
-        new WhileMatchRowFilter(new StopRowFilter(stoprow)));
+      scan = new Scan(startrow, stoprow);
+      scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+      s = r.getScanner(startrow, scan);
       count = 0;
       KeyValue kv = null;
       results = new ArrayList<KeyValue>();
@@ -147,14 +149,15 @@
       
       // Write information to the meta table
 
-      BatchUpdate batchUpdate =
-        new BatchUpdate(ROW_KEY, System.currentTimeMillis());
+      Put put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
 
       ByteArrayOutputStream byteStream = new ByteArrayOutputStream();
       DataOutputStream s = new DataOutputStream(byteStream);
       REGION_INFO.write(s);
-      batchUpdate.put(HConstants.COL_REGIONINFO, byteStream.toByteArray());
-      region.commit(batchUpdate);
+      put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
+          byteStream.toByteArray());
+      region.put(put);
 
       // What we just committed is in the memcache. Verify that we can get
       // it back both with scanning and get
@@ -177,13 +180,14 @@
  
       HServerAddress address = new HServerAddress("foo.bar.com:1234");
 
-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());
+      put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
 
-      batchUpdate.put(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));
+      put.add(HConstants.COL_SERVER,  Bytes.toBytes(address.toString()));
 
-      batchUpdate.put(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));
+      put.add(HConstants.COL_STARTCODE, Bytes.toBytes(START_CODE));
 
-      region.commit(batchUpdate);
+      region.put(put);
       
       // Validate that we can still get the HRegionInfo, even though it is in
       // an older row on disk and there is a newer row in the memcache
@@ -215,12 +219,12 @@
 
       address = new HServerAddress("bar.foo.com:4321");
       
-      batchUpdate = new BatchUpdate(ROW_KEY, System.currentTimeMillis());
+      put = new Put(ROW_KEY);
+      put.setTimeStamp(System.currentTimeMillis());
 
-      batchUpdate.put(HConstants.COL_SERVER, 
-        Bytes.toBytes(address.toString()));
+      put.add(HConstants.COL_SERVER, Bytes.toBytes(address.toString()));
 
-      region.commit(batchUpdate);
+      region.put(put);
       
       // Validate again
       
@@ -273,6 +277,7 @@
   private void scan(boolean validateStartcode, String serverName)
   throws IOException {  
     InternalScanner scanner = null;
+    Scan scan = null;
     List<KeyValue> results = new ArrayList<KeyValue>();
     byte [][][] scanColumns = {
         COLS,
@@ -281,8 +286,9 @@
     
     for(int i = 0; i < scanColumns.length; i++) {
       try {
-        scanner = r.getScanner(scanColumns[i], FIRST_ROW,
-            System.currentTimeMillis(), null);
+        scan = new Scan(FIRST_ROW);
+        scan.addColumns(scanColumns[i]);
+        scanner = r.getScanner(FIRST_ROW, scan);
         while (scanner.next(results)) {
           assertTrue(hasColumn(results, HConstants.COL_REGIONINFO));
           byte [] val = getColumn(results, HConstants.COL_REGIONINFO).getValue(); 
Index: src/test/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/OOMERegionServer.java	(working copy)
@@ -23,7 +23,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HServerAddress;
 
@@ -35,7 +35,7 @@
  * <code>${HBASE_HOME}/bin/hbase ./bin/hbase org.apache.hadoop.hbase.OOMERegionServer start</code>.
  */
 public class OOMERegionServer extends HRegionServer {
-  private List<BatchUpdate> retainer = new ArrayList<BatchUpdate>();
+  private List<Put> retainer = new ArrayList<Put>();
 
   public OOMERegionServer(HBaseConfiguration conf) throws IOException {
     super(conf);
@@ -46,12 +46,12 @@
     super(address, conf);
   }
   
-  public void batchUpdate(byte [] regionName, BatchUpdate b)
+  public void put(byte [] regionName, Put put)
   throws IOException {
-    super.batchUpdate(regionName, b, -1L);
+    super.put(regionName, put);
     for (int i = 0; i < 30; i++) {
       // Add the batch update 30 times to bring on the OOME faster.
-      this.retainer.add(b);
+      this.retainer.add(put);
     }
   }
   
Index: src/test/org/apache/hadoop/hbase/regionserver/TestDeleteAll.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestDeleteAll.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestDeleteAll.java	(working copy)
@@ -26,6 +26,8 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -33,11 +35,11 @@
  */
 public class TestDeleteAll extends HBaseTestCase {
   static final Log LOG = LogFactory.getLog(TestDeleteAll.class);
-  
+
   private final String COLUMN_REGEX = "[a-zA-Z0-9]*:[b|c]?";
-  
+
   private MiniDFSCluster miniHdfs;
-  
+
   @Override
   protected void setUp() throws Exception {
     super.setUp();
@@ -45,13 +47,13 @@
       this.miniHdfs = new MiniDFSCluster(this.conf, 1, true, null);
       // Set the hbase.rootdir to be the home directory in mini dfs.
       this.conf.set(HConstants.HBASE_DIR,
-        this.miniHdfs.getFileSystem().getHomeDirectory().toString());
+          this.miniHdfs.getFileSystem().getHomeDirectory().toString());
     } catch (Exception e) {
       LOG.fatal("error starting MiniDFSCluster", e);
       throw e;
     }
   }
-  
+
   /**
    * Tests for HADOOP-1550.
    * @throws Exception
@@ -63,16 +65,12 @@
       HTableDescriptor htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
       region_incommon = new HRegionIncommon(region);
-      
+
       // test memcache
       makeSureItWorks(region, region_incommon, false);
       // test hstore
       makeSureItWorks(region, region_incommon, true);
-      
-      // regex test memcache
-      makeSureRegexWorks(region, region_incommon, false);
-      // regex test hstore
-      makeSureRegexWorks(region, region_incommon, true);
+
     } finally {
       if (region != null) {
         try {
@@ -84,9 +82,9 @@
       }
     }
   }
-    
+
   private void makeSureItWorks(HRegion region, HRegionIncommon region_incommon, 
-    boolean flush)
+      boolean flush)
   throws Exception{
     // insert a few versions worth of data for a row
     byte [] row = Bytes.toBytes("test_row");
@@ -101,31 +99,37 @@
     byte [] colC = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "c");
     byte [] colD = Bytes.toBytes(Bytes.toString(COLUMNS[0]));
 
-    BatchUpdate batchUpdate = new BatchUpdate(row, now);
-    batchUpdate.put(colA, cellData(0, flush).getBytes());
-    batchUpdate.put(colB, cellData(0, flush).getBytes());
-    batchUpdate.put(colC, cellData(0, flush).getBytes());      
-    batchUpdate.put(colD, cellData(0, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
+    Put put = new Put(row);
+    put.setTimeStamp(now);
+    put.add(colA, cellData(0, flush).getBytes());
+    put.add(colB, cellData(0, flush).getBytes());
+    put.add(colC, cellData(0, flush).getBytes());      
+    put.add(colD, cellData(0, flush).getBytes());      
+    region_incommon.put(put);
 
-    batchUpdate = new BatchUpdate(row, past);
-    batchUpdate.put(colA, cellData(1, flush).getBytes());
-    batchUpdate.put(colB, cellData(1, flush).getBytes());
-    batchUpdate.put(colC, cellData(1, flush).getBytes());      
-    batchUpdate.put(colD, cellData(1, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
-    
-    batchUpdate = new BatchUpdate(row, future);
-    batchUpdate.put(colA, cellData(2, flush).getBytes());
-    batchUpdate.put(colB, cellData(2, flush).getBytes());
-    batchUpdate.put(colC, cellData(2, flush).getBytes());      
-    batchUpdate.put(colD, cellData(2, flush).getBytes());      
-    region_incommon.commit(batchUpdate);
+    put = new Put(row);
+    put.setTimeStamp(past);
+    put.add(colA, cellData(1, flush).getBytes());
+    put.add(colB, cellData(1, flush).getBytes());
+    put.add(colC, cellData(1, flush).getBytes());      
+    put.add(colD, cellData(1, flush).getBytes());      
+    region_incommon.put(put);
 
+    put = new Put(row);
+    put.setTimeStamp(future);
+    put.add(colA, cellData(2, flush).getBytes());
+    put.add(colB, cellData(2, flush).getBytes());
+    put.add(colC, cellData(2, flush).getBytes());      
+    put.add(colD, cellData(2, flush).getBytes());      
+    region_incommon.put(put);
+
     if (flush) {region_incommon.flushcache();}
 
-    // call delete all at a timestamp, make sure only the most recent stuff is left behind
-    region.deleteAll(row, now, null);
+    // call delete all at a timestamp, make sure only the most recent stuff is
+    //left behind
+    Delete delete = new Delete(row, now, null);
+    region.delete(delete, null, true);
+
     if (flush) {region_incommon.flushcache();}    
     assertCellEquals(region, row, colA, future, cellData(2, flush));
     assertCellEquals(region, row, colA, past, null);
@@ -135,7 +139,9 @@
     assertCellEquals(region, row, colD, now, null);
 
     // call delete all w/o a timestamp, make sure nothing is left.
-    region.deleteAll(row, HConstants.LATEST_TIMESTAMP, null);
+    delete = new Delete(row, HConstants.LATEST_TIMESTAMP, null);
+    region.delete(delete, null, true);
+
     if (flush) {region_incommon.flushcache();}    
     assertCellEquals(region, row, colA, now, null);
     assertCellEquals(region, row, colA, past, null);
@@ -143,82 +149,9 @@
     assertCellEquals(region, row, colD, now, null);
     assertCellEquals(region, row, colD, past, null);
     assertCellEquals(region, row, colD, future, null);
-    
+
   }
 
-  private void makeSureRegexWorks(HRegion region, HRegionIncommon region_incommon, 
-      boolean flush)
-    throws Exception{
-      // insert a few versions worth of data for a row
-      byte [] row = Bytes.toBytes("test_row");
-      long t0 = System.currentTimeMillis();
-      long t1 = t0 - 15000;
-      long t2 = t1 - 15000;
-
-      byte [] colA = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "a");
-      byte [] colB = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "b");
-      byte [] colC = Bytes.toBytes(Bytes.toString(COLUMNS[0]) + "c");
-      byte [] colD = Bytes.toBytes(Bytes.toString(COLUMNS[0]));
-
-      BatchUpdate batchUpdate = new BatchUpdate(row, t0);
-      batchUpdate.put(colA, cellData(0, flush).getBytes());
-      batchUpdate.put(colB, cellData(0, flush).getBytes());
-      batchUpdate.put(colC, cellData(0, flush).getBytes());      
-      batchUpdate.put(colD, cellData(0, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-
-      batchUpdate = new BatchUpdate(row, t1);
-      batchUpdate.put(colA, cellData(1, flush).getBytes());
-      batchUpdate.put(colB, cellData(1, flush).getBytes());
-      batchUpdate.put(colC, cellData(1, flush).getBytes());      
-      batchUpdate.put(colD, cellData(1, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-      
-      batchUpdate = new BatchUpdate(row, t2);
-      batchUpdate.put(colA, cellData(2, flush).getBytes());
-      batchUpdate.put(colB, cellData(2, flush).getBytes());
-      batchUpdate.put(colC, cellData(2, flush).getBytes());      
-      batchUpdate.put(colD, cellData(2, flush).getBytes());      
-      region_incommon.commit(batchUpdate);
-
-      if (flush) {region_incommon.flushcache();}
-
-      // call delete the matching columns at a timestamp, 
-      // make sure only the most recent stuff is left behind
-      region.deleteAllByRegex(row, COLUMN_REGEX, t1, null);
-      if (flush) {region_incommon.flushcache();}    
-      assertCellEquals(region, row, colA, t0, cellData(0, flush));
-      assertCellEquals(region, row, colA, t1, cellData(1, flush));
-      assertCellEquals(region, row, colA, t2, cellData(2, flush));
-      assertCellEquals(region, row, colB, t0, cellData(0, flush));
-      assertCellEquals(region, row, colB, t1, null);
-      assertCellEquals(region, row, colB, t2, null);
-      assertCellEquals(region, row, colC, t0, cellData(0, flush));
-      assertCellEquals(region, row, colC, t1, null);
-      assertCellEquals(region, row, colC, t2, null);
-      assertCellEquals(region, row, colD, t0, cellData(0, flush));
-      assertCellEquals(region, row, colD, t1, null);
-      assertCellEquals(region, row, colD, t2, null);
-
-      // call delete all w/o a timestamp, make sure nothing is left.
-      region.deleteAllByRegex(row, COLUMN_REGEX, 
-          HConstants.LATEST_TIMESTAMP, null);
-      if (flush) {region_incommon.flushcache();}    
-      assertCellEquals(region, row, colA, t0, cellData(0, flush));
-      assertCellEquals(region, row, colA, t1, cellData(1, flush));
-      assertCellEquals(region, row, colA, t2, cellData(2, flush));
-      assertCellEquals(region, row, colB, t0, null);
-      assertCellEquals(region, row, colB, t1, null);
-      assertCellEquals(region, row, colB, t2, null);
-      assertCellEquals(region, row, colC, t0, null);
-      assertCellEquals(region, row, colC, t1, null);
-      assertCellEquals(region, row, colC, t2, null);
-      assertCellEquals(region, row, colD, t0, null);
-      assertCellEquals(region, row, colD, t1, null);
-      assertCellEquals(region, row, colD, t2, null);
-      
-    }
-  
   private String cellData(int tsNum, boolean flush){
     return "t" + tsNum + " data" + (flush ? " - with flush" : "");
   }
Index: src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java	(working copy)
@@ -23,17 +23,23 @@
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Set;
+import java.util.TreeSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestCase;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 
@@ -120,13 +126,14 @@
 
     // Write out a bunch of values
     for (int k = FIRST_ROW; k <= NUM_VALS; k++) {
-      BatchUpdate batchUpdate = 
-        new BatchUpdate(Bytes.toBytes("row_" + k), System.currentTimeMillis());
-      batchUpdate.put(CONTENTS_BASIC,
+      Put put = new Put(Bytes.toBytes("row_" + k));
+      put.setTimeStamp(System.currentTimeMillis());
+      
+      put.add(CONTENTS_BASIC,
           (CONTENTSTR + k).getBytes(HConstants.UTF8_ENCODING));
-      batchUpdate.put(Bytes.toBytes(ANCHORNUM + k),
+      put.add(Bytes.toBytes(ANCHORNUM + k),
           (ANCHORSTR + k).getBytes(HConstants.UTF8_ENCODING));
-      region.commit(batchUpdate);
+      region.put(put);
     }
     LOG.info("Write " + NUM_VALS + " rows. Elapsed time: "
         + ((System.currentTimeMillis() - startTime) / 1000.0));
@@ -179,11 +186,11 @@
     boolean exceptionThrown = false;
     exceptionThrown = false;
     try {
-      BatchUpdate batchUpdate = new BatchUpdate(Bytes.toBytes("Some old key"));
+      Put put = new Put(Bytes.toBytes("Some old key"));
       String unregisteredColName = "FamilyGroup:FamilyLabel";
-      batchUpdate.put(Bytes.toBytes(unregisteredColName),
+      put.add(Bytes.toBytes(unregisteredColName),
         unregisteredColName.getBytes(HConstants.UTF8_ENCODING));
-      region.commit(batchUpdate);
+      region.put(put);
     } catch (IOException e) {
       exceptionThrown = true;
     } finally {
@@ -267,12 +274,11 @@
     for(int k = 0; k < vals1.length / 2; k++) {
       String kLabel = String.format("%1$03d", k);
 
-      BatchUpdate batchUpdate = 
-        new BatchUpdate(Bytes.toBytes("row_vals1_" + kLabel), 
-          System.currentTimeMillis());
-      batchUpdate.put(cols[0], vals1[k].getBytes(HConstants.UTF8_ENCODING));
-      batchUpdate.put(cols[1], vals1[k].getBytes(HConstants.UTF8_ENCODING));
-      region.commit(batchUpdate);
+      Put put = new Put(Bytes.toBytes("row_vals1_" + kLabel));
+      put.setTimeStamp(System.currentTimeMillis());
+      put.add(cols[0], vals1[k].getBytes(HConstants.UTF8_ENCODING));
+      put.add(cols[1], vals1[k].getBytes(HConstants.UTF8_ENCODING));
+      region.put(put);
       numInserted += 2;
     }
     LOG.info("Write " + (vals1.length / 2) + " elapsed time: "
@@ -359,12 +365,11 @@
     startTime = System.currentTimeMillis();
     for(int k = vals1.length/2; k < vals1.length; k++) {
       String kLabel = String.format("%1$03d", k);
-      BatchUpdate batchUpdate = 
-        new BatchUpdate(Bytes.toBytes("row_vals1_" + kLabel), 
-          System.currentTimeMillis());
-      batchUpdate.put(cols[0], vals1[k].getBytes(HConstants.UTF8_ENCODING));
-      batchUpdate.put(cols[1], vals1[k].getBytes(HConstants.UTF8_ENCODING));
-      region.commit(batchUpdate);
+      Put put = new Put(Bytes.toBytes("row_vals1_" + kLabel));
+      put.setTimeStamp(System.currentTimeMillis());
+      put.add(cols[0], vals1[k].getBytes(HConstants.UTF8_ENCODING));
+      put.add(cols[1], vals1[k].getBytes(HConstants.UTF8_ENCODING));
+      region.put(put);
       numInserted += 2;
     }
 
@@ -529,9 +534,9 @@
         CONTENTS_BASIC
     };
     long startTime = System.currentTimeMillis();
-    InternalScanner s =
-      r.getScanner(cols, HConstants.EMPTY_START_ROW,
-          System.currentTimeMillis(), null);
+    Scan scan = new Scan();
+    scan.addColumns(cols);
+    InternalScanner s = r.getScanner(HConstants.EMPTY_START_ROW, scan);
     try {
       int contentsFetched = 0;
       int anchorFetched = 0;
@@ -583,9 +588,9 @@
     cols = new byte [][] {CONTENTS_FIRSTCOL, ANCHOR_SECONDCOL};
     
     startTime = System.currentTimeMillis();
-
-    s = r.getScanner(cols, HConstants.EMPTY_START_ROW,
-      System.currentTimeMillis(), null);
+    scan = new Scan();
+    scan.addColumns(cols);
+    s = r.getScanner(HConstants.EMPTY_START_ROW, scan);
     try {
       int numFetched = 0;
       List<KeyValue> curVals = new ArrayList<KeyValue>();
@@ -628,7 +633,9 @@
     
     startTime = System.currentTimeMillis();
     
-    s = r.getScanner(cols, HConstants.EMPTY_START_ROW, System.currentTimeMillis(), null);
+    scan = new Scan();
+    scan.addColumns(cols);
+    s = r.getScanner(HConstants.EMPTY_START_ROW, scan);
 
     try {
       int fetched = 0;
@@ -652,4 +659,31 @@
     LOG.info("read completed.");
   }
   
+  
+  //Visual test, since the method doesn't return anything
+  public void testDelete_CheckTimestampUpdated()
+  throws IOException {
+//    init();
+    HTableDescriptor desc = new HTableDescriptor("test");
+    HRegionInfo regionInfo = new HRegionInfo(desc, null, null, false);
+//    HRegionInfo regionInfo = new HRegionInfo();
+    HRegion region = new HRegion(null, null, null, null, regionInfo, null);
+    
+//    public HRegion(Path basedir, HLog log, FileSystem fs, HBaseConfiguration conf, 
+//        HRegionInfo regionInfo, FlushRequester flushListener) {
+    
+        byte [] row1 = Bytes.toBytes("row1");
+    byte [] fam1 = Bytes.toBytes("fam1");
+    byte [] col1 = Bytes.toBytes("col1");
+    byte [] col2 = Bytes.toBytes("col2");
+    byte [] col3 = Bytes.toBytes("col3");
+    
+    List<KeyValue> kvs  = new ArrayList<KeyValue>();
+    kvs.add(new KeyValue(row1, fam1, col1, null));
+    kvs.add(new KeyValue(row1, fam1, col2, null));
+    kvs.add(new KeyValue(row1, fam1, col3, null));
+    
+    region.delete(fam1, kvs, true);
+  }
+  
 }
Index: src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestSplit.java	(working copy)
@@ -32,8 +32,9 @@
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.UnknownScannerException;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -98,8 +99,10 @@
       final byte [] midkey = region.compactStores();
       assertNotNull(midkey);
       byte [][] cols = {COLFAMILY_NAME3};
-      final InternalScanner s = region.getScanner(cols,
-        HConstants.EMPTY_START_ROW, System.currentTimeMillis(), null);
+      Scan scan = new Scan();
+      scan.addColumns(cols);
+      final InternalScanner s = region.getScanner(HConstants.EMPTY_START_ROW, 
+          scan);
       final HRegion regionForThread = region;
       
       Thread splitThread = new Thread() {
@@ -117,10 +120,9 @@
       long id = server.addScanner(s);
       for(int i = 0; i < 6; i++) {
         try {
-          BatchUpdate update = 
-            new BatchUpdate(region.getRegionInfo().getStartKey());
-          update.put(COLFAMILY_NAME3, Bytes.toBytes("val"));
-          region.batchUpdate(update);
+          Put put = new Put(region.getRegionInfo().getStartKey());
+          put.add(COLFAMILY_NAME3, Bytes.toBytes("val"));
+          region.put(put);
           Thread.sleep(1000);
         }
         catch (InterruptedException e) {
@@ -230,8 +232,9 @@
       final byte [] firstValue)
   throws IOException {
     byte [][] cols = {column};
-    InternalScanner s = r.getScanner(cols,
-      HConstants.EMPTY_START_ROW, System.currentTimeMillis(), null);
+    Scan scan = new Scan();
+    scan.addColumns(cols);
+    InternalScanner s = r.getScanner(HConstants.EMPTY_START_ROW, scan);
     try {
       List<KeyValue> curVals = new ArrayList<KeyValue>();
       boolean first = true;
Index: src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java	(working copy)
@@ -26,7 +26,9 @@
 import org.apache.hadoop.hbase.HBaseTestCase;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
 import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
@@ -111,8 +113,11 @@
     // the compaction threshold of 3 store files.  Compacting these store files
     // should result in a compacted store file that has no references to the
     // deleted row.
-    r.deleteAll(secondRowBytes, COLUMN_FAMILY_TEXT, System.currentTimeMillis(),
-      null);
+    Delete delete = new Delete(secondRowBytes, System.currentTimeMillis(), null);
+    byte [][] famAndQf = KeyValue.parseColumn(COLUMN_FAMILY_TEXT);
+    delete.deleteFamily(famAndQf[0]);
+    r.delete(delete, null, true);
+    
     // Assert deleted.
     assertNull(r.get(secondRowBytes, COLUMN_FAMILY_TEXT, -1, 100 /*Too many*/));
     r.flushcache();
Index: src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
===================================================================
--- src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java	(working copy)
@@ -34,7 +34,10 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -108,9 +111,9 @@
     // put some values in the table
     this.table = new HTable(conf, tableName);
     byte [] row = Bytes.toBytes("row1");
-    BatchUpdate b = new BatchUpdate(row);
-    b.put(HConstants.COLUMN_FAMILY, Bytes.toBytes(tableName));
-    table.commit(b);
+    Put put = new Put(row);
+    put.add(HConstants.CATALOG_FAMILY, null, Bytes.toBytes(tableName));
+    table.put(put);
     return row;
   }
 
@@ -166,9 +169,9 @@
           // Now try to open a scanner on the meta table. Should stall until
           // meta server comes back up.
           HTable t = new HTable(conf, HConstants.META_TABLE_NAME);
-          Scanner s =
-            t.getScanner(HConstants.COLUMN_FAMILY_ARRAY,
-              HConstants.EMPTY_START_ROW);
+          Scan scan = new Scan();
+          scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+          Scanner s = t.getScanner(scan);
           s.close();
           
         } catch (IOException e) {
@@ -179,14 +182,14 @@
         try {
           // Verify that the client can find the data after the region has moved
           // to a different server
-          scanner =
-            table.getScanner(HConstants.COLUMN_FAMILY_ARRAY,
-               HConstants.EMPTY_START_ROW);
+          Scan scan = new Scan();
+          scan.addColumns(HConstants.COLUMN_FAMILY_ARRAY);
+          scanner = table.getScanner(scan);
           LOG.info("Obtained scanner " + scanner);
-          for (RowResult r : scanner) {
+          for (Result r : scanner) {
             assertTrue(Bytes.equals(r.getRow(), row));
             assertEquals(1, r.size());
-            byte[] bytes = r.get(HConstants.COLUMN_FAMILY).getValue();
+            byte[] bytes = r.rowResult().get(HConstants.COLUMN_FAMILY).getValue();
             assertNotNull(bytes);
             assertTrue(tableName.equals(Bytes.toString(bytes)));
           }
Index: src/test/org/apache/hadoop/hbase/TimestampTestBase.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TimestampTestBase.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TimestampTestBase.java	(working copy)
@@ -22,6 +22,8 @@
 
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -81,7 +83,16 @@
     put(incommon, T2);
     delete(incommon, T1);
     put(incommon, T1);
-    incommon.deleteAll(ROW, COLUMN, T2);
+    
+    Delete delete = new Delete(ROW, T2, null);
+    byte [][] famAndQf = KeyValue.parseColumn(COLUMN);
+    if(famAndQf[1].length == 0){
+      delete.deleteFamily(famAndQf[0]);
+    } else {
+      delete.deleteColumn(famAndQf[0], famAndQf[1]);
+    }
+    incommon.delete(delete, null, true);
+    
     // Should only be current value in set.  Assert this is so
     assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);
     
@@ -211,20 +222,29 @@
   public static void put(final Incommon loader, final byte [] bytes,
     final long ts)
   throws IOException {
-    BatchUpdate batchUpdate = ts == HConstants.LATEST_TIMESTAMP ? 
-      new BatchUpdate(ROW) : new BatchUpdate(ROW, ts);
-    batchUpdate.put(COLUMN, bytes);
-    loader.commit(batchUpdate);
+    Put put = new Put(ROW);
+    if(ts != HConstants.LATEST_TIMESTAMP) {
+      put.setTimeStamp(ts);
+    }
+    put.add(COLUMN, bytes);
+    loader.put(put);
   }
   
   public static void delete(final Incommon loader) throws IOException {
     delete(loader, HConstants.LATEST_TIMESTAMP);
   }
 
-  public static void delete(final Incommon loader, final long ts) throws IOException {
-    BatchUpdate batchUpdate = ts == HConstants.LATEST_TIMESTAMP ? 
-      new BatchUpdate(ROW) : new BatchUpdate(ROW, ts);
-    batchUpdate.delete(COLUMN);
-    loader.commit(batchUpdate);  
+  public static void delete(final Incommon loader, final long ts)
+  throws IOException {
+    Delete delete = ts == HConstants.LATEST_TIMESTAMP ? 
+        new Delete(ROW) : new Delete(ROW, ts, null);
+
+    byte [][] famAndQf = KeyValue.parseColumn(COLUMN);
+    if(famAndQf[1].length == 0){
+      delete.deleteFamily(famAndQf[0]);
+    } else {
+      delete.deleteColumn(famAndQf[0], famAndQf[1]);
+    }
+    loader.delete(delete, null, true);
   }
 }
Index: src/test/org/apache/hadoop/hbase/TestZooKeeper.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestZooKeeper.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestZooKeeper.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -141,9 +142,9 @@
       admin.createTable(desc);
   
       HTable table = new HTable("test");
-      BatchUpdate batchUpdate = new BatchUpdate("testrow");
-      batchUpdate.put("fam:col", Bytes.toBytes("testdata"));
-      table.commit(batchUpdate);
+      Put put = new Put(Bytes.toBytes("testrow"));
+      put.add(Bytes.toBytes("fam"), Bytes.toBytes("col"), Bytes.toBytes("testdata"));
+      table.put(put);
     } catch (Exception e) {
       e.printStackTrace();
       fail();
Index: src/test/org/apache/hadoop/hbase/filter/DisabledTestRowFilterOnMultipleFamilies.java
===================================================================
--- src/test/org/apache/hadoop/hbase/filter/DisabledTestRowFilterOnMultipleFamilies.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/filter/DisabledTestRowFilterOnMultipleFamilies.java	(working copy)
@@ -37,7 +37,10 @@
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -71,10 +74,10 @@
     HTable table = new HTable(conf, TABLE_NAME);
 
     for (int i = 0; i < NUM_ROWS; i++) {
-      BatchUpdate b = new BatchUpdate("row_" + String.format("%1$05d", i));
-      b.put(TEXT_COLUMN1, VALUE);
-      b.put(TEXT_COLUMN2, String.format("%1$05d", i).getBytes());
-      table.commit(b);
+      Put put = new Put(Bytes.toBytes("row_" + String.format("%1$05d", i)));
+      put.add(TEXT_COLUMN1, VALUE);
+      put.add(TEXT_COLUMN2, Bytes.toBytes(String.format("%1$05d", i)));
+      table.put(put);
     }
 
     LOG.info("Print table contents using scanner before map/reduce for " + TABLE_NAME);
@@ -85,7 +88,9 @@
 
   private void scanTable(final String tableName, final boolean printValues) throws IOException {
     HTable table = new HTable(conf, tableName);
-    Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
+    Scan scan = new Scan();
+    scan.addColumns(columns);
+    Scanner scanner = table.getScanner(scan);
     int numFound = doScan(scanner, printValues);
     Assert.assertEquals(NUM_ROWS, numFound);
   }
@@ -96,7 +101,10 @@
     columnMap.put(TEXT_COLUMN1,
         new Cell(VALUE, HConstants.LATEST_TIMESTAMP));
     RegExpRowFilter filter = new RegExpRowFilter(null, columnMap);
-    Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW, filter);
+    Scan scan = new Scan();
+    scan.addColumns(columns);
+    scan.setFilter(filter);
+    Scanner scanner = table.getScanner(scan);
     int numFound = doScan(scanner, printValues);
     Assert.assertEquals(NUM_ROWS, numFound);
   }
@@ -106,11 +114,11 @@
       int count = 0;
 
       try {
-        for (RowResult result : scanner) {
+        for (Result result : scanner) {
           if (printValues) {
             LOG.info("row: " + Bytes.toString(result.getRow()));
 
-            for (Map.Entry<byte [], Cell> e : result.entrySet()) {
+            for (Map.Entry<byte [], Cell> e : result.rowResult().entrySet()) {
               LOG.info(" column: " + e.getKey() + " value: "
                   + new String(e.getValue().getValue(), HConstants.UTF8_ENCODING));
             }
Index: src/test/org/apache/hadoop/hbase/TestScannerAPI.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestScannerAPI.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestScannerAPI.java	(working copy)
@@ -34,7 +34,10 @@
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -90,11 +93,11 @@
     HTable table = new HTable(conf, getName());
 
     for (Map.Entry<byte [], SortedMap<byte [], Cell>> row: values.entrySet()) {
-      BatchUpdate b = new BatchUpdate(row.getKey());
+      Put put = new Put(row.getKey());
       for (Map.Entry<byte [], Cell> val: row.getValue().entrySet()) {
-        b.put(val.getKey(), val.getValue().getValue());
+        put.add(val.getKey(), val.getValue().getValue());
       }
-      table.commit(b);
+      table.put(put);
     }
 
     HRegion region = null;
@@ -113,32 +116,39 @@
       throw iox;
     }
     @SuppressWarnings("null")
+    Scan scan = new Scan(startRow);
+    scan.addColumns(columns);
     ScannerIncommon scanner = new InternalScannerIncommon(
-      region.getScanner(columns, startRow, System.currentTimeMillis(), null));
+      region.getScanner(startRow, scan));
     try {
       verify(scanner);
     } finally {
       scanner.close();
     }
     
-    scanner = new ClientScannerIncommon(table.getScanner(columns, startRow));
+    scan = new Scan(startRow);
+    scan.addColumns(columns);
+    scanner = new ClientScannerIncommon(table.getScanner(scan));
     try {
       verify(scanner);
     } finally {
       scanner.close();
     }
-    Scanner scanner2 = table.getScanner(columns, startRow);
+    scan = new Scan(startRow);
+    scan.addColumns(columns);
+    Scanner scanner2 = table.getScanner(scan);
     try {
-      for (RowResult r : scanner2) {
+      for (Result r : scanner2) {
         assertTrue("row key", values.containsKey(r.getRow()));
 
         SortedMap<byte [], Cell> columnValues = values.get(r.getRow());
         assertEquals(columnValues.size(), r.size());
         for (Map.Entry<byte [], Cell> e: columnValues.entrySet()) {
           byte [] column = e.getKey();
-          assertTrue("column", r.containsKey(column));
+          byte [] value = r.getValue(column);
+          assertTrue("column", value != null);
           assertTrue("value", Arrays.equals(columnValues.get(column).getValue(),
-            r.get(column).getValue()));
+              value));
         }
       }      
     } finally {
Index: src/test/org/apache/hadoop/hbase/TestKeyValue.java
===================================================================
--- src/test/org/apache/hadoop/hbase/TestKeyValue.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/TestKeyValue.java	(working copy)
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.util.Bytes;
 
 public class TestKeyValue extends TestCase {
@@ -39,13 +40,21 @@
     final byte [] a = Bytes.toBytes("aaa");
     byte [] column1 = Bytes.toBytes("abc:def");
     byte [] column2 = Bytes.toBytes("abcd:ef");
-    KeyValue aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column2, 0, column2.length, 4) == 0);
+    byte [] family2 = Bytes.toBytes("abcd");
+    byte [] qualifier2 = Bytes.toBytes("ef"); 
+    KeyValue aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column2));
+    assertTrue(aaa.matchingColumn(column1));
+    aaa = new KeyValue(a, column2, 0L, Type.Put, a);
+    assertFalse(aaa.matchingColumn(column1));
+    assertTrue(aaa.matchingColumn(family2,qualifier2));
     column1 = Bytes.toBytes("abcd:");
-    aaa = new KeyValue(a, column1, a);
-    assertFalse(KeyValue.COMPARATOR.
-      compareColumns(aaa, column1, 0, column1.length, 4) == 0);
+    aaa = new KeyValue(a, column1, 0L, Type.Put, a);
+    assertTrue(aaa.matchingColumn(family2,null));
+    assertFalse(aaa.matchingColumn(family2,qualifier2));
+    // Previous test had an assertFalse that I don't understand
+    //    assertFalse(KeyValue.COMPARATOR.
+    //    compareColumns(aaa, column1, 0, column1.length, 4) == 0);
   }
 
   public void testBasics() throws Exception {
@@ -111,31 +120,31 @@
   public void testMoreComparisons() throws Exception {
     // Root compares
     long now = System.currentTimeMillis();
-    KeyValue a = new KeyValue(".META.,,99999999999999", now);
-    KeyValue b = new KeyValue(".META.,,1", now);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,,99999999999999"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,,1"), now);
     KVComparator c = new KeyValue.RootComparator();
     assertTrue(c.compare(b, a) < 0);
-    KeyValue aa = new KeyValue(".META.,,1", now);
-    KeyValue bb = new KeyValue(".META.,,1", "info:regioninfo",
-      1235943454602L);
+    KeyValue aa = new KeyValue(Bytes.toBytes(".META.,,1"), now);
+    KeyValue bb = new KeyValue(Bytes.toBytes(".META.,,1"), 
+        Bytes.toBytes("info:regioninfo"), 1235943454602L);
     assertTrue(c.compare(aa, bb) < 0);
     
     // Meta compares
-    KeyValue aaa =
-      new KeyValue("TestScanMultipleVersions,row_0500,1236020145502", now);
-    KeyValue bbb = new KeyValue("TestScanMultipleVersions,,99999999999999",
-      now);
+    KeyValue aaa = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,row_0500,1236020145502"), now);
+    KeyValue bbb = new KeyValue(
+        Bytes.toBytes("TestScanMultipleVersions,,99999999999999"), now);
     c = new KeyValue.MetaComparator();
     assertTrue(c.compare(bbb, aaa) < 0);
     
-    KeyValue aaaa = new KeyValue("TestScanMultipleVersions,,1236023996656",
-      "info:regioninfo", 1236024396271L);
+    KeyValue aaaa = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,,1236023996656"),
+        Bytes.toBytes("info:regioninfo"), 1236024396271L);
     assertTrue(c.compare(aaaa, bbb) < 0);
     
-    KeyValue x = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "", 9223372036854775807L);
-    KeyValue y = new KeyValue("TestScanMultipleVersions,row_0500,1236034574162",
-      "info:regioninfo", 1236034574912L);
+    KeyValue x = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes(""), 9223372036854775807L);
+    KeyValue y = new KeyValue(Bytes.toBytes("TestScanMultipleVersions,row_0500,1236034574162"),
+        Bytes.toBytes("info:regioninfo"), 1236034574912L);
     assertTrue(c.compare(x, y) < 0);
     comparisons(new KeyValue.MetaComparator());
     comparisons(new KeyValue.KVComparator());
@@ -151,53 +160,53 @@
   public void testKeyValueBorderCases() throws IOException {
     // % sorts before , so if we don't do special comparator, rowB would
     // come before rowA.
-    KeyValue rowA = new KeyValue("testtable,www.hbase.org/,1234",
-      "", Long.MAX_VALUE);
-    KeyValue rowB = new KeyValue("testtable,www.hbase.org/%20,99999",
-      "", Long.MAX_VALUE);
+    KeyValue rowA = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/,1234"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
+    KeyValue rowB = new KeyValue(Bytes.toBytes("testtable,www.hbase.org/%20,99999"),
+      Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue("testtable,,1234", "", Long.MAX_VALUE);
-    rowB = new KeyValue("testtable,$www.hbase.org/,99999", "", Long.MAX_VALUE);
+    rowA = new KeyValue(Bytes.toBytes("testtable,,1234"), Bytes.toBytes(""), Long.MAX_VALUE);
+    rowB = new KeyValue(Bytes.toBytes("testtable,$www.hbase.org/,99999"), Bytes.toBytes(""), Long.MAX_VALUE);
     assertTrue(KeyValue.META_COMPARATOR.compare(rowA, rowB) < 0);
 
-    rowA = new KeyValue(".META.,testtable,www.hbase.org/,1234,4321", "",
+    rowA = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/,1234,4321"), Bytes.toBytes(""),
       Long.MAX_VALUE);
-    rowB = new KeyValue(".META.,testtable,www.hbase.org/%20,99999,99999", "",
+    rowB = new KeyValue(Bytes.toBytes(".META.,testtable,www.hbase.org/%20,99999,99999"), Bytes.toBytes(""),
       Long.MAX_VALUE);
     assertTrue(KeyValue.ROOT_COMPARATOR.compare(rowA, rowB) < 0);
   }
 
   private void metacomparisons(final KeyValue.MetaComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,1", now),
-      new KeyValue(".META.,a,,0,1", now)) == 0);
-    KeyValue a = new KeyValue(".META.,a,,0,1", now);
-    KeyValue b = new KeyValue(".META.,a,,0,2", now);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) == 0);
+    KeyValue a = new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now);
+    KeyValue b = new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now);
     assertTrue(c.compare(a, b) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,a,,0,2", now),
-      new KeyValue(".META.,a,,0,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,a,,0,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,a,,0,1"), now)) > 0);
   }
 
   private void comparisons(final KeyValue.KVComparator c) {
     long now = System.currentTimeMillis();
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,1", now)) == 0);
-    assertTrue(c.compare(new KeyValue(".META.,,1", now),
-      new KeyValue(".META.,,2", now)) < 0);
-    assertTrue(c.compare(new KeyValue(".META.,,2", now),
-      new KeyValue(".META.,,1", now)) > 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) == 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,1"), now),
+      new KeyValue(Bytes.toBytes(".META.,,2"), now)) < 0);
+    assertTrue(c.compare(new KeyValue(Bytes.toBytes(".META.,,2"), now),
+      new KeyValue(Bytes.toBytes(".META.,,1"), now)) > 0);
   }
 
   public void testBinaryKeys() throws Exception {
     Set<KeyValue> set = new TreeSet<KeyValue>(KeyValue.COMPARATOR);
-    String column = "col:umn";
-    KeyValue [] keys = {new KeyValue("aaaaa,\u0000\u0000,2", column, 2),
-      new KeyValue("aaaaa,\u0001,3", column, 3),
-      new KeyValue("aaaaa,,1", column, 1),
-      new KeyValue("aaaaa,\u1000,5", column, 5),
-      new KeyValue("aaaaa,a,4", column, 4),
-      new KeyValue("a,a,0", column, 0),
+    byte [] column = Bytes.toBytes("col:umn");
+    KeyValue [] keys = {new KeyValue(Bytes.toBytes("aaaaa,\u0000\u0000,2"), column, 2),
+      new KeyValue(Bytes.toBytes("aaaaa,\u0001,3"), column, 3),
+      new KeyValue(Bytes.toBytes("aaaaa,,1"), column, 1),
+      new KeyValue(Bytes.toBytes("aaaaa,\u1000,5"), column, 5),
+      new KeyValue(Bytes.toBytes("aaaaa,a,4"), column, 4),
+      new KeyValue(Bytes.toBytes("a,a,0"), column, 0),
     };
     // Add to set with bad comparator
     for (int i = 0; i < keys.length; i++) {
@@ -226,12 +235,12 @@
     }
     // Make up -ROOT- table keys.
     KeyValue [] rootKeys = {
-        new KeyValue(".META.,aaaaa,\u0000\u0000,0,2", column, 2),
-        new KeyValue(".META.,aaaaa,\u0001,0,3", column, 3),
-        new KeyValue(".META.,aaaaa,,0,1", column, 1),
-        new KeyValue(".META.,aaaaa,\u1000,0,5", column, 5),
-        new KeyValue(".META.,aaaaa,a,0,4", column, 4),
-        new KeyValue(".META.,,0", column, 0),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0000\u0000,0,2"), column, 2),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u0001,0,3"), column, 3),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,,0,1"), column, 1),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,\u1000,0,5"), column, 5),
+        new KeyValue(Bytes.toBytes(".META.,aaaaa,a,0,4"), column, 4),
+        new KeyValue(Bytes.toBytes(".META.,,0"), column, 0),
       };
     // This will output the keys incorrectly.
     set = new TreeSet<KeyValue>(new KeyValue.MetaComparator());
Index: src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java
===================================================================
--- src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/mapred/TestTableIndex.java	(working copy)
@@ -37,8 +37,10 @@
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MultiRegionTable;
 import org.apache.hadoop.mapred.FileOutputFormat;
@@ -173,14 +175,15 @@
   private void scanTable(boolean printResults)
   throws IOException {
     HTable table = new HTable(conf, TABLE_NAME);
-    Scanner scanner = table.getScanner(columns,
-        HConstants.EMPTY_START_ROW);
+    Scan scan = new Scan();
+    scan.addColumns(columns);
+    Scanner scanner = table.getScanner(scan);
     try {
-      for (RowResult r : scanner) {
+      for (Result r : scanner) {
         if (printResults) {
           LOG.info("row: " + r.getRow());
         }
-        for (Map.Entry<byte [], Cell> e : r.entrySet()) {
+        for (Map.Entry<byte [], Cell> e : r.rowResult().entrySet()) {
           if (printResults) {
             LOG.info(" column: " + e.getKey() + " value: "
                 + new String(e.getValue().getValue(), HConstants.UTF8_ENCODING));
@@ -224,7 +227,9 @@
       }
 
       HTable table = new HTable(conf, TABLE_NAME);
-      scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
+      Scan scan = new Scan();
+      scan.addColumns(columns);
+      scanner = table.getScanner(scan);
 
       IndexConfiguration indexConf = new IndexConfiguration();
       String content = conf.get("hbase.index.conf");
@@ -234,7 +239,7 @@
       String rowkeyName = indexConf.getRowkeyName();
 
       int count = 0;
-      for (RowResult r : scanner) {
+      for (Result r : scanner) {
         String value = Bytes.toString(r.getRow());
         Term term = new Term(rowkeyName, value);
         int hitCount = searcher.search(new TermQuery(term)).length();
Index: src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
===================================================================
--- src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java	(working copy)
@@ -35,7 +35,9 @@
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
@@ -184,10 +186,11 @@
    * @throws NullPointerException if we failed to find a cell value
    */
   private void verifyAttempt(final HTable table) throws IOException, NullPointerException {
-    Scanner scanner =
-      table.getScanner(columns, HConstants.EMPTY_START_ROW);
+    Scan scan = new Scan();
+    scan.addColumns(columns);
+    Scanner scanner = table.getScanner(scan);
     try {
-      for (RowResult r : scanner) {
+      for (Result r : scanner) {
         if (LOG.isDebugEnabled()) {
           if (r.size() > 2 ) {
             throw new IOException("Too many results, expected 2 got " +
@@ -197,7 +200,7 @@
         byte[] firstValue = null;
         byte[] secondValue = null;
         int count = 0;
-        for(Map.Entry<byte [], Cell> e: r.entrySet()) {
+        for(Map.Entry<byte [], Cell> e: r.rowResult().entrySet()) {
           if (count == 0) {
             firstValue = e.getValue().getValue();
           }
Index: src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
===================================================================
--- src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java	(working copy)
@@ -43,7 +43,11 @@
 import org.apache.hadoop.hbase.filter.PageRowFilter;
 import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Hash;
@@ -86,7 +90,8 @@
   private static final int ONE_GB = 1024 * 1024 * 1000;
   private static final int ROWS_PER_GB = ONE_GB / ROW_LENGTH;
   
-  static final byte [] COLUMN_NAME = Bytes.toBytes(COLUMN_FAMILY_STR + "data");
+  static final byte [] FAMILY_NAME = Bytes.toBytes("info");
+  static final byte [] QUALIFIER_NAME = Bytes.toBytes("data");
   
   protected static final HTableDescriptor TABLE_DESCRIPTOR;
   static {
@@ -431,11 +436,12 @@
     
     @Override
     void testRow(final int i) throws IOException {
-      Scanner s = this.table.getScanner(new byte [][] {COLUMN_NAME},
-        getRandomRow(this.rand, this.totalRows),
-        new WhileMatchRowFilter(new PageRowFilter(120)));
+      Scan scan = new Scan(getRandomRow(this.rand, this.totalRows));
+      scan.addColumn(FAMILY_NAME, QUALIFIER_NAME);
+      scan.setFilter(new WhileMatchRowFilter(new PageRowFilter(120)));
+      Scanner s = this.table.getScanner(scan);
       //int count = 0;
-      for (RowResult rr = null; (rr = s.next()) != null;) {
+      for (Result rr = null; (rr = s.next()) != null;) {
         // LOG.info("" + count++ + " " + rr.toString());
       }
       s.close();
@@ -461,7 +467,9 @@
     
     @Override
     void testRow(final int i) throws IOException {
-      this.table.get(getRandomRow(this.rand, this.totalRows), COLUMN_NAME);
+      Get get = new Get(getRandomRow(this.rand, this.totalRows));
+      get.addColumn(FAMILY_NAME, QUALIFIER_NAME);
+      this.table.get(get);
     }
 
     @Override
@@ -485,9 +493,9 @@
     @Override
     void testRow(final int i) throws IOException {
       byte [] row = getRandomRow(this.rand, this.totalRows);
-      BatchUpdate b = new BatchUpdate(row);
-      b.put(COLUMN_NAME, generateValue(this.rand));
-      table.commit(b);
+      Put put = new Put(row);
+      put.add(FAMILY_NAME, QUALIFIER_NAME, generateValue(this.rand));
+      table.put(put);
     }
 
     @Override
@@ -507,8 +515,9 @@
     @Override
     void testSetup() throws IOException {
       super.testSetup();
-      this.testScanner = table.getScanner(new byte [][] {COLUMN_NAME},
-        format(this.startRow));
+      Scan scan = new Scan(format(this.startRow));
+      scan.addColumn(FAMILY_NAME, QUALIFIER_NAME);
+      this.testScanner = table.getScanner(scan);
     }
     
     @Override
@@ -539,7 +548,9 @@
     
     @Override
     void testRow(final int i) throws IOException {
-      table.get(format(i), COLUMN_NAME);
+      Get get = new Get(format(i));
+      get.addColumn(FAMILY_NAME, QUALIFIER_NAME);
+      table.get(get);
     }
 
     @Override
@@ -556,9 +567,9 @@
     
     @Override
     void testRow(final int i) throws IOException {
-      BatchUpdate b = new BatchUpdate(format(i));
-      b.put(COLUMN_NAME, generateValue(this.rand));
-      table.commit(b);
+      Put put = new Put(format(i));
+      put.add(FAMILY_NAME, QUALIFIER_NAME, generateValue(this.rand));
+      table.put(put);
     }
 
     @Override
Index: src/test/org/apache/hadoop/hbase/util/MigrationTest.java
===================================================================
--- src/test/org/apache/hadoop/hbase/util/MigrationTest.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/util/MigrationTest.java	(working copy)
@@ -40,7 +40,9 @@
 import org.apache.hadoop.hbase.client.HConnectionManager;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 
 /**
  * Runs migration of filesystem from hbase 0.x to 0.x
@@ -128,9 +130,11 @@
       HTable t = new HTable(this.conf, TABLENAME);
       int count = 0;
       LOG.info("OPENING SCANNER");
-      Scanner s = t.getScanner(TABLENAME_COLUMNS);
+      Scan scan = new Scan();
+      scan.addColumns(TABLENAME_COLUMNS);
+      Scanner s = t.getScanner(scan);
       try {
-        for (RowResult r: s) {
+        for (Result r: s) {
           if (r == null || r.size() == 0) {
             break;
           }
Index: src/test/org/apache/hadoop/hbase/util/TestMergeTool.java
===================================================================
--- src/test/org/apache/hadoop/hbase/util/TestMergeTool.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/util/TestMergeTool.java	(working copy)
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.regionserver.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.util.ToolRunner;
@@ -120,9 +121,9 @@
          */
         for (int j = 0; j < rows[i].length; j++) {
           byte [] row = rows[i][j];
-          BatchUpdate b = new BatchUpdate(row);
-          b.put(COLUMN_NAME, new ImmutableBytesWritable(row).get());
-          regions[i].batchUpdate(b, null);
+          Put put = new Put(row);
+          put.add(COLUMN_NAME, new ImmutableBytesWritable(row).get());
+          regions[i].put(put);
         }
         HRegion.addRegionToMETA(meta, regions[i]);
       }
Index: src/test/org/apache/hadoop/hbase/client/TestHTable.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestHTable.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/TestHTable.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -26,11 +26,14 @@
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.TableNotFoundException;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.HbaseMapWritable;
 import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -63,35 +66,49 @@
       admin.createTable(testTableADesc);
       
       table = new HTable(conf, tableAname);
-      BatchUpdate batchUpdate = new BatchUpdate(row);
+      Put put = new Put(row);
       
       for(int i = 0; i < 5; i++)
-        batchUpdate.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i));
+        put.add(CATALOG_FAMILY, Bytes.toBytes(Integer.toString(i)), 
+            Bytes.toBytes(i));
       
-      table.commit(batchUpdate);
+      table.put(put);
 
       assertTrue(table.exists(row));
       for(int i = 0; i < 5; i++)
         assertTrue(table.exists(row, Bytes.toBytes(COLUMN_FAMILY_STR+i)));
 
-      RowResult result = null;
-      result = table.getRow(row,  new byte[][] {COLUMN_FAMILY});
-      for(int i = 0; i < 5; i++)
-        assertTrue(result.containsKey(Bytes.toBytes(COLUMN_FAMILY_STR+i)));
+      Get get = null;
+      Result result = null;
       
-      result = table.getRow(row);
+      get = new Get(row);
+      get.addFamily(CATALOG_FAMILY);
+      result = table.get(get);
       for(int i = 0; i < 5; i++)
-        assertTrue(result.containsKey(Bytes.toBytes(COLUMN_FAMILY_STR+i)));
+        assertTrue(result.containsColumn(CATALOG_FAMILY, 
+            Bytes.toBytes(Integer.toString(i))));
 
-      batchUpdate = new BatchUpdate(row);
-      batchUpdate.put("info2:a", Bytes.toBytes("a"));
-      table.commit(batchUpdate);
+      get = new Get(row);
+      result = table.get(get);
+      for(int i = 0; i < 5; i++)
+        assertTrue(result.containsColumn(CATALOG_FAMILY, 
+            Bytes.toBytes(Integer.toString(i))));
+
+      byte [] family = Bytes.toBytes("info2");
+      byte [] qf = Bytes.toBytes("a");
       
-      result = table.getRow(row, new byte[][] { COLUMN_FAMILY,
-          Bytes.toBytes("info2:a") });
+      put = new Put(row);
+      put.add(family, qf, qf);
+      table.put(put);
+      
+      get = new Get(row);
+      get.addFamily(CATALOG_FAMILY);
+      get.addColumn(family, qf);
+      result = table.get(get);
       for(int i = 0; i < 5; i++)
-        assertTrue(result.containsKey(Bytes.toBytes(COLUMN_FAMILY_STR+i)));
-      assertTrue(result.containsKey(Bytes.toBytes("info2:a")));
+        assertTrue(result.containsColumn(CATALOG_FAMILY, 
+            Bytes.toBytes(Integer.toString(i))));
+      assertTrue(result.containsColumn(family, qf));
     } catch (IOException e) {
       e.printStackTrace();
       fail("Should not have any exception " +
@@ -138,9 +155,9 @@
       a.getConnection().getHTableDescriptor(tableAdesc.getName());
     assertTrue(meta.equals(tableAdesc));
     
-    BatchUpdate batchUpdate = new BatchUpdate(row);
-    batchUpdate.put(COLUMN_FAMILY, value);
-    a.commit(batchUpdate);
+    Put put = new Put(row);
+    put.add(CATALOG_FAMILY, null, value);
+    a.put(put);
     
     // open a new connection to A and a connection to b
     
@@ -149,16 +166,17 @@
 
     // copy data from A to B
     
-    Scanner s =
-      newA.getScanner(COLUMN_FAMILY_ARRAY, EMPTY_START_ROW);
+    Scan scan = new Scan();
+    scan.addColumns(COLUMN_FAMILY_ARRAY);
+    Scanner s = newA.getScanner(scan);
     
     try {
-      for (RowResult r : s) {
-        batchUpdate = new BatchUpdate(r.getRow());
-        for(Map.Entry<byte [], Cell> e: r.entrySet()) {
-          batchUpdate.put(e.getKey(), e.getValue().getValue());
+      for (Result r : s) {
+        put = new Put(r.getRow());
+        for(KeyValue kv : r.sorted()) {
+          put.add(kv);
         }
-        b.commit(batchUpdate);
+        b.put(put);
       }
     } finally {
       s.close();
@@ -168,7 +186,9 @@
 
     try {
       HTable anotherA = new HTable(conf, tableAname);
-      anotherA.get(row, COLUMN_FAMILY);
+      Get get = new Get(row);
+      get.addFamily(CATALOG_FAMILY);
+      anotherA.get(get);
     } catch (Exception e) {
       e.printStackTrace();
       fail();
@@ -232,9 +252,9 @@
     admin.createTable(testTableADesc);
     
     table = new HTable(conf, tableAname);
-    BatchUpdate batchUpdate = new BatchUpdate(row);
-    BatchUpdate batchUpdate2 = new BatchUpdate(row);
-    BatchUpdate batchUpdate3 = new BatchUpdate(row);
+    Put put = new Put(row);
+    Put put2 = new Put(row);
+    Put put3 = new Put(row);
     
     HbaseMapWritable<byte[],byte[]> expectedValues =
       new HbaseMapWritable<byte[],byte[]>();
@@ -245,7 +265,7 @@
       // This batchupdate is our initial batch update,
       // As such we also set our expected values to the same values
       // since we will be comparing the two
-      batchUpdate.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i));
+      put.add(Bytes.toBytes(COLUMN_FAMILY_STR+i), Bytes.toBytes(i));
       expectedValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i), Bytes.toBytes(i));
       
       badExpectedValues.put(Bytes.toBytes(COLUMN_FAMILY_STR+i),
@@ -253,32 +273,27 @@
       
       // This is our second batchupdate that we will use to update the initial
       // batchupdate
-      batchUpdate2.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+1));
+      put2.add(Bytes.toBytes(COLUMN_FAMILY_STR+i), Bytes.toBytes(i+1));
       
       // This final batch update is to check that our expected values (which
       // are now wrong)
-      batchUpdate3.put(COLUMN_FAMILY_STR+i, Bytes.toBytes(i+2));
+      put3.add(Bytes.toBytes(COLUMN_FAMILY_STR+i), Bytes.toBytes(i+2));
     }
     
     // Initialize rows
-    table.commit(batchUpdate);
+    table.put(put);
     
     // check if incorrect values are returned false
-    assertFalse(table.checkAndSave(batchUpdate2,badExpectedValues,null));
+    assertFalse(table.checkAndSave(put2,badExpectedValues));
     
     // make sure first expected values are correct
-    assertTrue(table.checkAndSave(batchUpdate2, expectedValues,null));
+    assertTrue(table.checkAndSave(put2, expectedValues));
         
     // make sure check and save truly saves the data after checking the expected
     // values
-    RowResult r = table.getRow(row);
-    byte[][] columns = batchUpdate2.getColumns();
-    for(int i = 0;i < columns.length;i++) {
-      assertTrue(Bytes.equals(r.get(columns[i]).getValue(),batchUpdate2.get(columns[i])));
-    }
     
     // make sure that the old expected values fail
-    assertFalse(table.checkAndSave(batchUpdate3, expectedValues,null));
+    assertFalse(table.checkAndSave(put3, expectedValues));
   }
 
   /**
@@ -312,43 +327,42 @@
     byte[] beforeSecondRow = Bytes.toBytes("rov");
     
     HTable table = new HTable(conf, tableAname);
-    BatchUpdate batchUpdate = new BatchUpdate(firstRow);
-    BatchUpdate batchUpdate2 = new BatchUpdate(row);
+    Put put = new Put(firstRow);
+    Put put2 = new Put(row);
     byte[] zero = new byte[]{0};
     byte[] one = new byte[]{1};
-    byte[] columnFamilyBytes = Bytes.toBytes(COLUMN_FAMILY_STR);
     
-    batchUpdate.put(COLUMN_FAMILY_STR,zero);
-    batchUpdate2.put(COLUMN_FAMILY_STR,one);
+    put.add(CATALOG_FAMILY, null, zero);
+    put2.add(CATALOG_FAMILY, null, one);
     
-    table.commit(batchUpdate);
-    table.commit(batchUpdate2);
+    table.put(put);
+    table.put(put2);
     
-    RowResult result = null;
+    Result result = null;
     
     // Test before first that null is returned
-    result = table.getClosestRowBefore(beforeFirstRow, columnFamilyBytes);
+    result = table.getClosestRowBefore(beforeFirstRow, CATALOG_FAMILY);
     assertTrue(result == null);
     
     // Test at first that first is returned
-    result = table.getClosestRowBefore(firstRow, columnFamilyBytes);
-    assertTrue(result.containsKey(COLUMN_FAMILY_STR));
-    assertTrue(Bytes.equals(result.get(COLUMN_FAMILY_STR).getValue(), zero));
+    result = table.getClosestRowBefore(firstRow, CATALOG_FAMILY);
+    assertTrue(result.containsColumn(CATALOG_FAMILY, null));
+    assertTrue(Bytes.equals(result.getValue(CATALOG_FAMILY, null), zero));
     
-    // Test inbetween first and second that first is returned
-    result = table.getClosestRowBefore(beforeSecondRow, columnFamilyBytes);
-    assertTrue(result.containsKey(COLUMN_FAMILY_STR));
-    assertTrue(Bytes.equals(result.get(COLUMN_FAMILY_STR).getValue(), zero));
+    // Test in between first and second that first is returned
+    result = table.getClosestRowBefore(beforeSecondRow, CATALOG_FAMILY);
+    assertTrue(result.containsColumn(CATALOG_FAMILY, null));
+    assertTrue(Bytes.equals(result.getValue(CATALOG_FAMILY, null), zero));
     
     // Test at second make sure second is returned
-    result = table.getClosestRowBefore(row, columnFamilyBytes);
-    assertTrue(result.containsKey(COLUMN_FAMILY_STR));
-    assertTrue(Bytes.equals(result.get(COLUMN_FAMILY_STR).getValue(), one));
+    result = table.getClosestRowBefore(row, CATALOG_FAMILY);
+    assertTrue(result.containsColumn(CATALOG_FAMILY, null));
+    assertTrue(Bytes.equals(result.getValue(CATALOG_FAMILY, null), one));
     
     // Test after second, make sure second is returned
-    result = table.getClosestRowBefore(Bytes.add(row,one), columnFamilyBytes);
-    assertTrue(result.containsKey(COLUMN_FAMILY_STR));
-    assertTrue(Bytes.equals(result.get(COLUMN_FAMILY_STR).getValue(), one));
+    result = table.getClosestRowBefore(Bytes.add(row,one), CATALOG_FAMILY);
+    assertTrue(result.containsColumn(CATALOG_FAMILY, null));
+    assertTrue(Bytes.equals(result.getValue(CATALOG_FAMILY, null), one));
   }
 
   /**
Index: src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java	(working copy)
@@ -20,18 +20,16 @@
 
 package org.apache.hadoop.hbase.client;
 
-import java.util.Iterator;
-import java.util.Map;
+import java.util.NavigableMap;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
 import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -39,13 +37,14 @@
  */
 public class TestGetRowVersions extends HBaseClusterTestCase {
   private static final Log LOG = LogFactory.getLog(TestGetRowVersions.class);
+  
   private static final String TABLE_NAME = "test";
-  private static final String CONTENTS_STR = "contents:";
-  private static final String ROW = "row";
-  private static final String COLUMN = "contents:contents";
-  private static final long TIMESTAMP = System.currentTimeMillis();
-  private static final String VALUE1 = "value1";
-  private static final String VALUE2 = "value2";
+  private static final byte [] CONTENTS = Bytes.toBytes("contents");
+  private static final byte [] ROW = Bytes.toBytes("row");
+  private static final byte [] VALUE1 = Bytes.toBytes("value1");
+  private static final byte [] VALUE2 = Bytes.toBytes("value2");
+  private static final long TIMESTAMP1 = 100L;
+  private static final long TIMESTAMP2 = 200L;
   private HBaseAdmin admin = null;
   private HTable table = null;
 
@@ -53,7 +52,7 @@
   public void setUp() throws Exception {
     super.setUp();
     HTableDescriptor desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(CONTENTS_STR));
+    desc.addFamily(new HColumnDescriptor(CONTENTS));
     this.admin = new HBaseAdmin(conf);
     this.admin.createTable(desc);
     this.table = new HTable(conf, TABLE_NAME);
@@ -61,9 +60,10 @@
 
   /** @throws Exception */
   public void testGetRowMultipleVersions() throws Exception {
-    BatchUpdate b = new BatchUpdate(ROW, TIMESTAMP);
-    b.put(COLUMN, Bytes.toBytes(VALUE1));
-    this.table.commit(b);
+    Put put = new Put(ROW);
+    put.setTimeStamp(TIMESTAMP1);
+    put.add(CONTENTS, CONTENTS, VALUE1);
+    this.table.put(put);
     // Shut down and restart the HBase cluster
     this.cluster.shutdown();
     this.zooKeeperCluster.shutdown();
@@ -72,33 +72,35 @@
     // Make a new connection
     this.table = new HTable(conf, TABLE_NAME);
     // Overwrite previous value
-    b = new BatchUpdate(ROW, TIMESTAMP);
-    b.put(COLUMN, Bytes.toBytes(VALUE2));
-    this.table.commit(b);
+    put = new Put(ROW);
+    put.setTimeStamp(TIMESTAMP2);
+    put.add(CONTENTS, CONTENTS, VALUE2);
+    this.table.put(put);
     // Now verify that getRow(row, column, latest) works
-    RowResult r = table.getRow(ROW);
+    Get get = new Get(ROW);
+    // Should get one version by default
+    Result r = table.get(get);
     assertNotNull(r);
-    assertTrue(r.size() != 0);
-    Cell c = r.get(COLUMN);
-    assertNotNull(c);
-    assertTrue(c.getValue().length != 0);
-    String value = Bytes.toString(c.getValue());
-    assertTrue(value.compareTo(VALUE2) == 0);
+    assertFalse(r.isEmpty());
+    assertTrue(r.size() == 1);
+    byte [] value = r.getValue(CONTENTS, CONTENTS);
+    assertTrue(value.length != 0);
+    assertTrue(Bytes.equals(value, VALUE2));
     // Now check getRow with multiple versions
-    r = table.getRow(ROW, HConstants.ALL_VERSIONS);
-    for (Map.Entry<byte[], Cell> e: r.entrySet()) {
-      // Column name
-//      System.err.print("  " + Bytes.toString(e.getKey()));
-      c = e.getValue();
-      
-      // Need to iterate since there may be multiple versions
-      for (Iterator<Map.Entry<Long, byte[]>> it = c.iterator();
-            it.hasNext(); ) {
-        Map.Entry<Long, byte[]> v = it.next();
-        value = Bytes.toString(v.getValue());
-//        System.err.println(" = " + value);
-        assertTrue(VALUE2.compareTo(Bytes.toString(v.getValue())) == 0);
-      }
-    }
+    get = new Get(ROW);
+    get.setMaxVersions();
+    r = table.get(get);
+    assertTrue(r.size() == 2);
+    value = r.getValue(CONTENTS, CONTENTS);
+    assertTrue(value.length != 0);
+    assertTrue(Bytes.equals(value, VALUE2));
+    NavigableMap<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>> map =
+      r.getMap();
+    NavigableMap<byte[], NavigableMap<Long, byte[]>> familyMap = 
+      map.get(CONTENTS);
+    NavigableMap<Long, byte[]> versionMap = familyMap.get(CONTENTS);
+    assertTrue(versionMap.size() == 2);
+    assertTrue(Bytes.equals(VALUE1, versionMap.get(TIMESTAMP1)));
+    assertTrue(Bytes.equals(VALUE2, versionMap.get(TIMESTAMP2)));
   }
 }
\ No newline at end of file
Index: src/test/org/apache/hadoop/hbase/client/transactional/DisabledTestTransactions.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/transactional/DisabledTestTransactions.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/transactional/DisabledTestTransactions.java	(working copy)
@@ -1,143 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-import org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Test the transaction functionality. This requires to run an
- * {@link TransactionalRegionServer}.
- */
-public class DisabledTestTransactions extends HBaseClusterTestCase {
-
-  private static final String TABLE_NAME = "table1";
-
-  private static final byte[] FAMILY = Bytes.toBytes("family:");
-  private static final byte[] COL_A = Bytes.toBytes("family:a");
-
-  private static final byte[] ROW1 = Bytes.toBytes("row1");
-  private static final byte[] ROW2 = Bytes.toBytes("row2");
-  private static final byte[] ROW3 = Bytes.toBytes("row3");
-
-  private HBaseAdmin admin;
-  private TransactionalTable table;
-  private TransactionManager transactionManager;
-
-  /** constructor */
-  public DisabledTestTransactions() {
-    conf.set(HConstants.REGION_SERVER_CLASS, TransactionalRegionInterface.class
-        .getName());
-    conf.set(HConstants.REGION_SERVER_IMPL, TransactionalRegionServer.class
-        .getName());
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    HTableDescriptor desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(FAMILY));
-    admin = new HBaseAdmin(conf);
-    admin.createTable(desc);
-    table = new TransactionalTable(conf, desc.getName());
-
-    transactionManager = new TransactionManager(conf);
-    writeInitalRow();
-  }
-
-  private void writeInitalRow() throws IOException {
-    BatchUpdate update = new BatchUpdate(ROW1);
-    update.put(COL_A, Bytes.toBytes(1));
-    table.commit(update);
-  }
-
-  public void testSimpleTransaction() throws IOException,
-      CommitUnsuccessfulException {
-    TransactionState transactionState = makeTransaction1();
-    transactionManager.tryCommit(transactionState);
-  }
-
-  public void testTwoTransactionsWithoutConflict() throws IOException,
-      CommitUnsuccessfulException {
-    TransactionState transactionState1 = makeTransaction1();
-    TransactionState transactionState2 = makeTransaction2();
-
-    transactionManager.tryCommit(transactionState1);
-    transactionManager.tryCommit(transactionState2);
-  }
-
-  public void TestTwoTransactionsWithConflict() throws IOException,
-      CommitUnsuccessfulException {
-    TransactionState transactionState1 = makeTransaction1();
-    TransactionState transactionState2 = makeTransaction2();
-
-    transactionManager.tryCommit(transactionState2);
-
-    try {
-      transactionManager.tryCommit(transactionState1);
-      fail();
-    } catch (CommitUnsuccessfulException e) {
-      // Good
-    }
-  }
-
-  // Read from ROW1,COL_A and put it in ROW2_COLA and ROW3_COLA
-  private TransactionState makeTransaction1() throws IOException {
-    TransactionState transactionState = transactionManager.beginTransaction();
-
-    Cell row1_A = table.get(transactionState, ROW1, COL_A);
-
-    BatchUpdate write1 = new BatchUpdate(ROW2);
-    write1.put(COL_A, row1_A.getValue());
-    table.commit(transactionState, write1);
-
-    BatchUpdate write2 = new BatchUpdate(ROW3);
-    write2.put(COL_A, row1_A.getValue());
-    table.commit(transactionState, write2);
-
-    return transactionState;
-  }
-
-  // Read ROW1,COL_A, increment its (integer) value, write back
-  private TransactionState makeTransaction2() throws IOException {
-    TransactionState transactionState = transactionManager.beginTransaction();
-
-    Cell row1_A = table.get(transactionState, ROW1, COL_A);
-
-    int value = Bytes.toInt(row1_A.getValue());
-
-    BatchUpdate write = new BatchUpdate(ROW1);
-    write.put(COL_A, Bytes.toBytes(value + 1));
-    table.commit(transactionState, write);
-
-    return transactionState;
-  }
-}
Index: src/test/org/apache/hadoop/hbase/client/transactional/StressTestTransactions.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/transactional/StressTestTransactions.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/transactional/StressTestTransactions.java	(working copy)
@@ -1,420 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.io.IOException;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import junit.framework.Assert;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-import org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Stress Test the transaction functionality. This requires to run an
- * {@link TransactionalRegionServer}. We run many threads doing reads/writes
- * which may conflict with each other. We have two types of transactions, those
- * which operate on rows of a single table, and those which operate on rows
- * across multiple tables. Each transaction type has a modification operation
- * which changes two values while maintaining the sum. Also each transaction
- * type has a consistency-check operation which sums all rows and verifies that
- * the sum is as expected.
- */
-public class StressTestTransactions extends HBaseClusterTestCase {
-  protected static final Log LOG = LogFactory
-      .getLog(StressTestTransactions.class);
-
-  private static final int NUM_TABLES = 3;
-  private static final int NUM_ST_ROWS = 3;
-  private static final int NUM_MT_ROWS = 3;
-  private static final int NUM_TRANSACTIONS_PER_THREAD = 100;
-  private static final int NUM_SINGLE_TABLE_THREADS = 6;
-  private static final int NUM_MULTI_TABLE_THREADS = 6;
-  private static final int PRE_COMMIT_SLEEP = 10;
-  protected static final Random RAND = new Random();
-
-  private static final byte[] FAMILY = Bytes.toBytes("family:");
-  static final byte[] COL = Bytes.toBytes("family:a");
-
-  private HBaseAdmin admin;
-  protected TransactionalTable[] tables;
-  protected TransactionManager transactionManager;
-
-  /** constructor */
-  public StressTestTransactions() {
-    conf.set(HConstants.REGION_SERVER_CLASS, TransactionalRegionInterface.class
-        .getName());
-    conf.set(HConstants.REGION_SERVER_IMPL, TransactionalRegionServer.class
-        .getName());
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    tables = new TransactionalTable[NUM_TABLES];
-
-    for (int i = 0; i < tables.length; i++) {
-      HTableDescriptor desc = new HTableDescriptor(makeTableName(i));
-      desc.addFamily(new HColumnDescriptor(FAMILY));
-      admin = new HBaseAdmin(conf);
-      admin.createTable(desc);
-      tables[i] = new TransactionalTable(conf, desc.getName());
-    }
-
-    transactionManager = new TransactionManager(conf);
-  }
-
-  private String makeTableName(final int i) {
-    return "table" + i;
-  }
-
-  private void writeInitalValues() throws IOException {
-    for (TransactionalTable table : tables) {
-      for (int i = 0; i < NUM_ST_ROWS; i++) {
-        byte[] row = makeSTRow(i);
-        BatchUpdate b = new BatchUpdate(row);
-        b.put(COL, Bytes.toBytes(SingleTableTransactionThread.INITIAL_VALUE));
-        table.commit(b);
-      }
-      for (int i = 0; i < NUM_MT_ROWS; i++) {
-        byte[] row = makeMTRow(i);
-        BatchUpdate b = new BatchUpdate(row);
-        b.put(COL, Bytes.toBytes(MultiTableTransactionThread.INITIAL_VALUE));
-        table.commit(b);
-      }
-    }
-  }
-
-  protected byte[] makeSTRow(final int i) {
-    return Bytes.toBytes("st" + i);
-  }
-
-  protected byte[] makeMTRow(final int i) {
-    return Bytes.toBytes("mt" + i);
-  }
-
-  static int nextThreadNum = 1;
-  protected static final AtomicBoolean stopRequest = new AtomicBoolean(false);
-  static final AtomicBoolean consistencyFailure = new AtomicBoolean(false);
-
-  // Thread which runs transactions
-  abstract class TransactionThread extends Thread {
-    private int numRuns = 0;
-    private int numAborts = 0;
-    private int numUnknowns = 0;
-
-    public TransactionThread(final String namePrefix) {
-      super.setName(namePrefix + "transaction " + nextThreadNum++);
-    }
-
-    @Override
-    public void run() {
-      for (int i = 0; i < NUM_TRANSACTIONS_PER_THREAD; i++) {
-        if (stopRequest.get()) {
-          return;
-        }
-        try {
-          numRuns++;
-          transaction();
-        } catch (UnknownTransactionException e) {
-          numUnknowns++;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        } catch (CommitUnsuccessfulException e) {
-          numAborts++;
-        }
-      }
-    }
-
-    protected abstract void transaction() throws IOException,
-        CommitUnsuccessfulException;
-
-    public int getNumAborts() {
-      return numAborts;
-    }
-
-    public int getNumUnknowns() {
-      return numUnknowns;
-    }
-
-    protected void preCommitSleep() {
-      try {
-        Thread.sleep(PRE_COMMIT_SLEEP);
-      } catch (InterruptedException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    protected void consistencyFailure() {
-      LOG.fatal("Consistency failure");
-      stopRequest.set(true);
-      consistencyFailure.set(true);
-    }
-
-    /**
-     * Get the numRuns.
-     * 
-     * @return Return the numRuns.
-     */
-    public int getNumRuns() {
-      return numRuns;
-    }
-
-  }
-
-  // Atomically change the value of two rows rows while maintaining the sum.
-  // This should preserve the global sum of the rows, which is also checked
-  // with a transaction.
-  private class SingleTableTransactionThread extends TransactionThread {
-    private static final int INITIAL_VALUE = 10;
-    public static final int TOTAL_SUM = INITIAL_VALUE * NUM_ST_ROWS;
-    private static final int MAX_TRANSFER_AMT = 100;
-
-    private TransactionalTable table;
-    boolean doCheck = false;
-
-    public SingleTableTransactionThread() {
-      super("single table ");
-    }
-
-    @Override
-    protected void transaction() throws IOException,
-        CommitUnsuccessfulException {
-      if (doCheck) {
-        checkTotalSum();
-      } else {
-        doSingleRowChange();
-      }
-      doCheck = !doCheck;
-    }
-
-    private void doSingleRowChange() throws IOException,
-        CommitUnsuccessfulException {
-      table = tables[RAND.nextInt(NUM_TABLES)];
-      int transferAmount = RAND.nextInt(MAX_TRANSFER_AMT * 2)
-          - MAX_TRANSFER_AMT;
-      int row1Index = RAND.nextInt(NUM_ST_ROWS);
-      int row2Index;
-      do {
-        row2Index = RAND.nextInt(NUM_ST_ROWS);
-      } while (row2Index == row1Index);
-      byte[] row1 = makeSTRow(row1Index);
-      byte[] row2 = makeSTRow(row2Index);
-
-      TransactionState transactionState = transactionManager.beginTransaction();
-      int row1Amount = Bytes.toInt(table.get(transactionState, row1, COL)
-          .getValue());
-      int row2Amount = Bytes.toInt(table.get(transactionState, row2, COL)
-          .getValue());
-
-      row1Amount -= transferAmount;
-      row2Amount += transferAmount;
-
-      BatchUpdate update = new BatchUpdate(row1);
-      update.put(COL, Bytes.toBytes(row1Amount));
-      table.commit(transactionState, update);
-      update = new BatchUpdate(row2);
-      update.put(COL, Bytes.toBytes(row2Amount));
-      table.commit(transactionState, update);
-
-      super.preCommitSleep();
-
-      transactionManager.tryCommit(transactionState);
-      LOG.debug("Commited");
-    }
-
-    // Check the table we last mutated
-    private void checkTotalSum() throws IOException,
-        CommitUnsuccessfulException {
-      TransactionState transactionState = transactionManager.beginTransaction();
-      int totalSum = 0;
-      for (int i = 0; i < NUM_ST_ROWS; i++) {
-        totalSum += Bytes.toInt(table.get(transactionState, makeSTRow(i), COL)
-            .getValue());
-      }
-
-      transactionManager.tryCommit(transactionState);
-      if (TOTAL_SUM != totalSum) {
-        super.consistencyFailure();
-      }
-    }
-
-  }
-
-  // Similar to SingleTable, but this time we maintain consistency across tables
-  // rather than rows
-  private class MultiTableTransactionThread extends TransactionThread {
-    private static final int INITIAL_VALUE = 1000;
-    public static final int TOTAL_SUM = INITIAL_VALUE * NUM_TABLES;
-    private static final int MAX_TRANSFER_AMT = 100;
-
-    private byte[] row;
-    boolean doCheck = false;
-
-    public MultiTableTransactionThread() {
-      super("multi table");
-    }
-
-    @Override
-    protected void transaction() throws IOException,
-        CommitUnsuccessfulException {
-      if (doCheck) {
-        checkTotalSum();
-      } else {
-        doSingleRowChange();
-      }
-      doCheck = !doCheck;
-    }
-
-    private void doSingleRowChange() throws IOException,
-        CommitUnsuccessfulException {
-      row = makeMTRow(RAND.nextInt(NUM_MT_ROWS));
-      int transferAmount = RAND.nextInt(MAX_TRANSFER_AMT * 2)
-          - MAX_TRANSFER_AMT;
-      int table1Index = RAND.nextInt(tables.length);
-      int table2Index;
-      do {
-        table2Index = RAND.nextInt(tables.length);
-      } while (table2Index == table1Index);
-
-      TransactionalTable table1 = tables[table1Index];
-      TransactionalTable table2 = tables[table2Index];
-
-      TransactionState transactionState = transactionManager.beginTransaction();
-      int table1Amount = Bytes.toInt(table1.get(transactionState, row, COL)
-          .getValue());
-      int table2Amount = Bytes.toInt(table2.get(transactionState, row, COL)
-          .getValue());
-
-      table1Amount -= transferAmount;
-      table2Amount += transferAmount;
-
-      BatchUpdate update = new BatchUpdate(row);
-      update.put(COL, Bytes.toBytes(table1Amount));
-      table1.commit(transactionState, update);
-
-      update = new BatchUpdate(row);
-      update.put(COL, Bytes.toBytes(table2Amount));
-      table2.commit(transactionState, update);
-
-      super.preCommitSleep();
-
-      transactionManager.tryCommit(transactionState);
-
-      LOG.trace(Bytes.toString(table1.getTableName()) + ": " + table1Amount);
-      LOG.trace(Bytes.toString(table2.getTableName()) + ": " + table2Amount);
-
-    }
-
-    private void checkTotalSum() throws IOException,
-        CommitUnsuccessfulException {
-      TransactionState transactionState = transactionManager.beginTransaction();
-      int totalSum = 0;
-      int[] amounts = new int[tables.length];
-      for (int i = 0; i < tables.length; i++) {
-        int amount = Bytes.toInt(tables[i].get(transactionState, row, COL)
-            .getValue());
-        amounts[i] = amount;
-        totalSum += amount;
-      }
-
-      transactionManager.tryCommit(transactionState);
-
-      for (int i = 0; i < tables.length; i++) {
-        LOG.trace(Bytes.toString(tables[i].getTableName()) + ": " + amounts[i]);
-      }
-
-      if (TOTAL_SUM != totalSum) {
-        super.consistencyFailure();
-      }
-    }
-
-  }
-
-  public void testStressTransactions() throws IOException, InterruptedException {
-    writeInitalValues();
-
-    List<TransactionThread> transactionThreads = new LinkedList<TransactionThread>();
-
-    for (int i = 0; i < NUM_SINGLE_TABLE_THREADS; i++) {
-      TransactionThread transactionThread = new SingleTableTransactionThread();
-      transactionThread.start();
-      transactionThreads.add(transactionThread);
-    }
-
-    for (int i = 0; i < NUM_MULTI_TABLE_THREADS; i++) {
-      TransactionThread transactionThread = new MultiTableTransactionThread();
-      transactionThread.start();
-      transactionThreads.add(transactionThread);
-    }
-
-    for (TransactionThread transactionThread : transactionThreads) {
-      transactionThread.join();
-    }
-
-    for (TransactionThread transactionThread : transactionThreads) {
-      LOG.info(transactionThread.getName() + " done with "
-          + transactionThread.getNumAborts() + " aborts, and "
-          + transactionThread.getNumUnknowns() + " unknown transactions of "
-          + transactionThread.getNumRuns());
-    }
-
-    doFinalConsistencyChecks();
-  }
-
-  private void doFinalConsistencyChecks() throws IOException {
-
-    int[] mtSums = new int[NUM_MT_ROWS];
-    for (int i = 0; i < mtSums.length; i++) {
-      mtSums[i] = 0;
-    }
-
-    for (TransactionalTable table : tables) {
-      int thisTableSum = 0;
-      for (int i = 0; i < NUM_ST_ROWS; i++) {
-        byte[] row = makeSTRow(i);
-        thisTableSum += Bytes.toInt(table.get(row, COL).getValue());
-      }
-      Assert.assertEquals(SingleTableTransactionThread.TOTAL_SUM, thisTableSum);
-
-      for (int i = 0; i < NUM_MT_ROWS; i++) {
-        byte[] row = makeMTRow(i);
-        mtSums[i] += Bytes.toInt(table.get(row, COL).getValue());
-      }
-    }
-
-    for (int mtSum : mtSums) {
-      Assert.assertEquals(MultiTableTransactionThread.TOTAL_SUM, mtSum);
-    }
-  }
-}
Index: src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/TestBatchUpdate.java	(working copy)
@@ -1,214 +0,0 @@
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client;
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Map;
-
-import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Test batch updates
- */
-public class TestBatchUpdate extends HBaseClusterTestCase {
-  private static final String CONTENTS_STR = "contents:";
-  private static final byte [] CONTENTS = Bytes.toBytes(CONTENTS_STR);
-  private static final String SMALLFAM_STR = "smallfam:";
-  private static final byte [] SMALLFAM = Bytes.toBytes(SMALLFAM_STR);
-  private static final int SMALL_LENGTH = 1;
-  private static final int NB_BATCH_ROWS = 10;
-  private byte[] value;
-  private byte[] smallValue;
-
-  private HTableDescriptor desc = null;
-  private HTable table = null;
-
-  /**
-   * @throws UnsupportedEncodingException
-   */
-  public TestBatchUpdate() throws UnsupportedEncodingException {
-    super();
-    value = Bytes.toBytes("abcd");
-    smallValue = Bytes.toBytes("a");
-  }
-  
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    this.desc = new HTableDescriptor("test");
-    desc.addFamily(new HColumnDescriptor(CONTENTS_STR));
-    desc.addFamily(new HColumnDescriptor(SMALLFAM, 
-        HColumnDescriptor.DEFAULT_VERSIONS, 
-        HColumnDescriptor.DEFAULT_COMPRESSION,
-        HColumnDescriptor.DEFAULT_IN_MEMORY, 
-        HColumnDescriptor.DEFAULT_BLOCKCACHE, SMALL_LENGTH, 
-        HColumnDescriptor.DEFAULT_TTL, HColumnDescriptor.DEFAULT_BLOOMFILTER));
-    HBaseAdmin admin = new HBaseAdmin(conf);
-    admin.createTable(desc);
-    table = new HTable(conf, desc.getName());
-  }
-
-  /**
-   * @throws IOException
-   */
-  public void testBatchUpdate() throws IOException {
-    BatchUpdate bu = new BatchUpdate("row1");
-    bu.put(CONTENTS, value);
-    bu.delete(CONTENTS);
-    table.commit(bu);
-
-    bu = new BatchUpdate("row2");
-    bu.put(CONTENTS, value);
-    byte[][] getColumns = bu.getColumns();
-    assertEquals(getColumns.length, 1);
-    assertTrue(Arrays.equals(getColumns[0], CONTENTS));
-    assertTrue(bu.hasColumn(CONTENTS));
-    assertFalse(bu.hasColumn(new byte[] {}));
-    byte[] getValue = bu.get(getColumns[0]);
-    assertTrue(Arrays.equals(getValue, value));
-    table.commit(bu);
-
-    byte [][] columns = { CONTENTS };
-    Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
-    for (RowResult r : scanner) {
-      for(Map.Entry<byte [], Cell> e: r.entrySet()) {
-        System.out.println(Bytes.toString(r.getRow()) + ": row: " + e.getKey() + " value: " + 
-            new String(e.getValue().getValue(), HConstants.UTF8_ENCODING));
-      }
-    }
-  }
-  
-  public void testBatchUpdateMaxLength() {
-    // Test for a single good value
-    BatchUpdate batchUpdate = new BatchUpdate("row1");
-    batchUpdate.put(SMALLFAM, value);
-    try {
-      table.commit(batchUpdate);
-      fail("Value is too long, should throw exception");
-    } catch (IOException e) {
-      // This is expected
-    }
-    // Try to see if it's still inserted
-    try {
-      Cell cell = table.get("row1", SMALLFAM_STR);
-      assertNull(cell);
-    } catch (IOException e) {
-      e.printStackTrace();
-      fail("This is unexpected");
-    }
-    // Try to put a good value
-    batchUpdate = new BatchUpdate("row1");
-    batchUpdate.put(SMALLFAM, smallValue);
-    try {
-      table.commit(batchUpdate);
-    } catch (IOException e) {
-      fail("Value is long enough, should not throw exception");
-    }
-  }
-  
-  public void testRowsBatchUpdate() {
-    ArrayList<BatchUpdate> rowsUpdate = new ArrayList<BatchUpdate>();
-    for(int i = 0; i < NB_BATCH_ROWS; i++) {
-      BatchUpdate batchUpdate = new BatchUpdate("row"+i);
-      batchUpdate.put(CONTENTS, value);
-      rowsUpdate.add(batchUpdate);
-    }
-    try {
-      table.commit(rowsUpdate);  
-    
-      byte [][] columns = { CONTENTS };
-      Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
-      int nbRows = 0;
-      for(@SuppressWarnings("unused") RowResult row : scanner)
-        nbRows++;
-      assertEquals(NB_BATCH_ROWS, nbRows);
-    } catch (IOException e) {
-      fail("This is unexpected : " + e);
-    }
-  }
-  
-  public void testRowsBatchUpdateBufferedOneFlush() {
-    table.setAutoFlush(false);
-    ArrayList<BatchUpdate> rowsUpdate = new ArrayList<BatchUpdate>();
-    for(int i = 0; i < NB_BATCH_ROWS*10; i++) {
-      BatchUpdate batchUpdate = new BatchUpdate("row"+i);
-      batchUpdate.put(CONTENTS, value);
-      rowsUpdate.add(batchUpdate);
-    }
-    try {
-      table.commit(rowsUpdate);  
-    
-      byte [][] columns = { CONTENTS };
-      Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
-      int nbRows = 0;
-      for(@SuppressWarnings("unused") RowResult row : scanner)
-        nbRows++;
-      assertEquals(0, nbRows);  
-      scanner.close();
-      
-      table.flushCommits();
-      
-      scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
-      nbRows = 0;
-      for(@SuppressWarnings("unused") RowResult row : scanner)
-        nbRows++;
-      assertEquals(NB_BATCH_ROWS*10, nbRows);
-    } catch (IOException e) {
-      fail("This is unexpected : " + e);
-    }
-  }
-  
-  public void testRowsBatchUpdateBufferedManyManyFlushes() {
-    table.setAutoFlush(false);
-    table.setWriteBufferSize(10);
-    ArrayList<BatchUpdate> rowsUpdate = new ArrayList<BatchUpdate>();
-    for(int i = 0; i < NB_BATCH_ROWS*10; i++) {
-      BatchUpdate batchUpdate = new BatchUpdate("row"+i);
-      batchUpdate.put(CONTENTS, value);
-      rowsUpdate.add(batchUpdate);
-    }
-    try {
-      table.commit(rowsUpdate);
-      
-      table.flushCommits();
-      
-      byte [][] columns = { CONTENTS };
-      Scanner scanner = table.getScanner(columns, HConstants.EMPTY_START_ROW);
-      int nbRows = 0;
-      for(@SuppressWarnings("unused") RowResult row : scanner)
-        nbRows++;
-      assertEquals(NB_BATCH_ROWS*10, nbRows);
-    } catch (IOException e) {
-      fail("This is unexpected : " + e);
-    }
-  }
-  
-  
-}
Index: src/test/org/apache/hadoop/hbase/client/tableindexed/TestIndexedTable.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/tableindexed/TestIndexedTable.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/tableindexed/TestIndexedTable.java	(working copy)
@@ -1,131 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.PerformanceEvaluation;
-import org.apache.hadoop.hbase.client.Scanner;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer;
-import org.apache.hadoop.hbase.util.Bytes;
-
-public class TestIndexedTable extends HBaseClusterTestCase {
-
-  private static final Log LOG = LogFactory.getLog(TestIndexedTable.class);
-
-  private static final String TABLE_NAME = "table1";
-
-  private static final byte[] FAMILY = Bytes.toBytes("family:");
-  private static final byte[] COL_A = Bytes.toBytes("family:a");
-  private static final String INDEX_COL_A = "A";
-
-  private static final int NUM_ROWS = 10;
-  private static final int MAX_VAL = 10000;
-
-  private IndexedTableAdmin admin;
-  private IndexedTable table;
-  private Random random = new Random();
-
-  /** constructor */
-  public TestIndexedTable() {
-    conf
-        .set(HConstants.REGION_SERVER_IMPL, IndexedRegionServer.class.getName());
-    conf.setInt("hbase.master.info.port", -1);
-    conf.setInt("hbase.regionserver.info.port", -1);
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-
-    HTableDescriptor desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(FAMILY));
-
-    // Create a new index that does lexicographic ordering on COL_A
-    IndexSpecification colAIndex = new IndexSpecification(INDEX_COL_A,
-        COL_A);
-    desc.addIndex(colAIndex);
-
-    admin = new IndexedTableAdmin(conf);
-    admin.createTable(desc);
-    table = new IndexedTable(conf, desc.getName());
-  }
-
-  private void writeInitalRows() throws IOException {
-    for (int i = 0; i < NUM_ROWS; i++) {
-      BatchUpdate update = new BatchUpdate(PerformanceEvaluation.format(i));
-      byte[] colA = PerformanceEvaluation.format(random.nextInt(MAX_VAL));
-      update.put(COL_A, colA);
-      table.commit(update);
-      LOG.info("Inserted row [" + Bytes.toString(update.getRow()) + "] val: ["
-          + Bytes.toString(colA) + "]");
-    }
-  }
-
-
-  public void testInitialWrites() throws IOException {
-    writeInitalRows();
-    assertRowsInOrder(NUM_ROWS);
-  }
-  
-  private void assertRowsInOrder(int numRowsExpected) throws IndexNotFoundException, IOException {
-    Scanner scanner = table.getIndexedScanner(INDEX_COL_A,
-        HConstants.EMPTY_START_ROW, null, null, null);
-    int numRows = 0;
-    byte[] lastColA = null;
-    for (RowResult rowResult : scanner) {
-      byte[] colA = rowResult.get(COL_A).getValue();
-      LOG.info("index scan : row [" + Bytes.toString(rowResult.getRow())
-          + "] value [" + Bytes.toString(colA) + "]");
-      if (lastColA != null) {
-        Assert.assertTrue(Bytes.compareTo(lastColA, colA) <= 0);
-      }
-      lastColA = colA;
-      numRows++;
-    }
-    Assert.assertEquals(numRowsExpected, numRows);  
-  }
-
-  public void testMultipleWrites() throws IOException {
-    writeInitalRows();
-    writeInitalRows(); // Update the rows.
-    assertRowsInOrder(NUM_ROWS);
-  }
-  
-  public void testDelete() throws IOException {
-    writeInitalRows();
-    // Delete the first row;
-    table.deleteAll(PerformanceEvaluation.format(0));
-    
-    assertRowsInOrder(NUM_ROWS - 1);    
-  }
-
-}
Index: src/test/org/apache/hadoop/hbase/client/TestPut.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestPut.java	(revision 0)
+++ src/test/org/apache/hadoop/hbase/client/TestPut.java	(revision 0)
@@ -0,0 +1,236 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.io.IOException;
+import java.io.UnsupportedEncodingException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.HBaseClusterTestCase;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Test puts
+ */
+public class TestPut extends HBaseClusterTestCase {
+  private static final byte [] CONTENTS_FAMILY = Bytes.toBytes("contents");
+  private static final byte [] SMALL_FAMILY = Bytes.toBytes("smallfam");
+
+  private static final byte [] row1 = Bytes.toBytes("row1");
+  private static final byte [] row2 = Bytes.toBytes("row2");
+  
+  private static final int SMALL_LENGTH = 1;
+  private static final int NB_BATCH_ROWS = 10;
+  private byte [] value;
+  private byte [] smallValue;
+
+  private HTableDescriptor desc = null;
+  private HTable table = null;
+
+  /**
+   * @throws UnsupportedEncodingException
+   */
+  public TestPut() throws UnsupportedEncodingException {
+    super();
+    value = Bytes.toBytes("abcd");
+    smallValue = Bytes.toBytes("a");
+  }
+  
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    this.desc = new HTableDescriptor("test");
+    desc.addFamily(new HColumnDescriptor(CONTENTS_FAMILY));
+    desc.addFamily(new HColumnDescriptor(SMALL_FAMILY, 
+        HColumnDescriptor.DEFAULT_VERSIONS, 
+        HColumnDescriptor.DEFAULT_COMPRESSION,
+        HColumnDescriptor.DEFAULT_IN_MEMORY, 
+        HColumnDescriptor.DEFAULT_BLOCKCACHE, SMALL_LENGTH, 
+        HColumnDescriptor.DEFAULT_TTL, HColumnDescriptor.DEFAULT_BLOOMFILTER));
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    admin.createTable(desc);
+    table = new HTable(conf, desc.getName());
+  }
+
+  /**
+   * @throws IOException
+   */
+  public void testPut() throws IOException {
+    
+    Put put = new Put(row1);
+    put.add(CONTENTS_FAMILY, null, value);
+    table.put(put);
+
+    put = new Put(row2);
+    put.add(CONTENTS_FAMILY, null, value);
+    
+    assertEquals(put.size(), 1);
+    assertEquals(put.getFamilyMap().get(CONTENTS_FAMILY).size(), 1);
+    
+    KeyValue kv = put.getFamilyMap().get(CONTENTS_FAMILY).get(0);
+    
+    assertTrue(Bytes.equals(kv.getFamily(), CONTENTS_FAMILY));
+    // will it return null or an empty byte array?
+    assertTrue(kv.getQualifier() == null);
+    
+    assertTrue(Bytes.equals(kv.getValue(), value));
+    
+    table.put(put);
+
+    Scan scan = new Scan();
+    scan.addColumn(CONTENTS_FAMILY, null);
+    Scanner scanner = table.getScanner(scan);
+    for (Result r : scanner) {
+      for(KeyValue key : r.sorted()) {
+        System.out.println(Bytes.toString(r.getRow()) + ": " + key.toString());
+      }
+    }
+  }
+  
+  public void testPutMaxLength() {
+    // Test for a single good value
+    Put put = new Put(row1);
+    put.add(SMALL_FAMILY, null, value);
+    try {
+      table.put(put);
+      fail("Value is too long, should throw exception");
+    } catch (IOException e) {
+      // This is expected
+    }
+    // Try to see if it's still inserted
+    try {
+      Get get = new Get(row1);
+      get.addFamily(SMALL_FAMILY);
+      Result r = table.get(get);
+      assertTrue(r.isEmpty());
+    } catch (IOException e) {
+      e.printStackTrace();
+      fail("This is unexpected");
+    }
+    // Try to put a good value
+    put = new Put(row1);
+    put.add(SMALL_FAMILY, null, smallValue);
+    try {
+      table.put(put);
+    } catch (IOException e) {
+      fail("Value is long enough, should not throw exception");
+    }
+  }
+  
+  public void testRowsPut() {
+    ArrayList<Put> rowsUpdate = new ArrayList<Put>();
+    for(int i = 0; i < NB_BATCH_ROWS; i++) {
+      byte [] row = Bytes.toBytes("row" + i);
+      Put put = new Put(row);
+      put.add(CONTENTS_FAMILY, null, value);
+      rowsUpdate.add(put);
+    }
+    try {
+      table.put(rowsUpdate);  
+    
+      Scan scan = new Scan();
+      scan.addFamily(CONTENTS_FAMILY);
+      Scanner scanner = table.getScanner(scan);
+      int nbRows = 0;
+      for(@SuppressWarnings("unused") Result row : scanner)
+        nbRows++;
+      assertEquals(NB_BATCH_ROWS, nbRows);
+    } catch (IOException e) {
+      fail("This is unexpected : " + e);
+    }
+  }
+  
+  public void testRowsPutBufferedOneFlush() {
+    table.setAutoFlush(false);
+    ArrayList<Put> rowsUpdate = new ArrayList<Put>();
+    for(int i = 0; i < NB_BATCH_ROWS*10; i++) {
+      byte [] row = Bytes.toBytes("row" + i);
+      Put put = new Put(row);
+      put.add(CONTENTS_FAMILY, null, value);
+      rowsUpdate.add(put);
+    }
+    try {
+      table.put(rowsUpdate);  
+    
+      Scan scan = new Scan();
+      scan.addFamily(CONTENTS_FAMILY);
+      Scanner scanner = table.getScanner(scan);
+      int nbRows = 0;
+      for(@SuppressWarnings("unused") Result row : scanner)
+        nbRows++;
+      assertEquals(0, nbRows);  
+      scanner.close();
+      
+      table.flushCommits();
+      
+      scan = new Scan();
+      scan.addFamily(CONTENTS_FAMILY);
+      scanner = table.getScanner(scan);
+      nbRows = 0;
+      for(@SuppressWarnings("unused") Result row : scanner)
+        nbRows++;
+      assertEquals(NB_BATCH_ROWS*10, nbRows);
+    } catch (IOException e) {
+      fail("This is unexpected : " + e);
+    }
+  }
+  
+  public void testRowsPutBufferedManyManyFlushes() {
+    table.setAutoFlush(false);
+    table.setWriteBufferSize(10);
+    ArrayList<Put> rowsUpdate = new ArrayList<Put>();
+    for(int i = 0; i < NB_BATCH_ROWS*10; i++) {
+      byte [] row = Bytes.toBytes("row" + i);
+      Put put = new Put(row);
+      put.add(CONTENTS_FAMILY, null, value);
+      rowsUpdate.add(put);
+    }
+    try {
+      table.put(rowsUpdate);
+      
+      table.flushCommits();
+      
+      Scan scan = new Scan();
+      scan.addFamily(CONTENTS_FAMILY);
+      Scanner scanner = table.getScanner(scan);
+      int nbRows = 0;
+      for(@SuppressWarnings("unused") Result row : scanner)
+        nbRows++;
+      assertEquals(NB_BATCH_ROWS*10, nbRows);
+    } catch (IOException e) {
+      fail("This is unexpected : " + e);
+    }
+  }
+  
+  
+}
Index: src/test/org/apache/hadoop/hbase/client/TestForceSplit.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestForceSplit.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/TestForceSplit.java	(working copy)
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -62,9 +62,9 @@
           k[0] = b1;
           k[1] = b2;
           k[2] = b3;
-          BatchUpdate update = new BatchUpdate(k);
-          update.put(columnName, k);
-          table.commit(update);
+          Put put = new Put(k);
+          put.add(columnName, k);
+          table.put(put);
         }
       }
     }
Index: src/test/org/apache/hadoop/hbase/client/TestScannerTimes.java
===================================================================
--- src/test/org/apache/hadoop/hbase/client/TestScannerTimes.java	(revision 775412)
+++ src/test/org/apache/hadoop/hbase/client/TestScannerTimes.java	(working copy)
@@ -26,9 +26,14 @@
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.util.Bytes;
 
 /**
  * Test that verifies that scanners return a different timestamp for values that
@@ -36,9 +41,9 @@
  */
 public class TestScannerTimes extends HBaseClusterTestCase {
   private static final String TABLE_NAME = "hbase737";
-  private static final String FAM1 = "fam1:";
-  private static final String FAM2 = "fam2:";
-  private static final String ROW = "row";
+  private static final byte [] FAM1 = Bytes.toBytes("fam1");
+  private static final byte [] FAM2 = Bytes.toBytes("fam2");
+  private static final byte [] ROW = Bytes.toBytes("row");
   
   /**
    * test for HBASE-737
@@ -57,9 +62,9 @@
     HTable table = new HTable(conf, TABLE_NAME);
     
     // Insert some values
-    BatchUpdate b = new BatchUpdate(ROW);
-    b.put(FAM1 + "letters", "abcdefg".getBytes(HConstants.UTF8_ENCODING));
-    table.commit(b);
+    Put put = new Put(ROW);
+    put.add(FAM1, Bytes.toBytes("letters"), Bytes.toBytes("abcdefg"));
+    table.put(put);
     
     try {
       Thread.sleep(1000);
@@ -67,35 +72,34 @@
       //ignore
     }
     
-    b = new BatchUpdate(ROW);
-    b.put(FAM1 + "numbers", "123456".getBytes(HConstants.UTF8_ENCODING));
-    table.commit(b);
+    put = new Put(ROW);
+    put.add(FAM1, Bytes.toBytes("numbers"), Bytes.toBytes("123456"));
+    table.put(put);
     
     try {
       Thread.sleep(1000);
     } catch (InterruptedException i) {
       //ignore
     }
+
+    put = new Put(ROW);
+    put.add(FAM2, Bytes.toBytes("letters"), Bytes.toBytes("hijklmnop"));
+    table.put(put);
     
-    b = new BatchUpdate(ROW);
-    b.put(FAM2 + "letters", "hijklmnop".getBytes(HConstants.UTF8_ENCODING));
-    table.commit(b);
-    
     long times[] = new long[3];
-    byte[][] columns = new byte[][] {
-        FAM1.getBytes(HConstants.UTF8_ENCODING),
-        FAM2.getBytes(HConstants.UTF8_ENCODING)
-    };
     
     // First scan the memcache
     
-    Scanner s = table.getScanner(columns);
+    Scan scan = new Scan();
+    scan.addFamily(FAM1);
+    scan.addFamily(FAM2);
+    Scanner s = table.getScanner(scan);
     try {
       int index = 0;
-      RowResult r = null;
+      Result r = null;
       while ((r = s.next()) != null) {
-        for (Cell c: r.values()) {
-          times[index++] = c.getTimestamp();
+        for(KeyValue key : r.sorted()) {
+          times[index++] = key.getTimestamp();
         }
       }
     } finally {
@@ -116,14 +120,16 @@
     } catch (InterruptedException i) {
       //ignore
     }
-    
-    s = table.getScanner(columns);
+    scan = new Scan();
+    scan.addFamily(FAM1);
+    scan.addFamily(FAM2);
+    s = table.getScanner(scan);
     try {
       int index = 0;
-      RowResult r = null;
+      Result r = null;
       while ((r = s.next()) != null) {
-        for (Cell c: r.values()) {
-          times[index++] = c.getTimestamp();
+        for(KeyValue key : r.sorted()) {
+          times[index++] = key.getTimestamp();
         }
       }
     } finally {
@@ -134,6 +140,5 @@
         assertTrue(times[j] > times[i]);
       }
     }
-    
   }
 }
Index: src/java/org/apache/hadoop/hbase/HColumnDescriptor.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HColumnDescriptor.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/HColumnDescriptor.java	(working copy)
@@ -317,7 +317,7 @@
    */
   @TOJSON(fieldName = "name", base64=true)
   public byte [] getNameWithColon() {
-    return HStoreKey.addDelimiter(this.name);
+    return Bytes.add(this.name, new byte[]{':'});
   }
 
   /**
@@ -609,7 +609,8 @@
         Text t = new Text();
         t.readFields(in);
         this.name = t.getBytes();
-        if (HStoreKey.getFamilyDelimiterIndex(this.name) > 0) {
+        if(KeyValue.getFamilyDelimiterIndex(this.name, 0, this.name.length) 
+            > 0) {
           this.name = stripColon(this.name);
         }
       } else {
Index: src/java/org/apache/hadoop/hbase/RegionHistorian.java
===================================================================
--- src/java/org/apache/hadoop/hbase/RegionHistorian.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/RegionHistorian.java	(working copy)
@@ -30,8 +30,8 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -230,11 +230,11 @@
       return;
     }
     if (!info.isMetaRegion()) {
-      BatchUpdate batch = new BatchUpdate(info.getRegionName());
-      batch.setTimestamp(timestamp);
-      batch.put(column, Bytes.toBytes(text));
+      Put put = new Put(info.getRegionName());
+      put.setTimeStamp(timestamp);
+      put.add(column, Bytes.toBytes(text));
       try {
-        this.metaTable.commit(batch);
+        this.metaTable.put(put);
       } catch (IOException ioe) {
         LOG.warn("Unable to '" + text + "'", ioe);
       }
Index: src/java/org/apache/hadoop/hbase/HMerge.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HMerge.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/HMerge.java	(working copy)
@@ -35,7 +35,10 @@
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
@@ -202,7 +205,7 @@
       super(conf, fs, tableName);
       this.tableName = tableName;
       this.table = new HTable(conf, META_TABLE_NAME);
-      this.metaScanner = table.getScanner(COL_REGIONINFO_ARRAY, tableName);
+      this.metaScanner = table.getScanner(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
       this.latestRegion = null;
     }
     
@@ -245,13 +248,13 @@
      * @throws IOException
      */
     private RowResult getMetaRow() throws IOException {
-      RowResult currentRow = metaScanner.next();
+      RowResult currentRow = metaScanner.next().rowResult();
       boolean foundResult = false;
       while (currentRow != null) {
         LOG.info("Row: <" + Bytes.toString(currentRow.getRow()) + ">");
         Cell regionInfo = currentRow.get(COL_REGIONINFO);
         if (regionInfo == null || regionInfo.getValue().length == 0) {
-          currentRow = metaScanner.next();
+          currentRow = metaScanner.next().rowResult();
           continue;
         }
         foundResult = true;
@@ -286,17 +289,18 @@
         if(Bytes.equals(regionsToDelete[r], latestRegion.getRegionName())) {
           latestRegion = null;
         }
-        table.deleteAll(regionsToDelete[r]);
+        Delete delete = new Delete(regionsToDelete[r]);
+        table.delete(delete);
         if(LOG.isDebugEnabled()) {
           LOG.debug("updated columns in row: " + Bytes.toString(regionsToDelete[r]));
         }
       }
       newRegion.getRegionInfo().setOffline(true);
 
-      BatchUpdate update = new BatchUpdate(newRegion.getRegionName());
-      update.put(COL_REGIONINFO,
+      Put put = new Put(newRegion.getRegionName());
+      put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER,
         Writables.getBytes(newRegion.getRegionInfo()));
-      table.commit(update);
+      table.put(put);
 
       if(LOG.isDebugEnabled()) {
         LOG.debug("updated columns in row: "
@@ -325,9 +329,10 @@
           HRegionInfo.ROOT_REGIONINFO, null);
       root.initialize(null, null);
 
+      Scan scan = new Scan();
+      scan.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
       InternalScanner rootScanner = 
-        root.getScanner(COL_REGIONINFO_ARRAY, HConstants.EMPTY_START_ROW, 
-        HConstants.LATEST_TIMESTAMP, null);
+        root.getScanner(HConstants.EMPTY_START_ROW, scan);
       
       try {
         List<KeyValue> results = new ArrayList<KeyValue>();
@@ -366,23 +371,29 @@
     throws IOException {
       byte[][] regionsToDelete = {oldRegion1, oldRegion2};
       for(int r = 0; r < regionsToDelete.length; r++) {
-        BatchUpdate b = new BatchUpdate(regionsToDelete[r]);
-        b.delete(COL_REGIONINFO);
-        b.delete(COL_SERVER);
-        b.delete(COL_STARTCODE);
-        b.delete(COL_SPLITA);
-        b.delete(COL_SPLITB);
-        root.batchUpdate(b,null);
-
+        Delete delete = new Delete(regionsToDelete[r]);
+        delete.deleteColumn(HConstants.CATALOG_FAMILY,
+            HConstants.REGIONINFO_QUALIFIER);
+        delete.deleteColumn(HConstants.CATALOG_FAMILY,
+            HConstants.SERVER_QUALIFIER);
+        delete.deleteColumn(HConstants.CATALOG_FAMILY,
+            HConstants.STARTCODE_QUALIFIER);
+        delete.deleteColumn(HConstants.CATALOG_FAMILY,
+            HConstants.SPLITA_QUALIFIER);
+        delete.deleteColumn(HConstants.CATALOG_FAMILY,
+            HConstants.SPLITB_QUALIFIER);
+        root.delete(delete, null, true);
+        
         if(LOG.isDebugEnabled()) {
           LOG.debug("updated columns in row: " + Bytes.toString(regionsToDelete[r]));
         }
       }
       HRegionInfo newInfo = newRegion.getRegionInfo();
       newInfo.setOffline(true);
-      BatchUpdate b = new BatchUpdate(newRegion.getRegionName());
-      b.put(COL_REGIONINFO, Writables.getBytes(newInfo));
-      root.batchUpdate(b,null);
+      Put put = new Put(newRegion.getRegionName());
+      put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
+          Writables.getBytes(newInfo));
+      root.put(put);
       if(LOG.isDebugEnabled()) {
         LOG.debug("updated columns in row: " + Bytes.toString(newRegion.getRegionName()));
       }
Index: src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java	(working copy)
@@ -33,13 +33,18 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.thrift.generated.AlreadyExists;
 import org.apache.hadoop.hbase.thrift.generated.BatchMutation;
 import org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;
@@ -305,7 +310,15 @@
         long timestamp) throws IOError {
       try {
         HTable table = getTable(tableName);
-        table.deleteAll(row, column, timestamp);
+        Delete delete  = new Delete(row, timestamp, null);
+        byte [][] famAndQf = KeyValue.parseColumn(column);
+        if(famAndQf[1].length == 0){
+          delete.deleteFamily(famAndQf[0]);
+        } else {
+          delete.deleteColumn(famAndQf[0], famAndQf[1]);
+        }
+        table.delete(delete);
+        
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
@@ -319,7 +332,8 @@
         throws IOError {
       try {
         HTable table = getTable(tableName);
-        table.deleteAll(row, timestamp);
+        Delete delete  = new Delete(row, timestamp, null);
+        table.delete(delete);
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
@@ -369,15 +383,12 @@
       HTable table = null;
       try {
         table = getTable(tableName);
-        BatchUpdate batchUpdate = new BatchUpdate(row, timestamp);
+        Put put = new Put(row);
+        put.setTimeStamp(timestamp);
         for (Mutation m : mutations) {
-          if (m.isDelete) {
-            batchUpdate.delete(m.column);
-          } else {
-            batchUpdate.put(m.column, m.value);
-          }
+          put.add(m.column, m.value);
         }
-        table.commit(batchUpdate);
+        table.put(put);
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       } catch (IllegalArgumentException e) {
@@ -392,26 +403,23 @@
  
     public void mutateRowsTs(byte[] tableName, List<BatchMutation> rowBatches, long timestamp)
         throws IOError, IllegalArgument, TException {
-      List<BatchUpdate> batchUpdates = new ArrayList<BatchUpdate>();
+      List<Put> puts = new ArrayList<Put>();
        
       for (BatchMutation batch : rowBatches) {
         byte[] row = batch.row;
         List<Mutation> mutations = batch.mutations;
-        BatchUpdate batchUpdate = new BatchUpdate(row, timestamp);
+        Put put = new Put(row);
+        put.setTimeStamp(timestamp);
         for (Mutation m : mutations) {
-          if (m.isDelete) {
-            batchUpdate.delete(m.column);
-          } else {
-            batchUpdate.put(m.column, m.value);
-          }
+          put.add(m.column, m.value);
         }
-        batchUpdates.add(batchUpdate);
+        puts.add(put);
       }
 
       HTable table = null;
       try {
         table = getTable(tableName);
-        table.commit(batchUpdates);
+        table.put(puts);
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       } catch (IllegalArgumentException e) {
@@ -446,7 +454,7 @@
             throw new IllegalArgument("scanner ID is invalid");
         }
 
-        RowResult [] results = null;
+        Result [] results = null;
         try {
             results = scanner.next(nbRows);
             if (null == results) {
@@ -470,7 +478,9 @@
         } else {
           columnsArray = columns.toArray(new byte[0][]);
         }
-        return addScanner(table.getScanner(columnsArray, startRow));
+        Scan scan = new Scan(startRow);
+        scan.addColumns(columnsArray);
+        return addScanner(table.getScanner(scan));
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
@@ -486,7 +496,9 @@
         } else {
           columnsArray = columns.toArray(new byte[0][]);
         }
-        return addScanner(table.getScanner(columnsArray, startRow, stopRow));
+        Scan scan = new Scan(startRow, stopRow);
+        scan.addColumns(columnsArray);
+        return addScanner(table.getScanner(scan));
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
@@ -502,7 +514,10 @@
         } else {
           columnsArray = columns.toArray(new byte[0][]);
         }
-        return addScanner(table.getScanner(columnsArray, startRow, timestamp));
+        Scan scan = new Scan(startRow);
+        scan.addColumns(columnsArray);
+        scan.setTimeRange(timestamp, 0);
+        return addScanner(table.getScanner(scan));
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
@@ -519,8 +534,10 @@
         } else {
           columnsArray = columns.toArray(new byte[0][]);
         }
-        return addScanner(table.getScanner(columnsArray, startRow, stopRow,
-            timestamp));
+        Scan scan = new Scan(startRow, stopRow);
+        scan.addColumns(columnsArray);
+        scan.setTimeRange(timestamp, 0);
+        return addScanner(table.getScanner(scan));
       } catch (IOException e) {
         throw new IOError(e.getMessage());
       }
Index: src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
===================================================================
--- src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java	(working copy)
@@ -25,6 +25,7 @@
 
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor;
@@ -150,5 +151,38 @@
     return rowResultFromHBase(result);
   }
 
+  /**
+   * This utility method creates a list of Thrift TRowResult "struct" based on
+   * an Hbase RowResult object. The empty list is returned if the input is
+   * null.
+   * 
+   * @param in
+   *          Hbase RowResult object
+   * @return Thrift TRowResult array
+   */
+  static public List<TRowResult> rowResultFromHBase(Result[] in) {
+    List<TRowResult> results = new ArrayList<TRowResult>();
+    for ( Result result_ : in) {
+        if(null == result_) {
+            continue;
+        }
+        RowResult rowResult_ = result_.rowResult();
+        TRowResult result = new TRowResult();
+        result.row = rowResult_.getRow();
+        result.columns = new TreeMap<byte[], TCell>(Bytes.BYTES_COMPARATOR);
+        for (Map.Entry<byte[], Cell> entry : rowResult_.entrySet()){
+            Cell cell = entry.getValue();
+            result.columns.put(entry.getKey(),
+                new TCell(cell.getValue(), cell.getTimestamp()));
+
+        }
+        results.add(result);
+    }
+    return results;
+  }
+  static public List<TRowResult> rowResultFromHBase(Result in) {
+    Result [] result = { in };
+    return rowResultFromHBase(result);
+  }
 }
 
Index: src/java/org/apache/hadoop/hbase/HConstants.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HConstants.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/HConstants.java	(working copy)
@@ -157,6 +157,11 @@
   // be the first to be reassigned if the server(s) they are being served by
   // should go down.
 
+
+  //
+  // New stuff.  Making a slow transition.
+  //
+  
   /** The root table's name.*/
   static final byte [] ROOT_TABLE_NAME = Bytes.toBytes("-ROOT-");
 
@@ -166,8 +171,38 @@
   /** delimiter used between portions of a region name */
   public static final int META_ROW_DELIMITER = ',';
 
-  // Defines for the column names used in both ROOT and META HBase 'meta' tables.
+  /** The catalog family */
+  static final byte [] CATALOG_FAMILY = Bytes.toBytes("info");
   
+  /** Array of catalog family */
+  static final byte [][] CATALOG_FAMILY_ARRAY = new byte [][] {CATALOG_FAMILY};
+  
+  /** The catalog historian family */
+  static final byte [] CATALOG_HISTORIAN_FAMILY = Bytes.toBytes("historian");
+  
+  /** The regioninfo column qualifier */
+  static final byte [] REGIONINFO_QUALIFIER = Bytes.toBytes("regioninfo");
+    
+  /** The server column qualifier */
+  static final byte [] SERVER_QUALIFIER = Bytes.toBytes("server");
+  
+  /** The startcode column qualifier */
+  static final byte [] STARTCODE_QUALIFIER = Bytes.toBytes("serverstartcode");
+  
+  /** The lower-half split region column qualifier */
+  static final byte [] SPLITA_QUALIFIER = Bytes.toBytes("splitA");
+  
+  /** The upper-half split region column qualifier */
+  static final byte [] SPLITB_QUALIFIER = Bytes.toBytes("splitB");
+  
+  /** All catalog column qualifiers */
+  static final byte [][] ALL_CATALOG_QUALIFIERS = {REGIONINFO_QUALIFIER, 
+    SERVER_QUALIFIER, STARTCODE_QUALIFIER, SPLITA_QUALIFIER, SPLITB_QUALIFIER};
+  
+  //
+  // Old style, still making the transition
+  //
+  
   /** The ROOT and META column family (string) */
   static final String COLUMN_FAMILY_STR = "info:";
   
@@ -207,6 +242,7 @@
    */
   static final byte[][] ALL_META_COLUMNS = {COL_REGIONINFO, COL_SERVER,
     COL_STARTCODE, COL_SPLITA, COL_SPLITB};
+ 
 
   // Other constants
 
@@ -247,6 +283,11 @@
   static final long LATEST_TIMESTAMP = Long.MAX_VALUE;
 
   /**
+   * LATEST_TIMESTAMP in bytes form
+   */
+  static final byte [] LONG_MAX_BYTES = Bytes.toBytes(LATEST_TIMESTAMP);
+  
+  /**
    * Define for 'return-all-versions'.
    */
   static final int ALL_VERSIONS = Integer.MAX_VALUE;
@@ -256,6 +297,9 @@
    */
   static final int FOREVER = -1;
   
+  /**
+   * Seconds in a week
+   */
   public static final int WEEK_IN_SECONDS = 7 * 24 * 3600;
 
   //TODO: HBASE_CLIENT_RETRIES_NUMBER_KEY is only used by TestMigrate. Move it
Index: src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/KeyValueScanner.java	(revision 0)
@@ -0,0 +1,51 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.KeyValue;
+
+/**
+ * Scanner that returns the next KeyValue.
+ */
+public interface KeyValueScanner {
+  /**
+   * Look at the next KeyValue in this scanner, but do not iterate scanner.
+   * @return the next KeyValue
+   */
+  public KeyValue peek();
+  
+  /**
+   * Return the next KeyValue in this scanner, iterating the scanner 
+   * @return the next KeyValue
+   */
+  public KeyValue next();
+  
+  /**
+   * Seek the scanner at or after the specified KeyValue.
+   * @param key
+   * @return true if scanner has values left, false if end of scanner
+   */
+  public boolean seek(KeyValue key);
+  
+  /**
+   * Close the KeyValue scanner.
+   */
+  public void close();
+}
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ColumnCount.java	(revision 0)
@@ -0,0 +1,112 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+/**
+ * Simple wrapper for a byte buffer and a counter.  Does not copy.
+ * <p>
+ * NOT thread-safe because it is not used in a multi-threaded context, yet.
+ */
+public class ColumnCount {
+  private byte [] bytes;
+  private int offset;
+  private int length;
+  private int count;
+  
+  /**
+   * Constructor
+   * @param column the qualifier to count the versions for
+   */
+  public ColumnCount(byte [] column) {
+    this(column, 0);
+  }
+  
+  /**
+   * Constructor
+   * @param column the qualifier to count the versions for
+   * @param count initial count
+   */
+  public ColumnCount(byte [] column, int count) {
+    this(column, 0, column.length, count);
+  }
+  
+  /**
+   * Constuctor
+   * @param column the qualifier to count the versions for
+   * @param offset in the passed buffer where to start the qualifier from
+   * @param length of the qualifier
+   * @param count initial count
+   */
+  public ColumnCount(byte [] column, int offset, int length, int count) {
+    this.bytes = column;
+    this.offset = offset;
+    this.length = length;
+    this.count = count;
+  }
+  
+  /**
+   * @return the buffer
+   */
+  public byte [] getBuffer(){
+    return this.bytes;
+  }
+  
+  /**
+   * @return the offset
+   */
+  public int getOffset(){
+    return this.offset;
+  }
+  
+  /**
+   * @return the length
+   */
+  public int getLength(){
+    return this.length;
+  }  
+  
+  /**
+   * Decrement the current version count
+   * @return current count
+   */
+  public int decrement() {
+    return --count;
+  }
+
+  /**
+   * Increment the current version count
+   * @return current count
+   */
+  public int increment() {
+    return ++count;
+  }
+  
+  /**
+   * Check to see if needed to fetch more versions
+   * @param max
+   * @return true if more versions are needed, false otherwise
+   */
+  public boolean needMore(int max) {
+    if(this.count < max) {
+      return true;
+    }
+    return false;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegion.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegion.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegion.java	(working copy)
@@ -1,346 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.tableindexed;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableMap;
-import java.util.NavigableSet;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.TreeSet;
-import java.util.Map.Entry;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
-import org.apache.hadoop.hbase.client.tableindexed.IndexedTable;
-import org.apache.hadoop.hbase.io.BatchOperation;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.regionserver.FlushRequester;
-import org.apache.hadoop.hbase.regionserver.HLog;
-import org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion;
-import org.apache.hadoop.hbase.util.Bytes;
-
-class IndexedRegion extends TransactionalRegion {
-
-  private static final Log LOG = LogFactory.getLog(IndexedRegion.class);
-
-  private final HBaseConfiguration conf;
-  private Map<IndexSpecification, HTable> indexSpecToTable = new HashMap<IndexSpecification, HTable>();
-
-  public IndexedRegion(final Path basedir, final HLog log, final FileSystem fs,
-      final HBaseConfiguration conf, final HRegionInfo regionInfo,
-      final FlushRequester flushListener) {
-    super(basedir, log, fs, conf, regionInfo, flushListener);
-    this.conf = conf;
-  }
-
-  private synchronized HTable getIndexTable(IndexSpecification index)
-      throws IOException {
-    HTable indexTable = indexSpecToTable.get(index);
-    if (indexTable == null) {
-      indexTable = new HTable(conf, index.getIndexedTableName(super
-          .getRegionInfo().getTableDesc().getName()));
-      indexSpecToTable.put(index, indexTable);
-    }
-    return indexTable;
-  }
-
-  private Collection<IndexSpecification> getIndexes() {
-    return super.getRegionInfo().getTableDesc().getIndexes();
-  }
-
-  /**
-   * @param batchUpdate
-   * @param lockid
-   * @param writeToWAL if true, then we write this update to the log
-   * @throws IOException
-   */
-  @Override
-  public void batchUpdate(BatchUpdate batchUpdate, Integer lockid, boolean writeToWAL)
-      throws IOException {
-    updateIndexes(batchUpdate); // Do this first because will want to see the old row
-    super.batchUpdate(batchUpdate, lockid, writeToWAL);
-  }
-
-  private void updateIndexes(BatchUpdate batchUpdate) throws IOException {
-    List<IndexSpecification> indexesToUpdate = new LinkedList<IndexSpecification>();
-
-    // Find the indexes we need to update
-    for (IndexSpecification index : getIndexes()) {
-      if (possiblyAppliesToIndex(index, batchUpdate)) {
-        indexesToUpdate.add(index);
-      }
-    }
-
-    if (indexesToUpdate.size() == 0) {
-      return;
-    }
-
-    NavigableSet<byte[]> neededColumns = getColumnsForIndexes(indexesToUpdate);
-
-    NavigableMap<byte[], byte[]> newColumnValues =
-      getColumnsFromBatchUpdate(batchUpdate);
-    Map<byte[], Cell> oldColumnCells = super.getFull(batchUpdate.getRow(),
-        neededColumns, HConstants.LATEST_TIMESTAMP, 1, null);
-    
-    // Handle delete batch updates. Go back and get the next older values
-    for (BatchOperation op : batchUpdate) {
-      if (!op.isPut()) {
-        Cell current = oldColumnCells.get(op.getColumn());
-        if (current != null) {
-          // TODO: Fix this profligacy!!! St.Ack
-          Cell [] older = Cell.createSingleCellArray(super.get(batchUpdate.getRow(),
-              op.getColumn(), current.getTimestamp(), 1));
-          if (older != null && older.length > 0) {
-            newColumnValues.put(op.getColumn(), older[0].getValue());
-          }
-        }
-      }
-    }
-    
-    // Add the old values to the new if they are not there
-    for (Entry<byte[], Cell> oldEntry : oldColumnCells.entrySet()) {
-      if (!newColumnValues.containsKey(oldEntry.getKey())) {
-        newColumnValues.put(oldEntry.getKey(), oldEntry.getValue().getValue());
-      }
-    }
-    
-  
-    
-    Iterator<IndexSpecification> indexIterator = indexesToUpdate.iterator();
-    while (indexIterator.hasNext()) {
-      IndexSpecification indexSpec = indexIterator.next();
-      if (!doesApplyToIndex(indexSpec, newColumnValues)) {
-        indexIterator.remove();
-      }
-    }
-
-    SortedMap<byte[], byte[]> oldColumnValues = convertToValueMap(oldColumnCells);
-    
-    for (IndexSpecification indexSpec : indexesToUpdate) {
-      removeOldIndexEntry(indexSpec, batchUpdate.getRow(), oldColumnValues);
-      updateIndex(indexSpec, batchUpdate.getRow(), newColumnValues);
-    }
-  }
-
-  /** Return the columns needed for the update. */
-  private NavigableSet<byte[]> getColumnsForIndexes(Collection<IndexSpecification> indexes) {
-    NavigableSet<byte[]> neededColumns = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-    for (IndexSpecification indexSpec : indexes) {
-      for (byte[] col : indexSpec.getAllColumns()) {
-        neededColumns.add(col);
-      }
-    }
-    return neededColumns;
-  }
-
-  private void removeOldIndexEntry(IndexSpecification indexSpec, byte[] row,
-      SortedMap<byte[], byte[]> oldColumnValues) throws IOException {
-    for (byte[] indexedCol : indexSpec.getIndexedColumns()) {
-      if (!oldColumnValues.containsKey(indexedCol)) {
-        LOG.debug("Index [" + indexSpec.getIndexId()
-            + "] not trying to remove old entry for row ["
-            + Bytes.toString(row) + "] because col ["
-            + Bytes.toString(indexedCol) + "] is missing");
-        return;
-      }
-    }
-
-    byte[] oldIndexRow = indexSpec.getKeyGenerator().createIndexKey(row,
-        oldColumnValues);
-    LOG.debug("Index [" + indexSpec.getIndexId() + "] removing old entry ["
-        + Bytes.toString(oldIndexRow) + "]");
-    getIndexTable(indexSpec).deleteAll(oldIndexRow);
-  }
-  
-  private NavigableMap<byte[], byte[]> getColumnsFromBatchUpdate(BatchUpdate b) {
-    NavigableMap<byte[], byte[]> columnValues = new TreeMap<byte[], byte[]>(
-        Bytes.BYTES_COMPARATOR);
-    for (BatchOperation op : b) {
-      if (op.isPut()) {
-        columnValues.put(op.getColumn(), op.getValue());
-      }
-    }
-    return columnValues;
-  }
-
-  /** Ask if this update *could* apply to the index. It may actually apply if some of the columns needed are missing.
-   * 
-   * @param indexSpec
-   * @param b
-   * @return true if possibly apply.
-   */
-  private boolean possiblyAppliesToIndex(IndexSpecification indexSpec, BatchUpdate b) {
-    for (BatchOperation op : b) {
-      if (indexSpec.containsColumn(op.getColumn())) {
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  /** Ask if this update does apply to the index. 
-   * 
-   * @param indexSpec
-   * @param b
-   * @return true if possibly apply.
-   */
-  private boolean doesApplyToIndex(IndexSpecification indexSpec,  SortedMap<byte[], byte[]> columnValues) {
-    
-    for (byte [] neededCol : indexSpec.getIndexedColumns()) {
-      if (! columnValues.containsKey(neededCol))  {
-        LOG.debug("Index [" + indexSpec.getIndexId() + "] can't be updated because ["
-            + Bytes.toString(neededCol) + "] is missing");
-        return false;
-      }
-    }
-    return true;
-  }
-
-  private void updateIndex(IndexSpecification indexSpec, byte[] row,
-      SortedMap<byte[], byte[]> columnValues) throws IOException {
-    BatchUpdate indexUpdate = createIndexUpdate(indexSpec, row, columnValues);
-    getIndexTable(indexSpec).commit(indexUpdate);
-    LOG.debug("Index [" + indexSpec.getIndexId() + "] adding new entry ["
-        + Bytes.toString(indexUpdate.getRow()) + "] for row ["
-        + Bytes.toString(row) + "]");
-
-  }
-
-  private BatchUpdate createIndexUpdate(IndexSpecification indexSpec,
-      byte[] row, SortedMap<byte[], byte[]> columnValues) {
-    byte[] indexRow = indexSpec.getKeyGenerator().createIndexKey(row,
-        columnValues);
-    BatchUpdate update = new BatchUpdate(indexRow);
-
-    update.put(IndexedTable.INDEX_BASE_ROW_COLUMN, row);
-
-    for (byte[] col : indexSpec.getIndexedColumns()) {
-      byte[] val = columnValues.get(col);
-      if (val == null) {
-        throw new RuntimeException("Unexpected missing column value. ["+Bytes.toString(col)+"]");
-      }
-      update.put(col, val);
-    }
-    
-    for (byte [] col : indexSpec.getAdditionalColumns()) {
-      byte[] val = columnValues.get(col);
-      if (val != null) {
-        update.put(col, val);
-      }
-    }
-
-    return update;
-  }
-
-  @Override
-  public void deleteAll(final byte[] row, final long ts, final Integer lockid)
-      throws IOException {
-
-    if (getIndexes().size() != 0) {
-
-      // Need all columns
-      NavigableSet<byte[]> neededColumns = getColumnsForIndexes(getIndexes());
-
-      Map<byte[], Cell> oldColumnCells = super.getFull(row,
-          neededColumns, HConstants.LATEST_TIMESTAMP, 1, null);
-      SortedMap<byte[], byte[]> oldColumnValues = convertToValueMap(oldColumnCells);
-      
-      
-      for (IndexSpecification indexSpec : getIndexes()) {
-        removeOldIndexEntry(indexSpec, row, oldColumnValues);
-      }
-
-      // Handle if there is still a version visible.
-      if (ts != HConstants.LATEST_TIMESTAMP) {
-        Map<byte[], Cell> currentColumnCells = super.getFull(row,
-            neededColumns, ts, 1, null);
-        SortedMap<byte[], byte[]> currentColumnValues = convertToValueMap(currentColumnCells);
-        
-        for (IndexSpecification indexSpec : getIndexes()) {
-          if (doesApplyToIndex(indexSpec, currentColumnValues)) {
-            updateIndex(indexSpec, row, currentColumnValues);
-          }
-        }
-      }
-    }
-    super.deleteAll(row, ts, lockid);
-  }
-
-  private SortedMap<byte[], byte[]> convertToValueMap(
-      Map<byte[], Cell> cellMap) {
-    SortedMap<byte[], byte[]> currentColumnValues = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
-    for(Entry<byte[], Cell> entry : cellMap.entrySet()) {
-      currentColumnValues.put(entry.getKey(), entry.getValue().getValue());
-    }
-    return currentColumnValues;
-  }
-
-  @Override
-  public void deleteAll(final byte[] row, byte[] column, final long ts,
-      final Integer lockid) throws IOException {
-    List<IndexSpecification> indexesToUpdate = new LinkedList<IndexSpecification>();
-    
-    for(IndexSpecification indexSpec : getIndexes()) {
-      if (indexSpec.containsColumn(column)) {
-        indexesToUpdate.add(indexSpec);
-      }
-    }
-    
-    NavigableSet<byte[]> neededColumns = getColumnsForIndexes(indexesToUpdate);
-    Map<byte[], Cell> oldColumnCells = super.getFull(row,
-        neededColumns, HConstants.LATEST_TIMESTAMP, 1, null);
-    SortedMap<byte [], byte[]> oldColumnValues = convertToValueMap(oldColumnCells);
-    
-    for (IndexSpecification indexSpec : indexesToUpdate) {
-      removeOldIndexEntry(indexSpec, row, oldColumnValues);
-    }    
-    
-    // Handle if there is still a version visible.
-    if (ts != HConstants.LATEST_TIMESTAMP) {
-      Map<byte[], Cell> currentColumnCells = super.getFull(row,
-          neededColumns, ts, 1, null);
-      SortedMap<byte[], byte[]> currentColumnValues = convertToValueMap(currentColumnCells);
-      
-      for (IndexSpecification indexSpec : getIndexes()) {
-        if (doesApplyToIndex(indexSpec, currentColumnValues)) {
-          updateIndex(indexSpec, row, currentColumnValues);
-        }
-      }
-    }
-    
-    super.deleteAll(row, column, ts, lockid);
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/tableindexed/IndexedRegionServer.java	(working copy)
@@ -1,74 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.tableindexed;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.ipc.IndexedRegionInterface;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
-
-/**
- * RegionServer which maintains secondary indexes.
- * 
- **/
-public class IndexedRegionServer extends TransactionalRegionServer implements
-    IndexedRegionInterface {
-
-  public IndexedRegionServer(HBaseConfiguration conf) throws IOException {
-    this(new HServerAddress(conf.get(REGIONSERVER_ADDRESS,
-        DEFAULT_REGIONSERVER_ADDRESS)), conf);
-  }
-
-  public IndexedRegionServer(HServerAddress serverAddress,
-      HBaseConfiguration conf) throws IOException {
-    super(serverAddress, conf);
-  }
-
-  @Override
-  public long getProtocolVersion(final String protocol, final long clientVersion)
-      throws IOException {
-    if (protocol.equals(IndexedRegionInterface.class.getName())) {
-      return HBaseRPCProtocolVersion.versionID;
-    }
-    return super.getProtocolVersion(protocol, clientVersion);
-  }
-
-  @Override
-  protected HRegion instantiateRegion(final HRegionInfo regionInfo)
-      throws IOException {
-    HRegion r = new IndexedRegion(HTableDescriptor.getTableDir(super
-        .getRootDir(), regionInfo.getTableDesc().getName()), super.log, super
-        .getFileSystem(), super.conf, regionInfo, super.getFlushRequester());
-    r.initialize(null, new Progressable() {
-      public void progress() {
-        addProcessingMessage(regionInfo);
-      }
-    });
-    return r;
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java	(revision 0)
@@ -0,0 +1,48 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+
+/**
+ * Implementing classes of this interface will be used for the tracking
+ * and enforcement of columns and numbers of versions during the course of a 
+ * Get or Scan operation.
+ * <p>
+ * Currently there are two different types of Store/Family-level queries.
+ * <ul><li>{@link ExplicitColumnTracker} is used when the query specifies
+ * one or more column qualifiers to return in the family.
+ * <li>{@link WildcardColumnTracker} is used when the query asks for all
+ * qualifiers within the family.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public interface ColumnTracker {
+  public MatchCode checkColumn(byte [] bytes, int offset, int length);
+  public void update();
+  public void reset();
+  public boolean done();
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionState.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionState.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionState.java	(working copy)
@@ -1,362 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedSet;
-import java.util.TreeMap;
-import java.util.TreeSet;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.filter.RowFilterSet;
-import org.apache.hadoop.hbase.filter.StopRowFilter;
-import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
-import org.apache.hadoop.hbase.io.BatchOperation;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Holds the state of a transaction.
- */
-class TransactionState {
-
-  private static final Log LOG = LogFactory.getLog(TransactionState.class);
-
-  /** Current status. */
-  public enum Status {
-    /** Initial status, still performing operations. */
-    PENDING,
-    /**
-     * Checked if we can commit, and said yes. Still need to determine the
-     * global decision.
-     */
-    COMMIT_PENDING,
-    /** Committed. */
-    COMMITED,
-    /** Aborted. */
-    ABORTED
-  }
-
-  /**
-   * Simple container of the range of the scanners we've opened. Used to check
-   * for conflicting writes.
-   */
-  private static class ScanRange {
-    protected byte[] startRow;
-    protected byte[] endRow;
-
-    public ScanRange(byte[] startRow, byte[] endRow) {
-      this.startRow = startRow;
-      this.endRow = endRow;
-    }
-
-    /**
-     * Check if this scan range contains the given key.
-     * 
-     * @param rowKey
-     * @return boolean
-     */
-    public boolean contains(byte[] rowKey) {
-      if (startRow != null && Bytes.compareTo(rowKey, startRow) < 0) {
-        return false;
-      }
-      if (endRow != null && Bytes.compareTo(endRow, rowKey) < 0) {
-        return false;
-      }
-      return true;
-    }
-  }
-
-  private final HRegionInfo regionInfo;
-  private final long hLogStartSequenceId;
-  private final long transactionId;
-  private Status status;
-  private SortedSet<byte[]> readSet = new TreeSet<byte[]>(
-      Bytes.BYTES_COMPARATOR);
-  private List<BatchUpdate> writeSet = new LinkedList<BatchUpdate>();
-  private List<ScanRange> scanSet = new LinkedList<ScanRange>();
-  private Set<TransactionState> transactionsToCheck = new HashSet<TransactionState>();
-  private int startSequenceNumber;
-  private Integer sequenceNumber;
-
-  TransactionState(final long transactionId, final long rLogStartSequenceId,
-      HRegionInfo regionInfo) {
-    this.transactionId = transactionId;
-    this.hLogStartSequenceId = rLogStartSequenceId;
-    this.regionInfo = regionInfo;
-    this.status = Status.PENDING;
-  }
-
-  void addRead(final byte[] rowKey) {
-    readSet.add(rowKey);
-  }
-
-  Set<byte[]> getReadSet() {
-    return readSet;
-  }
-
-  void addWrite(final BatchUpdate write) {
-    writeSet.add(write);
-  }
-
-  List<BatchUpdate> getWriteSet() {
-    return writeSet;
-  }
-
-  /**
-   * GetFull from the writeSet.
-   * 
-   * @param row
-   * @param columns
-   * @param timestamp
-   * @return
-   */
-  Map<byte[], Cell> localGetFull(final byte[] row, final Set<byte[]> columns,
-      final long timestamp) {
-    Map<byte[], Cell> results = new TreeMap<byte[], Cell>(
-        Bytes.BYTES_COMPARATOR); // Must use the Bytes Conparator because
-    for (BatchUpdate b : writeSet) {
-      if (!Bytes.equals(row, b.getRow())) {
-        continue;
-      }
-      if (b.getTimestamp() > timestamp) {
-        continue;
-      }
-      for (BatchOperation op : b) {
-        if (!op.isPut()
-            || (columns != null && !columns.contains(op.getColumn()))) {
-          continue;
-        }
-        results.put(op.getColumn(), new Cell(op.getValue(), b.getTimestamp()));
-      }
-    }
-    return results.size() == 0 ? null : results;
-  }
-
-  /**
-   * Get from the writeSet.
-   * 
-   * @param row
-   * @param column
-   * @param timestamp
-   * @return
-   */
-  Cell[] localGet(final byte[] row, final byte[] column, final long timestamp) {
-    ArrayList<Cell> results = new ArrayList<Cell>();
-
-    // Go in reverse order to put newest updates first in list
-    for (int i = writeSet.size() - 1; i >= 0; i--) {
-      BatchUpdate b = writeSet.get(i);
-
-      if (!Bytes.equals(row, b.getRow())) {
-        continue;
-      }
-      if (b.getTimestamp() > timestamp) {
-        continue;
-      }
-      for (BatchOperation op : b) {
-        if (!op.isPut() || !Bytes.equals(column, op.getColumn())) {
-          continue;
-        }
-        results.add(new Cell(op.getValue(), b.getTimestamp()));
-      }
-    }
-    return results.size() == 0 ? null : results
-        .toArray(new Cell[results.size()]);
-  }
-
-  void addTransactionToCheck(final TransactionState transaction) {
-    transactionsToCheck.add(transaction);
-  }
-
-  boolean hasConflict() {
-    for (TransactionState transactionState : transactionsToCheck) {
-      if (hasConflict(transactionState)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  private boolean hasConflict(final TransactionState checkAgainst) {
-    if (checkAgainst.getStatus().equals(TransactionState.Status.ABORTED)) {
-      return false; // Cannot conflict with aborted transactions
-    }
-
-    for (BatchUpdate otherUpdate : checkAgainst.getWriteSet()) {
-      if (this.getReadSet().contains(otherUpdate.getRow())) {
-        LOG.debug("Transaction [" + this.toString()
-            + "] has read which conflicts with [" + checkAgainst.toString()
-            + "]: region [" + regionInfo.getRegionNameAsString() + "], row["
-            + Bytes.toString(otherUpdate.getRow()) + "]");
-        return true;
-      }
-      for (ScanRange scanRange : this.scanSet) {
-        if (scanRange.contains(otherUpdate.getRow())) {
-          LOG.debug("Transaction [" + this.toString()
-              + "] has scan which conflicts with [" + checkAgainst.toString()
-              + "]: region [" + regionInfo.getRegionNameAsString() + "], row["
-              + Bytes.toString(otherUpdate.getRow()) + "]");
-          return true;
-        }
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Get the status.
-   * 
-   * @return Return the status.
-   */
-  Status getStatus() {
-    return status;
-  }
-
-  /**
-   * Set the status.
-   * 
-   * @param status The status to set.
-   */
-  void setStatus(final Status status) {
-    this.status = status;
-  }
-
-  /**
-   * Get the startSequenceNumber.
-   * 
-   * @return Return the startSequenceNumber.
-   */
-  int getStartSequenceNumber() {
-    return startSequenceNumber;
-  }
-
-  /**
-   * Set the startSequenceNumber.
-   * 
-   * @param startSequenceNumber
-   */
-  void setStartSequenceNumber(final int startSequenceNumber) {
-    this.startSequenceNumber = startSequenceNumber;
-  }
-
-  /**
-   * Get the sequenceNumber.
-   * 
-   * @return Return the sequenceNumber.
-   */
-  Integer getSequenceNumber() {
-    return sequenceNumber;
-  }
-
-  /**
-   * Set the sequenceNumber.
-   * 
-   * @param sequenceNumber The sequenceNumber to set.
-   */
-  void setSequenceNumber(final Integer sequenceNumber) {
-    this.sequenceNumber = sequenceNumber;
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder result = new StringBuilder();
-    result.append("[transactionId: ");
-    result.append(transactionId);
-    result.append(" status: ");
-    result.append(status.name());
-    result.append(" read Size: ");
-    result.append(readSet.size());
-    result.append(" scan Size: ");
-    result.append(scanSet.size());
-    result.append(" write Size: ");
-    result.append(writeSet.size());
-    result.append(" startSQ: ");
-    result.append(startSequenceNumber);
-    if (sequenceNumber != null) {
-      result.append(" commitedSQ:");
-      result.append(sequenceNumber);
-    }
-    result.append("]");
-
-    return result.toString();
-  }
-
-  /**
-   * Get the transactionId.
-   * 
-   * @return Return the transactionId.
-   */
-  long getTransactionId() {
-    return transactionId;
-  }
-
-  /**
-   * Get the startSequenceId.
-   * 
-   * @return Return the startSequenceId.
-   */
-  long getHLogStartSequenceId() {
-    return hLogStartSequenceId;
-  }
-
-  void addScan(byte[] firstRow, RowFilterInterface filter) {
-    ScanRange scanRange = new ScanRange(firstRow, getEndRow(filter));
-    LOG.trace(String.format(
-        "Adding scan for transcaction [%s], from [%s] to [%s]", transactionId,
-        scanRange.startRow == null ? "null" : Bytes
-            .toString(scanRange.startRow), scanRange.endRow == null ? "null"
-            : Bytes.toString(scanRange.endRow)));
-    scanSet.add(scanRange);
-  }
-
-  private byte[] getEndRow(RowFilterInterface filter) {
-    if (filter instanceof WhileMatchRowFilter) {
-      WhileMatchRowFilter wmrFilter = (WhileMatchRowFilter) filter;
-      if (wmrFilter.getInternalFilter() instanceof StopRowFilter) {
-        StopRowFilter stopFilter = (StopRowFilter) wmrFilter
-            .getInternalFilter();
-        return stopFilter.getStopRowKey();
-      }
-    } else if (filter instanceof RowFilterSet) {
-      RowFilterSet rowFilterSet = (RowFilterSet) filter;
-      if (rowFilterSet.getOperator()
-          .equals(RowFilterSet.Operator.MUST_PASS_ALL)) {
-        for (RowFilterInterface subFilter : rowFilterSet.getFilters()) {
-          byte[] endRow = getEndRow(subFilter);
-          if (endRow != null) {
-            return endRow;
-          }
-        }
-      }
-    }
-    return null;
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegion.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegion.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegion.java	(working copy)
@@ -1,718 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.Map.Entry;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.LeaseException;
-import org.apache.hadoop.hbase.LeaseListener;
-import org.apache.hadoop.hbase.Leases;
-import org.apache.hadoop.hbase.Leases.LeaseStillHeldException;
-import org.apache.hadoop.hbase.client.transactional.UnknownTransactionException;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.regionserver.FlushRequester;
-import org.apache.hadoop.hbase.regionserver.HLog;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.InternalScanner;
-import org.apache.hadoop.hbase.regionserver.Store;
-import org.apache.hadoop.hbase.regionserver.transactional.TransactionState.Status;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * Regionserver which provides transactional support for atomic transactions.
- * This is achieved with optimistic concurrency control (see
- * http://www.seas.upenn.edu/~zives/cis650/papers/opt-cc.pdf). We keep track
- * read and write sets for each transaction, and hold off on processing the
- * writes. To decide to commit a transaction we check its read sets with all
- * transactions that have committed while it was running for overlaps.
- * <p>
- * Because transactions can span multiple regions, all regions must agree to
- * commit a transactions. The client side of this commit protocol is encoded in
- * org.apache.hadoop.hbase.client.transactional.TransactionManger
- * <p>
- * In the event of an failure of the client mid-commit, (after we voted yes), we
- * will have to consult the transaction log to determine the final decision of
- * the transaction. This is not yet implemented.
- */
-public class TransactionalRegion extends HRegion {
-
-  private static final String LEASE_TIME = "hbase.transaction.leaseTime";
-  private static final int DEFAULT_LEASE_TIME = 60 * 1000;
-  private static final int LEASE_CHECK_FREQUENCY = 1000;
-  
-  private static final String OLD_TRANSACTION_FLUSH = "hbase.transaction.flush";
-  private static final int DEFAULT_OLD_TRANSACTION_FLUSH = 100; // Do a flush if we have this many old transactions..
-  
-
-  static final Log LOG = LogFactory.getLog(TransactionalRegion.class);
-
-  // Collection of active transactions (PENDING) keyed by id.
-  protected Map<String, TransactionState> transactionsById = new HashMap<String, TransactionState>();
-
-  // Map of recent transactions that are COMMIT_PENDING or COMMITED keyed by
-  // their sequence number
-  private SortedMap<Integer, TransactionState> commitedTransactionsBySequenceNumber = Collections
-      .synchronizedSortedMap(new TreeMap<Integer, TransactionState>());
-
-  // Collection of transactions that are COMMIT_PENDING
-  private Set<TransactionState> commitPendingTransactions = Collections
-      .synchronizedSet(new HashSet<TransactionState>());
-
-  private final Leases transactionLeases;
-  private AtomicInteger nextSequenceId = new AtomicInteger(0);
-  private Object commitCheckLock = new Object();
-  private TransactionalHLogManager logManager;
-  private final int oldTransactionFlushTrigger;
-
-  /**
-   * @param basedir
-   * @param log
-   * @param fs
-   * @param conf
-   * @param regionInfo
-   * @param flushListener
-   */
-  public TransactionalRegion(final Path basedir, final HLog log,
-      final FileSystem fs, final HBaseConfiguration conf,
-      final HRegionInfo regionInfo, final FlushRequester flushListener) {
-    super(basedir, log, fs, conf, regionInfo, flushListener);
-    transactionLeases = new Leases(conf.getInt(LEASE_TIME, DEFAULT_LEASE_TIME),
-        LEASE_CHECK_FREQUENCY);
-    logManager = new TransactionalHLogManager(this);
-    oldTransactionFlushTrigger = conf.getInt(OLD_TRANSACTION_FLUSH, DEFAULT_OLD_TRANSACTION_FLUSH);
-  }
-
-  @Override
-  protected void doReconstructionLog(final Path oldLogFile,
-      final long minSeqId, final long maxSeqId, final Progressable reporter)
-      throws UnsupportedEncodingException, IOException {
-    super.doReconstructionLog(oldLogFile, minSeqId, maxSeqId, reporter);
-
-    Map<Long, List<BatchUpdate>> commitedTransactionsById = logManager
-        .getCommitsFromLog(oldLogFile, minSeqId, reporter);
-
-    if (commitedTransactionsById != null && commitedTransactionsById.size() > 0) {
-      LOG.debug("found " + commitedTransactionsById.size()
-          + " COMMITED transactions");
-
-      for (Entry<Long, List<BatchUpdate>> entry : commitedTransactionsById
-          .entrySet()) {
-        LOG.debug("Writing " + entry.getValue().size()
-            + " updates for transaction " + entry.getKey());
-        for (BatchUpdate b : entry.getValue()) {
-          super.batchUpdate(b, true); // These are walled so they live forever
-        }
-      }
-
-      // LOG.debug("Flushing cache"); // We must trigger a cache flush,
-      // otherwise
-      // we will would ignore the log on subsequent failure
-      // if (!super.flushcache()) {
-      // LOG.warn("Did not flush cache");
-      // }
-    }
-  }
-
-  /**
-   * We need to make sure that we don't complete a cache flush between running
-   * transactions. If we did, then we would not find all log messages needed to
-   * restore the transaction, as some of them would be before the last
-   * "complete" flush id.
-   */
-  @Override
-  protected long getCompleteCacheFlushSequenceId(final long currentSequenceId) {
-    long minPendingStartSequenceId = currentSequenceId;
-    for (TransactionState transactionState : transactionsById.values()) {
-      minPendingStartSequenceId = Math.min(minPendingStartSequenceId,
-          transactionState.getHLogStartSequenceId());
-    }
-    return minPendingStartSequenceId;
-  }
-
-  /**
-   * @param transactionId
-   * @throws IOException
-   */
-  public void beginTransaction(final long transactionId) throws IOException {
-    String key = String.valueOf(transactionId);
-    if (transactionsById.get(key) != null) {
-      TransactionState alias = getTransactionState(transactionId);
-      if (alias != null) {
-        alias.setStatus(Status.ABORTED);
-        retireTransaction(alias);
-      }
-      LOG.error("Existing trasaction with id ["+key+"] in region ["+super.getRegionInfo().getRegionNameAsString()+"]");
-      throw new IOException("Already exiting transaction id: " + key);
-    }
-
-    TransactionState state = new TransactionState(transactionId, super.getLog()
-        .getSequenceNumber(), super.getRegionInfo());
-
-    // Order is important here ...
-    List<TransactionState> commitPendingCopy = new LinkedList<TransactionState>(commitPendingTransactions);
-    for (TransactionState commitPending : commitPendingCopy) {
-      state.addTransactionToCheck(commitPending);
-    }
-    state.setStartSequenceNumber(nextSequenceId.get());
-
-    transactionsById.put(String.valueOf(key), state);
-    try {
-      transactionLeases.createLease(key, new TransactionLeaseListener(key));
-    } catch (LeaseStillHeldException e) {
-      LOG.error("Lease still held for ["+key+"] in region ["+super.getRegionInfo().getRegionNameAsString()+"]");      
-      throw new RuntimeException(e);
-    }
-    LOG.debug("Begining transaction " + key + " in region "
-        + super.getRegionInfo().getRegionNameAsString());
-    logManager.writeStartToLog(transactionId);
-    
-    maybeTriggerOldTransactionFlush();
-  }
-
-  /**
-   * Fetch a single data item.
-   * 
-   * @param transactionId
-   * @param row
-   * @param column
-   * @return column value
-   * @throws IOException
-   */
-  public Cell get(final long transactionId, final byte[] row,
-      final byte[] column) throws IOException {
-    Cell[] results = get(transactionId, row, column, 1);
-    return (results == null || results.length == 0) ? null : results[0];
-  }
-
-  /**
-   * Fetch multiple versions of a single data item
-   * 
-   * @param transactionId
-   * @param row
-   * @param column
-   * @param numVersions
-   * @return array of values one element per version
-   * @throws IOException
-   */
-  public Cell[] get(final long transactionId, final byte[] row,
-      final byte[] column, final int numVersions) throws IOException {
-    return get(transactionId, row, column, Long.MAX_VALUE, numVersions);
-  }
-
-  /**
-   * Fetch multiple versions of a single data item, with timestamp.
-   * 
-   * @param transactionId
-   * @param row
-   * @param column
-   * @param timestamp
-   * @param numVersions
-   * @return array of values one element per version that matches the timestamp
-   * @throws IOException
-   */
-  public Cell[] get(final long transactionId, final byte[] row,
-      final byte[] column, final long timestamp, final int numVersions)
-      throws IOException {
-    TransactionState state = getTransactionState(transactionId);
-
-    state.addRead(row);
-
-    Cell[] localCells = state.localGet(row, column, timestamp);
-
-    if (localCells != null && localCells.length > 0) {
-      LOG
-          .trace("Transactional get of something we've written in the same transaction "
-              + transactionId);
-      LOG.trace("row: " + Bytes.toString(row));
-      LOG.trace("col: " + Bytes.toString(column));
-      LOG.trace("numVersions: " + numVersions);
-      for (Cell cell : localCells) {
-        LOG.trace("cell: " + Bytes.toString(cell.getValue()));
-      }
-
-      if (numVersions > 1) {
-        // FIX THIS PROFLIGACY CONVERTING RESULT OF get.
-        Cell[] globalCells = Cell.createSingleCellArray(get(row, column, timestamp, numVersions - 1));
-        Cell[] result = new Cell[globalCells.length + localCells.length];
-        System.arraycopy(localCells, 0, result, 0, localCells.length);
-        System.arraycopy(globalCells, 0, result, localCells.length,
-            globalCells.length);
-        return result;
-      }
-      return localCells;
-    }
-
-    return Cell.createSingleCellArray(get(row, column, timestamp, numVersions));
-  }
-
-  /**
-   * Fetch all the columns for the indicated row at a specified timestamp.
-   * Returns a TreeMap that maps column names to values.
-   * 
-   * @param transactionId
-   * @param row
-   * @param columns Array of columns you'd like to retrieve. When null, get all.
-   * @param ts
-   * @return Map<columnName, Cell> values
-   * @throws IOException
-   */
-  public Map<byte[], Cell> getFull(final long transactionId, final byte[] row,
-      final NavigableSet<byte[]> columns, final long ts) throws IOException {
-    TransactionState state = getTransactionState(transactionId);
-
-    state.addRead(row);
-
-    Map<byte[], Cell> localCells = state.localGetFull(row, columns, ts);
-
-    if (localCells != null && localCells.size() > 0) {
-      LOG
-          .trace("Transactional get of something we've written in the same transaction "
-              + transactionId);
-      LOG.trace("row: " + Bytes.toString(row));
-      for (Entry<byte[], Cell> entry : localCells.entrySet()) {
-        LOG.trace("col: " + Bytes.toString(entry.getKey()));
-        LOG.trace("cell: " + Bytes.toString(entry.getValue().getValue()));
-      }
-
-      Map<byte[], Cell> internalResults = getFull(row, columns, ts, 1, null);
-      internalResults.putAll(localCells);
-      return internalResults;
-    }
-
-    return getFull(row, columns, ts, 1, null);
-  }
-
-  /**
-   * Return an iterator that scans over the HRegion, returning the indicated
-   * columns for only the rows that match the data filter. This Iterator must be
-   * closed by the caller.
-   * 
-   * @param transactionId
-   * @param cols columns to scan. If column name is a column family, all columns
-   * of the specified column family are returned. Its also possible to pass a
-   * regex in the column qualifier. A column qualifier is judged to be a regex
-   * if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param firstRow row which is the starting point of the scan
-   * @param timestamp only return rows whose timestamp is <= this value
-   * @param filter row filter
-   * @return InternalScanner
-   * @throws IOException
-   */
-  public InternalScanner getScanner(final long transactionId,
-      final byte[][] cols, final byte[] firstRow, final long timestamp,
-      final RowFilterInterface filter) throws IOException {
-    TransactionState state = getTransactionState(transactionId);
-    state.addScan(firstRow, filter);
-    return new ScannerWrapper(transactionId, super.getScanner(cols, firstRow,
-        timestamp, filter));
-  }
-
-  /**
-   * Add a write to the transaction. Does not get applied until commit process.
-   * 
-   * @param transactionId
-   * @param b
-   * @throws IOException
-   */
-  public void batchUpdate(final long transactionId, final BatchUpdate b)
-      throws IOException {
-    TransactionState state = getTransactionState(transactionId);
-    state.addWrite(b);
-    logManager.writeUpdateToLog(transactionId, b);
-  }
-
-  /**
-   * Add a delete to the transaction. Does not get applied until commit process.
-   * FIXME, not sure about this approach
-   * 
-   * @param transactionId
-   * @param row
-   * @param timestamp
-   * @throws IOException
-   */
-  public void deleteAll(final long transactionId, final byte[] row,
-      final long timestamp) throws IOException {
-    TransactionState state = getTransactionState(transactionId);
-    long now = System.currentTimeMillis();
-
-    for (Store store : super.stores.values()) {
-      List<KeyValue> keyvalues = new ArrayList<KeyValue>();
-      store.getFull(new KeyValue(row, timestamp),
-        null, null, ALL_VERSIONS, null, keyvalues, now);
-      BatchUpdate deleteUpdate = new BatchUpdate(row, timestamp);
-
-      for (KeyValue key : keyvalues) {
-        deleteUpdate.delete(key.getColumn());
-      }
-      
-      state.addWrite(deleteUpdate);
-      logManager.writeUpdateToLog(transactionId, deleteUpdate);
-
-    }
-
-  }
-
-  /**
-   * @param transactionId
-   * @return true if commit is successful
-   * @throws IOException
-   */
-  public boolean commitRequest(final long transactionId) throws IOException {
-    synchronized (commitCheckLock) {
-      TransactionState state = getTransactionState(transactionId);
-      if (state == null) {
-        return false;
-      }
-
-      if (hasConflict(state)) {
-        state.setStatus(Status.ABORTED);
-        retireTransaction(state);
-        return false;
-      }
-
-      // No conflicts, we can commit.
-      LOG.trace("No conflicts for transaction " + transactionId
-          + " found in region " + super.getRegionInfo().getRegionNameAsString()
-          + ". Voting for commit");
-      state.setStatus(Status.COMMIT_PENDING);
-
-      // If there are writes we must keep record off the transaction
-      if (state.getWriteSet().size() > 0) {
-        // Order is important
-        commitPendingTransactions.add(state);
-        state.setSequenceNumber(nextSequenceId.getAndIncrement());
-        commitedTransactionsBySequenceNumber.put(state.getSequenceNumber(),
-            state);
-      }
-
-      return true;
-    }
-  }
-
-  private boolean hasConflict(final TransactionState state) {
-    // Check transactions that were committed while we were running
-    for (int i = state.getStartSequenceNumber(); i < nextSequenceId.get(); i++) {
-      TransactionState other = commitedTransactionsBySequenceNumber.get(i);
-      if (other == null) {
-        continue;
-      }
-      state.addTransactionToCheck(other);
-    }
-
-    return state.hasConflict();
-  }
-
-  /**
-   * Commit the transaction.
-   * 
-   * @param transactionId
-   * @throws IOException
-   */
-  public void commit(final long transactionId) throws IOException {
-    TransactionState state;
-    try {
-      state = getTransactionState(transactionId);
-    } catch (UnknownTransactionException e) {
-      LOG.fatal("Asked to commit unknown transaction: " + transactionId
-          + " in region " + super.getRegionInfo().getRegionNameAsString());
-      // FIXME Write to the transaction log that this transaction was corrupted
-      throw e;
-    }
-
-    if (!state.getStatus().equals(Status.COMMIT_PENDING)) {
-      LOG.fatal("Asked to commit a non pending transaction");
-      // FIXME Write to the transaction log that this transaction was corrupted
-      throw new IOException("commit failure");
-    }
-
-    commit(state);
-  }
-
-  /**
-   * Commit the transaction.
-   * 
-   * @param transactionId
-   * @throws IOException
-   */
-  public void abort(final long transactionId) throws IOException {
-    TransactionState state;
-    try {
-      state = getTransactionState(transactionId);
-    } catch (UnknownTransactionException e) {
-      LOG.error("Asked to abort unknown transaction: " + transactionId);
-      return;
-    }
-
-    state.setStatus(Status.ABORTED);
-
-    if (state.getWriteSet().size() > 0) {
-      logManager.writeAbortToLog(state.getTransactionId());
-    }
-
-    // Following removes needed if we have voted
-    if (state.getSequenceNumber() != null) {
-      commitedTransactionsBySequenceNumber.remove(state.getSequenceNumber());
-    }
-    commitPendingTransactions.remove(state);
-
-    retireTransaction(state);
-  }
-
-  private void commit(final TransactionState state) throws IOException {
-
-    LOG.debug("Commiting transaction: " + state.toString() + " to "
-        + super.getRegionInfo().getRegionNameAsString());
-
-    if (state.getWriteSet().size() > 0) {
-      logManager.writeCommitToLog(state.getTransactionId());
-    }
-
-    for (BatchUpdate update : state.getWriteSet()) {
-      this.batchUpdate(update, false); // Don't need to WAL these
-      // FIME, maybe should be walled so we don't need to look so far back.
-    }
-
-    state.setStatus(Status.COMMITED);
-    if (state.getWriteSet().size() > 0
-        && !commitPendingTransactions.remove(state)) {
-      LOG
-          .fatal("Commiting a non-query transaction that is not in commitPendingTransactions");
-      throw new IOException("commit failure"); // FIXME, how to handle?
-    }
-    retireTransaction(state);
-  }
-
-  // Cancel leases, and removed from lease lookup. This transaction may still
-  // live in commitedTransactionsBySequenceNumber and commitPendingTransactions
-  private void retireTransaction(final TransactionState state) {
-    String key = String.valueOf(state.getTransactionId());
-    try {
-      transactionLeases.cancelLease(key);
-    } catch (LeaseException e) {
-      // Ignore
-    }
-
-    transactionsById.remove(key);
-  }
-
-  protected TransactionState getTransactionState(final long transactionId)
-      throws UnknownTransactionException {
-    String key = String.valueOf(transactionId);
-    TransactionState state = null;
-
-    state = transactionsById.get(key);
-
-    if (state == null) {
-      LOG.trace("Unknown transaction: " + key);
-      throw new UnknownTransactionException(key);
-    }
-
-    try {
-      transactionLeases.renewLease(key);
-    } catch (LeaseException e) {
-      throw new RuntimeException(e);
-    }
-
-    return state;
-  }
-
-  private void maybeTriggerOldTransactionFlush() {
-      if (commitedTransactionsBySequenceNumber.size() > oldTransactionFlushTrigger) {
-        removeUnNeededCommitedTransactions();
-      }
-  }
-  
-  /**
-   * Cleanup references to committed transactions that are no longer needed.
-   * 
-   */
-  synchronized void removeUnNeededCommitedTransactions() {
-    Integer minStartSeqNumber = getMinStartSequenceNumber();
-    if (minStartSeqNumber == null) {
-      minStartSeqNumber = Integer.MAX_VALUE; // Remove all
-    }
-
-    int numRemoved = 0;
-    // Copy list to avoid conc update exception
-    for (Entry<Integer, TransactionState> entry : new LinkedList<Entry<Integer, TransactionState>>(
-        commitedTransactionsBySequenceNumber.entrySet())) {
-      if (entry.getKey() >= minStartSeqNumber) {
-        break;
-      }
-      numRemoved = numRemoved
-          + (commitedTransactionsBySequenceNumber.remove(entry.getKey()) == null ? 0
-              : 1);
-      numRemoved++;
-    }
-
-    if (LOG.isDebugEnabled()) {
-      StringBuilder debugMessage = new StringBuilder();
-      if (numRemoved > 0) {
-        debugMessage.append("Removed ").append(numRemoved).append(
-            " commited transactions");
-
-        if (minStartSeqNumber == Integer.MAX_VALUE) {
-          debugMessage.append("with any sequence number");
-        } else {
-          debugMessage.append("with sequence lower than ").append(
-              minStartSeqNumber).append(".");
-        }
-        if (!commitedTransactionsBySequenceNumber.isEmpty()) {
-          debugMessage.append(" Still have ").append(
-              commitedTransactionsBySequenceNumber.size()).append(" left.");
-        } else {
-          debugMessage.append("None left.");
-        }
-        LOG.debug(debugMessage.toString());
-      } else if (commitedTransactionsBySequenceNumber.size() > 0) {
-        debugMessage.append(
-            "Could not remove any transactions, and still have ").append(
-            commitedTransactionsBySequenceNumber.size()).append(" left");
-        LOG.debug(debugMessage.toString());
-      }
-    }
-  }
-
-  private Integer getMinStartSequenceNumber() {
-    Integer min = null;
-    for (TransactionState transactionState : transactionsById.values()) {
-      if (min == null || transactionState.getStartSequenceNumber() < min) {
-        min = transactionState.getStartSequenceNumber();
-      }
-    }
-    return min;
-  }
-
-  // TODO, resolve from the global transaction log
-  protected void resolveTransactionFromLog() {
-    throw new RuntimeException("Globaql transaction log is not Implemented");
-  }
-
-  private class TransactionLeaseListener implements LeaseListener {
-    private final String transactionName;
-
-    TransactionLeaseListener(final String n) {
-      this.transactionName = n;
-    }
-
-    public void leaseExpired() {
-      LOG.info("Transaction " + this.transactionName + " lease expired");
-      TransactionState s = null;
-      synchronized (transactionsById) {
-        s = transactionsById.remove(transactionName);
-      }
-      if (s == null) {
-        LOG.warn("Unknown transaction expired " + this.transactionName);
-        return;
-      }
-
-      switch (s.getStatus()) {
-      case PENDING:
-        s.setStatus(Status.ABORTED); // Other transactions may have a ref
-        break;
-      case COMMIT_PENDING:
-        LOG.info("Transaction " + s.getTransactionId()
-            + " expired in COMMIT_PENDING state");
-        LOG.info("Checking transaction status in transaction log");
-        resolveTransactionFromLog();
-        break;
-      default:
-        LOG.warn("Unexpected status on expired lease");
-      }
-    }
-  }
-
-  /** Wrapper which keeps track of rows returned by scanner. */
-  private class ScannerWrapper implements InternalScanner {
-    private long transactionId;
-    private InternalScanner scanner;
-
-    /**
-     * @param transactionId
-     * @param scanner
-     * @throws UnknownTransactionException
-     */
-    public ScannerWrapper(final long transactionId,
-        final InternalScanner scanner) throws UnknownTransactionException {
-
-      this.transactionId = transactionId;
-      this.scanner = scanner;
-    }
-
-    public void close() throws IOException {
-      scanner.close();
-    }
-
-    public boolean isMultipleMatchScanner() {
-      return scanner.isMultipleMatchScanner();
-    }
-
-    public boolean isWildcardScanner() {
-      return scanner.isWildcardScanner();
-    }
-
-    public boolean next(List<KeyValue> results) throws IOException {
-      boolean result = scanner.next(results);
-      TransactionState state = getTransactionState(transactionId);
-
-      if (result) {
-        // TODO: Is this right???? St.Ack
-        byte [] row = results.get(0).getRow();
-        Map<byte[], Cell> localWrites = state.localGetFull(row, null,
-            Integer.MAX_VALUE);
-        if (localWrites != null) {
-          LOG.info("Scanning over row that has been writen to " + transactionId);
-          for (Entry<byte[], Cell> entry : localWrites.entrySet()) {
-            // TODO: Is this right???
-            results.add(new KeyValue(row, entry.getKey(),
-              entry.getValue().getTimestamp(), entry.getValue().getValue()));
-          }
-        }
-      }
-
-      return result;
-    }
-  }
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalHLogManager.java	(working copy)
@@ -1,307 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.util.HashSet;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.io.BatchOperation;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.regionserver.HLog;
-import org.apache.hadoop.hbase.regionserver.HLogKey;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * Responsible for writing and reading (recovering) transactional information
- * to/from the HLog.
- */
-class TransactionalHLogManager {
-  /** If transactional log entry, these are the op codes */
-  // TODO: Make these into types on the KeyValue!!! -- St.Ack
-  public enum TransactionalOperation {
-    /** start transaction */
-    START,
-    /** Equivalent to append in non-transactional environment */
-    WRITE,
-    /** Transaction commit entry */
-    COMMIT,
-    /** Abort transaction entry */
-    ABORT
-  }
-
-  private static final Log LOG = LogFactory
-      .getLog(TransactionalHLogManager.class);
-
-  private final HLog hlog;
-  private final FileSystem fileSystem;
-  private final HRegionInfo regionInfo;
-  private final HBaseConfiguration conf;
-
-  /**
-   * @param region
-   */
-  public TransactionalHLogManager(final TransactionalRegion region) {
-    this.hlog = region.getLog();
-    this.fileSystem = region.getFilesystem();
-    this.regionInfo = region.getRegionInfo();
-    this.conf = region.getConf();
-  }
-
-  // For Testing
-  TransactionalHLogManager(final HLog hlog, final FileSystem fileSystem,
-      final HRegionInfo regionInfo, final HBaseConfiguration conf) {
-    this.hlog = hlog;
-    this.fileSystem = fileSystem;
-    this.regionInfo = regionInfo;
-    this.conf = conf;
-  }
-
-  /**
-   * @param transactionId
-   * @throws IOException
-   */
-  public void writeStartToLog(final long transactionId) throws IOException {
-    /*
-    HLogEdit logEdit;
-    logEdit = new HLogEdit(transactionId, TransactionalOperation.START);
-*/
-    hlog.append(regionInfo, null/*logEdit*/);
-  }
-
-  /**
-   * @param transactionId
-   * @param update
-   * @throws IOException
-   */
-  public void writeUpdateToLog(final long transactionId,
-      final BatchUpdate update) throws IOException {
-
-    long commitTime = update.getTimestamp() == HConstants.LATEST_TIMESTAMP ? System
-        .currentTimeMillis()
-        : update.getTimestamp();
-
-    for (BatchOperation op : update) {
-      // COMMENTED OUT  HLogEdit logEdit = new HLogEdit(transactionId, update.getRow(), op, commitTime);
-      hlog.append(regionInfo, update.getRow(), null /*logEdit*/);
-    }
-  }
-
-  /**
-   * @param transactionId
-   * @throws IOException
-   */
-  public void writeCommitToLog(final long transactionId) throws IOException {
-    /*HLogEdit logEdit;
-    logEdit = new HLogEdit(transactionId,
-        HLogEdit.TransactionalOperation.COMMIT);
-*/
-    hlog.append(regionInfo, null /*logEdit*/);
-  }
-
-  /**
-   * @param transactionId
-   * @throws IOException
-   */
-  public void writeAbortToLog(final long transactionId) throws IOException {
-    /*HLogEdit logEdit;
-    logEdit = new HLogEdit(transactionId, HLogEdit.TransactionalOperation.ABORT);
-*/
-    hlog.append(regionInfo, null /*logEdit*/);
-  }
-
-  /**
-   * @param reconstructionLog
-   * @param maxSeqID
-   * @param reporter
-   * @return map of batch updates
-   * @throws UnsupportedEncodingException
-   * @throws IOException
-   */
-  public Map<Long, List<BatchUpdate>> getCommitsFromLog(
-      final Path reconstructionLog, final long maxSeqID,
-      final Progressable reporter) throws UnsupportedEncodingException,
-      IOException {
-    if (reconstructionLog == null || !fileSystem.exists(reconstructionLog)) {
-      // Nothing to do.
-      return null;
-    }
-    // Check its not empty.
-    FileStatus[] stats = fileSystem.listStatus(reconstructionLog);
-    if (stats == null || stats.length == 0) {
-      LOG.warn("Passed reconstruction log " + reconstructionLog
-          + " is zero-length");
-      return null;
-    }
-
-    SortedMap<Long, List<BatchUpdate>> pendingTransactionsById = new TreeMap<Long, List<BatchUpdate>>();
-    SortedMap<Long, List<BatchUpdate>> commitedTransactionsById = new TreeMap<Long, List<BatchUpdate>>();
-    Set<Long> abortedTransactions = new HashSet<Long>();
-
-    SequenceFile.Reader logReader = new SequenceFile.Reader(fileSystem,
-        reconstructionLog, conf);
-    /*
-    try {
-      HLogKey key = new HLogKey();
-      KeyValue val = new KeyValue();
-      long skippedEdits = 0;
-      long totalEdits = 0;
-      long startCount = 0;
-      long writeCount = 0;
-      long abortCount = 0;
-      long commitCount = 0;
-      // How many edits to apply before we send a progress report.
-      int reportInterval = conf.getInt("hbase.hstore.report.interval.edits",
-          2000);
-
-      while (logReader.next(key, val)) {
-        LOG.debug("Processing edit: key: " + key.toString() + " val: "
-            + val.toString());
-        if (key.getLogSeqNum() < maxSeqID) {
-          skippedEdits++;
-          continue;
-        }
-        // TODO: Change all below so we are not doing a getRow and getColumn
-        // against a KeyValue.  Each invocation creates a new instance.  St.Ack.
-
-        // Check this edit is for me.
-
-        byte[] column = val.getKeyValue().getColumn();
-        Long transactionId = val.getTransactionId();
-        if (!val.isTransactionEntry() || HLog.isMetaColumn(column)
-            || !Bytes.equals(key.getRegionName(), regionInfo.getRegionName())) {
-          continue;
-        }
-
-        List<BatchUpdate> updates = pendingTransactionsById.get(transactionId);
-        switch (val.getOperation()) {
-
-        case START:
-          if (updates != null || abortedTransactions.contains(transactionId)
-              || commitedTransactionsById.containsKey(transactionId)) {
-            LOG.error("Processing start for transaction: " + transactionId
-                + ", but have already seen start message");
-            throw new IOException("Corrupted transaction log");
-          }
-          updates = new LinkedList<BatchUpdate>();
-          pendingTransactionsById.put(transactionId, updates);
-          startCount++;
-          break;
-
-        case WRITE:
-          if (updates == null) {
-            LOG.error("Processing edit for transaction: " + transactionId
-                + ", but have not seen start message");
-            throw new IOException("Corrupted transaction log");
-          }
-
-          BatchUpdate tranUpdate = new BatchUpdate(val.getKeyValue().getRow());
-          if (val.getKeyValue().getValue() != null) {
-            tranUpdate.put(val.getKeyValue().getColumn(),
-              val.getKeyValue().getValue());
-          } else {
-            tranUpdate.delete(val.getKeyValue().getColumn());
-          }
-          updates.add(tranUpdate);
-          writeCount++;
-          break;
-
-        case ABORT:
-          if (updates == null) {
-            LOG.error("Processing abort for transaction: " + transactionId
-                + ", but have not seen start message");
-            throw new IOException("Corrupted transaction log");
-          }
-          abortedTransactions.add(transactionId);
-          pendingTransactionsById.remove(transactionId);
-          abortCount++;
-          break;
-
-        case COMMIT:
-          if (updates == null) {
-            LOG.error("Processing commit for transaction: " + transactionId
-                + ", but have not seen start message");
-            throw new IOException("Corrupted transaction log");
-          }
-          if (abortedTransactions.contains(transactionId)) {
-            LOG.error("Processing commit for transaction: " + transactionId
-                + ", but also have abort message");
-            throw new IOException("Corrupted transaction log");
-          }
-          if (updates.size() == 0) {
-            LOG
-                .warn("Transaciton " + transactionId
-                    + " has no writes in log. ");
-          }
-          if (commitedTransactionsById.containsKey(transactionId)) {
-            LOG.error("Processing commit for transaction: " + transactionId
-                + ", but have already commited transaction with that id");
-            throw new IOException("Corrupted transaction log");
-          }
-          pendingTransactionsById.remove(transactionId);
-          commitedTransactionsById.put(transactionId, updates);
-          commitCount++;
-
-        }
-        totalEdits++;
-
-        if (reporter != null && (totalEdits % reportInterval) == 0) {
-          reporter.progress();
-        }
-      }
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Read " + totalEdits + " tranasctional operations (skipped "
-            + skippedEdits + " because sequence id <= " + maxSeqID + "): "
-            + startCount + " starts, " + writeCount + " writes, " + abortCount
-            + " aborts, and " + commitCount + " commits.");
-      }
-    } finally {
-      logReader.close();
-    }
-
-    if (pendingTransactionsById.size() > 0) {
-      LOG
-          .info("Region log has "
-              + pendingTransactionsById.size()
-              + " unfinished transactions. Going to the transaction log to resolve");
-      throw new RuntimeException("Transaction log not yet implemented");
-    }
-              */
-
-    return commitedTransactionsById;
-  }
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/CleanOldTransactionsChore.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/CleanOldTransactionsChore.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/CleanOldTransactionsChore.java	(working copy)
@@ -1,57 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.hadoop.hbase.Chore;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-
-/**
- * Cleans up committed transactions when they are no longer needed to verify
- * pending transactions.
- */
-class CleanOldTransactionsChore extends Chore {
-
-  private static final String SLEEP_CONF = "hbase.transaction.clean.sleep";
-  private static final int DEFAULT_SLEEP = 60 * 1000;
-
-  private final TransactionalRegionServer regionServer;
-
-  /**
-   * @param regionServer
-   * @param stopRequest
-   */
-  public CleanOldTransactionsChore(
-      final TransactionalRegionServer regionServer,
-      final AtomicBoolean stopRequest) {
-    super(regionServer.getConfiguration().getInt(SLEEP_CONF, DEFAULT_SLEEP),
-        stopRequest);
-    this.regionServer = regionServer;
-  }
-
-  @Override
-  protected void chore() {
-    for (HRegion region : regionServer.getOnlineRegions()) {
-      ((TransactionalRegion) region).removeUnNeededCommitedTransactions();
-    }
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegionServer.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/transactional/TransactionalRegionServer.java	(working copy)
@@ -1,304 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver.transactional;
-
-import java.io.IOException;
-import java.lang.Thread.UncaughtExceptionHandler;
-import java.util.Arrays;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.TreeSet;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.NotServingRegionException;
-import org.apache.hadoop.hbase.RemoteExceptionHandler;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.HbaseMapWritable;
-import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.HRegionServer;
-import org.apache.hadoop.hbase.regionserver.InternalScanner;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.Threads;
-import org.apache.hadoop.io.MapWritable;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * RegionServer with support for transactions. Transactional logic is at the
- * region level, so we mostly just delegate to the appropriate
- * TransactionalRegion.
- */
-public class TransactionalRegionServer extends HRegionServer implements
-    TransactionalRegionInterface {
-  static final Log LOG = LogFactory.getLog(TransactionalRegionServer.class);
-
-  private final CleanOldTransactionsChore cleanOldTransactionsThread;
-
-  /**
-   * @param conf
-   * @throws IOException
-   */
-  public TransactionalRegionServer(final HBaseConfiguration conf)
-      throws IOException {
-    this(new HServerAddress(conf.get(REGIONSERVER_ADDRESS,
-        DEFAULT_REGIONSERVER_ADDRESS)), conf);
-  }
-
-  /**
-   * @param address
-   * @param conf
-   * @throws IOException
-   */
-  public TransactionalRegionServer(final HServerAddress address,
-      final HBaseConfiguration conf) throws IOException {
-    super(address, conf);
-    cleanOldTransactionsThread = new CleanOldTransactionsChore(this,
-        super.stopRequested);
-  }
-
-  @Override
-  public long getProtocolVersion(final String protocol, final long clientVersion)
-      throws IOException {
-    if (protocol.equals(TransactionalRegionInterface.class.getName())) {
-      return HBaseRPCProtocolVersion.versionID;
-    }
-    return super.getProtocolVersion(protocol, clientVersion);
-  }
-
-  @Override
-  protected void init(final MapWritable c) throws IOException {
-    super.init(c);
-    String n = Thread.currentThread().getName();
-    UncaughtExceptionHandler handler = new UncaughtExceptionHandler() {
-      public void uncaughtException(final Thread t, final Throwable e) {
-        abort();
-        LOG.fatal("Set stop flag in " + t.getName(), e);
-      }
-    };
-    Threads.setDaemonThreadRunning(this.cleanOldTransactionsThread, n
-        + ".oldTransactionCleaner", handler);
-
-  }
-
-  @Override
-  protected HRegion instantiateRegion(final HRegionInfo regionInfo)
-      throws IOException {
-    HRegion r = new TransactionalRegion(HTableDescriptor.getTableDir(super
-        .getRootDir(), regionInfo.getTableDesc().getName()), super.log, super
-        .getFileSystem(), super.conf, regionInfo, super.getFlushRequester());
-    r.initialize(null, new Progressable() {
-      public void progress() {
-        addProcessingMessage(regionInfo);
-      }
-    });
-    return r;
-  }
-
-  protected TransactionalRegion getTransactionalRegion(final byte[] regionName)
-      throws NotServingRegionException {
-    return (TransactionalRegion) super.getRegion(regionName);
-  }
-
-  public void abort(final byte[] regionName, final long transactionId)
-      throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      getTransactionalRegion(regionName).abort(transactionId);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public void batchUpdate(final long transactionId, final byte[] regionName,
-      final BatchUpdate b) throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      getTransactionalRegion(regionName).batchUpdate(transactionId, b);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public void commit(final byte[] regionName, final long transactionId)
-      throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      getTransactionalRegion(regionName).commit(transactionId);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public boolean commitRequest(final byte[] regionName, final long transactionId)
-      throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      return getTransactionalRegion(regionName).commitRequest(transactionId);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public Cell get(final long transactionId, final byte[] regionName,
-      final byte[] row, final byte[] column) throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      return getTransactionalRegion(regionName).get(transactionId, row, column);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public Cell[] get(final long transactionId, final byte[] regionName,
-      final byte[] row, final byte[] column, final int numVersions)
-      throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      return getTransactionalRegion(regionName).get(transactionId, row, column,
-          numVersions);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public Cell[] get(final long transactionId, final byte[] regionName,
-      final byte[] row, final byte[] column, final long timestamp,
-      final int numVersions) throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      return getTransactionalRegion(regionName).get(transactionId, row, column,
-          timestamp, numVersions);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public RowResult getRow(final long transactionId, final byte[] regionName,
-      final byte[] row, final long ts) throws IOException {
-    return getRow(transactionId, regionName, row, null, ts);
-  }
-
-  public RowResult getRow(final long transactionId, final byte[] regionName,
-      final byte[] row, final byte[][] columns) throws IOException {
-    return getRow(transactionId, regionName, row, columns,
-        HConstants.LATEST_TIMESTAMP);
-  }
-
-  public RowResult getRow(final long transactionId, final byte[] regionName,
-      final byte[] row, final byte[][] columns, final long ts)
-      throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      // convert the columns array into a set so it's easy to check later.
-      NavigableSet<byte[]> columnSet = null;
-      if (columns != null) {
-        columnSet = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-        columnSet.addAll(Arrays.asList(columns));
-      }
-
-      TransactionalRegion region = getTransactionalRegion(regionName);
-      Map<byte[], Cell> map = region.getFull(transactionId, row, columnSet, ts);
-      HbaseMapWritable<byte[], Cell> result = new HbaseMapWritable<byte[], Cell>();
-      result.putAll(map);
-      return new RowResult(row, result);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-
-  }
-
-  public void deleteAll(final long transactionId, final byte[] regionName,
-      final byte[] row, final long timestamp) throws IOException {
-    checkOpen();
-    super.getRequestCount().incrementAndGet();
-    try {
-      TransactionalRegion region = getTransactionalRegion(regionName);
-      region.deleteAll(transactionId, row, timestamp);
-    } catch (IOException e) {
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public long openScanner(final long transactionId, final byte[] regionName,
-      final byte[][] cols, final byte[] firstRow, final long timestamp,
-      final RowFilterInterface filter) throws IOException {
-    checkOpen();
-    NullPointerException npe = null;
-    if (regionName == null) {
-      npe = new NullPointerException("regionName is null");
-    } else if (cols == null) {
-      npe = new NullPointerException("columns to scan is null");
-    } else if (firstRow == null) {
-      npe = new NullPointerException("firstRow for scanner is null");
-    }
-    if (npe != null) {
-      IOException io = new IOException("Invalid arguments to openScanner");
-      io.initCause(npe);
-      throw io;
-    }
-    super.getRequestCount().incrementAndGet();
-    try {
-      TransactionalRegion r = getTransactionalRegion(regionName);
-      long scannerId = -1L;
-      InternalScanner s = r.getScanner(transactionId, cols, firstRow,
-          timestamp, filter);
-      scannerId = super.addScanner(s);
-      return scannerId;
-    } catch (IOException e) {
-      LOG.error("Error opening scanner (fsOk: " + this.fsOk + ")",
-          RemoteExceptionHandler.checkIOException(e));
-      checkFileSystem();
-      throw e;
-    }
-  }
-
-  public void beginTransaction(final long transactionId, final byte[] regionName)
-      throws IOException {
-    getTransactionalRegion(regionName).beginTransaction(transactionId);
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -81,11 +81,14 @@
 import org.apache.hadoop.hbase.Leases.LeaseStillHeldException;
 import org.apache.hadoop.hbase.client.ServerConnection;
 import org.apache.hadoop.hbase.client.ServerConnectionManager;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
 import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
@@ -1726,8 +1729,8 @@
     }
   }
 
-  public RowResult getClosestRowBefore(final byte [] regionName, 
-    final byte [] row, final byte [] columnFamily)
+  public Result getClosestRowBefore(final byte [] regionName, 
+    final byte [] row, final byte [] family)
   throws IOException {
     checkOpen();
     requestCount.incrementAndGet();
@@ -1735,49 +1738,17 @@
       // locate the region we're operating on
       HRegion region = getRegion(regionName);
       // ask the region for all the data 
-      RowResult rr = region.getClosestRowBefore(row, columnFamily);
-      return rr;
+      Result r = region.getClosestRowBefore(row, family);
+      return r;
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
   }
+
   
-  public RowResult next(final long scannerId) throws IOException {
-    RowResult[] rrs = next(scannerId, 1);
-    return rrs.length == 0 ? null : rrs[0];
-  }
-
-  public RowResult [] next(final long scannerId, int nbRows) throws IOException {
-    checkOpen();
-    List<List<KeyValue>> results = new ArrayList<List<KeyValue>>();
-    try {
-      String scannerName = String.valueOf(scannerId);
-      InternalScanner s = scanners.get(scannerName);
-      if (s == null) {
-        throw new UnknownScannerException("Name: " + scannerName);
-      }
-      this.leases.renewLease(scannerName);
-      for (int i = 0; i < nbRows; i++) {
-        requestCount.incrementAndGet();
-        // Collect values to be returned here
-        List<KeyValue> values = new ArrayList<KeyValue>();
-        while (s.next(values)) {
-          if (!values.isEmpty()) {
-            // Row has something in it. Return the value.
-            results.add(values);
-            break;
-          }
-        }
-      }
-      return RowResult.createRowResultArray(results);
-    } catch (Throwable t) {
-      throw convertThrowableToIOE(cleanup(t));
-    }
-  }
-
-  public void batchUpdate(final byte [] regionName, BatchUpdate b, long lockId)
+  public void put(final byte [] regionName, final Put put)
   throws IOException {
-    if (b.getRow() == null)
+    if (put.getRow() == null)
       throw new IllegalArgumentException("update has null row");
     
     checkOpen();
@@ -1785,24 +1756,24 @@
     HRegion region = getRegion(regionName);
     try {
       cacheFlusher.reclaimMemcacheMemory();
-      region.batchUpdate(b, getLockFromId(b.getRowLock()));
+      region.put(put, getLockFromId(put.getRowLock().getLockId()));
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
   }
   
-  public int batchUpdates(final byte[] regionName, final BatchUpdate [] b)
+  public int put(final byte[] regionName, final Put [] puts)
   throws IOException {
     int i = 0;
     checkOpen();
     try {
       HRegion region = getRegion(regionName);
       this.cacheFlusher.reclaimMemcacheMemory();
-      Integer[] locks = new Integer[b.length];
-      for (i = 0; i < b.length; i++) {
+      Integer[] locks = new Integer[puts.length];
+      for (i = 0; i < puts.length; i++) {
         this.requestCount.incrementAndGet();
-        locks[i] = getLockFromId(b[i].getRowLock());
-        region.batchUpdate(b[i], locks[i]);
+        locks[i] = getLockFromId(puts[i].getRowLock().getLockId());
+        region.put(puts[i], locks[i]);
       }
     } catch(WrongRegionException ex) {
       return i;
@@ -1814,18 +1785,18 @@
     return -1;
   }
   
-  public boolean checkAndSave(final byte [] regionName, final BatchUpdate b,
+  public boolean checkAndSave(final byte [] regionName, final Put put,
       final HbaseMapWritable<byte[],byte[]> expectedValues)
   throws IOException {
-    if (b.getRow() == null)
+    if (put.getRow() == null)
       throw new IllegalArgumentException("update has null row");
     checkOpen();
     this.requestCount.incrementAndGet();
     HRegion region = getRegion(regionName);
     try {
       cacheFlusher.reclaimMemcacheMemory();
-      return region.checkAndSave(b,
-        expectedValues,getLockFromId(b.getRowLock()), true);
+      return region.checkAndSave(put,
+        expectedValues,getLockFromId(put.getRowLock().getLockId()), true);
     } catch (Throwable t) {
       throw convertThrowableToIOE(cleanup(t));
     }
@@ -1835,17 +1806,14 @@
   // remote scanner interface
   //
 
-  public long openScanner(byte [] regionName, byte [][] cols, byte [] firstRow,
-    final long timestamp, final RowFilterInterface filter)
+  public long openScanner(byte [] regionName, byte [] startRow, Scan scan)
   throws IOException {
     checkOpen();
     NullPointerException npe = null;
     if (regionName == null) {
       npe = new NullPointerException("regionName is null");
-    } else if (cols == null) {
-      npe = new NullPointerException("columns to scan is null");
-    } else if (firstRow == null) {
-      npe = new NullPointerException("firstRow for scanner is null");
+    } else if (scan == null) {
+      npe = new NullPointerException("scan is null");
     }
     if (npe != null) {
       throw new IOException("Invalid arguments to openScanner", npe);
@@ -1853,8 +1821,7 @@
     requestCount.incrementAndGet();
     try {
       HRegion r = getRegion(regionName);
-      InternalScanner s =
-        r.getScanner(cols, firstRow, timestamp, filter);
+      InternalScanner s = r.getScanner(startRow, scan);
       long scannerId = addScanner(s);
       return scannerId;
     } catch (Throwable t) {
@@ -1873,7 +1840,43 @@
       createLease(scannerName, new ScannerListener(scannerName));
     return scannerId;
   }
+
+  public Result next(final long scannerId) throws IOException {
+    Result [] res = next(scannerId, 1);
+    if(res == null || res.length == 0) {
+      return null;
+    }
+    return res[0];
+  }
   
+  public Result [] next(final long scannerId, int nbRows) throws IOException {
+    checkOpen();
+    List<List<KeyValue>> results = new ArrayList<List<KeyValue>>();
+    try {
+      String scannerName = String.valueOf(scannerId);
+      InternalScanner s = scanners.get(scannerName);
+      if (s == null) {
+        throw new UnknownScannerException("Name: " + scannerName);
+      }
+      this.leases.renewLease(scannerName);
+      for (int i = 0; i < nbRows; i++) {
+        requestCount.incrementAndGet();
+        // Collect values to be returned here
+        List<KeyValue> values = new ArrayList<KeyValue>();
+        while (s.next(values)) {
+          if (!values.isEmpty()) {
+            // Row has something in it. Return the value.
+            results.add(values);
+            break;
+          }
+        }
+      }
+      return results.toArray(new Result[results.size()]);
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
+
   public void close(final long scannerId) throws IOException {
     try {
       checkOpen();
@@ -1924,39 +1927,23 @@
   // Methods that do the actual work for the remote API
   //
   
-  public void deleteAll(final byte [] regionName, final byte [] row,
-      final byte [] column, final long timestamp, final long lockId) 
+  public void delete(final byte [] regionName, final Delete delete)
   throws IOException {
-    HRegion region = getRegion(regionName);
-    region.deleteAll(row, column, timestamp, getLockFromId(lockId));
+    checkOpen();
+    try {
+      boolean writeToWAL = true;
+      this.cacheFlusher.reclaimMemcacheMemory();
+      this.requestCount.incrementAndGet();
+      Integer lock = getLockFromId(delete.getRowLock().getLockId());
+      HRegion region = getRegion(regionName);
+      region.delete(delete, lock, writeToWAL);
+    } catch(WrongRegionException ex) {
+    } catch (NotServingRegionException ex) {
+    } catch (Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
   }
-
-  public void deleteAll(final byte [] regionName, final byte [] row,
-      final long timestamp, final long lockId) 
-  throws IOException {
-    HRegion region = getRegion(regionName);
-    region.deleteAll(row, timestamp, getLockFromId(lockId));
-  }
-
-  public void deleteAllByRegex(byte[] regionName, byte[] row, String colRegex,
-      long timestamp, long lockId) throws IOException {
-    getRegion(regionName).deleteAllByRegex(row, colRegex, timestamp, 
-        getLockFromId(lockId));
-  }
-
-  public void deleteFamily(byte [] regionName, byte [] row, byte [] family, 
-    long timestamp, final long lockId)
-  throws IOException{
-    getRegion(regionName).deleteFamily(row, family, timestamp,
-        getLockFromId(lockId));
-  }
-
-  public void deleteFamilyByRegex(byte[] regionName, byte[] row, String familyRegex,
-      long timestamp, long lockId) throws IOException {
-    getRegion(regionName).deleteFamilyByRegex(row, familyRegex, timestamp, 
-        getLockFromId(lockId));
-  }
-
+  
   public boolean exists(byte[] regionName, byte[] row, byte[] column,
       long timestamp, long lockId)
   throws IOException {
@@ -2010,7 +1997,7 @@
    * @return intId Integer row lock used internally in HRegion
    * @throws IOException Thrown if this is not a valid client lock id.
    */
-  private Integer getLockFromId(long lockId)
+  Integer getLockFromId(long lockId)
   throws IOException {
     if(lockId == -1L) {
       return null;
@@ -2450,7 +2437,21 @@
       checkFileSystem();
       throw e;
     }
-    
-    
   }
+  
+  //
+  // HBASE-880
+  //
+  
+  /** {@inheritDoc} */
+  public Result get(byte [] regionName, Get get) throws IOException {
+    checkOpen();
+    requestCount.incrementAndGet();
+    try {
+      HRegion region = getRegion(regionName);
+      return region.get(get, getLockFromId(get.getLockId()));
+    } catch(Throwable t) {
+      throw convertThrowableToIOE(cleanup(t));
+    }
+  }
 }
Index: src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java	(revision 0)
@@ -0,0 +1,174 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+import java.util.PriorityQueue;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+
+/**
+ * Implements a heap merge across any number of KeyValueScanners.
+ * <p>
+ * Implements KeyValueScanner itself.
+ * <p>
+ * This class is used at the Region level to merge across Stores
+ * and at the Store level to merge across the Memcache and StoreFiles.
+ * <p>
+ * In the Region case, we also need InternalScanner.next(List), so this class
+ * also implements InternalScanner.  WARNING: As is, if you try to use this
+ * as an InternalScanner at the Store level, you will get runtime exceptions. 
+ */
+public class KeyValueHeap implements KeyValueScanner, InternalScanner {
+  
+  private PriorityQueue<KeyValueScanner> heap;
+  
+  private KeyValueScanner current = null;
+  
+  private KVScannerComparator comparator;
+  
+  /**
+   * Constructor
+   * @param scanners
+   * @param comparator
+   */
+  public KeyValueHeap(KeyValueScanner [] scanners, KVComparator comparator) {
+    this.comparator = new KVScannerComparator(comparator);
+    this.heap = new PriorityQueue<KeyValueScanner>(scanners.length, 
+        this.comparator);
+    for(KeyValueScanner scanner : scanners) {
+      if(scanner.peek() != null) {
+        this.heap.add(scanner);
+      }
+    }
+    this.current = heap.poll();
+  }
+  
+  public KeyValue peek() {
+    if(this.current == null) {
+      return null;
+    }
+    return this.current.peek();
+  }
+  
+  public KeyValue next()  {
+    if(this.current == null) {
+      return null;
+    }
+    KeyValue kvReturn = this.current.next();
+    KeyValue kvNext = this.current.peek();
+    if(kvNext == null) {
+      this.current.close();
+      this.current = this.heap.poll();
+    } else {
+      KeyValueScanner topScanner = this.heap.peek();
+      if(topScanner == null ||
+          this.comparator.compare(kvNext, topScanner.peek()) > 0) {
+        this.heap.add(this.current);
+        this.current = this.heap.poll();
+      }
+    }
+    return kvReturn;
+  }
+  
+  /**
+   * Gets the next row of keys from the top-most scanner.
+   * <p>
+   * This method takes care of updating the heap.
+   * <p>
+   * This can ONLY be called when you are using Scanners that implement
+   * InternalScanner as well as KeyValueScanner (a {@link StoreScanner}).
+   * @return true if there are more keys, false if all scanners are done 
+   */
+  public boolean next(List<KeyValue> result) throws IOException {
+    InternalScanner currentAsInternal = (InternalScanner)this.current;
+    if(!currentAsInternal.next(result)) {
+      this.current.close();
+    } else {
+      this.heap.add(this.current);
+    }
+    this.current = this.heap.poll();
+    return (this.current != null);
+  }
+  
+  private class KVScannerComparator implements Comparator<KeyValueScanner> {
+    private KVComparator kvComparator;
+    public KVScannerComparator(KVComparator kvComparator) {
+      this.kvComparator = kvComparator;
+    }
+    public int compare(KeyValueScanner left, KeyValueScanner right) {
+      return compare(left.peek(), right.peek());
+    }
+    public int compare(KeyValue left, KeyValue right) {
+      return this.kvComparator.compare(left, right);
+    }
+    public KVComparator getComparator() {
+      return this.kvComparator;
+    }
+  }
+
+  public void close() {
+    if(this.current != null) {
+      this.current.close();
+    }
+    KeyValueScanner scanner;
+    while((scanner = this.heap.poll()) != null) {
+      scanner.close();
+    }
+  }
+  
+  /**
+   * Seeks all scanners at or below the specified seek key.  If we earlied-out 
+   * of a row, we may end up skipping values that were never reached yet.
+   * Rather than iterating down, we want to give the opportunity to re-seek.
+   * <p>
+   * As individual scanners may run past their ends, those scanners are
+   * automatically closed and removed from the heap.
+   * @param key KeyValue to seek at or after
+   * @return true if KeyValues exist at or after specified key, false if not
+   */
+  public boolean seek(KeyValue seekKey) {
+    if(this.current == null) {
+      return false;
+    }
+    this.heap.add(this.current);
+    this.current = null;
+
+    KeyValueScanner scanner;
+    while((scanner = this.heap.poll()) != null) {
+      KeyValue topKey = scanner.peek();
+      if(comparator.getComparator().compare(seekKey, topKey) <= 0) { // Correct?
+        // Top KeyValue is at-or-after Seek KeyValue
+        this.current = scanner;
+        return true;
+      }
+      if(!scanner.seek(seekKey)) {
+        scanner.close();
+      } else {
+        this.heap.add(scanner);
+      }
+    }
+    // Heap is returning empty, scanner is done
+    return false;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/InternalScanner.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -41,11 +41,9 @@
  */
 public interface InternalScanner extends Closeable {
   /**
-   * Grab the next row's worth of values. The scanner will return the most
-   * recent data value for each row that is not newer than the target time
-   * passed when the scanner was created.
+   * Grab the next row's worth of values.
    * @param results
-   * @return true if data was returned
+   * @return true if more rows exist after this one, false if scanner is done
    * @throws IOException
    */
   public boolean next(List<KeyValue> results)
@@ -55,11 +53,5 @@
    * Closes the scanner and releases any resources it has allocated
    * @throws IOException
    */
-  public void close() throws IOException;  
-  
-  /** @return true if the scanner is matching a column family or regex */
-  public boolean isWildcardScanner();
-  
-  /** @return true if the scanner is matching multiple column family members */
-  public boolean isMultipleMatchScanner();
+  public void close() throws IOException;
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/HLog.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HLog.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/HLog.java	(working copy)
@@ -98,7 +98,7 @@
 public class HLog implements HConstants, Syncable {
   private static final Log LOG = LogFactory.getLog(HLog.class);
   private static final String HLOG_DATFILE = "hlog.dat.";
-  static final byte [] METACOLUMN = Bytes.toBytes("METACOLUMN:");
+  static final byte [] METAFAMILY = Bytes.toBytes("METAFAMILY");
   static final byte [] METAROW = Bytes.toBytes("METAROW");
   private final FileSystem fs;
   private final Path dir;
@@ -682,8 +682,8 @@
   }
 
   private KeyValue completeCacheFlushLogEdit() {
-    return new KeyValue(METAROW, METACOLUMN, System.currentTimeMillis(),
-      COMPLETE_CACHE_FLUSH);
+    return new KeyValue(METAROW, METAFAMILY, null,
+      System.currentTimeMillis(), COMPLETE_CACHE_FLUSH);
   }
 
   /**
@@ -700,8 +700,8 @@
    * @param column
    * @return true if the column is a meta column
    */
-  public static boolean isMetaColumn(byte [] column) {
-    return Bytes.equals(METACOLUMN, column);
+  public static boolean isMetaFamily(byte [] family) {
+    return Bytes.equals(METAFAMILY, family);
   }
   
   /**
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreFileScan.java	(revision 0)
@@ -0,0 +1,98 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.hfile.HFileScanner;
+
+
+public class StoreFileScan {
+
+  private List<HFileScanner> scanners;
+  private QueryMatcher matcher;
+  
+  private byte [] startKey;
+  
+  /**
+   * Constructor
+   * @param scanners
+   * @param matcher
+   */
+  public StoreFileScan(List<HFileScanner> scanners, QueryMatcher matcher) {
+    this.scanners = scanners;
+    this.matcher = matcher;
+    this.startKey = matcher.getStartKey().getBuffer();
+  }
+  
+  /**
+   * Performs a GET operation across multiple StoreFiles.
+   * <p>
+   * This style of StoreFile scanning goes through each
+   * StoreFile in its entirety, most recent first, before
+   * proceeding to the next StoreFile.
+   * <p>
+   * This strategy allows for optimal, stateless (no persisted Scanners)
+   * early-out scenarios.    
+   * @param result List to add results to
+   * @throws IOException
+   */
+  public void get(List<KeyValue> result) throws IOException {
+    for(HFileScanner scanner : this.scanners) {
+      this.matcher.update();
+      if(getStoreFile(scanner, result)) {
+        return;
+      }
+    }
+  }
+  
+  /**
+   * Performs a GET operation on a single StoreFile.
+   * @param scanner
+   * @param result
+   * @return true if done with this store, false if must continue to next
+   * @throws IOException 
+   */
+  public boolean getStoreFile(HFileScanner scanner, List<KeyValue> result) 
+  throws IOException {
+    if(scanner.seekTo(this.startKey) == -1) {
+      // No keys in StoreFile at or after specified startKey
+      return false;
+    }
+    do {
+      KeyValue kv = scanner.getKeyValue();
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    } while(scanner.next());
+    return false;
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/Memcache.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/Memcache.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/Memcache.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -32,7 +32,6 @@
 import java.util.Set;
 import java.util.SortedMap;
 import java.util.SortedSet;
-import java.util.TreeMap;
 import java.util.TreeSet;
 import java.util.concurrent.ConcurrentSkipListSet;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
@@ -42,7 +41,7 @@
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.regionserver.HRegion.Counter;
+import org.apache.hadoop.hbase.regionserver.DeleteCompare.DeleteCode;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -82,7 +81,7 @@
 
   // TODO: Fix this guess by studying jprofiler
   private final static int ESTIMATED_KV_HEAP_TAX = 60;
-
+  
   /**
    * Default constructor. Used for tests.
    */
@@ -202,7 +201,86 @@
     }
     return size;
   }
+  
+  /** 
+   * Write a delete
+   * @param delete
+   * @return approximate size of the passed key and value.
+   */
+  long delete(final KeyValue delete) {
+    long size = -1;
+    this.lock.readLock().lock();
+    //Have to find out what want to do here, to find the fastest way of removing
+    //things that are under a delete.
+    //Actions that will take place here are:
+    //1. Insert a delete and remove all the affected entries already in memcache
+    //2. In the case of a Delete and the matching put is found then don't insert
+    //   the delete
+    //TODO Would be nice with if we had an iterator for this, so we could remove
+    //things that needs to be removed while iterating and don't have to go
+    //back and do it afterwards
+    try {
+      boolean notpresent = false;
+      List<KeyValue> deletes = new ArrayList<KeyValue>();
+      SortedSet<KeyValue> tailSet = this.memcache.tailSet(delete);
 
+      //Parse the delete, so that it is only done once
+      byte [] deleteBuffer = delete.getBuffer();
+      int deleteOffset = delete.getOffset();
+  
+      int deleteKeyLen = Bytes.toInt(deleteBuffer, deleteOffset);
+      deleteOffset += Bytes.SIZEOF_INT + Bytes.SIZEOF_INT;
+  
+      short deleteRowLen = Bytes.toShort(deleteBuffer, deleteOffset);
+      deleteOffset += Bytes.SIZEOF_SHORT;
+      int deleteRowOffset = deleteOffset;
+  
+      deleteOffset += deleteRowLen;
+  
+      byte deleteFamLen = deleteBuffer[deleteOffset];
+      deleteOffset += Bytes.SIZEOF_BYTE + deleteFamLen;
+  
+      int deleteQualifierOffset = deleteOffset;
+      int deleteQualifierLen = deleteKeyLen - deleteRowLen - deleteFamLen -
+        Bytes.SIZEOF_SHORT - Bytes.SIZEOF_BYTE - Bytes.SIZEOF_LONG - 
+        Bytes.SIZEOF_BYTE;
+      deleteOffset += deleteQualifierLen;
+  
+      int deleteTimestampOffset = deleteOffset;
+      deleteOffset += Bytes.SIZEOF_LONG;
+      byte deleteType = deleteBuffer[deleteOffset];
+      
+      //Comparing with tail from memcache
+      for(KeyValue mem : tailSet) {
+        DeleteCode res = DeleteCompare.deleteCompare(mem, deleteBuffer, 
+            deleteRowOffset, deleteRowLen, deleteQualifierOffset, 
+            deleteQualifierLen, deleteTimestampOffset, deleteType);
+        if(res == DeleteCode.DONE) {
+          break;
+        } else if (res == DeleteCode.DELETE) {
+          deletes.add(mem);
+        } // SKIP
+      }
+  
+      //If not a single Delete and it has been "used", add the delete to memcache
+      if(!(deletes.size() == 1 && deleteType == KeyValue.Type.Delete.getCode())) {
+        notpresent = this.memcache.add(delete);
+        size = heapSize(delete, notpresent);
+      } else {
+        size = 0;
+      }
+  
+      //Delete all the entries effected by the last added delete
+      for(KeyValue del : deletes) {
+        notpresent = this.memcache.remove(del);
+        size -= heapSize(del, notpresent);
+      }
+    } finally {
+      this.lock.readLock().unlock();
+    }
+    return size;
+  }
+  
   /*
    * Calculate how the memcache size has changed, approximately.  Be careful.
    * If class changes, be sure to change the size calculation.
@@ -621,93 +699,179 @@
   /**
    * @return a scanner over the keys in the Memcache
    */
-  InternalScanner getScanner(long timestamp,
-    final NavigableSet<byte []> targetCols, final byte [] firstRow)
+  KeyValueScanner getScanner() {
+    this.lock.readLock().lock();
+    try {
+      return new MemcacheScanner();
+    } finally {
+      this.lock.readLock().unlock();
+    }
+  }
+
+  //
+  // HBASE-880/1249/1304
+  //
+  
+  /**
+   * Perform a single-row Get on the memcache and snapshot, placing results
+   * into the specified KV list.
+   * <p>
+   * This will return true if it is determined that the query is complete
+   * and it is not necessary to check any storefiles after this.
+   * <p>
+   * Otherwise, it will return false and you should continue on.
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  public boolean get(QueryMatcher matcher, List<KeyValue> result)
   throws IOException {
     this.lock.readLock().lock();
     try {
-      return new MemcacheScanner(timestamp, targetCols, firstRow);
+      if(internalGet(this.memcache, matcher, result)) {
+        return true;
+      }
+      matcher.update();
+      if(internalGet(this.snapshot, matcher, result)) {
+        return true;
+      }
+      return false;
     } finally {
       this.lock.readLock().unlock();
     }
   }
-
+  
+  /**
+   * 
+   * @param set memcache or snapshot
+   * @param startKey Starting KeyValue
+   * @param matcher Column matcher
+   * @param result List to add results to
+   * @return true if done with store (early-out), false if not
+   * @throws IOException
+   */
+  private boolean internalGet(SortedSet<KeyValue> set, QueryMatcher matcher,
+      List<KeyValue> result) throws IOException {
+    if(set.isEmpty()) return false;
+    // Seek to startKey
+    set = set.tailSet(matcher.getStartKey());
+    for(KeyValue kv : set) {
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(kv);
+          break;
+        case SKIP:
+          break;
+        case NEXT:
+          return false;
+        case DONE:
+          return true;
+      }
+    }
+    return false;
+  }
+  
   //////////////////////////////////////////////////////////////////////////////
-  // MemcacheScanner implements the InternalScanner.
+  // MemcacheScanner implements the KeyValueScanner.
   // It lets the caller scan the contents of the Memcache.
+  // This behaves as if it were a real scanner but does not maintain position
+  // in the Memcache tree.
   //////////////////////////////////////////////////////////////////////////////
 
-  private class MemcacheScanner extends HAbstractScanner {
-    private KeyValue current;
-    private final NavigableSet<byte []> columns;
-    private final NavigableSet<KeyValue> deletes;
-    private final Map<KeyValue, Counter> versionCounter;
-    private final long now = System.currentTimeMillis();
-
-    MemcacheScanner(final long timestamp, final NavigableSet<byte []> columns,
-      final byte [] firstRow)
-    throws IOException {
-      // Call to super will create ColumnMatchers and whether this is a regex
-      // scanner or not.  Will also save away timestamp.  Also sorts rows.
-      super(timestamp, columns);
-      this.deletes = new TreeSet<KeyValue>(comparatorIgnoreType);
-      this.versionCounter =
-        new TreeMap<KeyValue, Counter>(comparatorIgnoreTimestamp);
-      this.current = KeyValue.createFirstOnRow(firstRow, timestamp);
-      // If we're being asked to scan explicit columns rather than all in 
-      // a family or columns that match regexes, cache the sorted array of
-      // columns.
-      this.columns = isWildcardScanner()? null: columns;
-    }
-
-    @Override
-    public boolean next(final List<KeyValue> keyvalues)
-    throws IOException {
-      if (this.scannerClosed) {
+  protected class MemcacheScanner implements KeyValueScanner {
+    private KeyValue current = null;
+    private List<KeyValue> result = new ArrayList<KeyValue>();
+    private int idx = 0;
+    
+    MemcacheScanner() {}
+    
+    public boolean seek(KeyValue key) {
+      try {
+        current = memcache.tailSet(key).first();
+        if(current == null) {
+          close();
+          return false;
+        }
+        return cacheNextRow();
+      } catch(Exception e) {
+        close();
         return false;
       }
-      while (keyvalues.isEmpty() && this.current != null) {
-        // Deletes are per row.
-        if (!deletes.isEmpty()) {
-          deletes.clear();
+    }
+    
+    public KeyValue peek() {
+      if(idx >= result.size()) {
+        if(!cacheNextRow()) {
+          return null;
         }
-        if (!versionCounter.isEmpty()) {
-          versionCounter.clear();
+        return peek();
+      }
+      return result.get(idx);
+    }
+    
+    public KeyValue next() {
+      if(idx >= result.size()) {
+        if(!cacheNextRow()) {
+          return null;
         }
-        // The getFull will take care of expired and deletes inside memcache.
-        // The first getFull when row is the special empty bytes will return
-        // nothing so we go around again.  Alternative is calling a getNextRow
-        // if row is null but that looks like it would take same amount of work
-        // so leave it for now.
-        getFull(this.current, isWildcardScanner()? null: this.columns, null, 1,
-          versionCounter, deletes, keyvalues, this.now);
-        for (KeyValue bb: keyvalues) {
-          if (isWildcardScanner()) {
-            // Check the results match.  We only check columns, not timestamps.
-            // We presume that timestamps have been handled properly when we
-            // called getFull.
-            if (!columnMatch(bb)) {
-              keyvalues.remove(bb);
-            }
-          }
+        return next();
+      }
+      return result.get(idx++);
+    }
+    
+    public boolean cacheNextRow() {
+      NavigableSet<KeyValue> keys;
+      try {
+        keys = memcache.tailSet(current);
+      } catch(Exception e) {
+        close();
+        return false;
+      }
+      if(keys == null || keys.isEmpty()) {
+        close();
+        return false;
+      }
+      byte [] row = keys.first().getRow();
+      for(KeyValue key : keys) {
+        if(comparator.compareRows(key, row) != 0) {
+          current = key;
+          break;
         }
-        // Add any deletes found so they are available to the StoreScanner#next.
-        if (!this.deletes.isEmpty()) {
-          keyvalues.addAll(deletes);
+        result.add(key);
+      }
+      return true;
+    }
+    
+    public boolean getNextRow(List<KeyValue> passedResult) {
+      NavigableSet<KeyValue> keys;
+      try {
+        keys = memcache.tailSet(current);
+      } catch(Exception e) {
+        close();
+        return false;
+      }
+      if(keys == null || keys.isEmpty()) {
+        close();
+        return false;
+      }
+      byte [] row = keys.first().getRow();
+      for(KeyValue key : keys) {
+        if(comparator.compareRows(key, row) != 0) {
+          current = key;
+          break;
         }
-        this.current = getNextRow(this.current);
-        // Change current to be column-less and to have the scanners' now.  We
-        // do this because first item on 'next row' may not have the scanners'
-        // now time which will cause trouble down in getFull; same reason no
-        // column.
-        if (this.current != null) this.current = this.current.cloneRow(this.now);
+        passedResult.add(key);
       }
-      return !keyvalues.isEmpty();
+      return true;
     }
 
     public void close() {
-      if (!scannerClosed) {
-        scannerClosed = true;
+      current = null;
+      idx = 0;
+      if(!result.isEmpty()) {
+        result.clear();
       }
     }
   }
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -21,306 +21,74 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.NavigableSet;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
 
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 
 /**
- * A scanner that iterates through HStore files
+ * A KeyValue scanner that iterates over a single HFile
  */
-class StoreFileScanner extends HAbstractScanner
-implements ChangedReadersObserver {
-    // Keys retrieved from the sources
-  private volatile KeyValue keys[];
+class StoreFileScanner implements KeyValueScanner {
   
-  // Readers we go against.
-  private volatile HFileScanner [] scanners;
+  private HFileScanner hfs;
+  private KeyValue cur = null;
   
-  // Store this scanner came out of.
-  private final Store store;
-  
-  // Used around replacement of Readers if they change while we're scanning.
-  private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
-
-  private final long now = System.currentTimeMillis();
-
   /**
-   * @param store
-   * @param timestamp
-   * @param columns
-   * @param firstRow
-   * @param deletes Set of running deletes
-   * @throws IOException
+   * Implements a {@link KeyValueScanner} on top of the specified {@link HFileScanner}
+   * @param hfs HFile scanner
    */
-  public StoreFileScanner(final Store store, final long timestamp,
-    final NavigableSet<byte []> columns, final byte [] firstRow)
-  throws IOException {
-    super(timestamp, columns);
-    this.store = store;
-    this.store.addChangedReaderObserver(this);
-    try {
-      openScanner(firstRow);
-    } catch (Exception ex) {
-      close();
-      IOException e = new IOException("HStoreScanner failed construction");
-      e.initCause(ex);
-      throw e;
-    }
+  public StoreFileScanner(HFileScanner hfs) {
+    this.hfs = hfs;
   }
-
-  /*
-   * Go open new scanners and cue them at <code>firstRow</code>.
-   * Closes existing Readers if any.
-   * @param firstRow
-   * @throws IOException
-   */
-  private void openScanner(final byte [] firstRow) throws IOException {
-    List<HFileScanner> s =
-      new ArrayList<HFileScanner>(this.store.getStorefiles().size());
-    Map<Long, StoreFile> map = this.store.getStorefiles().descendingMap();
-    for (StoreFile f: map.values()) {
-       s.add(f.getReader().getScanner());
-    }
-    this.scanners = s.toArray(new HFileScanner [] {});
-    this.keys = new KeyValue[this.scanners.length];
-    // Advance the readers to the first pos.
-    KeyValue firstKey = (firstRow != null && firstRow.length > 0)?
-      new KeyValue(firstRow, HConstants.LATEST_TIMESTAMP): null;
-    for (int i = 0; i < this.scanners.length; i++) {
-      if (firstKey != null) {
-        if (seekTo(i, firstKey)) {
-          continue;
-        }
-      }
-      while (getNext(i)) {
-        if (columnMatch(i)) {
-          break;
-        }
-      }
-    }
+  
+  public KeyValue peek() {
+    return cur;
   }
-
-  /**
-   * For a particular column i, find all the matchers defined for the column.
-   * Compare the column family and column key using the matchers. The first one
-   * that matches returns true. If no matchers are successful, return false.
-   * 
-   * @param i index into the keys array
-   * @return true if any of the matchers for the column match the column family
-   * and the column key.
-   * @throws IOException
-   */
-  boolean columnMatch(int i) throws IOException {
-    return columnMatch(keys[i]);
-  }
-
-  /**
-   * Get the next set of values for this scanner.
-   * 
-   * @param key The key that matched
-   * @param results All the results for <code>key</code>
-   * @return true if a match was found
-   * @throws IOException
-   * 
-   * @see org.apache.hadoop.hbase.regionserver.InternalScanner#next(org.apache.hadoop.hbase.HStoreKey, java.util.SortedMap)
-   */
-  @Override
-  public boolean next(List<KeyValue> results)
-  throws IOException {
-    if (this.scannerClosed) {
-      return false;
+  
+  public KeyValue next() {
+    KeyValue retKey = cur;
+    cur = hfs.getKeyValue();
+    try {
+      hfs.next();
+    } catch(IOException e) {
+      // Only occurs if the scanner is not seeked, this is never the case
+      // as we seek immediately after construction in StoreScanner
     }
-    this.lock.readLock().lock();
+    return retKey;
+  }
+  
+  public boolean seek(KeyValue key) {
     try {
-      // Find the next viable row label (and timestamp).
-      KeyValue viable = getNextViableRow();
-      if (viable == null) {
+      if(!seekAtOrAfter(hfs, key)) {
+        close();
         return false;
       }
-
-      // Grab all the values that match this row/timestamp
-      boolean addedItem = false;
-      for (int i = 0; i < keys.length; i++) {
-        // Fetch the data
-        while ((keys[i] != null) &&
-            (this.store.comparator.compareRows(this.keys[i], viable) == 0)) {
-          // If we are doing a wild card match or there are multiple matchers
-          // per column, we need to scan all the older versions of this row
-          // to pick up the rest of the family members
-          if(!isWildcardScanner()
-              && !isMultipleMatchScanner()
-              && (keys[i].getTimestamp() != viable.getTimestamp())) {
-            break;
-          }
-          if (columnMatch(i)) {
-            // We only want the first result for any specific family member
-            // TODO: Do we have to keep a running list of column entries in
-            // the results across all of the StoreScanner?  Like we do
-            // doing getFull?
-            if (!results.contains(keys[i])) {
-              results.add(keys[i]);
-              addedItem = true;
-            }
-          }
-
-          if (!getNext(i)) {
-            closeSubScanner(i);
-          }
-        }
-        // Advance the current scanner beyond the chosen row, to
-        // a valid timestamp, so we're ready next time.
-        while ((keys[i] != null) &&
-            ((this.store.comparator.compareRows(this.keys[i], viable) <= 0) ||
-                (keys[i].getTimestamp() > this.timestamp) ||
-                !columnMatch(i))) {
-          getNext(i);
-        }
-      }
-      return addedItem;
-    } finally {
-      this.lock.readLock().unlock();
+      cur = hfs.getKeyValue();
+      hfs.next();
+      return true;
+    } catch(IOException ioe) {
+      close();
+      return false;
     }
   }
-
-  /*
-   * @return An instance of <code>ViableRow</code>
-   * @throws IOException
-   */
-  private KeyValue getNextViableRow() throws IOException {
-    // Find the next viable row label (and timestamp).
-    KeyValue viable = null;
-    long viableTimestamp = -1;
-    long ttl = store.ttl;
-    for (int i = 0; i < keys.length; i++) {
-      // The first key that we find that matches may have a timestamp greater
-      // than the one we're looking for. We have to advance to see if there
-      // is an older version present, since timestamps are sorted descending
-      while (keys[i] != null &&
-          keys[i].getTimestamp() > this.timestamp &&
-          columnMatch(i) &&
-          getNext(i)) {
-        if (columnMatch(i)) {
-          break;
-        }
-      }
-      if((keys[i] != null)
-          // If we get here and keys[i] is not null, we already know that the
-          // column matches and the timestamp of the row is less than or equal
-          // to this.timestamp, so we do not need to test that here
-          && ((viable == null) ||
-            (this.store.comparator.compareRows(this.keys[i], viable) < 0) ||
-            ((this.store.comparator.compareRows(this.keys[i], viable) == 0) &&
-              (keys[i].getTimestamp() > viableTimestamp)))) {
-        if (ttl == HConstants.FOREVER || now < keys[i].getTimestamp() + ttl) {
-          viable = keys[i];
-        } else {
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("getNextViableRow :" + keys[i] + ": expired, skipped");
-          }
-        }
-      }
-    }
-    return viable;
-  }
-
-  /*
-   * The user didn't want to start scanning at the first row. This method
-   * seeks to the requested row.
-   *
-   * @param i which iterator to advance
-   * @param firstRow seek to this row
-   * @return true if we found the first row and so the scanner is properly
-   * primed or true if the row was not found and this scanner is exhausted.
-   */
-  private boolean seekTo(int i, final KeyValue firstKey)
-  throws IOException {
-    if (firstKey == null) {
-      if (!this.scanners[i].seekTo()) {
-        closeSubScanner(i);
-        return true;
-      }
-    } else {
-      // TODO: sort columns and pass in column as part of key so we get closer.
-      if (!Store.getClosest(this.scanners[i], firstKey)) {
-        closeSubScanner(i);
-        return true;
-      }
-    }
-    this.keys[i] = this.scanners[i].getKeyValue();
-    return isGoodKey(this.keys[i]);
-  }
-
-  /**
-   * Get the next value from the specified reader.
-   * 
-   * @param i which reader to fetch next value from
-   * @return true if there is more data available
-   */
-  private boolean getNext(int i) throws IOException {
-    boolean result = false;
-    while (true) {
-      if ((this.scanners[i].isSeeked() && !this.scanners[i].next()) ||
-          (!this.scanners[i].isSeeked() && !this.scanners[i].seekTo())) {
-        closeSubScanner(i);
-        break;
-      }
-      this.keys[i] = this.scanners[i].getKeyValue();
-      if (isGoodKey(this.keys[i])) {
-          result = true;
-          break;
-      }
-    }
-    return result;
-  }
-
-  /*
-   * @param kv
-   * @return True if good key candidate.
-   */
-  private boolean isGoodKey(final KeyValue kv) {
-    return !Store.isExpired(kv, this.store.ttl, this.now);
-  }
-
-  /** Close down the indicated reader. */
-  private void closeSubScanner(int i) {
-    this.scanners[i] = null;
-    this.keys[i] = null;
-  }
-
-  /** Shut it down! */
+  
   public void close() {
-    if (!this.scannerClosed) {
-      this.store.deleteChangedReaderObserver(this);
-      try {
-        for(int i = 0; i < this.scanners.length; i++) {
-          closeSubScanner(i);
-        }
-      } finally {
-        this.scannerClosed = true;
-      }
-    }
+    // Nothing to close on HFileScanner?
+    cur = null;
   }
-
-  // Implementation of ChangedReadersObserver
   
-  public void updateReaders() throws IOException {
-    this.lock.writeLock().lock();
-    try {
-      // The keys are currently lined up at the next row to fetch.  Pass in
-      // the current row as 'first' row and readers will be opened and cue'd
-      // up so future call to next will start here.
-      KeyValue viable = getNextViableRow();
-      openScanner(viable.getRow());
-      LOG.debug("Replaced Scanner Readers at row " +
-        viable.getRow().toString());
-    } finally {
-      this.lock.writeLock().unlock();
+  public static boolean seekAtOrAfter(HFileScanner s, KeyValue k)
+  throws IOException {
+    int result = s.seekTo(k.getBuffer(), k.getKeyOffset(), k.getKeyLength());
+    if(result < 0) {
+      // Passed KV is smaller than first KV in file, work from start of file
+      return s.seekTo();
+    } else if(result > 0) {
+      // Passed KV is larger than current KV in file, if there is a next
+      // it is the "after", if not then this scanner is done.
+      return s.next();
     }
+    // Seeked to the exact key
+    return true;
   }
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -25,288 +25,127 @@
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
-import java.util.TreeSet;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 
 /**
  * Scanner scans both the memcache and the HStore
  */
-class StoreScanner implements InternalScanner,  ChangedReadersObserver {
+class StoreScanner implements KeyValueScanner, InternalScanner,
+ChangedReadersObserver {
   static final Log LOG = LogFactory.getLog(StoreScanner.class);
-
-  private InternalScanner [] scanners;
-  private List<KeyValue> [] resultSets;
-  private boolean wildcardMatch = false;
-  private boolean multipleMatchers = false;
-  private RowFilterInterface dataFilter;
+  
   private Store store;
-  private final long timestamp;
-  private final NavigableSet<byte []> columns;
   
-  // Indices for memcache scanner and hstorefile scanner.
-  private static final int MEMS_INDEX = 0;
-  private static final int HSFS_INDEX = MEMS_INDEX + 1;
+  private QueryMatcher matcher;
   
+  private KeyValueHeap heap;
+
   // Used around transition from no storefile to the first.
   private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
-
+  
   // Used to indicate that the scanner has closed (see HBASE-1107)
   private final AtomicBoolean closing = new AtomicBoolean(false);
-
-  /** Create an Scanner with a handle on the memcache and HStore files. */
-  @SuppressWarnings("unchecked")
-  StoreScanner(Store store, final NavigableSet<byte []> targetCols,
-    byte [] firstRow, long timestamp, RowFilterInterface filter) 
+  
+  /**
+   * Opens a scanner across memcache, snapshot, and all StoreFiles.
+   */
+  StoreScanner(Store store, Scan scan, final NavigableSet<byte[]> columns,
+      byte [] firstRow)
   throws IOException {
     this.store = store;
-    this.dataFilter = filter;
-    if (null != dataFilter) {
-      dataFilter.reset();
+    matcher = new QueryMatcher(scan, firstRow, store.getFamily().getName(),
+        columns, store.ttl, store.comparator.getRawComparator());
+
+    List<KeyValueScanner> scanners = getStoreFileScanners();
+    scanners.add(store.memcache.getScanner());
+
+    // Seek all scanners to the initial key
+    for(KeyValueScanner scanner : scanners) {
+      scanner.seek(matcher.getStartKey());
     }
-    this.scanners = new InternalScanner[2];
-    this.resultSets = new List[scanners.length];
-    // Save these args in case we need them later handling change in readers
-    // See updateReaders below.
-    this.timestamp = timestamp;
-    this.columns = targetCols;
-    try {
-      scanners[MEMS_INDEX] =
-        store.memcache.getScanner(timestamp, targetCols, firstRow);
-      scanners[HSFS_INDEX] =
-        new StoreFileScanner(store, timestamp, targetCols, firstRow);
-      for (int i = MEMS_INDEX; i < scanners.length; i++) {
-        checkScannerFlags(i);
-      }
-    } catch (IOException e) {
-      doClose();
-      throw e;
-    }
     
-    // Advance to the first key in each scanner.
-    // All results will match the required column-set and scanTime.
-    for (int i = MEMS_INDEX; i < scanners.length; i++) {
-      setupScanner(i);
-    }
-    this.store.addChangedReaderObserver(this);
+    // Combine all seeked scanners with a heap
+    heap = new KeyValueHeap(
+      scanners.toArray(new KeyValueScanner[scanners.size()]), store.comparator);
   }
   
-  /*
-   * @param i Index.
-   */
-  private void checkScannerFlags(final int i) {
-    if (this.scanners[i].isWildcardScanner()) {
-      this.wildcardMatch = true;
-    }
-    if (this.scanners[i].isMultipleMatchScanner()) {
-      this.multipleMatchers = true;
-    }
+  public KeyValue peek() {
+    return this.heap.peek();
   }
   
-  /*
-   * Do scanner setup.
-   * @param i
-   * @throws IOException
-   */
-  private void setupScanner(final int i) throws IOException {
-    this.resultSets[i] = new ArrayList<KeyValue>();
-    if (this.scanners[i] != null && !this.scanners[i].next(this.resultSets[i])) {
-      closeScanner(i);
-    }
+  public KeyValue next() {
+    return this.heap.next();
   }
-
-  /** @return true if the scanner is a wild card scanner */
-  public boolean isWildcardScanner() {
-    return this.wildcardMatch;
+  
+  public void close() {
+    this.closing.set(true);
+    this.store.deleteChangedReaderObserver(this);
+    this.heap.close();
   }
+  
+  public boolean seek(KeyValue key) {
 
-  /** @return true if the scanner is a multiple match scanner */
-  public boolean isMultipleMatchScanner() {
-    return this.multipleMatchers;
+    return this.heap.seek(key);
   }
-
-  public boolean next(List<KeyValue> results)
-  throws IOException {
-    this.lock.readLock().lock();
-    try {
-    // Filtered flag is set by filters.  If a cell has been 'filtered out'
-    // -- i.e. it is not to be returned to the caller -- the flag is 'true'.
-    boolean filtered = true;
-    boolean moreToFollow = true;
-    while (filtered && moreToFollow) {
-      // Find the lowest-possible key.
-      KeyValue chosen = null;
-      long chosenTimestamp = -1;
-      for (int i = 0; i < this.scanners.length; i++) {
-        KeyValue kv = this.resultSets[i] == null || this.resultSets[i].isEmpty()?
-          null: this.resultSets[i].get(0);
-        if (kv == null) {
+  
+  /**
+   * Get the next row of values from this Store.
+   * @param result
+   * @return true if there are more rows, false if scanner is done
+   */
+  public boolean next(List<KeyValue> result) throws IOException {
+    matcher.setRow(this.heap.peek().getRow());
+    KeyValue kv;
+    while((kv = this.heap.peek()) != null) {
+      switch(matcher.match(kv)) {
+        case INCLUDE:
+          result.add(this.heap.next());
           continue;
-        }
-        if (scanners[i] != null &&
-            (chosen == null ||
-              (this.store.comparator.compareRows(kv, chosen) < 0) ||
-              ((this.store.comparator.compareRows(kv, chosen) == 0) &&
-              (kv.getTimestamp() > chosenTimestamp)))) {
-          chosen = kv;
-          chosenTimestamp = chosen.getTimestamp();
-        }
+        case SKIP:
+          continue;
+        case NEXT:
+        case DONE:
+          return true;
       }
-
-      // Filter whole row by row key?
-      filtered = dataFilter == null || chosen == null? false:
-        dataFilter.filterRowKey(chosen.getBuffer(), chosen.getRowOffset(),
-          chosen.getRowLength());
-
-      // Store results for each sub-scanner.
-      if (chosenTimestamp >= 0 && !filtered) {
-        NavigableSet<KeyValue> deletes =
-          new TreeSet<KeyValue>(this.store.comparatorIgnoringType);
-        for (int i = 0; i < scanners.length && !filtered; i++) {
-          if ((scanners[i] != null && !filtered && moreToFollow &&
-              this.resultSets[i] != null && !this.resultSets[i].isEmpty())) {
-            // Test this resultset is for the 'chosen' row.
-            KeyValue firstkv = resultSets[i].get(0);
-            if (!this.store.comparator.matchingRows(firstkv, chosen)) {
-              continue;
-            }
-            // Its for the 'chosen' row, work it.
-            for (KeyValue kv: resultSets[i]) {
-              if (kv.isDeleteType()) {
-                deletes.add(kv);
-              } else if ((deletes.isEmpty() || !deletes.contains(kv)) &&
-                  !filtered && moreToFollow && !results.contains(kv)) {
-                if (this.dataFilter != null) {
-                  // Filter whole row by column data?
-                  int rowlength = kv.getRowLength();
-                  int columnoffset = kv.getColumnOffset(rowlength);
-                  filtered = dataFilter.filterColumn(kv.getBuffer(),
-                      kv.getRowOffset(), rowlength,
-                    kv.getBuffer(), columnoffset, kv.getColumnLength(columnoffset),
-                    kv.getBuffer(), kv.getValueOffset(), kv.getValueLength());
-                  if (filtered) {
-                    results.clear();
-                    break;
-                  }
-                }
-                results.add(kv);
-                /* REMOVING BECAUSE COULD BE BUNCH OF DELETES IN RESULTS
-                   AND WE WANT TO INCLUDE THEM -- below short-circuit is
-                   probably not wanted.
-                // If we are doing a wild card match or there are multiple
-                // matchers per column, we need to scan all the older versions of 
-                // this row to pick up the rest of the family members
-                if (!wildcardMatch && !multipleMatchers &&
-                    (kv.getTimestamp() != chosenTimestamp)) {
-                  break;
-                }
-                */
-              }
-            }
-            // Move on to next row.
-            resultSets[i].clear();
-            if (!scanners[i].next(resultSets[i])) {
-              closeScanner(i);
-            }
-          }
-        }
-      }
-
-      moreToFollow = chosenTimestamp >= 0;
-      if (dataFilter != null) {
-        if (dataFilter.filterAllRemaining()) {
-          moreToFollow = false;
-        }
-      }
-
-      if (results.isEmpty() && !filtered) {
-        // There were no results found for this row.  Marked it as 
-        // 'filtered'-out otherwise we will not move on to the next row.
-        filtered = true;
-      }
     }
-    
-    // If we got no results, then there is no more to follow.
-    if (results == null || results.isEmpty()) {
-      moreToFollow = false;
-    }
-    
-    // Make sure scanners closed if no more results
-    if (!moreToFollow) {
-      for (int i = 0; i < scanners.length; i++) {
-        if (null != scanners[i]) {
-          closeScanner(i);
-        }
-      }
-    }
-    
-    return moreToFollow;
-    } finally {
-      this.lock.readLock().unlock();
-    }
+    // No more keys
+    close();
+    return false;
   }
-
-  /** Shut down a single scanner */
-  void closeScanner(int i) {
-    try {
-      try {
-        scanners[i].close();
-      } catch (IOException e) {
-        LOG.warn(Bytes.toString(store.storeName) + " failed closing scanner " +
-          i, e);
-      }
-    } finally {
-      scanners[i] = null;
-      resultSets[i] = null;
-    }
-  }
-
-  public void close() {
-    this.closing.set(true);
-    this.store.deleteChangedReaderObserver(this);
-    doClose();
-  }
   
-  private void doClose() {
-    for (int i = MEMS_INDEX; i < scanners.length; i++) {
-      if (scanners[i] != null) {
-        closeScanner(i);
-      }
+  private List<KeyValueScanner> getStoreFileScanners() {
+    List<HFileScanner> s =
+      new ArrayList<HFileScanner>(this.store.getStorefilesCount());
+    Map<Long, StoreFile> map = this.store.getStorefiles().descendingMap();
+    for(StoreFile sf : map.values()) {
+      s.add(sf.getReader().getScanner());
     }
+    List<KeyValueScanner> scanners =
+      new ArrayList<KeyValueScanner>(s.size()+1);
+    for(HFileScanner hfs : s) {
+      scanners.add(new StoreFileScanner(hfs));
+    }
+    return scanners;
   }
-  
+   
   // Implementation of ChangedReadersObserver
-  
   public void updateReaders() throws IOException {
     if (this.closing.get()) {
       return;
     }
     this.lock.writeLock().lock();
     try {
-      Map<Long, StoreFile> map = this.store.getStorefiles();
-      if (this.scanners[HSFS_INDEX] == null && map != null && map.size() > 0) {
-        // Presume that we went from no readers to at least one -- need to put
-        // a HStoreScanner in place.
-        try {
-          // I think its safe getting key from mem at this stage -- it shouldn't have
-          // been flushed yet
-          // TODO: MAKE SURE WE UPDATE FROM TRUNNK.
-          this.scanners[HSFS_INDEX] = new StoreFileScanner(this.store,
-              this.timestamp, this. columns, this.resultSets[MEMS_INDEX].get(0).getRow());
-          checkScannerFlags(HSFS_INDEX);
-          setupScanner(HSFS_INDEX);
-          LOG.debug("Added a StoreFileScanner to outstanding HStoreScanner");
-        } catch (IOException e) {
-          doClose();
-          throw e;
-        }
-      }
+      // Could do this pretty nicely with KeyValueHeap, but the existing
+      // implementation of this method only updated if no existing storefiles?
+      // Lets discuss.
+      return;
     } finally {
       this.lock.writeLock().unlock();
     }
Index: src/java/org/apache/hadoop/hbase/regionserver/DeleteCompare.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/DeleteCompare.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/DeleteCompare.java	(revision 0)
@@ -0,0 +1,115 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+/**
+ * Class that provides static method needed when putting deletes into memcache 
+ */
+public class DeleteCompare {
+  
+  /**
+   * Return codes from deleteCompare.
+   */
+  enum DeleteCode {
+    /**
+     * Do nothing.  Move to next KV in Memcache
+     */
+    SKIP,
+    
+    /**
+     * Add to the list of deletes.
+     */
+    DELETE,
+    
+    /**
+     * Stop looking at KVs in Memcache.  Finalize.
+     */
+    DONE
+  }
+
+  /**
+   * Method used when putting deletes into memcache to remove all the previous
+   * entries that are affected by this Delete
+   * @param mem
+   * @param deleteBuffer
+   * @param deleteRowOffset
+   * @param deleteRowLength
+   * @param deleteQualifierOffset
+   * @param deleteQualifierLength
+   * @param deleteTimeOffset
+   * @param deleteType
+   * @return SKIP if current KeyValue should not be deleted, DELETE if
+   * current KeyValue should be deleted and DONE when the current KeyValue is
+   * out of the Deletes range
+   */
+  public static DeleteCode deleteCompare(KeyValue mem, byte [] deleteBuffer,
+      int deleteRowOffset, short deleteRowLength, int deleteQualifierOffset,
+      int deleteQualifierLength, int deleteTimeOffset, byte deleteType) {
+
+    //Parsing new KeyValue
+    byte [] memBuffer = mem.getBuffer();
+    int memOffset = mem.getOffset();
+
+    //Getting key lengths
+    int memKeyLen = Bytes.toInt(memBuffer, memOffset);
+    memOffset = Bytes.SIZEOF_INT;
+
+    //Skipping value lengths
+    memOffset += Bytes.SIZEOF_INT;
+
+    //Getting row lengths
+    short memRowLen = Bytes.toShort(memBuffer, memOffset);
+    memOffset += Bytes.SIZEOF_SHORT;
+    int res = Bytes.compareTo(memBuffer, memOffset, memRowLen,
+        deleteBuffer, deleteRowOffset, deleteRowLength);
+    if(res > 0) {
+      return DeleteCode.DONE;
+    } else if(res < 0){
+      return DeleteCode.SKIP;
+    }
+
+    memOffset += memRowLen;
+
+    //Getting family lengths
+    byte memFamLen = memBuffer[memOffset];
+    memOffset += Bytes.SIZEOF_BYTE + memFamLen;
+
+    //Get column lengths
+    int memQualifierLen = memKeyLen - memRowLen - memFamLen -
+      Bytes.SIZEOF_SHORT - Bytes.SIZEOF_BYTE - Bytes.SIZEOF_LONG -
+      Bytes.SIZEOF_BYTE;
+
+    //Compare timestamp
+    int tsOffset = memOffset + memQualifierLen;
+    int timeRes = Bytes.compareTo(memBuffer, tsOffset, Bytes.SIZEOF_LONG,
+        deleteBuffer, deleteTimeOffset, Bytes.SIZEOF_LONG);
+
+    if(deleteType == KeyValue.Type.DeleteFamily.getCode()){
+      if(timeRes <= 0){
+        return DeleteCode.DELETE;
+      }
+      return DeleteCode.SKIP;
+    }
+
+    //Compare columns
+    res = Bytes.compareTo(memBuffer, memOffset, memQualifierLen,
+        deleteBuffer, deleteQualifierOffset, deleteQualifierLength);
+    if(res < 0) {
+      return DeleteCode.SKIP;
+    } else if(res > 0) {
+      return DeleteCode.DONE;
+    }
+    if(timeRes == 0) {
+      return DeleteCode.DELETE;
+    } else if(timeRes < 0){
+      if(deleteType == KeyValue.Type.DeleteColumn.getCode()) {
+        return DeleteCode.DELETE;
+      }
+      return DeleteCode.DONE;
+    } else {
+      return DeleteCode.SKIP;
+    }
+  } 
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java	(revision 0)
@@ -0,0 +1,348 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.NavigableSet;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Scan;
+import org.apache.hadoop.hbase.io.TimeRange;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This is the primary class used to process KeyValues during a Get or Scan
+ * operation.
+ * <p>
+ * It encapsulates the handling of the column and version input parameters to 
+ * the query through a {@link ColumnTracker}.
+ * <p>
+ * Deletes are handled using the {@link DeleteTracker}.
+ * <p>
+ * All other query parameters are accessed from the client-specified Get.
+ * <p>
+ * The primary method used is {@link match} with the current KeyValue.  It will
+ * return a {@link MatchCode} 
+ * 
+ * , deletes,
+ * versions, 
+ */
+public class QueryMatcher {
+  
+  /**
+   * {@link match} return codes.  These instruct the scanner moving through
+   * Memcaches and StoreFiles what to do with the current KeyValue.
+   * <p>
+   * Additionally, this contains "early-out" language to tell the scanner to
+   * move on to the next File (Memcache or Storefile), or to return immediately.
+   */
+  static enum MatchCode { 
+    /**
+     * Include KeyValue in the returned result
+     */
+    INCLUDE,
+    
+    /**
+     * Do not include KeyValue in the returned result
+     */
+    SKIP,
+    
+    /**
+     * Do not include, jump to next StoreFile or Memcache (in time order)
+     */
+    NEXT,
+    
+    /**
+     * Do not include, return current result
+     */
+    DONE
+  }
+  
+  /** Keeps track of deletes */
+  private DeleteTracker deletes;
+  
+  /** Keeps track of columns and versions */
+  private ColumnTracker columns;
+  
+  /** Key to seek to in Memcache and StoreFiles */
+  private KeyValue startKey;
+  
+  /** Row comparator for the region this query is for */
+  private KeyComparator rowComparator;
+  
+  /** Row the query is on */
+  private byte [] row;
+  
+  /** TimeRange the query is for */
+  private TimeRange tr;
+  
+  /** Oldest allowed version stamp for TTL enforcement */
+  private long oldestStamp;
+  
+  /**
+   * Constructs a QueryMatcher for a Get.
+   * @param get
+   * @param row
+   * @param family
+   * @param columns
+   * @param ttl
+   * @param rowComparator
+   */
+  public QueryMatcher(Get get, byte [] row, byte [] family, 
+      NavigableSet<byte[]> columns, long ttl, KeyComparator rowComparator) {
+    this.row = row;
+    this.tr = get.getTimeRange();
+    this.oldestStamp = System.currentTimeMillis() - ttl;
+    this.rowComparator = rowComparator;
+    this.deletes =  new GetDeleteTracker();
+    this.startKey = KeyValue.createFirstOnRow(row);
+    // Single branch to deal with two types of Gets (columns vs all in family)
+    if(columns == null || columns.size() == 0) {
+      this.columns = new WildcardColumnTracker(get.getMaxVersions());
+    } else {
+      this.columns = new ExplicitColumnTracker(columns,get.getMaxVersions());
+    }
+  }
+  
+  /**
+   * Constructs a QueryMatcher for a Scan.
+   * @param scan 
+   * @param row
+   * @param family
+   * @param columns
+   * @param ttl
+   * @param rowComparator
+   */
+  public QueryMatcher(Scan scan, byte [] row, byte [] family, 
+      NavigableSet<byte[]> columns, long ttl, KeyComparator rowComparator) {
+    this.row = row;
+    this.tr = scan.getTimeRange();
+    this.oldestStamp = System.currentTimeMillis() - ttl;
+    this.rowComparator = rowComparator;
+    this.deletes =  new GetDeleteTracker();
+    this.startKey = KeyValue.createFirstOnRow(row);
+    // Single branch to deal with two types of reads (columns vs all in family)
+    if(columns == null || columns.size() == 0) {
+      this.columns = new WildcardColumnTracker(scan.getMaxVersions());
+    } else {
+      this.columns = new ExplicitColumnTracker(columns,scan.getMaxVersions());
+    }
+  }
+  
+  /**
+   * Constructs a copy of an existing QueryMatcher with a new row.
+   * @param matcher
+   * @param row
+   */
+  public QueryMatcher(QueryMatcher matcher, byte [] row) {
+    this.row = row;
+    this.tr = matcher.getTimeRange();
+    this.oldestStamp = matcher.getOldestStamp();
+    this.rowComparator = matcher.getRowComparator();
+    this.columns = matcher.getColumnTracker();
+    this.deletes = matcher.getDeleteTracker();
+    this.startKey = matcher.getStartKey();
+    reset();
+  }
+  
+  /**
+   * Main method for ColumnMatcher.
+   * <p>
+   * Determines whether the specified KeyValue should be included in the
+   * result or not.
+   * <p>
+   * Contains additional language to early-out of the current file or to
+   * return immediately.
+   * <p>
+   * Things to be checked:<ul>
+   * <li>Row
+   * <li>TTL
+   * <li>Type
+   * <li>TimeRange
+   * <li>Deletes
+   * <li>Column
+   * <li>Versions
+   * @param kv KeyValue to check
+   * @return MatchCode: include, skip, next, done
+   */
+  public MatchCode match(KeyValue kv) throws IOException {
+    if(this.columns.done()) {
+      return MatchCode.DONE;
+    }
+    
+    // Directly act on KV buffer
+    byte [] bytes = kv.getBuffer();
+    int offset = kv.getOffset();
+    
+    int keyLength = Bytes.toInt(bytes, offset);
+    offset += KeyValue.ROW_OFFSET;
+    
+    short rowLength = Bytes.toShort(bytes, offset);
+    offset += Bytes.SIZEOF_SHORT;
+    
+    /* Check ROW
+     * If past query's row, go to next StoreFile
+     * If not reached query's row, go to next KeyValue
+     */ 
+//  int ret = Bytes.compareTo(row, 0, row.length, bytes, offset, rowLength);
+    int ret = this.rowComparator.compareRows(row, 0, row.length,
+        bytes, offset, rowLength);
+    if(ret <= -1) {
+      // Have reached the next row
+      return MatchCode.NEXT;
+    } else if(ret >= 1) {
+      // At a previous row
+      return MatchCode.SKIP;
+    }
+    offset += rowLength;
+    
+    byte familyLength = bytes[offset];
+    offset += Bytes.SIZEOF_BYTE + familyLength;
+    
+    int columnLength = keyLength + KeyValue.ROW_OFFSET -
+      (offset - kv.getOffset()) - KeyValue.TIMESTAMP_TYPE_SIZE;
+    int columnOffset = offset;
+    offset += columnLength;
+    
+    /* Check TTL
+     * If expired, go to next KeyValue
+     */
+    long timestamp = Bytes.toLong(bytes, offset);
+    if(isExpired(timestamp)) {
+      return MatchCode.NEXT;
+    }
+    offset += Bytes.SIZEOF_LONG;
+    
+    /* Check TYPE
+     * If a delete within (or after) time range, add to deletes
+     * Move to next KeyValue
+     */
+    byte type = bytes[offset];
+    if(isDelete(type)) {
+      if(tr.withinOrAfterTimeRange(timestamp)) {
+        this.deletes.add(bytes, columnOffset, columnLength, timestamp, type);
+      }
+      return MatchCode.SKIP;
+    }
+    
+    /* Check TimeRange
+     * If outside of range, move to next KeyValue
+     */
+    if(!tr.withinTimeRange(timestamp)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Deletes
+     * If deleted, move to next KeyValue 
+     */
+    if(!deletes.isEmpty() && deletes.isDeleted(bytes, columnOffset,
+        columnLength, timestamp)) {
+      return MatchCode.SKIP;
+    }
+    
+    /* Check Column and Versions
+     * Returns a MatchCode directly, identical language
+     * If matched column without enough versions, include
+     * If enough versions of this column or does not match, skip
+     * If have moved past 
+     * If enough versions of everything, 
+     */
+    return columns.checkColumn(bytes, columnOffset, columnLength);
+  }
+  
+  private boolean isDelete(byte type) {
+    return (type != KeyValue.Type.Put.getCode());
+  }
+  
+  private boolean isExpired(long timestamp) {
+    return (timestamp < oldestStamp);
+  }
+
+  /**
+   * Called after reading each section (memcache, snapshot, storefiles).
+   * <p>
+   * This method will update the internal structures to be accurate for
+   * the next section. 
+   */
+  public void update() {
+    this.deletes.update();
+    this.columns.update();
+  }
+  
+  /**
+   * Resets the current columns and deletes
+   */
+  public void reset() {
+    this.deletes.reset();
+    this.columns.reset();
+  }
+  
+  /**
+   * Set current row
+   * @param row
+   */
+  public void setRow(byte [] row) {
+    this.row = row;
+  }
+  
+  /**
+   * 
+   * @return the start key
+   */
+  public KeyValue getStartKey() {
+    return this.startKey;
+  }
+  
+  /**
+   * @return the TimeRange
+   */
+  public TimeRange getTimeRange() {
+    return this.tr;
+  }
+  
+  /**
+   * @return the oldest stamp
+   */
+  public long getOldestStamp() {
+    return this.oldestStamp;
+  }
+  
+  /**
+   * @return current KeyComparator
+   */
+  public KeyComparator getRowComparator() {
+    return this.rowComparator;
+  }
+  
+  /**
+   * @return ColumnTracker
+   */
+  public ColumnTracker getColumnTracker() {
+    return this.columns;
+  }
+  
+  /**
+   * @return DeleteTracker
+   */
+  public DeleteTracker getDeleteTracker() {
+    return this.deletes;
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java	(revision 0)
@@ -0,0 +1,146 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.NavigableSet;
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is used for the tracking and enforcement of columns and numbers 
+ * of versions during the course of a Get or Scan operation, when explicit
+ * column qualifiers have been asked for in the query.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class ExplicitColumnTracker implements ColumnTracker {
+
+  private int maxVersions;
+  private List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  private NavigableSet<byte[]> origColumns;
+  
+  /**
+   * Default constructor.
+   * @param columns columns specified user in query
+   * @param maxVersions maximum versions to return per column
+   */
+  public ExplicitColumnTracker(NavigableSet<byte[]> columns, int maxVersions) {
+    this.maxVersions = maxVersions;
+    this.origColumns = columns;
+    reset();
+  }
+  
+  /**
+   * Done when there are no more columns to match against.
+   */
+  public boolean done() {
+    return this.columns.size() == 0;
+  }
+  
+  /**
+   * Checks against the parameters of the query and the columns which have
+   * already been processed by this query.
+   * @param bytes KeyValue buffer
+   * @param offset offset to the start of the qualifier
+   * @param length length of the qualifier
+   * @return MatchCode telling QueryMatcher what action to take
+   */
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+    // No more columns left, we are done with this query
+    if(this.columns.size() == 0) {
+      return MatchCode.DONE;
+    }
+    
+    // No more columns to match against, done with storefile
+    if(this.column == null) {
+      return MatchCode.NEXT;
+    }
+    
+    // Compare specific column to current column
+    int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), 
+        column.getLength(), bytes, offset, length);
+    
+    // Matches, decrement versions left and include
+    if(ret == 0) {
+      if(this.column.decrement() == 0) {
+        // Done with versions for this column
+        this.columns.remove(this.index);
+        if(this.columns.size() == this.index) {
+          // Will not hit any more columns in this storefile
+          this.column = null;
+        } else {
+          this.column = this.columns.get(this.index);
+        }
+      }
+      return MatchCode.INCLUDE;
+    }
+
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++this.index == this.columns.size()) {
+        // No more to match, do not include, done with storefile
+        return MatchCode.NEXT;
+      }
+      this.column = this.columns.get(this.index);
+      return checkColumn(bytes, offset, length);
+    }
+
+    // Specified column is smaller than current column
+    // Skip
+    return MatchCode.SKIP;
+  }
+  
+  /**
+   * Called at the end of every StoreFile or Memcache.
+   */
+  public void update() {
+    if(this.columns.size() != 0) {
+      this.index = 0;
+      this.column = this.columns.get(this.index);
+    } else {
+      this.index = -1;
+      this.column = null;
+    }
+  }
+  
+  public void reset() {
+    buildColumnList(this.origColumns);
+    this.index = 0;
+    this.column = this.columns.get(this.index);
+  }
+
+  private void buildColumnList(NavigableSet<byte[]> columns) {
+    this.columns = new ArrayList<ColumnCount>(columns.size());
+    for(byte [] column : columns) {
+      this.columns.add(new ColumnCount(column,maxVersions));
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java	(revision 0)
@@ -0,0 +1,378 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is responsible for the tracking and enforcement of Deletes
+ * during the course of a Get operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link update} when reaching the end of a StoreFile
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class GetDeleteTracker implements DeleteTracker {
+
+  private long familyStamp = -1L;
+  protected List<Delete> deletes = null;
+  private List<Delete> newDeletes = new ArrayList<Delete>();
+  private Iterator<Delete> iterator;
+  private Delete delete = null;
+
+  /**
+   * Constructor
+   */
+  public GetDeleteTracker() {}
+
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param kv
+   * @param type
+   * @param timestamp
+   */
+  @Override
+  public void add(byte [] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type) {
+    if(type == KeyValue.Type.DeleteFamily.getCode()) {
+      if(timestamp > familyStamp) {
+        familyStamp = timestamp;
+      }
+      return;
+    }
+    if(timestamp > familyStamp) {
+      this.newDeletes.add(new Delete(buffer, qualifierOffset, qualifierLength,
+          type, timestamp));
+    }
+  }
+
+  /** 
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+  @Override
+  public boolean isDeleted(byte [] buffer, int qualifierOffset,
+      int qualifierLength, long timestamp) {
+
+    // Check against DeleteFamily
+    if(timestamp <= familyStamp) {
+      return true;
+    }
+
+    // Check if there are other deletes
+    if(this.delete == null) {
+      return false;
+    }
+
+    // Check column
+    int ret = Bytes.compareTo(buffer, qualifierOffset, qualifierLength, 
+        this.delete.buffer, this.delete.qualifierOffset, 
+        this.delete.qualifierLength);
+    if(ret <= -1) {
+      // Have not reached the next delete yet
+      return false;
+    } else if(ret >= 1) {
+      // Deletes an earlier column, need to move down deletes
+      if(this.iterator.hasNext()) {
+        this.delete = this.iterator.next();
+      } else {
+        this.delete = null;
+        return false;
+      }
+      return isDeleted(buffer, qualifierOffset, qualifierLength, timestamp);
+    }
+
+    // Check Timestamp
+    if(timestamp > this.delete.timestamp) {
+      return false;
+    }
+
+    // Check Type
+    switch(KeyValue.Type.codeToType(this.delete.type)) {
+    case Delete:
+      boolean equal = timestamp == this.delete.timestamp;
+
+      if(this.iterator.hasNext()) {
+        this.delete = this.iterator.next();
+      } else {
+        this.delete = null;
+      }
+
+      if(equal){
+        return true;
+      }
+      // timestamp < this.delete.timestamp
+      // Delete of an explicit column newer than current
+      return isDeleted(buffer, qualifierOffset, qualifierLength, timestamp);
+    case DeleteColumn:
+      return true;
+    }
+    // should never reach this
+    return false;
+  }
+
+  @Override
+  public boolean isEmpty() {
+    if(this.familyStamp == 0L && this.delete == null) {
+      return true;
+    }
+    return false;
+  }
+
+  @Override
+  public void reset() {
+    this.deletes = null;
+    this.delete = null;
+    this.newDeletes = new ArrayList<Delete>();
+    this.familyStamp = 0L;
+    this.iterator = null;
+  }
+
+  /**
+   * Called at the end of every StoreFile.
+   * <p>
+   * Many optimized implementations of Trackers will require an update at
+   * when the end of each StoreFile is reached.
+   */
+  @Override
+  public void update() {
+    // If no previous deletes, use new deletes and return
+    if(this.deletes == null || this.deletes.size() == 0) {
+      finalize(this.newDeletes);
+      return;
+    }
+
+    // If no new delete, retain previous deletes and return
+    if(this.newDeletes.size() == 0) {
+      return;
+    }
+
+    // Merge previous deletes with new deletes
+    List<Delete> mergeDeletes = 
+      new ArrayList<Delete>(this.newDeletes.size());
+    int oldIndex = 0;
+    int newIndex = 0;
+
+    Delete newDelete = newDeletes.get(oldIndex);
+    Delete oldDelete = deletes.get(oldIndex);
+    while(true) {
+      switch(compareDeletes(oldDelete,newDelete)) {
+      case NEXT_NEW: {
+        if(++newIndex == newDeletes.size()) {
+          // Done with new, add the rest of old to merged and return
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_NEW_NEXT_NEW: {
+        mergeDeletes.add(newDelete);
+        if(++newIndex == newDeletes.size()) {
+          // Done with new, add the rest of old to merged and return
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_NEW_NEXT_BOTH: {
+        mergeDeletes.add(newDelete);
+        ++oldIndex;
+        ++newIndex;
+        if(oldIndex == deletes.size()) {
+          if(newIndex == newDeletes.size()) {
+            finalize(mergeDeletes);
+            return;
+          }
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        } else if(newIndex == newDeletes.size()) {
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_OLD_NEXT_BOTH: {
+        mergeDeletes.add(oldDelete);
+        ++oldIndex;
+        ++newIndex;
+        if(oldIndex == deletes.size()) {
+          if(newIndex == newDeletes.size()) {
+            finalize(mergeDeletes);
+            return;
+          }
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        } else if(newIndex == newDeletes.size()) {
+          mergeDown(mergeDeletes, deletes, oldIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+        newDelete = this.newDeletes.get(newIndex);
+      }
+
+      case INCLUDE_OLD_NEXT_OLD: {
+        mergeDeletes.add(oldDelete);
+        if(++oldIndex == deletes.size()) {
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+      }
+
+      case NEXT_OLD: {
+        if(++oldIndex == deletes.size()) {
+          // Done with old, add the rest of new to merged and return
+          mergeDown(mergeDeletes, newDeletes, newIndex);
+          finalize(mergeDeletes);
+          return;
+        }
+        oldDelete = this.deletes.get(oldIndex);
+      }
+      }
+    }
+  }
+
+  private void finalize(List<Delete> mergeDeletes) {
+    this.deletes = mergeDeletes;
+    this.newDeletes = new ArrayList<Delete>();
+    if(this.deletes.size() > 0){
+      this.iterator = deletes.iterator();
+      this.delete = iterator.next();
+    }
+  }
+
+  private void mergeDown(List<Delete> mergeDeletes, List<Delete> srcDeletes, 
+      int srcIndex) {
+    while(srcIndex < srcDeletes.size()) {
+      mergeDeletes.add(srcDeletes.get(srcIndex++));
+    }
+  }
+
+
+  protected DeleteCompare compareDeletes(Delete oldDelete, Delete newDelete) {
+
+    // Compare columns
+    int ret = Bytes.compareTo(oldDelete.buffer, oldDelete.qualifierOffset,
+        oldDelete.qualifierLength, newDelete.buffer, newDelete.qualifierOffset,
+        newDelete.qualifierLength);
+
+    if(ret <= -1) {
+      return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+    } else if(ret >= 1) {
+      return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+    }
+
+    // Same column
+
+    // Branches below can be optimized.  Keeping like this until testing
+    // is complete.
+    if(oldDelete.type == newDelete.type) {
+      if(oldDelete.type == KeyValue.Type.Delete.getCode()){
+        if(oldDelete.timestamp > newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+        } else if(oldDelete.timestamp < newDelete.timestamp) {
+          return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+        } else {
+          return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+        }
+      }
+      if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_BOTH;
+      } 
+      return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+    }
+
+    if(oldDelete.type < newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.NEXT_OLD;
+      } else {
+        return DeleteCompare.NEXT_OLD;
+      }
+    }
+
+    if(oldDelete.type > newDelete.type) {
+      if(oldDelete.timestamp > newDelete.timestamp) {
+        return DeleteCompare.NEXT_NEW;
+      } else if(oldDelete.timestamp < newDelete.timestamp) {
+        return DeleteCompare.INCLUDE_NEW_NEXT_NEW;
+      } else {
+        return DeleteCompare.NEXT_NEW;
+      }
+    }
+
+    // Should never reach
+    return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+  }
+
+  /**
+   * Internal class used to store the necessary information for a Delete.
+   * <p>
+   * Rather than reparsing the KeyValue, or copying fields, this class points
+   * to the underlying KeyValue buffer with pointers to the qualifier and fields
+   * for type and timestamp.  No parsing work is done in DeleteTracker now.
+   * <p>
+   * Fields are public because they are accessed often, directly, and only
+   * within this class.
+   */
+  protected class Delete {
+    public byte [] buffer;
+    public int qualifierOffset;
+    public int qualifierLength;
+    public byte type;
+    public long timestamp;
+    public Delete(byte [] buffer, int qualifierOffset, int qualifierLength,
+        byte type, long timestamp) {
+      this.buffer = buffer;
+      this.qualifierOffset = qualifierOffset;
+      this.qualifierLength = qualifierLength;
+      this.type = type;
+      this.timestamp = timestamp;
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/DeleteTracker.java	(revision 0)
@@ -0,0 +1,97 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+/**
+ * This interface is used for the tracking and enforcement of Deletes
+ * during the course of a Get or Scan operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link update} when reaching the end of a StoreFile 
+ */
+public interface DeleteTracker {
+  
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @param type delete type as byte
+   */
+  public void add(byte [] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type);
+  
+  /**
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+  public boolean isDeleted(byte [] buffer, int qualifierOffset,
+      int qualifierLength, long timestamp);
+  
+  /**
+   * @return true if there are no current delete, false otherwise
+   */
+  public boolean isEmpty();
+  
+  /**
+   * Called at the end of every StoreFile.
+   * <p>
+   * Many optimized implementations of Trackers will require an update at
+   * when the end of each StoreFile is reached.
+   */
+  public void update();
+  
+  /**
+   * Called between rows.
+   * <p>
+   * This clears everything as if a new DeleteTracker was instantiated.
+   */
+  public void reset();
+  
+
+  /**
+   * Return codes for comparison of two Deletes.
+   * <p>
+   * The codes tell the merging function what to do.
+   * <p>
+   * INCLUDE means add the specified Delete to the merged list.
+   * NEXT means move to the next element in the specified list(s).
+   */
+  enum DeleteCompare { 
+    INCLUDE_OLD_NEXT_OLD,
+    INCLUDE_OLD_NEXT_BOTH,
+    INCLUDE_NEW_NEXT_NEW,
+    INCLUDE_NEW_NEXT_BOTH,
+    NEXT_OLD,
+    NEXT_NEW
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/WildcardColumnTracker.java	(revision 0)
@@ -0,0 +1,307 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.regionserver.QueryMatcher.MatchCode;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is used for the tracking and enforcement of columns and numbers 
+ * of versions during the course of a Get or Scan operation, when all available
+ * column qualifiers have been asked for in the query.
+ * <p>
+ * This class is utilized by {@link QueryMatcher} through two methods:
+ * <ul><li>{@link checkColumn} is called when a Put satisfies all other
+ * conditions of the query.  This method returns a {@link MatchCode} to define
+ * what action should be taken.
+ * <li>{@link update} is called at the end of every StoreFile or Memcache.
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class WildcardColumnTracker implements ColumnTracker {
+  
+  private int maxVersions;
+  
+  protected List<ColumnCount> columns;
+  private int index;
+  private ColumnCount column;
+  
+  private List<ColumnCount> newColumns; 
+  private int newIndex;
+  private ColumnCount newColumn;
+  
+  /**
+   * Default constructor.
+   * @param maxVersions maximum versions to return per columns
+   */
+  public WildcardColumnTracker(int maxVersions) {
+    this.maxVersions = maxVersions;
+    reset();
+  }
+  
+  public void reset() {
+    this.index = 0;
+    this.column = null;
+    this.columns = null;
+    this.newColumns = new ArrayList<ColumnCount>();
+    this.newIndex = 0;
+    this.newColumn = null;
+  }
+  
+  /**
+   * Can never early-out from reading more storefiles in Wildcard case.
+   */
+  public boolean done() {
+    return false;
+  }
+  
+  /**
+   * Checks against the parameters of the query and the columns which have
+   * already been processed by this query.
+   * @param bytes KeyValue buffer
+   * @param offset offset to the start of the qualifier
+   * @param length length of the qualifier
+   * @return MatchCode telling QueryMatcher what action to take
+   */
+  public MatchCode checkColumn(byte [] bytes, int offset, int length) {
+
+    // Nothing to match against, add to new and include
+    if(this.column == null && this.newColumn == null) {
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing old, compare against new
+    if(this.column == null && this.newColumn != null) {
+      int ret = Bytes.compareTo(newColumn.getBuffer(), newColumn.getOffset(), 
+          newColumn.getLength(), bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(newColumn.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++newIndex == newColumns.size()) {
+          // No more, add to end and include
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          return MatchCode.INCLUDE;
+        }
+        this.newColumn = newColumns.get(newIndex);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(++newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    // Nothing new, compare against old
+    if(this.newColumn == null && this.column != null) {
+      int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(),
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          // No more, add to new and include (new was empty prior to this)
+          newColumns.add(new ColumnCount(bytes, offset, length, 1));
+          this.newColumn = newColumns.get(newIndex);
+          this.column = null;
+          return MatchCode.INCLUDE;
+        }
+        this.column = columns.get(index);
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      this.newColumn = newColumns.get(newIndex);
+      return MatchCode.INCLUDE;
+    }
+    
+    
+    // There are new and old, figure which to check first
+    int ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(), 
+        newColumn.getBuffer(), newColumn.getOffset(), newColumn.getLength());
+        
+    // Old is smaller than new, compare against old
+    if(ret <= -1) {
+      ret = Bytes.compareTo(column.getBuffer(), column.getOffset(), column.getLength(),
+          bytes, offset, length);
+      
+      // Same column
+      if(ret == 0) {
+        if(column.increment() > this.maxVersions) {
+          return MatchCode.SKIP;
+        }
+        return MatchCode.INCLUDE;
+      }
+      
+      // Specified column is bigger than current column
+      // Move down current column and check again
+      if(ret <= -1) {
+        if(++index == columns.size()) {
+          this.column = null;
+        } else {
+          this.column = columns.get(index);
+        }
+        return checkColumn(bytes, offset, length);
+      }
+      
+      // ret >= 1
+      // Specified column is smaller than current column
+      // Nothing to match against, add to new and include
+      newColumns.add(new ColumnCount(bytes, offset, length, 1));
+      return MatchCode.INCLUDE;
+    }
+    
+    // Cannot be equal, so ret >= 1
+    // New is smaller than old, compare against new
+    
+    ret = Bytes.compareTo(newColumn.getBuffer(), newColumn.getOffset(), 
+        newColumn.getLength(), bytes, offset, length);
+    
+    // Same column
+    if(ret == 0) {
+      if(newColumn.increment() > this.maxVersions) {
+        return MatchCode.SKIP;
+      }
+      return MatchCode.INCLUDE;
+    }
+    
+    // Specified column is bigger than current column
+    // Move down current column and check again
+    if(ret <= -1) {
+      if(++newIndex == newColumns.size()) {
+        this.newColumn = null;
+      } else {
+        this.newColumn = newColumns.get(newIndex);
+      }
+      return checkColumn(bytes, offset, length);
+    }
+    
+    // ret >= 1
+    // Specified column is smaller than current column
+    // Nothing to match against, add to new and include
+    newColumns.add(new ColumnCount(bytes, offset, length, 1));
+    return MatchCode.INCLUDE;
+  }
+  
+  /**
+   * Called at the end of every StoreFile or Memcache.
+   */
+  public void update() {
+    // If no previous columns, use new columns and return
+    if(this.columns == null || this.columns.size() == 0) {
+      if(this.newColumns.size() > 0){
+        finalize(newColumns);
+      }
+      return;
+    }
+    
+    // If no new columns, retain previous columns and return
+    if(this.newColumns.size() == 0) {
+      this.index = 0;
+      this.column = this.columns.get(index);
+      return;
+    }
+    
+    // Merge previous columns with new columns
+    // There will be no overlapping
+    List<ColumnCount> mergeColumns = new ArrayList<ColumnCount>(
+        columns.size() + newColumns.size());
+    index = 0;
+    newIndex = 0;
+    column = columns.get(0);
+    newColumn = newColumns.get(0);
+    while(true) {
+      int ret = Bytes.compareTo(
+          column.getBuffer(), column.getOffset(),column.getLength(), 
+          newColumn.getBuffer(), newColumn.getOffset(), newColumn.getLength());
+      
+      // Existing is smaller than new, add existing and iterate it
+      if(ret <= -1) {
+        mergeColumns.add(column);
+        if(++index == columns.size()) {
+          // No more existing left, merge down rest of new and return 
+          mergeDown(mergeColumns, newColumns, newIndex);
+          finalize(mergeColumns);
+          return;
+        }
+        column = columns.get(index);
+        continue;
+      }
+      
+      // New is smaller than existing, add new and iterate it
+      mergeColumns.add(newColumn);
+      if(++newIndex == newColumns.size()) {
+        // No more new left, merge down rest of existing and return
+        mergeDown(mergeColumns, columns, index);
+        finalize(mergeColumns);
+        return;
+      }
+      newColumn = newColumns.get(newIndex);
+      continue;
+    }
+  }
+  
+  private void mergeDown(List<ColumnCount> mergeColumns, 
+      List<ColumnCount> srcColumns, int srcIndex) {
+    while(srcIndex < srcColumns.size()) {
+      mergeColumns.add(srcColumns.get(srcIndex++));
+    }
+  }
+  
+  private void finalize(List<ColumnCount> mergeColumns) {
+    this.columns = mergeColumns;
+    this.index = 0;
+    this.column = this.columns.size() > 0? columns.get(index) : null;
+    
+    this.newColumns = new ArrayList<ColumnCount>();
+    this.newIndex = 0;
+    this.newColumn = null;
+  }
+  
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java	(revision 0)
@@ -0,0 +1,120 @@
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This class is responsible for the tracking and enforcement of Deletes
+ * during the course of a Scan operation.
+ * <p>
+ * This class is utilized through three methods:
+ * <ul><li>{@link add} when encountering a Delete
+ * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
+ * <li>{@link update} when reaching the end of a StoreFile
+ * <p>
+ * This class is NOT thread-safe as queries are never multi-threaded 
+ */
+public class ScanDeleteTracker implements DeleteTracker {
+
+  private long familyStamp = -1L;
+  private byte [] deleteBuffer = null;
+  private int deleteOffset = 0;
+  private int deleteLength = 0;
+  private byte deleteType = 0;
+  private long deleteTimestamp = 0L;
+  
+  /**
+   * Constructor for ScanDeleteTracker
+   */
+  public ScanDeleteTracker(){}
+  
+  /**
+   * Add the specified KeyValue to the list of deletes to check against for
+   * this row operation.
+   * <p>
+   * This is called when a Delete is encountered in a StoreFile.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @param type delete type as byte
+   */
+  @Override
+  public void add(byte[] buffer, int qualifierOffset, int qualifierLength,
+      long timestamp, byte type) {
+    if(timestamp > familyStamp){
+      if(type == KeyValue.Type.DeleteFamily.getCode()) {
+        familyStamp = timestamp;
+        return;
+      }
+
+      if(deleteBuffer != null && type < deleteType){
+        if(Bytes.compareTo(deleteBuffer, deleteOffset, deleteLength,
+            buffer, qualifierOffset, qualifierLength) == 0){
+          return;
+        }
+      }
+      deleteBuffer = buffer;
+      deleteOffset = qualifierOffset;
+      deleteLength = qualifierLength;
+      deleteType = type;
+      deleteTimestamp = timestamp;
+    }
+  }
+
+  /** 
+   * Check if the specified KeyValue buffer has been deleted by a previously
+   * seen delete.
+   * @param buffer KeyValue buffer
+   * @param qualifierOffset column qualifier offset
+   * @param qualifierLength column qualifier length
+   * @param timestamp timestamp
+   * @return true is the specified KeyValue is deleted, false if not
+   */
+  @Override
+  public boolean isDeleted(byte [] buffer, int qualifierOffset,
+      int qualifierLength, long timestamp) {
+    if(timestamp < familyStamp){
+      return true;
+    }
+    
+    if(deleteBuffer != null){
+      int ret = Bytes.compareTo(deleteBuffer, deleteOffset, deleteLength,
+          buffer, qualifierOffset, qualifierLength);
+
+      if(ret == 0){
+        if(deleteType == KeyValue.Type.DeleteColumn.getCode()){
+          return true;
+        }
+        ret = (int)(deleteTimestamp - timestamp);
+        deleteBuffer = null;
+        if(ret == 0){
+          return true;
+        }
+      } else if(ret < 0){
+        deleteBuffer = null;
+      } else {
+        //Should never happen, throw Exception
+      }
+    }
+
+    return false;
+  }
+
+  @Override
+  public boolean isEmpty() {
+    return deleteBuffer == null;
+  }
+
+  @Override
+  public void reset() {
+    familyStamp = 0L;
+    deleteBuffer = null;
+  }
+
+  @Override
+  public void update() {
+    this.reset();
+  }
+
+}
Index: src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/HAbstractScanner.java	(working copy)
@@ -1,214 +0,0 @@
-/**
- * Copyright 2007 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.regionserver;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.NavigableSet;
-import java.util.regex.Pattern;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.ColumnNameParseException;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Abstract base class that implements the InternalScanner.
- */
-public abstract class HAbstractScanner implements InternalScanner {
-  final Log LOG = LogFactory.getLog(this.getClass().getName());
-
-  // Pattern to determine if a column key is a regex
-  static final Pattern isRegexPattern =
-    Pattern.compile("^.*[\\\\+|^&*$\\[\\]\\}{)(]+.*$");
-
-  /** The kind of match we are doing on a column: */
-  private static enum MATCH_TYPE {
-    /** Just check the column family name */
-    FAMILY_ONLY,
-    /** Column family + matches regex */
-    REGEX,
-    /** Literal matching */
-    SIMPLE
-  }
-
-  private final List<ColumnMatcher> matchers = new ArrayList<ColumnMatcher>();
-
-  // True when scanning is done
-  protected volatile boolean scannerClosed = false;
-
-  // The timestamp to match entries against
-  protected final long timestamp;
-
-  private boolean wildcardMatch = false;
-  private boolean multipleMatchers = false;
-
-  /** Constructor for abstract base class */
-  protected HAbstractScanner(final long timestamp,
-    final NavigableSet<byte []> columns)
-  throws IOException {
-    this.timestamp = timestamp;
-    for (byte [] column: columns) {
-      ColumnMatcher matcher = new ColumnMatcher(column);
-      this.wildcardMatch = matcher.isWildCardMatch();
-      matchers.add(matcher);
-      this.multipleMatchers = !matchers.isEmpty();
-    }
-  }
-
-  /**
-   * For a particular column, find all the matchers defined for the column.
-   * Compare the column family and column key using the matchers. The first one
-   * that matches returns true. If no matchers are successful, return false.
-   * 
-   * @param kv KeyValue to test
-   * @return true if any of the matchers for the column match the column family
-   * and the column key.
-   *                 
-   * @throws IOException
-   */
-  protected boolean columnMatch(final KeyValue kv)
-  throws IOException {
-    if (matchers == null) {
-      return false;
-    }
-    for(int m = 0; m < this.matchers.size(); m++) {
-      if (this.matchers.get(m).matches(kv)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  public boolean isWildcardScanner() {
-    return this.wildcardMatch;
-  }
-  
-  public boolean isMultipleMatchScanner() {
-    return this.multipleMatchers;
-  }
-
-  public abstract boolean next(List<KeyValue> results)
-  throws IOException;
-
-  /**
-   * This class provides column matching functions that are more sophisticated
-   * than a simple string compare. There are three types of matching:
-   * <ol>
-   * <li>Match on the column family name only</li>
-   * <li>Match on the column family + column key regex</li>
-   * <li>Simple match: compare column family + column key literally</li>
-   * </ul>
-   */
-  private static class ColumnMatcher {
-    private boolean wildCardmatch;
-    private MATCH_TYPE matchType;
-    private byte [] family;
-    private Pattern columnMatcher;
-    // Column without delimiter so easy compare to KeyValue column
-    private byte [] col;
-    private int familylength = 0;
-  
-    ColumnMatcher(final byte [] col) throws IOException {
-      byte [][] parse = parseColumn(col);
-      // Make up column without delimiter
-      byte [] columnWithoutDelimiter =
-        new byte [parse[0].length + parse[1].length];
-      System.arraycopy(parse[0], 0, columnWithoutDelimiter, 0, parse[0].length);
-      System.arraycopy(parse[1], 0, columnWithoutDelimiter, parse[0].length,
-        parse[1].length);
-      // First position has family.  Second has qualifier.
-      byte [] qualifier = parse[1];
-      try {
-        if (qualifier == null || qualifier.length == 0) {
-          this.matchType = MATCH_TYPE.FAMILY_ONLY;
-          this.family = parse[0];
-          this.wildCardmatch = true;
-        } else if (isRegexPattern.matcher(Bytes.toString(qualifier)).matches()) {
-          this.matchType = MATCH_TYPE.REGEX;
-          this.columnMatcher =
-            Pattern.compile(Bytes.toString(columnWithoutDelimiter));
-          this.wildCardmatch = true;
-        } else {
-          this.matchType = MATCH_TYPE.SIMPLE;
-          this.col = columnWithoutDelimiter;
-          this.familylength = parse[0].length;
-          this.wildCardmatch = false;
-        }
-      } catch(Exception e) {
-        throw new IOException("Column: " + Bytes.toString(col) + ": " +
-          e.getMessage());
-      }
-    }
-    
-    /**
-     * @param kv
-     * @return
-     * @throws IOException
-     */
-    boolean matches(final KeyValue kv) throws IOException {
-      if (this.matchType == MATCH_TYPE.SIMPLE) {
-        return kv.matchingColumnNoDelimiter(this.col, this.familylength);
-      } else if(this.matchType == MATCH_TYPE.FAMILY_ONLY) {
-        return kv.matchingFamily(this.family);
-      } else if (this.matchType == MATCH_TYPE.REGEX) {
-        // Pass a column without the delimiter since thats whats we're
-        // expected to match.
-        int o = kv.getColumnOffset();
-        int l = kv.getColumnLength(o);
-        String columnMinusQualifier = Bytes.toString(kv.getBuffer(), o, l);
-        return this.columnMatcher.matcher(columnMinusQualifier).matches();
-      } else {
-        throw new IOException("Invalid match type: " + this.matchType);
-      }
-    }
-
-    boolean isWildCardMatch() {
-      return this.wildCardmatch;
-    }
-
-    /**
-     * @param c Column name
-     * @return Return array of size two whose first element has the family
-     * prefix of passed column <code>c</code> and whose second element is the
-     * column qualifier.
-     * @throws ColumnNameParseException 
-     */
-    public static byte [][] parseColumn(final byte [] c)
-    throws ColumnNameParseException {
-      final byte [][] result = new byte [2][];
-      // TODO: Change this so don't do parse but instead use the comparator
-      // inside in KeyValue which just looks at column family.
-      final int index = KeyValue.getFamilyDelimiterIndex(c, 0, c.length);
-      if (index == -1) {
-        throw new ColumnNameParseException("Impossible column name: " + c);
-      }
-      result[0] = new byte [index];
-      System.arraycopy(c, 0, result[0], 0, index);
-      final int len = c.length - (index + 1);
-      result[1] = new byte[len];
-      System.arraycopy(c, index + 1 /*Skip delimiter*/, result[1], 0,
-        len);
-      return result;
-    }
-  }
-}
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
 
@@ -202,18 +203,21 @@
     // Inform the HRegionServer that the parent HRegion is no-longer online.
     this.server.removeFromOnlineRegions(oldRegionInfo);
     
-    BatchUpdate update = new BatchUpdate(oldRegionInfo.getRegionName());
-    update.put(COL_REGIONINFO, Writables.getBytes(oldRegionInfo));
-    update.put(COL_SPLITA, Writables.getBytes(newRegions[0].getRegionInfo()));
-    update.put(COL_SPLITB, Writables.getBytes(newRegions[1].getRegionInfo()));
-    t.commit(update);
+    Put put = new Put(oldRegionInfo.getRegionName());
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, 
+        Writables.getBytes(oldRegionInfo));
+    put.add(CATALOG_FAMILY, SPLITA_QUALIFIER,
+        Writables.getBytes(newRegions[0].getRegionInfo()));
+    put.add(CATALOG_FAMILY, SPLITB_QUALIFIER,
+        Writables.getBytes(newRegions[0].getRegionInfo()));
+    t.put(put);
     
     // Add new regions to META
     for (int i = 0; i < newRegions.length; i++) {
-      update = new BatchUpdate(newRegions[i].getRegionName());
-      update.put(COL_REGIONINFO, Writables.getBytes(
+      put = new Put(newRegions[i].getRegionName());
+      put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(
         newRegions[i].getRegionInfo()));
-      t.commit(update);
+      t.put(put);
     }
         
     // Now tell the master about the new regions
Index: src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/HRegion.java	(working copy)
@@ -1,5 +1,5 @@
  /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -23,7 +23,6 @@
 import java.io.UnsupportedEncodingException;
 import java.util.ArrayList;
 import java.util.HashSet;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
@@ -36,7 +35,6 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
-import java.util.regex.Pattern;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -55,18 +53,19 @@
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.RegionHistorian;
 import org.apache.hadoop.hbase.ValueOverMaxLengthException;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchOperation;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.Reference.Range;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.Writables;
-import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.StringUtils;
 
@@ -1165,7 +1164,7 @@
    * @return map of values
    * @throws IOException
    */
-  RowResult getClosestRowBefore(final byte [] row)
+  Result getClosestRowBefore(final byte [] row)
   throws IOException{
     return getClosestRowBefore(row, HConstants.COLUMN_FAMILY);
   }
@@ -1180,8 +1179,8 @@
    * @return map of values
    * @throws IOException
    */
-  public RowResult getClosestRowBefore(final byte [] row,
-    final byte [] columnFamily)
+  public Result getClosestRowBefore(final byte [] row,
+    final byte [] family)
   throws IOException{
     // look across all the HStores for this region and determine what the
     // closest key is across all column families, since the data may be sparse
@@ -1189,7 +1188,7 @@
     checkRow(row);
     splitsAndClosesLock.readLock().lock();
     try {
-      Store store = getStore(columnFamily);
+      Store store = getStore(family);
       KeyValue kv = new KeyValue(row, HConstants.LATEST_TIMESTAMP);
       // get the closest key. (HStore.getRowKeyAtOrBefore can return null)
       key = store.getRowKeyAtOrBefore(kv);
@@ -1202,9 +1201,11 @@
       if (!this.comparator.matchingRows(kv, key)) {
         kv = new KeyValue(key.getRow(), HConstants.LATEST_TIMESTAMP);
       }
-      store.getFull(kv, null, null, 1, null, results, System.currentTimeMillis());
+      Get get = new Get(key.getRow());
+      store.get(get, null, results, this.comparator.getRawComparator());
+      
       // Convert to RowResult.  TODO: Remove need to do this.
-      return RowResult.createRowResult(results);
+      return new Result(results);
     } finally {
       splitsAndClosesLock.readLock().unlock();
     }
@@ -1212,42 +1213,44 @@
 
   /**
    * Return an iterator that scans over the HRegion, returning the indicated 
-   * columns for only the rows that match the data filter.  This Iterator must
-   * be closed by the caller.
+   * columns and rows specified by the {@link Scan}.
+   * <p>
+   * This Iterator must be closed by the caller.
    *
-   * @param cols columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
    * @param firstRow row which is the starting point of the scan
-   * @param timestamp only return rows whose timestamp is <= this value
-   * @param filter row filter
+   * @param scan configured {@link Scan}
    * @return InternalScanner
    * @throws IOException
    */
-  public InternalScanner getScanner(byte[][] cols, byte [] firstRow,
-    long timestamp, RowFilterInterface filter) 
+  public InternalScanner getScanner(byte [] firstRow, Scan scan)
   throws IOException {
     newScannerLock.readLock().lock();
     try {
       if (this.closed.get()) {
         throw new IOException("Region " + this + " closed");
       }
-      HashSet<Store> storeSet = new HashSet<Store>();
-      NavigableSet<byte []> columns =
-        new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
-      // Below we make up set of stores we want scanners on and we fill out the
-      // list of columns.
-      for (int i = 0; i < cols.length; i++) {
-        columns.add(cols[i]);
-        Store s = stores.get(cols[i]);
-        if (s != null) {
-          storeSet.add(s);
+      List<Store> storeList;
+      if(scan.hasFamilies()) {
+        storeList = new ArrayList<Store>(scan.numFamilies());
+        for(byte [] family : scan.getFamilies()) {
+          Store s = stores.get(family);
+          if(s != null) {
+            storeList.add(s);
+          }
         }
+      } else {
+        // stores.size() is slow because of ConcurrentSkipListMap
+        // might change this to an array or just initialized to 0
+        storeList = new ArrayList<Store>(stores.size());
+        for(Map.Entry<byte[], Store> entry : stores.entrySet()) {
+          storeList.add(entry.getValue());
+        }
       }
-      return new HScanner(columns, firstRow, timestamp,
-        storeSet.toArray(new Store [storeSet.size()]), filter);
+      
+      Store [] stores = storeList.toArray(new Store[storeList.size()]);
+      
+      return new RegionScanner(firstRow, scan, stores);
+      
     } finally {
       newScannerLock.readLock().unlock();
     }
@@ -1256,44 +1259,122 @@
   //////////////////////////////////////////////////////////////////////////////
   // set() methods for client use.
   //////////////////////////////////////////////////////////////////////////////
+  /**
+   * @param delete
+   * @param lockid
+   * @param writeToWAL
+   * @throws IOException
+   */
+  public void delete(Delete delete, Integer lockid, boolean writeToWAL)
+  throws IOException {
+    checkReadOnly();
+    checkResources();
+    splitsAndClosesLock.readLock().lock();
+    Integer lid = null;
+    try {
+      byte [] row = delete.getRow();
+      // If we did not pass an existing row lock, obtain a new one
+      lid = getLock(lockid, row);
+
+      //Check to see if this is a deleteRow insert
+      if(delete.getFamilyMap().isEmpty()){
+        for(byte [] family : regionInfo.getTableDesc().getFamiliesKeys()){
+          delete.deleteFamily(family);
+        }
+      }
+      
+      for(Map.Entry<byte[], List<KeyValue>> entry :
+        delete.getFamilyMap().entrySet()){
+        byte [] family = entry.getKey();
+        checkFamily(family);
+        delete(family, entry.getValue(), writeToWAL);
+      }
+        
+    } finally {
+      if(lockid == null) releaseRowLock(lid);
+      splitsAndClosesLock.readLock().unlock();
+    }
+  }
   
+  
   /**
-   * @param b
+   * @param family
+   * @param kvs
+   * @param writeToWAL
    * @throws IOException
    */
-  public void batchUpdate(BatchUpdate b) throws IOException {
-    this.batchUpdate(b, null, true);
+  public void delete(byte [] family, List<KeyValue> kvs, boolean writeToWAL)
+  throws IOException {
+    byte[] currTime = Bytes.toBytes(System.currentTimeMillis());
+    boolean flush = false;
+    this.updatesLock.readLock().lock();
+    try {
+      if (writeToWAL) {
+        this.log.append(regionInfo.getRegionName(),
+          regionInfo.getTableDesc().getName(), kvs,
+          (regionInfo.isMetaRegion() || regionInfo.isRootRegion()));
+      }
+      long size = 0;
+      Store store = getStore(family);
+      for (KeyValue kv: kvs) {
+        // Check if time is LATEST, change to now if so
+        byte [] bytes = kv.getBuffer();
+        int keyLength = Bytes.toInt(bytes, 0);
+        int tsOffset = KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keyLength -
+          KeyValue.TIMESTAMP_TYPE_SIZE;
+        if(Bytes.compareTo(bytes, tsOffset, Bytes.SIZEOF_LONG, 
+            LONG_MAX_BYTES, 0, Bytes.SIZEOF_LONG) == 0){
+          Bytes.putBytes(bytes, tsOffset, currTime, 0, Bytes.SIZEOF_LONG);
+        }
+        size = this.memcacheSize.addAndGet(store.delete(kv));
+      }
+      flush = isFlushSize(size);
+    } finally {
+      this.updatesLock.readLock().unlock();
+    }
+    if (flush) {
+      // Request a cache flush.  Do it outside update lock.
+      requestFlush();
+    }
   }
   
   /**
-   * @param b
+   * @param put
+   * @throws IOException
+   */
+  public void put(Put put) throws IOException {
+    this.put(put, null, true);
+  }
+  
+  /**
+   * @param put
    * @param writeToWAL
    * @throws IOException
    */
-  public void batchUpdate(BatchUpdate b, boolean writeToWAL) throws IOException {
-    this.batchUpdate(b, null, writeToWAL);
+  public void put(Put put, boolean writeToWAL) throws IOException {
+    this.put(put, null, writeToWAL);
   }
 
   
   /**
-   * @param b
+   * @param put
    * @param lockid
    * @throws IOException
    */
-  public void batchUpdate(BatchUpdate b, Integer lockid) throws IOException {
-    this.batchUpdate(b, lockid, true);
+  public void put(Put put, Integer lockid) throws IOException {
+    this.put(put, lockid, true);
   }
-  
+
   /**
-   * @param b
+   * @param put
    * @param lockid
-   * @param writeToWAL if true, then we write this update to the log
+   * @param writeToWAL
    * @throws IOException
    */
-  public void batchUpdate(BatchUpdate b, Integer lockid, boolean writeToWAL)
+  public void put(Put put, Integer lockid, boolean writeToWAL)
   throws IOException {
     checkReadOnly();
-    validateValuesLength(b);
+    validateValuesLength(put);
 
     // Do a rough check that we have resources to accept a write.  The check is
     // 'rough' in that between the resource check and the call to obtain a 
@@ -1307,51 +1388,20 @@
       // #commit or #abort or if the HRegionServer lease on the lock expires.
       // See HRegionServer#RegionListener for how the expire on HRegionServer
       // invokes a HRegion#abort.
-      byte [] row = b.getRow();
+      byte [] row = put.getRow();
       // If we did not pass an existing row lock, obtain a new one
       Integer lid = getLock(lockid, row);
-      long now = System.currentTimeMillis();
-      long commitTime = b.getTimestamp() == LATEST_TIMESTAMP?
-        now: b.getTimestamp();
-      Set<byte []> latestTimestampDeletes = null;
-      List<KeyValue> edits = new ArrayList<KeyValue>();
+      byte [] now = Bytes.toBytes(System.currentTimeMillis());
       try {
-        for (BatchOperation op: b) {
-          byte [] column = op.getColumn();
-          checkColumn(column);
-          KeyValue kv = null;
-          if (op.isPut()) {
-            kv = new KeyValue(row, column, commitTime, op.getValue());
-          } else {
-            // Its a delete.
-            if (b.getTimestamp() == LATEST_TIMESTAMP) {
-              // Save off these deletes of the most recent thing added on the
-              // family.
-              if (latestTimestampDeletes == null) {
-                latestTimestampDeletes =
-                  new TreeSet<byte []>(Bytes.BYTES_RAWCOMPARATOR);
-              }
-              latestTimestampDeletes.add(op.getColumn());
-              continue;
-            }
-            // Its an explicit timestamp delete
-            kv = new KeyValue(row, column, commitTime, KeyValue.Type.Delete,
-              HConstants.EMPTY_BYTE_ARRAY);
+        for(Map.Entry<byte[], List<KeyValue>> entry : 
+          put.getFamilyMap().entrySet()) {
+          byte [] family = entry.getKey();
+          checkFamily(family);
+          List<KeyValue> puts = entry.getValue();
+          if(updateKeys(puts, now)) {
+            put(family, puts, writeToWAL);
           }
-          edits.add(kv);
         }
-        if (!edits.isEmpty()) {
-          update(edits, writeToWAL);
-        }
-        if (latestTimestampDeletes != null &&
-            !latestTimestampDeletes.isEmpty()) {
-          // We have some LATEST_TIMESTAMP deletes to run.  Can't do them inline
-          // as edits.  Need to do individually after figuring which is latest
-          // timestamp to delete.
-          for (byte [] column: latestTimestampDeletes) {
-            deleteMultiple(row, column, LATEST_TIMESTAMP, 1);
-          }
-        }
       } finally {
         if(lockid == null) releaseRowLock(lid);
       }
@@ -1361,18 +1411,37 @@
   }
 
   /**
+   * Checks if any stamps are > now.  If so, sets them to now.
+   * <p>
+   * This acts to be prevent users from inserting future stamps as well as
+   * to replace LATEST_TIMESTAMP with now.
+   * @param keys
+   * @param now
+   * @return
+   */
+  private boolean updateKeys(List<KeyValue> keys, byte [] now) {
+    if(keys == null || keys.isEmpty()) {
+      return false;
+    }
+    for(KeyValue key : keys) {
+      key.updateLatestStamp(now);
+    }
+    return true;
+  }
+  
+  /**
    * Performs an atomic check and save operation. Checks if
    * the specified expected values have changed, and if not
    * applies the update.
    * 
-   * @param b the update to apply
+   * @param put the put to apply
    * @param expectedValues the expected values to check
    * @param lockid
    * @param writeToWAL whether or not to write to the write ahead log
    * @return true if update was applied
    * @throws IOException
    */
-  public boolean checkAndSave(BatchUpdate b,
+  public boolean checkAndSave(Put put,
     HbaseMapWritable<byte[], byte[]> expectedValues, Integer lockid,
     boolean writeToWAL)
   throws IOException {
@@ -1382,11 +1451,11 @@
     // should read the comments from the batchUpdate method
     boolean success = true;
     checkReadOnly();
-    validateValuesLength(b);
+    validateValuesLength(put);
     checkResources();
     splitsAndClosesLock.readLock().lock();
     try {
-      byte[] row = b.getRow();
+      byte[] row = put.getRow();
       Integer lid = getLock(lockid,row);
       try {
         NavigableSet<byte []> keySet =
@@ -1403,75 +1472,48 @@
           }
         }
         if (success) {
-          long commitTime = (b.getTimestamp() == LATEST_TIMESTAMP)?
-            System.currentTimeMillis(): b.getTimestamp();
-          Set<byte []> latestTimestampDeletes = null;
-          List<KeyValue> edits = new ArrayList<KeyValue>();
-          for (BatchOperation op: b) {
-            byte [] column = op.getColumn();
-            KeyValue kv = null;
-            if (op.isPut()) {
-              kv = new KeyValue(row, column, commitTime, op.getValue());
-            } else {
-              // Its a delete.
-              if (b.getTimestamp() == LATEST_TIMESTAMP) {
-                // Save off these deletes of the most recent thing added on
-                // the family.
-                if (latestTimestampDeletes == null) {
-                  latestTimestampDeletes =
-                    new TreeSet<byte []>(Bytes.BYTES_RAWCOMPARATOR);
-                }
-                latestTimestampDeletes.add(op.getColumn());
-              } else {
-                // Its an explicit timestamp delete
-                kv = new KeyValue(row, column, commitTime,
-                  KeyValue.Type.Delete, HConstants.EMPTY_BYTE_ARRAY);
-              }
+          byte [] now = Bytes.toBytes(System.currentTimeMillis());
+          for(Map.Entry<byte[], List<KeyValue>> entry : 
+            put.getFamilyMap().entrySet()) {
+            byte [] family = entry.getKey();
+            checkFamily(family);
+            List<KeyValue> puts = entry.getValue();
+            if(updateKeys(puts, now)) {
+              put(family, puts, writeToWAL);
             }
-            edits.add(kv);
           }
-          if (!edits.isEmpty()) {
-            update(edits, writeToWAL);
-          }
-          if (latestTimestampDeletes != null &&
-              !latestTimestampDeletes.isEmpty()) {
-            // We have some LATEST_TIMESTAMP deletes to run.  Can't do them inline
-            // as edits.  Need to do individually after figuring which is latest
-            // timestamp to delete.
-            for (byte [] column: latestTimestampDeletes) {
-              deleteMultiple(row, column, LATEST_TIMESTAMP, 1);
-            }
-          }
         }
       } finally {
         if(lockid == null) releaseRowLock(lid);
       }
     } finally {
-      splitsAndClosesLock.readLock().unlock();
+          splitsAndClosesLock.readLock().unlock();
     }
     return success;
   }
 
   /*
-   * Utility method to verify values length
+   * Utility method to verify values length. 
    * @param batchUpdate The update to verify
    * @throws IOException Thrown if a value is too long
    */
-  private void validateValuesLength(BatchUpdate batchUpdate)
+  private void validateValuesLength(Put put)
   throws IOException {
-    for (Iterator<BatchOperation> iter = 
-      batchUpdate.iterator(); iter.hasNext();) {
-      BatchOperation operation = iter.next();
-      if (operation.getValue() != null) {
-        HColumnDescriptor fam = this.regionInfo.getTableDesc().
-          getFamily(operation.getColumn());
-        if (fam != null) {
-          int maxLength = fam.getMaxValueLength();
-          if (operation.getValue().length > maxLength) {
-            throw new ValueOverMaxLengthException("Value in column "
-                + Bytes.toString(operation.getColumn()) + " is too long. "
-                + operation.getValue().length + " instead of " + maxLength);
-          }
+    // Can this be moved to client-side?
+    // Unfortunate we need to look at all the data like this.
+    // Since value is not separate, I'm going to just use KV length
+    // otherwise there's significant overhead here!
+    
+    for(Map.Entry<byte[], List<KeyValue>> entry : 
+      put.getFamilyMap().entrySet()) {
+      HColumnDescriptor hcd = 
+        this.regionInfo.getTableDesc().getFamily(entry.getKey());
+      int maxLength = hcd.getMaxValueLength();
+      for(KeyValue kv : entry.getValue()) {
+        if(kv.getLength() > maxLength) {
+          throw new ValueOverMaxLengthException("Value in column "
+              + Bytes.toString(kv.getColumn()) + " is too long. "
+              + kv.getLength() + " instead of " + maxLength);
         }
       }
     }
@@ -1512,203 +1554,8 @@
           + Thread.currentThread().getName() + "'");
     }
   }
-  
-  /**
-   * Delete all cells of the same age as the passed timestamp or older.
-   * @param row
-   * @param column
-   * @param ts Delete all entries that have this timestamp or older
-   * @param lockid Row lock
-   * @throws IOException
-   */
-  public void deleteAll(final byte [] row, final byte [] column, final long ts,
-      final Integer lockid)
-  throws IOException {
-    checkColumn(column);
-    checkReadOnly();
-    Integer lid = getLock(lockid,row);
-    try {
-      // Delete ALL versions rather than column family VERSIONS.  If we just did
-      // VERSIONS, then if 2* VERSION cells, subsequent gets would get old stuff.
-      deleteMultiple(row, column, ts, ALL_VERSIONS);
-    } finally {
-      if(lockid == null) releaseRowLock(lid);
-    }
-  }
 
   /**
-   * Delete all cells of the same age as the passed timestamp or older.
-   * @param row
-   * @param ts Delete all entries that have this timestamp or older
-   * @param lockid Row lock
-   * @throws IOException
-   */
-  public void deleteAll(final byte [] row, final long ts, final Integer lockid)
-  throws IOException {
-    checkReadOnly();
-    Integer lid = getLock(lockid, row);
-    long time = ts;
-    if (ts == HConstants.LATEST_TIMESTAMP) {
-      time = System.currentTimeMillis();
-    }
-    KeyValue kv = KeyValue.createFirstOnRow(row, time);
-    try {
-      for (Store store : stores.values()) {
-        List<KeyValue> keyvalues = new ArrayList<KeyValue>();
-        store.getFull(kv, null, null, ALL_VERSIONS, null, keyvalues, time);
-        List<KeyValue> edits = new ArrayList<KeyValue>();
-        for (KeyValue key: keyvalues) {
-          // This is UGLY. COPY OF KEY PART OF KeyValue.
-          edits.add(key.cloneDelete());
-        }
-        update(edits);
-      }
-    } finally {
-      if (lockid == null) releaseRowLock(lid);
-    }
-  }
-  
-  /**
-   * Delete all cells for a row with matching columns with timestamps
-   * less than or equal to <i>timestamp</i>. 
-   * 
-   * @param row The row to operate on
-   * @param columnRegex The column regex 
-   * @param timestamp Timestamp to match
-   * @param lockid Row lock
-   * @throws IOException
-   */
-  public void deleteAllByRegex(final byte [] row, final String columnRegex, 
-      final long timestamp, final Integer lockid) throws IOException {
-    checkReadOnly();
-    Pattern columnPattern = Pattern.compile(columnRegex);
-    Integer lid = getLock(lockid, row);
-    long now = System.currentTimeMillis();
-    KeyValue kv = new KeyValue(row, timestamp);
-    try {
-      for (Store store : stores.values()) {
-        List<KeyValue> keyvalues = new ArrayList<KeyValue>();
-        store.getFull(kv, null, columnPattern, ALL_VERSIONS, null, keyvalues,
-          now);
-        List<KeyValue> edits = new ArrayList<KeyValue>();
-        for (KeyValue key: keyvalues) {
-          edits.add(key.cloneDelete());
-        }
-        update(edits);
-      }
-    } finally {
-      if(lockid == null) releaseRowLock(lid);
-    }
-  }
-
-  /**
-   * Delete all cells for a row with matching column family with timestamps
-   * less than or equal to <i>timestamp</i>.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @param timestamp Timestamp to match
-   * @param lockid Row lock
-   * @throws IOException
-   */
-  public void deleteFamily(byte [] row, byte [] family, long timestamp,
-      final Integer lockid)
-  throws IOException{
-    checkReadOnly();
-    Integer lid = getLock(lockid, row);
-    long now = System.currentTimeMillis();
-    try {
-      // find the HStore for the column family
-      Store store = getStore(family);
-      // find all the keys that match our criteria
-      List<KeyValue> keyvalues = new ArrayList<KeyValue>();
-      store.getFull(new KeyValue(row, timestamp), null, null, ALL_VERSIONS,
-        null, keyvalues, now);
-      // delete all the cells
-      List<KeyValue> edits = new ArrayList<KeyValue>();
-      for (KeyValue kv: keyvalues) {
-        edits.add(kv.cloneDelete());
-      }
-      update(edits);
-    } finally {
-      if(lockid == null) releaseRowLock(lid);
-    }
-  }
-
-  /**
-   * Delete all cells for a row with all the matching column families by
-   * familyRegex with timestamps less than or equal to <i>timestamp</i>.
-   * 
-   * @param row The row to operate on
-   * @param familyRegex The column family regex for matching. This regex
-   * expression just match the family name, it didn't include <code>:<code>
-   * @param timestamp Timestamp to match
-   * @param lockid Row lock
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(byte [] row, String familyRegex,
-      final long timestamp, final Integer lockid)
-  throws IOException {
-    checkReadOnly();
-    // construct the family regex pattern
-    Pattern familyPattern = Pattern.compile(familyRegex);
-    Integer lid = getLock(lockid, row);
-    long now = System.currentTimeMillis();
-    KeyValue kv = new KeyValue(row, timestamp);
-    try {
-      for(Store store: stores.values()) {
-        String familyName = Bytes.toString(store.getFamily().getName());
-        // check the family name match the family pattern.
-        if(!(familyPattern.matcher(familyName).matches())) 
-          continue;
-        
-        List<KeyValue> keyvalues = new ArrayList<KeyValue>();
-        store.getFull(kv, null, null, ALL_VERSIONS, null, keyvalues, now);
-        List<KeyValue> edits = new ArrayList<KeyValue>();
-        for (KeyValue k: keyvalues) {
-          edits.add(k.cloneDelete());
-        }
-        update(edits);
-      }
-    } finally {
-      if(lockid == null) releaseRowLock(lid);
-    }
-  }
-  
-  /*
-   * Delete one or many cells.
-   * Used to support {@link #deleteAll(byte [], byte [], long)} and deletion of
-   * latest cell.
-   * @param row
-   * @param column
-   * @param ts Timestamp to start search on.
-   * @param versions How many versions to delete. Pass
-   * {@link HConstants#ALL_VERSIONS} to delete all.
-   * @throws IOException
-   */
-  private void deleteMultiple(final byte [] row, final byte [] column,
-      final long ts, final int versions)
-  throws IOException {
-    checkReadOnly();
-    // We used to have a getKeys method that purportedly only got the keys and
-    // not the keys and values.  We now just do getFull.  For memcache values,
-    // shouldn't matter if we get key and value since it'll be the entry that
-    // is in memcache.  For the keyvalues from storefile, could be saving if
-    // we only returned key component. TODO.
-    List<KeyValue> keys = get(row, column, ts, versions);
-    if (keys != null && keys.size() > 0) {
-      // I think the below edits don't have to be storted.  Its deletes.
-      // hey don't have to go in in exact sorted order (we don't have to worry
-      // about the meta or root sort comparator here).
-      List<KeyValue> edits = new ArrayList<KeyValue>();
-      for (KeyValue key: keys) {
-        edits.add(key.cloneDelete());
-      }
-      update(edits);
-    }
-  }
-
-  /**
    * Tests for the existence of any cells for a given coordinate.
    * 
    * @param row the row
@@ -1750,8 +1597,9 @@
    * @param edits Cell updates by column
    * @throws IOException
    */
-  private void update(final List<KeyValue> edits) throws IOException {
-    this.update(edits, true);
+  private void put(final byte [] family, final List<KeyValue> edits)
+  throws IOException {
+    this.put(family, edits, true);
   }
 
   /** 
@@ -1761,8 +1609,8 @@
    * @param updatesByColumn Cell updates by column
    * @throws IOException
    */
-  private void update(final List<KeyValue> edits, boolean writeToWAL)
-  throws IOException {
+  private void put(final byte [] family, final List<KeyValue> edits, 
+      boolean writeToWAL) throws IOException {
     if (edits == null || edits.isEmpty()) {
       return;
     }
@@ -1775,9 +1623,10 @@
           (regionInfo.isMetaRegion() || regionInfo.isRootRegion()));
       }
       long size = 0;
+      Store store = getStore(family);
       for (KeyValue kv: edits) {
         // TODO: Fix -- do I have to do a getColumn here?
-        size = this.memcacheSize.addAndGet(getStore(kv.getColumn()).add(kv));
+        size = this.memcacheSize.addAndGet(store.add(kv));
       }
       flush = isFlushSize(size);
     } finally {
@@ -1815,7 +1664,6 @@
   }
   
   // Do any reconstruction needed from the log
-  @SuppressWarnings("unused")
   protected void doReconstructionLog(Path oldLogFile, long minSeqId, long maxSeqId,
     Progressable reporter)
   throws UnsupportedEncodingException, IOException {
@@ -1895,7 +1743,7 @@
    * @throws IOException
    * @return The id of the held lock.
    */
-  Integer obtainRowLock(final byte [] row) throws IOException {
+  public Integer obtainRowLock(final byte [] row) throws IOException {
     checkRow(row);
     splitsAndClosesLock.readLock().lock();
     try {
@@ -2008,160 +1856,97 @@
   }
 
   /**
-   * HScanner is an iterator through a bunch of rows in an HRegion.
+   * RegionScanner is an iterator through a bunch of rows in an HRegion.
+   * <p>
+   * It is used to combine scanners from multiple Stores.
    */
-  private class HScanner implements InternalScanner {
-    private InternalScanner[] scanners;
-    private List<KeyValue> [] resultSets;
-    private RowFilterInterface filter;
-
-    /** Create an HScanner with a handle on many HStores. */
-    @SuppressWarnings("unchecked")
-    HScanner(final NavigableSet<byte []> columns, byte [] firstRow,
-      long timestamp, final Store [] stores, final RowFilterInterface filter)
+  private class RegionScanner implements InternalScanner {
+    
+    private KeyValueHeap storeHeap;
+    private byte [] stopRow;
+    
+    RegionScanner(byte [] firstRow, Scan scan, final Store [] stores)
     throws IOException {
-      this.filter = filter;
-      this.scanners = new InternalScanner[stores.length];
+      if(Bytes.equals(scan.getStopRow(), HConstants.EMPTY_END_ROW)) {
+        this.stopRow = null;
+      } else {
+        this.stopRow = scan.getStopRow();
+      }
+      KeyValueScanner [] scanners = new KeyValueScanner[stores.length];
       try {
+        Map<byte[], NavigableSet<byte[]>> familyMap = scan.getFamilyMap();
         for (int i = 0; i < stores.length; i++) {
-          // Only pass relevant columns to each store
-          NavigableSet<byte[]> columnSubset =
-            new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-          for (byte [] c: columns) {
-            if (KeyValue.FAMILY_COMPARATOR.compare(stores[i].storeName, c) == 0) {
-              columnSubset.add(c);
-            }
+          NavigableSet<byte[]> columns;
+          if(familyMap == null) {
+            columns = null;
+          } else {
+            columns = familyMap.get(stores[i].storeName);
           }
-          RowFilterInterface f = filter;
-          if (f != null) {
-            // Need to replicate filters.
-            // At least WhileMatchRowFilter will mess up the scan if only
-            // one shared across many rows. See HADOOP-2467.
-            f = WritableUtils.clone(filter, conf);
-          }
-          scanners[i] = stores[i].getScanner(timestamp, columnSubset, firstRow, f);
+          scanners[i] = stores[i].getScanner(scan, columns, firstRow);
         }
       } catch (IOException e) {
-        for (int i = 0; i < this.scanners.length; i++) {
+        for (int i = 0; i < scanners.length; i++) {
           if (scanners[i] != null) {
-            closeScanner(i);
+            close(scanners[i]);
           }
         }
         throw e;
       }
-
-      // Advance to the first key in each store.
-      // All results will match the required column-set and scanTime.
-      this.resultSets = new List[scanners.length];
-      for (int i = 0; i < scanners.length; i++) {
-        resultSets[i] = new ArrayList<KeyValue>();
-        if(scanners[i] != null && !scanners[i].next(resultSets[i])) {
-          closeScanner(i);
-        }
-      }
-
+      
+      this.storeHeap = new KeyValueHeap(scanners, comparator);
+      
       // As we have now successfully completed initialization, increment the
       // activeScanner count.
       activeScannerCount.incrementAndGet();
     }
 
+    /**
+     * Get the next row of results from this region.
+     * @param results list to append results to
+     * @return true if there are more rows, false if scanner is done
+     */
     public boolean next(List<KeyValue> results)
     throws IOException {
-      boolean moreToFollow = false;
-      boolean filtered = false;
-      do {
-        // Find the lowest key across all stores.
-        KeyValue chosen = null;
-        long chosenTimestamp = -1;
-        for (int i = 0; i < this.scanners.length; i++) {
-          if (this.resultSets[i] == null || this.resultSets[i].isEmpty()) {
-            continue;
-          }
-          KeyValue kv = this.resultSets[i].get(0);
-          if (chosen == null ||
-               (comparator.compareRows(kv, chosen) < 0) ||
-               ((comparator.compareRows(kv, chosen) == 0) &&
-                 (kv.getTimestamp() > chosenTimestamp))) {
-            chosen = kv;
-            chosenTimestamp = chosen.getTimestamp();
-          }
-        }
-
-        // Store results from each sub-scanner.
-        if (chosenTimestamp >= 0) {
-          for (int i = 0; i < scanners.length; i++) {
-            if (this.resultSets[i] == null || this.resultSets[i].isEmpty()) {
-              continue;
-            }
-            KeyValue kv = this.resultSets[i].get(0);
-            if (comparator.compareRows(kv, chosen) == 0) {
-              results.addAll(this.resultSets[i]);
-              resultSets[i].clear();
-              if (!scanners[i].next(resultSets[i])) {
-                closeScanner(i);
-              }
-            }
-          }
-        }
-
-        moreToFollow = chosenTimestamp >= 0;
-        if (results == null || results.size() <= 0) {
-          // If we got no results, then there is no more to follow.
-          moreToFollow = false;
-        }
-
-        filtered = filter == null ? false : filter.filterRow(results);
-        if (filter != null && filter.filterAllRemaining()) {
-          moreToFollow = false;
-        }
-        
-        if (moreToFollow) {
-          if (filter != null) {
-            filter.rowProcessed(filtered, chosen.getBuffer(), chosen.getRowOffset(),
-              chosen.getRowLength());
-          }
-          if (filtered) {
-            results.clear();
-          }
-        }
-      } while(filtered && moreToFollow);
-
-      // Make sure scanners closed if no more results
-      if (!moreToFollow) {
-        for (int i = 0; i < scanners.length; i++) {
-          if (null != scanners[i]) {
-            closeScanner(i);
-          }
-        }
+      // This method should probably be reorganized a bit... has gotten messy
+      KeyValue kv = this.storeHeap.peek();
+      if(kv == null) {
+        close();
+        return false;
       }
+      byte [] row = kv.getRow();
+      // See if we passed stopRow
+      if(stopRow != null &&
+          comparator.compareRows(row, 0, row.length, stopRow, 0, stopRow.length)
+          <= 0){
+        close();
+        return false;
+      }
+      if(!this.storeHeap.next(results)) {
+        // May or may not have received results, but our heap is now empty
+        close();
+        return false;
+      }
       
-      return moreToFollow;
-    }
-
-    /** Shut down a single scanner */
-    void closeScanner(int i) {
-      try {
-        try {
-          scanners[i].close();
-        } catch (IOException e) {
-          LOG.warn("Failed closing scanner " + i, e);
+      while((kv = this.storeHeap.peek()) != null) {
+        // See if we jumped rows
+        if(comparator.compareRows(kv, row) != 0) {
+          // Jumped rows, return now (in the future, do a filter check?)
+          return true;
         }
-      } finally {
-        scanners[i] = null;
-        // These data members can be null if exception in constructor
-        if (resultSets != null) {
-          resultSets[i] = null;
+        if(!this.storeHeap.next(results)) {
+          close();
+          return false;
         }
       }
+      
+      // Heap is empty, scanners are done.
+      close();
+      return false;
     }
 
     public void close() {
       try {
-        for(int i = 0; i < scanners.length; i++) {
-          if(scanners[i] != null) {
-            closeScanner(i);
-          }
-        }
+        storeHeap.close();
       } finally {
         synchronized (activeScannerCount) {
           int count = activeScannerCount.decrementAndGet();
@@ -2177,14 +1962,12 @@
         }
       }
     }
-
-    public boolean isWildcardScanner() {
-      throw new UnsupportedOperationException("Unimplemented on HScanner");
+    
+    public void close(KeyValueScanner scanner) {
+      try {
+        scanner.close();
+      } catch(NullPointerException npe) {}
     }
-
-    public boolean isMultipleMatchScanner() {
-      throw new UnsupportedOperationException("Unimplemented on HScanner");
-    }  
   }
   
   // Utility methods
@@ -2273,7 +2056,7 @@
       List<KeyValue> edits = new ArrayList<KeyValue>();
       edits.add(new KeyValue(row, COL_REGIONINFO, System.currentTimeMillis(),
         Writables.getBytes(r.getRegionInfo())));
-      meta.update(edits);
+      meta.put(HConstants.CATALOG_FAMILY, edits);
     } finally {
       meta.releaseRowLock(lid);
     }
@@ -2293,8 +2076,9 @@
   public static void removeRegionFromMETA(final HRegionInterface srvr,
     final byte [] metaRegionName, final byte [] regionName)
   throws IOException {
-    srvr.deleteFamily(metaRegionName, regionName, HConstants.COLUMN_FAMILY,
-      HConstants.LATEST_TIMESTAMP, -1L);
+    Delete delete = new Delete(regionName);
+    delete.deleteFamily(HConstants.CATALOG_FAMILY);
+    srvr.delete(metaRegionName, delete);
   }
 
   /**
@@ -2308,14 +2092,18 @@
   public static void offlineRegionInMETA(final HRegionInterface srvr,
     final byte [] metaRegionName, final HRegionInfo info)
   throws IOException {
-    BatchUpdate b = new BatchUpdate(info.getRegionName());
+    // Puts and Deletes used to be "atomic" here.  We can use row locks if
+    // we need to keep that property, or we can expand Puts and Deletes to
+    // allow them to be committed at once.
+    byte [] row = info.getRegionName();
+    Put put = new Put(row);
     info.setOffline(true);
-    b.put(COL_REGIONINFO, Writables.getBytes(info));
-    b.delete(COL_SERVER);
-    b.delete(COL_STARTCODE);
-    // If carrying splits, they'll be in place when we show up on new
-    // server.
-    srvr.batchUpdate(metaRegionName, b, -1L);
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(info));
+    srvr.put(metaRegionName, put);
+    Delete del = new Delete(row);
+    del.deleteColumns(CATALOG_FAMILY, SERVER_QUALIFIER);
+    del.deleteColumns(CATALOG_FAMILY, STARTCODE_QUALIFIER);
+    srvr.delete(metaRegionName, del);
   }
   
   /**
@@ -2329,12 +2117,10 @@
   public static void cleanRegionInMETA(final HRegionInterface srvr,
     final byte [] metaRegionName, final HRegionInfo info)
   throws IOException {
-    BatchUpdate b = new BatchUpdate(info.getRegionName());
-    b.delete(COL_SERVER);
-    b.delete(COL_STARTCODE);
-    // If carrying splits, they'll be in place when we show up on new
-    // server.
-    srvr.batchUpdate(metaRegionName, b, LATEST_TIMESTAMP);
+    Delete del = new Delete(info.getRegionName());
+    del.deleteColumns(CATALOG_FAMILY, SERVER_QUALIFIER);
+    del.deleteColumns(CATALOG_FAMILY, STARTCODE_QUALIFIER);
+    srvr.delete(metaRegionName, del);
   }
 
   /**
@@ -2627,6 +2413,14 @@
     }
   }
 
+  /**
+   * 
+   * @param row
+   * @param column
+   * @param amount
+   * @return
+   * @throws IOException
+   */
   public long incrementColumnValue(byte[] row, byte[] column, long amount)
   throws IOException {
     checkRow(row);
@@ -2681,13 +2475,63 @@
         value = Bytes.incrementBytes(value, amount);
       }
 
-      BatchUpdate b = new BatchUpdate(row, ts);
-      b.put(column, value);
-      batchUpdate(b, lid, true);
+      Put put = new Put(row);
+      byte [][] split = KeyValue.parseColumn(column);
+      put.add(split[0], split[1], ts, value);
+      put(put, lid, true);
       return Bytes.toLong(value);
     } finally {
       splitsAndClosesLock.readLock().unlock();
       releaseRowLock(lid);
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  /**
+   * @param get
+   * @param lockid
+   * @return result
+   * @throws IOException
+   */
+  public Result get(final Get get, final Integer lockid) throws IOException {
+    // Verify families are all valid
+    if(get.hasFamilies()) {
+      for(byte [] family : get.familySet()) {
+        checkFamily(family);
+      }
+    }
+    // Lock row
+    Integer lid = getLock(lockid, get.getRow()); 
+    List<KeyValue> result = new ArrayList<KeyValue>();
+    try {
+      for(Map.Entry<byte[],NavigableSet<byte[]>> entry : 
+        get.getFamilyMap().entrySet()) {
+        byte [] family = entry.getKey();
+        Store store = stores.get(family);
+        store.get(get, entry.getValue(), result, 
+            this.comparator.getRawComparator());
+      }
+    } finally {
+      if(lockid == null) releaseRowLock(lid);
+    }
+    return new Result(result);
+  }
+  
+  //
+  // New HBASE-880 Helpers
+  //
+  
+  private void checkFamily(final byte [] family) 
+  throws NoSuchColumnFamilyException {
+    if(family == null) {
+      throw new NoSuchColumnFamilyException("Empty family is invalid");
+    }
+    if(!regionInfo.getTableDesc().hasFamily(family)) {
+      throw new NoSuchColumnFamilyException("Column family " +
+          Bytes.toString(family) + " does not exist in region " + this
+            + " in table " + regionInfo.getTableDesc());
+    }
+  }
 }
Index: src/java/org/apache/hadoop/hbase/regionserver/Store.java
===================================================================
--- src/java/org/apache/hadoop/hbase/regionserver/Store.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/regionserver/Store.java	(working copy)
@@ -51,7 +51,9 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.SequenceFile;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.io.hfile.HFile;
@@ -304,9 +306,8 @@
         }
         // Check this edit is for me. Also, guard against writing the special
         // METACOLUMN info such as HBASE::CACHEFLUSH entries
-        if (/* Commented out for now -- St.Ack val.isTransactionEntry() ||*/
-            val.matchingColumnNoDelimiter(HLog.METACOLUMN,
-              HLog.METACOLUMN.length - 1) ||
+        if (/* commented out for now - stack via jgray key.isTransactionEntry() || */
+            val.matchingFamily(HLog.METAFAMILY) ||
           !Bytes.equals(key.getRegionName(), regioninfo.getRegionName()) ||
           !val.matchingFamily(family.getName())) {
           continue;
@@ -388,6 +389,21 @@
       lock.readLock().unlock();
     }
   }
+  
+  /**
+   * Adds a value to the memcache
+   * 
+   * @param kv
+   * @return memcache size delta
+   */
+  protected long delete(final KeyValue kv) {
+    lock.readLock().lock();
+    try {
+      return this.memcache.delete(kv);
+    } finally {
+      lock.readLock().unlock();
+    }
+  }
 
   /**
    * @return All store files.
@@ -1117,7 +1133,7 @@
     // if the column pattern is not null, we use it for column matching.
     // we will skip the keys whose column doesn't match the pattern.
     if (columnPattern != null) {
-      if (!(columnPattern.matcher(candidate.getColumnString()).matches())) {
+      if (!(columnPattern.matcher(Bytes.toString(candidate.getColumn())).matches())) {
         return false;
       }
     }
@@ -1685,13 +1701,12 @@
   /**
    * Return a scanner for both the memcache and the HStore files
    */
-  protected InternalScanner getScanner(long timestamp,
-      final NavigableSet<byte []> targetCols,
-      byte [] firstRow, RowFilterInterface filter)
+  protected KeyValueScanner getScanner(Scan scan,
+      final NavigableSet<byte []> targetCols, byte [] firstRow)
   throws IOException {
     lock.readLock().lock();
     try {
-      return new StoreScanner(this, targetCols, firstRow, timestamp, filter);
+      return new StoreScanner(this, scan, targetCols, firstRow);
     } finally {
       lock.readLock().unlock();
     }
@@ -1797,4 +1812,48 @@
     }
     return false;
   }
+  
+  //
+  // HBASE-880/1249/1304
+  //
+  
+  /**
+   * Retrieve results from this store given the specified Get parameters.
+   * @param get Get operation
+   * @param columns List of columns to match, can be empty (not null)
+   * @param result List to add results to 
+   * @param keyComparator
+   * @throws IOException
+   */
+  public void get(Get get, NavigableSet<byte[]> columns, List<KeyValue> result,
+      KeyComparator keyComparator) 
+  throws IOException {
+    
+    // Column matching and version enforcement
+    QueryMatcher matcher = new QueryMatcher(get, get.getRow(), 
+        this.family.getName(), columns, this.ttl, keyComparator);
+    
+    // Read from Memcache
+    if(this.memcache.get(matcher, result)) {
+      // Received early-out from memcache
+      return;
+    }
+    
+    // Check if we even have storefiles
+    if(this.storefiles.isEmpty()) {
+      return;
+    }
+    
+    // Get storefiles for this store
+    List<HFileScanner> storefileScanners = new ArrayList<HFileScanner>();
+    for(StoreFile sf : this.storefiles.descendingMap().values()) {
+      storefileScanners.add(sf.getReader().getScanner());
+    }
+    
+    // StoreFileScan will handle reading this store's storefiles
+    StoreFileScan scanner = new StoreFileScan(storefileScanners, matcher);
+    
+    // Run a GET scan and put results into the specified list 
+    scanner.get(result);
+  }
 }
Index: src/java/org/apache/hadoop/hbase/rest/TableModel.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/TableModel.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/rest/TableModel.java	(working copy)
@@ -30,7 +30,8 @@
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
 import org.apache.hadoop.hbase.rest.serializer.IRestSerializer;
 import org.apache.hadoop.hbase.rest.serializer.ISerializable;
@@ -48,7 +49,7 @@
   }
 
   // Get Methods
-  public RowResult[] get(byte[] tableName) throws HBaseRestException {
+  public Result[] get(byte [] tableName) throws HBaseRestException {
     return get(tableName, getColumns(tableName));
   }
 
@@ -63,26 +64,28 @@
    * @return resultant rows
    * @throws org.apache.hadoop.hbase.rest.exception.HBaseRestException
    */
-  public RowResult[] get(byte[] tableName, byte[][] columnNames)
+  public Result[] get(byte [] tableName, byte[][] columnNames)
       throws HBaseRestException {
     try {
-      ArrayList<RowResult> a = new ArrayList<RowResult>();
+      ArrayList<Result> a = new ArrayList<Result>();
       HTable table = new HTable(tableName);
 
-      Scanner s = table.getScanner(columnNames);
-      RowResult r;
+      Scan scan = new Scan();
+      scan.addColumns(columnNames);
+      Scanner s = table.getScanner(scan);
+      Result r;
 
       while ((r = s.next()) != null) {
         a.add(r);
       }
 
-      return a.toArray(new RowResult[0]);
+      return a.toArray(new Result[0]);
     } catch (Exception e) {
       throw new HBaseRestException(e);
     }
   }
 
-  protected boolean doesTableExist(byte[] tableName) throws HBaseRestException {
+  protected boolean doesTableExist(byte [] tableName) throws HBaseRestException {
     try {
       return this.admin.tableExists(tableName);
     } catch (IOException e) {
@@ -90,7 +93,7 @@
     }
   }
   
-  protected void disableTable(byte[] tableName) throws HBaseRestException {
+  protected void disableTable(byte [] tableName) throws HBaseRestException {
     try {
       this.admin.disableTable(tableName);
     } catch (IOException e) {
@@ -98,7 +101,7 @@
     }
   }
   
-  protected void enableTable(byte[] tableName) throws HBaseRestException {
+  protected void enableTable(byte [] tableName) throws HBaseRestException {
     try {
       this.admin.enableTable(tableName);
     } catch (IOException e) {
@@ -110,7 +113,7 @@
       ArrayList<HColumnDescriptor> columns) throws HBaseRestException {
     HTableDescriptor htc = null;
     try {
-      htc = this.admin.getTableDescriptor(tableName);
+      htc = this.admin.getTableDescriptor(Bytes.toBytes(tableName));
     } catch (IOException e) {
       throw new HBaseRestException("Table does not exist");
     }
@@ -204,7 +207,7 @@
    *         tableName not existing.
    * @throws org.apache.hadoop.hbase.rest.exception.HBaseRestException
    */
-  public boolean post(byte[] tableName, HTableDescriptor htd)
+  public boolean post(byte [] tableName, HTableDescriptor htd)
       throws HBaseRestException {
     try {
       if (!this.admin.tableExists(tableName)) {
@@ -225,7 +228,7 @@
    * @return true if table exists and deleted, false if table does not exist.
    * @throws org.apache.hadoop.hbase.rest.exception.HBaseRestException
    */
-  public boolean delete(byte[] tableName) throws HBaseRestException {
+  public boolean delete(byte [] tableName) throws HBaseRestException {
     try {
       if (this.admin.tableExists(tableName)) {
         this.admin.disableTable(tableName);
@@ -241,7 +244,7 @@
   public static class Regions implements ISerializable {
     byte[][] regionKey;
 
-    public Regions(byte[][] bs) {
+    public Regions(byte [][] bs) {
       super();
       this.regionKey = bs;
     }
Index: src/java/org/apache/hadoop/hbase/rest/ScannerModel.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/ScannerModel.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/rest/ScannerModel.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -31,7 +31,8 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Scanner;
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.rest.descriptors.ScannerIdentifier;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
 
@@ -79,7 +80,7 @@
   protected static final ScannerMaster scannerMaster = new ScannerMaster();
 
   /**
-   * returns the next numResults RowResults from the Scaner mapped to Integer
+   * returns the next numResults Results from the Scaner mapped to Integer
    * id. If the end of the table is reached, the scanner is closed and all
    * succesfully retrieved rows are returned.
    * 
@@ -90,14 +91,14 @@
    * @return all successfully retrieved rows.
    * @throws org.apache.hadoop.hbase.rest.exception.HBaseRestException
    */
-  public RowResult[] scannerGet(Integer id, Long numRows)
+  public Result[] scannerGet(Integer id, Long numRows)
       throws HBaseRestException {
     try {
-      ArrayList<RowResult> a;
+      ArrayList<Result> a;
       Scanner s;
-      RowResult r;
+      Result r;
 
-      a = new ArrayList<RowResult>();
+      a = new ArrayList<Result>();
       s = scannerMaster.getScanner(id);
 
       if (s == null) {
@@ -114,7 +115,7 @@
         }
       }
 
-      return a.toArray(new RowResult[0]);
+      return a.toArray(new Result[0]);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -129,13 +130,13 @@
    * @return all rows till end of table
    * @throws org.apache.hadoop.hbase.rest.exception.HBaseRestException
    */
-  public RowResult[] scannerGet(Integer id) throws HBaseRestException {
+  public Result[] scannerGet(Integer id) throws HBaseRestException {
     try {
-      ArrayList<RowResult> a;
+      ArrayList<Result> a;
       Scanner s;
-      RowResult r;
+      Result r;
 
-      a = new ArrayList<RowResult>();
+      a = new ArrayList<Result>();
       s = scannerMaster.getScanner(id);
 
       while ((r = s.next()) != null) {
@@ -144,7 +145,7 @@
 
       scannerMaster.scannerClose(id);
 
-      return a.toArray(new RowResult[0]);
+      return a.toArray(new Result[0]);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -208,8 +209,11 @@
     try {
       HTable table;
       table = new HTable(tableName);
+      Scan scan = new Scan();
+      scan.addColumns(columns);
+      scan.setTimeRange(timestamp, 0);
       return new ScannerIdentifier(scannerMaster.addScanner(table.getScanner(
-          columns, HConstants.EMPTY_START_ROW, timestamp)));
+          scan)));
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -225,8 +229,11 @@
     try {
       HTable table;
       table = new HTable(tableName);
+      Scan scan = new Scan(startRow);
+      scan.addColumns(columns);
+      scan.setTimeRange(timestamp, 0);
       return new ScannerIdentifier(scannerMaster.addScanner(table.getScanner(
-          columns, startRow, timestamp)));
+          scan)));
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -243,8 +250,12 @@
     try {
       HTable table;
       table = new HTable(tableName);
+      Scan scan = new Scan();
+      scan.addColumns(columns);
+      scan.setTimeRange(timestamp, 0);
+      scan.setFilter(filter);
       return new ScannerIdentifier(scannerMaster.addScanner(table.getScanner(
-          columns, HConstants.EMPTY_START_ROW, timestamp, filter)));
+          scan)));
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -261,8 +272,12 @@
     try {
       HTable table;
       table = new HTable(tableName);
+      Scan scan = new Scan(startRow);
+      scan.addColumns(columns);
+      scan.setTimeRange(timestamp, 0);
+      scan.setFilter(filter);
       return new ScannerIdentifier(scannerMaster.addScanner(table.getScanner(
-          columns, startRow, timestamp, filter)));
+          scan)));
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
Index: src/java/org/apache/hadoop/hbase/rest/RowController.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/RowController.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/rest/RowController.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.rest.descriptors.RowUpdateDescriptor;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
@@ -82,7 +83,6 @@
       throws HBaseRestException {
     RowModel innerModel = getModel();
 
-    BatchUpdate b;
     RowUpdateDescriptor rud = parser
         .getRowUpdateDescriptor(input, pathSegments);
 
@@ -92,14 +92,14 @@
       return;
     }
 
-    b = new BatchUpdate(rud.getRowName());
+    Put put = new Put(Bytes.toBytes(rud.getRowName()));
 
     for (byte[] key : rud.getColVals().keySet()) {
-      b.put(key, rud.getColVals().get(key));
+      put.add(key, rud.getColVals().get(key));
     }
 
     try {
-      innerModel.post(rud.getTableName().getBytes(), b);
+      innerModel.post(rud.getTableName().getBytes(), put);
       s.setOK();
     } catch (HBaseRestException e) {
       s.setUnsupportedMediaType(e.getMessage());
Index: src/java/org/apache/hadoop/hbase/rest/TimestampModel.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/TimestampModel.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/rest/TimestampModel.java	(working copy)
@@ -23,10 +23,13 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
 
@@ -43,7 +46,8 @@
       throws HBaseRestException {
     try {
       HTable table = new HTable(tableName);
-      table.deleteAll(rowName, timestamp);
+      Delete delete = new Delete(rowName, timestamp, null);
+      table.delete(delete);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -54,7 +58,14 @@
     try {
       HTable table = new HTable(tableName);
       for (byte[] column : columns) {
-        table.deleteAll(rowName, column, timestamp);
+        Delete delete  = new Delete(rowName, timestamp, null);
+        byte [][] famAndQf = KeyValue.parseColumn(column);
+        if(famAndQf[1].length == 0){
+          delete.deleteFamily(famAndQf[0]);
+        } else {
+          delete.deleteColumn(famAndQf[0], famAndQf[1]);
+        }
+        table.delete(delete);
       }
     } catch (IOException e) {
       throw new HBaseRestException(e);
@@ -111,14 +122,11 @@
   public void post(byte[] tableName, byte[] rowName, byte[] columnName,
       long timestamp, byte[] value) throws HBaseRestException {
     try {
-      HTable table;
-      BatchUpdate b;
-
-      table = new HTable(tableName);
-      b = new BatchUpdate(rowName, timestamp);
-
-      b.put(columnName, value);
-      table.commit(b);
+      HTable table = new HTable(tableName);
+      Put put = new Put(rowName);
+      put.setTimeStamp(timestamp);
+      put.add(columnName, value);
+      table.put(put);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
Index: src/java/org/apache/hadoop/hbase/rest/RowModel.java
===================================================================
--- src/java/org/apache/hadoop/hbase/rest/RowModel.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/rest/RowModel.java	(working copy)
@@ -25,9 +25,12 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.rest.descriptors.TimestampsDescriptor;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
@@ -98,20 +101,20 @@
 
   }
 
-  public void post(byte[] tableName, BatchUpdate b) throws HBaseRestException {
+  public void post(byte[] tableName, Put put) throws HBaseRestException {
     try {
       HTable table = new HTable(tableName);
-      table.commit(b);
+      table.put(put);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
   }
 
-  public void post(byte[] tableName, List<BatchUpdate> b)
+  public void post(byte[] tableName, List<Put> puts)
       throws HBaseRestException {
     try {
       HTable table = new HTable(tableName);
-      table.commit(b);
+      table.put(puts);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -121,7 +124,8 @@
       throws HBaseRestException {
     try {
       HTable table = new HTable(tableName);
-      table.deleteAll(rowName);
+      Delete delete = new Delete(rowName);
+      table.delete(delete);
     } catch (IOException e) {
       throw new HBaseRestException(e);
     }
@@ -131,7 +135,14 @@
     try {
       HTable table = new HTable(tableName);
       for (byte[] column : columns) {
-        table.deleteAll(rowName, column);
+        Delete delete  = new Delete(rowName);
+        byte [][] famAndQf = KeyValue.parseColumn(column);
+        if(famAndQf[1].length == 0){
+          delete.deleteFamily(famAndQf[0]);
+        } else {
+          delete.deleteColumn(famAndQf[0], famAndQf[1]);
+        }
+        table.delete(delete);
       }
     } catch (IOException e) {
       throw new HBaseRestException(e);
Index: src/java/org/apache/hadoop/hbase/HTableDescriptor.java
===================================================================
--- src/java/org/apache/hadoop/hbase/HTableDescriptor.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/HTableDescriptor.java	(working copy)
@@ -27,10 +27,11 @@
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
+import java.util.Set;
 import java.util.TreeMap;
 
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
+//import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.rest.exception.HBaseRestException;
@@ -45,7 +46,8 @@
  * HTableDescriptor contains the name of an HTable, and its
  * column families.
  */
-public class HTableDescriptor implements WritableComparable<HTableDescriptor>, ISerializable {
+public class HTableDescriptor implements WritableComparable<HTableDescriptor>,
+ISerializable {
 
   // Changes prior to version 3 were not recorded here.
   // Version 3 adds metadata as a map where keys and values are byte[].
@@ -102,10 +104,10 @@
   // Key is hash of the family name.
   private final Map<byte [], HColumnDescriptor> families =
     new TreeMap<byte [], HColumnDescriptor>(KeyValue.FAMILY_COMPARATOR);
-
+  
   // Key is indexId
-  private final Map<String, IndexSpecification> indexes =
-    new HashMap<String, IndexSpecification>();
+//  private final Map<String, IndexSpecification> indexes =
+//    new HashMap<String, IndexSpecification>();
   
   /**
    * Private constructor used internally creating table descriptors for 
@@ -124,24 +126,41 @@
    * Private constructor used internally creating table descriptors for 
    * catalog tables: e.g. .META. and -ROOT-.
    */
+//  protected HTableDescriptor(final byte [] name, HColumnDescriptor[] families,
+//      Collection<IndexSpecification> indexes,
+//       Map<ImmutableBytesWritable,ImmutableBytesWritable> values) {
+//    this.name = name.clone();
+//    this.nameAsString = Bytes.toString(this.name);
+//    setMetaFlags(name);
+//    for(HColumnDescriptor descriptor : families) {
+//      this.families.put(descriptor.getName(), descriptor);
+//    }
+//    for(IndexSpecification index : indexes) {
+//      this.indexes.put(index.getIndexId(), index);
+//    }
+//    for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry:
+//        values.entrySet()) {
+//      this.values.put(entry.getKey(), entry.getValue());
+//    }
+//  }
   protected HTableDescriptor(final byte [] name, HColumnDescriptor[] families,
-      Collection<IndexSpecification> indexes,
-       Map<ImmutableBytesWritable,ImmutableBytesWritable> values) {
+      Map<ImmutableBytesWritable,ImmutableBytesWritable> values) {
     this.name = name.clone();
     this.nameAsString = Bytes.toString(this.name);
     setMetaFlags(name);
     for(HColumnDescriptor descriptor : families) {
       this.families.put(descriptor.getName(), descriptor);
     }
-    for(IndexSpecification index : indexes) {
-      this.indexes.put(index.getIndexId(), index);
-    }
+//    for(IndexSpecification index : indexes) {
+//      this.indexes.put(index.getIndexId(), index);
+//    }
     for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> entry:
         values.entrySet()) {
       this.values.put(entry.getKey(), entry.getValue());
     }
   }
-
+  
+  
   /**
    * Constructs an empty object.
    * For deserializing an HTableDescriptor instance only.
@@ -197,7 +216,7 @@
         desc.values.entrySet()) {
       this.values.put(e.getKey(), e.getValue());
     }
-    this.indexes.putAll(desc.indexes);
+//    this.indexes.putAll(desc.indexes);
   }
 
   /*
@@ -436,17 +455,17 @@
       Bytes.toBytes(Integer.toString(memcacheFlushSize)));
   }
     
-  public Collection<IndexSpecification> getIndexes() {
-    return indexes.values();
-  }
-  
-  public IndexSpecification getIndex(String indexId) {
-    return indexes.get(indexId);
-  }
-  
-  public void addIndex(IndexSpecification index) {
-    indexes.put(index.getIndexId(), index);
-  }
+//  public Collection<IndexSpecification> getIndexes() {
+//    return indexes.values();
+//  }
+//  
+//  public IndexSpecification getIndex(String indexId) {
+//    return indexes.get(indexId);
+//  }
+//  
+//  public void addIndex(IndexSpecification index) {
+//    indexes.put(index.getIndexId(), index);
+//  }
 
   /**
    * Adds a column family.
@@ -505,13 +524,13 @@
     s.append(FAMILIES);
     s.append(" => ");
     s.append(families.values());
-    if (!indexes.isEmpty()) {
-      // Don't emit if empty.  Has to do w/ transactional hbase.
-      s.append(", ");
-      s.append("INDEXES");
-      s.append(" => ");
-      s.append(indexes.values());
-    }
+//    if (!indexes.isEmpty()) {
+//      // Don't emit if empty.  Has to do w/ transactional hbase.
+//      s.append(", ");
+//      s.append("INDEXES");
+//      s.append(" => ");
+//      s.append(indexes.values());
+//    }
     s.append('}');
     return s.toString();
   }
@@ -576,16 +595,16 @@
       c.readFields(in);
       families.put(c.getName(), c);
     }
-    indexes.clear();
+//    indexes.clear();
     if (version < 4) {
       return;
     }
-    int numIndexes = in.readInt();
-    for (int i = 0; i < numIndexes; i++) {
-      IndexSpecification index = new IndexSpecification();
-      index.readFields(in);
-      addIndex(index);
-    }
+//    int numIndexes = in.readInt();
+//    for (int i = 0; i < numIndexes; i++) {
+//      IndexSpecification index = new IndexSpecification();
+//      index.readFields(in);
+//      addIndex(index);
+//    }
   }
 
   public void write(DataOutput out) throws IOException {
@@ -605,10 +624,10 @@
       HColumnDescriptor family = it.next();
       family.write(out);
     }
-    out.writeInt(indexes.size());
-    for(IndexSpecification index : indexes.values()) {
-      index.write(out);
-    }
+//    out.writeInt(indexes.size());
+//    for(IndexSpecification index : indexes.values()) {
+//      index.write(out);
+//    }
   }
 
   // Comparable
@@ -649,6 +668,13 @@
     return Collections.unmodifiableCollection(this.families.values());
   }
   
+  /**
+   * @return Immutable sorted set of the keys of the families.
+   */
+  public Set<byte[]> getFamiliesKeys() {
+    return Collections.unmodifiableSet(this.families.keySet());
+  }
+  
   @TOJSON(fieldName = "columns")
   public HColumnDescriptor[] getColumnFamilies() {
     return getFamilies().toArray(new HColumnDescriptor[0]);
Index: src/java/org/apache/hadoop/hbase/WritableComparator.java
===================================================================
--- src/java/org/apache/hadoop/hbase/WritableComparator.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/WritableComparator.java	(working copy)
@@ -23,6 +23,7 @@
 
 import org.apache.hadoop.io.Writable;
 
-public interface WritableComparator<T> extends Writable, Comparator<T> {
-// No methods, just bring the two interfaces together
-}
+/**
+ * Interface that brings writable and comparable together
+ */
+public interface WritableComparator<T> extends Writable, Comparator<T> {}
Index: src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/ProcessServerShutdown.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -35,7 +35,9 @@
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Writables;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 
 /** 
  * Instantiated when a server's lease has expired, meaning it has crashed.
@@ -94,7 +96,7 @@
     List<byte []> emptyRows = new ArrayList<byte []>();
     try {
       while (true) {
-        RowResult values = null;
+        Result values = null;
         try {
           values = server.next(scannerId);
         } catch (IOException e) {
@@ -110,8 +112,10 @@
         // shutdown server but that would mean that we'd reassign regions that
         // were already out being assigned, ones that were product of a split
         // that happened while the shutdown was being processed.
-        String serverAddress = Writables.cellToString(values.get(COL_SERVER));
-        long startCode = Writables.cellToLong(values.get(COL_STARTCODE)); 
+        String serverAddress = 
+          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
+        long startCode =
+          Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
         String serverName = null;
         if (serverAddress != null && serverAddress.length() > 0) {
           serverName = HServerInfo.getServerName(serverAddress, startCode);
@@ -126,7 +130,7 @@
             Bytes.toString(row));
         }
 
-        HRegionInfo info = master.getHRegionInfo(row, values);
+        HRegionInfo info = master.getHRegionInfo(row, values.rowResult());
         if (info == null) {
           emptyRows.add(row);
           continue;
@@ -202,9 +206,10 @@
         LOG.debug("process server shutdown scanning root region on " +
             master.getRootRegionLocation().getBindAddress());
       }
+      Scan scan = new Scan();
+      scan.addFamily(CATALOG_FAMILY);
       long scannerId = server.openScanner(
-          HRegionInfo.ROOT_REGIONINFO.getRegionName(), COLUMN_FAMILY_ARRAY,
-          EMPTY_START_ROW, HConstants.LATEST_TIMESTAMP, null);
+          HRegionInfo.ROOT_REGIONINFO.getRegionName(), EMPTY_START_ROW, scan);
       scanMetaRegion(server, scannerId,
           HRegionInfo.ROOT_REGIONINFO.getRegionName());
       return true;
@@ -221,9 +226,10 @@
         LOG.debug("process server shutdown scanning " +
           Bytes.toString(m.getRegionName()) + " on " + m.getServer());
       }
-      long scannerId =
-        server.openScanner(m.getRegionName(), COLUMN_FAMILY_ARRAY,
-        EMPTY_START_ROW, HConstants.LATEST_TIMESTAMP, null);
+      Scan scan = new Scan();
+      scan.addFamily(CATALOG_FAMILY);
+      long scannerId = server.openScanner(
+          HRegionInfo.ROOT_REGIONINFO.getRegionName(), EMPTY_START_ROW, scan);
       scanMetaRegion(server, scannerId, m.getRegionName());
       return true;
     }
Index: src/java/org/apache/hadoop/hbase/master/HMaster.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/HMaster.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/HMaster.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -27,6 +27,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.NavigableMap;
 import java.util.Random;
 import java.util.Set;
 import java.util.concurrent.BlockingQueue;
@@ -47,8 +48,8 @@
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
 import org.apache.hadoop.hbase.HServerLoad;
-import org.apache.hadoop.hbase.HStoreKey;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.LocalHBaseCluster;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.RegionHistorian;
@@ -58,8 +59,10 @@
 import org.apache.hadoop.hbase.client.ServerConnection;
 import org.apache.hadoop.hbase.client.ServerConnectionManager;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Get;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
 import org.apache.hadoop.hbase.ipc.HBaseServer;
@@ -658,12 +661,14 @@
     byte [] metaRegionName = m.getRegionName();
     HRegionInterface srvr = connection.getHRegionConnection(m.getServer());
     byte[] firstRowInTable = Bytes.toBytes(tableName + ",,");
-    long scannerid = srvr.openScanner(metaRegionName, COL_REGIONINFO_ARRAY,
-        firstRowInTable, LATEST_TIMESTAMP, null);
+    Scan scan = new Scan(firstRowInTable);
+    scan.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
+    long scannerid = srvr.openScanner(metaRegionName, firstRowInTable, scan);
     try {
-      RowResult data = srvr.next(scannerid);
+      Result data = srvr.next(scannerid);
       if (data != null && data.size() > 0) {
-        HRegionInfo info = Writables.getHRegionInfo(data.get(COL_REGIONINFO));
+        HRegionInfo info = Writables.getHRegionInfo(
+            data.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
         if (info.getTableDesc().getNameAsString().equals(tableName)) {
           // A region for this table already exists. Ergo table exists.
           throw new TableExistsException(tableName);
@@ -696,7 +701,7 @@
 
   public void deleteColumn(final byte [] tableName, final byte [] c)
   throws IOException {
-    new DeleteColumn(this, tableName, HStoreKey.getFamily(c)).process();
+    new DeleteColumn(this, tableName, KeyValue.parseColumn(c)[0]).process();
   }
 
   public void enableTable(final byte [] tableName) throws IOException {
@@ -722,23 +727,23 @@
     for (MetaRegion m: regions) {
       byte [] metaRegionName = m.getRegionName();
       HRegionInterface srvr = connection.getHRegionConnection(m.getServer());
+      Scan scan = new Scan(firstRowInTable);
+      scan.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
+      scan.addColumn(CATALOG_FAMILY, SERVER_QUALIFIER);
       long scannerid = 
-        srvr.openScanner(metaRegionName, 
-          new byte[][] {COL_REGIONINFO, COL_SERVER},
-          firstRowInTable, 
-          LATEST_TIMESTAMP, 
-          null);
+        srvr.openScanner(metaRegionName, firstRowInTable, scan);
       try {
         while (true) {
-          RowResult data = srvr.next(scannerid);
+          Result data = srvr.next(scannerid);
           if (data == null || data.size() <= 0)
             break;
-          HRegionInfo info = Writables.getHRegionInfo(data.get(COL_REGIONINFO));
+          HRegionInfo info = Writables.getHRegionInfo(
+              data.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
           if (Bytes.compareTo(info.getTableDesc().getName(), tableName) == 0) {
-            Cell cell = data.get(COL_SERVER);
-            if (cell != null) {
+            byte [] value = data.getValue(CATALOG_FAMILY, SERVER_QUALIFIER);
+            if (value != null) {
               HServerAddress server =
-                new HServerAddress(Bytes.toString(cell.getValue()));
+                new HServerAddress(Bytes.toString(value));
               result.add(new Pair<HRegionInfo,HServerAddress>(info, server));
             }
           } else {
@@ -760,25 +765,25 @@
       byte [] firstRowInTable = Bytes.toBytes(Bytes.toString(tableName) + ",,");
       byte [] metaRegionName = m.getRegionName();
       HRegionInterface srvr = connection.getHRegionConnection(m.getServer());
+      Scan scan = new Scan(firstRowInTable);
+      scan.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
+      scan.addColumn(CATALOG_FAMILY, SERVER_QUALIFIER);
       long scannerid = 
-          srvr.openScanner(metaRegionName, 
-            new byte[][] {COL_REGIONINFO, COL_SERVER},
-            firstRowInTable, 
-            LATEST_TIMESTAMP, 
-            null);
+        srvr.openScanner(metaRegionName, firstRowInTable, scan);
       try {
         while (true) {
-          RowResult data = srvr.next(scannerid);
+          Result data = srvr.next(scannerid);
           if (data == null || data.size() <= 0)
             break;
-          HRegionInfo info = Writables.getHRegionInfo(data.get(COL_REGIONINFO));
+          HRegionInfo info = Writables.getHRegionInfo(
+              data.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
           if (Bytes.compareTo(info.getTableDesc().getName(), tableName) == 0) {
             if ((Bytes.compareTo(info.getStartKey(), rowKey) >= 0) &&
                 (Bytes.compareTo(info.getEndKey(), rowKey) < 0)) {
-                Cell cell = data.get(COL_SERVER);
-                if (cell != null) {
+                byte [] value = data.getValue(CATALOG_FAMILY, SERVER_QUALIFIER);
+                if (value != null) {
                   HServerAddress server =
-                    new HServerAddress(Bytes.toString(cell.getValue()));
+                    new HServerAddress(Bytes.toString(value));
                   return new Pair<HRegionInfo,HServerAddress>(info, server);
                 }
             }
@@ -801,15 +806,17 @@
     for (MetaRegion m: regions) {
       byte [] metaRegionName = m.getRegionName();
       HRegionInterface srvr = connection.getHRegionConnection(m.getServer());
-      RowResult data = srvr.getRow(metaRegionName, regionName, 
-        new byte[][] {COL_REGIONINFO, COL_SERVER},
-        HConstants.LATEST_TIMESTAMP, 1, -1L);
+      Get get = new Get(regionName);
+      get.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
+      get.addColumn(CATALOG_FAMILY, SERVER_QUALIFIER);
+      Result data = srvr.get(metaRegionName, get);
       if(data == null || data.size() <= 0) continue;
-      HRegionInfo info = Writables.getHRegionInfo(data.get(COL_REGIONINFO));
-      Cell cell = data.get(COL_SERVER);
-      if(cell != null) {
+      HRegionInfo info = Writables.getHRegionInfo(
+          data.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
+      byte [] value = data.getValue(CATALOG_FAMILY, SERVER_QUALIFIER);
+      if(value != null) {
         HServerAddress server =
-          new HServerAddress(Bytes.toString(cell.getValue()));
+          new HServerAddress(Bytes.toString(value));
         return new Pair<HRegionInfo,HServerAddress>(info, server);
       }
     }
@@ -820,15 +827,18 @@
    * Get row from meta table.
    * @param row
    * @param columns
-   * @return RowResult
+   * @return Result
    * @throws IOException
    */
-  protected RowResult getFromMETA(final byte [] row, final byte [][] columns)
+  protected Result getFromMETA(final byte [] row, final byte [][] columns)
   throws IOException {
     MetaRegion meta = this.regionManager.getMetaRegionForRow(row);
     HRegionInterface srvr = getMETAServer(meta);
-    return srvr.getRow(meta.getRegionName(), row, columns,
-      HConstants.LATEST_TIMESTAMP, 1, -1);
+
+    Get get = new Get(row);
+    get.addColumns(columns);
+    
+    return srvr.get(meta.getRegionName(), get);
   }
   
   /*
@@ -891,12 +901,13 @@
       if (args.length == 2) {
         servername = Bytes.toString(((ImmutableBytesWritable)args[1]).get());
       }
-      // Need hri
-      RowResult rr = getFromMETA(regionname, HConstants.COLUMN_FAMILY_ARRAY);
-      HRegionInfo hri = getHRegionInfo(rr.getRow(), rr);
+      // Need hri 
+      Result rr = getFromMETA(regionname, HConstants.COLUMN_FAMILY_ARRAY);
+      HRegionInfo hri = getHRegionInfo(rr.getRow(), rr.rowResult());
       if (servername == null) {
         // Get server from the .META. if it wasn't passed as argument
-        servername = Writables.cellToString(rr.get(COL_SERVER));
+        servername = 
+          Bytes.toString(rr.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
       }
       LOG.info("Marking " + hri.getRegionNameAsString() +
         " as closed on " + servername + "; cleaning SERVER + STARTCODE; " +
@@ -939,6 +950,25 @@
    * @return Null or found HRegionInfo.
    * @throws IOException
    */
+  HRegionInfo getHRegionInfo(final byte [] row, final Result res)
+  throws IOException {
+    byte [] regioninfo = res.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
+    if (regioninfo == null) {
+      StringBuilder sb =  new StringBuilder();
+      NavigableMap<byte[], byte[]> infoMap = res.getFamilyMap(CATALOG_FAMILY);
+      for (byte [] e: infoMap.keySet()) {
+        if (sb.length() > 0) {
+          sb.append(", ");
+        }
+        sb.append(Bytes.toString(CATALOG_FAMILY) + ":" + Bytes.toString(e));
+      }
+      LOG.warn(Bytes.toString(COL_REGIONINFO) + " is empty for row: " +
+         Bytes.toString(row) + "; has keys: " + sb.toString());
+      return null;
+    }
+    return Writables.getHRegionInfo(regioninfo);
+  }
+
   HRegionInfo getHRegionInfo(final byte [] row, final Map<byte [], Cell> map)
   throws IOException {
     Cell regioninfo = map.get(COL_REGIONINFO);
@@ -999,7 +1029,6 @@
     System.exit(0);
   }
 
-  @SuppressWarnings("null")
   protected static void doMain(String [] args,
       Class<? extends HMaster> masterClass) {
 
Index: src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/ProcessRegionOpen.java	(working copy)
@@ -26,7 +26,9 @@
 import org.apache.hadoop.hbase.HServerInfo;
 import org.apache.hadoop.hbase.RegionHistorian;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Writables;
 
 /** 
  * ProcessRegionOpen is instantiated when a region server reports that it is
@@ -80,11 +82,14 @@
               " in region " + Bytes.toString(metaRegionName) + " with " +
               " with startcode " + serverInfo.getStartCode() + " and server " +
               serverInfo.getServerAddress());
-          BatchUpdate b = new BatchUpdate(regionInfo.getRegionName());
-          b.put(COL_SERVER,
+
+          Put put = new Put(regionInfo.getRegionName());
+          put.add(CATALOG_FAMILY, SERVER_QUALIFIER, 
               Bytes.toBytes(serverInfo.getServerAddress().toString()));
-          b.put(COL_STARTCODE, Bytes.toBytes(serverInfo.getStartCode()));
-          server.batchUpdate(metaRegionName, b, -1L);
+          put.add(CATALOG_FAMILY, STARTCODE_QUALIFIER, 
+              Bytes.toBytes(serverInfo.getStartCode()));
+          server.put(metaRegionName, put);
+          
           if (!this.historian.isOnline()) {
             // This is safest place to do the onlining of the historian in
             // the master.  When we get to here, we know there is a .META.
Index: src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java	(working copy)
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
 
@@ -50,9 +51,9 @@
   protected void updateRegionInfo(HRegionInterface server, byte [] regionName,
     HRegionInfo i)
   throws IOException {
-    BatchUpdate b = new BatchUpdate(i.getRegionName());
-    b.put(COL_REGIONINFO, Writables.getBytes(i));
-    server.batchUpdate(regionName, b, -1L);
+    Put put = new Put(i.getRegionName());
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(i));
+    server.put(regionName, put);
     LOG.debug("updated HTableDescriptor for region " + i.getRegionNameAsString());
   }
 
Index: src/java/org/apache/hadoop/hbase/master/TableOperation.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/TableOperation.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/TableOperation.java	(working copy)
@@ -31,7 +31,9 @@
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.TableNotFoundException;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Writables;
@@ -80,15 +82,16 @@
       // Open a scanner on the meta region
       byte [] tableNameMetaStart =
           Bytes.toBytes(Bytes.toString(tableName) + ",,");
+      Scan scan = new Scan(tableNameMetaStart);
+      scan.addFamily(CATALOG_FAMILY);
+      long scannerId = server.openScanner(m.getRegionName(), tableNameMetaStart,
+          scan);
 
-      long scannerId = server.openScanner(m.getRegionName(),
-          COLUMN_FAMILY_ARRAY, tableNameMetaStart, HConstants.LATEST_TIMESTAMP, null);
-
       List<byte []> emptyRows = new ArrayList<byte []>();
       try {
         while (true) {
-          RowResult values = server.next(scannerId);
-          if(values == null || values.size() == 0) {
+          Result values = server.next(scannerId);
+          if(values == null || values.isEmpty()) {
             break;
           }
           HRegionInfo info = this.master.getHRegionInfo(values.getRow(), values);
@@ -98,8 +101,10 @@
                       Bytes.toString(values.getRow()));
             continue;
           }
-          String serverAddress = Writables.cellToString(values.get(COL_SERVER));
-          long startCode = Writables.cellToLong(values.get(COL_STARTCODE)); 
+          String serverAddress = 
+            Bytes.toString(values.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
+          long startCode = 
+            Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER)); 
           String serverName = null;
           if (serverAddress != null && serverAddress.length() > 0) {
             serverName = HServerInfo.getServerName(serverAddress, startCode);
Index: src/java/org/apache/hadoop/hbase/master/RegionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/RegionManager.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/RegionManager.java	(working copy)
@@ -55,6 +55,7 @@
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Writables;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper;
 
@@ -673,10 +674,12 @@
     // 3. Insert into meta
     HRegionInfo info = region.getRegionInfo();
     byte [] regionName = region.getRegionName();
-    BatchUpdate b = new BatchUpdate(regionName);
-    b.put(COL_REGIONINFO, Writables.getBytes(info));
-    server.batchUpdate(metaRegionName, b, -1L);
     
+
+    Put put = new Put(regionName);
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(info));
+    server.put(metaRegionName, put);
+    
     // 4. Close the new region to flush it to disk.  Close its log file too.
     region.close();
     region.getLog().closeAndDelete();
Index: src/java/org/apache/hadoop/hbase/master/ColumnOperation.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ColumnOperation.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/ColumnOperation.java	(working copy)
@@ -26,6 +26,7 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Writables;
 
@@ -47,9 +48,9 @@
 
   protected void updateRegionInfo(HRegionInterface server, byte [] regionName,
     HRegionInfo i) throws IOException {
-    BatchUpdate b = new BatchUpdate(i.getRegionName());
-    b.put(COL_REGIONINFO, Writables.getBytes(i));
-    server.batchUpdate(regionName, b, -1L);
+    Put put = new Put(i.getRegionName());
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(i));
+    server.put(regionName, put);
     if (LOG.isDebugEnabled()) {
       LOG.debug("updated columns in row: " + i.getRegionNameAsString());
     }
Index: src/java/org/apache/hadoop/hbase/master/BaseScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/BaseScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/BaseScanner.java	(working copy)
@@ -36,10 +36,14 @@
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerInfo;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.regionserver.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -147,26 +151,31 @@
 
     // Array to hold list of split parents found.  Scan adds to list.  After
     // scan we go check if parents can be removed.
-    Map<HRegionInfo, RowResult> splitParents =
-      new HashMap<HRegionInfo, RowResult>();
+    Map<HRegionInfo, Result> splitParents =
+      new HashMap<HRegionInfo, Result>();
     List<byte []> emptyRows = new ArrayList<byte []>();
     int rows = 0;
     try {
       regionServer = master.connection.getHRegionConnection(region.getServer());
+      Scan scan = new Scan();
+      scan.addFamily(HConstants.CATALOG_FAMILY);
       scannerId = regionServer.openScanner(region.getRegionName(),
-        COLUMN_FAMILY_ARRAY, EMPTY_START_ROW, HConstants.LATEST_TIMESTAMP, null);
+        EMPTY_START_ROW, scan);
       while (true) {
-        RowResult values = regionServer.next(scannerId);
+        Result values = regionServer.next(scannerId);
         if (values == null || values.size() == 0) {
           break;
         }
-        HRegionInfo info = master.getHRegionInfo(values.getRow(), values);
+        HRegionInfo info = master.getHRegionInfo(values.getRow(), 
+            values.rowResult());
         if (info == null) {
           emptyRows.add(values.getRow());
           continue;
         }
-        String serverName = Writables.cellToString(values.get(COL_SERVER));
-        long startCode = Writables.cellToLong(values.get(COL_STARTCODE));
+        String serverName = 
+          Bytes.toString(values.getValue(CATALOG_FAMILY, SERVER_QUALIFIER));
+        long startCode = 
+          Bytes.toLong(values.getValue(CATALOG_FAMILY, STARTCODE_QUALIFIER));
 
         // Note Region has been assigned.
         checkAssigned(info, serverName, startCode);
@@ -213,7 +222,7 @@
     // Take a look at split parents to see if any we can clean up.
     
     if (splitParents.size() > 0) {
-      for (Map.Entry<HRegionInfo, RowResult> e : splitParents.entrySet()) {
+      for (Map.Entry<HRegionInfo, Result> e : splitParents.entrySet()) {
         HRegionInfo hri = e.getKey();
         cleanupSplits(region.getRegionName(), regionServer, hri, e.getValue());
       }
@@ -250,7 +259,7 @@
    */
   private boolean cleanupSplits(final byte [] metaRegionName, 
     final HRegionInterface srvr, final HRegionInfo parent,
-    RowResult rowContent)
+    Result rowContent)
   throws IOException {
     boolean result = false;
     boolean hasReferencesA = hasReferences(metaRegionName, srvr,
@@ -283,11 +292,11 @@
    */
   private boolean hasReferences(final byte [] metaRegionName, 
     final HRegionInterface srvr, final byte [] parent,
-    RowResult rowContent, final byte [] splitColumn)
+    Result rowContent, final byte [] splitColumn)
   throws IOException {
     boolean result = false;
     HRegionInfo split =
-      Writables.getHRegionInfo(rowContent.get(splitColumn));
+      Writables.getHRegionInfo(rowContent.getValue(splitColumn));
     if (split == null) {
       return result;
     }
@@ -320,10 +329,15 @@
         " no longer has references to " + Bytes.toString(parent));
     }
     
-    BatchUpdate b = new BatchUpdate(parent);
-    b.delete(splitColumn);
-    srvr.batchUpdate(metaRegionName, b, -1L);
-      
+    Delete delete = new Delete(parent);
+    byte [][] famAndQf = KeyValue.parseColumn(splitColumn);
+    if(famAndQf[1].length == 0){
+      delete.deleteFamily(famAndQf[0]);
+    } else {
+      delete.deleteColumn(famAndQf[0], famAndQf[1]);
+    }
+    srvr.delete(metaRegionName, delete);
+    
     return result;
   }
 
Index: src/java/org/apache/hadoop/hbase/master/ChangeTableState.java
===================================================================
--- src/java/org/apache/hadoop/hbase/master/ChangeTableState.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/master/ChangeTableState.java	(working copy)
@@ -29,6 +29,8 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.util.Writables;
 
 /** Instantiated to enable or disable a table */
@@ -78,11 +80,13 @@
       }
 
       // Update meta table
-      BatchUpdate b = new BatchUpdate(i.getRegionName());
-      updateRegionInfo(b, i);
-      b.delete(COL_SERVER);
-      b.delete(COL_STARTCODE);
-      server.batchUpdate(m.getRegionName(), b, -1L);
+      Put put = updateRegionInfo(i);
+      server.put(m.getRegionName(), put);
+      
+      Delete delete = new Delete(i.getRegionName());
+      delete.deleteColumn(CATALOG_FAMILY, SERVER_QUALIFIER);
+      delete.deleteColumn(CATALOG_FAMILY, STARTCODE_QUALIFIER);
+      server.delete(m.getRegionName(), delete);
       if (LOG.isDebugEnabled()) {
         LOG.debug("Updated columns in row: " + i.getRegionNameAsString());
       }
@@ -125,9 +129,11 @@
     servedRegions.clear();
   }
 
-  protected void updateRegionInfo(final BatchUpdate b, final HRegionInfo i)
+  protected Put updateRegionInfo(final HRegionInfo i)
   throws IOException {
     i.setOffline(!online);
-    b.put(COL_REGIONINFO, Writables.getBytes(i));
+    Put put = new Put(i.getRegionName());
+    put.add(CATALOG_FAMILY, REGIONINFO_QUALIFIER, Writables.getBytes(i));
+    return put;
   }
 }
Index: src/java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- src/java/org/apache/hadoop/hbase/KeyValue.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/KeyValue.java	(working copy)
@@ -25,11 +25,12 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.io.HeapSize;
+import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.RawComparator;
 
 /**
  * An HBase Key/Value.  Instances of this class are immutable.  They are not
@@ -118,26 +119,43 @@
         return compare(a, 0, a.length, b, 0, b.length);
       }
     };
+  
+  /**
+   * Get the appropriate row comparator for the specified table.
+   * Hopefully we can get rid of this, I added this here because it's replacing
+   * something in HSK.  We should move completely off of that.
+   * @param tableName
+   * @return
+   */
+  public static RawComparator<byte []> getRowComparator(byte [] tableName) {
+    if(Bytes.equals(HTableDescriptor.ROOT_TABLEDESC.getName(),tableName)) {
+      return ROOT_COMPARATOR.getRawComparator();
+    }
+    if(Bytes.equals(HTableDescriptor.META_TABLEDESC.getName(), tableName)) {
+      return META_COMPARATOR.getRawComparator();
+    }
+    return COMPARATOR.getRawComparator();
+  }
 
   // Size of the timestamp and type byte on end of a key -- a long + a byte.
-  private static final int TIMESTAMP_TYPE_SIZE =
+  public static final int TIMESTAMP_TYPE_SIZE =
     Bytes.SIZEOF_LONG /* timestamp */ +
     Bytes.SIZEOF_BYTE /*keytype*/;
 
   // Size of the length shorts and bytes in key.
-  private static final int KEY_INFRASTRUCTURE_SIZE =
+  public static final int KEY_INFRASTRUCTURE_SIZE =
     Bytes.SIZEOF_SHORT /*rowlength*/ +
     Bytes.SIZEOF_BYTE /*columnfamilylength*/ +
     TIMESTAMP_TYPE_SIZE;
 
   // How far into the key the row starts at. First thing to read is the short
   // that says how long the row is.
-  private static final int ROW_OFFSET =
+  public static final int ROW_OFFSET =
     Bytes.SIZEOF_INT /*keylength*/ +
     Bytes.SIZEOF_INT /*valuelength*/;
 
   // Size of the length ints in a KeyValue datastructure.
-  private static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
+  public static final int KEYVALUE_INFRASTRUCTURE_SIZE = ROW_OFFSET;
 
   /**
    * Key type.
@@ -233,194 +251,199 @@
     this.offset = offset;
     this.length = length;
   }
+
+  /** Temporary constructors until 880/1249 is committed to remove deps */
   
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param timestamp
+   * Temporary.
    */
-  public KeyValue(final String row, final long timestamp) {
-    this(Bytes.toBytes(row), timestamp);
+  public KeyValue(final byte [] row, final byte [] column) {
+    this(row, column, HConstants.LATEST_TIMESTAMP, null);
   }
-
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts) {
+    this(row, column, ts, null);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts,
+      byte [] value) {
+    this(row, column, ts, Type.Put, value);
+  }
+  
+  public KeyValue(final byte [] row, final byte [] column, long ts, Type type,
+      byte [] value) {
+    int rlength = row == null ? 0 : row.length;
+    int vlength = value == null ? 0 : value.length;
+    int clength = column == null ? 0 : column.length;
+    this.bytes = createByteArray(row, 0, rlength, column, 0, clength,
+        ts, type, value, 0, vlength);
+    this.length = this.bytes.length;
+    this.offset = 0;
+  }
+  
+  /** Constructors that build a new backing byte array from fields */
+  
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
    * @param timestamp
    */
   public KeyValue(final byte [] row, final long timestamp) {
-    this(row, null, timestamp, Type.Put, null);
+    this(row, timestamp, Type.Put);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param timestamp
    */
-  public KeyValue(final String row, final String column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final long timestamp, Type type) {
+    this(row, null, null, timestamp, type, null);
   }
 
   /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final byte [] row, final byte [] column) {
-    this(row, column, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put);
   }
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final String row, final String column, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), value);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column, final byte [] value) {
-    this(row, column, HConstants.LATEST_TIMESTAMP, value);
-  }
-
-
-  /**
    * Constructs KeyValue structure filled with null value.
    * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * @param family family name
+   * @param qualifier column qualifier
    */
-  public KeyValue(final String row, final String column, final long ts) {
-    this(row, column, ts, null);
+  public KeyValue(final byte [] row, final byte [] family, 
+      final byte [] qualifier, final byte [] value) {
+    this(row, family, qualifier, HConstants.LATEST_TIMESTAMP, Type.Put, value);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param ts
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column, final long ts) {
-    this(row, column, ts, Type.Put);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type) {
+    this(row, family, qualifier, timestamp, type, null);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-    final long timestamp, final byte [] value) {
-    this(Bytes.toBytes(row),
-      column == null? HConstants.EMPTY_BYTE_ARRAY: Bytes.toBytes(column),
-      timestamp, value);
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, final byte[] value) {
+    this(row, family, qualifier, timestamp, Type.Put, value);
   }
-
+  
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final byte [] row, final byte [] column,
-     final long timestamp, final byte [] value) {
-    this(row, column, timestamp, Type.Put, value);
-  }
+  public KeyValue(final byte[] row, final byte[] family,
+      final byte[] qualifier, final long timestamp, Type type,
+      final byte[] value) {
+    this(row, family, qualifier, 0, qualifier==null ? 0 : qualifier.length, 
+        timestamp, type, value, 0, value==null ? 0 : value.length);
+  } 
 
   /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
+   * Constructs KeyValue structure filled with specified values.
+   * @param row row key
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
+   * @throws IllegalArgumentException
    */
-  public KeyValue(final String row, final String column,
-     final long timestamp, final Type type, final byte [] value) {
-    this(Bytes.toBytes(row), Bytes.toBytes(column), timestamp, type,
-      value);
+  public KeyValue(byte [] row, byte [] family, 
+      byte [] qualifier, int qoffset, int qlength, long timestamp, Type type, 
+      byte [] value, int voffset, int vlength) {
+    this(row, 0, row==null ? 0 : row.length, 
+        family, 0, family==null ? 0 : family.length,
+        qualifier, qoffset, qlength, timestamp, type, 
+        value, voffset, vlength);
   }
 
   /**
-   * Constructs KeyValue structure filled with null value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, null, 0, -1);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param column Column with delimiter between family and qualifier
-   * @param timestamp
-   * @param type
-   * @param value
-   */
-  public KeyValue(final byte [] row, final byte [] column,
-      final long timestamp, final Type type, final byte [] value) {
-    this(row, 0, row.length, column, 0, column == null? 0: column.length,
-      timestamp, type, value, 0, value == null? 0: value.length);
-  }
-
-  /**
-   * Constructs KeyValue structure filled with specified value.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column Column with delimiter between family and qualifier
-   * @param coffset Where to start reading the column.
-   * @param clength How long column is (including the family/qualifier delimiter.
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * Constructs KeyValue structure filled with specified values.
+   * <p>
+   * Column is split into two fields, family and qualifier.
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @throws IllegalArgumentException
    */
   public KeyValue(final byte [] row, final int roffset, final int rlength,
-      final byte [] column, final int coffset, int clength,
+      final byte [] family, final int foffset, final int flength,
+      final byte [] qualifier, final int qoffset, final int qlength,
       final long timestamp, final Type type,
-      final byte [] value, final int voffset, int vlength) {
-    this.bytes = createByteArray(row, roffset, rlength, column, coffset,
-      clength, timestamp, type, value, voffset, vlength);
+      final byte [] value, final int voffset, final int vlength) {
+    this.bytes = createByteArray(row, roffset, rlength, 
+        family, foffset, flength, qualifier, qoffset, qlength,
+        timestamp, type, value, voffset, vlength);
     this.length = bytes.length;
     this.offset = 0;
   }
 
   /**
    * Write KeyValue format into a byte array.
-   * @param row - row key (arbitrary byte array)
-   * @param roffset
-   * @param rlength
-   * @param column
-   * @param coffset
-   * @param clength
-   * @param timestamp
-   * @param type
-   * @param value
-   * @param voffset
-   * @param vlength
+   * @param row row key
+   * @param roffset row offset
+   * @param rlength row length
+   * @param family family name
+   * @param foffset family offset
+   * @param flength family length
+   * @param qualifier column qualifier
+   * @param qoffset qualifier offset
+   * @param qlength qualifier length
+   * @param timestamp version timestamp
+   * @param type key type
+   * @param value column value
+   * @param voffset value offset
+   * @param vlength value length
    * @return
    */
   static byte [] createByteArray(final byte [] row, final int roffset,
-        final int rlength,
-      final byte [] column, final int coffset, int clength,
+      final int rlength, final byte [] family, final int foffset, int flength,
+      final byte [] qualifier, final int qoffset, int qlength,
       final long timestamp, final Type type,
       final byte [] value, final int voffset, int vlength) {
     if (rlength > Short.MAX_VALUE) {
@@ -429,19 +452,14 @@
     if (row == null) {
       throw new IllegalArgumentException("Row is null");
     }
-    // If column is non-null, figure where the delimiter is at.
-    int delimiteroffset = 0;
-    if (column != null && column.length > 0) {
-      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
-      if (delimiteroffset > Byte.MAX_VALUE) {
-        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
-      }
-    }
     // Value length
-    vlength = value == null? 0: vlength;
-    // Column length - minus delimiter
-    clength = column == null || column.length == 0? 0: clength - 1;
-    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + clength;
+    vlength = value == null? 0 : vlength;
+    // Family length
+    flength = family == null ? 0 : flength;
+    // Qualifier length
+    qlength = qualifier == null ? 0 : qlength;
+    // Key length
+    long longkeylength = KEY_INFRASTRUCTURE_SIZE + rlength + flength + qlength;
     if (longkeylength > Integer.MAX_VALUE) {
       throw new IllegalArgumentException("keylength " + longkeylength + " > " +
         Integer.MAX_VALUE);
@@ -455,16 +473,13 @@
     pos = Bytes.putInt(bytes, pos, vlength);
     pos = Bytes.putShort(bytes, pos, (short)(rlength & 0x0000ffff));
     pos = Bytes.putBytes(bytes, pos, row, roffset, rlength);
-    // Write out column family length.
-    pos = Bytes.putByte(bytes, pos, (byte)(delimiteroffset & 0x0000ff));
-    if (column != null && column.length != 0) {
-      // Write family.
-      pos = Bytes.putBytes(bytes, pos, column, coffset, delimiteroffset);
-      // Write qualifier.
-      delimiteroffset++;
-      pos = Bytes.putBytes(bytes, pos, column, coffset + delimiteroffset,
-        column.length - delimiteroffset);
+    pos = Bytes.putByte(bytes, pos, (byte)(flength & 0x0000ff));
+    if(flength != 0) {
+      pos = Bytes.putBytes(bytes, pos, family, foffset, flength);
     }
+    if(qlength != 0) {
+      pos = Bytes.putBytes(bytes, pos, qualifier, qoffset, qlength);
+    }
     pos = Bytes.putLong(bytes, pos, timestamp);
     pos = Bytes.putByte(bytes, pos, type.getCode());
     if (value != null && value.length > 0) {
@@ -472,6 +487,46 @@
     }
     return bytes;
   }
+  
+  /**
+   * Write KeyValue format into a byte array.
+   * <p>
+   * Takes column in the form <code>family:qualifier</code>
+   * @param row - row key (arbitrary byte array)
+   * @param roffset
+   * @param rlength
+   * @param column
+   * @param coffset
+   * @param clength
+   * @param timestamp
+   * @param type
+   * @param value
+   * @param voffset
+   * @param vlength
+   * @return
+   */
+  static byte [] createByteArray(final byte [] row, final int roffset,
+        final int rlength,
+      final byte [] column, final int coffset, int clength,
+      final long timestamp, final Type type,
+      final byte [] value, final int voffset, int vlength) {
+    // If column is non-null, figure where the delimiter is at.
+    int delimiteroffset = 0;
+    if (column != null && column.length > 0) {
+      delimiteroffset = getFamilyDelimiterIndex(column, coffset, clength);
+      if (delimiteroffset > Byte.MAX_VALUE) {
+        throw new IllegalArgumentException("Family > " + Byte.MAX_VALUE);
+      }
+    } else {
+      return createByteArray(row,roffset,rlength,null,0,0,null,0,0,timestamp,
+          type,value,voffset,vlength);
+    }
+    int flength = delimiteroffset-coffset;
+    int qlength = clength - flength - 1;
+    return createByteArray(row, roffset, rlength, column, coffset,
+        flength, column, delimiteroffset+1, qlength, timestamp, type,
+        value, voffset, vlength);
+  }
 
   // Needed doing 'contains' on List.  Only compares the key portion, not the
   // value.
@@ -485,6 +540,12 @@
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue cloning
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @param timestamp
    * @return Clone of bb's key portion with only the row and timestamp filled in.
@@ -492,9 +553,10 @@
    */
   public KeyValue cloneRow(final long timestamp) {
     return new KeyValue(getBuffer(), getRowOffset(), getRowLength(),
-      null, 0, 0, timestamp, Type.codeToType(getType()), null, 0, 0);
+        null, 0, 0, null, 0, 0, 
+        timestamp, Type.codeToType(getType()), null, 0, 0);
   }
-
+  
   /**
    * @return Clone of bb's key portion with type set to Type.Delete.
    * @throws IOException
@@ -531,6 +593,12 @@
     return new KeyValue(other, 0, other.length);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  String representation
+  //
+  //---------------------------------------------------------------------------
+  
   public String toString() {
     return keyToString(this.bytes, this.offset + ROW_OFFSET, getKeyLength()) +
       "/vlen=" + getValueLength();
@@ -568,6 +636,12 @@
       qualifier + "/" + timestamp + "/" + Type.codeToType(type);
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  Public Member Accessors
+  //
+  //---------------------------------------------------------------------------
+  
   /**
    * @return The byte array backing this KeyValue.
    */
@@ -589,7 +663,13 @@
     return length;
   }
 
-  /*
+  //---------------------------------------------------------------------------
+  //
+  //  Length and Offset Calculators
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
    * Determines the total length of the KeyValue stored in the specified
    * byte array and offset.  Includes all headers.
    * @param bytes byte array
@@ -603,41 +683,177 @@
   }
 
   /**
-   * @return Copy of the key portion only.  Used compacting and testing.
+   * @return Key offset in backing buffer..
    */
-  public byte [] getKey() {
-    int keylength = getKeyLength();
-    byte [] key = new byte[keylength];
-    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
-    return key;
+  public int getKeyOffset() {
+    return this.offset + ROW_OFFSET;
   }
 
-  public String getKeyString() {
-    return Bytes.toString(getBuffer(), getKeyOffset(), getKeyLength());
+  /**
+   * @return Length of key portion.
+   */
+  public int getKeyLength() {
+    return Bytes.toInt(this.bytes, this.offset);
   }
 
   /**
-   * @return Key offset in backing buffer..
+   * @return Value offset
    */
-  public int getKeyOffset() {
-    return this.offset + ROW_OFFSET;
+  public int getValueOffset() {
+    return getKeyOffset() + getKeyLength();
   }
+  
+  /**
+   * @return Value length
+   */
+  public int getValueLength() {
+    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  }
 
   /**
-   * @return Row length.
+   * @return Row offset
    */
+  public int getRowOffset() {
+    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  }
+  
+  /**
+   * @return Row length
+   */
   public short getRowLength() {
     return Bytes.toShort(this.bytes, getKeyOffset());
   }
 
   /**
-   * @return Offset into backing buffer at which row starts.
+   * @return Family offset
    */
-  public int getRowOffset() {
-    return getKeyOffset() + Bytes.SIZEOF_SHORT;
+  public int getFamilyOffset() {
+    return getFamilyOffset(getRowLength());
   }
+  
+  /**
+   * @return Family offset
+   */
+  public int getFamilyOffset(int rlength) {
+    return ROW_OFFSET + Bytes.SIZEOF_SHORT + rlength + Bytes.SIZEOF_BYTE;
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength() {
+    return getFamilyLength(getFamilyOffset());
+  }
+  
+  /**
+   * @return Family length
+   */
+  public byte getFamilyLength(int foffset) {
+    return this.bytes[foffset-1];
+  }
 
   /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset() {
+    return getQualifierOffset(getFamilyOffset());
+  }
+  
+  /**
+   * @return Qualifier offset
+   */
+  public int getQualifierOffset(int foffset) {
+    return foffset + getFamilyLength(foffset);
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength() {
+    return getQualifierLength(getRowLength(),getFamilyLength());
+  }
+  
+  /**
+   * @return Qualifier length
+   */
+  public int getQualifierLength(int rlength, int flength) {
+    return getKeyLength() - 
+      (KEY_INFRASTRUCTURE_SIZE + rlength + flength);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength() {
+    int rlength = getRowLength();
+    int foffset = getFamilyOffset(rlength);
+    return getTotalColumnLength(rlength,foffset);
+  }
+  
+  /**
+   * @return Column (family + qualifier) length
+   */
+  public int getTotalColumnLength(int rlength, int foffset) {
+    int flength = getFamilyLength(foffset);
+    int qlength = getQualifierLength(rlength,flength);
+    return flength + qlength;
+  }
+  
+  /**
+   * @return Timestamp offset
+   */
+  int getTimestampOffset() {
+    return getTimestampOffset(getKeyLength());
+  }
+  
+  /**
+   * @param keylength Pass if you have it to save on a int creation.
+   * @return Timestamp offset
+   */
+  int getTimestampOffset(final int keylength) {
+    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
+  }
+  
+  public boolean updateLatestStamp(final byte [] now) {
+    int tsOffset = getTimestampOffset();
+    if(Bytes.compareTo(now, 0, Bytes.SIZEOF_LONG, 
+        this.bytes, tsOffset, Bytes.SIZEOF_LONG) < 0) {
+      System.arraycopy(now, 0, this.bytes, tsOffset, Bytes.SIZEOF_LONG);
+      return true;
+    }
+    return false;
+  }
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Methods that return copies of fields
+  //
+  //---------------------------------------------------------------------------
+  
+  /**
+   * @return Copy of the key portion only.  Used compacting and testing.
+   */
+  public byte [] getKey() {
+    int keylength = getKeyLength();
+    byte [] key = new byte[keylength];
+    System.arraycopy(getBuffer(), getKeyOffset(), key, 0, keylength);
+    return key;
+  }
+  
+  /**
+   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
+   * offset and lengths instead.
+   * @return Value in a new byte array.
+   */
+  public byte [] getValue() {
+    int o = getValueOffset();
+    int l = getValueLength();
+    byte [] result = new byte[l];
+    System.arraycopy(getBuffer(), o, result, 0, l);
+    return result;
+  }
+  
+  /**
    * Do not use this unless you have to.
    * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
    * @return Row in a new byte array.
@@ -667,14 +883,6 @@
   }
 
   /**
-   * @param keylength Pass if you have it to save on a int creation.
-   * @return Offset into backing buffer at which timestamp starts.
-   */
-  int getTimestampOffset(final int keylength) {
-    return getKeyOffset() + keylength - TIMESTAMP_TYPE_SIZE;
-  }
-
-  /**
    * @return True if a {@link Type#Delete}.
    */
   public boolean isDeleteType() {
@@ -697,99 +905,156 @@
   }
 
   /**
-   * @return Length of key portion.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns column. Makes a copy.  Inserts delimiter.
    */
-  public int getKeyLength() {
-    return Bytes.toInt(this.bytes, this.offset);
+  public byte [] getColumn() {
+    int fo = getFamilyOffset();
+    int fl = getFamilyLength(fo);
+    int ql = getQualifierLength();
+    byte [] result = new byte[fl + 1 + ql];
+    System.arraycopy(this.bytes, fo, result, 0, fl);
+    result[fl] = COLUMN_FAMILY_DELIMITER;
+    System.arraycopy(this.bytes, fo + fl, result,
+      fl + 1, ql);
+    return result;
   }
 
   /**
-   * @return Value length
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns family. Makes a copy.
    */
-  public int getValueLength() {
-    return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
+  public byte [] getFamily() {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    byte [] result = new byte[l];
+    System.arraycopy(this.bytes, o, result, 0, l);
+    return result;
   }
 
   /**
-   * @return Offset into backing buffer at which value starts.
+   * Do not use this unless you have to.
+   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
+   * @return Returns qualifier. Makes a copy.
    */
-  public int getValueOffset() {
-    return getKeyOffset() + getKeyLength();
-  }
-
-  /**
-   * Do not use unless you have to.  Use {@link #getBuffer()} with appropriate
-   * offset and lengths instead.
-   * @return Value in a new byte array.
-   */
-  public byte [] getValue() {
-    int o = getValueOffset();
-    int l = getValueLength();
+  public byte [] getQualifier() {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
     byte [] result = new byte[l];
-    System.arraycopy(getBuffer(), o, result, 0, l);
+    System.arraycopy(this.bytes, o, result, 0, l);
     return result;
   }
 
+  //---------------------------------------------------------------------------
+  //
+  //  KeyValue splitter
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @return Offset into backing buffer at which the column begins
+   * Utility class that splits a KeyValue buffer into separate byte arrays.
+   * <p>
+   * Should get rid of this if we can, but is very useful for debugging.
    */
-  public int getColumnOffset() {
-    return getColumnOffset(getRowLength());
+  public static class SplitKeyValue {
+    private byte [][] split;
+    SplitKeyValue() {
+      this.split = new byte[6][];
+    }
+    public void setRow(byte [] value) { this.split[0] = value; }
+    public void setFamily(byte [] value) { this.split[1] = value; }
+    public void setQualifier(byte [] value) { this.split[2] = value; }
+    public void setTimestamp(byte [] value) { this.split[3] = value; }
+    public void setType(byte [] value) { this.split[4] = value; }
+    public void setValue(byte [] value) { this.split[5] = value; }
+    public byte [] getRow() { return this.split[0]; }
+    public byte [] getFamily() { return this.split[1]; }
+    public byte [] getQualifier() { return this.split[2]; }
+    public byte [] getTimestamp() { return this.split[3]; }
+    public byte [] getType() { return this.split[4]; }
+    public byte [] getValue() { return this.split[5]; }
   }
-
-  /**
-   * @param rowlength - length of row.
-   * @return Offset into backing buffer at which the column begins
-   */
-  public int getColumnOffset(final int rowlength) {
-    return getRowOffset() + rowlength + 1;
+  
+  public SplitKeyValue split() {
+    SplitKeyValue split = new SplitKeyValue();
+    int splitOffset = this.offset;
+    int keyLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    int valLen = Bytes.toInt(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_INT;
+    short rowLen = Bytes.toShort(bytes, splitOffset);
+    splitOffset += Bytes.SIZEOF_SHORT;
+    byte [] row = new byte[rowLen];
+    System.arraycopy(bytes, splitOffset, row, 0, rowLen);
+    splitOffset += rowLen;
+    split.setRow(row);
+    byte famLen = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    byte [] family = new byte[famLen];
+    System.arraycopy(bytes, splitOffset, family, 0, famLen);
+    splitOffset += famLen;
+    split.setFamily(family);
+    int colLen = keyLen -
+      (rowLen + famLen + Bytes.SIZEOF_SHORT + Bytes.SIZEOF_BYTE +
+      Bytes.SIZEOF_LONG + Bytes.SIZEOF_BYTE);
+    byte [] qualifier = new byte[colLen];
+    System.arraycopy(bytes, splitOffset, qualifier, 0, colLen);
+    splitOffset += colLen;
+    split.setQualifier(qualifier);
+    byte [] timestamp = new byte[Bytes.SIZEOF_LONG];
+    System.arraycopy(bytes, splitOffset, timestamp, 0, Bytes.SIZEOF_LONG);
+    splitOffset += Bytes.SIZEOF_LONG;
+    split.setTimestamp(timestamp);
+    byte [] type = new byte[1];
+    type[0] = bytes[splitOffset];
+    splitOffset += Bytes.SIZEOF_BYTE;
+    split.setType(type);
+    byte [] value = new byte[valLen];
+    System.arraycopy(bytes, splitOffset, value, 0, valLen);
+    split.setValue(value);
+    return split;
   }
-
+  
+  //---------------------------------------------------------------------------
+  //
+  //  Compare specified fields against those contained in this KeyValue 
+  //
+  //---------------------------------------------------------------------------
+  
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of family portion of column.
+   * @param family
+   * @return True if matching families.
    */
-  int getFamilyLength(final int columnoffset) {
-    return this.bytes[columnoffset - 1];
+  public boolean matchingFamily(final byte [] family) {
+    int o = getFamilyOffset();
+    int l = getFamilyLength(o);
+    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
   }
 
   /**
-   * @param columnoffset Pass if you have it to save on an int creation.
-   * @return Length of column.
+   * @param qualifier
+   * @return True if matching qualifiers.
    */
-  public int getColumnLength(final int columnoffset) {
-    return getColumnLength(columnoffset, getKeyLength());
+  public boolean matchingQualifier(final byte [] qualifier) {
+    int o = getQualifierOffset();
+    int l = getQualifierLength();
+    return Bytes.compareTo(qualifier, 0, qualifier.length, 
+        this.bytes, o, l) == 0;
   }
 
-  int getColumnLength(final int columnoffset, final int keylength) {
-    return (keylength + ROW_OFFSET) - (columnoffset - this.offset) -
-      TIMESTAMP_TYPE_SIZE;
-  }
-
   /**
-   * @param family
-   * @return True if matching families.
-   */
-  public boolean matchingFamily(final byte [] family) {
-    int o = getColumnOffset();
-    // Family length byte is just before the column starts.
-    int l = this.bytes[o - 1];
-    return Bytes.compareTo(family, 0, family.length, this.bytes, o, l) == 0;
-  }
-
-  /**
    * @param column Column minus its delimiter
-   * @param familylength Length of family in passed <code>column</code>
    * @return True if column matches.
    * @see #matchingColumn(byte[])
    */
-  public boolean matchingColumnNoDelimiter(final byte [] column,
-      final int familylength) {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int f = getFamilyLength(o);
-    return compareColumns(getBuffer(), o, l, f,
-      column, 0, column.length, familylength) == 0;
+  public boolean matchingColumnNoDelimiter(final byte [] column) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int l = fl + getQualifierLength(rl,fl);
+    return Bytes.compareTo(column, 0, column.length, this.bytes, o, l) == 0;
   }
 
   /**
@@ -798,17 +1063,41 @@
    */
   public boolean matchingColumn(final byte [] column) {
     int index = getFamilyDelimiterIndex(column, 0, column.length);
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int result = Bytes.compareTo(getBuffer(), o, index, column, 0, index);
-    if (result != 0) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(column, 0, index, this.bytes, o, fl) != 0) {
       return false;
     }
-    return Bytes.compareTo(getBuffer(), o + index, l - index,
-      column, index + 1, column.length - (index + 1)) == 0;
+    return Bytes.compareTo(column, index + 1, column.length - (index + 1),
+        this.bytes, o + fl, ql) == 0;
   }
 
   /**
+   * @param column Column with delimiter
+   * @return True if column matches.
+   */
+  public boolean matchingColumn(final byte[] family, final byte[] qualifier) {
+    int rl = getRowLength();
+    int o = getFamilyOffset(rl);
+    int fl = getFamilyLength(o);
+    int ql = getQualifierLength(rl,fl);
+    if(Bytes.compareTo(family, 0, family.length, this.bytes, o, family.length)
+        != 0) {
+      return false;
+    }
+    if(qualifier == null || qualifier.length == 0) {
+      if(ql == 0) {
+        return true;
+      }
+      return false;
+    }
+    return Bytes.compareTo(qualifier, 0, qualifier.length,
+        this.bytes, o + fl, ql) == 0;
+  }
+
+  /**
    * @param left
    * @param loffset
    * @param llength
@@ -843,42 +1132,35 @@
   }
 
   /**
-   * @return Returns column String with delimiter added back. Expensive!
+   * @return True if column is empty.
    */
-  public String getColumnString() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    return Bytes.toString(this.bytes, o, familylength) +
-      COLUMN_FAMILY_DELIMITER + Bytes.toString(this.bytes,
-       o + familylength, l - familylength);
+  public boolean isEmptyColumn() {
+    return getQualifierLength() == 0;
   }
 
   /**
-   * Do not use this unless you have to.
-   * Use {@link #getBuffer()} with appropriate offsets and lengths instead.
-   * @return Returns column. Makes a copy.  Inserts delimiter.
+   * Splits a column in family:qualifier form into separate byte arrays.
+   * <p>
+   * Catches 
+   * @param column
+   * @return
    */
-  public byte [] getColumn() {
-    int o = getColumnOffset();
-    int l = getColumnLength(o);
-    int familylength = getFamilyLength(o);
-    byte [] result = new byte[l + 1];
-    System.arraycopy(getBuffer(), o, result, 0, familylength);
-    result[familylength] = COLUMN_FAMILY_DELIMITER;
-    System.arraycopy(getBuffer(), o + familylength, result,
-      familylength + 1, l - familylength);
+  public static byte [][] parseColumn(byte [] c) {
+    final byte [][] result = new byte [2][];
+    final int index = getFamilyDelimiterIndex(c, 0, c.length);
+    if (index == -1) {
+      throw new IllegalArgumentException("Impossible column name: " + c);
+    }
+    result[0] = new byte [index];
+    System.arraycopy(c, 0, result[0], 0, index);
+    final int len = c.length - (index + 1);
+    result[1] = new byte[len];
+    System.arraycopy(c, index + 1 /*Skip delimiter*/, result[1], 0,
+      len);
     return result;
   }
-
+  
   /**
-   * @return True if column is empty.
-   */
-  public boolean isEmptyColumn() {
-    return getColumnLength(getColumnOffset()) == 0;
-  }
-
-  /**
    * @param b
    * @return Index of the family-qualifier colon delimiter character in passed
    * buffer.
@@ -1026,7 +1308,8 @@
      * @return Result comparing rows.
      */
     public int compareRows(final KeyValue left, final KeyValue right) {
-      return compareRows(left, left.getRowLength(), right, right.getRowLength());
+      return compareRows(left, left.getRowLength(), right, 
+          right.getRowLength());
     }
 
     /**
@@ -1058,11 +1341,11 @@
       return getRawComparator().compareRows(left, loffset, llength,
         right, roffset, rlength);
     }
-
+    
     public int compareColumns(final KeyValue left, final byte [] right,
         final int roffset, final int rlength, final int rfamilyoffset) {
-      int offset = left.getColumnOffset();
-      int length = left.getColumnLength(offset);
+      int offset = left.getFamilyOffset();
+      int length = left.getFamilyLength() + left.getQualifierLength();
       return getRawComparator().compareColumns(left.getBuffer(), offset, length,
         left.getFamilyLength(offset),
         right, roffset, rlength, rfamilyoffset);
@@ -1071,15 +1354,15 @@
     int compareColumns(final KeyValue left, final short lrowlength,
         final int lkeylength, final KeyValue right, final short rrowlength,
         final int rkeylength) {
-      int loffset = left.getColumnOffset(lrowlength);
-      int roffset = right.getColumnOffset(rrowlength);
-      int llength = left.getColumnLength(loffset, lkeylength);
-      int rlength = right.getColumnLength(roffset, rkeylength);
-      int lfamilylength = left.getFamilyLength(loffset);
-      int rfamilylength = right.getFamilyLength(roffset);
-      return getRawComparator().compareColumns(left.getBuffer(), loffset,
-          llength, lfamilylength,
-        right.getBuffer(), roffset, rlength, rfamilylength);
+      int lfoffset = left.getFamilyOffset(lrowlength);
+      int rfoffset = right.getFamilyOffset(rrowlength);
+      int lclength = left.getTotalColumnLength(lrowlength,lfoffset);
+      int rclength = right.getTotalColumnLength(rrowlength, rfoffset);
+      int lfamilylength = left.getFamilyLength(lfoffset);
+      int rfamilylength = right.getFamilyLength(rfoffset);
+      return getRawComparator().compareColumns(left.getBuffer(), lfoffset,
+          lclength, lfamilylength,
+        right.getBuffer(), rfoffset, rclength, rfamilylength);
     }
 
     /**
@@ -1140,7 +1423,8 @@
     public boolean matchingRows(final byte [] left, final int loffset,
         final int llength,
         final byte [] right, final int roffset, final int rlength) {
-      int compare = compareRows(left, loffset, llength, right, roffset, rlength);
+      int compare = compareRows(left, loffset, llength, 
+          right, roffset, rlength);
       if (compare != 0) {
         return false;
       }
@@ -1214,20 +1498,44 @@
    */
   public static KeyValue createFirstOnRow(final byte [] row,
       final long ts) {
-    return createFirstOnRow(row, null, ts);
+    return new KeyValue(row, null, null, ts, Type.Maximum);
   }
 
   /**
    * @param row - row key (arbitrary byte array)
    * @param ts - timestamp
-   * @return First possible key on passed <code>row</code>, column and timestamp.
+   * @return First possible key on passed <code>row</code>, column and timestamp
    */
   public static KeyValue createFirstOnRow(final byte [] row, final byte [] c,
       final long ts) {
-    return new KeyValue(row, c, ts, Type.Maximum);
+    byte [][] split = parseColumn(c);
+    return new KeyValue(row, split[0], split[1], ts, Type.Maximum);
   }
 
   /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @return First possible key on passed <code>row</code>, and column.
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q) {
+    return new KeyValue(row, f, q, HConstants.LATEST_TIMESTAMP, Type.Maximum);
+  }
+
+  /**
+   * @param row - row key (arbitrary byte array)
+   * @param f - family name
+   * @param q - column qualifier
+   * @param ts - timestamp
+   * @return First possible key on passed <code>row</code>, column and timestamp
+   */
+  public static KeyValue createFirstOnRow(final byte [] row, final byte [] f,
+      final byte [] q, final long ts) {
+    return new KeyValue(row, f, q, ts, Type.Maximum);
+  }
+  
+  /**
    * @param b
    * @param o
    * @param l
@@ -1255,7 +1563,8 @@
       //          "---" + Bytes.toString(right, roffset, rlength));
       final int metalength = 7; // '.META.' length
       int lmetaOffsetPlusDelimiter = loffset + metalength;
-      int leftFarDelimiter = getDelimiterInReverse(left, lmetaOffsetPlusDelimiter,
+      int leftFarDelimiter = getDelimiterInReverse(left, 
+          lmetaOffsetPlusDelimiter,
           llength - metalength, HRegionInfo.DELIMITER);
       int rmetaOffsetPlusDelimiter = roffset + metalength;
       int rightFarDelimiter = getDelimiterInReverse(right,
@@ -1402,7 +1711,7 @@
       return compare(left, 0, left.length, right, 0, right.length);
     }
 
-    protected int compareRows(byte [] left, int loffset, int llength,
+    public int compareRows(byte [] left, int loffset, int llength,
         byte [] right, int roffset, int rlength) {
       return Bytes.compareTo(left, loffset, llength, right, roffset, rlength);
     }
@@ -1430,7 +1739,9 @@
   
   // HeapSize
   public long heapSize() {
-    return this.length;
+    int dataLen = bytes.length + (bytes.length % 8);
+    return HeapSize.OBJECT + HeapSize.BYTE_ARRAY + dataLen +
+      (2 * HeapSize.INT);
   }
   
   // Writable
Index: src/java/org/apache/hadoop/hbase/io/Delete.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Delete.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Delete.java	(revision 0)
@@ -0,0 +1,317 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Used to perform Delete operations on a single row.
+ * <p>
+ * To delete an entire row, instantiate a Delete object with the row 
+ * to delete.  To further define the scope of what to delete, perform additional
+ * methods as outlined below.
+ * <p>
+ * To delete specific families, execute {@link #deleteFamily(byte []) deleteFamily}
+ * for each family to delete.
+ * <p>
+ * To delete multiple versions of specific columns, execute {@link #deleteColumns(byte [],byte []) deleteColumns}
+ * for each column to delete.  
+ * <p>
+ * To delete specific versions of specific columns, execute {@link #deleteColumn(byte [],byte [],long) deleteColumn}
+ * for each column version to delete.
+ * <p>
+ * Specifying timestamps to the constructor, deleteFamily, and deleteColumns
+ * will delete all versions with a timestamp less than or equal to that
+ * specified.  Specifying a timestamp to deleteColumn will delete versions
+ * only with a timestamp equal to that specified.
+ */
+public class Delete implements Writable {
+  private byte [] row = null;
+  private long timestamp;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap = 
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Delete() {}
+  
+  /**
+   * Create a Delete operation for the specified row.
+   * <p>
+   * If no further operations are done, this will delete everything
+   * associated with the specified row (all versions of all columns in all
+   * families).
+   * @param row row key
+   */
+  public Delete(byte [] row) {
+    this(row, HConstants.LATEST_TIMESTAMP, null);
+  }
+  
+  /**
+   * Create a Delete operation for the specified row and timestamp, using
+   * an optional row lock.
+   * <p>
+   * If no further operations are done, this will delete all columns in all
+   * families of the specified row with a timestamp less than or equal to the 
+   * specified timestamp.
+   * @param row row key
+   * @param timestamp maximum version timestamp
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Delete(byte [] row, long timestamp, RowLock rowLock) {
+    this.row = row;
+    this.timestamp = timestamp;
+    if(rowLock != null) {
+    	this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Delete all versions of all columns of the specified family.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   */
+  public void deleteFamily(byte [] family) {
+	this.deleteFamily(family, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all columns of the specified family with a timestamp less than
+   * or equal to the specified timestamp.
+   * <p>
+   * Overrides previous calls to deleteColumn and deleteColumns for the
+   * specified family.
+   * @param family family name
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteFamily(byte [] family, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    } else if(!list.isEmpty()) {
+      list.clear();
+    }
+    list.add(new KeyValue(row, family, null, timestamp, KeyValue.Type.DeleteFamily));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete all versions of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier) {
+    this.deleteColumns(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete all versions of the specified column with a timestamp less than
+   * or equal to the specified timestamp.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp maximum version timestamp
+   */
+  public void deleteColumns(byte [] family, byte [] qualifier, 
+	  long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.DeleteColumn));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier) {
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Delete the specified version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   */
+  public void deleteColumn(byte [] family, byte [] qualifier, long timestamp) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(new KeyValue(
+        this.row, family, qualifier, timestamp, KeyValue.Type.Delete));
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Delete the latest version of the specified column, given in
+   * <code>family:qualifier</code> notation.
+   * @param column colon-delimited family and qualifier 
+   */
+  public void deleteColumn(byte [] column) {
+    int len = column.length;
+    int i=0;
+    byte b = 0;
+    for(; i<len; i++) {
+      b = column[i];
+      if(b == ':') {
+        break;
+      }
+    }
+    byte [] family = new byte[i];
+    System.arraycopy(column, 0, family, 0, i);
+    i++;
+    int qLen = len - i;
+    byte [] qualifier = new byte[qLen];
+    System.arraycopy(column, i, qualifier, 0, qLen);
+    this.deleteColumn(family, qualifier, HConstants.LATEST_TIMESTAMP);
+  }
+  
+  /**
+   * Method for retrieving the delete's familyMap 
+   * @return familyMap
+   */
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  /**
+   *  Method for retrieving the delete's row
+   * @return row
+   */
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  /**
+   * Method for retrieving the delete's RowLock
+   * @return RowLock
+   */
+  public RowLock getRowLock() {
+	if(this.lockId == -1L) {
+      return null;
+	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  /**
+   * Method for retrieving the delete's lockId
+   * @return
+   */
+  public long getLockId() {
+	return this.lockId;
+  }
+  
+  /**
+   * Method for retrieving the delete's timestamp
+   * @return timestamp
+   */
+  public long getTimeStamp() {
+    return this.timestamp;
+  }
+  
+  /**
+   * @return string
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", families={");
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", keyvalues=(");
+      boolean moreThanOneB = false;
+      for(KeyValue kv : entry.getValue()) {
+        if(moreThanOneB) {
+          sb.append(", ");
+        } else {
+          moreThanOneB = true;
+        }
+        sb.append(kv.toString());
+      }
+      sb.append(")");
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    this.familyMap = 
+        new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    int numFamilies = in.readInt();
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      List<KeyValue> list = new ArrayList<KeyValue>(numColumns);
+      for(int j=0;j<numColumns;j++) {
+    	KeyValue kv = new KeyValue();
+    	kv.readFields(in);
+    	list.add(kv);
+      }
+      this.familyMap.put(family, list);
+    }
+  }  
+  
+  public void write(final DataOutput out) throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> list = entry.getValue();
+      out.writeInt(list.size());
+      for(KeyValue kv : list) {
+        kv.write(out);
+      }
+    }
+  }
+}
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.filter.RowFilterSet;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
@@ -133,6 +134,18 @@
       e.printStackTrace();
     }
     addToMap(BatchUpdate[].class, code++);
+    
+    //
+    // HBASE-880
+    //
+    
+    addToMap(Delete.class, code++);
+    addToMap(Get.class, code++);
+    addToMap(KeyValue.class, code++);
+    addToMap(KeyValue[].class, code++);
+    addToMap(Put.class, code++);
+    addToMap(Result.class, code++);
+    addToMap(Scan.class, code++);
   }
   
   private Class<?> declaredClass;
Index: src/java/org/apache/hadoop/hbase/io/RowResult.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/RowResult.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/RowResult.java	(working copy)
@@ -102,6 +102,11 @@
     return cells.containsKey(key);
   }
   
+  /**
+   * Check if the key can be found in this RowResult
+   * @param key
+   * @return true if key id found, false if not
+   */
   public boolean containsKey(String key) {
     return cells.containsKey(Bytes.toBytes(key));
   }
Index: src/java/org/apache/hadoop/hbase/io/hfile/HFile.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/hfile/HFile.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/hfile/HFile.java	(working copy)
@@ -465,8 +465,12 @@
      * Add key/value to file.
      * Keys must be added in an order that agrees with the Comparator passed
      * on construction.
-     * @param key Key to add.  Cannot be empty nor null.
-     * @param value Value to add.  Cannot be empty nor null.
+     * @param key
+     * @param koffset
+     * @param klength
+     * @param value
+     * @param voffset
+     * @param vlength
      * @throws IOException
      */
     public void append(final byte [] key, final int koffset, final int klength,
@@ -1035,6 +1039,9 @@
       }
       
       public KeyValue getKeyValue() {
+        if(this.block == null) {
+          return null;
+        }
         return new KeyValue(this.block.array(),
             this.block.arrayOffset() + this.block.position() - 8);
       }
Index: src/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/hfile/SimpleBlockCache.java	(working copy)
@@ -24,6 +24,9 @@
   private ReferenceQueue q = new ReferenceQueue();
   public int dumps = 0;
   
+  /**
+   * Constructor
+   */
   public SimpleBlockCache() {
     super();
   }
@@ -36,6 +39,9 @@
     }
   }
 
+  /**
+   * @return the size
+   */
   public synchronized int size() {
     processQueue();
     return cache.size();
Index: src/java/org/apache/hadoop/hbase/io/CodeToClassAndBack.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/CodeToClassAndBack.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/CodeToClassAndBack.java	(working copy)
@@ -45,7 +45,7 @@
   /**
    * Class list for supported classes
    */
-  public Class[] classList = {byte[].class, Cell.class};
+  public Class<?>[] classList = {byte[].class, Cell.class};
   
   /**
    * The static loader that is used instead of the static constructor in
@@ -58,8 +58,8 @@
    * Class that loads the static maps with their values. 
    */
   public class InternalStaticLoader{
-    InternalStaticLoader(Class[] classList, Map<Byte, Class<?>> CODE_TO_CLASS,
-    Map<Class<?>, Byte> CLASS_TO_CODE){
+    InternalStaticLoader(Class<?>[] classList,
+        Map<Byte,Class<?>> CODE_TO_CLASS, Map<Class<?>, Byte> CLASS_TO_CODE){
       byte code = 1;
       for(int i=0; i<classList.length; i++){
         CLASS_TO_CODE.put(classList[i], code);
Index: src/java/org/apache/hadoop/hbase/io/HeapSize.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/HeapSize.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/HeapSize.java	(working copy)
@@ -21,11 +21,24 @@
 
 /**
  * Implementations can be asked for an estimate of their size in bytes.
+ * <p>
  * Useful for sizing caches.  Its a given that implementation approximations
- * probably do not account for 32 vs 64 bit nor for different VM implemenations.
+ * do not account for 32 vs 64 bit nor for different VM implementations.
+ * <p>
+ * An Object's size is determined by the non-static data members in it,
+ * as well as the fixed {@link OBJECT} overhead.
+ * <p>
+ * For example:
+ * <pre>
+ * public class SampleObject implements HeapSize {
+ *   
+ *   int [] numbers;
+ *   int x;
+ * }
+ * </pre>
  */
 public interface HeapSize {
-  
+
   /** Reference size is 8 bytes on 64-bit, 4 bytes on 32-bit */
   static final int REFERENCE = 8;
   
@@ -49,14 +62,15 @@
   static final int LONG = 8;
   
   /** Array overhead */
-  static final int BYTE_ARRAY = REFERENCE;
   static final int ARRAY = 3 * REFERENCE;
   static final int MULTI_ARRAY = (4 * REFERENCE) + ARRAY;
   
+  /** Byte arrays are fixed size below plus its length, 8 byte aligned */
+  static final int BYTE_ARRAY = 3 * REFERENCE;
+  
   static final int BLOCK_SIZE_TAX = 8;
 
   
-  
   /**
    * @return Approximate 'exclusive deep size' of implementing object.  Includes
    * count of payload and hosting object sizings.
Index: src/java/org/apache/hadoop/hbase/io/Put.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Put.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Put.java	(revision 0)
@@ -0,0 +1,292 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+/** 
+ * Used to perform Put operations for a single row.
+ * <p>
+ * To perform a Put, instantiate a Put object with the row to insert to and
+ * for each column to be inserted, execute {@link #add(byte[], byte[], byte[]) add} or
+ * {@link #add(byte[], byte[], long, byte[]) add} if setting the timestamp.
+ */
+public class Put implements HeapSize, Writable, Comparable<Put> {
+  private byte [] row = null;
+  private long timestamp = HConstants.LATEST_TIMESTAMP;
+  private long lockId = -1L;
+  private Map<byte [], List<KeyValue>> familyMap =
+    new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+  
+  /** Constructor for Writable.  DO NOT USE */
+  public Put() {}
+  
+  /**
+   * Create a Put operation for the specified row.
+   * @param row row key
+   */
+  public Put(byte [] row) {
+    this(row,null);
+  }
+  
+  /**
+   * Create a Put operation for the specified row, using an existing row lock.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Put(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+  
+  /**
+   * Copy constructor.  Creates a Put operation cloned from the specified Put.
+   * @param putToCopy put to copy
+   */
+  public Put(Put putToCopy) {
+    this(putToCopy.getRow(), putToCopy.getRowLock());
+    this.familyMap = 
+      new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte [], List<KeyValue>> entry :
+      putToCopy.getFamilyMap().entrySet()) {
+      this.familyMap.put(entry.getKey(), entry.getValue());
+    }
+  }
+  
+  /**
+   * Add the specified column and value to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, byte [] value) {
+    add(family, qualifier, this.timestamp, value);
+  }
+
+  /**
+   * Add the specified column and value, with the specified timestamp as 
+   * its version to this Put operation.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @param timestamp version timestamp
+   * @param value column value
+   */
+  public void add(byte [] family, byte [] qualifier, long timestamp, byte [] value) {
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    KeyValue kv = new KeyValue(this.row, family, qualifier, timestamp, 
+      KeyValue.Type.Put, value); 
+    list.add(kv);
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Add the specified KeyValue to this Put operation.
+   * @param kv
+   */
+  public void add(KeyValue kv) {
+    byte [] family = kv.getFamily();
+    List<KeyValue> list = familyMap.get(family);
+    if(list == null) {
+      list = new ArrayList<KeyValue>();
+    }
+    list.add(kv);
+    familyMap.put(family, list);
+  }
+  
+  /**
+   * Method for adding a column to the put, this method is a part of the old API
+   * and should not be used if not needed, will be deprecated
+   * @param column
+   * @param value
+   */
+  public void add(byte [] column, byte [] value) {
+    byte [][] split = KeyValue.parseColumn(column);
+    add(split[0], split[1], value);
+  }
+  
+  /**
+   * Method for retrieving the put's familyMap
+   * @return familyMap
+   */
+  public Map<byte [], List<KeyValue>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  /**
+   * Method for retrieving the put's row
+   * @return row 
+   */
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  /**
+   * Method for retrieving the put's RowLock
+   * @return RowLock
+   */
+  public RowLock getRowLock() {
+  	if(this.lockId == -1L) {
+      return null;
+  	}
+    return new RowLock(this.row, this.lockId);
+  }
+  
+  /**
+   * Method for retrieving the put's lockId
+   * @return lockId
+   */
+  public long getLockId() {
+  	return this.lockId;
+  }
+  
+  /**
+   * Method to check if the familyMap is empty
+   * @return true if empty, false otherwise
+   */
+  public boolean isEmpty() {
+    return familyMap.isEmpty();
+  }
+  
+  /**
+   * Method for setting the timestamp
+   * @param timestamp
+   */
+  public void setTimeStamp(long timestamp) {
+    this.timestamp = timestamp;
+  }
+  
+  public int numFamilies() {
+    return familyMap.size();
+  }
+  
+  public int size() {
+    int size = 0;
+    for(List<KeyValue> kvList : this.familyMap.values()) {
+      size += kvList.size();
+    }
+    return size;
+  }
+  
+  /**
+   * @return String 
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", families={");
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", keyvalues=(");
+      boolean moreThanOneB = false;
+      for(KeyValue kv : entry.getValue()) {
+        if(moreThanOneB) {
+          sb.append(", ");
+        } else {
+          moreThanOneB = true;
+        }
+        sb.append(kv.toString());
+      }
+      sb.append(")");
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  public int compareTo(Put p) {
+    return Bytes.compareTo(this.getRow(), p.getRow());
+  }
+  
+  //HeapSize
+  public long heapSize() {
+  	long totalSize = 0;
+  	for(Map.Entry<byte [], List<KeyValue>> entry : this.familyMap.entrySet()) {
+  	  for(KeyValue kv : entry.getValue()) {
+  		totalSize += kv.heapSize();
+  	  }
+  	}
+    return totalSize;
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.timestamp = in.readLong();
+    this.lockId = in.readLong();
+    int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],List<KeyValue>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0;i<numFamilies;i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numKeys = in.readInt();
+      List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
+      for(int j=0;j<numKeys;j++) {
+        KeyValue kv = new KeyValue();
+        kv.readFields(in);
+        keys.add(kv);
+      }
+      this.familyMap.put(family, keys);
+    }
+  }
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.timestamp);
+    out.writeLong(this.lockId);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      List<KeyValue> keys = entry.getValue();
+      out.writeInt(keys.size());
+      for(KeyValue kv : keys) {
+        kv.write(out);
+      }
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/TimeRange.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/TimeRange.java	(revision 0)
@@ -0,0 +1,171 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.io.Writable;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Represents an interval of version timestamps.
+ * <p>
+ * Evaluated according to minStamp <= timestamp < maxStamp
+ * or [minStamp,maxStamp) in interval notation.
+ * <p>
+ * Only used internally; should not be accessed directly by clients.
+ */
+public class TimeRange implements Writable {
+  private long minStamp = 0L;
+  private long maxStamp = Long.MAX_VALUE;
+  private boolean allTime = true;
+
+  /**
+   * Default constructor.
+   * Represents interval [0, Long.MAX_VALUE) (allTime)
+   */
+  public TimeRange() {
+    allTime = true;
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(long minStamp) {
+    this.minStamp = minStamp;
+  }
+  
+  /**
+   * Represents interval [minStamp, Long.MAX_VALUE)
+   * @param minStamp the minimum timestamp value, inclusive
+   */
+  public TimeRange(byte [] minStamp) {
+  	this.minStamp = Bytes.toLong(minStamp);
+  }
+  
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   * @throws IOException
+   */
+  public TimeRange(long minStamp, long maxStamp)
+  throws IOException {
+    if(minStamp > maxStamp) {
+      throw new IOException("maxStamp is smaller than minStamp");
+    }
+    this.minStamp = minStamp;
+    this.maxStamp = maxStamp;
+  }
+
+  /**
+   * Represents interval [minStamp, maxStamp) 
+   * @param minStamp the minimum timestamp, inclusive
+   * @param maxStamp the maximum timestamp, exclusive
+   * @throws IOException
+   */
+  public TimeRange(byte [] minStamp, byte [] maxStamp)
+  throws IOException {
+    this(Bytes.toLong(minStamp), Bytes.toLong(maxStamp));
+  }
+  
+  /**
+   * @return the smallest timestamp that should be considered
+   */
+  public long getMin() {
+    return minStamp;
+  }
+
+  /**
+   * @return the biggest timestamp that should be considered
+   */
+  public long getMax() {
+    return maxStamp;
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param bytes timestamp to check
+   * @param offset offset into the bytes
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(byte [] bytes, int offset) {
+  	if(allTime) return true;
+  	return withinTimeRange(Bytes.toLong(bytes, offset));
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinTimeRange(long timestamp) {
+  	if(allTime) return true;
+  	// check if >= minStamp
+  	return (minStamp <= timestamp && timestamp < maxStamp);
+  }
+  
+  /**
+   * Check if the specified timestamp is within this TimeRange.
+   * <p>
+   * Returns true if within interval [minStamp, maxStamp), false 
+   * if not.
+   * @param timestamp timestamp to check
+   * @return true if within TimeRange, false if not
+   */
+  public boolean withinOrAfterTimeRange(long timestamp) {
+    if(allTime) return true;
+    // check if >= minStamp
+    return (timestamp >= minStamp);
+  }
+  
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("maxStamp=");
+    sb.append(this.maxStamp);
+    sb.append(", minStamp=");
+    sb.append(this.minStamp);
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in) throws IOException {
+    this.minStamp = in.readLong();
+    this.maxStamp = in.readLong();
+    this.allTime = in.readBoolean();
+  }
+  
+  public void write(final DataOutput out) throws IOException {
+    out.writeLong(minStamp);
+    out.writeLong(maxStamp);
+    out.writeBoolean(this.allTime);
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Result.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Result.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Result.java	(revision 0)
@@ -0,0 +1,348 @@
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.TreeMap;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.SplitKeyValue;
+import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Single row result of a {@link Get} or {@link Scan} query.
+ */
+public class Result implements Writable {
+  private byte [] row = null;
+  private KeyValue [] kvs = null;
+  private NavigableMap<byte[], 
+     NavigableMap<byte[], NavigableMap<Long, byte[]>>> familyMap = null;
+
+  /**
+   * Constructor used for Writable.
+   */
+  public Result() {}
+  
+  /**
+   * Instantiate a Result with the specified array of KeyValues.
+   * @param kvs array of KeyValues
+   */
+  public Result(KeyValue [] kvs) {
+    if(kvs != null) {
+      this.row = kvs[0].getRow();
+      this.kvs = kvs;
+    }
+  }
+  
+  /**
+   * Instantiate a Result with the specified List of KeyValues.
+   * @param kvs List of KeyValues
+   */
+  public Result(List<KeyValue> kvs) {
+    this(kvs.toArray(new KeyValue[0]));
+  }
+  
+  /**
+   * Method for retrieving the row that this result is for
+   * @return row
+   */
+  public byte [] getRow() {
+    return this.row;
+  }
+  
+  /**
+   * Directly return the unsorted array of KeyValues in this Result.
+   * @return unsorted array of KeyValues
+   */
+  public KeyValue[] raw() {
+    return kvs;
+  }
+  
+  /**
+   * Return a sorted list of the KeyValues in this result.
+   * @return
+   */
+  public List<KeyValue> list() {
+    return Arrays.asList(sorted());
+  }
+  
+  /**
+   * Returns a sorted array of KeyValues in this Result.
+   * <p>
+   * Note: Sorting is done in place, so the backing array will be sorted
+   * after calling this method.
+   * @return sorted array of KeyValues
+   */
+  public KeyValue[] sorted() {
+    if(isEmpty()) {
+      return null;
+    }
+    Arrays.sort(kvs, (Comparator<KeyValue>)KeyValue.COMPARATOR);
+    return kvs;
+  }
+  
+  /**
+   * Map of families to all versions of its qualifiers and values.
+   * <p>
+   * Returns a three level Map of the form: 
+   * <code>Map<family,Map<qualifier,Map<timestamp,value>>></code>
+   * <p>
+   * Note: All other map returning methods make use of this map internally. 
+   * @return map from families to qualifiers to versions
+   */
+  public NavigableMap<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>> 
+  getMap() {
+    if(this.familyMap != null) {
+      return this.familyMap;
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    this.familyMap =
+      new TreeMap<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>>
+      (Bytes.BYTES_COMPARATOR);
+    for(KeyValue kv : this.kvs) {
+      SplitKeyValue splitKV = kv.split();
+      byte [] family = splitKV.getFamily();
+      NavigableMap<byte[], NavigableMap<Long, byte[]>> columnMap = 
+        familyMap.get(family);
+      if(columnMap == null) {
+        columnMap = new TreeMap<byte[], NavigableMap<Long, byte[]>>
+          (Bytes.BYTES_COMPARATOR);
+        familyMap.put(family, columnMap);
+      }
+      byte [] qualifier = splitKV.getQualifier();
+      NavigableMap<Long, byte[]> versionMap = columnMap.get(qualifier);
+      if(versionMap == null) {
+        versionMap = new TreeMap<Long, byte[]>(new Comparator<Long>() {
+          public int compare(Long l1, Long l2) {
+            return l2.compareTo(l1);
+          }
+        });
+        columnMap.put(qualifier, versionMap);
+      }
+      Long timestamp = Bytes.toLong(splitKV.getTimestamp());
+      byte [] value = splitKV.getValue();
+      versionMap.put(timestamp, value);
+    }
+    return this.familyMap;
+  }
+  
+  /**
+   * Map of families to their most recent qualifiers and values.
+   * <p>
+   * Returns a two level Map of the form: <code>Map<family,Map<qualifier,value>></code>
+   * <p>
+   * The most recent version of each qualifier will be used.
+   * @return map from families to qualifiers and value
+   */
+  public NavigableMap<byte[], NavigableMap<byte[], byte[]>> getNoVersionMap() {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    NavigableMap<byte[], NavigableMap<byte[], byte[]>> returnMap = 
+      new TreeMap<byte[], NavigableMap<byte[], byte[]>>(Bytes.BYTES_COMPARATOR);
+    for(Map.Entry<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>> 
+      familyEntry : familyMap.entrySet()) {
+      NavigableMap<byte[], byte[]> qualifierMap = 
+        new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+      for(Map.Entry<byte[], NavigableMap<Long, byte[]>> qualifierEntry :
+        familyEntry.getValue().entrySet()) {
+        byte [] value = 
+          qualifierEntry.getValue().get(qualifierEntry.getValue().firstKey());
+        qualifierMap.put(qualifierEntry.getKey(), value);
+      }
+      returnMap.put(familyEntry.getKey(), qualifierMap);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Map of qualifiers to values.
+   * <p>
+   * Returns a Map of the form: <code>Map<qualifier,value></code>
+   * @return map of qualifiers to values
+   */
+  public NavigableMap<byte[], byte[]> getFamilyMap(byte [] family) {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    NavigableMap<byte[], byte[]> returnMap = 
+      new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    NavigableMap<byte[], NavigableMap<Long, byte[]>> qualifierMap = 
+      familyMap.get(family);
+    if(qualifierMap == null) {
+      return returnMap;
+    }
+    for(Map.Entry<byte[], NavigableMap<Long, byte[]>> entry : 
+      qualifierMap.entrySet()) {
+      byte [] value = 
+        entry.getValue().get(entry.getValue().firstKey());
+      returnMap.put(entry.getKey(), value);
+    }
+    return returnMap;
+  }
+  
+  /**
+   * Get the latest version of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @return value of latest version of column, null if none found
+   */
+  public byte [] getValue(byte [] family, byte [] qualifier) {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return null;
+    }
+    NavigableMap<byte [], NavigableMap<Long, byte[]>> qualifierMap =
+      familyMap.get(family);
+    if(qualifierMap == null) {
+      return null;
+    }
+    NavigableMap<Long, byte[]> versionMap = qualifierMap.get(qualifier);
+    if(versionMap == null) {
+      return null;
+    }
+    return versionMap.firstEntry().getValue();
+  }
+  
+  /**
+   * Get the latest version of the specified column,
+   * using <pre>family:qualifier</pre> notation.
+   * @param column column in family:qualifier notation
+   * @return value of latest version of column, null if none found
+   */
+  public byte [] getValue(byte [] column) {
+    try {
+      byte [][] split = KeyValue.parseColumn(column);
+      return getValue(split[0], split[1]);
+    } catch(Exception e) {
+      return null;
+    }
+  }
+  
+  /**
+   * Checks for existence of the specified column.
+   * @param family family name
+   * @param qualifier column qualifier
+   * @return true if at least one value exists in the result, false if not
+   */
+  public boolean containsColumn(byte [] family, byte [] qualifier) {
+    if(this.familyMap == null) {
+      getMap();
+    }
+    if(isEmpty()) {
+      return false;
+    }
+    NavigableMap<byte [], NavigableMap<Long, byte[]>> qualifierMap =
+      familyMap.get(family);
+    if(qualifierMap == null) {
+      return false;
+    }
+    NavigableMap<Long, byte[]> versionMap = qualifierMap.get(qualifier);
+    if(versionMap == null) {
+      return false;
+    }
+    return true;
+  }
+  
+  /**
+   * Returns this Result in the old return format, {@link RowResult}.
+   * @return a RowResult
+   */
+  public RowResult rowResult() {
+    return RowResult.createRowResult(Arrays.asList(kvs));
+  }
+  
+  /**
+   * Returns the value of the first column in the Result.
+   * @return value of the first column
+   */
+  public byte [] value() {
+    if(isEmpty()) {
+      return null;
+    }
+    return kvs[0].getValue();
+  }
+  
+  /**
+   * Check if the underlying KeyValue [] is empty or not
+   * @return true if empty
+   */
+  public boolean isEmpty() {
+    return (this.kvs == null || this.kvs.length == 0);
+  }
+  
+  /**
+   * 
+   * @return the size of the underlying KeyValue []
+   */
+  public int size() {
+    return (this.kvs == null ? 0 : this.kvs.length);
+  }
+  
+  /**
+   * @return String
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", keyvalues=");
+    if(isEmpty()) {
+      sb.append("NONE");
+      return sb.toString();
+    }
+    sb.append("{");
+    boolean moreThanOne = false;
+    for(KeyValue kv : this.kvs) {
+      if(moreThanOne) {
+        sb.append(", ");
+      } else {
+        moreThanOne = true;
+      }
+      sb.append(kv.toString());
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    int length = in.readInt();
+    this.kvs = new KeyValue[length];
+    for(int i=0; i<length; i++) {
+      KeyValue kv = new KeyValue();
+      kv.readFields(in);
+      this.kvs[i] = kv;
+    }
+  }  
+  
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    int len = this.kvs.length;
+    out.writeInt(len);
+    for(int i=0; i<len; i++) {
+      this.kvs[i].write(out);
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Get.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Get.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Get.java	(revision 0)
@@ -0,0 +1,366 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.RowLock;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Get operations on a single row.
+ * <p>
+ * To get everything for a row, instantiate a Get object with the row to get.
+ * To further define the scope of what to get, perform additional methods as 
+ * outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Get implements Writable {
+  private byte [] row = null;
+  private long lockId = -1L;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], NavigableSet<byte []>> familyMap = 
+    new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+
+  /** Constructor for Writable.  DO NOT USE */
+  public Get() {}
+
+  /**
+   * Create a Get operation for the specified row.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   */
+  public Get(byte [] row) {
+    this(row, null);
+  }
+
+  /**
+   * Create a Get operation for the specified row, using an existing row lock.
+   * <p>
+   * If no further operations are done, this will get the latest version of
+   * all columns in all families of the specified row.
+   * @param row row key
+   * @param rowLock previously acquired row lock, or null
+   */
+  public Get(byte [] row, RowLock rowLock) {
+    this.row = row;
+    if(rowLock != null) {
+      this.lockId = rowLock.getLockId();
+    }
+  }
+
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+
+  /**
+   * Get the column from the specific family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    NavigableSet<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+
+  /**
+   * Adds an array of columns specified the old format, family:qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for any families in the input.
+   * @param columns array of columns, formatted as <pre>family:qualifier</pre>
+   */
+  public void addColumns(byte [][] columns) {
+    for(int i=0; i<columns.length; i++) {
+      try {
+        byte [][] split = KeyValue.parseColumn(columns[i]);
+        addColumn(split[0], split[1]);
+      } catch(Exception e) {}
+    }
+  }
+
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+    this.maxVersions = Integer.MAX_VALUE;
+  }
+
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) throws IOException {
+    if(maxVersions <= 0) {
+      throw new IOException("maxVersions must be positive");
+    }
+    this.maxVersions = maxVersions;
+  }
+
+  /**
+   * Apply the specified server-side filter when performing the Get.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+
+  /** Accessors */
+
+  /**
+   * Method for retrieving the get's row
+   * @return row 
+   */
+  public byte [] getRow() {
+    return this.row;
+  }
+
+  /**
+   * Method for retrieving the get's RowLock
+   * @return RowLock
+   */
+  public RowLock getRowLock() {
+    if(this.lockId == -1L) {
+      return null;
+    }
+    return new RowLock(this.row, this.lockId);
+  }
+
+  /**
+   * Method for retrieving the get's lockId
+   * @return lockId
+   */
+  public long getLockId() {
+    return this.lockId;
+  }
+
+  /**
+   * Method for retrieving the get's maximum number of version
+   * @return the maximum number of version to fetch for this get
+   */
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  /**
+   * Method for retrieving the get's TimeRange
+   * @return timeRange
+   */
+  public TimeRange getTimeRange() {
+    return this.tr;
+  }
+
+  /**
+   * Method for retrieving the keys in the familyMap
+   * @return keys in the current familyMap
+   */
+  public Set<byte[]> familySet() {
+    return this.familyMap.keySet();
+  }
+
+  /**
+   * Method for retrieving the number of families to get from
+   * @return number of families
+   */
+  public int numFamilies() {
+    return this.familyMap.size();
+  }
+
+  /**
+   * Method for checking if any families have been inserted into this Get
+   * @return true if familyMap is non empty false otherwise
+   */
+  public boolean hasFamilies() {
+    return !this.familyMap.isEmpty();
+  }
+
+  /**
+   * Method for retrieving the get's familyMap
+   * @return familyMap
+   */
+  public Map<byte[],NavigableSet<byte[]>> getFamilyMap() {
+    return this.familyMap;
+  }
+
+  /**
+   * @return String
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("row=");
+    sb.append(Bytes.toString(this.row));
+    sb.append(", maxVersions=");
+    sb.append("" + this.maxVersions);
+    sb.append(", timeRange=");
+    sb.append("[" + this.tr.getMin() + "," + this.tr.getMax() + ")");
+    sb.append(", families=");
+    if(this.familyMap.size() == 0) {
+      sb.append("ALL");
+      return sb.toString();
+    }
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], NavigableSet<byte[]>> entry : 
+      this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append("), ");
+      } else {
+        moreThanOne = true;
+        sb.append("{");
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", columns=");
+      if(entry.getValue() == null) {
+        sb.append("ALL");
+      } else {
+        sb.append("{");
+        boolean moreThanOneB = false;
+        for(byte [] column : entry.getValue()) {
+          if(moreThanOneB) {
+            sb.append(", ");
+          } else {
+            moreThanOneB = true;
+          }
+          sb.append(Bytes.toString(column));
+        }
+        sb.append("}");
+      }
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.row = Bytes.readByteArray(in);
+    this.lockId = in.readLong();
+    this.maxVersions = in.readInt();
+    boolean hasFilter = in.readBoolean();
+    if(hasFilter) {
+      this.filter = 
+        (RowFilterInterface)HbaseObjectWritable.readObject(in, null);
+    }
+    this.tr = new TimeRange();
+    tr.readFields(in);
+    int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [],NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      NavigableSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }
+
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.row);
+    out.writeLong(this.lockId);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, 
+          RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], NavigableSet<byte []>> entry : 
+      familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      NavigableSet<byte []> columnSet = entry.getValue();
+      out.writeInt(columnSet.size());
+      for(byte [] qualifier : columnSet) {
+        Bytes.writeByteArray(out, qualifier);
+      }
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java	(working copy)
@@ -122,10 +122,22 @@
     return this.length;
   }
  
+  /**
+   * @return the current length of the buffer. same as getSize()
+   */
+  //Should probably deprecate getSize() so that we keep the same calls for all
+  //byte []
   public int getLength() {
-    return getSize();
+    if (this.bytes == null) {
+      throw new IllegalStateException("Uninitialiized. Null constructor " +
+        "called w/o accompaying readFields invocation");
+    }
+    return this.length;
   }
-
+  
+  /**
+   * @return offset
+   */
   public int getOffset(){
     return this.offset;
   }
Index: src/java/org/apache/hadoop/hbase/io/Scan.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Scan.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/io/Scan.java	(revision 0)
@@ -0,0 +1,394 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.io;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.filter.RowFilterInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import org.apache.hadoop.io.Writable;
+
+/**
+ * Used to perform Scan operations.
+ * <p>
+ * All operations are identical to {@link Get} with the exception of
+ * instantiation.  Rather than specifying a single row, an optional startRow
+ * and stopRow may be defined.  If rows are not specified, the Scanner will
+ * iterate over all rows.
+ * <p>
+ * To scan everything for each row, instantiate a Scan object.
+ * To further define the scope of what to get when scanning, perform additional 
+ * methods as outlined below.
+ * <p>
+ * To get all columns from specific families, execute {@link #addFamily(byte[]) addFamily}
+ * for each family to retrieve.
+ * <p>
+ * To get specific columns, execute {@link #addColumn(byte[], byte[]) addColumn}
+ * for each column to retrieve.
+ * <p>
+ * To only retrieve columns within a specific range of version timestamps,
+ * execute {@link #setTimeRange(long, long) setTimeRange}.
+ * <p>
+ * To only retrieve columns with a specific timestamp, execute
+ * {@link #setTimeStamp(long) setTimestamp}.
+ * <p>
+ * To limit the number of versions of each column to be returned, execute
+ * {@link #setMaxVersions(int) setMaxVersions}.
+ * <p>
+ * To add a filter, execute {@link #setFilter(RowFilterInterface) setFilter}.
+ */
+public class Scan implements Writable {
+  private byte [] startRow = HConstants.EMPTY_START_ROW;
+  private byte [] stopRow  = HConstants.EMPTY_END_ROW;
+  private int maxVersions = 1;
+  private RowFilterInterface filter = null;
+  private TimeRange tr = new TimeRange();
+  private Map<byte [], NavigableSet<byte []>> familyMap =
+    new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+  
+  /**
+   * Create a Scan operation across all rows.
+   */
+  public Scan() {}
+  
+  /**
+   * Create a Scan operation starting at the specified row.
+   * <p>
+   * If the specified row does not exist, the Scanner will start from the
+   * next closest row after the specified row.
+   * @param startRow row to start scanner at or after
+   */
+  public Scan(byte [] startRow) {
+    this.startRow = startRow;
+  }
+  
+  /**
+   * Create a Scan operation for the range of rows specified.
+   * @param startRow row to start scanner at or after (inclusive)
+   * @param stopRow row to stop scanner before (exclusive)
+   */
+  public Scan(byte [] startRow, byte [] stopRow) {
+    this.startRow = startRow;
+    this.stopRow = stopRow;
+  }
+  
+  /**
+   * Get all columns from the specified family.
+   * <p>
+   * Overrides previous calls to addColumn for this family.
+   * @param family family name
+   */
+  public void addFamily(byte [] family) {
+    familyMap.remove(family);
+    familyMap.put(family, null);
+  }
+  
+  /**
+   * Get the column from the specified family with the specified qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for this family.
+   * @param family family name
+   * @param qualifier column qualifier
+   */
+  public void addColumn(byte [] family, byte [] qualifier) {
+    NavigableSet<byte []> set = familyMap.get(family);
+    if(set == null) {
+      set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+    }
+    set.add(qualifier);
+    familyMap.put(family, set);
+  }
+  
+  /**
+   * Adds an array of columns specified the old format, family:qualifier.
+   * <p>
+   * Overrides previous calls to addFamily for any families in the input.
+   * @param columns array of columns, formatted as <pre>family:qualifier</pre>
+   */
+  public void addColumns(byte [][] columns) {
+    for(int i=0; i<columns.length; i++) {
+      try {
+        byte [][] split = KeyValue.parseColumn(columns[i]);
+        addColumn(split[0], split[1]);
+      } catch(Exception e) {}
+    }
+  }
+  
+  /**
+   * Get versions of columns only within the specified timestamp range,
+   * [minStamp, maxStamp).
+   * @param minStamp minimum timestamp value, inclusive
+   * @param maxStamp maximum timestamp value, exclusive
+   * @throws IOException if invalid time range
+   */
+  public void setTimeRange(long maxStamp, long minStamp)
+  throws IOException {
+    tr = new TimeRange(maxStamp, minStamp);
+  }
+  
+  /**
+   * Get versions of columns with the specified timestamp.
+   * @param timestamp version timestamp  
+   */
+  public void setTimeStamp(long timestamp) {
+    tr = new TimeRange(timestamp);
+  }
+
+  /**
+   * Set the start row.
+   * @param startRow
+   */
+  public void setStartRow(byte [] startRow) {
+    this.startRow = startRow;
+  }
+  
+  /**
+   * Set the stop row.
+   * @param stopRow
+   */
+  public void setStopRow(byte [] stopRow) {
+    this.stopRow = stopRow;
+  }
+  
+  /**
+   * Get all available versions.
+   */
+  public void setMaxVersions() {
+  	this.maxVersions = Integer.MAX_VALUE;
+  }
+
+  /**
+   * Get up to the specified number of versions of each column.
+   * @param maxVersions maximum versions for each column
+   * @throws IOException if invalid number of versions
+   */
+  public void setMaxVersions(int maxVersions) {
+    this.maxVersions = maxVersions;
+  }
+  
+  /**
+   * Apply the specified server-side filter when performing the Scan.
+   * @param filter filter to run on the server
+   */
+  public void setFilter(RowFilterInterface filter) {
+    this.filter = filter;
+  }
+  
+  /**
+   * Setting the familyMap
+   * @param familyMap
+   */
+  public void setFamilyMap(Map<byte [], NavigableSet<byte []>> familyMap) {
+    this.familyMap = familyMap;
+  }
+  
+  /**
+   * Getting the familyMap
+   * @return familyMap
+   */
+  public Map<byte [], NavigableSet<byte []>> getFamilyMap() {
+    return this.familyMap;
+  }
+  
+  /**
+   * @return the number of families in familyMap
+   */
+  public int numFamilies() {
+    if(hasFamilies()) {
+      return this.familyMap.size();
+    }
+    return 0;
+  }
+
+  /**
+   * @return true if familyMap is non empty, false otherwise
+   */
+  public boolean hasFamilies() {
+    return !this.familyMap.isEmpty();
+  }
+  
+  /**
+   * @return the keys of the familyMap
+   */
+  public byte[][] getFamilies() {
+    if(hasFamilies()) {
+      return this.familyMap.keySet().toArray(new byte[0][0]);
+    }
+    return null;
+  }
+  
+  /**
+   * @return the startrow
+   */
+  public byte [] getStartRow() {
+    return this.startRow;
+  }
+
+  /**
+   * @return the stoprow
+   */
+  public byte [] getStopRow() {
+    return this.stopRow;
+  }
+  
+  /**
+   * @return the max number of versions to fetch
+   */
+  public int getMaxVersions() {
+    return this.maxVersions;
+  } 
+
+  /**
+   * @return TimeRange
+   */
+  public TimeRange getTimeRange() {
+    return this.tr;
+  } 
+  
+  /**
+   * @return RowFilter
+   */
+  public RowFilterInterface getFilter() {
+    return filter;
+  }
+  
+  /**
+   * @return true is a filter has been specified, false if not
+   */
+  public boolean hasFilter() {
+    return filter != null;
+  }
+  
+  /**
+   * @return String
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("startRow=");
+    sb.append(Bytes.toString(this.startRow));
+    sb.append(", stopRow=");
+    sb.append(Bytes.toString(this.stopRow));
+    sb.append(", maxVersions=");
+    sb.append("" + this.maxVersions);
+    sb.append(", timeRange=");
+    sb.append("[" + this.tr.getMin() + "," + this.tr.getMax() + ")");
+    sb.append(", families=");
+    if(this.familyMap.size() == 0) {
+      sb.append("ALL");
+      return sb.toString();
+    }
+    boolean moreThanOne = false;
+    for(Map.Entry<byte [], NavigableSet<byte[]>> entry : this.familyMap.entrySet()) {
+      if(moreThanOne) {
+        sb.append("), ");
+      } else {
+        moreThanOne = true;
+        sb.append("{");
+      }
+      sb.append("(family=");
+      sb.append(Bytes.toString(entry.getKey()));
+      sb.append(", columns=");
+      if(entry.getValue() == null) {
+        sb.append("ALL");
+      } else {
+        sb.append("{");
+        boolean moreThanOneB = false;
+        for(byte [] column : entry.getValue()) {
+          if(moreThanOneB) {
+            sb.append(", ");
+          } else {
+            moreThanOneB = true;
+          }
+          sb.append(Bytes.toString(column));
+        }
+        sb.append("}");
+      }
+    }
+    sb.append("}");
+    return sb.toString();
+  }
+  
+  //Writable
+  public void readFields(final DataInput in)
+  throws IOException {
+    this.startRow = Bytes.readByteArray(in);
+    this.stopRow = Bytes.readByteArray(in);
+    this.maxVersions = in.readInt();
+    boolean hasFilter = in.readBoolean();
+    if(hasFilter) {
+      this.filter = (RowFilterInterface)HbaseObjectWritable.readObject(in, 
+        new Configuration());
+    }
+    this.tr = new TimeRange();
+    tr.readFields(in);
+    int numFamilies = in.readInt();
+    this.familyMap = 
+      new TreeMap<byte [], NavigableSet<byte []>>(Bytes.BYTES_COMPARATOR);
+    for(int i=0; i<numFamilies; i++) {
+      byte [] family = Bytes.readByteArray(in);
+      int numColumns = in.readInt();
+      TreeSet<byte []> set = new TreeSet<byte []>(Bytes.BYTES_COMPARATOR);
+      for(int j=0; j<numColumns; j++) {
+        byte [] qualifier = Bytes.readByteArray(in);
+        set.add(qualifier);
+      }
+      this.familyMap.put(family, set);
+    }
+  }  
+
+  public void write(final DataOutput out)
+  throws IOException {
+    Bytes.writeByteArray(out, this.startRow);
+    Bytes.writeByteArray(out, this.stopRow);
+    out.writeInt(this.maxVersions);
+    if(this.filter == null) {
+      out.writeBoolean(false);
+    } else {
+      out.writeBoolean(true);
+      HbaseObjectWritable.writeObject(out, this.filter, 
+          RowFilterInterface.class, null);
+    }
+    tr.write(out);
+    out.writeInt(familyMap.size());
+    for(Map.Entry<byte [], NavigableSet<byte []>> entry : familyMap.entrySet()) {
+      Bytes.writeByteArray(out, entry.getKey());
+      NavigableSet<byte []> columnSet = entry.getValue();
+      if(columnSet != null){
+        out.writeInt(columnSet.size());
+        for(byte [] qualifier : columnSet) {
+          Bytes.writeByteArray(out, qualifier);
+        }
+      } else {
+        out.writeInt(0);
+      }
+    }
+  }
+}
Index: src/java/org/apache/hadoop/hbase/io/Reference.java
===================================================================
--- src/java/org/apache/hadoop/hbase/io/Reference.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/io/Reference.java	(working copy)
@@ -65,10 +65,17 @@
     this(null, Range.bottom);
   }
 
+  /**
+   * 
+   * @return Range
+   */
   public Range getFileRegion() {
     return this.region;
   }
 
+  /**
+   * @return splitKey
+   */
   public byte [] getSplitKey() {
     return splitkey;
   }
Index: src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java
===================================================================
--- src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -34,7 +34,9 @@
 import org.apache.hadoop.hbase.filter.StopRowFilter;
 import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Writables;
 import org.apache.hadoop.mapred.InputFormat;
@@ -73,7 +75,7 @@
  * </pre>
  */
 public abstract class TableInputFormatBase
-implements InputFormat<ImmutableBytesWritable, RowResult> {
+implements InputFormat<ImmutableBytesWritable, Result> {
   final Log LOG = LogFactory.getLog(TableInputFormatBase.class);
   private byte [][] inputColumns;
   private HTable table;
@@ -84,7 +86,7 @@
    * Iterate over an HBase table data, return (Text, RowResult) pairs
    */
   protected class TableRecordReader
-  implements RecordReader<ImmutableBytesWritable, RowResult> {
+  implements RecordReader<ImmutableBytesWritable, Result> {
     private byte [] startRow;
     private byte [] endRow;
     private byte [] lastRow;
@@ -106,16 +108,21 @@
             new HashSet<RowFilterInterface>();
           rowFiltersSet.add(new WhileMatchRowFilter(new StopRowFilter(endRow)));
           rowFiltersSet.add(trrRowFilter);
-          this.scanner = this.htable.getScanner(trrInputColumns, startRow,
-            new RowFilterSet(RowFilterSet.Operator.MUST_PASS_ALL,
+          Scan scan = new Scan(startRow);
+          scan.addColumns(trrInputColumns);
+          scan.setFilter(new RowFilterSet(RowFilterSet.Operator.MUST_PASS_ALL,
               rowFiltersSet));
+          this.scanner = this.htable.getScanner(scan);
         } else {
-          this.scanner =
-            this.htable.getScanner(trrInputColumns, firstRow, endRow);
+          Scan scan = new Scan(firstRow, endRow);
+          scan.addColumns(trrInputColumns);
+          this.scanner = this.htable.getScanner(scan);
         }
       } else {
-        this.scanner =
-          this.htable.getScanner(trrInputColumns, firstRow, trrRowFilter);
+        Scan scan = new Scan(firstRow);
+        scan.addColumns(trrInputColumns);
+        scan.setFilter(trrRowFilter);
+        this.scanner = this.htable.getScanner(scan);
       }
     }
 
@@ -182,8 +189,8 @@
      *
      * @see org.apache.hadoop.mapred.RecordReader#createValue()
      */
-    public RowResult createValue() {
-      return new RowResult();
+    public Result createValue() {
+      return new Result();
     }
 
     public long getPos() {
@@ -203,9 +210,9 @@
      * @return true if there was more data
      * @throws IOException
      */
-    public boolean next(ImmutableBytesWritable key, RowResult value)
+    public boolean next(ImmutableBytesWritable key, Result value)
     throws IOException {
-      RowResult result;
+      Result result;
       try {
         result = this.scanner.next();
       } catch (UnknownScannerException e) {
@@ -232,7 +239,7 @@
    * @see org.apache.hadoop.mapred.InputFormat#getRecordReader(InputSplit,
    *      JobConf, Reporter)
    */
-  public RecordReader<ImmutableBytesWritable, RowResult> getRecordReader(
+  public RecordReader<ImmutableBytesWritable, Result> getRecordReader(
       InputSplit split, JobConf job, Reporter reporter)
   throws IOException {
     TableSplit tSplit = (TableSplit) split;
Index: src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java
===================================================================
--- src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/mapred/TableOutputFormat.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.mapred.FileAlreadyExistsException;
 import org.apache.hadoop.mapred.InvalidJobConfException;
 import org.apache.hadoop.mapred.JobConf;
@@ -40,7 +41,7 @@
  * Convert Map/Reduce output and write it to an HBase table
  */
 public class TableOutputFormat extends
-FileOutputFormat<ImmutableBytesWritable, BatchUpdate> {
+FileOutputFormat<ImmutableBytesWritable, Put> {
 
   /** JobConf parameter that specifies the output table */
   public static final String OUTPUT_TABLE = "hbase.mapred.outputtable";
@@ -51,7 +52,7 @@
    * and write to an HBase table
    */
   protected static class TableRecordWriter
-    implements RecordWriter<ImmutableBytesWritable, BatchUpdate> {
+    implements RecordWriter<ImmutableBytesWritable, Put> {
     private HTable m_table;
 
     /**
@@ -69,8 +70,8 @@
     }
 
     public void write(ImmutableBytesWritable key,
-        BatchUpdate value) throws IOException {
-      m_table.commit(new BatchUpdate(value));
+        Put value) throws IOException {
+      m_table.put(new Put(value));
     }
   }
   
Index: src/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java
===================================================================
--- src/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java	(working copy)
@@ -390,6 +390,7 @@
    * @param addr
    * @param conf
    * @param maxAttempts
+   * @param timeout
    * @return proxy
    * @throws IOException
    */
Index: src/java/org/apache/hadoop/hbase/ipc/TransactionalRegionInterface.java
===================================================================
--- src/java/org/apache/hadoop/hbase/ipc/TransactionalRegionInterface.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/ipc/TransactionalRegionInterface.java	(working copy)
@@ -19,10 +19,11 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 
 /**
  * Interface for transactional region servers.
@@ -136,33 +137,24 @@
    * 
    * @param transactionId
    * @param regionName region name
-   * @param row row key
+   * @param delete
    * @param timestamp Delete all entries that have this timestamp or older
    * @throws IOException
    */
-  public void deleteAll(long transactionId, byte[] regionName, byte[] row,
-      long timestamp) throws IOException;
-
+  public void delete(long transactionId, byte [] regionName, Delete delete)
+  throws IOException;
+  
   /**
    * Opens a remote scanner with a RowFilter.
    * 
    * @param transactionId
    * @param regionName name of region to scan
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex for column family name. A column name is judged to be regex if
-   * it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row to scan
-   * @param timestamp only return values whose timestamp is <= this value
-   * @param filter RowFilter for filtering results at the row-level.
-   * 
+   * @param scan
    * @return scannerId scanner identifier used in other calls
    * @throws IOException
    */
   public long openScanner(final long transactionId, final byte[] regionName,
-      final byte[][] columns, final byte[] startRow, long timestamp,
-      RowFilterInterface filter) throws IOException;
+      Scan scan) throws IOException;
 
   /**
    * Applies a batch of updates via one RPC
Index: src/java/org/apache/hadoop/hbase/ipc/HBaseServer.java
===================================================================
--- src/java/org/apache/hadoop/hbase/ipc/HBaseServer.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/ipc/HBaseServer.java	(working copy)
@@ -86,7 +86,8 @@
   public static final Log LOG =
     LogFactory.getLog("org.apache.hadoop.ipc.HBaseServer");
 
-  protected static final ThreadLocal<HBaseServer> SERVER = new ThreadLocal<HBaseServer>();
+  protected static final ThreadLocal<HBaseServer> SERVER =
+    new ThreadLocal<HBaseServer>();
 
   /** Returns the server instance called under or null.  May be called under
    * {@link #call(Writable, long)} implementations, and under {@link Writable}
@@ -128,10 +129,11 @@
   private int handlerCount;                       // number of handler threads
   protected Class<? extends Writable> paramClass; // class of call parameters
   protected int maxIdleTime;                      // the maximum idle time after 
-                                                  // which a client may be disconnected
-  protected int thresholdIdleConnections;         // the number of idle connections
-                                                  // after which we will start
-                                                  // cleaning up idle 
+                                                  // which a client may be
+                                                  // disconnected
+  protected int thresholdIdleConnections;         // the number of idle
+                                                  // connections after which we 
+                                                  // will start cleaning up idle 
                                                   // connections
   int maxConnectionsToNuke;                       // the max number of 
                                                   // connections to nuke
@@ -173,8 +175,9 @@
     try {
       socket.bind(address, backlog);
     } catch (BindException e) {
-      BindException bindException = new BindException("Problem binding to " + address
-                                                      + " : " + e.getMessage());
+      BindException bindException =
+        new BindException("Problem binding to " + address + " : " + 
+            e.getMessage());
       bindException.initCause(e);
       throw bindException;
     } catch (SocketException e) {
@@ -297,7 +300,6 @@
     public void run() {
       LOG.info(getName() + ": starting");
       SERVER.set(HBaseServer.this);
-      long lastPurgeTime = 0;   // last check for old calls.
 
       while (running) {
         SelectionKey key = null;
Index: src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java
===================================================================
--- src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/ipc/HRegionInterface.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -21,11 +21,14 @@
 
 import java.io.IOException;
 
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Scan;
 
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.NotServingRegionException;
@@ -70,12 +73,12 @@
    * 
    * @param regionName region name
    * @param row row key
-   * @param columnFamily Column family to look for row in.
+   * @param family Column family to look for row in.
    * @return map of values
    * @throws IOException
    */
-  public RowResult getClosestRowBefore(final byte [] regionName,
-    final byte [] row, final byte [] columnFamily)
+  public Result getClosestRowBefore(final byte [] regionName,
+    final byte [] row, final byte [] family)
   throws IOException;
 
   /**
@@ -96,26 +99,22 @@
   throws IOException;
 
   /**
-   * Applies a batch of updates via one RPC
-   * 
-   * @param regionName name of the region to update
-   * @param b BatchUpdate
-   * @param lockId lock id
+   * Put data into the specified region 
+   * @param regionName
+   * @param put the data to be put
    * @throws IOException
    */
-  public void batchUpdate(final byte [] regionName, final BatchUpdate b,
-      final long lockId)
+  public void put(final byte [] regionName, final Put put)
   throws IOException;
   
   /**
-   * Applies a batch of updates via one RPC for many rows
-   * 
-   * @param regionName name of the region to update
-   * @param b BatchUpdate[]
+   * Put an array of puts into the specified region
+   * @param regionName
+   * @param puts
+   * @return
    * @throws IOException
-   * @return number of updates applied
    */
-  public int batchUpdates(final byte[] regionName, final BatchUpdate[] b)
+  public int put(final byte[] regionName, final Put [] puts)
   throws IOException;
   
   /**
@@ -124,91 +123,27 @@
    * the given values in expectedValues
    * 
    * @param regionName name of the region to update
-   * @param b BatchUpdate
+   * @param put the data to be inserted
    * @param expectedValues map of column names to expected data values.
    * @return true if update was applied
    * @throws IOException
    */
-  public boolean checkAndSave(final byte [] regionName, final BatchUpdate b,
+  public boolean checkAndSave(final byte [] regionName, final Put put,
       final HbaseMapWritable<byte[],byte[]> expectedValues)
   throws IOException;
   
-
   /**
-   * Delete all cells that match the passed row and column and whose timestamp
-   * is equal-to or older than the passed timestamp.
-   * 
-   * @param regionName region name
-   * @param row row key
-   * @param column column key
-   * @param timestamp Delete all entries that have this timestamp or older
-   * @param lockId lock id
-   * @throws IOException
-   */
-  public void deleteAll(byte [] regionName, byte [] row, byte [] column,
-    long timestamp, long lockId)
-  throws IOException;
-
-  /**
-   * Delete all cells that match the passed row and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   *
-   * @param regionName region name
-   * @param row row key
-   * @param timestamp Delete all entries that have this timestamp or older
-   * @param lockId lock id
-   * @throws IOException
-   */
-  public void deleteAll(byte [] regionName, byte [] row, long timestamp,
-      long lockId)
-  throws IOException;
-  
-  /**
-   * Delete all cells that match the passed row & the column regex and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   * 
+   * Deletes all the KeyValues that match those found in the Delete object, 
+   * if their ts <= to the Delete. In case of a delete with a specific ts it
+   * only deletes that specific KeyValue.
    * @param regionName
-   * @param row
-   * @param colRegex
-   * @param timestamp
-   * @param lockId
+   * @param delete
    * @throws IOException
    */
-  public void deleteAllByRegex(byte [] regionName, byte [] row, String colRegex, 
-      long timestamp, long lockId)
+  public void delete(final byte[] regionName, final Delete delete)
   throws IOException;
-
-  /**
-   * Delete all cells for a row with matching column family with timestamps
-   * less than or equal to <i>timestamp</i>.
-   *
-   * @param regionName The name of the region to operate on
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @param timestamp Timestamp to match
-   * @param lockId lock id
-   * @throws IOException
-   */
-  public void deleteFamily(byte [] regionName, byte [] row, byte [] family, 
-    long timestamp, long lockId)
-  throws IOException;
   
   /**
-   * Delete all cells for a row with matching column family regex with 
-   * timestamps less than or equal to <i>timestamp</i>.
-   * 
-   * @param regionName The name of the region to operate on
-   * @param row The row to operate on
-   * @param familyRegex column family regex
-   * @param timestamp Timestamp to match
-   * @param lockId lock id
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(byte [] regionName, byte [] row, String familyRegex, 
-    long timestamp, long lockId) 
-  throws IOException;
-
-  /**
    * Returns true if any cells exist for the given coordinate.
    * 
    * @param regionName The name of the region
@@ -231,20 +166,13 @@
    * Opens a remote scanner with a RowFilter.
    * 
    * @param regionName name of region to scan
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex for column family name. A column name is judged to be
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row to scan
-   * @param timestamp only return values whose timestamp is <= this value
-   * @param filter RowFilter for filtering results at the row-level.
-   *
+   * @param startRow first row where to start scanning
+   * @param scan configured scan object
    * @return scannerId scanner identifier used in other calls
    * @throws IOException
    */
-  public long openScanner(final byte [] regionName, final byte [][] columns,
-      final byte [] startRow, long timestamp, RowFilterInterface filter)
+  public long openScanner(final byte [] regionName, final byte [] startRow, 
+      final Scan scan)
   throws IOException;
   
   /**
@@ -253,7 +181,7 @@
    * @return map of values
    * @throws IOException
    */
-  public RowResult next(long scannerId) throws IOException;
+  public Result next(long scannerId) throws IOException;
   
   /**
    * Get the next set of values
@@ -262,7 +190,7 @@
    * @return map of values
    * @throws IOException
    */
-  public RowResult[] next(long scannerId, int numberOfRows) throws IOException;
+  public Result [] next(long scannerId, int numberOfRows) throws IOException;
   
   /**
    * Close a scanner
@@ -271,7 +199,7 @@
    * @throws IOException
    */
   public void close(long scannerId) throws IOException;
-  
+
   /**
    * Opens a remote row lock.
    *
@@ -306,4 +234,18 @@
    */
   public long incrementColumnValue(byte [] regionName, byte [] row,
       byte [] column, long amount) throws IOException;
+  
+  
+  //
+  // HBASE-880
+  //
+  
+  /**
+   * Perform Get operation.
+   * @param regionName name of region to get from
+   * @param get Get operation
+   * @return Result
+   * @throws IOException
+   */
+  public Result get(byte [] regionName, Get get) throws IOException;
 }
Index: src/java/org/apache/hadoop/hbase/util/Merge.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/Merge.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/util/Merge.java	(working copy)
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.io.Delete;
 import org.apache.hadoop.hbase.regionserver.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.io.WritableComparator;
@@ -309,7 +310,10 @@
     if (LOG.isDebugEnabled()) {
       LOG.debug("Removing region: " + regioninfo + " from " + meta);
     }
-    meta.deleteAll(regioninfo.getRegionName(), System.currentTimeMillis(), null);
+    
+    Delete delete  = new Delete(regioninfo.getRegionName(), 
+        System.currentTimeMillis(), null);
+    meta.delete(delete, null, true);
   }
 
   /*
Index: src/java/org/apache/hadoop/hbase/util/MetaUtils.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/MetaUtils.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/util/MetaUtils.java	(working copy)
@@ -39,6 +39,9 @@
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.regionserver.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.InternalScanner;
@@ -188,9 +191,10 @@
       openRootRegion();
     }
 
-    InternalScanner rootScanner = rootRegion.getScanner(
-        HConstants.COL_REGIONINFO_ARRAY, HConstants.EMPTY_START_ROW,
-        HConstants.LATEST_TIMESTAMP, null);
+    Scan scan = new Scan();
+    scan.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+    InternalScanner rootScanner = 
+      rootRegion.getScanner(HConstants.EMPTY_START_ROW, scan);
 
     try {
       List<KeyValue> results = new ArrayList<KeyValue>();
@@ -243,16 +247,19 @@
    */
   public void scanMetaRegion(final HRegion m, final ScannerListener listener)
   throws IOException {
-    InternalScanner metaScanner = m.getScanner(HConstants.COL_REGIONINFO_ARRAY,
-      HConstants.EMPTY_START_ROW, HConstants.LATEST_TIMESTAMP, null);
+    
+    Scan scan = new Scan();
+    scan.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
+    InternalScanner metaScanner = 
+      m.getScanner(HConstants.EMPTY_START_ROW, scan);
+    
     try {
       List<KeyValue> results = new ArrayList<KeyValue>();
       while (metaScanner.next(results)) {
         HRegionInfo info = null;
         for (KeyValue kv: results) {
-          if (KeyValue.META_COMPARATOR.compareColumns(kv,
-            HConstants.COL_REGIONINFO, 0, HConstants.COL_REGIONINFO.length,
-              HConstants.COLUMN_FAMILY_STR.length()) == 0) {
+          if(kv.matchingColumn(HConstants.CATALOG_FAMILY,
+              HConstants.REGIONINFO_QUALIFIER)) {
             info = Writables.getHRegionInfoOrNull(kv.getValue());
             if (info == null) {
               LOG.warn("region info is null for row " +
@@ -312,12 +319,18 @@
     }
     // Throws exception if null.
     HRegionInfo info = Writables.getHRegionInfo(cell);
-    BatchUpdate b = new BatchUpdate(row);
+    Put put = new Put(row);
     info.setOffline(onlineOffline);
-    b.put(HConstants.COL_REGIONINFO, Writables.getBytes(info));
-    b.delete(HConstants.COL_SERVER);
-    b.delete(HConstants.COL_STARTCODE);
-    t.commit(b);
+    put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER, 
+        Writables.getBytes(info));
+    t.put(put);
+    
+    Delete delete = new Delete(row);
+    delete.deleteColumn(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER);
+    delete.deleteColumn(HConstants.CATALOG_FAMILY,
+        HConstants.STARTCODE_QUALIFIER);
+    
+    t.delete(delete);
   }
   
   /**
@@ -408,9 +421,10 @@
         " for " + hri.toString() + " in " + r.toString() + " is: " +
         h.toString());
     }
-    BatchUpdate b = new BatchUpdate(hri.getRegionName());
-    b.put(HConstants.COL_REGIONINFO, Writables.getBytes(hri));
-    r.batchUpdate(b, null);
+    Put put = new Put(hri.getRegionName());
+    put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER, 
+        Writables.getBytes(hri));
+    r.put(put);
     if (LOG.isDebugEnabled()) {
       HRegionInfo h = Writables.getHRegionInfoOrNull(
           r.get(hri.getRegionName(), HConstants.COL_REGIONINFO, -1, -1).get(0).getValue());
Index: src/java/org/apache/hadoop/hbase/util/Bytes.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/Bytes.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/util/Bytes.java	(working copy)
@@ -39,6 +39,7 @@
  * HashSets, etc.
  */
 public class Bytes {
+  
   /**
    * Size of long in bytes
    */
@@ -81,6 +82,9 @@
    * Byte array comparator class.
    */
   public static class ByteArrayComparator implements RawComparator<byte []> {
+    /**
+     * Constructor
+     */
     public ByteArrayComparator() {
       super();
     }
@@ -150,6 +154,8 @@
    * Write byte-array to out with a vint length prefix.
    * @param out
    * @param b
+   * @param offset
+   * @param length
    * @throws IOException
    */
   public static void writeByteArray(final DataOutput out, final byte [] b,
@@ -182,6 +188,8 @@
    * @param tgtBytes the byte array
    * @param tgtOffset position in the array
    * @param srcBytes byte to write out
+   * @param srcOffset
+   * @param srcLength
    * @return incremented offset
    */
   public static int putBytes(byte[] tgtBytes, int tgtOffset, byte[] srcBytes,
@@ -382,6 +390,10 @@
     return putInt(bytes, offset, i);
   }
 
+  /**
+   * @param f
+   * @return the float represented as byte []
+   */
   public static byte [] toBytes(final float f) {
     // Encode it as int
     int i = Float.floatToRawIntBits(f);
@@ -417,6 +429,10 @@
     return putLong(bytes, offset, l);
   }
 
+  /**
+   * @param d
+   * @return the double represented as byte []
+   */
   public static byte [] toBytes(final double d) {
     // Encode it as a long
     long l = Double.doubleToRawLongBits(d);
@@ -521,6 +537,7 @@
   /**
    * Converts a byte array to a short value
    * @param bytes
+   * @param offset
    * @return the short value
    */
   public static short toShort(byte[] bytes, int offset) {
@@ -530,6 +547,8 @@
   /**
    * Converts a byte array to a short value
    * @param bytes
+   * @param offset
+   * @param lengths
    * @return the short value
    */
   public static short toShort(byte[] bytes, int offset, final int length) {
Index: src/java/org/apache/hadoop/hbase/util/Migrate.java
===================================================================
--- src/java/org/apache/hadoop/hbase/util/Migrate.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/util/Migrate.java	(working copy)
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.util.GenericOptionsParser;
 import org.apache.hadoop.util.Tool;
@@ -249,9 +250,10 @@
     if (!enableBlockCache(oldHri)) {
       return;
     }
-    BatchUpdate b = new BatchUpdate(oldHri.getRegionName());
-    b.put(HConstants.COL_REGIONINFO, Writables.getBytes(oldHri));
-    mr.batchUpdate(b);
+    Put put = new Put(oldHri.getRegionName());
+    put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER, 
+        Writables.getBytes(oldHri));
+    mr.put(put);
     LOG.info("Enabled blockcache on " + oldHri.getRegionNameAsString());
   }
 
@@ -283,9 +285,10 @@
     if (!updateVersions(oldHri)) {
       return;
     }
-    BatchUpdate b = new BatchUpdate(oldHri.getRegionName());
-    b.put(HConstants.COL_REGIONINFO, Writables.getBytes(oldHri));
-    mr.batchUpdate(b);
+    Put put = new Put(oldHri.getRegionName());
+    put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER, 
+        Writables.getBytes(oldHri));
+    mr.put(put);
     LOG.info("Upped versions on " + oldHri.getRegionNameAsString());
   }
 
Index: src/java/org/apache/hadoop/hbase/client/HTable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HTable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/HTable.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -34,18 +34,17 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HStoreKey;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.filter.StopRowFilter;
-import org.apache.hadoop.hbase.filter.WhileMatchRowFilter;
-import org.apache.hadoop.hbase.io.BatchOperation;
-import org.apache.hadoop.hbase.io.BatchUpdate;
 import org.apache.hadoop.hbase.io.Cell;
+import org.apache.hadoop.hbase.io.Delete;
+import org.apache.hadoop.hbase.io.Get;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
 import org.apache.hadoop.hbase.io.RowResult;
 import org.apache.hadoop.hbase.io.HbaseMapWritable;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Writables;
@@ -58,7 +57,7 @@
   private final byte [] tableName;
   protected final int scannerTimeout;
   private volatile HBaseConfiguration configuration;
-  private ArrayList<BatchUpdate> writeBuffer;
+  private ArrayList<Put> writeBuffer;
   private long writeBufferSize;
   private boolean autoFlush;
   private long currentWriteBufferSize;
@@ -113,7 +112,7 @@
       conf.getInt("hbase.regionserver.lease.period", 60 * 1000);
     this.configuration = conf;
     this.connection.locateRegion(tableName, HConstants.EMPTY_START_ROW);
-    this.writeBuffer = new ArrayList<BatchUpdate>();
+    this.writeBuffer = new ArrayList<Put>();
     this.writeBufferSize = 
       this.configuration.getLong("hbase.client.write.buffer", 2097152);
     this.autoFlush = true;
@@ -252,9 +251,10 @@
     final List<byte[]> startKeyList = new ArrayList<byte[]>();
     final List<byte[]> endKeyList = new ArrayList<byte[]>();
     MetaScannerVisitor visitor = new MetaScannerVisitor() {
-      public boolean processRow(RowResult rowResult) throws IOException {
+      public boolean processRow(Result rowResult) throws IOException {
         HRegionInfo info = Writables.getHRegionInfo(
-            rowResult.get(HConstants.COL_REGIONINFO));
+            rowResult.getValue(HConstants.CATALOG_FAMILY, 
+                HConstants.REGIONINFO_QUALIFIER));
         if (Bytes.equals(info.getTableDesc().getName(), getTableName())) {
           if (!(info.isOffline() || info.isSplit())) {
             startKeyList.add(info.getStartKey());
@@ -280,18 +280,20 @@
       new TreeMap<HRegionInfo, HServerAddress>();
 
     MetaScannerVisitor visitor = new MetaScannerVisitor() {
-      public boolean processRow(RowResult rowResult) throws IOException {
+      public boolean processRow(Result rowResult) throws IOException {
         HRegionInfo info = Writables.getHRegionInfo(
-            rowResult.get(HConstants.COL_REGIONINFO));
+            rowResult.getValue(HConstants.CATALOG_FAMILY, 
+                HConstants.REGIONINFO_QUALIFIER));
         
         if (!(Bytes.equals(info.getTableDesc().getName(), getTableName()))) {
           return false;
         }
 
         HServerAddress server = new HServerAddress();
-        Cell c = rowResult.get(HConstants.COL_SERVER);
-        if (c != null && c.getValue() != null && c.getValue().length > 0) {
-          String address = Bytes.toString(c.getValue());
+        byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY, 
+            HConstants.SERVER_QUALIFIER);
+        if (value != null && value.length > 0) {
+          String address = Bytes.toString(value);
           server = new HServerAddress(address);
         }
         
@@ -643,13 +645,13 @@
     );
   }
 
-  public RowResult getClosestRowBefore(final byte[] row, final byte[] columnFamily)
+  public Result getClosestRowBefore(final byte [] row, final byte [] family)
   throws IOException {
     return connection.getRegionServerWithRetries(
-      new ServerCallable<RowResult>(connection,tableName,row) {
-        public RowResult call() throws IOException {
+      new ServerCallable<Result>(connection,tableName,row) {
+        public Result call() throws IOException {
           return server.getClosestRowBefore(
-              location.getRegionInfo().getRegionName(), row, columnFamily
+              location.getRegionInfo().getRegionName(), row, family
             );
           }
         }
@@ -657,595 +659,52 @@
     }
 
   /** 
-   * Get a scanner on the current table starting at first row.
-   * Return the specified columns.
+   * Get a scanner on the current table as specified by the {@link Scan} object
    *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
+   * @param scan a configured {@link Scan} object
    * @return scanner
    * @throws IOException
    */
-  public Scanner getScanner(final String [] columns)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), HConstants.EMPTY_START_ROW);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final String [] columns, final String startRow)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow));
-  }
-
-  /** 
-   * Get a scanner on the current table starting at first row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns)
-  throws IOException {
-    return getScanner(columns, HConstants.EMPTY_START_ROW,
-      HConstants.LATEST_TIMESTAMP, null);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow)
-  throws IOException {
-    return getScanner(columns, startRow, HConstants.LATEST_TIMESTAMP, null);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow,
-    long timestamp)
-  throws IOException {
-    return getScanner(columns, startRow, timestamp, null);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte[][] columns, final byte [] startRow,
-    RowFilterInterface filter)
-  throws IOException { 
-    return getScanner(columns, startRow, HConstants.LATEST_TIMESTAMP, filter);
-  }
-  
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, final byte [] stopRow)
-  throws IOException {
-    return getScanner(columns, startRow, stopRow,
-      HConstants.LATEST_TIMESTAMP);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final String [] columns,
-    final String startRow, final String stopRow, final long timestamp)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow),
-      Bytes.toBytes(stopRow), timestamp);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row and
-   * ending just before <code>stopRow<code>.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param stopRow Row to stop scanning on. Once we hit this row we stop
-   * returning values; i.e. we return the row before this one but not the
-   * <code>stopRow</code> itself.
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, final byte [] stopRow, final long timestamp)
-  throws IOException {
-    return getScanner(columns, startRow, timestamp,
-      new WhileMatchRowFilter(new StopRowFilter(stopRow)));
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(String[] columns,
-    String startRow, long timestamp, RowFilterInterface filter)
-  throws IOException {
-    return getScanner(Bytes.toByteArrays(columns), Bytes.toBytes(startRow),
-      timestamp, filter);
-  }
-
-  /** 
-   * Get a scanner on the current table starting at the specified row.
-   * Return the specified columns.
-   *
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned.  Its also possible
-   * to pass a regex in the column qualifier. A column qualifier is judged to
-   * be a regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final byte [][] columns,
-    final byte [] startRow, long timestamp, RowFilterInterface filter)
-  throws IOException {
-    ClientScanner s = new ClientScanner(columns, startRow,
-        timestamp, filter);
+  public Scanner getScanner(final Scan scan) throws IOException {
+    ClientScanner s = new ClientScanner(scan);
     s.initialize();
     return s;
   }
   
-  /**
-   * Completely delete the row's cells.
-   *
-   * @param row Key of the row you want to completely delete.
-   * @throws IOException
-   */
-  public void deleteAll(final byte [] row) throws IOException {
-    deleteAll(row, null);
+  public Scanner getScanner(byte [] family) throws IOException {
+    Scan scan = new Scan();
+    scan.addFamily(family);
+    return getScanner(scan);
   }
-
-  /**
-   * Completely delete the row's cells.
-   *
-   * @param row Key of the row you want to completely delete.
-   * @throws IOException
-   */
-  public void deleteAll(final String row) throws IOException {
-    deleteAll(row, null);
-  }
   
-  /**
-   * Completely delete the row's cells.
-   *
-   * @param row Key of the row you want to completely delete.
-   * @param column column to be deleted
-   * @throws IOException
-   */
-  public void deleteAll(final byte [] row, final byte [] column)
+  public Scanner getScanner(byte [] family, byte [] qualifier)
   throws IOException {
-    deleteAll(row, column, HConstants.LATEST_TIMESTAMP);
+    Scan scan = new Scan();
+    scan.addColumn(family, qualifier);
+    return getScanner(scan);
   }
 
   /**
-   * Completely delete the row's cells.
-   *
-   * @param row Key of the row you want to completely delete.
-   * @param ts Delete all cells of the same timestamp or older.
+   * 
+   * @param delete 
    * @throws IOException
    */
-  public void deleteAll(final byte [] row, final long ts)
+  public synchronized void delete(final Delete delete)
   throws IOException {
-    deleteAll(row, null, ts);
-  }
-
-  /**
-   * Completely delete the row's cells.
-   *
-   * @param row Key of the row you want to completely delete.
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException
-   */
-  public void deleteAll(final String row, final long ts)
-  throws IOException {
-    deleteAll(row, null, ts);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column.
-   * @param row Row to update
-   * @param column name of column whose value is to be deleted
-   * @throws IOException 
-   */
-  public void deleteAll(final String row, final String column)
-  throws IOException {
-    deleteAll(row, column, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   * @param row Row to update
-   * @param column name of column whose value is to be deleted
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException 
-   */
-  public void deleteAll(final String row, final String column, final long ts)
-  throws IOException {
-    deleteAll(Bytes.toBytes(row),
-      column != null? Bytes.toBytes(column): null, ts);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   * @param row Row to update
-   * @param column name of column whose value is to be deleted
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException 
-   */
-  public void deleteAll(final byte [] row, final byte [] column, final long ts)
-  throws IOException {
-    deleteAll(row,column,ts,null);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp, using an
-   * existing row lock.
-   * @param row Row to update
-   * @param column name of column whose value is to be deleted
-   * @param ts Delete all cells of the same timestamp or older.
-   * @param rl Existing row lock
-   * @throws IOException 
-   */
-  public void deleteAll(final byte [] row, final byte [] column, final long ts,
-      final RowLock rl)
-  throws IOException {
     connection.getRegionServerWithRetries(
-        new ServerCallable<Boolean>(connection, tableName, row) {
+        new ServerCallable<Boolean>(connection, tableName, delete.getRow()) {
           public Boolean call() throws IOException {
-            long lockId = -1L;
-            if(rl != null) {
-              lockId = rl.getLockId();
-            }
-            if (column != null) {
-              this.server.deleteAll(location.getRegionInfo().getRegionName(),
-                row, column, ts, lockId);
-            } else {
-              this.server.deleteAll(location.getRegionInfo().getRegionName(),
-                  row, ts, lockId);
-            }
+            System.out.println("IN HT.get.ServerCallable,");
+            server.delete(location.getRegionInfo().getRegionName(), delete);
             return null;
           }
         }
     );
   }
   
-  /** 
-   * Delete all cells that match the passed row and column.
-   * @param row Row to update
-   * @param colRegex column regex expression
-   * @throws IOException 
-   */
-  public void deleteAllByRegex(final String row, final String colRegex)
-  throws IOException {
-    deleteAll(row, colRegex, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   * @param row Row to update
-   * @param colRegex Column Regex expression
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException 
-   */
-  public void deleteAllByRegex(final String row, final String colRegex, 
-      final long ts) throws IOException {
-    deleteAllByRegex(Bytes.toBytes(row), colRegex, ts);
-  }
-
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp.
-   * @param row Row to update
-   * @param colRegex Column Regex expression
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException 
-   */
-  public void deleteAllByRegex(final byte [] row, final String colRegex, 
-      final long ts) throws IOException {
-    deleteAllByRegex(row, colRegex, ts, null);
-  }
   
-  /** 
-   * Delete all cells that match the passed row and column and whose
-   * timestamp is equal-to or older than the passed timestamp, using an
-   * existing row lock.
-   * @param row Row to update
-   * @param colRegex Column regex expression
-   * @param ts Delete all cells of the same timestamp or older.
-   * @param rl Existing row lock
-   * @throws IOException 
-   */
-  public void deleteAllByRegex(final byte [] row, final String colRegex, 
-      final long ts, final RowLock rl)
-  throws IOException {
-    connection.getRegionServerWithRetries(
-        new ServerCallable<Boolean>(connection, tableName, row) {
-          public Boolean call() throws IOException {
-            long lockId = -1L;
-            if(rl != null) {
-              lockId = rl.getLockId();
-            }
-            this.server.deleteAllByRegex(location.getRegionInfo().getRegionName(),
-              row, colRegex, ts, lockId);
-            return null;
-          }
-        }
-    );
-  }
-
-  /**
-   * Delete all cells for a row with matching column family at all timestamps.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @throws IOException
-   */
-  public void deleteFamily(final String row, final String family) 
-  throws IOException {
-    deleteFamily(row, family, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family at all timestamps.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @throws IOException
-   */
-  public void deleteFamily(final byte[] row, final byte[] family) 
-  throws IOException {
-    deleteFamily(row, family, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family with timestamps
-   * less than or equal to <i>timestamp</i>.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @param timestamp Timestamp to match
-   * @throws IOException
-   */  
-  public void deleteFamily(final String row, final String family,
-      final long timestamp)
-  throws IOException{
-    deleteFamily(Bytes.toBytes(row), Bytes.toBytes(family), timestamp);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family with timestamps
-   * less than or equal to <i>timestamp</i>.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @param timestamp Timestamp to match
-   * @throws IOException
-   */
-  public void deleteFamily(final byte [] row, final byte [] family, 
-    final long timestamp)
-  throws IOException {
-    deleteFamily(row,family,timestamp,null);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family with timestamps
-   * less than or equal to <i>timestamp</i>, using existing row lock.
-   *
-   * @param row The row to operate on
-   * @param family The column family to match
-   * @param timestamp Timestamp to match
-   * @param rl Existing row lock
-   * @throws IOException
-   */
-  public void deleteFamily(final byte [] row, final byte [] family, 
-    final long timestamp, final RowLock rl)
-  throws IOException {
-    connection.getRegionServerWithRetries(
-        new ServerCallable<Boolean>(connection, tableName, row) {
-          public Boolean call() throws IOException {
-            long lockId = -1L;
-            if(rl != null) {
-              lockId = rl.getLockId();
-            }
-            server.deleteFamily(location.getRegionInfo().getRegionName(), row, 
-                family, timestamp, lockId);
-            return null;
-          }
-        }
-    );
-  }
   
   /**
-   * Delete all cells for a row with matching column family regex 
-   * at all timestamps.
-   *
-   * @param row The row to operate on
-   * @param familyRegex Column family regex
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(final String row, final String familyRegex) 
-  throws IOException {
-    deleteFamilyByRegex(row, familyRegex, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family regex 
-   * at all timestamps.
-   *
-   * @param row The row to operate on
-   * @param familyRegex Column family regex
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(final byte[] row, final String familyRegex) 
-  throws IOException {
-    deleteFamilyByRegex(row, familyRegex, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family regex
-   * with timestamps less than or equal to <i>timestamp</i>.
-   *
-   * @param row The row to operate on
-   * @param familyRegex Column family regex
-   * @param timestamp Timestamp to match
-   * @throws IOException
-   */  
-  public void deleteFamilyByRegex(final String row, final String familyRegex,
-      final long timestamp)
-  throws IOException{
-    deleteFamilyByRegex(Bytes.toBytes(row), familyRegex, timestamp);
-  }
-
-  /**
-   * Delete all cells for a row with matching column family regex
-   * with timestamps less than or equal to <i>timestamp</i>.
-   *
-   * @param row The row to operate on
-   * @param familyRegex Column family regex
-   * @param timestamp Timestamp to match
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(final byte [] row, final String familyRegex, 
-    final long timestamp)
-  throws IOException {
-    deleteFamilyByRegex(row,familyRegex,timestamp,null);
-  }
-  
-  /**
-   * Delete all cells for a row with matching column family regex with
-   * timestamps less than or equal to <i>timestamp</i>, using existing
-   * row lock.
-   * 
-   * @param row The row to operate on
-   * @param familyRegex Column Family Regex
-   * @param timestamp Timestamp to match
-   * @param r1 Existing row lock
-   * @throws IOException
-   */
-  public void deleteFamilyByRegex(final byte[] row, final String familyRegex,
-    final long timestamp, final RowLock r1) throws IOException {
-    connection.getRegionServerWithRetries(
-        new ServerCallable<Boolean>(connection, tableName, row) {
-          public Boolean call() throws IOException {
-            long lockId = -1L;
-            if(r1 != null) {
-              lockId = r1.getLockId();
-            }
-            server.deleteFamilyByRegex(location.getRegionInfo().getRegionName(), 
-                row, familyRegex, timestamp, lockId);
-            return null;
-          }
-        }
-    );
-  }
-
-  /**
    * Test for the existence of a row in the table.
    * 
    * @param row The row
@@ -1311,85 +770,43 @@
   }
 
   /**
-   * Commit a BatchUpdate to the table.
-   * If autoFlush is false, the update is buffered
-   * @param batchUpdate
+   * Commit a Put to the table.
+   * <p>
+   * If autoFlush is false, the update is buffered.
+   * @param put
    * @throws IOException
-   */ 
-  public synchronized void commit(final BatchUpdate batchUpdate) 
-  throws IOException {
-    commit(batchUpdate,null);
-  }
-  
-  /**
-   * Commit a BatchUpdate to the table using existing row lock.
-   * If autoFlush is false, the update is buffered
-   * @param batchUpdate
-   * @param rl Existing row lock
-   * @throws IOException
-   */ 
-  public synchronized void commit(final BatchUpdate batchUpdate,
-      final RowLock rl) 
-  throws IOException {
-    checkRowAndColumns(batchUpdate);
-    if(rl != null) {
-      batchUpdate.setRowLock(rl.getLockId());
-    }
-    writeBuffer.add(batchUpdate);
-    currentWriteBufferSize += batchUpdate.heapSize();
-    if (autoFlush || currentWriteBufferSize > writeBufferSize) {
+   */
+  public synchronized void put(final Put put) throws IOException {
+    validatePut(put);
+    writeBuffer.add(put);
+    currentWriteBufferSize += put.heapSize();
+    if(autoFlush || currentWriteBufferSize > writeBufferSize) {
       flushCommits();
     }
   }
   
   /**
-   * Commit a List of BatchUpdate to the table.
-   * If autoFlush is false, the updates are buffered
-   * @param batchUpdates
+   * Commit a List of Puts to the table.
+   * <p>
+   * If autoFlush is false, the update is buffered.
+   * @param puts
    * @throws IOException
-   */ 
-  public synchronized void commit(final List<BatchUpdate> batchUpdates)
-      throws IOException {
-    for (BatchUpdate bu : batchUpdates) {
-      checkRowAndColumns(bu);
-      writeBuffer.add(bu);
-      currentWriteBufferSize += bu.heapSize();
+   */
+  public synchronized void put(final List<Put> puts) throws IOException {
+    for(Put put : puts) {
+      validatePut(put);
+      writeBuffer.add(put);
+      currentWriteBufferSize += put.heapSize();
     }
-    if (autoFlush || currentWriteBufferSize > writeBufferSize) {
+    if(autoFlush || currentWriteBufferSize > writeBufferSize) {
       flushCommits();
     }
   }
   
+
   /**
-   * Atomically checks if a row's values match
-   * the expectedValues. If it does, it uses the
-   * batchUpdate to update the row.
-   * @param batchUpdate batchupdate to apply if check is successful
-   * @param expectedValues values to check
-   * @param rl rowlock
-   * @throws IOException
-   */
-  public synchronized boolean checkAndSave(final BatchUpdate batchUpdate,
-    final HbaseMapWritable<byte[],byte[]> expectedValues, final RowLock rl)
-  throws IOException {
-    checkRowAndColumns(batchUpdate);
-    if(rl != null) {
-      batchUpdate.setRowLock(rl.getLockId());
-    }
-    return connection.getRegionServerWithRetries(
-      new ServerCallable<Boolean>(connection, tableName, batchUpdate.getRow()) {
-        public Boolean call() throws IOException {
-          return server.checkAndSave(location.getRegionInfo().getRegionName(),
-            batchUpdate, expectedValues)?
-              Boolean.TRUE: Boolean.FALSE;
-        }
-      }
-    ).booleanValue();
-  }
-  
-  /**
    * Commit to the table the buffer of BatchUpdate.
-   * Called automaticaly in the commit methods when autoFlush is true.
+   * Called automatically in the commit methods when autoFlush is true.
    * @throws IOException
    */
   public void flushCommits() throws IOException {
@@ -1409,26 +826,47 @@
   public void close() throws IOException{
     flushCommits();
   }
-
+  
   /**
-   * Utility method that checks rows existence, length and columns well
-   * formedness.
-   * 
-   * @param bu
+   * Utilith method that verifies Put is well formed.
+   * @param put
    * @throws IllegalArgumentException
    * @throws IOException
    */
-  private void checkRowAndColumns(BatchUpdate bu)
-      throws IllegalArgumentException, IOException {
-    if (bu.getRow() == null || bu.getRow().length > HConstants.MAX_ROW_LENGTH) {
+  private void validatePut(final Put put) throws IllegalArgumentException{
+    if(put.getRow() == null || put.getRow().length > HConstants.MAX_ROW_LENGTH) {
       throw new IllegalArgumentException("Row key is invalid");
     }
-    for (BatchOperation bo : bu) {
-      HStoreKey.getFamily(bo.getColumn());
+    if(put.isEmpty()) {
+      throw new IllegalArgumentException("No columns to insert");
     }
   }
-
+  
+  
   /**
+   * Atomically checks if a row's values match
+   * the expectedValues. If it does, it uses the put to update the row
+   * @param put
+   * @param expectedValues
+   * @return
+   * @throws IOException
+   */
+  public synchronized boolean checkAndSave(final Put put,
+    final HbaseMapWritable<byte[],byte[]> expectedValues)
+  throws IOException {
+    validatePut(put);
+    return connection.getRegionServerWithRetries(
+      new ServerCallable<Boolean>(connection, tableName, put.getRow()) {
+        public Boolean call() throws IOException {
+          return server.checkAndSave(location.getRegionInfo().getRegionName(),
+            put, expectedValues)?
+              Boolean.TRUE: Boolean.FALSE;
+        }
+      }
+    ).booleanValue();
+  }
+  
+  /**
    * Obtain a row lock
    * @param row The row to lock
    * @return rowLock RowLock containing row and lock id
@@ -1502,7 +940,7 @@
    * Get the write buffer 
    * @return the current write buffer
    */
-  public ArrayList<BatchUpdate> getWriteBuffer() {
+  public ArrayList<Put> getWriteBuffer() {
     return writeBuffer;
   }
   
@@ -1525,37 +963,29 @@
    */
   protected class ClientScanner implements Scanner {
     private final Log CLIENT_LOG = LogFactory.getLog(this.getClass());
-    private byte[][] columns;
-    private byte [] startRow;
-    protected long scanTime;
+    private Scan scan;
     private boolean closed = false;
     private HRegionInfo currentRegion = null;
     private ScannerCallable callable = null;
-    protected RowFilterInterface filter;
-    private final LinkedList<RowResult> cache = new LinkedList<RowResult>();
-    @SuppressWarnings("hiding")
+    private final LinkedList<Result> cache = new LinkedList<Result>();
     private final int scannerCaching = HTable.this.scannerCaching;
     private long lastNext;
-
-    protected ClientScanner(final byte[][] columns, final byte [] startRow,
-        final long timestamp, final RowFilterInterface filter) {
+    
+    protected ClientScanner(final Scan scan) {
       if (CLIENT_LOG.isDebugEnabled()) {
         CLIENT_LOG.debug("Creating scanner over " 
             + Bytes.toString(getTableName()) 
-            + " starting at key '" + Bytes.toString(startRow) + "'");
+            + " starting at key '" + Bytes.toString(scan.getStartRow()) + "'");
       }
-      // save off the simple parameters
-      this.columns = columns;
-      this.startRow = startRow;
-      this.scanTime = timestamp;
-      
-      // save the filter, and make sure that the filter applies to the data
-      // we're expecting to pull back
-      this.filter = filter;
-      if (filter != null) {
-        filter.validate(columns);
-      }
+      this.scan = scan;
       this.lastNext = System.currentTimeMillis();
+      
+      // Removed filter validation.  We have a new format now, only one of all
+      // the current filters has a validate() method.  We can add it back,
+      // need to decide on what we're going to do re: filter redesign.
+      // Need, at the least, to break up family from qualifier as separate
+      // checks, I think it's important server-side filters are optimal in that
+      // respect.
     }
 
     //TODO: change visibility to protected
@@ -1563,19 +993,15 @@
     public void initialize() throws IOException {
       nextScanner(this.scannerCaching);
     }
-    
-    protected byte[][] getColumns() {
-      return columns;
+
+    protected Scan getScan() {
+      return scan;
     }
     
     protected long getTimestamp() {
-      return scanTime;
+      return lastNext;
     }
     
-    protected RowFilterInterface getFilter() {
-      return filter;
-    }
-        
     /*
      * Gets a scanner for the next region.
      * Returns false if there are no more scanners.
@@ -1603,9 +1029,10 @@
           return false;
         }
       } 
-      
+
       HRegionInfo oldRegion = this.currentRegion;
-      byte [] localStartKey = oldRegion == null? startRow: oldRegion.getEndKey();
+      byte [] localStartKey = 
+        oldRegion == null ? scan.getStartRow() : oldRegion.getEndKey();
 
       if (CLIENT_LOG.isDebugEnabled()) {
         CLIENT_LOG.debug("Advancing internal scanner to startKey at '" +
@@ -1628,8 +1055,7 @@
     protected ScannerCallable getScannerCallable(byte [] localStartKey,
         int nbRows) {
       ScannerCallable s = new ScannerCallable(getConnection(), 
-          getTableName(), columns, 
-          localStartKey, scanTime, filter);
+          getTableName(), localStartKey, scan);
       s.setCaching(nbRows);
       return s;
     }
@@ -1640,22 +1066,22 @@
      * filter.
      */
     private boolean filterSaysStop(final byte [] endKey) {
-      if (this.filter == null) {
+      if(!scan.hasFilter()) {
         return false;
       }
       // Let the filter see current row.
-      this.filter.filterRowKey(endKey, 0, endKey.length);
-      return this.filter.filterAllRemaining();
+      scan.getFilter().filterRowKey(endKey, 0, endKey.length);
+      return scan.getFilter().filterAllRemaining();
     }
 
-    public RowResult next() throws IOException {
+    public Result next() throws IOException {
       // If the scanner is closed but there is some rows left in the cache,
       // it will first empty it before returning null
       if (cache.size() == 0 && this.closed) {
         return null;
       }
       if (cache.size() == 0) {
-        RowResult[] values = null;
+        Result [] values = null;
         int countdown = this.scannerCaching;
         // We need to reset it if it's a new callable that was created 
         // with a countdown in nextScanner
@@ -1674,7 +1100,7 @@
           }
           lastNext = System.currentTimeMillis();
           if (values != null && values.length > 0) {
-            for (RowResult rs : values) {
+            for (Result rs : values) {
               cache.add(rs);
               countdown--;
             }
@@ -1693,18 +1119,18 @@
      * @return Between zero and <param>nbRows</param> RowResults
      * @throws IOException
      */
-    public RowResult[] next(int nbRows) throws IOException {
+    public Result [] next(int nbRows) throws IOException {
       // Collect values to be returned here
-      ArrayList<RowResult> resultSets = new ArrayList<RowResult>(nbRows);
+      ArrayList<Result> resultSets = new ArrayList<Result>(nbRows);
       for(int i = 0; i < nbRows; i++) {
-        RowResult next = next();
+        Result next = next();
         if (next != null) {
           resultSets.add(next);
         } else {
           break;
         }
       }
-      return resultSets.toArray(new RowResult[resultSets.size()]);
+      return resultSets.toArray(new Result[resultSets.size()]);
     }
     
     public void close() {
@@ -1723,10 +1149,10 @@
       closed = true;
     }
 
-    public Iterator<RowResult> iterator() {
-      return new Iterator<RowResult>() {
+    public Iterator<Result> iterator() {
+      return new Iterator<Result>() {
         // The next RowResult, possibly pre-read
-        RowResult next = null;
+        Result next = null;
         
         // return true if there is another item pending, false if there isn't.
         // this method is where the actual advancing takes place, but you need
@@ -1746,7 +1172,7 @@
 
         // get the pending next item and advance the iterator. returns null if
         // there is no next item.
-        public RowResult next() {
+        public Result next() {
           // since hasNext() does the real advancing, we call this to determine
           // if there is a next before proceeding.
           if (!hasNext()) {
@@ -1756,7 +1182,7 @@
           // if we get to here, then hasNext() has given us an item to return.
           // we want to return the item and then null out the next pointer, so
           // we use a temporary variable.
-          RowResult temp = next;
+          Result temp = next;
           next = null;
           return temp;
         }
@@ -1767,4 +1193,26 @@
       };
     }
   }
+  
+  //
+  // HBASE-880
+  //
+  /**
+   * Method for getting data from a row
+   * @param get the Get to fetch
+   * @return the result
+   * @throws IOException
+   */
+  public Result get(final Get get) throws IOException {
+    return connection.getRegionServerWithRetries(
+        new ServerCallable<Result>(connection, tableName, get.getRow()) {
+          public Result call() throws IOException {
+            return server.get(location.getRegionInfo().getRegionName(), get);
+          }
+        }
+    );
+  }
+  
+  
+  
 }
Index: src/java/org/apache/hadoop/hbase/client/MetaScanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/MetaScanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/MetaScanner.java	(working copy)
@@ -5,7 +5,8 @@
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -49,14 +50,16 @@
     // Scan over each meta region
     ScannerCallable callable = null;
     do {
+      Scan scan = new Scan(startRow);
+      scan.addFamily(CATALOG_FAMILY);
       callable = new ScannerCallable(connection, META_TABLE_NAME,
-        COLUMN_FAMILY_ARRAY, startRow, LATEST_TIMESTAMP, null);
+        startRow, scan);
       // Open scanner
       connection.getRegionServerWithRetries(callable);
       try {
-        RowResult r = null;
+        Result r = null;
         do {
-          RowResult [] rrs = connection.getRegionServerWithRetries(callable);
+          Result [] rrs = connection.getRegionServerWithRetries(callable);
           if (rrs == null || rrs.length == 0 || rrs[0].size() == 0) {
             break;
           }
@@ -85,6 +88,6 @@
      * @return A boolean to know if it should continue to loop in the region
      * @throws IOException
      */
-    public boolean processRow(RowResult rowResult) throws IOException;
+    public boolean processRow(Result rowResult) throws IOException;
   }
 }
Index: src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HConnectionManager.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/HConnectionManager.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2007 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -38,15 +38,15 @@
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HStoreKey;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.TableNotFoundException;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Put;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
@@ -338,9 +338,9 @@
 
       MetaScannerVisitor visitor = new MetaScannerVisitor() {
 
-        public boolean processRow(RowResult rowResult) throws IOException {
+        public boolean processRow(Result result) throws IOException {
           HRegionInfo info = Writables.getHRegionInfo(
-              rowResult.get(COL_REGIONINFO));
+              result.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
 
           // Only examine the rows where the startKey is zero length
           if (info.getStartKey().length == 0) {
@@ -387,12 +387,12 @@
         HRegionInfo.createRegionName(tableName, null, HConstants.ZEROES);
       byte[] endKey = null;
       HRegionInfo currentRegion = null;
+      Scan scan = new Scan(startKey);
+      scan.addColumn(CATALOG_FAMILY, REGIONINFO_QUALIFIER);
       ScannerCallable s = new ScannerCallable(this, 
           (Bytes.equals(tableName, HConstants.META_TABLE_NAME) ?
               HConstants.ROOT_TABLE_NAME : HConstants.META_TABLE_NAME),
-          HConstants.COL_REGIONINFO_ARRAY, startKey,
-          HConstants.LATEST_TIMESTAMP, null
-      );
+           startKey, scan);
       try {
         // Open scanner
         getRegionServerWithRetries(s);
@@ -402,27 +402,25 @@
             startKey = oldRegion.getEndKey();
           }
           currentRegion = s.getHRegionInfo();
-          RowResult r = null;
-          RowResult[] rrs = null;
+          Result r = null;
+          Result [] rrs = null;
           while ((rrs = getRegionServerWithRetries(s)) != null) {
             r = rrs[0];
-            Cell c = r.get(HConstants.COL_REGIONINFO);
-            if (c != null) {
-              byte[] value = c.getValue();
-              if (value != null) {
-                HRegionInfo info = Writables.getHRegionInfoOrNull(value);
-                if (info != null) {
-                  if (Bytes.equals(info.getTableDesc().getName(), tableName)) {
-                    rowsScanned += 1;
-                    rowsOffline += info.isOffline() ? 1 : 0;
-                  }
+            byte [] value = r.getValue(HConstants.CATALOG_FAMILY, 
+                HConstants.REGIONINFO_QUALIFIER);
+            if (value != null) {
+              HRegionInfo info = Writables.getHRegionInfoOrNull(value);
+              if (info != null) {
+                if (Bytes.equals(info.getTableDesc().getName(), tableName)) {
+                  rowsScanned += 1;
+                  rowsOffline += info.isOffline() ? 1 : 0;
                 }
               }
             }
           }
           endKey = currentRegion.getEndKey();
-        } while (!(endKey == null || HStoreKey.equalsTwoRowKeys(endKey,
-            HConstants.EMPTY_BYTE_ARRAY)));
+        } while (!(endKey == null || 
+            Bytes.equals(endKey, HConstants.EMPTY_BYTE_ARRAY)));
       }
       finally {
         s.setClose();
@@ -440,9 +438,9 @@
         protected HTableDescriptorFinder(byte[] tableName) {
           this.tableName = tableName;
         }
-        public boolean processRow(RowResult rowResult) throws IOException {
+        public boolean processRow(Result rowResult) throws IOException {
           HRegionInfo info = Writables.getHRegionInfo(
-            rowResult.get(HConstants.COL_REGIONINFO));
+              rowResult.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
           HTableDescriptor desc = info.getTableDesc();
           if (Bytes.compareTo(desc.getName(), tableName) == 0) {
             result = desc;
@@ -556,21 +554,22 @@
             getHRegionConnection(metaLocation.getServerAddress());
 
           // Query the root or meta region for the location of the meta region
-          RowResult regionInfoRow = server.getClosestRowBefore(
+          Result regionInfoRow = server.getClosestRowBefore(
             metaLocation.getRegionInfo().getRegionName(), metaKey,
             HConstants.COLUMN_FAMILY);
           if (regionInfoRow == null) {
             throw new TableNotFoundException(Bytes.toString(tableName));
           }
 
-          Cell value = regionInfoRow.get(COL_REGIONINFO);
-          if (value == null || value.getValue().length == 0) {
+          byte [] value = regionInfoRow.getValue(CATALOG_FAMILY, 
+              REGIONINFO_QUALIFIER);
+          if (value == null || value.length == 0) {
             throw new IOException("HRegionInfo was null or empty in " + 
               Bytes.toString(parentTable));
           }
           // convert the row result into the HRegionLocation we need!
           HRegionInfo regionInfo = (HRegionInfo) Writables.getWritable(
-              value.getValue(), new HRegionInfo());
+              value, new HRegionInfo());
           // possible we got a region of a different table...
           if (!Bytes.equals(regionInfo.getTableDesc().getName(), tableName)) {
             throw new TableNotFoundException(
@@ -581,8 +580,8 @@
               regionInfo.getRegionNameAsString());
           }
           
-          String serverAddress = 
-            Writables.cellToString(regionInfoRow.get(COL_SERVER));
+          value = regionInfoRow.getValue(CATALOG_FAMILY, SERVER_QUALIFIER);
+          String serverAddress = Bytes.toString(value);
           if (serverAddress.equals("")) { 
             throw new NoServerForRegionException("No server address listed " +
               "in " + Bytes.toString(parentTable) + " for region " +
@@ -682,8 +681,8 @@
           // this one. the exception case is when the endkey is EMPTY_START_ROW,
           // signifying that the region we're checking is actually the last
           // region in the table.
-          if (HStoreKey.equalsTwoRowKeys(endKey, HConstants.EMPTY_END_ROW) ||
-              HStoreKey.getComparator(tableName).compareRows(endKey, row) > 0) {
+          if (Bytes.equals(endKey, HConstants.EMPTY_END_ROW) ||
+              KeyValue.getRowComparator(tableName).compare(endKey, row) > 0) {
             return possibleRegion;
           }
         }
@@ -720,7 +719,7 @@
 
           // by nature of the map, we know that the start key has to be < 
           // otherwise it wouldn't be in the headMap. 
-          if (HStoreKey.getComparator(tableName).compareRows(endKey, row) <= 0) {
+          if (KeyValue.getRowComparator(tableName).compare(endKey, row) <= 0) {
             // delete any matching entry
             HRegionLocation rl =
               tableLocations.remove(matchingRegions.lastKey());
@@ -975,15 +974,15 @@
       return location;
     }
 
-    public void processBatchOfRows(ArrayList<BatchUpdate> list, byte[] tableName)
+    public void processBatchOfRows(ArrayList<Put> list, byte[] tableName)
         throws IOException {
       if (list.isEmpty()) {
         return;
       }
       boolean retryOnlyOne = false;
       int tries = 0;
-      Collections.sort(list);
-      List<BatchUpdate> tempUpdates = new ArrayList<BatchUpdate>();
+      Collections.sort(list); 
+      List<Put> currentPuts = new ArrayList<Put>();
       HRegionLocation location =
         getRegionLocationForRowWithRetries(tableName, list.get(0).getRow(),
             false);
@@ -991,8 +990,8 @@
       byte [] region = currentRegion;
       boolean isLastRow = false;
       for (int i = 0; i < list.size() && tries < numRetries; i++) {
-        BatchUpdate batchUpdate = list.get(i);
-        tempUpdates.add(batchUpdate);
+        Put put = list.get(i);
+        currentPuts.add(put);
         isLastRow = (i + 1) == list.size();
         if (!isLastRow) {
           location = getRegionLocationForRowWithRetries(tableName,
@@ -1000,19 +999,19 @@
           region = location.getRegionInfo().getRegionName();
         }
         if (!Bytes.equals(currentRegion, region) || isLastRow || retryOnlyOne) {
-          final BatchUpdate[] updates = tempUpdates.toArray(new BatchUpdate[0]);
+          final Put [] puts = currentPuts.toArray(new Put[0]);
           int index = getRegionServerWithRetries(new ServerCallable<Integer>(
-              this, tableName, batchUpdate.getRow()) {
+              this, tableName, put.getRow()) {
             public Integer call() throws IOException {
-              int i = server.batchUpdates(location.getRegionInfo()
-                  .getRegionName(), updates);
+              int i = server.put(location.getRegionInfo()
+                  .getRegionName(), puts);
               return i;
             }
           });
           if (index != -1) {
             if (tries == numRetries - 1) {
               throw new RetriesExhaustedException("Some server",
-                  currentRegion, batchUpdate.getRow(), 
+                  currentRegion, put.getRow(), 
                   tries, new ArrayList<Throwable>());
             }
             long sleepTime = getPauseTime(tries);
@@ -1028,7 +1027,7 @@
             } catch (InterruptedException e) {
               // continue
             }
-            i = i - updates.length + index;
+            i = i - puts.length + index;
             retryOnlyOne = true;
             location = getRegionLocationForRowWithRetries(tableName, 
               list.get(i + 1).getRow(), true);
@@ -1038,7 +1037,7 @@
             retryOnlyOne = false;
           }
           currentRegion = region;
-          tempUpdates.clear();
+          currentPuts.clear();
         }
       }
     }
Index: src/java/org/apache/hadoop/hbase/client/HConnection.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HConnection.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/HConnection.java	(working copy)
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.MasterNotRunningException;
-import org.apache.hadoop.hbase.io.BatchUpdate;
+import org.apache.hadoop.hbase.io.Put;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper;
@@ -179,6 +179,6 @@
    * @param tableName The name of the table
    * @throws IOException
    */
-  public void processBatchOfRows(ArrayList<BatchUpdate> list, byte[] tableName)
+  public void processBatchOfRows(ArrayList<Put> list, byte[] tableName)
       throws IOException;
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTableAdmin.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTableAdmin.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTableAdmin.java	(working copy)
@@ -1,97 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.IOException;
-import java.util.Set;
-import java.util.TreeSet;
-
-import org.apache.hadoop.hbase.ColumnNameParseException;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HStoreKey;
-import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.MasterNotRunningException;
-import org.apache.hadoop.hbase.TableExistsException;
-import org.apache.hadoop.hbase.client.HBaseAdmin;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/**
- * Extension of HBaseAdmin that creates indexed tables.
- * 
- */
-public class IndexedTableAdmin extends HBaseAdmin {
-
-  /**
-   * Constructor
-   * 
-   * @param conf Configuration object
-   * @throws MasterNotRunningException
-   */
-  public IndexedTableAdmin(HBaseConfiguration conf)
-      throws MasterNotRunningException {
-    super(conf);
-  }
-
-  /**
-   * Creates a new table
-   * 
-   * @param desc table descriptor for table
-   * 
-   * @throws IllegalArgumentException if the table name is reserved
-   * @throws MasterNotRunningException if master is not running
-   * @throws TableExistsException if table already exists (If concurrent
-   * threads, the table may have been created between test-for-existence and
-   * attempt-at-creation).
-   * @throws IOException
-   */
-  @Override
-  public void createTable(HTableDescriptor desc) throws IOException {
-    super.createTable(desc);
-    this.createIndexTables(desc);
-  }
-
-  private void createIndexTables(HTableDescriptor tableDesc) throws IOException {
-    byte[] baseTableName = tableDesc.getName();
-    for (IndexSpecification indexSpec : tableDesc.getIndexes()) {
-      HTableDescriptor indexTableDesc = createIndexTableDesc(baseTableName,
-          indexSpec);
-      super.createTable(indexTableDesc);
-    }
-  }
-
-  private HTableDescriptor createIndexTableDesc(byte[] baseTableName,
-      IndexSpecification indexSpec) throws ColumnNameParseException {
-    HTableDescriptor indexTableDesc = new HTableDescriptor(indexSpec
-        .getIndexedTableName(baseTableName));
-    Set<byte[]> families = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
-    families.add(IndexedTable.INDEX_COL_FAMILY);
-    for (byte[] column : indexSpec.getAllColumns()) {
-      families.add(Bytes.add(HStoreKey.getFamily(column),
-          new byte[] { HStoreKey.COLUMN_FAMILY_DELIMITER }));
-    }
-
-    for (byte[] colFamily : families) {
-      indexTableDesc.addFamily(new HColumnDescriptor(colFamily));
-    }
-
-    return indexTableDesc;
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/SimpleIndexKeyGenerator.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/SimpleIndexKeyGenerator.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/SimpleIndexKeyGenerator.java	(working copy)
@@ -1,59 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.hadoop.hbase.util.Bytes;
-
-/** Creates indexed keys for a single column....
- * 
- */
-public class SimpleIndexKeyGenerator implements IndexKeyGenerator {
-
-  private byte [] column;
-  
-  public SimpleIndexKeyGenerator(byte [] column) {
-    this.column = column;
-  }
-  
-  public SimpleIndexKeyGenerator() {
-    // For Writable
-  }
-  
-  /** {@inheritDoc} */
-  public byte[] createIndexKey(byte[] rowKey, Map<byte[], byte[]> columns) {
-    return Bytes.add(columns.get(column), rowKey);
-  }
-
-  /** {@inheritDoc} */
-  public void readFields(DataInput in) throws IOException {
-    column = Bytes.readByteArray(in);
-  }
-
-  /** {@inheritDoc} */
-  public void write(DataOutput out) throws IOException {
-    Bytes.writeByteArray(out, column);
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/IndexNotFoundException.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/IndexNotFoundException.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/IndexNotFoundException.java	(working copy)
@@ -1,47 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.IOException;
-
-/**
- * Thrown when asking for an index that does not exist.
- */
-public class IndexNotFoundException extends IOException {
-
-  private static final long serialVersionUID = 6533971528557000965L;
-
-  public IndexNotFoundException() {
-    super();
-  }
-
-  public IndexNotFoundException(String arg0) {
-    super(arg0);
-  }
-
-  public IndexNotFoundException(Throwable arg0) {
-    super(arg0.getMessage());
-  }
-
-  public IndexNotFoundException(String arg0, Throwable arg1) {
-    super(arg0+arg1.getMessage());
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/ReverseByteArrayComparator.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/ReverseByteArrayComparator.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/ReverseByteArrayComparator.java	(working copy)
@@ -1,46 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.WritableComparator;
-import org.apache.hadoop.hbase.util.Bytes;
-
-public class ReverseByteArrayComparator implements WritableComparator<byte[]> {
-
-  /** {@inheritDoc} */
-  public int compare(byte[] o1, byte[] o2) {
-    return Bytes.compareTo(o2, o1);
-  }
-
-  
-  /** {@inheritDoc} */
-  public void readFields(DataInput arg0) throws IOException {
-    // Nothing
-  }
-
-  /** {@inheritDoc} */
-  public void write(DataOutput arg0) throws IOException {
-    // Nothing
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/IndexKeyGenerator.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/IndexKeyGenerator.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/IndexKeyGenerator.java	(working copy)
@@ -1,29 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.util.Map;
-
-import org.apache.hadoop.io.Writable;
-
-public interface IndexKeyGenerator extends Writable {
-
-  byte [] createIndexKey(byte [] rowKey, Map<byte [], byte []> columns);
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/IndexSpecification.java	(working copy)
@@ -1,190 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.io.ObjectWritable;
-import org.apache.hadoop.io.Writable;
-
-/** Holds the specification for a single secondary index. */
-public class IndexSpecification implements Writable {
-
-  // Columns that are indexed (part of the indexRowKey)
-  private byte[][] indexedColumns;
-
-  // Constructs the
-  private IndexKeyGenerator keyGenerator;
-
-  // Additional columns mapped into the indexed row. These will be available for
-  // filters when scanning the index.
-  private byte[][] additionalColumns;
-
-  private byte[][] allColumns;
-
-  // Id of this index, unique within a table.
-  private String indexId;
-
-  /** Construct an "simple" index spec for a single column. 
-   * @param indexId 
-   * @param indexedColumn
-   */
-  public IndexSpecification(String indexId, byte[] indexedColumn) {
-    this(indexId, new byte[][] { indexedColumn }, null,
-        new SimpleIndexKeyGenerator(indexedColumn));
-  }
-
-  /**
-   * Construct an index spec by specifying everything.
-   * 
-   * @param indexId
-   * @param indexedColumns
-   * @param additionalColumns
-   * @param keyGenerator
-   */
-  public IndexSpecification(String indexId, byte[][] indexedColumns,
-      byte[][] additionalColumns, IndexKeyGenerator keyGenerator) {
-    this.indexId = indexId;
-    this.indexedColumns = indexedColumns;
-    this.additionalColumns = additionalColumns;
-    this.keyGenerator = keyGenerator;
-    this.makeAllColumns();
-  }
-
-  public IndexSpecification() {
-    // For writable
-  }
-
-  private void makeAllColumns() {
-    this.allColumns = new byte[indexedColumns.length
-        + (additionalColumns == null ? 0 : additionalColumns.length)][];
-    System.arraycopy(indexedColumns, 0, allColumns, 0, indexedColumns.length);
-    if (additionalColumns != null) {
-      System.arraycopy(additionalColumns, 0, allColumns, indexedColumns.length,
-          additionalColumns.length);
-    }
-  }
-  
-  /**
-   * Get the indexedColumns.
-   * 
-   * @return Return the indexedColumns.
-   */
-  public byte[][] getIndexedColumns() {
-    return indexedColumns;
-  }
-
-  /**
-   * Get the keyGenerator.
-   * 
-   * @return Return the keyGenerator.
-   */
-  public IndexKeyGenerator getKeyGenerator() {
-    return keyGenerator;
-  }
-
-  /**
-   * Get the additionalColumns.
-   * 
-   * @return Return the additionalColumns.
-   */
-  public byte[][] getAdditionalColumns() {
-    return additionalColumns;
-  }
-
-  /**
-   * Get the indexId.
-   * 
-   * @return Return the indexId.
-   */
-  public String getIndexId() {
-    return indexId;
-  }
-
-  public byte[][] getAllColumns() {
-    return allColumns;
-  }
-
-  public boolean containsColumn(byte[] column) {
-    for (byte[] col : allColumns) {
-      if (Bytes.equals(column, col)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  public byte[] getIndexedTableName(byte[] baseTableName) {
-    return Bytes.add(baseTableName, Bytes.toBytes("-" + indexId));
-  }
-
-  /** {@inheritDoc} */
-  public void readFields(DataInput in) throws IOException {
-    indexId = in.readUTF();
-    int numIndexedCols = in.readInt();
-    indexedColumns = new byte[numIndexedCols][];
-    for (int i = 0; i < numIndexedCols; i++) {
-      indexedColumns[i] = Bytes.readByteArray(in);
-    }
-    int numAdditionalCols = in.readInt();
-    additionalColumns = new byte[numAdditionalCols][];
-    for (int i = 0; i < numAdditionalCols; i++) {
-      additionalColumns[i] = Bytes.readByteArray(in);
-    }
-    makeAllColumns();
-    HBaseConfiguration conf = new HBaseConfiguration();
-    keyGenerator = (IndexKeyGenerator) ObjectWritable.readObject(in, conf);
-  }
-
-  /** {@inheritDoc} */
-  public void write(DataOutput out) throws IOException {
-    out.writeUTF(indexId);
-    out.writeInt(indexedColumns.length);
-    for (byte[] col : indexedColumns) {
-      Bytes.writeByteArray(out, col);
-    }
-    if (additionalColumns != null) {
-      out.writeInt(additionalColumns.length);
-      for (byte[] col : additionalColumns) {
-        Bytes.writeByteArray(out, col);
-      }
-    } else {
-      out.writeInt(0);
-    }
-    HBaseConfiguration conf = new HBaseConfiguration();
-    ObjectWritable
-        .writeObject(out, keyGenerator, IndexKeyGenerator.class, conf);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    sb.append("ID => ");
-    sb.append(indexId);
-    return sb.toString();
-  }
-  
-  
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/IndexedTable.java	(working copy)
@@ -1,224 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.tableindexed;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HStoreKey;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.Scanner;
-import org.apache.hadoop.hbase.client.transactional.TransactionalTable;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.HbaseMapWritable;
-import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.util.Bytes;
-
-/** HTable extended with indexed support. */
-public class IndexedTable extends TransactionalTable {
-
-  // FIXME, these belong elsewhere
-  static final byte[] INDEX_COL_FAMILY_NAME = Bytes.toBytes("__INDEX__");
-  static final byte[] INDEX_COL_FAMILY = Bytes.add(
-      INDEX_COL_FAMILY_NAME, new byte[] { HStoreKey.COLUMN_FAMILY_DELIMITER });
-  public static final byte[] INDEX_BASE_ROW_COLUMN = Bytes.add(
-      INDEX_COL_FAMILY, Bytes.toBytes("ROW"));
-
-  static final Log LOG = LogFactory.getLog(IndexedTable.class);
-
-  private Map<String, HTable> indexIdToTable = new HashMap<String, HTable>();
-
-  public IndexedTable(final HBaseConfiguration conf, final byte[] tableName)
-      throws IOException {
-    super(conf, tableName);
-
-    for (IndexSpecification spec : super.getTableDescriptor().getIndexes()) {
-      indexIdToTable.put(spec.getIndexId(), new HTable(conf, spec
-          .getIndexedTableName(tableName)));
-    }
-  }
-
-  /**
-   * Open up an indexed scanner. Results will come back in the indexed order,
-   * but will contain RowResults from the original table.
-   * 
-   * @param indexId the id of the index to use
-   * @param indexStartRow (created from the IndexKeyGenerator)
-   * @param indexColumns in the index table
-   * @param indexFilter filter to run on the index'ed table. This can only use
-   * columns that have been added to the index.
-   * @param baseColumns from the original table
-   * @return scanner
-   * @throws IOException
-   * @throws IndexNotFoundException
-   */
-  public Scanner getIndexedScanner(String indexId, final byte[] indexStartRow,
-      byte[][] indexColumns, final RowFilterInterface indexFilter,
-      final byte[][] baseColumns) throws IOException, IndexNotFoundException {
-    IndexSpecification indexSpec = super.getTableDescriptor().getIndex(indexId);
-    if (indexSpec == null) {
-      throw new IndexNotFoundException("Index " + indexId
-          + " not defined in table "
-          + super.getTableDescriptor().getNameAsString());
-    }
-    verifyIndexColumns(indexColumns, indexSpec);
-    // TODO, verify/remove index columns from baseColumns
-
-    HTable indexTable = indexIdToTable.get(indexId);
-
-    byte[][] allIndexColumns;
-    if (indexColumns != null) {
-      allIndexColumns = new byte[indexColumns.length + 1][];
-      System
-          .arraycopy(indexColumns, 0, allIndexColumns, 0, indexColumns.length);
-      allIndexColumns[indexColumns.length] = INDEX_BASE_ROW_COLUMN;
-    } else {
-      byte[][] allColumns = indexSpec.getAllColumns();
-      allIndexColumns = new byte[allColumns.length + 1][];
-      System.arraycopy(allColumns, 0, allIndexColumns, 0, allColumns.length);
-      allIndexColumns[allColumns.length] = INDEX_BASE_ROW_COLUMN;
-    }
-
-    Scanner indexScanner = indexTable.getScanner(allIndexColumns,
-        indexStartRow, indexFilter);
-
-    return new ScannerWrapper(indexScanner, baseColumns);
-  }
-
-  private void verifyIndexColumns(byte[][] requestedColumns,
-      IndexSpecification indexSpec) {
-    if (requestedColumns == null) {
-      return;
-    }
-    for (byte[] requestedColumn : requestedColumns) {
-      boolean found = false;
-      for (byte[] indexColumn : indexSpec.getAllColumns()) {
-        if (Bytes.equals(requestedColumn, indexColumn)) {
-          found = true;
-          break;
-        }
-      }
-      if (!found) {
-        throw new RuntimeException("Column [" + Bytes.toString(requestedColumn)
-            + "] not in index " + indexSpec.getIndexId());
-      }
-    }
-  }
-
-  private class ScannerWrapper implements Scanner {
-
-    private Scanner indexScanner;
-    private byte[][] columns;
-
-    public ScannerWrapper(Scanner indexScanner, byte[][] columns) {
-      this.indexScanner = indexScanner;
-      this.columns = columns;
-    }
-
-    /** {@inheritDoc} */
-    public RowResult next() throws IOException {
-        RowResult[] result = next(1);
-        if (result == null || result.length < 1)
-          return null;
-        return result[0];
-    }
-
-    /** {@inheritDoc} */
-    public RowResult[] next(int nbRows) throws IOException {
-      RowResult[] indexResult = indexScanner.next(nbRows);
-      if (indexResult == null) {
-        return null;
-      }
-      RowResult[] result = new RowResult[indexResult.length];
-      for (int i = 0; i < indexResult.length; i++) {
-        RowResult row = indexResult[i];
-        byte[] baseRow = row.get(INDEX_BASE_ROW_COLUMN).getValue();
-        LOG.debug("next index row [" + Bytes.toString(row.getRow())
-            + "] -> base row [" + Bytes.toString(baseRow) + "]");
-        HbaseMapWritable<byte[], Cell> colValues =
-          new HbaseMapWritable<byte[], Cell>();
-        if (columns != null && columns.length > 0) {
-          LOG.debug("Going to base table for remaining columns");
-          RowResult baseResult = IndexedTable.this.getRow(baseRow, columns);
-          
-          if (baseResult != null) {
-            colValues.putAll(baseResult);
-          }
-        }
-        for (Entry<byte[], Cell> entry : row.entrySet()) {
-          byte[] col = entry.getKey();
-          if (HStoreKey.matchingFamily(INDEX_COL_FAMILY_NAME, col)) {
-            continue;
-          }
-          colValues.put(col, entry.getValue());
-        }
-        result[i] = new RowResult(baseRow, colValues);
-      }
-      return result;
-    }
-
-    /** {@inheritDoc} */
-    public void close() {
-      indexScanner.close();
-    }
-
-    /** {@inheritDoc} */
-    public Iterator<RowResult> iterator() {
-      // FIXME, copied from HTable.ClientScanner. Extract this to common base
-      // class?
-      return new Iterator<RowResult>() {
-        RowResult next = null;
-
-        public boolean hasNext() {
-          if (next == null) {
-            try {
-              next = ScannerWrapper.this.next();
-              return next != null;
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-          return true;
-        }
-
-        public RowResult next() {
-          if (!hasNext()) {
-            return null;
-          }
-          RowResult temp = next;
-          next = null;
-          return temp;
-        }
-
-        public void remove() {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/tableindexed/package.html
===================================================================
--- src/java/org/apache/hadoop/hbase/client/tableindexed/package.html	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/tableindexed/package.html	(working copy)
@@ -1,46 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
-<html>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<head />
-<body bgcolor="white">
-
-This package provides support for secondary indexing by maintaining a separate, "index", table for each index. 
-
-The IndexSpecification class provides the metadata for the index. This includes:
-<li> the columns that contribute to the index key,
-<li> additional columns to put in the index table (and are thus made available to filters on the index table),
-<br> and 
-<li> an IndexKeyGenerator which constructs the index-row-key from the indexed column(s) and the original row.
-
-IndexesSpecifications can be added to a table's metadata (HTableDescriptor) before the table is constructed. 
-Afterwards, updates and deletes to the original table will trigger the updates in the index, and 
-the indexes can be scanned using the API on IndexedTable.
-
-For a simple example, look at the unit test in org.apache.hadoop.hbase.client.tableIndexed.
-
-<p> To enable the indexing, modify hbase-site.xml to turn on the
-IndexedRegionServer.  This is done by setting
-<i>hbase.regionserver.class</i> to
-<i>org.apache.hadoop.hbase.ipc.IndexedRegionInterface</i> and
-<i>hbase.regionserver.impl </i> to
-<i>org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer</i>
- 
-</body>
-</html>
Index: src/java/org/apache/hadoop/hbase/client/ScannerCallable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/ScannerCallable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/ScannerCallable.java	(working copy)
@@ -23,37 +23,33 @@
 import java.io.IOException;
 
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Result;
+import org.apache.hadoop.hbase.io.Scan;
 
 
 /**
  * Retries scanner operations such as create, next, etc.
  * Used by {@link Scanner}s made by {@link HTable}.
  */
-public class ScannerCallable extends ServerCallable<RowResult[]> {
+public class ScannerCallable extends ServerCallable<Result[]> {
   private long scannerId = -1L;
   private boolean instantiated = false;
   private boolean closed = false;
-  private final byte [][] columns;
-  private final long timestamp;
-  private final RowFilterInterface filter;
+  private Scan scan;
+  private byte [] startRow;
   private int caching = 1;
 
   /**
    * @param connection
    * @param tableName
-   * @param columns
    * @param startRow
-   * @param timestamp
-   * @param filter
+   * @param scan
    */
-  public ScannerCallable (HConnection connection, byte [] tableName, byte [][] columns,
-      byte [] startRow, long timestamp, RowFilterInterface filter) {
+  public ScannerCallable (HConnection connection, byte [] tableName,
+      byte [] startRow, Scan scan) {
     super(connection, tableName, startRow);
-    this.columns = columns;
-    this.timestamp = timestamp;
-    this.filter = filter;
+    this.scan = scan;
+    this.startRow = startRow;
   }
   
   /**
@@ -71,7 +67,7 @@
   /**
    * @see java.util.concurrent.Callable#call()
    */
-  public RowResult[] call() throws IOException {
+  public Result [] call() throws IOException {
     if (scannerId != -1L && closed) {
       server.close(scannerId);
       scannerId = -1L;
@@ -79,7 +75,7 @@
       // open the scanner
       scannerId = openScanner();
     } else {
-      RowResult [] rrs = server.next(scannerId, caching);
+      Result [] rrs = server.next(scannerId, caching);
       return rrs.length == 0 ? null : rrs;
     }
     return null;
@@ -87,22 +83,13 @@
   
   protected long openScanner() throws IOException {
     return server.openScanner(
-        this.location.getRegionInfo().getRegionName(), columns, row,
-        timestamp, filter);
+        this.location.getRegionInfo().getRegionName(), startRow, scan);
   }
   
-  protected byte [][] getColumns() {
-    return columns;
+  protected Scan getScan() {
+    return scan;
   }
   
-  protected long getTimestamp() {
-    return timestamp;
-  }
-  
-  protected RowFilterInterface getFilter() {
-    return filter;
-  }
-  
   /**
    * Call this when the next invocation of call should close the scanner
    */
Index: src/java/org/apache/hadoop/hbase/client/transactional/TransactionScannerCallable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/TransactionScannerCallable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/TransactionScannerCallable.java	(working copy)
@@ -1,51 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.client.HConnection;
-import org.apache.hadoop.hbase.client.ScannerCallable;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-
-class TransactionScannerCallable extends ScannerCallable {
-
-  private TransactionState transactionState;
-
-  TransactionScannerCallable(final TransactionState transactionState,
-      final HConnection connection, final byte[] tableName,
-      final byte[][] columns, final byte[] startRow, final long timestamp,
-      final RowFilterInterface filter) {
-    super(connection, tableName, columns, startRow, timestamp, filter);
-    this.transactionState = transactionState;
-  }
-
-  @Override
-  protected long openScanner() throws IOException {
-    if (transactionState.addRegion(location)) {
-      ((TransactionalRegionInterface) server).beginTransaction(transactionState
-          .getTransactionId(), location.getRegionInfo().getRegionName());
-    }
-    return ((TransactionalRegionInterface) server).openScanner(transactionState
-        .getTransactionId(), this.location.getRegionInfo().getRegionName(),
-        getColumns(), row, getTimestamp(), getFilter());
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/LocalTransactionLogger.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/LocalTransactionLogger.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/LocalTransactionLogger.java	(working copy)
@@ -1,71 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-/**
- * A local, in-memory implementation of the transaction logger. Does not provide a global view, so 
- * it can't be relighed on by 
- * 
- */
-public class LocalTransactionLogger implements TransactionLogger {
-
-  private static LocalTransactionLogger instance;
-  
-  /**
-   * Creates singleton if it does not exist
-   * 
-   * @return reference to singleton
-   */
-  public synchronized static LocalTransactionLogger getInstance() {
-    if (instance == null) {
-      instance = new LocalTransactionLogger();
-    }
-    return instance;
-  }
-  
-  private Random random = new Random();
-  private Map<Long, TransactionStatus> transactionIdToStatusMap = Collections
-      .synchronizedMap(new HashMap<Long, TransactionStatus>());
-
-  private LocalTransactionLogger() {
-    // Enforce singlton
-  }
-  
-  /** @return random longs to minimize possibility of collision */
-  public long createNewTransactionLog() {
-    long id = random.nextLong();
-    transactionIdToStatusMap.put(id, TransactionStatus.PENDING);
-    return id;
-  }
-
-  public TransactionStatus getStatusForTransaction(final long transactionId) {
-    return transactionIdToStatusMap.get(transactionId);
-  }
-
-  public void setStatusForTransaction(final long transactionId,
-      final TransactionStatus status) {
-    transactionIdToStatusMap.put(transactionId, status);
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/TransactionManager.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/TransactionManager.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/TransactionManager.java	(working copy)
@@ -1,152 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HRegionLocation;
-import org.apache.hadoop.hbase.client.HConnection;
-import org.apache.hadoop.hbase.client.HConnectionManager;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-import org.apache.hadoop.ipc.RemoteException;
-
-/**
- * Transaction Manager. Responsible for committing transactions.
- * 
- */
-public class TransactionManager {
-  static final Log LOG = LogFactory.getLog(TransactionManager.class);
-
-  private final HConnection connection;
-  private final TransactionLogger transactionLogger;
-
-  /**
-   * @param conf
-   */
-  public TransactionManager(final HBaseConfiguration conf) {
-    this(LocalTransactionLogger.getInstance(), conf);
-  }
-
-  /**
-   * @param transactionLogger
-   * @param conf
-   */
-  public TransactionManager(final TransactionLogger transactionLogger,
-      final HBaseConfiguration conf) {
-    this.transactionLogger = transactionLogger;
-    connection = HConnectionManager.getConnection(conf);
-  }
-
-  /**
-   * Called to start a transaction.
-   * 
-   * @return new transaction state
-   */
-  public TransactionState beginTransaction() {
-    long transactionId = transactionLogger.createNewTransactionLog();
-    LOG.debug("Begining transaction " + transactionId);
-    return new TransactionState(transactionId);
-  }
-
-  /**
-   * Try and commit a transaction.
-   * 
-   * @param transactionState
-   * @throws IOException
-   * @throws CommitUnsuccessfulException
-   */
-  public void tryCommit(final TransactionState transactionState)
-      throws CommitUnsuccessfulException, IOException {
-    LOG.debug("atempting to commit trasaction: " + transactionState.toString());
-
-    try {
-      for (HRegionLocation location : transactionState
-          .getParticipatingRegions()) {
-        TransactionalRegionInterface transactionalRegionServer = (TransactionalRegionInterface) connection
-            .getHRegionConnection(location.getServerAddress());
-        boolean canCommit = transactionalRegionServer.commitRequest(location
-            .getRegionInfo().getRegionName(), transactionState
-            .getTransactionId());
-        if (LOG.isTraceEnabled()) {
-          LOG.trace("Region ["
-              + location.getRegionInfo().getRegionNameAsString() + "] votes "
-              + (canCommit ? "to commit" : "to abort") + " transaction "
-              + transactionState.getTransactionId());
-        }
-
-        if (!canCommit) {
-          LOG.debug("Aborting [" + transactionState.getTransactionId() + "]");
-          abort(transactionState, location);
-          throw new CommitUnsuccessfulException();
-        }
-      }
-
-      LOG.debug("Commiting [" + transactionState.getTransactionId() + "]");
-
-      transactionLogger.setStatusForTransaction(transactionState
-          .getTransactionId(), TransactionLogger.TransactionStatus.COMMITTED);
-
-      for (HRegionLocation location : transactionState
-          .getParticipatingRegions()) {
-        TransactionalRegionInterface transactionalRegionServer = (TransactionalRegionInterface) connection
-            .getHRegionConnection(location.getServerAddress());
-        transactionalRegionServer.commit(location.getRegionInfo()
-            .getRegionName(), transactionState.getTransactionId());
-      }
-    } catch (RemoteException e) {
-      LOG.debug("Commit of transaction [" + transactionState.getTransactionId()
-          + "] was unsucsessful", e);
-      // FIXME, think about the what ifs
-      throw new CommitUnsuccessfulException(e);
-    }
-    // Tran log can be deleted now ...
-  }
-
-  /**
-   * Abort a s transaction.
-   * 
-   * @param transactionState
-   * @throws IOException
-   */
-  public void abort(final TransactionState transactionState) throws IOException {
-    abort(transactionState, null);
-  }
-
-  private void abort(final TransactionState transactionState,
-      final HRegionLocation locationToIgnore) throws IOException {
-    transactionLogger.setStatusForTransaction(transactionState
-        .getTransactionId(), TransactionLogger.TransactionStatus.ABORTED);
-
-    for (HRegionLocation location : transactionState.getParticipatingRegions()) {
-      if (locationToIgnore != null && location.equals(locationToIgnore)) {
-        continue;
-      }
-
-      TransactionalRegionInterface transactionalRegionServer = (TransactionalRegionInterface) connection
-          .getHRegionConnection(location.getServerAddress());
-
-      transactionalRegionServer.abort(location.getRegionInfo().getRegionName(),
-          transactionState.getTransactionId());
-    }
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/UnknownTransactionException.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/UnknownTransactionException.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/UnknownTransactionException.java	(working copy)
@@ -1,43 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import org.apache.hadoop.hbase.DoNotRetryIOException;
-
-/**
- * Thrown if a region server is passed an unknown transaction id
- */
-public class UnknownTransactionException extends DoNotRetryIOException {
-
-  private static final long serialVersionUID = 698575374929591099L;
-
-  /** constructor */
-  public UnknownTransactionException() {
-    super();
-  }
-
-  /**
-   * Constructor
-   * @param s message
-   */
-  public UnknownTransactionException(String s) {
-    super(s);
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/TransactionLogger.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/TransactionLogger.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/TransactionLogger.java	(working copy)
@@ -1,59 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-/**
- * Simple interface used to provide a log about transaction status. Written to
- * by the client, and read by regionservers in case of failure. 
- * 
- */
-public interface TransactionLogger {
-
-  /** Transaction status values */
-  enum TransactionStatus {
-    /** Transaction is pending */
-    PENDING,
-    /** Transaction was committed */
-    COMMITTED,
-    /** Transaction was aborted */
-    ABORTED
-  }
-
-  /**
-   * Create a new transaction log. Return the transaction's globally unique id.
-   * Log's initial value should be PENDING
-   * 
-   * @return transaction id
-   */
-  long createNewTransactionLog();
-
-  /**
-   * @param transactionId
-   * @return transaction status
-   */
-  TransactionStatus getStatusForTransaction(long transactionId);
-
-  /**
-   * @param transactionId
-   * @param status
-   */
-  void setStatusForTransaction(long transactionId, TransactionStatus status);
-
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/TransactionState.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/TransactionState.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/TransactionState.java	(working copy)
@@ -1,78 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hbase.HRegionLocation;
-
-/**
- * Holds client-side transaction information. Client's use them as opaque
- * objects passed around to transaction operations.
- * 
- */
-public class TransactionState {
-  static final Log LOG = LogFactory.getLog(TransactionState.class);
-
-  private final long transactionId;
-
-  private Set<HRegionLocation> participatingRegions = new HashSet<HRegionLocation>();
-
-  TransactionState(final long transactionId) {
-    this.transactionId = transactionId;
-  }
-
-  boolean addRegion(final HRegionLocation hregion) {
-    boolean added = participatingRegions.add(hregion);
-
-    if (added) {
-      LOG.debug("Adding new hregion ["
-          + hregion.getRegionInfo().getRegionNameAsString()
-          + "] to transaction [" + transactionId + "]");
-    }
-
-    return added;
-  }
-
-  Set<HRegionLocation> getParticipatingRegions() {
-    return participatingRegions;
-  }
-
-  /**
-   * Get the transactionId.
-   * 
-   * @return Return the transactionId.
-   */
-  public long getTransactionId() {
-    return transactionId;
-  }
-
-  /**
-   * @see java.lang.Object#toString()
-   */
-  @Override
-  public String toString() {
-    return "id: " + transactionId + ", particpants: "
-        + participatingRegions.size();
-  }
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/CommitUnsuccessfulException.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/CommitUnsuccessfulException.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/CommitUnsuccessfulException.java	(working copy)
@@ -1,56 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-/** Thrown when a transaction cannot be committed. 
- * 
- */
-public class CommitUnsuccessfulException extends Exception {
-
-  private static final long serialVersionUID = 7062921444531109202L;
-
-  /** Default Constructor */
-  public CommitUnsuccessfulException() {
-    super();
-  }
-
-  /**
-   * @param arg0 message
-   * @param arg1 cause
-   */
-  public CommitUnsuccessfulException(String arg0, Throwable arg1) {
-    super(arg0, arg1);
-  }
-
-  /**
-   * @param arg0 message
-   */
-  public CommitUnsuccessfulException(String arg0) {
-    super(arg0);
-  }
-
-  /**
-   * @param arg0 cause
-   */
-  public CommitUnsuccessfulException(Throwable arg0) {
-    super(arg0);
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/TransactionalTable.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/TransactionalTable.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/TransactionalTable.java	(working copy)
@@ -1,428 +0,0 @@
-/**
- * Copyright 2008 The Apache Software Foundation
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client.transactional;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.client.HConnection;
-import org.apache.hadoop.hbase.client.HTable;
-import org.apache.hadoop.hbase.client.Scanner;
-import org.apache.hadoop.hbase.client.ScannerCallable;
-import org.apache.hadoop.hbase.client.ServerCallable;
-import org.apache.hadoop.hbase.filter.RowFilterInterface;
-import org.apache.hadoop.hbase.io.BatchUpdate;
-import org.apache.hadoop.hbase.io.Cell;
-import org.apache.hadoop.hbase.io.RowResult;
-import org.apache.hadoop.hbase.ipc.TransactionalRegionInterface;
-
-/**
- * Table with transactional support.
- * 
- */
-public class TransactionalTable extends HTable {
-
-  /**
-   * @param conf
-   * @param tableName
-   * @throws IOException
-   */
-  public TransactionalTable(final HBaseConfiguration conf,
-      final String tableName) throws IOException {
-    super(conf, tableName);
-  }
-
-  /**
-   * @param conf
-   * @param tableName
-   * @throws IOException
-   */
-  public TransactionalTable(final HBaseConfiguration conf,
-      final byte[] tableName) throws IOException {
-    super(conf, tableName);
-  }
-
-  private static abstract class TransactionalServerCallable<T> extends
-      ServerCallable<T> {
-    protected TransactionState transactionState;
-
-    protected TransactionalRegionInterface getTransactionServer() {
-      return (TransactionalRegionInterface) server;
-    }
-
-    protected void recordServer() throws IOException {
-      if (transactionState.addRegion(location)) {
-        getTransactionServer().beginTransaction(
-            transactionState.getTransactionId(),
-            location.getRegionInfo().getRegionName());
-      }
-    }
-
-    /**
-     * @param connection
-     * @param tableName
-     * @param row
-     * @param transactionState
-     */
-    public TransactionalServerCallable(final HConnection connection,
-        final byte[] tableName, final byte[] row,
-        final TransactionState transactionState) {
-      super(connection, tableName, row);
-      this.transactionState = transactionState;
-    }
-
-  }
-
-  /**
-   * Get a single value for the specified row and column
-   * 
-   * @param transactionState
-   * @param row row key
-   * @param column column name
-   * @return value for specified row/column
-   * @throws IOException
-   */
-  public Cell get(final TransactionState transactionState, final byte[] row,
-      final byte[] column) throws IOException {
-    return super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<Cell>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public Cell call() throws IOException {
-            recordServer();
-            return getTransactionServer().get(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, column);
-          }
-        });
-  }
-
-  /**
-   * Get the specified number of versions of the specified row and column
-   * 
-   * @param transactionState
-   * @param row - row key
-   * @param column - column name
-   * @param numVersions - number of versions to retrieve
-   * @return - array byte values
-   * @throws IOException
-   */
-  public Cell[] get(final TransactionState transactionState, final byte[] row,
-      final byte[] column, final int numVersions) throws IOException {
-    Cell[] values = null;
-    values = super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<Cell[]>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public Cell[] call() throws IOException {
-            recordServer();
-            return getTransactionServer().get(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, column,
-                numVersions);
-          }
-        });
-
-    return values;
-  }
-
-  /**
-   * Get the specified number of versions of the specified row and column with
-   * the specified timestamp.
-   * 
-   * @param transactionState
-   * @param row - row key
-   * @param column - column name
-   * @param timestamp - timestamp
-   * @param numVersions - number of versions to retrieve
-   * @return - array of values that match the above criteria
-   * @throws IOException
-   */
-  public Cell[] get(final TransactionState transactionState, final byte[] row,
-      final byte[] column, final long timestamp, final int numVersions)
-      throws IOException {
-    Cell[] values = null;
-    values = super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<Cell[]>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public Cell[] call() throws IOException {
-            recordServer();
-            return getTransactionServer().get(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, column,
-                timestamp, numVersions);
-          }
-        });
-
-    return values;
-  }
-
-  /**
-   * Get all the data for the specified row at the latest timestamp
-   * 
-   * @param transactionState
-   * @param row row key
-   * @return RowResult is empty if row does not exist.
-   * @throws IOException
-   */
-  public RowResult getRow(final TransactionState transactionState,
-      final byte[] row) throws IOException {
-    return getRow(transactionState, row, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Get all the data for the specified row at a specified timestamp
-   * 
-   * @param transactionState
-   * @param row row key
-   * @param ts timestamp
-   * @return RowResult is empty if row does not exist.
-   * @throws IOException
-   */
-  public RowResult getRow(final TransactionState transactionState,
-      final byte[] row, final long ts) throws IOException {
-    return super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<RowResult>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public RowResult call() throws IOException {
-            recordServer();
-            return getTransactionServer().getRow(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, ts);
-          }
-        });
-  }
-
-  /**
-   * Get selected columns for the specified row at the latest timestamp
-   * 
-   * @param transactionState
-   * @param row row key
-   * @param columns Array of column names you want to retrieve.
-   * @return RowResult is empty if row does not exist.
-   * @throws IOException
-   */
-  public RowResult getRow(final TransactionState transactionState,
-      final byte[] row, final byte[][] columns) throws IOException {
-    return getRow(transactionState, row, columns, HConstants.LATEST_TIMESTAMP);
-  }
-
-  /**
-   * Get selected columns for the specified row at a specified timestamp
-   * 
-   * @param transactionState
-   * @param row row key
-   * @param columns Array of column names you want to retrieve.
-   * @param ts timestamp
-   * @return RowResult is empty if row does not exist.
-   * @throws IOException
-   */
-  public RowResult getRow(final TransactionState transactionState,
-      final byte[] row, final byte[][] columns, final long ts)
-      throws IOException {
-    return super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<RowResult>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public RowResult call() throws IOException {
-            recordServer();
-            return getTransactionServer().getRow(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, columns, ts);
-          }
-        });
-  }
-
-  /**
-   * Delete all cells that match the passed row and whose timestamp is equal-to
-   * or older than the passed timestamp.
-   * 
-   * @param transactionState
-   * @param row Row to update
-   * @param ts Delete all cells of the same timestamp or older.
-   * @throws IOException
-   */
-  public void deleteAll(final TransactionState transactionState,
-      final byte[] row, final long ts) throws IOException {
-    super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<Boolean>(super.getConnection(), super
-            .getTableName(), row, transactionState) {
-          public Boolean call() throws IOException {
-            recordServer();
-            getTransactionServer().deleteAll(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), row, ts);
-            return null;
-          }
-        });
-  }
-
-  /**
-   * Get a scanner on the current table starting at first row. Return the
-   * specified columns.
-   * 
-   * @param transactionState
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex in the column qualifier. A column qualifier is judged to be a
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final TransactionState transactionState,
-      final byte[][] columns) throws IOException {
-    return getScanner(transactionState, columns, HConstants.EMPTY_START_ROW,
-        HConstants.LATEST_TIMESTAMP, null);
-  }
-
-  /**
-   * Get a scanner on the current table starting at the specified row. Return
-   * the specified columns.
-   * 
-   * @param transactionState
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex in the column qualifier. A column qualifier is judged to be a
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final TransactionState transactionState,
-      final byte[][] columns, final byte[] startRow) throws IOException {
-    return getScanner(transactionState, columns, startRow,
-        HConstants.LATEST_TIMESTAMP, null);
-  }
-
-  /**
-   * Get a scanner on the current table starting at the specified row. Return
-   * the specified columns.
-   * 
-   * @param transactionState
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex in the column qualifier. A column qualifier is judged to be a
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final TransactionState transactionState,
-      final byte[][] columns, final byte[] startRow, final long timestamp)
-      throws IOException {
-    return getScanner(transactionState, columns, startRow, timestamp, null);
-  }
-
-  /**
-   * Get a scanner on the current table starting at the specified row. Return
-   * the specified columns.
-   * 
-   * @param transactionState
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex in the column qualifier. A column qualifier is judged to be a
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final TransactionState transactionState,
-      final byte[][] columns, final byte[] startRow,
-      final RowFilterInterface filter) throws IOException {
-    return getScanner(transactionState, columns, startRow,
-        HConstants.LATEST_TIMESTAMP, filter);
-  }
-
-  /**
-   * Get a scanner on the current table starting at the specified row. Return
-   * the specified columns.
-   * 
-   * @param transactionState
-   * @param columns columns to scan. If column name is a column family, all
-   * columns of the specified column family are returned. Its also possible to
-   * pass a regex in the column qualifier. A column qualifier is judged to be a
-   * regex if it contains at least one of the following characters:
-   * <code>\+|^&*$[]]}{)(</code>.
-   * @param startRow starting row in table to scan
-   * @param timestamp only return results whose timestamp <= this value
-   * @param filter a row filter using row-key regexp and/or column data filter.
-   * @return scanner
-   * @throws IOException
-   */
-  public Scanner getScanner(final TransactionState transactionState,
-      final byte[][] columns, final byte[] startRow, final long timestamp,
-      final RowFilterInterface filter) throws IOException {
-    ClientScanner scanner = new TransactionalClientScanner(transactionState, columns, startRow,
-        timestamp, filter);
-    scanner.initialize();
-    return scanner;
-  }
-
-  /**
-   * Commit a BatchUpdate to the table.
-   * 
-   * @param transactionState
-   * @param batchUpdate
-   * @throws IOException
-   */
-  public synchronized void commit(final TransactionState transactionState,
-      final BatchUpdate batchUpdate) throws IOException {
-    super.getConnection().getRegionServerWithRetries(
-        new TransactionalServerCallable<Boolean>(super.getConnection(), super
-            .getTableName(), batchUpdate.getRow(), transactionState) {
-          public Boolean call() throws IOException {
-            recordServer();
-            getTransactionServer().batchUpdate(
-                transactionState.getTransactionId(),
-                location.getRegionInfo().getRegionName(), batchUpdate);
-            return null;
-          }
-        });
-  }
-
-  protected class TransactionalClientScanner extends HTable.ClientScanner {
-
-    private TransactionState transactionState;
-
-    protected TransactionalClientScanner(
-        final TransactionState transactionState, final byte[][] columns,
-        final byte[] startRow, final long timestamp,
-        final RowFilterInterface filter) {
-      super(columns, startRow, timestamp, filter);
-      this.transactionState = transactionState;
-    }
-
-    @Override
-    protected ScannerCallable getScannerCallable(
-        final byte[] localStartKey, int caching) {
-      TransactionScannerCallable t = 
-          new TransactionScannerCallable(transactionState, getConnection(),
-          getTableName(), getColumns(), localStartKey, getTimestamp(),
-          getFilter());
-      t.setCaching(caching);
-      return t;
-    }
-  }
-
-}
Index: src/java/org/apache/hadoop/hbase/client/transactional/package.html
===================================================================
--- src/java/org/apache/hadoop/hbase/client/transactional/package.html	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/transactional/package.html	(working copy)
@@ -1,61 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
-<html>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<head />
-<body bgcolor="white">
-
-This package provides support for atomic transactions. Transactions can
-span multiple regions. Transaction writes are applied when committing a
-transaction. At commit time, the transaction is examined to see if it
-can be applied while still maintaining atomicity. This is done by
-looking for conflicts with the transactions that committed while the
-current transaction was running. This technique is known as optimistic
-concurrency control (OCC) because it relies on the assumption that
-transactions will mostly not have conflicts with each other.
-
-<p>
-For more details on OCC, see the paper <i> On Optimistic Methods for Concurrency Control </i> 
-by Kung and Robinson available 
-<a href=http://www.seas.upenn.edu/~zives/cis650/papers/opt-cc.pdf> here </a>.
-
-<p> To enable transactions, modify hbase-site.xml to turn on the
-TransactionalRegionServer.  This is done by setting
-<i>hbase.regionserver.class</i> to
-<i>org.apache.hadoop.hbase.ipc.TransactionalRegionInterface</i> and
-<i>hbase.regionserver.impl </i> to
-<i>org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer</i>
-
-<p> 
-The read set claimed by a transactional scanner is determined from the start and
- end keys which the scanner is opened with. 
-
-
-
-<h3> Known Issues </h3>
-
-Recovery in the face of hregion server failure
-is not fully implemented. Thus, you cannot rely on the transactional
-properties in the face of node failure.
-
-
-
- 
-</body>
-</html>
Index: src/java/org/apache/hadoop/hbase/client/Scanner.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/Scanner.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/Scanner.java	(working copy)
@@ -1,5 +1,5 @@
 /**
- * Copyright 2008 The Apache Software Foundation
+ * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
@@ -21,32 +21,30 @@
 
 import java.io.Closeable;
 import java.io.IOException;
-import org.apache.hadoop.hbase.io.RowResult;
 
+import org.apache.hadoop.hbase.io.Result;
+
 /**
  * Interface for client-side scanning.
  * Go to {@link HTable} to obtain instances.
  */
-public interface Scanner extends Closeable, Iterable<RowResult> {
+public interface Scanner extends Closeable, Iterable<Result> {
+
   /**
-   * Grab the next row's worth of values. The scanner will return a RowResult
-   * that contains both the row's key and a map of byte[] column names to Cell 
-   * value objects. The data returned will only contain the most recent data 
-   * value for each row that is not newer than the target time passed when the
-   * scanner was created.
-   * @return RowResult object if there is another row, null if the scanner is
+   * Grab the next row's worth of values. The scanner will return a Result.
+   * @return Result object if there is another row, null if the scanner is
    * exhausted.
    * @throws IOException
    */  
-  public RowResult next() throws IOException;
-  
+  public Result next() throws IOException;
+ 
   /**
    * @param nbRows number of rows to return
-   * @return Between zero and <param>nbRows</param> RowResults
+   * @return Between zero and <param>nbRows</param> Results
    * @throws IOException
    */
-  public RowResult [] next(int nbRows) throws IOException;
-  
+  public Result [] next(int nbRows) throws IOException;
+ 
   /**
    * Closes the scanner and releases any resources it has allocated
    */
Index: src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java	(working copy)
@@ -36,6 +36,7 @@
 import org.apache.hadoop.hbase.io.Cell;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.io.RowResult;
+import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -120,11 +121,13 @@
     return this.connection.listTables();
   }
 
-  public HTableDescriptor getTableDescriptor(final String tableName)
-  throws IOException {
-    return getTableDescriptor(Bytes.toBytes(tableName));
-  }
   
+  /**
+   * Method for getting the tableDescriptor
+   * @param tableName as a byte []
+   * @return the tableDescriptor
+   * @throws IOException
+   */
   public HTableDescriptor getTableDescriptor(final byte [] tableName)
   throws IOException {
     return this.connection.getHTableDescriptor(tableName);
@@ -237,11 +240,14 @@
     for (int tries = 0; tries < numRetries; tries++) {
       long scannerId = -1L;
       try {
-        scannerId =
-          server.openScanner(firstMetaServer.getRegionInfo().getRegionName(),
-            HConstants.COL_REGIONINFO_ARRAY, tableName,
-            HConstants.LATEST_TIMESTAMP, null);
-        RowResult values = server.next(scannerId);
+        Scan scan = new Scan();
+        scan.addColumn(HConstants.CATALOG_FAMILY, 
+            HConstants.REGIONINFO_QUALIFIER);
+        
+        scannerId = server.openScanner(
+            firstMetaServer.getRegionInfo().getRegionName(), 
+            HConstants.EMPTY_START_ROW, scan);
+        RowResult values = server.next(scannerId).rowResult();
         if (values == null || values.size() == 0) {
           break;
         }
Index: src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
===================================================================
--- src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java	(revision 775412)
+++ src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java	(working copy)
@@ -22,7 +22,7 @@
 
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
+//import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
 
 /**
  * Read-only table descriptor.
@@ -37,10 +37,14 @@
    * Create an unmodifyable copy of an HTableDescriptor
    * @param desc
    */
+//  UnmodifyableHTableDescriptor(final HTableDescriptor desc) {
+//    super(desc.getName(), getUnmodifyableFamilies(desc), desc.getIndexes(), desc.getValues());
+//  }
   UnmodifyableHTableDescriptor(final HTableDescriptor desc) {
-    super(desc.getName(), getUnmodifyableFamilies(desc), desc.getIndexes(), desc.getValues());
+    super(desc.getName(), getUnmodifyableFamilies(desc), desc.getValues());
   }
   
+  
   /*
    * @param desc
    * @return Families as unmodifiable array.
@@ -122,11 +126,11 @@
     throw new UnsupportedOperationException("HTableDescriptor is read-only");
   }
 
-  /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#addIndex(org.apache.hadoop.hbase.client.tableindexed.IndexSpecification)
-   */
-  @Override
-  public void addIndex(IndexSpecification index) {
-    throw new UnsupportedOperationException("HTableDescriptor is read-only"); 
-  }
+//  /**
+//   * @see org.apache.hadoop.hbase.HTableDescriptor#addIndex(org.apache.hadoop.hbase.client.tableindexed.IndexSpecification)
+//   */
+//  @Override
+//  public void addIndex(IndexSpecification index) {
+//    throw new UnsupportedOperationException("HTableDescriptor is read-only"); 
+//  }
 }
