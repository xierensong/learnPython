diff --git a/src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java b/src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
index d72462f..136709a 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java
@@ -79,6 +79,9 @@ public class ExplicitColumnTracker implements ColumnTracker {
     }
     
     // No more columns to match against, done with storefile
+    // TODO ryan this needs to be a skip or a done.
+    // this needs to be a 'skip' for the scanner, since it must keep
+    // on iterating until we get to the next row.
     if(this.column == null) {
       return MatchCode.NEXT;
     }
@@ -107,6 +110,7 @@ public class ExplicitColumnTracker implements ColumnTracker {
     if(ret <= -1) {
       if(++this.index == this.columns.size()) {
         // No more to match, do not include, done with storefile
+        // TODO ryan this needs to also be a 'skip' or 'done.
         return MatchCode.NEXT;
       }
       this.column = this.columns.get(this.index);
@@ -130,7 +134,8 @@ public class ExplicitColumnTracker implements ColumnTracker {
       this.column = null;
     }
   }
-  
+
+  // Called between every row.
   public void reset() {
     buildColumnList(this.origColumns);
     this.index = 0;
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java b/src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
index 43e1a6d..5db8128 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/GetDeleteTracker.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
@@ -311,6 +311,7 @@ public class GetDeleteTracker implements DeleteTracker {
     // Branches below can be optimized.  Keeping like this until testing
     // is complete.
     if(oldDelete.type == newDelete.type) {
+      // the one case where we can merge 2 deletes -> 1 delete.
       if(oldDelete.type == KeyValue.Type.Delete.getCode()){
         if(oldDelete.timestamp > newDelete.timestamp) {
           return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
@@ -326,6 +327,9 @@ public class GetDeleteTracker implements DeleteTracker {
       return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
     }
 
+    // old delete is more specific than the new delete.
+    // if the olddelete is newer then the newdelete, we have to
+    //  keep it
     if(oldDelete.type < newDelete.type) {
       if(oldDelete.timestamp > newDelete.timestamp) {
         return DeleteCompare.INCLUDE_OLD_NEXT_OLD;
@@ -336,6 +340,7 @@ public class GetDeleteTracker implements DeleteTracker {
       }
     }
 
+    // new delete is more specific than the old delete.
     if(oldDelete.type > newDelete.type) {
       if(oldDelete.timestamp > newDelete.timestamp) {
         return DeleteCompare.NEXT_NEW;
@@ -346,8 +351,9 @@ public class GetDeleteTracker implements DeleteTracker {
       }
     }
 
-    // Should never reach
-    return DeleteCompare.INCLUDE_OLD_NEXT_BOTH;
+    // Should never reach,
+    // throw exception for assertion?
+    throw new RuntimeException("GetDeleteTracker:compareDelete reached terminal state");
   }
 
   /**
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 1f870d1..5e9ea04 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -1858,7 +1858,7 @@ public class HRegion implements HConstants {
   /**
    * RegionScanner is an iterator through a bunch of rows in an HRegion.
    * <p>
-   * It is used to combine scanners from multiple Stores.
+   * It is used to combine scanners from multiple Stores (aka column families).
    */
   private class RegionScanner implements InternalScanner {
     
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java b/src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
index 053227d..f52828b 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/QueryMatcher.java
@@ -1,4 +1,4 @@
-/**
+/*
  * Copyright 2009 The Apache Software Foundation
  *
  * Licensed to the Apache Software Foundation (ASF) under one
@@ -137,6 +137,7 @@ public class QueryMatcher {
     this.tr = scan.getTimeRange();
     this.oldestStamp = System.currentTimeMillis() - ttl;
     this.rowComparator = rowComparator;
+    // shouldn't this be ScanDeleteTracker?
     this.deletes =  new GetDeleteTracker();
     this.startKey = KeyValue.createFirstOnRow(row);
     // Single branch to deal with two types of reads (columns vs all in family)
@@ -197,7 +198,9 @@ public class QueryMatcher {
     
     short rowLength = Bytes.toShort(bytes, offset);
     offset += Bytes.SIZEOF_SHORT;
-    
+
+    // scanners are relying on us to check the row first, and return
+    // "NEXT" when we are there.
     /* Check ROW
      * If past query's row, go to next StoreFile
      * If not reached query's row, go to next KeyValue
@@ -227,6 +230,9 @@ public class QueryMatcher {
      */
     long timestamp = Bytes.toLong(bytes, offset);
     if(isExpired(timestamp)) {
+      // this should be a SKIP for Scans!
+      // for get, this lets us know we have finished with this file.
+      // hence the 'next'.
       return MatchCode.NEXT;
     }
     offset += Bytes.SIZEOF_LONG;
@@ -267,7 +273,8 @@ public class QueryMatcher {
      */
     return columns.checkColumn(bytes, columnOffset, columnLength);
   }
-  
+
+  // should be in KeyValue.
   private boolean isDelete(byte type) {
     return (type != KeyValue.Type.Put.getCode());
   }
@@ -301,6 +308,8 @@ public class QueryMatcher {
    */
   public void setRow(byte [] row) {
     this.row = row;
+    // should reset deletes and columns.
+    reset();
   }
   
   /**
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java b/src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
index b1da40f..612f763 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/ScanDeleteTracker.java
@@ -1,3 +1,23 @@
+/*
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.hadoop.hbase.regionserver;
 
 import org.apache.hadoop.hbase.KeyValue;
@@ -6,11 +26,15 @@ import org.apache.hadoop.hbase.util.Bytes;
 /**
  * This class is responsible for the tracking and enforcement of Deletes
  * during the course of a Scan operation.
+ *
+ * It only has to enforce Delete and DeleteColumn, since the
+ * DeleteFamily is handled at a higher level.
+ *
  * <p>
  * This class is utilized through three methods:
- * <ul><li>{@link add} when encountering a Delete
+ * <ul><li>{@link add} when encountering a Delete or DeleteColumn
  * <li>{@link isDeleted} when checking if a Put KeyValue has been deleted
- * <li>{@link update} when reaching the end of a StoreFile
+ * <li>{@link update} when reaching the end of a StoreFile or row for scans
  * <p>
  * This class is NOT thread-safe as queries are never multi-threaded 
  */
@@ -42,29 +66,33 @@ public class ScanDeleteTracker implements DeleteTracker {
   @Override
   public void add(byte[] buffer, int qualifierOffset, int qualifierLength,
       long timestamp, byte type) {
-    if(timestamp > familyStamp){
+    if(timestamp > familyStamp) {
       if(type == KeyValue.Type.DeleteFamily.getCode()) {
         familyStamp = timestamp;
         return;
       }
 
-      if(deleteBuffer != null && type < deleteType){
+      if(deleteBuffer != null && type < deleteType) {
+        // same column, so ignore less specific delete
         if(Bytes.compareTo(deleteBuffer, deleteOffset, deleteLength,
             buffer, qualifierOffset, qualifierLength) == 0){
           return;
         }
       }
+      // new column, or more general delete type
       deleteBuffer = buffer;
       deleteOffset = qualifierOffset;
       deleteLength = qualifierLength;
       deleteType = type;
       deleteTimestamp = timestamp;
     }
+    // missing else is never called.
   }
 
   /** 
    * Check if the specified KeyValue buffer has been deleted by a previously
    * seen delete.
+   *
    * @param buffer KeyValue buffer
    * @param qualifierOffset column qualifier offset
    * @param qualifierLength column qualifier length
@@ -83,15 +111,19 @@ public class ScanDeleteTracker implements DeleteTracker {
           buffer, qualifierOffset, qualifierLength);
 
       if(ret == 0){
-        if(deleteType == KeyValue.Type.DeleteColumn.getCode()){
+        if(deleteType == KeyValue.Type.DeleteColumn.getCode()) {
           return true;
         }
+        // if we handle DeleteFamily, we are incorrectly clearning it here.
+        // TODO ryan check this shit out.
+        // Looks like a bug - we'll clear out the DeleteFamily here. damn.
         ret = (int)(deleteTimestamp - timestamp);
         deleteBuffer = null;
-        if(ret == 0){
+        if(ret == 0 ) {
           return true;
         }
       } else if(ret < 0){
+        // Next column case.
         deleteBuffer = null;
       } else {
         //Should never happen, throw Exception
@@ -107,12 +139,14 @@ public class ScanDeleteTracker implements DeleteTracker {
   }
 
   @Override
+  // called between every row.
   public void reset() {
     familyStamp = 0L;
     deleteBuffer = null;
   }
 
   @Override
+  // should not be called at all even (!)  
   public void update() {
     this.reset();
   }
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java b/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
index 9e21c17..83e14f9 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
@@ -35,7 +35,8 @@ import org.apache.hadoop.hbase.io.Scan;
 import org.apache.hadoop.hbase.io.hfile.HFileScanner;
 
 /**
- * Scanner scans both the memcache and the HStore
+ * Scanner scans both the memcache and the HStore. Coaleace KeyValue stream
+ * into List<KeyValue> for a single row.
  */
 class StoreScanner implements KeyValueScanner, InternalScanner,
 ChangedReadersObserver {
@@ -101,6 +102,8 @@ ChangedReadersObserver {
    * @return true if there are more rows, false if scanner is done
    */
   public boolean next(List<KeyValue> result) throws IOException {
+    // this wont get us the next row if the previous round hasn't iterated
+    // past all the cols from the previous row. Potential bug!
     matcher.setRow(this.heap.peek().getRow());
     KeyValue kv;
     while((kv = this.heap.peek()) != null) {
@@ -109,9 +112,22 @@ ChangedReadersObserver {
           result.add(this.heap.next());
           continue;
         case SKIP:
+        case DONE:
+          // the query matcher returns 'done' when query has been satisified,
+          // but we still next to get to the next row in the sequence.
+          // at that point the query matcher returns 'next' letting us know we
+          // are there.
+
+          // if this NEXT was delivered to us by the column matcher, we need to
+          // ignore it anyways.
+
+          // Some test cases:
+          // scan on the following stream with col 'a':
+          // R1: put a, put b
+          // R2: put <dont-care>
+          // scan should fail.
           continue;
         case NEXT:
-        case DONE:
           return true;
       }
     }
