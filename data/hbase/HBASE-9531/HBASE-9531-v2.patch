From 7806f1e0bd4ac53ba61071665a37a021d7dc4ed9 Mon Sep 17 00:00:00 2001
From: Ashish Singhi <ashish.singhi@huawei.com>
Date: Thu, 12 Feb 2015 15:32:28 +0530
Subject: [PATCH] HBASE-9531 a command line (hbase shell) interface to retreive
 the replication metrics and show replication lag

---
 .../java/org/apache/hadoop/hbase/ServerLoad.java   |   27 +-
 .../apache/hadoop/hbase/protobuf/ProtobufUtil.java |   24 +
 .../hbase/replication/ReplicationLoadSink.java     |   36 +
 .../hbase/replication/ReplicationLoadSource.java   |   53 +
 .../regionserver/MetricsReplicationSinkSource.java |    1 +
 .../MetricsReplicationSourceSource.java            |    1 +
 .../MetricsReplicationGlobalSourceSource.java      |    5 +
 .../MetricsReplicationSinkSourceImpl.java          |    5 +
 .../MetricsReplicationSourceSourceImpl.java        |    5 +
 .../protobuf/generated/ClusterStatusProtos.java    | 5535 ++++++++++++++------
 .../src/main/protobuf/ClusterStatus.proto          |   23 +
 .../hadoop/hbase/regionserver/HRegionServer.java   |   17 +
 .../hbase/regionserver/ReplicationService.java     |    8 +-
 .../replication/regionserver/MetricsSink.java      |   17 +
 .../replication/regionserver/MetricsSource.java    |   34 +
 .../replication/regionserver/Replication.java      |   32 +-
 .../replication/regionserver/ReplicationLoad.java  |  153 +
 .../replication/regionserver/ReplicationSink.java  |    8 +
 .../regionserver/ReplicationSource.java            |    8 +
 .../replication/TestReplicationSmallTests.java     |   45 +
 hbase-shell/src/main/ruby/hbase/admin.rb           |   42 +-
 hbase-shell/src/main/ruby/shell/commands/status.rb |    9 +-
 hbase-shell/src/test/ruby/hbase/admin_test.rb      |   12 +
 hbase-shell/src/test/ruby/test_helper.rb           |    4 +
 24 files changed, 4472 insertions(+), 1632 deletions(-)
 create mode 100644 hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSink.java
 create mode 100644 hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSource.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java

diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
index 9141659..7813b4a 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/ServerLoad.java
@@ -28,8 +28,11 @@ import java.util.TreeSet;
 
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceStability;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor;
+import org.apache.hadoop.hbase.replication.ReplicationLoadSink;
+import org.apache.hadoop.hbase.replication.ReplicationLoadSource;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Strings;
 
@@ -52,7 +55,7 @@ public class ServerLoad {
   private int totalStaticBloomSizeKB = 0;
   private long totalCompactingKVs = 0;
   private long currentCompactedKVs = 0;
-  
+
   public ServerLoad(ClusterStatusProtos.ServerLoad serverLoad) {
     this.serverLoad = serverLoad;
     for (ClusterStatusProtos.RegionLoad rl: serverLoad.getRegionLoadsList()) {
@@ -70,7 +73,7 @@ public class ServerLoad {
       totalCompactingKVs += rl.getTotalCompactingKVs();
       currentCompactedKVs += rl.getCurrentCompactedKVs();
     }
-    
+
   }
 
   // NOTE: Function name cannot start with "get" because then an OpenDataException is thrown because
@@ -178,6 +181,26 @@ public class ServerLoad {
   }
 
   /**
+   * Call directly from client such as hbase shell
+   * @return the list of ReplicationLoadSource
+   */
+  public List<ReplicationLoadSource> getReplicationLoadSourceList() {
+    return ProtobufUtil.toReplicationLoadSourceList(serverLoad.getReplLoadSourceList());
+  }
+
+  /**
+   * Call directly from client such as hbase shell
+   * @return ReplicationLoadSink
+   */
+  public ReplicationLoadSink getReplicationLoadSink() {
+    if (serverLoad.hasReplLoadSink()) {
+      return ProtobufUtil.toReplicationLoadSink(serverLoad.getReplLoadSink());
+    } else {
+      return null;
+    }
+  }
+
+  /**
    * Originally, this method factored in the effect of requests going to the
    * server as well. However, this does not interact very well with the current
    * region rebalancing code, which only factors number of regions. For the
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index 9c383f8..492aefa 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -104,6 +104,7 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.Col
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.DeleteType;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutationProto.MutationType;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
+import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;
 import org.apache.hadoop.hbase.protobuf.generated.ComparatorProtos;
 import org.apache.hadoop.hbase.protobuf.generated.FilterProtos;
@@ -128,6 +129,8 @@ import org.apache.hadoop.hbase.protobuf.generated.WALProtos.RegionEventDescripto
 import org.apache.hadoop.hbase.quotas.QuotaScope;
 import org.apache.hadoop.hbase.quotas.QuotaType;
 import org.apache.hadoop.hbase.quotas.ThrottleType;
+import org.apache.hadoop.hbase.replication.ReplicationLoadSink;
+import org.apache.hadoop.hbase.replication.ReplicationLoadSource;
 import org.apache.hadoop.hbase.security.access.Permission;
 import org.apache.hadoop.hbase.security.access.TablePermission;
 import org.apache.hadoop.hbase.security.access.UserPermission;
@@ -2963,4 +2966,25 @@ public final class ProtobufUtil {
             .setScope(toProtoQuotaScope(scope))
             .build();
   }
+
+  public static ReplicationLoadSink toReplicationLoadSink(
+      ClusterStatusProtos.ReplicationLoadSink cls) {
+    return new ReplicationLoadSink(cls.getAgeOfLastAppliedOp(), cls.getTimeStampsOfLastAppliedOp());
+  }
+
+  public static ReplicationLoadSource toReplicationLoadSource(
+      ClusterStatusProtos.ReplicationLoadSource cls) {
+    return new ReplicationLoadSource(cls.getPeerID(), cls.getAgeOfLastShippedOp(),
+        cls.getSizeOfLogQueue(), cls.getTimeStampOfLastShippedOp(), cls.getReplicationLag());
+  }
+
+  public static List<ReplicationLoadSource> toReplicationLoadSourceList(
+      List<ClusterStatusProtos.ReplicationLoadSource> clsList) {
+    ArrayList<ReplicationLoadSource> rlsList = new ArrayList<ReplicationLoadSource>();
+    for (ClusterStatusProtos.ReplicationLoadSource cls : clsList) {
+      rlsList.add(toReplicationLoadSource(cls));
+    }
+    return rlsList;
+  }
+
 }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSink.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSink.java
new file mode 100644
index 0000000..63fe334
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSink.java
@@ -0,0 +1,36 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable
+ * law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
+ * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License
+ * for the specific language governing permissions and limitations under the License.
+ */
+package org.apache.hadoop.hbase.replication;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+
+/**
+ * A HBase ReplicationLoad to present MetricsSink information
+ */
+@InterfaceAudience.Private
+public class ReplicationLoadSink {
+  private long ageOfLastAppliedOp;
+  private long timeStampsOfLastAppliedOp;
+
+  public ReplicationLoadSink(long age, long timeStamp) {
+    this.ageOfLastAppliedOp = age;
+    this.timeStampsOfLastAppliedOp = timeStamp;
+  }
+
+  public long getAgeOfLastAppliedOp() {
+    return this.ageOfLastAppliedOp;
+  }
+
+  public long getTimeStampsOfLastAppliedOp() {
+    return this.timeStampsOfLastAppliedOp;
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSource.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSource.java
new file mode 100644
index 0000000..bfd1599
--- /dev/null
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/replication/ReplicationLoadSource.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license
+ * agreements. See the NOTICE file distributed with this work for additional information regarding
+ * copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License. You may obtain a
+ * copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable
+ * law or agreed to in writing, software distributed under the License is distributed on an "AS IS"
+ * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License
+ * for the specific language governing permissions and limitations under the License.
+ */
+package org.apache.hadoop.hbase.replication;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+
+/**
+ * A HBase ReplicationLoad to present MetricsSource information
+ */
+@InterfaceAudience.Private
+public class ReplicationLoadSource {
+  private String peerID;
+  private long ageOfLastShippedOp;
+  private int sizeOfLogQueue;
+  private long timeStampOfLastShippedOp;
+  private long replicationLag;
+
+  public ReplicationLoadSource(String id, long age, int size, long timeStamp, long lag) {
+    this.peerID = id;
+    this.ageOfLastShippedOp = age;
+    this.sizeOfLogQueue = size;
+    this.timeStampOfLastShippedOp = timeStamp;
+    this.replicationLag = lag;
+  }
+
+  public String getPeerID() {
+    return this.peerID;
+  }
+
+  public long getAgeOfLastShippedOp() {
+    return this.ageOfLastShippedOp;
+  }
+
+  public long getSizeOfLogQueue() {
+    return this.sizeOfLogQueue;
+  }
+
+  public long getTimeStampOfLastShippedOp() {
+    return this.timeStampOfLastShippedOp;
+  }
+
+  public long getReplicationLag() {
+    return this.replicationLag;
+  }
+}
\ No newline at end of file
diff --git a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java
index 805dfca..698a59a 100644
--- a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java
+++ b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSource.java
@@ -26,4 +26,5 @@ public interface MetricsReplicationSinkSource {
   void setLastAppliedOpAge(long age);
   void incrAppliedBatches(long batches);
   void incrAppliedOps(long batchsize);
+  long getLastAppliedOpAge();
 }
diff --git a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java
index 66d265a..fecf191 100644
--- a/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java
+++ b/hbase-hadoop-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSource.java
@@ -43,4 +43,5 @@ public interface MetricsReplicationSourceSource {
   void incrLogReadInBytes(long size);
   void incrLogReadInEdits(long size);
   void clear();
+  long getLastShippedAge();
 }
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java
index a210171..6dace10 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationGlobalSourceSource.java
@@ -95,4 +95,9 @@ public class MetricsReplicationGlobalSourceSource implements MetricsReplicationS
 
   @Override public void clear() {
   }
+
+  @Override
+  public long getLastShippedAge() {
+    return ageOfLastShippedOpGauge.value();
+  }
 }
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSourceImpl.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSourceImpl.java
index 3025e3e..14212ba 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSourceImpl.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSinkSourceImpl.java
@@ -44,4 +44,9 @@ public class MetricsReplicationSinkSourceImpl implements MetricsReplicationSinkS
   @Override public void incrAppliedOps(long batchsize) {
     opsCounter.incr(batchsize);
   }
+
+  @Override
+  public long getLastAppliedOpAge() {
+    return ageGauge.value();
+  }
 }
diff --git a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSourceImpl.java b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSourceImpl.java
index 89ef4de..1422e7e 100644
--- a/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSourceImpl.java
+++ b/hbase-hadoop2-compat/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsReplicationSourceSourceImpl.java
@@ -125,4 +125,9 @@ public class MetricsReplicationSourceSourceImpl implements MetricsReplicationSou
 
     rms.removeMetric(logEditsFilteredKey);
   }
+
+  @Override
+  public long getLastShippedAge() {
+    return ageOfLastShippedOpGauge.value();
+  }
 }
diff --git a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos.java b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos.java
index 6dc48fa..0d69d7a 100644
--- a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos.java
+++ b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClusterStatusProtos.java
@@ -4438,273 +4438,48 @@ public final class ClusterStatusProtos {
     // @@protoc_insertion_point(class_scope:RegionLoad)
   }
 
-  public interface ServerLoadOrBuilder
+  public interface ReplicationLoadSinkOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
 
-    // optional uint32 number_of_requests = 1;
-    /**
-     * <code>optional uint32 number_of_requests = 1;</code>
-     *
-     * <pre>
-     ** Number of requests since last report. 
-     * </pre>
-     */
-    boolean hasNumberOfRequests();
-    /**
-     * <code>optional uint32 number_of_requests = 1;</code>
-     *
-     * <pre>
-     ** Number of requests since last report. 
-     * </pre>
-     */
-    int getNumberOfRequests();
-
-    // optional uint32 total_number_of_requests = 2;
-    /**
-     * <code>optional uint32 total_number_of_requests = 2;</code>
-     *
-     * <pre>
-     ** Total Number of requests from the start of the region server. 
-     * </pre>
-     */
-    boolean hasTotalNumberOfRequests();
-    /**
-     * <code>optional uint32 total_number_of_requests = 2;</code>
-     *
-     * <pre>
-     ** Total Number of requests from the start of the region server. 
-     * </pre>
-     */
-    int getTotalNumberOfRequests();
-
-    // optional uint32 used_heap_MB = 3;
-    /**
-     * <code>optional uint32 used_heap_MB = 3;</code>
-     *
-     * <pre>
-     ** the amount of used heap, in MB. 
-     * </pre>
-     */
-    boolean hasUsedHeapMB();
-    /**
-     * <code>optional uint32 used_heap_MB = 3;</code>
-     *
-     * <pre>
-     ** the amount of used heap, in MB. 
-     * </pre>
-     */
-    int getUsedHeapMB();
-
-    // optional uint32 max_heap_MB = 4;
-    /**
-     * <code>optional uint32 max_heap_MB = 4;</code>
-     *
-     * <pre>
-     ** the maximum allowable size of the heap, in MB. 
-     * </pre>
-     */
-    boolean hasMaxHeapMB();
-    /**
-     * <code>optional uint32 max_heap_MB = 4;</code>
-     *
-     * <pre>
-     ** the maximum allowable size of the heap, in MB. 
-     * </pre>
-     */
-    int getMaxHeapMB();
-
-    // repeated .RegionLoad region_loads = 5;
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> 
-        getRegionLoadsList();
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index);
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    int getRegionLoadsCount();
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
-        getRegionLoadsOrBuilderList();
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
-        int index);
-
-    // repeated .Coprocessor coprocessors = 6;
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> 
-        getCoprocessorsList();
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index);
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    int getCoprocessorsCount();
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
-        getCoprocessorsOrBuilderList();
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
-        int index);
-
-    // optional uint64 report_start_time = 7;
-    /**
-     * <code>optional uint64 report_start_time = 7;</code>
-     *
-     * <pre>
-     **
-     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    boolean hasReportStartTime();
-    /**
-     * <code>optional uint64 report_start_time = 7;</code>
-     *
-     * <pre>
-     **
-     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    long getReportStartTime();
-
-    // optional uint64 report_end_time = 8;
+    // required uint64 ageOfLastAppliedOp = 1;
     /**
-     * <code>optional uint64 report_end_time = 8;</code>
-     *
-     * <pre>
-     **
-     * Time when report was generated.
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
+     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
      */
-    boolean hasReportEndTime();
+    boolean hasAgeOfLastAppliedOp();
     /**
-     * <code>optional uint64 report_end_time = 8;</code>
-     *
-     * <pre>
-     **
-     * Time when report was generated.
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
+     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
      */
-    long getReportEndTime();
+    long getAgeOfLastAppliedOp();
 
-    // optional uint32 info_server_port = 9;
+    // required uint64 timeStampsOfLastAppliedOp = 2;
     /**
-     * <code>optional uint32 info_server_port = 9;</code>
-     *
-     * <pre>
-     **
-     * The port number that this region server is hosing an info server on.
-     * </pre>
+     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
      */
-    boolean hasInfoServerPort();
+    boolean hasTimeStampsOfLastAppliedOp();
     /**
-     * <code>optional uint32 info_server_port = 9;</code>
-     *
-     * <pre>
-     **
-     * The port number that this region server is hosing an info server on.
-     * </pre>
+     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
      */
-    int getInfoServerPort();
+    long getTimeStampsOfLastAppliedOp();
   }
   /**
-   * Protobuf type {@code ServerLoad}
+   * Protobuf type {@code ReplicationLoadSink}
    */
-  public static final class ServerLoad extends
+  public static final class ReplicationLoadSink extends
       com.google.protobuf.GeneratedMessage
-      implements ServerLoadOrBuilder {
-    // Use ServerLoad.newBuilder() to construct.
-    private ServerLoad(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+      implements ReplicationLoadSinkOrBuilder {
+    // Use ReplicationLoadSink.newBuilder() to construct.
+    private ReplicationLoadSink(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
       super(builder);
       this.unknownFields = builder.getUnknownFields();
     }
-    private ServerLoad(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+    private ReplicationLoadSink(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
 
-    private static final ServerLoad defaultInstance;
-    public static ServerLoad getDefaultInstance() {
+    private static final ReplicationLoadSink defaultInstance;
+    public static ReplicationLoadSink getDefaultInstance() {
       return defaultInstance;
     }
 
-    public ServerLoad getDefaultInstanceForType() {
+    public ReplicationLoadSink getDefaultInstanceForType() {
       return defaultInstance;
     }
 
@@ -4714,7 +4489,7 @@ public final class ClusterStatusProtos {
         getUnknownFields() {
       return this.unknownFields;
     }
-    private ServerLoad(
+    private ReplicationLoadSink(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
         throws com.google.protobuf.InvalidProtocolBufferException {
@@ -4739,53 +4514,12 @@ public final class ClusterStatusProtos {
             }
             case 8: {
               bitField0_ |= 0x00000001;
-              numberOfRequests_ = input.readUInt32();
+              ageOfLastAppliedOp_ = input.readUInt64();
               break;
             }
             case 16: {
               bitField0_ |= 0x00000002;
-              totalNumberOfRequests_ = input.readUInt32();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              usedHeapMB_ = input.readUInt32();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              maxHeapMB_ = input.readUInt32();
-              break;
-            }
-            case 42: {
-              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
-                regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>();
-                mutable_bitField0_ |= 0x00000010;
-              }
-              regionLoads_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.PARSER, extensionRegistry));
-              break;
-            }
-            case 50: {
-              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
-                coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>();
-                mutable_bitField0_ |= 0x00000020;
-              }
-              coprocessors_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.PARSER, extensionRegistry));
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000010;
-              reportStartTime_ = input.readUInt64();
-              break;
-            }
-            case 64: {
-              bitField0_ |= 0x00000020;
-              reportEndTime_ = input.readUInt64();
-              break;
-            }
-            case 72: {
-              bitField0_ |= 0x00000040;
-              infoServerPort_ = input.readUInt32();
+              timeStampsOfLastAppliedOp_ = input.readUInt64();
               break;
             }
           }
@@ -4796,1957 +4530,4480 @@ public final class ClusterStatusProtos {
         throw new com.google.protobuf.InvalidProtocolBufferException(
             e.getMessage()).setUnfinishedMessage(this);
       } finally {
-        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
-          regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
-        }
-        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
-          coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
-        }
         this.unknownFields = unknownFields.build();
         makeExtensionsImmutable();
       }
     }
     public static final com.google.protobuf.Descriptors.Descriptor
         getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
     }
 
     protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
         internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_fieldAccessorTable
           .ensureFieldAccessorsInitialized(
-              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder.class);
     }
 
-    public static com.google.protobuf.Parser<ServerLoad> PARSER =
-        new com.google.protobuf.AbstractParser<ServerLoad>() {
-      public ServerLoad parsePartialFrom(
+    public static com.google.protobuf.Parser<ReplicationLoadSink> PARSER =
+        new com.google.protobuf.AbstractParser<ReplicationLoadSink>() {
+      public ReplicationLoadSink parsePartialFrom(
           com.google.protobuf.CodedInputStream input,
           com.google.protobuf.ExtensionRegistryLite extensionRegistry)
           throws com.google.protobuf.InvalidProtocolBufferException {
-        return new ServerLoad(input, extensionRegistry);
+        return new ReplicationLoadSink(input, extensionRegistry);
       }
     };
 
     @java.lang.Override
-    public com.google.protobuf.Parser<ServerLoad> getParserForType() {
+    public com.google.protobuf.Parser<ReplicationLoadSink> getParserForType() {
       return PARSER;
     }
 
     private int bitField0_;
-    // optional uint32 number_of_requests = 1;
-    public static final int NUMBER_OF_REQUESTS_FIELD_NUMBER = 1;
-    private int numberOfRequests_;
+    // required uint64 ageOfLastAppliedOp = 1;
+    public static final int AGEOFLASTAPPLIEDOP_FIELD_NUMBER = 1;
+    private long ageOfLastAppliedOp_;
     /**
-     * <code>optional uint32 number_of_requests = 1;</code>
-     *
-     * <pre>
-     ** Number of requests since last report. 
-     * </pre>
+     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
      */
-    public boolean hasNumberOfRequests() {
+    public boolean hasAgeOfLastAppliedOp() {
       return ((bitField0_ & 0x00000001) == 0x00000001);
     }
     /**
-     * <code>optional uint32 number_of_requests = 1;</code>
-     *
-     * <pre>
-     ** Number of requests since last report. 
-     * </pre>
+     * <code>required uint64 ageOfLastAppliedOp = 1;</code>
      */
-    public int getNumberOfRequests() {
-      return numberOfRequests_;
+    public long getAgeOfLastAppliedOp() {
+      return ageOfLastAppliedOp_;
     }
 
-    // optional uint32 total_number_of_requests = 2;
-    public static final int TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER = 2;
-    private int totalNumberOfRequests_;
+    // required uint64 timeStampsOfLastAppliedOp = 2;
+    public static final int TIMESTAMPSOFLASTAPPLIEDOP_FIELD_NUMBER = 2;
+    private long timeStampsOfLastAppliedOp_;
     /**
-     * <code>optional uint32 total_number_of_requests = 2;</code>
-     *
-     * <pre>
-     ** Total Number of requests from the start of the region server. 
-     * </pre>
+     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
      */
-    public boolean hasTotalNumberOfRequests() {
+    public boolean hasTimeStampsOfLastAppliedOp() {
       return ((bitField0_ & 0x00000002) == 0x00000002);
     }
     /**
-     * <code>optional uint32 total_number_of_requests = 2;</code>
-     *
-     * <pre>
-     ** Total Number of requests from the start of the region server. 
-     * </pre>
+     * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
      */
-    public int getTotalNumberOfRequests() {
-      return totalNumberOfRequests_;
+    public long getTimeStampsOfLastAppliedOp() {
+      return timeStampsOfLastAppliedOp_;
     }
 
-    // optional uint32 used_heap_MB = 3;
-    public static final int USED_HEAP_MB_FIELD_NUMBER = 3;
-    private int usedHeapMB_;
-    /**
-     * <code>optional uint32 used_heap_MB = 3;</code>
-     *
-     * <pre>
-     ** the amount of used heap, in MB. 
-     * </pre>
-     */
-    public boolean hasUsedHeapMB() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+    private void initFields() {
+      ageOfLastAppliedOp_ = 0L;
+      timeStampsOfLastAppliedOp_ = 0L;
     }
-    /**
-     * <code>optional uint32 used_heap_MB = 3;</code>
-     *
-     * <pre>
-     ** the amount of used heap, in MB. 
-     * </pre>
-     */
-    public int getUsedHeapMB() {
-      return usedHeapMB_;
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+
+      if (!hasAgeOfLastAppliedOp()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasTimeStampsOfLastAppliedOp()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
     }
 
-    // optional uint32 max_heap_MB = 4;
-    public static final int MAX_HEAP_MB_FIELD_NUMBER = 4;
-    private int maxHeapMB_;
-    /**
-     * <code>optional uint32 max_heap_MB = 4;</code>
-     *
-     * <pre>
-     ** the maximum allowable size of the heap, in MB. 
-     * </pre>
-     */
-    public boolean hasMaxHeapMB() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeUInt64(1, ageOfLastAppliedOp_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeUInt64(2, timeStampsOfLastAppliedOp_);
+      }
+      getUnknownFields().writeTo(output);
     }
-    /**
-     * <code>optional uint32 max_heap_MB = 4;</code>
-     *
-     * <pre>
-     ** the maximum allowable size of the heap, in MB. 
-     * </pre>
-     */
-    public int getMaxHeapMB() {
-      return maxHeapMB_;
+
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(1, ageOfLastAppliedOp_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(2, timeStampsOfLastAppliedOp_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
     }
 
-    // repeated .RegionLoad region_loads = 5;
-    public static final int REGION_LOADS_FIELD_NUMBER = 5;
-    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_;
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
-      return regionLoads_;
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
     }
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
-        getRegionLoadsOrBuilderList() {
-      return regionLoads_;
+
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) obj;
+
+      boolean result = true;
+      result = result && (hasAgeOfLastAppliedOp() == other.hasAgeOfLastAppliedOp());
+      if (hasAgeOfLastAppliedOp()) {
+        result = result && (getAgeOfLastAppliedOp()
+            == other.getAgeOfLastAppliedOp());
+      }
+      result = result && (hasTimeStampsOfLastAppliedOp() == other.hasTimeStampsOfLastAppliedOp());
+      if (hasTimeStampsOfLastAppliedOp()) {
+        result = result && (getTimeStampsOfLastAppliedOp()
+            == other.getTimeStampsOfLastAppliedOp());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
     }
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    public int getRegionLoadsCount() {
-      return regionLoads_.size();
+
+    private int memoizedHashCode = 0;
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasAgeOfLastAppliedOp()) {
+        hash = (37 * hash) + AGEOFLASTAPPLIEDOP_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getAgeOfLastAppliedOp());
+      }
+      if (hasTimeStampsOfLastAppliedOp()) {
+        hash = (37 * hash) + TIMESTAMPSOFLASTAPPLIEDOP_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getTimeStampsOfLastAppliedOp());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
     }
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
-      return regionLoads_.get(index);
+
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
     }
-    /**
-     * <code>repeated .RegionLoad region_loads = 5;</code>
-     *
-     * <pre>
-     ** Information on the load of individual regions. 
-     * </pre>
-     */
-    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
-        int index) {
-      return regionLoads_.get(index);
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
     }
-
-    // repeated .Coprocessor coprocessors = 6;
-    public static final int COPROCESSORS_FIELD_NUMBER = 6;
-    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_;
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
-      return coprocessors_;
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
     }
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
-        getCoprocessorsOrBuilderList() {
-      return coprocessors_;
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
     }
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    public int getCoprocessorsCount() {
-      return coprocessors_.size();
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
     }
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
-      return coprocessors_.get(index);
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    /**
-     * <code>repeated .Coprocessor coprocessors = 6;</code>
-     *
-     * <pre>
-     **
-     * Regionserver-level coprocessors, e.g., WALObserver implementations.
-     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-     * objects.
-     * </pre>
-     */
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
-        int index) {
-      return coprocessors_.get(index);
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input);
     }
-
-    // optional uint64 report_start_time = 7;
-    public static final int REPORT_START_TIME_FIELD_NUMBER = 7;
-    private long reportStartTime_;
-    /**
-     * <code>optional uint64 report_start_time = 7;</code>
-     *
-     * <pre>
-     **
-     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    public boolean hasReportStartTime() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
     }
-    /**
-     * <code>optional uint64 report_start_time = 7;</code>
-     *
-     * <pre>
-     **
-     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    public long getReportStartTime() {
-      return reportStartTime_;
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
     }
-
-    // optional uint64 report_end_time = 8;
-    public static final int REPORT_END_TIME_FIELD_NUMBER = 8;
-    private long reportEndTime_;
-    /**
-     * <code>optional uint64 report_end_time = 8;</code>
-     *
-     * <pre>
-     **
-     * Time when report was generated.
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    public boolean hasReportEndTime() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
     }
-    /**
-     * <code>optional uint64 report_end_time = 8;</code>
-     *
-     * <pre>
-     **
-     * Time when report was generated.
-     * time is measured as the difference, measured in milliseconds, between the current time
-     * and midnight, January 1, 1970 UTC.
-     * </pre>
-     */
-    public long getReportEndTime() {
-      return reportEndTime_;
+
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink prototype) {
+      return newBuilder().mergeFrom(prototype);
     }
+    public Builder toBuilder() { return newBuilder(this); }
 
-    // optional uint32 info_server_port = 9;
-    public static final int INFO_SERVER_PORT_FIELD_NUMBER = 9;
-    private int infoServerPort_;
-    /**
-     * <code>optional uint32 info_server_port = 9;</code>
-     *
-     * <pre>
-     **
-     * The port number that this region server is hosing an info server on.
-     * </pre>
-     */
-    public boolean hasInfoServerPort() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
     }
     /**
-     * <code>optional uint32 info_server_port = 9;</code>
-     *
-     * <pre>
-     **
-     * The port number that this region server is hosing an info server on.
-     * </pre>
+     * Protobuf type {@code ReplicationLoadSink}
      */
-    public int getInfoServerPort() {
-      return infoServerPort_;
-    }
-
-    private void initFields() {
-      numberOfRequests_ = 0;
-      totalNumberOfRequests_ = 0;
-      usedHeapMB_ = 0;
-      maxHeapMB_ = 0;
-      regionLoads_ = java.util.Collections.emptyList();
-      coprocessors_ = java.util.Collections.emptyList();
-      reportStartTime_ = 0L;
-      reportEndTime_ = 0L;
-      infoServerPort_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      for (int i = 0; i < getRegionLoadsCount(); i++) {
-        if (!getRegionLoads(i).isInitialized()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
       }
-      for (int i = 0; i < getCoprocessorsCount(); i++) {
-        if (!getCoprocessors(i).isInitialized()) {
-          memoizedIsInitialized = 0;
-          return false;
-        }
+
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder.class);
       }
-      memoizedIsInitialized = 1;
-      return true;
-    }
 
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt32(1, numberOfRequests_);
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
       }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(2, totalNumberOfRequests_);
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
       }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt32(3, usedHeapMB_);
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
       }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt32(4, maxHeapMB_);
+      private static Builder create() {
+        return new Builder();
       }
-      for (int i = 0; i < regionLoads_.size(); i++) {
-        output.writeMessage(5, regionLoads_.get(i));
+
+      public Builder clear() {
+        super.clear();
+        ageOfLastAppliedOp_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        timeStampsOfLastAppliedOp_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
       }
-      for (int i = 0; i < coprocessors_.size(); i++) {
-        output.writeMessage(6, coprocessors_.get(i));
+
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
       }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeUInt64(7, reportStartTime_);
+
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSink_descriptor;
       }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeUInt64(8, reportEndTime_);
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
       }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        output.writeUInt32(9, infoServerPort_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
 
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(1, numberOfRequests_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(2, totalNumberOfRequests_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(3, usedHeapMB_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(4, maxHeapMB_);
-      }
-      for (int i = 0; i < regionLoads_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(5, regionLoads_.get(i));
-      }
-      for (int i = 0; i < coprocessors_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(6, coprocessors_.get(i));
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(7, reportStartTime_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(8, reportEndTime_);
-      }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(9, infoServerPort_);
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink build() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
       }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
 
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)) {
-        return super.equals(obj);
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.ageOfLastAppliedOp_ = ageOfLastAppliedOp_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.timeStampsOfLastAppliedOp_ = timeStampsOfLastAppliedOp_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
       }
-      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) obj;
 
-      boolean result = true;
-      result = result && (hasNumberOfRequests() == other.hasNumberOfRequests());
-      if (hasNumberOfRequests()) {
-        result = result && (getNumberOfRequests()
-            == other.getNumberOfRequests());
-      }
-      result = result && (hasTotalNumberOfRequests() == other.hasTotalNumberOfRequests());
-      if (hasTotalNumberOfRequests()) {
-        result = result && (getTotalNumberOfRequests()
-            == other.getTotalNumberOfRequests());
-      }
-      result = result && (hasUsedHeapMB() == other.hasUsedHeapMB());
-      if (hasUsedHeapMB()) {
-        result = result && (getUsedHeapMB()
-            == other.getUsedHeapMB());
-      }
-      result = result && (hasMaxHeapMB() == other.hasMaxHeapMB());
-      if (hasMaxHeapMB()) {
-        result = result && (getMaxHeapMB()
-            == other.getMaxHeapMB());
-      }
-      result = result && getRegionLoadsList()
-          .equals(other.getRegionLoadsList());
-      result = result && getCoprocessorsList()
-          .equals(other.getCoprocessorsList());
-      result = result && (hasReportStartTime() == other.hasReportStartTime());
-      if (hasReportStartTime()) {
-        result = result && (getReportStartTime()
-            == other.getReportStartTime());
-      }
-      result = result && (hasReportEndTime() == other.hasReportEndTime());
-      if (hasReportEndTime()) {
-        result = result && (getReportEndTime()
-            == other.getReportEndTime());
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
       }
-      result = result && (hasInfoServerPort() == other.hasInfoServerPort());
-      if (hasInfoServerPort()) {
-        result = result && (getInfoServerPort()
-            == other.getInfoServerPort());
+
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance()) return this;
+        if (other.hasAgeOfLastAppliedOp()) {
+          setAgeOfLastAppliedOp(other.getAgeOfLastAppliedOp());
+        }
+        if (other.hasTimeStampsOfLastAppliedOp()) {
+          setTimeStampsOfLastAppliedOp(other.getTimeStampsOfLastAppliedOp());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
       }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
 
-    private int memoizedHashCode = 0;
-    @java.lang.Override
-    public int hashCode() {
-      if (memoizedHashCode != 0) {
-        return memoizedHashCode;
+      public final boolean isInitialized() {
+        if (!hasAgeOfLastAppliedOp()) {
+          
+          return false;
+        }
+        if (!hasTimeStampsOfLastAppliedOp()) {
+          
+          return false;
+        }
+        return true;
       }
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasNumberOfRequests()) {
-        hash = (37 * hash) + NUMBER_OF_REQUESTS_FIELD_NUMBER;
-        hash = (53 * hash) + getNumberOfRequests();
+
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
+          }
+        }
+        return this;
       }
-      if (hasTotalNumberOfRequests()) {
-        hash = (37 * hash) + TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER;
-        hash = (53 * hash) + getTotalNumberOfRequests();
+      private int bitField0_;
+
+      // required uint64 ageOfLastAppliedOp = 1;
+      private long ageOfLastAppliedOp_ ;
+      /**
+       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
+       */
+      public boolean hasAgeOfLastAppliedOp() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
       }
-      if (hasUsedHeapMB()) {
-        hash = (37 * hash) + USED_HEAP_MB_FIELD_NUMBER;
-        hash = (53 * hash) + getUsedHeapMB();
+      /**
+       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
+       */
+      public long getAgeOfLastAppliedOp() {
+        return ageOfLastAppliedOp_;
       }
-      if (hasMaxHeapMB()) {
-        hash = (37 * hash) + MAX_HEAP_MB_FIELD_NUMBER;
-        hash = (53 * hash) + getMaxHeapMB();
+      /**
+       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
+       */
+      public Builder setAgeOfLastAppliedOp(long value) {
+        bitField0_ |= 0x00000001;
+        ageOfLastAppliedOp_ = value;
+        onChanged();
+        return this;
       }
-      if (getRegionLoadsCount() > 0) {
-        hash = (37 * hash) + REGION_LOADS_FIELD_NUMBER;
-        hash = (53 * hash) + getRegionLoadsList().hashCode();
+      /**
+       * <code>required uint64 ageOfLastAppliedOp = 1;</code>
+       */
+      public Builder clearAgeOfLastAppliedOp() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        ageOfLastAppliedOp_ = 0L;
+        onChanged();
+        return this;
       }
-      if (getCoprocessorsCount() > 0) {
-        hash = (37 * hash) + COPROCESSORS_FIELD_NUMBER;
-        hash = (53 * hash) + getCoprocessorsList().hashCode();
+
+      // required uint64 timeStampsOfLastAppliedOp = 2;
+      private long timeStampsOfLastAppliedOp_ ;
+      /**
+       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
+       */
+      public boolean hasTimeStampsOfLastAppliedOp() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
       }
-      if (hasReportStartTime()) {
-        hash = (37 * hash) + REPORT_START_TIME_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getReportStartTime());
+      /**
+       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
+       */
+      public long getTimeStampsOfLastAppliedOp() {
+        return timeStampsOfLastAppliedOp_;
       }
-      if (hasReportEndTime()) {
-        hash = (37 * hash) + REPORT_END_TIME_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getReportEndTime());
+      /**
+       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
+       */
+      public Builder setTimeStampsOfLastAppliedOp(long value) {
+        bitField0_ |= 0x00000002;
+        timeStampsOfLastAppliedOp_ = value;
+        onChanged();
+        return this;
       }
-      if (hasInfoServerPort()) {
-        hash = (37 * hash) + INFO_SERVER_PORT_FIELD_NUMBER;
-        hash = (53 * hash) + getInfoServerPort();
+      /**
+       * <code>required uint64 timeStampsOfLastAppliedOp = 2;</code>
+       */
+      public Builder clearTimeStampsOfLastAppliedOp() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        timeStampsOfLastAppliedOp_ = 0L;
+        onChanged();
+        return this;
       }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      memoizedHashCode = hash;
-      return hash;
-    }
 
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
+      // @@protoc_insertion_point(builder_scope:ReplicationLoadSink)
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
+
+    static {
+      defaultInstance = new ReplicationLoadSink(true);
+      defaultInstance.initFields();
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+
+    // @@protoc_insertion_point(class_scope:ReplicationLoadSink)
+  }
+
+  public interface ReplicationLoadSourceOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+
+    // required string peerID = 1;
+    /**
+     * <code>required string peerID = 1;</code>
+     */
+    boolean hasPeerID();
+    /**
+     * <code>required string peerID = 1;</code>
+     */
+    java.lang.String getPeerID();
+    /**
+     * <code>required string peerID = 1;</code>
+     */
+    com.google.protobuf.ByteString
+        getPeerIDBytes();
+
+    // required uint64 ageOfLastShippedOp = 2;
+    /**
+     * <code>required uint64 ageOfLastShippedOp = 2;</code>
+     */
+    boolean hasAgeOfLastShippedOp();
+    /**
+     * <code>required uint64 ageOfLastShippedOp = 2;</code>
+     */
+    long getAgeOfLastShippedOp();
+
+    // required uint32 sizeOfLogQueue = 3;
+    /**
+     * <code>required uint32 sizeOfLogQueue = 3;</code>
+     */
+    boolean hasSizeOfLogQueue();
+    /**
+     * <code>required uint32 sizeOfLogQueue = 3;</code>
+     */
+    int getSizeOfLogQueue();
+
+    // required uint64 timeStampOfLastShippedOp = 4;
+    /**
+     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+     */
+    boolean hasTimeStampOfLastShippedOp();
+    /**
+     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+     */
+    long getTimeStampOfLastShippedOp();
+
+    // required uint64 replicationLag = 5;
+    /**
+     * <code>required uint64 replicationLag = 5;</code>
+     */
+    boolean hasReplicationLag();
+    /**
+     * <code>required uint64 replicationLag = 5;</code>
+     */
+    long getReplicationLag();
+  }
+  /**
+   * Protobuf type {@code ReplicationLoadSource}
+   */
+  public static final class ReplicationLoadSource extends
+      com.google.protobuf.GeneratedMessage
+      implements ReplicationLoadSourceOrBuilder {
+    // Use ReplicationLoadSource.newBuilder() to construct.
+    private ReplicationLoadSource(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+      super(builder);
+      this.unknownFields = builder.getUnknownFields();
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
+    private ReplicationLoadSource(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
+    private static final ReplicationLoadSource defaultInstance;
+    public static ReplicationLoadSource getDefaultInstance() {
+      return defaultInstance;
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+
+    public ReplicationLoadSource getDefaultInstanceForType() {
+      return defaultInstance;
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
     }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+    private ReplicationLoadSource(
         com.google.protobuf.CodedInputStream input,
         com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              peerID_ = input.readBytes();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              ageOfLastShippedOp_ = input.readUInt64();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              sizeOfLogQueue_ = input.readUInt32();
+              break;
+            }
+            case 32: {
+              bitField0_ |= 0x00000008;
+              timeStampOfLastShippedOp_ = input.readUInt64();
+              break;
+            }
+            case 40: {
+              bitField0_ |= 0x00000010;
+              replicationLag_ = input.readUInt64();
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
     }
 
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad prototype) {
-      return newBuilder().mergeFrom(prototype);
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder.class);
     }
-    public Builder toBuilder() { return newBuilder(this); }
+
+    public static com.google.protobuf.Parser<ReplicationLoadSource> PARSER =
+        new com.google.protobuf.AbstractParser<ReplicationLoadSource>() {
+      public ReplicationLoadSource parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new ReplicationLoadSource(input, extensionRegistry);
+      }
+    };
 
     @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
+    public com.google.protobuf.Parser<ReplicationLoadSource> getParserForType() {
+      return PARSER;
     }
+
+    private int bitField0_;
+    // required string peerID = 1;
+    public static final int PEERID_FIELD_NUMBER = 1;
+    private java.lang.Object peerID_;
     /**
-     * Protobuf type {@code ServerLoad}
+     * <code>required string peerID = 1;</code>
      */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
+    public boolean hasPeerID() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    /**
+     * <code>required string peerID = 1;</code>
+     */
+    public java.lang.String getPeerID() {
+      java.lang.Object ref = peerID_;
+      if (ref instanceof java.lang.String) {
+        return (java.lang.String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        java.lang.String s = bs.toStringUtf8();
+        if (bs.isValidUtf8()) {
+          peerID_ = s;
+        }
+        return s;
       }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
+    }
+    /**
+     * <code>required string peerID = 1;</code>
+     */
+    public com.google.protobuf.ByteString
+        getPeerIDBytes() {
+      java.lang.Object ref = peerID_;
+      if (ref instanceof java.lang.String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8(
+                (java.lang.String) ref);
+        peerID_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
       }
+    }
 
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
+    // required uint64 ageOfLastShippedOp = 2;
+    public static final int AGEOFLASTSHIPPEDOP_FIELD_NUMBER = 2;
+    private long ageOfLastShippedOp_;
+    /**
+     * <code>required uint64 ageOfLastShippedOp = 2;</code>
+     */
+    public boolean hasAgeOfLastShippedOp() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    /**
+     * <code>required uint64 ageOfLastShippedOp = 2;</code>
+     */
+    public long getAgeOfLastShippedOp() {
+      return ageOfLastShippedOp_;
+    }
 
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getRegionLoadsFieldBuilder();
-          getCoprocessorsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
+    // required uint32 sizeOfLogQueue = 3;
+    public static final int SIZEOFLOGQUEUE_FIELD_NUMBER = 3;
+    private int sizeOfLogQueue_;
+    /**
+     * <code>required uint32 sizeOfLogQueue = 3;</code>
+     */
+    public boolean hasSizeOfLogQueue() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    /**
+     * <code>required uint32 sizeOfLogQueue = 3;</code>
+     */
+    public int getSizeOfLogQueue() {
+      return sizeOfLogQueue_;
+    }
 
-      public Builder clear() {
-        super.clear();
-        numberOfRequests_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        totalNumberOfRequests_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        usedHeapMB_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        maxHeapMB_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000008);
-        if (regionLoadsBuilder_ == null) {
-          regionLoads_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000010);
-        } else {
-          regionLoadsBuilder_.clear();
-        }
-        if (coprocessorsBuilder_ == null) {
-          coprocessors_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000020);
-        } else {
-          coprocessorsBuilder_.clear();
-        }
-        reportStartTime_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000040);
-        reportEndTime_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000080);
-        infoServerPort_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000100);
-        return this;
-      }
+    // required uint64 timeStampOfLastShippedOp = 4;
+    public static final int TIMESTAMPOFLASTSHIPPEDOP_FIELD_NUMBER = 4;
+    private long timeStampOfLastShippedOp_;
+    /**
+     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+     */
+    public boolean hasTimeStampOfLastShippedOp() {
+      return ((bitField0_ & 0x00000008) == 0x00000008);
+    }
+    /**
+     * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+     */
+    public long getTimeStampOfLastShippedOp() {
+      return timeStampOfLastShippedOp_;
+    }
 
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
+    // required uint64 replicationLag = 5;
+    public static final int REPLICATIONLAG_FIELD_NUMBER = 5;
+    private long replicationLag_;
+    /**
+     * <code>required uint64 replicationLag = 5;</code>
+     */
+    public boolean hasReplicationLag() {
+      return ((bitField0_ & 0x00000010) == 0x00000010);
+    }
+    /**
+     * <code>required uint64 replicationLag = 5;</code>
+     */
+    public long getReplicationLag() {
+      return replicationLag_;
+    }
 
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
-      }
+    private void initFields() {
+      peerID_ = "";
+      ageOfLastShippedOp_ = 0L;
+      sizeOfLogQueue_ = 0;
+      timeStampOfLastShippedOp_ = 0L;
+      replicationLag_ = 0L;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
 
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
+      if (!hasPeerID()) {
+        memoizedIsInitialized = 0;
+        return false;
       }
-
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = buildPartial();
+      if (!hasAgeOfLastShippedOp()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasSizeOfLogQueue()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasTimeStampOfLastShippedOp()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasReplicationLag()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getPeerIDBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeUInt64(2, ageOfLastShippedOp_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeUInt32(3, sizeOfLogQueue_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        output.writeUInt64(4, timeStampOfLastShippedOp_);
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        output.writeUInt64(5, replicationLag_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getPeerIDBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(2, ageOfLastShippedOp_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(3, sizeOfLogQueue_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(4, timeStampOfLastShippedOp_);
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(5, replicationLag_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) obj;
+
+      boolean result = true;
+      result = result && (hasPeerID() == other.hasPeerID());
+      if (hasPeerID()) {
+        result = result && getPeerID()
+            .equals(other.getPeerID());
+      }
+      result = result && (hasAgeOfLastShippedOp() == other.hasAgeOfLastShippedOp());
+      if (hasAgeOfLastShippedOp()) {
+        result = result && (getAgeOfLastShippedOp()
+            == other.getAgeOfLastShippedOp());
+      }
+      result = result && (hasSizeOfLogQueue() == other.hasSizeOfLogQueue());
+      if (hasSizeOfLogQueue()) {
+        result = result && (getSizeOfLogQueue()
+            == other.getSizeOfLogQueue());
+      }
+      result = result && (hasTimeStampOfLastShippedOp() == other.hasTimeStampOfLastShippedOp());
+      if (hasTimeStampOfLastShippedOp()) {
+        result = result && (getTimeStampOfLastShippedOp()
+            == other.getTimeStampOfLastShippedOp());
+      }
+      result = result && (hasReplicationLag() == other.hasReplicationLag());
+      if (hasReplicationLag()) {
+        result = result && (getReplicationLag()
+            == other.getReplicationLag());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+
+    private int memoizedHashCode = 0;
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasPeerID()) {
+        hash = (37 * hash) + PEERID_FIELD_NUMBER;
+        hash = (53 * hash) + getPeerID().hashCode();
+      }
+      if (hasAgeOfLastShippedOp()) {
+        hash = (37 * hash) + AGEOFLASTSHIPPEDOP_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getAgeOfLastShippedOp());
+      }
+      if (hasSizeOfLogQueue()) {
+        hash = (37 * hash) + SIZEOFLOGQUEUE_FIELD_NUMBER;
+        hash = (53 * hash) + getSizeOfLogQueue();
+      }
+      if (hasTimeStampOfLastShippedOp()) {
+        hash = (37 * hash) + TIMESTAMPOFLASTSHIPPEDOP_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getTimeStampOfLastShippedOp());
+      }
+      if (hasReplicationLag()) {
+        hash = (37 * hash) + REPLICATIONLAG_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getReplicationLag());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
+    }
+
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    /**
+     * Protobuf type {@code ReplicationLoadSource}
+     */
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
+      }
+
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder.class);
+      }
+
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+
+      public Builder clear() {
+        super.clear();
+        peerID_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        ageOfLastShippedOp_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        sizeOfLogQueue_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000004);
+        timeStampOfLastShippedOp_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000008);
+        replicationLag_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000010);
+        return this;
+      }
+
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ReplicationLoadSource_descriptor;
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance();
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource build() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource result = buildPartial();
         if (!result.isInitialized()) {
           throw newUninitializedMessageException(result);
         }
         return result;
       }
 
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad(this);
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource(this);
         int from_bitField0_ = bitField0_;
         int to_bitField0_ = 0;
         if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
           to_bitField0_ |= 0x00000001;
         }
-        result.numberOfRequests_ = numberOfRequests_;
+        result.peerID_ = peerID_;
         if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
           to_bitField0_ |= 0x00000002;
         }
-        result.totalNumberOfRequests_ = totalNumberOfRequests_;
+        result.ageOfLastShippedOp_ = ageOfLastShippedOp_;
         if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
           to_bitField0_ |= 0x00000004;
         }
-        result.usedHeapMB_ = usedHeapMB_;
+        result.sizeOfLogQueue_ = sizeOfLogQueue_;
         if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
           to_bitField0_ |= 0x00000008;
         }
-        result.maxHeapMB_ = maxHeapMB_;
+        result.timeStampOfLastShippedOp_ = timeStampOfLastShippedOp_;
+        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+          to_bitField0_ |= 0x00000010;
+        }
+        result.replicationLag_ = replicationLag_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance()) return this;
+        if (other.hasPeerID()) {
+          bitField0_ |= 0x00000001;
+          peerID_ = other.peerID_;
+          onChanged();
+        }
+        if (other.hasAgeOfLastShippedOp()) {
+          setAgeOfLastShippedOp(other.getAgeOfLastShippedOp());
+        }
+        if (other.hasSizeOfLogQueue()) {
+          setSizeOfLogQueue(other.getSizeOfLogQueue());
+        }
+        if (other.hasTimeStampOfLastShippedOp()) {
+          setTimeStampOfLastShippedOp(other.getTimeStampOfLastShippedOp());
+        }
+        if (other.hasReplicationLag()) {
+          setReplicationLag(other.getReplicationLag());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+
+      public final boolean isInitialized() {
+        if (!hasPeerID()) {
+          
+          return false;
+        }
+        if (!hasAgeOfLastShippedOp()) {
+          
+          return false;
+        }
+        if (!hasSizeOfLogQueue()) {
+          
+          return false;
+        }
+        if (!hasTimeStampOfLastShippedOp()) {
+          
+          return false;
+        }
+        if (!hasReplicationLag()) {
+          
+          return false;
+        }
+        return true;
+      }
+
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
+          }
+        }
+        return this;
+      }
+      private int bitField0_;
+
+      // required string peerID = 1;
+      private java.lang.Object peerID_ = "";
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public boolean hasPeerID() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public java.lang.String getPeerID() {
+        java.lang.Object ref = peerID_;
+        if (!(ref instanceof java.lang.String)) {
+          java.lang.String s = ((com.google.protobuf.ByteString) ref)
+              .toStringUtf8();
+          peerID_ = s;
+          return s;
+        } else {
+          return (java.lang.String) ref;
+        }
+      }
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public com.google.protobuf.ByteString
+          getPeerIDBytes() {
+        java.lang.Object ref = peerID_;
+        if (ref instanceof String) {
+          com.google.protobuf.ByteString b = 
+              com.google.protobuf.ByteString.copyFromUtf8(
+                  (java.lang.String) ref);
+          peerID_ = b;
+          return b;
+        } else {
+          return (com.google.protobuf.ByteString) ref;
+        }
+      }
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public Builder setPeerID(
+          java.lang.String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        peerID_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public Builder clearPeerID() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        peerID_ = getDefaultInstance().getPeerID();
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required string peerID = 1;</code>
+       */
+      public Builder setPeerIDBytes(
+          com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        peerID_ = value;
+        onChanged();
+        return this;
+      }
+
+      // required uint64 ageOfLastShippedOp = 2;
+      private long ageOfLastShippedOp_ ;
+      /**
+       * <code>required uint64 ageOfLastShippedOp = 2;</code>
+       */
+      public boolean hasAgeOfLastShippedOp() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      /**
+       * <code>required uint64 ageOfLastShippedOp = 2;</code>
+       */
+      public long getAgeOfLastShippedOp() {
+        return ageOfLastShippedOp_;
+      }
+      /**
+       * <code>required uint64 ageOfLastShippedOp = 2;</code>
+       */
+      public Builder setAgeOfLastShippedOp(long value) {
+        bitField0_ |= 0x00000002;
+        ageOfLastShippedOp_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required uint64 ageOfLastShippedOp = 2;</code>
+       */
+      public Builder clearAgeOfLastShippedOp() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        ageOfLastShippedOp_ = 0L;
+        onChanged();
+        return this;
+      }
+
+      // required uint32 sizeOfLogQueue = 3;
+      private int sizeOfLogQueue_ ;
+      /**
+       * <code>required uint32 sizeOfLogQueue = 3;</code>
+       */
+      public boolean hasSizeOfLogQueue() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      /**
+       * <code>required uint32 sizeOfLogQueue = 3;</code>
+       */
+      public int getSizeOfLogQueue() {
+        return sizeOfLogQueue_;
+      }
+      /**
+       * <code>required uint32 sizeOfLogQueue = 3;</code>
+       */
+      public Builder setSizeOfLogQueue(int value) {
+        bitField0_ |= 0x00000004;
+        sizeOfLogQueue_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required uint32 sizeOfLogQueue = 3;</code>
+       */
+      public Builder clearSizeOfLogQueue() {
+        bitField0_ = (bitField0_ & ~0x00000004);
+        sizeOfLogQueue_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // required uint64 timeStampOfLastShippedOp = 4;
+      private long timeStampOfLastShippedOp_ ;
+      /**
+       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+       */
+      public boolean hasTimeStampOfLastShippedOp() {
+        return ((bitField0_ & 0x00000008) == 0x00000008);
+      }
+      /**
+       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+       */
+      public long getTimeStampOfLastShippedOp() {
+        return timeStampOfLastShippedOp_;
+      }
+      /**
+       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+       */
+      public Builder setTimeStampOfLastShippedOp(long value) {
+        bitField0_ |= 0x00000008;
+        timeStampOfLastShippedOp_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required uint64 timeStampOfLastShippedOp = 4;</code>
+       */
+      public Builder clearTimeStampOfLastShippedOp() {
+        bitField0_ = (bitField0_ & ~0x00000008);
+        timeStampOfLastShippedOp_ = 0L;
+        onChanged();
+        return this;
+      }
+
+      // required uint64 replicationLag = 5;
+      private long replicationLag_ ;
+      /**
+       * <code>required uint64 replicationLag = 5;</code>
+       */
+      public boolean hasReplicationLag() {
+        return ((bitField0_ & 0x00000010) == 0x00000010);
+      }
+      /**
+       * <code>required uint64 replicationLag = 5;</code>
+       */
+      public long getReplicationLag() {
+        return replicationLag_;
+      }
+      /**
+       * <code>required uint64 replicationLag = 5;</code>
+       */
+      public Builder setReplicationLag(long value) {
+        bitField0_ |= 0x00000010;
+        replicationLag_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>required uint64 replicationLag = 5;</code>
+       */
+      public Builder clearReplicationLag() {
+        bitField0_ = (bitField0_ & ~0x00000010);
+        replicationLag_ = 0L;
+        onChanged();
+        return this;
+      }
+
+      // @@protoc_insertion_point(builder_scope:ReplicationLoadSource)
+    }
+
+    static {
+      defaultInstance = new ReplicationLoadSource(true);
+      defaultInstance.initFields();
+    }
+
+    // @@protoc_insertion_point(class_scope:ReplicationLoadSource)
+  }
+
+  public interface ServerLoadOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+
+    // optional uint32 number_of_requests = 1;
+    /**
+     * <code>optional uint32 number_of_requests = 1;</code>
+     *
+     * <pre>
+     ** Number of requests since last report. 
+     * </pre>
+     */
+    boolean hasNumberOfRequests();
+    /**
+     * <code>optional uint32 number_of_requests = 1;</code>
+     *
+     * <pre>
+     ** Number of requests since last report. 
+     * </pre>
+     */
+    int getNumberOfRequests();
+
+    // optional uint32 total_number_of_requests = 2;
+    /**
+     * <code>optional uint32 total_number_of_requests = 2;</code>
+     *
+     * <pre>
+     ** Total Number of requests from the start of the region server. 
+     * </pre>
+     */
+    boolean hasTotalNumberOfRequests();
+    /**
+     * <code>optional uint32 total_number_of_requests = 2;</code>
+     *
+     * <pre>
+     ** Total Number of requests from the start of the region server. 
+     * </pre>
+     */
+    int getTotalNumberOfRequests();
+
+    // optional uint32 used_heap_MB = 3;
+    /**
+     * <code>optional uint32 used_heap_MB = 3;</code>
+     *
+     * <pre>
+     ** the amount of used heap, in MB. 
+     * </pre>
+     */
+    boolean hasUsedHeapMB();
+    /**
+     * <code>optional uint32 used_heap_MB = 3;</code>
+     *
+     * <pre>
+     ** the amount of used heap, in MB. 
+     * </pre>
+     */
+    int getUsedHeapMB();
+
+    // optional uint32 max_heap_MB = 4;
+    /**
+     * <code>optional uint32 max_heap_MB = 4;</code>
+     *
+     * <pre>
+     ** the maximum allowable size of the heap, in MB. 
+     * </pre>
+     */
+    boolean hasMaxHeapMB();
+    /**
+     * <code>optional uint32 max_heap_MB = 4;</code>
+     *
+     * <pre>
+     ** the maximum allowable size of the heap, in MB. 
+     * </pre>
+     */
+    int getMaxHeapMB();
+
+    // repeated .RegionLoad region_loads = 5;
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> 
+        getRegionLoadsList();
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index);
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    int getRegionLoadsCount();
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
+        getRegionLoadsOrBuilderList();
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
+        int index);
+
+    // repeated .Coprocessor coprocessors = 6;
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> 
+        getCoprocessorsList();
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index);
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    int getCoprocessorsCount();
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
+        getCoprocessorsOrBuilderList();
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
+        int index);
+
+    // optional uint64 report_start_time = 7;
+    /**
+     * <code>optional uint64 report_start_time = 7;</code>
+     *
+     * <pre>
+     **
+     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    boolean hasReportStartTime();
+    /**
+     * <code>optional uint64 report_start_time = 7;</code>
+     *
+     * <pre>
+     **
+     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    long getReportStartTime();
+
+    // optional uint64 report_end_time = 8;
+    /**
+     * <code>optional uint64 report_end_time = 8;</code>
+     *
+     * <pre>
+     **
+     * Time when report was generated.
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    boolean hasReportEndTime();
+    /**
+     * <code>optional uint64 report_end_time = 8;</code>
+     *
+     * <pre>
+     **
+     * Time when report was generated.
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    long getReportEndTime();
+
+    // optional uint32 info_server_port = 9;
+    /**
+     * <code>optional uint32 info_server_port = 9;</code>
+     *
+     * <pre>
+     **
+     * The port number that this region server is hosing an info server on.
+     * </pre>
+     */
+    boolean hasInfoServerPort();
+    /**
+     * <code>optional uint32 info_server_port = 9;</code>
+     *
+     * <pre>
+     **
+     * The port number that this region server is hosing an info server on.
+     * </pre>
+     */
+    int getInfoServerPort();
+
+    // repeated .ReplicationLoadSource replLoadSource = 10;
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> 
+        getReplLoadSourceList();
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index);
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    int getReplLoadSourceCount();
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
+        getReplLoadSourceOrBuilderList();
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
+        int index);
+
+    // optional .ReplicationLoadSink replLoadSink = 11;
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    boolean hasReplLoadSink();
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink();
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder();
+  }
+  /**
+   * Protobuf type {@code ServerLoad}
+   */
+  public static final class ServerLoad extends
+      com.google.protobuf.GeneratedMessage
+      implements ServerLoadOrBuilder {
+    // Use ServerLoad.newBuilder() to construct.
+    private ServerLoad(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+      super(builder);
+      this.unknownFields = builder.getUnknownFields();
+    }
+    private ServerLoad(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
+    private static final ServerLoad defaultInstance;
+    public static ServerLoad getDefaultInstance() {
+      return defaultInstance;
+    }
+
+    public ServerLoad getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private ServerLoad(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      int mutable_bitField0_ = 0;
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              numberOfRequests_ = input.readUInt32();
+              break;
+            }
+            case 16: {
+              bitField0_ |= 0x00000002;
+              totalNumberOfRequests_ = input.readUInt32();
+              break;
+            }
+            case 24: {
+              bitField0_ |= 0x00000004;
+              usedHeapMB_ = input.readUInt32();
+              break;
+            }
+            case 32: {
+              bitField0_ |= 0x00000008;
+              maxHeapMB_ = input.readUInt32();
+              break;
+            }
+            case 42: {
+              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
+                regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>();
+                mutable_bitField0_ |= 0x00000010;
+              }
+              regionLoads_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.PARSER, extensionRegistry));
+              break;
+            }
+            case 50: {
+              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
+                coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>();
+                mutable_bitField0_ |= 0x00000020;
+              }
+              coprocessors_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.PARSER, extensionRegistry));
+              break;
+            }
+            case 56: {
+              bitField0_ |= 0x00000010;
+              reportStartTime_ = input.readUInt64();
+              break;
+            }
+            case 64: {
+              bitField0_ |= 0x00000020;
+              reportEndTime_ = input.readUInt64();
+              break;
+            }
+            case 72: {
+              bitField0_ |= 0x00000040;
+              infoServerPort_ = input.readUInt32();
+              break;
+            }
+            case 82: {
+              if (!((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
+                replLoadSource_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource>();
+                mutable_bitField0_ |= 0x00000200;
+              }
+              replLoadSource_.add(input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.PARSER, extensionRegistry));
+              break;
+            }
+            case 90: {
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder subBuilder = null;
+              if (((bitField0_ & 0x00000080) == 0x00000080)) {
+                subBuilder = replLoadSink_.toBuilder();
+              }
+              replLoadSink_ = input.readMessage(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.PARSER, extensionRegistry);
+              if (subBuilder != null) {
+                subBuilder.mergeFrom(replLoadSink_);
+                replLoadSink_ = subBuilder.buildPartial();
+              }
+              bitField0_ |= 0x00000080;
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
+          regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
+        }
+        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
+          coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
+        }
+        if (((mutable_bitField0_ & 0x00000200) == 0x00000200)) {
+          replLoadSource_ = java.util.Collections.unmodifiableList(replLoadSource_);
+        }
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
+    }
+
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<ServerLoad> PARSER =
+        new com.google.protobuf.AbstractParser<ServerLoad>() {
+      public ServerLoad parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new ServerLoad(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<ServerLoad> getParserForType() {
+      return PARSER;
+    }
+
+    private int bitField0_;
+    // optional uint32 number_of_requests = 1;
+    public static final int NUMBER_OF_REQUESTS_FIELD_NUMBER = 1;
+    private int numberOfRequests_;
+    /**
+     * <code>optional uint32 number_of_requests = 1;</code>
+     *
+     * <pre>
+     ** Number of requests since last report. 
+     * </pre>
+     */
+    public boolean hasNumberOfRequests() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    /**
+     * <code>optional uint32 number_of_requests = 1;</code>
+     *
+     * <pre>
+     ** Number of requests since last report. 
+     * </pre>
+     */
+    public int getNumberOfRequests() {
+      return numberOfRequests_;
+    }
+
+    // optional uint32 total_number_of_requests = 2;
+    public static final int TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER = 2;
+    private int totalNumberOfRequests_;
+    /**
+     * <code>optional uint32 total_number_of_requests = 2;</code>
+     *
+     * <pre>
+     ** Total Number of requests from the start of the region server. 
+     * </pre>
+     */
+    public boolean hasTotalNumberOfRequests() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    /**
+     * <code>optional uint32 total_number_of_requests = 2;</code>
+     *
+     * <pre>
+     ** Total Number of requests from the start of the region server. 
+     * </pre>
+     */
+    public int getTotalNumberOfRequests() {
+      return totalNumberOfRequests_;
+    }
+
+    // optional uint32 used_heap_MB = 3;
+    public static final int USED_HEAP_MB_FIELD_NUMBER = 3;
+    private int usedHeapMB_;
+    /**
+     * <code>optional uint32 used_heap_MB = 3;</code>
+     *
+     * <pre>
+     ** the amount of used heap, in MB. 
+     * </pre>
+     */
+    public boolean hasUsedHeapMB() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    /**
+     * <code>optional uint32 used_heap_MB = 3;</code>
+     *
+     * <pre>
+     ** the amount of used heap, in MB. 
+     * </pre>
+     */
+    public int getUsedHeapMB() {
+      return usedHeapMB_;
+    }
+
+    // optional uint32 max_heap_MB = 4;
+    public static final int MAX_HEAP_MB_FIELD_NUMBER = 4;
+    private int maxHeapMB_;
+    /**
+     * <code>optional uint32 max_heap_MB = 4;</code>
+     *
+     * <pre>
+     ** the maximum allowable size of the heap, in MB. 
+     * </pre>
+     */
+    public boolean hasMaxHeapMB() {
+      return ((bitField0_ & 0x00000008) == 0x00000008);
+    }
+    /**
+     * <code>optional uint32 max_heap_MB = 4;</code>
+     *
+     * <pre>
+     ** the maximum allowable size of the heap, in MB. 
+     * </pre>
+     */
+    public int getMaxHeapMB() {
+      return maxHeapMB_;
+    }
+
+    // repeated .RegionLoad region_loads = 5;
+    public static final int REGION_LOADS_FIELD_NUMBER = 5;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_;
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
+      return regionLoads_;
+    }
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
+        getRegionLoadsOrBuilderList() {
+      return regionLoads_;
+    }
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    public int getRegionLoadsCount() {
+      return regionLoads_.size();
+    }
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
+      return regionLoads_.get(index);
+    }
+    /**
+     * <code>repeated .RegionLoad region_loads = 5;</code>
+     *
+     * <pre>
+     ** Information on the load of individual regions. 
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
+        int index) {
+      return regionLoads_.get(index);
+    }
+
+    // repeated .Coprocessor coprocessors = 6;
+    public static final int COPROCESSORS_FIELD_NUMBER = 6;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_;
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
+      return coprocessors_;
+    }
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
+        getCoprocessorsOrBuilderList() {
+      return coprocessors_;
+    }
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    public int getCoprocessorsCount() {
+      return coprocessors_.size();
+    }
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
+      return coprocessors_.get(index);
+    }
+    /**
+     * <code>repeated .Coprocessor coprocessors = 6;</code>
+     *
+     * <pre>
+     **
+     * Regionserver-level coprocessors, e.g., WALObserver implementations.
+     * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+     * objects.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
+        int index) {
+      return coprocessors_.get(index);
+    }
+
+    // optional uint64 report_start_time = 7;
+    public static final int REPORT_START_TIME_FIELD_NUMBER = 7;
+    private long reportStartTime_;
+    /**
+     * <code>optional uint64 report_start_time = 7;</code>
+     *
+     * <pre>
+     **
+     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    public boolean hasReportStartTime() {
+      return ((bitField0_ & 0x00000010) == 0x00000010);
+    }
+    /**
+     * <code>optional uint64 report_start_time = 7;</code>
+     *
+     * <pre>
+     **
+     * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    public long getReportStartTime() {
+      return reportStartTime_;
+    }
+
+    // optional uint64 report_end_time = 8;
+    public static final int REPORT_END_TIME_FIELD_NUMBER = 8;
+    private long reportEndTime_;
+    /**
+     * <code>optional uint64 report_end_time = 8;</code>
+     *
+     * <pre>
+     **
+     * Time when report was generated.
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    public boolean hasReportEndTime() {
+      return ((bitField0_ & 0x00000020) == 0x00000020);
+    }
+    /**
+     * <code>optional uint64 report_end_time = 8;</code>
+     *
+     * <pre>
+     **
+     * Time when report was generated.
+     * time is measured as the difference, measured in milliseconds, between the current time
+     * and midnight, January 1, 1970 UTC.
+     * </pre>
+     */
+    public long getReportEndTime() {
+      return reportEndTime_;
+    }
+
+    // optional uint32 info_server_port = 9;
+    public static final int INFO_SERVER_PORT_FIELD_NUMBER = 9;
+    private int infoServerPort_;
+    /**
+     * <code>optional uint32 info_server_port = 9;</code>
+     *
+     * <pre>
+     **
+     * The port number that this region server is hosing an info server on.
+     * </pre>
+     */
+    public boolean hasInfoServerPort() {
+      return ((bitField0_ & 0x00000040) == 0x00000040);
+    }
+    /**
+     * <code>optional uint32 info_server_port = 9;</code>
+     *
+     * <pre>
+     **
+     * The port number that this region server is hosing an info server on.
+     * </pre>
+     */
+    public int getInfoServerPort() {
+      return infoServerPort_;
+    }
+
+    // repeated .ReplicationLoadSource replLoadSource = 10;
+    public static final int REPLLOADSOURCE_FIELD_NUMBER = 10;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> replLoadSource_;
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> getReplLoadSourceList() {
+      return replLoadSource_;
+    }
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
+        getReplLoadSourceOrBuilderList() {
+      return replLoadSource_;
+    }
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    public int getReplLoadSourceCount() {
+      return replLoadSource_.size();
+    }
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index) {
+      return replLoadSource_.get(index);
+    }
+    /**
+     * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSource for the replication Source status of this region server.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
+        int index) {
+      return replLoadSource_.get(index);
+    }
+
+    // optional .ReplicationLoadSink replLoadSink = 11;
+    public static final int REPLLOADSINK_FIELD_NUMBER = 11;
+    private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink replLoadSink_;
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    public boolean hasReplLoadSink() {
+      return ((bitField0_ & 0x00000080) == 0x00000080);
+    }
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink() {
+      return replLoadSink_;
+    }
+    /**
+     * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
+     *
+     * <pre>
+     **
+     * The replicationLoadSink for the replication Sink status of this region server.
+     * </pre>
+     */
+    public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder() {
+      return replLoadSink_;
+    }
+
+    private void initFields() {
+      numberOfRequests_ = 0;
+      totalNumberOfRequests_ = 0;
+      usedHeapMB_ = 0;
+      maxHeapMB_ = 0;
+      regionLoads_ = java.util.Collections.emptyList();
+      coprocessors_ = java.util.Collections.emptyList();
+      reportStartTime_ = 0L;
+      reportEndTime_ = 0L;
+      infoServerPort_ = 0;
+      replLoadSource_ = java.util.Collections.emptyList();
+      replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+
+      for (int i = 0; i < getRegionLoadsCount(); i++) {
+        if (!getRegionLoads(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      for (int i = 0; i < getCoprocessorsCount(); i++) {
+        if (!getCoprocessors(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      for (int i = 0; i < getReplLoadSourceCount(); i++) {
+        if (!getReplLoadSource(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      if (hasReplLoadSink()) {
+        if (!getReplLoadSink().isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeUInt32(1, numberOfRequests_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeUInt32(2, totalNumberOfRequests_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeUInt32(3, usedHeapMB_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        output.writeUInt32(4, maxHeapMB_);
+      }
+      for (int i = 0; i < regionLoads_.size(); i++) {
+        output.writeMessage(5, regionLoads_.get(i));
+      }
+      for (int i = 0; i < coprocessors_.size(); i++) {
+        output.writeMessage(6, coprocessors_.get(i));
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        output.writeUInt64(7, reportStartTime_);
+      }
+      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+        output.writeUInt64(8, reportEndTime_);
+      }
+      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+        output.writeUInt32(9, infoServerPort_);
+      }
+      for (int i = 0; i < replLoadSource_.size(); i++) {
+        output.writeMessage(10, replLoadSource_.get(i));
+      }
+      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+        output.writeMessage(11, replLoadSink_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(1, numberOfRequests_);
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(2, totalNumberOfRequests_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(3, usedHeapMB_);
+      }
+      if (((bitField0_ & 0x00000008) == 0x00000008)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(4, maxHeapMB_);
+      }
+      for (int i = 0; i < regionLoads_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(5, regionLoads_.get(i));
+      }
+      for (int i = 0; i < coprocessors_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(6, coprocessors_.get(i));
+      }
+      if (((bitField0_ & 0x00000010) == 0x00000010)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(7, reportStartTime_);
+      }
+      if (((bitField0_ & 0x00000020) == 0x00000020)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt64Size(8, reportEndTime_);
+      }
+      if (((bitField0_ & 0x00000040) == 0x00000040)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeUInt32Size(9, infoServerPort_);
+      }
+      for (int i = 0; i < replLoadSource_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(10, replLoadSource_.get(i));
+      }
+      if (((bitField0_ & 0x00000080) == 0x00000080)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(11, replLoadSink_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) obj;
+
+      boolean result = true;
+      result = result && (hasNumberOfRequests() == other.hasNumberOfRequests());
+      if (hasNumberOfRequests()) {
+        result = result && (getNumberOfRequests()
+            == other.getNumberOfRequests());
+      }
+      result = result && (hasTotalNumberOfRequests() == other.hasTotalNumberOfRequests());
+      if (hasTotalNumberOfRequests()) {
+        result = result && (getTotalNumberOfRequests()
+            == other.getTotalNumberOfRequests());
+      }
+      result = result && (hasUsedHeapMB() == other.hasUsedHeapMB());
+      if (hasUsedHeapMB()) {
+        result = result && (getUsedHeapMB()
+            == other.getUsedHeapMB());
+      }
+      result = result && (hasMaxHeapMB() == other.hasMaxHeapMB());
+      if (hasMaxHeapMB()) {
+        result = result && (getMaxHeapMB()
+            == other.getMaxHeapMB());
+      }
+      result = result && getRegionLoadsList()
+          .equals(other.getRegionLoadsList());
+      result = result && getCoprocessorsList()
+          .equals(other.getCoprocessorsList());
+      result = result && (hasReportStartTime() == other.hasReportStartTime());
+      if (hasReportStartTime()) {
+        result = result && (getReportStartTime()
+            == other.getReportStartTime());
+      }
+      result = result && (hasReportEndTime() == other.hasReportEndTime());
+      if (hasReportEndTime()) {
+        result = result && (getReportEndTime()
+            == other.getReportEndTime());
+      }
+      result = result && (hasInfoServerPort() == other.hasInfoServerPort());
+      if (hasInfoServerPort()) {
+        result = result && (getInfoServerPort()
+            == other.getInfoServerPort());
+      }
+      result = result && getReplLoadSourceList()
+          .equals(other.getReplLoadSourceList());
+      result = result && (hasReplLoadSink() == other.hasReplLoadSink());
+      if (hasReplLoadSink()) {
+        result = result && getReplLoadSink()
+            .equals(other.getReplLoadSink());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+
+    private int memoizedHashCode = 0;
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasNumberOfRequests()) {
+        hash = (37 * hash) + NUMBER_OF_REQUESTS_FIELD_NUMBER;
+        hash = (53 * hash) + getNumberOfRequests();
+      }
+      if (hasTotalNumberOfRequests()) {
+        hash = (37 * hash) + TOTAL_NUMBER_OF_REQUESTS_FIELD_NUMBER;
+        hash = (53 * hash) + getTotalNumberOfRequests();
+      }
+      if (hasUsedHeapMB()) {
+        hash = (37 * hash) + USED_HEAP_MB_FIELD_NUMBER;
+        hash = (53 * hash) + getUsedHeapMB();
+      }
+      if (hasMaxHeapMB()) {
+        hash = (37 * hash) + MAX_HEAP_MB_FIELD_NUMBER;
+        hash = (53 * hash) + getMaxHeapMB();
+      }
+      if (getRegionLoadsCount() > 0) {
+        hash = (37 * hash) + REGION_LOADS_FIELD_NUMBER;
+        hash = (53 * hash) + getRegionLoadsList().hashCode();
+      }
+      if (getCoprocessorsCount() > 0) {
+        hash = (37 * hash) + COPROCESSORS_FIELD_NUMBER;
+        hash = (53 * hash) + getCoprocessorsList().hashCode();
+      }
+      if (hasReportStartTime()) {
+        hash = (37 * hash) + REPORT_START_TIME_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getReportStartTime());
+      }
+      if (hasReportEndTime()) {
+        hash = (37 * hash) + REPORT_END_TIME_FIELD_NUMBER;
+        hash = (53 * hash) + hashLong(getReportEndTime());
+      }
+      if (hasInfoServerPort()) {
+        hash = (37 * hash) + INFO_SERVER_PORT_FIELD_NUMBER;
+        hash = (53 * hash) + getInfoServerPort();
+      }
+      if (getReplLoadSourceCount() > 0) {
+        hash = (37 * hash) + REPLLOADSOURCE_FIELD_NUMBER;
+        hash = (53 * hash) + getReplLoadSourceList().hashCode();
+      }
+      if (hasReplLoadSink()) {
+        hash = (37 * hash) + REPLLOADSINK_FIELD_NUMBER;
+        hash = (53 * hash) + getReplLoadSink().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
+    }
+
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    /**
+     * Protobuf type {@code ServerLoad}
+     */
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoadOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
+      }
+
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.class, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.Builder.class);
+      }
+
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getRegionLoadsFieldBuilder();
+          getCoprocessorsFieldBuilder();
+          getReplLoadSourceFieldBuilder();
+          getReplLoadSinkFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+
+      public Builder clear() {
+        super.clear();
+        numberOfRequests_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        totalNumberOfRequests_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        usedHeapMB_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000004);
+        maxHeapMB_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000008);
+        if (regionLoadsBuilder_ == null) {
+          regionLoads_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000010);
+        } else {
+          regionLoadsBuilder_.clear();
+        }
+        if (coprocessorsBuilder_ == null) {
+          coprocessors_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000020);
+        } else {
+          coprocessorsBuilder_.clear();
+        }
+        reportStartTime_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000040);
+        reportEndTime_ = 0L;
+        bitField0_ = (bitField0_ & ~0x00000080);
+        infoServerPort_ = 0;
+        bitField0_ = (bitField0_ & ~0x00000100);
+        if (replLoadSourceBuilder_ == null) {
+          replLoadSource_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000200);
+        } else {
+          replLoadSourceBuilder_.clear();
+        }
+        if (replLoadSinkBuilder_ == null) {
+          replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
+        } else {
+          replLoadSinkBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000400);
+        return this;
+      }
+
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.internal_static_ServerLoad_descriptor;
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance();
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad build() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad result = new org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.numberOfRequests_ = numberOfRequests_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.totalNumberOfRequests_ = totalNumberOfRequests_;
+        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+          to_bitField0_ |= 0x00000004;
+        }
+        result.usedHeapMB_ = usedHeapMB_;
+        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
+          to_bitField0_ |= 0x00000008;
+        }
+        result.maxHeapMB_ = maxHeapMB_;
+        if (regionLoadsBuilder_ == null) {
+          if (((bitField0_ & 0x00000010) == 0x00000010)) {
+            regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
+            bitField0_ = (bitField0_ & ~0x00000010);
+          }
+          result.regionLoads_ = regionLoads_;
+        } else {
+          result.regionLoads_ = regionLoadsBuilder_.build();
+        }
+        if (coprocessorsBuilder_ == null) {
+          if (((bitField0_ & 0x00000020) == 0x00000020)) {
+            coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
+            bitField0_ = (bitField0_ & ~0x00000020);
+          }
+          result.coprocessors_ = coprocessors_;
+        } else {
+          result.coprocessors_ = coprocessorsBuilder_.build();
+        }
+        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
+          to_bitField0_ |= 0x00000010;
+        }
+        result.reportStartTime_ = reportStartTime_;
+        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
+          to_bitField0_ |= 0x00000020;
+        }
+        result.reportEndTime_ = reportEndTime_;
+        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
+          to_bitField0_ |= 0x00000040;
+        }
+        result.infoServerPort_ = infoServerPort_;
+        if (replLoadSourceBuilder_ == null) {
+          if (((bitField0_ & 0x00000200) == 0x00000200)) {
+            replLoadSource_ = java.util.Collections.unmodifiableList(replLoadSource_);
+            bitField0_ = (bitField0_ & ~0x00000200);
+          }
+          result.replLoadSource_ = replLoadSource_;
+        } else {
+          result.replLoadSource_ = replLoadSourceBuilder_.build();
+        }
+        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
+          to_bitField0_ |= 0x00000080;
+        }
+        if (replLoadSinkBuilder_ == null) {
+          result.replLoadSink_ = replLoadSink_;
+        } else {
+          result.replLoadSink_ = replLoadSinkBuilder_.build();
+        }
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance()) return this;
+        if (other.hasNumberOfRequests()) {
+          setNumberOfRequests(other.getNumberOfRequests());
+        }
+        if (other.hasTotalNumberOfRequests()) {
+          setTotalNumberOfRequests(other.getTotalNumberOfRequests());
+        }
+        if (other.hasUsedHeapMB()) {
+          setUsedHeapMB(other.getUsedHeapMB());
+        }
+        if (other.hasMaxHeapMB()) {
+          setMaxHeapMB(other.getMaxHeapMB());
+        }
+        if (regionLoadsBuilder_ == null) {
+          if (!other.regionLoads_.isEmpty()) {
+            if (regionLoads_.isEmpty()) {
+              regionLoads_ = other.regionLoads_;
+              bitField0_ = (bitField0_ & ~0x00000010);
+            } else {
+              ensureRegionLoadsIsMutable();
+              regionLoads_.addAll(other.regionLoads_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.regionLoads_.isEmpty()) {
+            if (regionLoadsBuilder_.isEmpty()) {
+              regionLoadsBuilder_.dispose();
+              regionLoadsBuilder_ = null;
+              regionLoads_ = other.regionLoads_;
+              bitField0_ = (bitField0_ & ~0x00000010);
+              regionLoadsBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getRegionLoadsFieldBuilder() : null;
+            } else {
+              regionLoadsBuilder_.addAllMessages(other.regionLoads_);
+            }
+          }
+        }
+        if (coprocessorsBuilder_ == null) {
+          if (!other.coprocessors_.isEmpty()) {
+            if (coprocessors_.isEmpty()) {
+              coprocessors_ = other.coprocessors_;
+              bitField0_ = (bitField0_ & ~0x00000020);
+            } else {
+              ensureCoprocessorsIsMutable();
+              coprocessors_.addAll(other.coprocessors_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.coprocessors_.isEmpty()) {
+            if (coprocessorsBuilder_.isEmpty()) {
+              coprocessorsBuilder_.dispose();
+              coprocessorsBuilder_ = null;
+              coprocessors_ = other.coprocessors_;
+              bitField0_ = (bitField0_ & ~0x00000020);
+              coprocessorsBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getCoprocessorsFieldBuilder() : null;
+            } else {
+              coprocessorsBuilder_.addAllMessages(other.coprocessors_);
+            }
+          }
+        }
+        if (other.hasReportStartTime()) {
+          setReportStartTime(other.getReportStartTime());
+        }
+        if (other.hasReportEndTime()) {
+          setReportEndTime(other.getReportEndTime());
+        }
+        if (other.hasInfoServerPort()) {
+          setInfoServerPort(other.getInfoServerPort());
+        }
+        if (replLoadSourceBuilder_ == null) {
+          if (!other.replLoadSource_.isEmpty()) {
+            if (replLoadSource_.isEmpty()) {
+              replLoadSource_ = other.replLoadSource_;
+              bitField0_ = (bitField0_ & ~0x00000200);
+            } else {
+              ensureReplLoadSourceIsMutable();
+              replLoadSource_.addAll(other.replLoadSource_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.replLoadSource_.isEmpty()) {
+            if (replLoadSourceBuilder_.isEmpty()) {
+              replLoadSourceBuilder_.dispose();
+              replLoadSourceBuilder_ = null;
+              replLoadSource_ = other.replLoadSource_;
+              bitField0_ = (bitField0_ & ~0x00000200);
+              replLoadSourceBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getReplLoadSourceFieldBuilder() : null;
+            } else {
+              replLoadSourceBuilder_.addAllMessages(other.replLoadSource_);
+            }
+          }
+        }
+        if (other.hasReplLoadSink()) {
+          mergeReplLoadSink(other.getReplLoadSink());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+
+      public final boolean isInitialized() {
+        for (int i = 0; i < getRegionLoadsCount(); i++) {
+          if (!getRegionLoads(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        for (int i = 0; i < getCoprocessorsCount(); i++) {
+          if (!getCoprocessors(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        for (int i = 0; i < getReplLoadSourceCount(); i++) {
+          if (!getReplLoadSource(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        if (hasReplLoadSink()) {
+          if (!getReplLoadSink().isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
+          }
+        }
+        return this;
+      }
+      private int bitField0_;
+
+      // optional uint32 number_of_requests = 1;
+      private int numberOfRequests_ ;
+      /**
+       * <code>optional uint32 number_of_requests = 1;</code>
+       *
+       * <pre>
+       ** Number of requests since last report. 
+       * </pre>
+       */
+      public boolean hasNumberOfRequests() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      /**
+       * <code>optional uint32 number_of_requests = 1;</code>
+       *
+       * <pre>
+       ** Number of requests since last report. 
+       * </pre>
+       */
+      public int getNumberOfRequests() {
+        return numberOfRequests_;
+      }
+      /**
+       * <code>optional uint32 number_of_requests = 1;</code>
+       *
+       * <pre>
+       ** Number of requests since last report. 
+       * </pre>
+       */
+      public Builder setNumberOfRequests(int value) {
+        bitField0_ |= 0x00000001;
+        numberOfRequests_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 number_of_requests = 1;</code>
+       *
+       * <pre>
+       ** Number of requests since last report. 
+       * </pre>
+       */
+      public Builder clearNumberOfRequests() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        numberOfRequests_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // optional uint32 total_number_of_requests = 2;
+      private int totalNumberOfRequests_ ;
+      /**
+       * <code>optional uint32 total_number_of_requests = 2;</code>
+       *
+       * <pre>
+       ** Total Number of requests from the start of the region server. 
+       * </pre>
+       */
+      public boolean hasTotalNumberOfRequests() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      /**
+       * <code>optional uint32 total_number_of_requests = 2;</code>
+       *
+       * <pre>
+       ** Total Number of requests from the start of the region server. 
+       * </pre>
+       */
+      public int getTotalNumberOfRequests() {
+        return totalNumberOfRequests_;
+      }
+      /**
+       * <code>optional uint32 total_number_of_requests = 2;</code>
+       *
+       * <pre>
+       ** Total Number of requests from the start of the region server. 
+       * </pre>
+       */
+      public Builder setTotalNumberOfRequests(int value) {
+        bitField0_ |= 0x00000002;
+        totalNumberOfRequests_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 total_number_of_requests = 2;</code>
+       *
+       * <pre>
+       ** Total Number of requests from the start of the region server. 
+       * </pre>
+       */
+      public Builder clearTotalNumberOfRequests() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        totalNumberOfRequests_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // optional uint32 used_heap_MB = 3;
+      private int usedHeapMB_ ;
+      /**
+       * <code>optional uint32 used_heap_MB = 3;</code>
+       *
+       * <pre>
+       ** the amount of used heap, in MB. 
+       * </pre>
+       */
+      public boolean hasUsedHeapMB() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      /**
+       * <code>optional uint32 used_heap_MB = 3;</code>
+       *
+       * <pre>
+       ** the amount of used heap, in MB. 
+       * </pre>
+       */
+      public int getUsedHeapMB() {
+        return usedHeapMB_;
+      }
+      /**
+       * <code>optional uint32 used_heap_MB = 3;</code>
+       *
+       * <pre>
+       ** the amount of used heap, in MB. 
+       * </pre>
+       */
+      public Builder setUsedHeapMB(int value) {
+        bitField0_ |= 0x00000004;
+        usedHeapMB_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 used_heap_MB = 3;</code>
+       *
+       * <pre>
+       ** the amount of used heap, in MB. 
+       * </pre>
+       */
+      public Builder clearUsedHeapMB() {
+        bitField0_ = (bitField0_ & ~0x00000004);
+        usedHeapMB_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // optional uint32 max_heap_MB = 4;
+      private int maxHeapMB_ ;
+      /**
+       * <code>optional uint32 max_heap_MB = 4;</code>
+       *
+       * <pre>
+       ** the maximum allowable size of the heap, in MB. 
+       * </pre>
+       */
+      public boolean hasMaxHeapMB() {
+        return ((bitField0_ & 0x00000008) == 0x00000008);
+      }
+      /**
+       * <code>optional uint32 max_heap_MB = 4;</code>
+       *
+       * <pre>
+       ** the maximum allowable size of the heap, in MB. 
+       * </pre>
+       */
+      public int getMaxHeapMB() {
+        return maxHeapMB_;
+      }
+      /**
+       * <code>optional uint32 max_heap_MB = 4;</code>
+       *
+       * <pre>
+       ** the maximum allowable size of the heap, in MB. 
+       * </pre>
+       */
+      public Builder setMaxHeapMB(int value) {
+        bitField0_ |= 0x00000008;
+        maxHeapMB_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional uint32 max_heap_MB = 4;</code>
+       *
+       * <pre>
+       ** the maximum allowable size of the heap, in MB. 
+       * </pre>
+       */
+      public Builder clearMaxHeapMB() {
+        bitField0_ = (bitField0_ & ~0x00000008);
+        maxHeapMB_ = 0;
+        onChanged();
+        return this;
+      }
+
+      // repeated .RegionLoad region_loads = 5;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_ =
+        java.util.Collections.emptyList();
+      private void ensureRegionLoadsIsMutable() {
+        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
+          regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>(regionLoads_);
+          bitField0_ |= 0x00000010;
+         }
+      }
+
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> regionLoadsBuilder_;
+
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
+        if (regionLoadsBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(regionLoads_);
+        } else {
+          return regionLoadsBuilder_.getMessageList();
+        }
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public int getRegionLoadsCount() {
+        if (regionLoadsBuilder_ == null) {
+          return regionLoads_.size();
+        } else {
+          return regionLoadsBuilder_.getCount();
+        }
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
+        if (regionLoadsBuilder_ == null) {
+          return regionLoads_.get(index);
+        } else {
+          return regionLoadsBuilder_.getMessage(index);
+        }
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder setRegionLoads(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
+        if (regionLoadsBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureRegionLoadsIsMutable();
+          regionLoads_.set(index, value);
+          onChanged();
+        } else {
+          regionLoadsBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder setRegionLoads(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
+        if (regionLoadsBuilder_ == null) {
+          ensureRegionLoadsIsMutable();
+          regionLoads_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          regionLoadsBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder addRegionLoads(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
         if (regionLoadsBuilder_ == null) {
-          if (((bitField0_ & 0x00000010) == 0x00000010)) {
-            regionLoads_ = java.util.Collections.unmodifiableList(regionLoads_);
-            bitField0_ = (bitField0_ & ~0x00000010);
+          if (value == null) {
+            throw new NullPointerException();
           }
-          result.regionLoads_ = regionLoads_;
+          ensureRegionLoadsIsMutable();
+          regionLoads_.add(value);
+          onChanged();
         } else {
-          result.regionLoads_ = regionLoadsBuilder_.build();
+          regionLoadsBuilder_.addMessage(value);
         }
-        if (coprocessorsBuilder_ == null) {
-          if (((bitField0_ & 0x00000020) == 0x00000020)) {
-            coprocessors_ = java.util.Collections.unmodifiableList(coprocessors_);
-            bitField0_ = (bitField0_ & ~0x00000020);
+        return this;
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder addRegionLoads(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
+        if (regionLoadsBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
           }
-          result.coprocessors_ = coprocessors_;
+          ensureRegionLoadsIsMutable();
+          regionLoads_.add(index, value);
+          onChanged();
         } else {
-          result.coprocessors_ = coprocessorsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
-          to_bitField0_ |= 0x00000010;
-        }
-        result.reportStartTime_ = reportStartTime_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000020;
-        }
-        result.reportEndTime_ = reportEndTime_;
-        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
-          to_bitField0_ |= 0x00000040;
+          regionLoadsBuilder_.addMessage(index, value);
         }
-        result.infoServerPort_ = infoServerPort_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
+        return this;
       }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad)other);
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder addRegionLoads(
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
+        if (regionLoadsBuilder_ == null) {
+          ensureRegionLoadsIsMutable();
+          regionLoads_.add(builderForValue.build());
+          onChanged();
         } else {
-          super.mergeFrom(other);
-          return this;
+          regionLoadsBuilder_.addMessage(builderForValue.build());
         }
+        return this;
       }
-
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad.getDefaultInstance()) return this;
-        if (other.hasNumberOfRequests()) {
-          setNumberOfRequests(other.getNumberOfRequests());
-        }
-        if (other.hasTotalNumberOfRequests()) {
-          setTotalNumberOfRequests(other.getTotalNumberOfRequests());
-        }
-        if (other.hasUsedHeapMB()) {
-          setUsedHeapMB(other.getUsedHeapMB());
-        }
-        if (other.hasMaxHeapMB()) {
-          setMaxHeapMB(other.getMaxHeapMB());
-        }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder addRegionLoads(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
         if (regionLoadsBuilder_ == null) {
-          if (!other.regionLoads_.isEmpty()) {
-            if (regionLoads_.isEmpty()) {
-              regionLoads_ = other.regionLoads_;
-              bitField0_ = (bitField0_ & ~0x00000010);
-            } else {
-              ensureRegionLoadsIsMutable();
-              regionLoads_.addAll(other.regionLoads_);
-            }
-            onChanged();
-          }
+          ensureRegionLoadsIsMutable();
+          regionLoads_.add(index, builderForValue.build());
+          onChanged();
         } else {
-          if (!other.regionLoads_.isEmpty()) {
-            if (regionLoadsBuilder_.isEmpty()) {
-              regionLoadsBuilder_.dispose();
-              regionLoadsBuilder_ = null;
-              regionLoads_ = other.regionLoads_;
-              bitField0_ = (bitField0_ & ~0x00000010);
-              regionLoadsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getRegionLoadsFieldBuilder() : null;
-            } else {
-              regionLoadsBuilder_.addAllMessages(other.regionLoads_);
-            }
-          }
+          regionLoadsBuilder_.addMessage(index, builderForValue.build());
         }
-        if (coprocessorsBuilder_ == null) {
-          if (!other.coprocessors_.isEmpty()) {
-            if (coprocessors_.isEmpty()) {
-              coprocessors_ = other.coprocessors_;
-              bitField0_ = (bitField0_ & ~0x00000020);
-            } else {
-              ensureCoprocessorsIsMutable();
-              coprocessors_.addAll(other.coprocessors_);
-            }
-            onChanged();
-          }
+        return this;
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder addAllRegionLoads(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> values) {
+        if (regionLoadsBuilder_ == null) {
+          ensureRegionLoadsIsMutable();
+          super.addAll(values, regionLoads_);
+          onChanged();
         } else {
-          if (!other.coprocessors_.isEmpty()) {
-            if (coprocessorsBuilder_.isEmpty()) {
-              coprocessorsBuilder_.dispose();
-              coprocessorsBuilder_ = null;
-              coprocessors_ = other.coprocessors_;
-              bitField0_ = (bitField0_ & ~0x00000020);
-              coprocessorsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getCoprocessorsFieldBuilder() : null;
-            } else {
-              coprocessorsBuilder_.addAllMessages(other.coprocessors_);
-            }
-          }
-        }
-        if (other.hasReportStartTime()) {
-          setReportStartTime(other.getReportStartTime());
-        }
-        if (other.hasReportEndTime()) {
-          setReportEndTime(other.getReportEndTime());
-        }
-        if (other.hasInfoServerPort()) {
-          setInfoServerPort(other.getInfoServerPort());
+          regionLoadsBuilder_.addAllMessages(values);
         }
-        this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
-
-      public final boolean isInitialized() {
-        for (int i = 0; i < getRegionLoadsCount(); i++) {
-          if (!getRegionLoads(i).isInitialized()) {
-            
-            return false;
-          }
-        }
-        for (int i = 0; i < getCoprocessorsCount(); i++) {
-          if (!getCoprocessors(i).isInitialized()) {
-            
-            return false;
-          }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder clearRegionLoads() {
+        if (regionLoadsBuilder_ == null) {
+          regionLoads_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000010);
+          onChanged();
+        } else {
+          regionLoadsBuilder_.clear();
         }
-        return true;
+        return this;
       }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ServerLoad) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public Builder removeRegionLoads(int index) {
+        if (regionLoadsBuilder_ == null) {
+          ensureRegionLoadsIsMutable();
+          regionLoads_.remove(index);
+          onChanged();
+        } else {
+          regionLoadsBuilder_.remove(index);
         }
         return this;
       }
-      private int bitField0_;
-
-      // optional uint32 number_of_requests = 1;
-      private int numberOfRequests_ ;
       /**
-       * <code>optional uint32 number_of_requests = 1;</code>
+       * <code>repeated .RegionLoad region_loads = 5;</code>
+       *
+       * <pre>
+       ** Information on the load of individual regions. 
+       * </pre>
+       */
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder getRegionLoadsBuilder(
+          int index) {
+        return getRegionLoadsFieldBuilder().getBuilder(index);
+      }
+      /**
+       * <code>repeated .RegionLoad region_loads = 5;</code>
        *
        * <pre>
-       ** Number of requests since last report. 
+       ** Information on the load of individual regions. 
        * </pre>
        */
-      public boolean hasNumberOfRequests() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
+          int index) {
+        if (regionLoadsBuilder_ == null) {
+          return regionLoads_.get(index);  } else {
+          return regionLoadsBuilder_.getMessageOrBuilder(index);
+        }
       }
       /**
-       * <code>optional uint32 number_of_requests = 1;</code>
+       * <code>repeated .RegionLoad region_loads = 5;</code>
        *
        * <pre>
-       ** Number of requests since last report. 
+       ** Information on the load of individual regions. 
        * </pre>
        */
-      public int getNumberOfRequests() {
-        return numberOfRequests_;
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
+           getRegionLoadsOrBuilderList() {
+        if (regionLoadsBuilder_ != null) {
+          return regionLoadsBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(regionLoads_);
+        }
       }
       /**
-       * <code>optional uint32 number_of_requests = 1;</code>
+       * <code>repeated .RegionLoad region_loads = 5;</code>
        *
        * <pre>
-       ** Number of requests since last report. 
+       ** Information on the load of individual regions. 
        * </pre>
        */
-      public Builder setNumberOfRequests(int value) {
-        bitField0_ |= 0x00000001;
-        numberOfRequests_ = value;
-        onChanged();
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder() {
+        return getRegionLoadsFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
       }
       /**
-       * <code>optional uint32 number_of_requests = 1;</code>
+       * <code>repeated .RegionLoad region_loads = 5;</code>
        *
        * <pre>
-       ** Number of requests since last report. 
+       ** Information on the load of individual regions. 
        * </pre>
        */
-      public Builder clearNumberOfRequests() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        numberOfRequests_ = 0;
-        onChanged();
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder(
+          int index) {
+        return getRegionLoadsFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
       }
-
-      // optional uint32 total_number_of_requests = 2;
-      private int totalNumberOfRequests_ ;
       /**
-       * <code>optional uint32 total_number_of_requests = 2;</code>
+       * <code>repeated .RegionLoad region_loads = 5;</code>
        *
        * <pre>
-       ** Total Number of requests from the start of the region server. 
+       ** Information on the load of individual regions. 
        * </pre>
        */
-      public boolean hasTotalNumberOfRequests() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder> 
+           getRegionLoadsBuilderList() {
+        return getRegionLoadsFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
+          getRegionLoadsFieldBuilder() {
+        if (regionLoadsBuilder_ == null) {
+          regionLoadsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder>(
+                  regionLoads_,
+                  ((bitField0_ & 0x00000010) == 0x00000010),
+                  getParentForChildren(),
+                  isClean());
+          regionLoads_ = null;
+        }
+        return regionLoadsBuilder_;
+      }
+
+      // repeated .Coprocessor coprocessors = 6;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_ =
+        java.util.Collections.emptyList();
+      private void ensureCoprocessorsIsMutable() {
+        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
+          coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>(coprocessors_);
+          bitField0_ |= 0x00000020;
+         }
       }
+
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> coprocessorsBuilder_;
+
       /**
-       * <code>optional uint32 total_number_of_requests = 2;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Total Number of requests from the start of the region server. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public int getTotalNumberOfRequests() {
-        return totalNumberOfRequests_;
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
+        if (coprocessorsBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(coprocessors_);
+        } else {
+          return coprocessorsBuilder_.getMessageList();
+        }
       }
       /**
-       * <code>optional uint32 total_number_of_requests = 2;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Total Number of requests from the start of the region server. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder setTotalNumberOfRequests(int value) {
-        bitField0_ |= 0x00000002;
-        totalNumberOfRequests_ = value;
-        onChanged();
-        return this;
+      public int getCoprocessorsCount() {
+        if (coprocessorsBuilder_ == null) {
+          return coprocessors_.size();
+        } else {
+          return coprocessorsBuilder_.getCount();
+        }
       }
       /**
-       * <code>optional uint32 total_number_of_requests = 2;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Total Number of requests from the start of the region server. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder clearTotalNumberOfRequests() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        totalNumberOfRequests_ = 0;
-        onChanged();
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
+        if (coprocessorsBuilder_ == null) {
+          return coprocessors_.get(index);
+        } else {
+          return coprocessorsBuilder_.getMessage(index);
+        }
       }
-
-      // optional uint32 used_heap_MB = 3;
-      private int usedHeapMB_ ;
       /**
-       * <code>optional uint32 used_heap_MB = 3;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the amount of used heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public boolean hasUsedHeapMB() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
+      public Builder setCoprocessors(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
+        if (coprocessorsBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureCoprocessorsIsMutable();
+          coprocessors_.set(index, value);
+          onChanged();
+        } else {
+          coprocessorsBuilder_.setMessage(index, value);
+        }
+        return this;
       }
       /**
-       * <code>optional uint32 used_heap_MB = 3;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the amount of used heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public int getUsedHeapMB() {
-        return usedHeapMB_;
+      public Builder setCoprocessors(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
+        if (coprocessorsBuilder_ == null) {
+          ensureCoprocessorsIsMutable();
+          coprocessors_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          coprocessorsBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
       }
       /**
-       * <code>optional uint32 used_heap_MB = 3;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the amount of used heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder setUsedHeapMB(int value) {
-        bitField0_ |= 0x00000004;
-        usedHeapMB_ = value;
-        onChanged();
+      public Builder addCoprocessors(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
+        if (coprocessorsBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureCoprocessorsIsMutable();
+          coprocessors_.add(value);
+          onChanged();
+        } else {
+          coprocessorsBuilder_.addMessage(value);
+        }
         return this;
       }
       /**
-       * <code>optional uint32 used_heap_MB = 3;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the amount of used heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder clearUsedHeapMB() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        usedHeapMB_ = 0;
-        onChanged();
+      public Builder addCoprocessors(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
+        if (coprocessorsBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureCoprocessorsIsMutable();
+          coprocessors_.add(index, value);
+          onChanged();
+        } else {
+          coprocessorsBuilder_.addMessage(index, value);
+        }
         return this;
       }
-
-      // optional uint32 max_heap_MB = 4;
-      private int maxHeapMB_ ;
       /**
-       * <code>optional uint32 max_heap_MB = 4;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the maximum allowable size of the heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public boolean hasMaxHeapMB() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
+      public Builder addCoprocessors(
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
+        if (coprocessorsBuilder_ == null) {
+          ensureCoprocessorsIsMutable();
+          coprocessors_.add(builderForValue.build());
+          onChanged();
+        } else {
+          coprocessorsBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
       }
       /**
-       * <code>optional uint32 max_heap_MB = 4;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the maximum allowable size of the heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public int getMaxHeapMB() {
-        return maxHeapMB_;
+      public Builder addCoprocessors(
+          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
+        if (coprocessorsBuilder_ == null) {
+          ensureCoprocessorsIsMutable();
+          coprocessors_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          coprocessorsBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
       }
       /**
-       * <code>optional uint32 max_heap_MB = 4;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the maximum allowable size of the heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder setMaxHeapMB(int value) {
-        bitField0_ |= 0x00000008;
-        maxHeapMB_ = value;
-        onChanged();
+      public Builder addAllCoprocessors(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> values) {
+        if (coprocessorsBuilder_ == null) {
+          ensureCoprocessorsIsMutable();
+          super.addAll(values, coprocessors_);
+          onChanged();
+        } else {
+          coprocessorsBuilder_.addAllMessages(values);
+        }
         return this;
       }
       /**
-       * <code>optional uint32 max_heap_MB = 4;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** the maximum allowable size of the heap, in MB. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder clearMaxHeapMB() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        maxHeapMB_ = 0;
-        onChanged();
+      public Builder clearCoprocessors() {
+        if (coprocessorsBuilder_ == null) {
+          coprocessors_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000020);
+          onChanged();
+        } else {
+          coprocessorsBuilder_.clear();
+        }
         return this;
       }
-
-      // repeated .RegionLoad region_loads = 5;
-      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> regionLoads_ =
-        java.util.Collections.emptyList();
-      private void ensureRegionLoadsIsMutable() {
-        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
-          regionLoads_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad>(regionLoads_);
-          bitField0_ |= 0x00000010;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> regionLoadsBuilder_;
-
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> getRegionLoadsList() {
-        if (regionLoadsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(regionLoads_);
+      public Builder removeCoprocessors(int index) {
+        if (coprocessorsBuilder_ == null) {
+          ensureCoprocessorsIsMutable();
+          coprocessors_.remove(index);
+          onChanged();
         } else {
-          return regionLoadsBuilder_.getMessageList();
+          coprocessorsBuilder_.remove(index);
         }
+        return this;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public int getRegionLoadsCount() {
-        if (regionLoadsBuilder_ == null) {
-          return regionLoads_.size();
-        } else {
-          return regionLoadsBuilder_.getCount();
-        }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder getCoprocessorsBuilder(
+          int index) {
+        return getCoprocessorsFieldBuilder().getBuilder(index);
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad getRegionLoads(int index) {
-        if (regionLoadsBuilder_ == null) {
-          return regionLoads_.get(index);
-        } else {
-          return regionLoadsBuilder_.getMessage(index);
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
+          int index) {
+        if (coprocessorsBuilder_ == null) {
+          return coprocessors_.get(index);  } else {
+          return coprocessorsBuilder_.getMessageOrBuilder(index);
         }
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder setRegionLoads(
-          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
-        if (regionLoadsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureRegionLoadsIsMutable();
-          regionLoads_.set(index, value);
-          onChanged();
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
+           getCoprocessorsOrBuilderList() {
+        if (coprocessorsBuilder_ != null) {
+          return coprocessorsBuilder_.getMessageOrBuilderList();
         } else {
-          regionLoadsBuilder_.setMessage(index, value);
+          return java.util.Collections.unmodifiableList(coprocessors_);
         }
-        return this;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder setRegionLoads(
-          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
-        if (regionLoadsBuilder_ == null) {
-          ensureRegionLoadsIsMutable();
-          regionLoads_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          regionLoadsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder() {
+        return getCoprocessorsFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder addRegionLoads(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
-        if (regionLoadsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureRegionLoadsIsMutable();
-          regionLoads_.add(value);
-          onChanged();
-        } else {
-          regionLoadsBuilder_.addMessage(value);
-        }
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder(
+          int index) {
+        return getCoprocessorsFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>repeated .Coprocessor coprocessors = 6;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Regionserver-level coprocessors, e.g., WALObserver implementations.
+       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
+       * objects.
        * </pre>
        */
-      public Builder addRegionLoads(
-          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad value) {
-        if (regionLoadsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureRegionLoadsIsMutable();
-          regionLoads_.add(index, value);
-          onChanged();
-        } else {
-          regionLoadsBuilder_.addMessage(index, value);
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder> 
+           getCoprocessorsBuilderList() {
+        return getCoprocessorsFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
+          getCoprocessorsFieldBuilder() {
+        if (coprocessorsBuilder_ == null) {
+          coprocessorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder>(
+                  coprocessors_,
+                  ((bitField0_ & 0x00000020) == 0x00000020),
+                  getParentForChildren(),
+                  isClean());
+          coprocessors_ = null;
         }
-        return this;
+        return coprocessorsBuilder_;
       }
+
+      // optional uint64 report_start_time = 7;
+      private long reportStartTime_ ;
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_start_time = 7;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public Builder addRegionLoads(
-          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
-        if (regionLoadsBuilder_ == null) {
-          ensureRegionLoadsIsMutable();
-          regionLoads_.add(builderForValue.build());
-          onChanged();
-        } else {
-          regionLoadsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
+      public boolean hasReportStartTime() {
+        return ((bitField0_ & 0x00000040) == 0x00000040);
+      }
+      /**
+       * <code>optional uint64 report_start_time = 7;</code>
+       *
+       * <pre>
+       **
+       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
+       * </pre>
+       */
+      public long getReportStartTime() {
+        return reportStartTime_;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_start_time = 7;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public Builder addRegionLoads(
-          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder builderForValue) {
-        if (regionLoadsBuilder_ == null) {
-          ensureRegionLoadsIsMutable();
-          regionLoads_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          regionLoadsBuilder_.addMessage(index, builderForValue.build());
-        }
+      public Builder setReportStartTime(long value) {
+        bitField0_ |= 0x00000040;
+        reportStartTime_ = value;
+        onChanged();
         return this;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_start_time = 7;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public Builder addAllRegionLoads(
-          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad> values) {
-        if (regionLoadsBuilder_ == null) {
-          ensureRegionLoadsIsMutable();
-          super.addAll(values, regionLoads_);
-          onChanged();
-        } else {
-          regionLoadsBuilder_.addAllMessages(values);
-        }
+      public Builder clearReportStartTime() {
+        bitField0_ = (bitField0_ & ~0x00000040);
+        reportStartTime_ = 0L;
+        onChanged();
         return this;
       }
+
+      // optional uint64 report_end_time = 8;
+      private long reportEndTime_ ;
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_end_time = 8;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when report was generated.
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public Builder clearRegionLoads() {
-        if (regionLoadsBuilder_ == null) {
-          regionLoads_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000010);
-          onChanged();
-        } else {
-          regionLoadsBuilder_.clear();
-        }
-        return this;
+      public boolean hasReportEndTime() {
+        return ((bitField0_ & 0x00000080) == 0x00000080);
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_end_time = 8;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when report was generated.
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public Builder removeRegionLoads(int index) {
-        if (regionLoadsBuilder_ == null) {
-          ensureRegionLoadsIsMutable();
-          regionLoads_.remove(index);
-          onChanged();
-        } else {
-          regionLoadsBuilder_.remove(index);
-        }
-        return this;
+      public long getReportEndTime() {
+        return reportEndTime_;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_end_time = 8;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when report was generated.
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder getRegionLoadsBuilder(
-          int index) {
-        return getRegionLoadsFieldBuilder().getBuilder(index);
+      public Builder setReportEndTime(long value) {
+        bitField0_ |= 0x00000080;
+        reportEndTime_ = value;
+        onChanged();
+        return this;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint64 report_end_time = 8;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * Time when report was generated.
+       * time is measured as the difference, measured in milliseconds, between the current time
+       * and midnight, January 1, 1970 UTC.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder getRegionLoadsOrBuilder(
-          int index) {
-        if (regionLoadsBuilder_ == null) {
-          return regionLoads_.get(index);  } else {
-          return regionLoadsBuilder_.getMessageOrBuilder(index);
-        }
+      public Builder clearReportEndTime() {
+        bitField0_ = (bitField0_ & ~0x00000080);
+        reportEndTime_ = 0L;
+        onChanged();
+        return this;
       }
+
+      // optional uint32 info_server_port = 9;
+      private int infoServerPort_ ;
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint32 info_server_port = 9;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * The port number that this region server is hosing an info server on.
        * </pre>
        */
-      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
-           getRegionLoadsOrBuilderList() {
-        if (regionLoadsBuilder_ != null) {
-          return regionLoadsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(regionLoads_);
-        }
+      public boolean hasInfoServerPort() {
+        return ((bitField0_ & 0x00000100) == 0x00000100);
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint32 info_server_port = 9;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * The port number that this region server is hosing an info server on.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder() {
-        return getRegionLoadsFieldBuilder().addBuilder(
-            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
+      public int getInfoServerPort() {
+        return infoServerPort_;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint32 info_server_port = 9;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * The port number that this region server is hosing an info server on.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder addRegionLoadsBuilder(
-          int index) {
-        return getRegionLoadsFieldBuilder().addBuilder(
-            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.getDefaultInstance());
+      public Builder setInfoServerPort(int value) {
+        bitField0_ |= 0x00000100;
+        infoServerPort_ = value;
+        onChanged();
+        return this;
       }
       /**
-       * <code>repeated .RegionLoad region_loads = 5;</code>
+       * <code>optional uint32 info_server_port = 9;</code>
        *
        * <pre>
-       ** Information on the load of individual regions. 
+       **
+       * The port number that this region server is hosing an info server on.
        * </pre>
        */
-      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder> 
-           getRegionLoadsBuilderList() {
-        return getRegionLoadsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder> 
-          getRegionLoadsFieldBuilder() {
-        if (regionLoadsBuilder_ == null) {
-          regionLoadsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoadOrBuilder>(
-                  regionLoads_,
-                  ((bitField0_ & 0x00000010) == 0x00000010),
-                  getParentForChildren(),
-                  isClean());
-          regionLoads_ = null;
-        }
-        return regionLoadsBuilder_;
+      public Builder clearInfoServerPort() {
+        bitField0_ = (bitField0_ & ~0x00000100);
+        infoServerPort_ = 0;
+        onChanged();
+        return this;
       }
 
-      // repeated .Coprocessor coprocessors = 6;
-      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> coprocessors_ =
+      // repeated .ReplicationLoadSource replLoadSource = 10;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> replLoadSource_ =
         java.util.Collections.emptyList();
-      private void ensureCoprocessorsIsMutable() {
-        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
-          coprocessors_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor>(coprocessors_);
-          bitField0_ |= 0x00000020;
+      private void ensureReplLoadSourceIsMutable() {
+        if (!((bitField0_ & 0x00000200) == 0x00000200)) {
+          replLoadSource_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource>(replLoadSource_);
+          bitField0_ |= 0x00000200;
          }
       }
 
       private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> coprocessorsBuilder_;
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> replLoadSourceBuilder_;
 
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> getCoprocessorsList() {
-        if (coprocessorsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(coprocessors_);
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> getReplLoadSourceList() {
+        if (replLoadSourceBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(replLoadSource_);
         } else {
-          return coprocessorsBuilder_.getMessageList();
+          return replLoadSourceBuilder_.getMessageList();
         }
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public int getCoprocessorsCount() {
-        if (coprocessorsBuilder_ == null) {
-          return coprocessors_.size();
+      public int getReplLoadSourceCount() {
+        if (replLoadSourceBuilder_ == null) {
+          return replLoadSource_.size();
         } else {
-          return coprocessorsBuilder_.getCount();
+          return replLoadSourceBuilder_.getCount();
         }
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor getCoprocessors(int index) {
-        if (coprocessorsBuilder_ == null) {
-          return coprocessors_.get(index);
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource getReplLoadSource(int index) {
+        if (replLoadSourceBuilder_ == null) {
+          return replLoadSource_.get(index);
         } else {
-          return coprocessorsBuilder_.getMessage(index);
+          return replLoadSourceBuilder_.getMessage(index);
         }
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder setCoprocessors(
-          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
-        if (coprocessorsBuilder_ == null) {
+      public Builder setReplLoadSource(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
+        if (replLoadSourceBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
           }
-          ensureCoprocessorsIsMutable();
-          coprocessors_.set(index, value);
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.set(index, value);
           onChanged();
         } else {
-          coprocessorsBuilder_.setMessage(index, value);
+          replLoadSourceBuilder_.setMessage(index, value);
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder setCoprocessors(
-          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
-        if (coprocessorsBuilder_ == null) {
-          ensureCoprocessorsIsMutable();
-          coprocessors_.set(index, builderForValue.build());
+      public Builder setReplLoadSource(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
+        if (replLoadSourceBuilder_ == null) {
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.set(index, builderForValue.build());
           onChanged();
         } else {
-          coprocessorsBuilder_.setMessage(index, builderForValue.build());
+          replLoadSourceBuilder_.setMessage(index, builderForValue.build());
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder addCoprocessors(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
-        if (coprocessorsBuilder_ == null) {
+      public Builder addReplLoadSource(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
+        if (replLoadSourceBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
           }
-          ensureCoprocessorsIsMutable();
-          coprocessors_.add(value);
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.add(value);
           onChanged();
         } else {
-          coprocessorsBuilder_.addMessage(value);
+          replLoadSourceBuilder_.addMessage(value);
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder addCoprocessors(
-          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor value) {
-        if (coprocessorsBuilder_ == null) {
+      public Builder addReplLoadSource(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource value) {
+        if (replLoadSourceBuilder_ == null) {
           if (value == null) {
             throw new NullPointerException();
           }
-          ensureCoprocessorsIsMutable();
-          coprocessors_.add(index, value);
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.add(index, value);
           onChanged();
         } else {
-          coprocessorsBuilder_.addMessage(index, value);
+          replLoadSourceBuilder_.addMessage(index, value);
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder addCoprocessors(
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
-        if (coprocessorsBuilder_ == null) {
-          ensureCoprocessorsIsMutable();
-          coprocessors_.add(builderForValue.build());
+      public Builder addReplLoadSource(
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
+        if (replLoadSourceBuilder_ == null) {
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.add(builderForValue.build());
           onChanged();
         } else {
-          coprocessorsBuilder_.addMessage(builderForValue.build());
+          replLoadSourceBuilder_.addMessage(builderForValue.build());
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder addCoprocessors(
-          int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder builderForValue) {
-        if (coprocessorsBuilder_ == null) {
-          ensureCoprocessorsIsMutable();
-          coprocessors_.add(index, builderForValue.build());
+      public Builder addReplLoadSource(
+          int index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder builderForValue) {
+        if (replLoadSourceBuilder_ == null) {
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.add(index, builderForValue.build());
           onChanged();
         } else {
-          coprocessorsBuilder_.addMessage(index, builderForValue.build());
+          replLoadSourceBuilder_.addMessage(index, builderForValue.build());
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder addAllCoprocessors(
-          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor> values) {
-        if (coprocessorsBuilder_ == null) {
-          ensureCoprocessorsIsMutable();
-          super.addAll(values, coprocessors_);
+      public Builder addAllReplLoadSource(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource> values) {
+        if (replLoadSourceBuilder_ == null) {
+          ensureReplLoadSourceIsMutable();
+          super.addAll(values, replLoadSource_);
           onChanged();
         } else {
-          coprocessorsBuilder_.addAllMessages(values);
+          replLoadSourceBuilder_.addAllMessages(values);
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder clearCoprocessors() {
-        if (coprocessorsBuilder_ == null) {
-          coprocessors_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000020);
+      public Builder clearReplLoadSource() {
+        if (replLoadSourceBuilder_ == null) {
+          replLoadSource_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000200);
           onChanged();
         } else {
-          coprocessorsBuilder_.clear();
+          replLoadSourceBuilder_.clear();
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public Builder removeCoprocessors(int index) {
-        if (coprocessorsBuilder_ == null) {
-          ensureCoprocessorsIsMutable();
-          coprocessors_.remove(index);
+      public Builder removeReplLoadSource(int index) {
+        if (replLoadSourceBuilder_ == null) {
+          ensureReplLoadSourceIsMutable();
+          replLoadSource_.remove(index);
           onChanged();
         } else {
-          coprocessorsBuilder_.remove(index);
+          replLoadSourceBuilder_.remove(index);
         }
         return this;
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder getCoprocessorsBuilder(
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder getReplLoadSourceBuilder(
           int index) {
-        return getCoprocessorsFieldBuilder().getBuilder(index);
+        return getReplLoadSourceFieldBuilder().getBuilder(index);
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder getCoprocessorsOrBuilder(
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder getReplLoadSourceOrBuilder(
           int index) {
-        if (coprocessorsBuilder_ == null) {
-          return coprocessors_.get(index);  } else {
-          return coprocessorsBuilder_.getMessageOrBuilder(index);
+        if (replLoadSourceBuilder_ == null) {
+          return replLoadSource_.get(index);  } else {
+          return replLoadSourceBuilder_.getMessageOrBuilder(index);
         }
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
-           getCoprocessorsOrBuilderList() {
-        if (coprocessorsBuilder_ != null) {
-          return coprocessorsBuilder_.getMessageOrBuilderList();
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
+           getReplLoadSourceOrBuilderList() {
+        if (replLoadSourceBuilder_ != null) {
+          return replLoadSourceBuilder_.getMessageOrBuilderList();
         } else {
-          return java.util.Collections.unmodifiableList(coprocessors_);
+          return java.util.Collections.unmodifiableList(replLoadSource_);
         }
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder() {
-        return getCoprocessorsFieldBuilder().addBuilder(
-            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder addReplLoadSourceBuilder() {
+        return getReplLoadSourceFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance());
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder addCoprocessorsBuilder(
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder addReplLoadSourceBuilder(
           int index) {
-        return getCoprocessorsFieldBuilder().addBuilder(
-            index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.getDefaultInstance());
+        return getReplLoadSourceFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.getDefaultInstance());
       }
       /**
-       * <code>repeated .Coprocessor coprocessors = 6;</code>
+       * <code>repeated .ReplicationLoadSource replLoadSource = 10;</code>
        *
        * <pre>
        **
-       * Regionserver-level coprocessors, e.g., WALObserver implementations.
-       * Region-level coprocessors, on the other hand, are stored inside RegionLoad
-       * objects.
+       * The replicationLoadSource for the replication Source status of this region server.
        * </pre>
        */
-      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder> 
-           getCoprocessorsBuilderList() {
-        return getCoprocessorsFieldBuilder().getBuilderList();
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder> 
+           getReplLoadSourceBuilderList() {
+        return getReplLoadSourceFieldBuilder().getBuilderList();
       }
       private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder> 
-          getCoprocessorsFieldBuilder() {
-        if (coprocessorsBuilder_ == null) {
-          coprocessorsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CoprocessorOrBuilder>(
-                  coprocessors_,
-                  ((bitField0_ & 0x00000020) == 0x00000020),
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder> 
+          getReplLoadSourceFieldBuilder() {
+        if (replLoadSourceBuilder_ == null) {
+          replLoadSourceBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSource.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSourceOrBuilder>(
+                  replLoadSource_,
+                  ((bitField0_ & 0x00000200) == 0x00000200),
                   getParentForChildren(),
                   isClean());
-          coprocessors_ = null;
+          replLoadSource_ = null;
         }
-        return coprocessorsBuilder_;
+        return replLoadSourceBuilder_;
       }
 
-      // optional uint64 report_start_time = 7;
-      private long reportStartTime_ ;
+      // optional .ReplicationLoadSink replLoadSink = 11;
+      private org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder> replLoadSinkBuilder_;
       /**
-       * <code>optional uint64 report_start_time = 7;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public boolean hasReportStartTime() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+      public boolean hasReplLoadSink() {
+        return ((bitField0_ & 0x00000400) == 0x00000400);
       }
       /**
-       * <code>optional uint64 report_start_time = 7;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public long getReportStartTime() {
-        return reportStartTime_;
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink getReplLoadSink() {
+        if (replLoadSinkBuilder_ == null) {
+          return replLoadSink_;
+        } else {
+          return replLoadSinkBuilder_.getMessage();
+        }
       }
       /**
-       * <code>optional uint64 report_start_time = 7;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder setReportStartTime(long value) {
-        bitField0_ |= 0x00000040;
-        reportStartTime_ = value;
-        onChanged();
+      public Builder setReplLoadSink(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink value) {
+        if (replLoadSinkBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          replLoadSink_ = value;
+          onChanged();
+        } else {
+          replLoadSinkBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000400;
         return this;
       }
       /**
-       * <code>optional uint64 report_start_time = 7;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when incremental (non-total) counts began being calculated (e.g. number_of_requests)
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder clearReportStartTime() {
-        bitField0_ = (bitField0_ & ~0x00000040);
-        reportStartTime_ = 0L;
-        onChanged();
+      public Builder setReplLoadSink(
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder builderForValue) {
+        if (replLoadSinkBuilder_ == null) {
+          replLoadSink_ = builderForValue.build();
+          onChanged();
+        } else {
+          replLoadSinkBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000400;
         return this;
       }
-
-      // optional uint64 report_end_time = 8;
-      private long reportEndTime_ ;
-      /**
-       * <code>optional uint64 report_end_time = 8;</code>
-       *
-       * <pre>
-       **
-       * Time when report was generated.
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
-       * </pre>
-       */
-      public boolean hasReportEndTime() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
-      }
-      /**
-       * <code>optional uint64 report_end_time = 8;</code>
-       *
-       * <pre>
-       **
-       * Time when report was generated.
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
-       * </pre>
-       */
-      public long getReportEndTime() {
-        return reportEndTime_;
-      }
       /**
-       * <code>optional uint64 report_end_time = 8;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when report was generated.
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder setReportEndTime(long value) {
-        bitField0_ |= 0x00000080;
-        reportEndTime_ = value;
-        onChanged();
+      public Builder mergeReplLoadSink(org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink value) {
+        if (replLoadSinkBuilder_ == null) {
+          if (((bitField0_ & 0x00000400) == 0x00000400) &&
+              replLoadSink_ != org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance()) {
+            replLoadSink_ =
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.newBuilder(replLoadSink_).mergeFrom(value).buildPartial();
+          } else {
+            replLoadSink_ = value;
+          }
+          onChanged();
+        } else {
+          replLoadSinkBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000400;
         return this;
       }
       /**
-       * <code>optional uint64 report_end_time = 8;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * Time when report was generated.
-       * time is measured as the difference, measured in milliseconds, between the current time
-       * and midnight, January 1, 1970 UTC.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder clearReportEndTime() {
-        bitField0_ = (bitField0_ & ~0x00000080);
-        reportEndTime_ = 0L;
-        onChanged();
+      public Builder clearReplLoadSink() {
+        if (replLoadSinkBuilder_ == null) {
+          replLoadSink_ = org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.getDefaultInstance();
+          onChanged();
+        } else {
+          replLoadSinkBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000400);
         return this;
       }
-
-      // optional uint32 info_server_port = 9;
-      private int infoServerPort_ ;
-      /**
-       * <code>optional uint32 info_server_port = 9;</code>
-       *
-       * <pre>
-       **
-       * The port number that this region server is hosing an info server on.
-       * </pre>
-       */
-      public boolean hasInfoServerPort() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
-      }
       /**
-       * <code>optional uint32 info_server_port = 9;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * The port number that this region server is hosing an info server on.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public int getInfoServerPort() {
-        return infoServerPort_;
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder getReplLoadSinkBuilder() {
+        bitField0_ |= 0x00000400;
+        onChanged();
+        return getReplLoadSinkFieldBuilder().getBuilder();
       }
       /**
-       * <code>optional uint32 info_server_port = 9;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * The port number that this region server is hosing an info server on.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder setInfoServerPort(int value) {
-        bitField0_ |= 0x00000100;
-        infoServerPort_ = value;
-        onChanged();
-        return this;
+      public org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder getReplLoadSinkOrBuilder() {
+        if (replLoadSinkBuilder_ != null) {
+          return replLoadSinkBuilder_.getMessageOrBuilder();
+        } else {
+          return replLoadSink_;
+        }
       }
       /**
-       * <code>optional uint32 info_server_port = 9;</code>
+       * <code>optional .ReplicationLoadSink replLoadSink = 11;</code>
        *
        * <pre>
        **
-       * The port number that this region server is hosing an info server on.
+       * The replicationLoadSink for the replication Sink status of this region server.
        * </pre>
        */
-      public Builder clearInfoServerPort() {
-        bitField0_ = (bitField0_ & ~0x00000100);
-        infoServerPort_ = 0;
-        onChanged();
-        return this;
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder> 
+          getReplLoadSinkFieldBuilder() {
+        if (replLoadSinkBuilder_ == null) {
+          replLoadSinkBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSink.Builder, org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.ReplicationLoadSinkOrBuilder>(
+                  replLoadSink_,
+                  getParentForChildren(),
+                  isClean());
+          replLoadSink_ = null;
+        }
+        return replLoadSinkBuilder_;
       }
 
       // @@protoc_insertion_point(builder_scope:ServerLoad)
@@ -10527,6 +12784,16 @@ public final class ClusterStatusProtos {
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_RegionLoad_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ReplicationLoadSink_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ReplicationLoadSink_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ReplicationLoadSource_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ReplicationLoadSource_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_ServerLoad_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
@@ -10575,26 +12842,34 @@ public final class ClusterStatusProtos {
       "(\r\022\"\n\032total_static_bloom_size_KB\030\016 \001(\r\022\034" +
       "\n\024complete_sequence_id\030\017 \001(\004\022\025\n\rdata_loc" +
       "ality\030\020 \001(\002\022#\n\030last_major_compaction_ts\030" +
-      "\021 \001(\004:\0010\"\212\002\n\nServerLoad\022\032\n\022number_of_req" +
-      "uests\030\001 \001(\r\022 \n\030total_number_of_requests\030" +
-      "\002 \001(\r\022\024\n\014used_heap_MB\030\003 \001(\r\022\023\n\013max_heap_" +
-      "MB\030\004 \001(\r\022!\n\014region_loads\030\005 \003(\0132\013.RegionL" +
-      "oad\022\"\n\014coprocessors\030\006 \003(\0132\014.Coprocessor\022",
-      "\031\n\021report_start_time\030\007 \001(\004\022\027\n\017report_end" +
-      "_time\030\010 \001(\004\022\030\n\020info_server_port\030\t \001(\r\"O\n" +
-      "\016LiveServerInfo\022\033\n\006server\030\001 \002(\0132\013.Server" +
-      "Name\022 \n\013server_load\030\002 \002(\0132\013.ServerLoad\"\340" +
-      "\002\n\rClusterStatus\022/\n\rhbase_version\030\001 \001(\0132" +
-      "\030.HBaseVersionFileContent\022%\n\014live_server" +
-      "s\030\002 \003(\0132\017.LiveServerInfo\022!\n\014dead_servers" +
-      "\030\003 \003(\0132\013.ServerName\0222\n\025regions_in_transi" +
-      "tion\030\004 \003(\0132\023.RegionInTransition\022\036\n\nclust" +
-      "er_id\030\005 \001(\0132\n.ClusterId\022)\n\023master_coproc",
-      "essors\030\006 \003(\0132\014.Coprocessor\022\033\n\006master\030\007 \001" +
-      "(\0132\013.ServerName\022#\n\016backup_masters\030\010 \003(\0132" +
-      "\013.ServerName\022\023\n\013balancer_on\030\t \001(\010BF\n*org" +
-      ".apache.hadoop.hbase.protobuf.generatedB" +
-      "\023ClusterStatusProtosH\001\240\001\001"
+      "\021 \001(\004:\0010\"T\n\023ReplicationLoadSink\022\032\n\022ageOf" +
+      "LastAppliedOp\030\001 \002(\004\022!\n\031timeStampsOfLastA" +
+      "ppliedOp\030\002 \002(\004\"\225\001\n\025ReplicationLoadSource" +
+      "\022\016\n\006peerID\030\001 \002(\t\022\032\n\022ageOfLastShippedOp\030\002" +
+      " \002(\004\022\026\n\016sizeOfLogQueue\030\003 \002(\r\022 \n\030timeStam",
+      "pOfLastShippedOp\030\004 \002(\004\022\026\n\016replicationLag" +
+      "\030\005 \002(\004\"\346\002\n\nServerLoad\022\032\n\022number_of_reque" +
+      "sts\030\001 \001(\r\022 \n\030total_number_of_requests\030\002 " +
+      "\001(\r\022\024\n\014used_heap_MB\030\003 \001(\r\022\023\n\013max_heap_MB" +
+      "\030\004 \001(\r\022!\n\014region_loads\030\005 \003(\0132\013.RegionLoa" +
+      "d\022\"\n\014coprocessors\030\006 \003(\0132\014.Coprocessor\022\031\n" +
+      "\021report_start_time\030\007 \001(\004\022\027\n\017report_end_t" +
+      "ime\030\010 \001(\004\022\030\n\020info_server_port\030\t \001(\r\022.\n\016r" +
+      "eplLoadSource\030\n \003(\0132\026.ReplicationLoadSou" +
+      "rce\022*\n\014replLoadSink\030\013 \001(\0132\024.ReplicationL",
+      "oadSink\"O\n\016LiveServerInfo\022\033\n\006server\030\001 \002(" +
+      "\0132\013.ServerName\022 \n\013server_load\030\002 \002(\0132\013.Se" +
+      "rverLoad\"\340\002\n\rClusterStatus\022/\n\rhbase_vers" +
+      "ion\030\001 \001(\0132\030.HBaseVersionFileContent\022%\n\014l" +
+      "ive_servers\030\002 \003(\0132\017.LiveServerInfo\022!\n\014de" +
+      "ad_servers\030\003 \003(\0132\013.ServerName\0222\n\025regions" +
+      "_in_transition\030\004 \003(\0132\023.RegionInTransitio" +
+      "n\022\036\n\ncluster_id\030\005 \001(\0132\n.ClusterId\022)\n\023mas" +
+      "ter_coprocessors\030\006 \003(\0132\014.Coprocessor\022\033\n\006" +
+      "master\030\007 \001(\0132\013.ServerName\022#\n\016backup_mast",
+      "ers\030\010 \003(\0132\013.ServerName\022\023\n\013balancer_on\030\t " +
+      "\001(\010BF\n*org.apache.hadoop.hbase.protobuf." +
+      "generatedB\023ClusterStatusProtosH\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -10619,20 +12894,32 @@ public final class ClusterStatusProtos {
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_RegionLoad_descriptor,
               new java.lang.String[] { "RegionSpecifier", "Stores", "Storefiles", "StoreUncompressedSizeMB", "StorefileSizeMB", "MemstoreSizeMB", "StorefileIndexSizeMB", "ReadRequestsCount", "WriteRequestsCount", "TotalCompactingKVs", "CurrentCompactedKVs", "RootIndexSizeKB", "TotalStaticIndexSizeKB", "TotalStaticBloomSizeKB", "CompleteSequenceId", "DataLocality", "LastMajorCompactionTs", });
-          internal_static_ServerLoad_descriptor =
+          internal_static_ReplicationLoadSink_descriptor =
             getDescriptor().getMessageTypes().get(3);
+          internal_static_ReplicationLoadSink_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ReplicationLoadSink_descriptor,
+              new java.lang.String[] { "AgeOfLastAppliedOp", "TimeStampsOfLastAppliedOp", });
+          internal_static_ReplicationLoadSource_descriptor =
+            getDescriptor().getMessageTypes().get(4);
+          internal_static_ReplicationLoadSource_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ReplicationLoadSource_descriptor,
+              new java.lang.String[] { "PeerID", "AgeOfLastShippedOp", "SizeOfLogQueue", "TimeStampOfLastShippedOp", "ReplicationLag", });
+          internal_static_ServerLoad_descriptor =
+            getDescriptor().getMessageTypes().get(5);
           internal_static_ServerLoad_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ServerLoad_descriptor,
-              new java.lang.String[] { "NumberOfRequests", "TotalNumberOfRequests", "UsedHeapMB", "MaxHeapMB", "RegionLoads", "Coprocessors", "ReportStartTime", "ReportEndTime", "InfoServerPort", });
+              new java.lang.String[] { "NumberOfRequests", "TotalNumberOfRequests", "UsedHeapMB", "MaxHeapMB", "RegionLoads", "Coprocessors", "ReportStartTime", "ReportEndTime", "InfoServerPort", "ReplLoadSource", "ReplLoadSink", });
           internal_static_LiveServerInfo_descriptor =
-            getDescriptor().getMessageTypes().get(4);
+            getDescriptor().getMessageTypes().get(6);
           internal_static_LiveServerInfo_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_LiveServerInfo_descriptor,
               new java.lang.String[] { "Server", "ServerLoad", });
           internal_static_ClusterStatus_descriptor =
-            getDescriptor().getMessageTypes().get(5);
+            getDescriptor().getMessageTypes().get(7);
           internal_static_ClusterStatus_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ClusterStatus_descriptor,
diff --git a/hbase-protocol/src/main/protobuf/ClusterStatus.proto b/hbase-protocol/src/main/protobuf/ClusterStatus.proto
index 2b2d9eb..bb531cc 100644
--- a/hbase-protocol/src/main/protobuf/ClusterStatus.proto
+++ b/hbase-protocol/src/main/protobuf/ClusterStatus.proto
@@ -119,6 +119,19 @@ message RegionLoad {
 
 /* Server-level protobufs */
 
+message ReplicationLoadSink {
+  required uint64 ageOfLastAppliedOp = 1;
+  required uint64 timeStampsOfLastAppliedOp = 2;
+}
+
+message ReplicationLoadSource {
+  required string peerID = 1;
+  required uint64 ageOfLastShippedOp = 2;
+  required uint32 sizeOfLogQueue = 3;
+  required uint64 timeStampOfLastShippedOp = 4;
+  required uint64 replicationLag = 5;
+}
+
 message ServerLoad {
   /** Number of requests since last report. */
   optional uint32 number_of_requests = 1;
@@ -160,6 +173,16 @@ message ServerLoad {
    * The port number that this region server is hosing an info server on.
    */
   optional uint32 info_server_port = 9;
+
+  /**
+   * The replicationLoadSource for the replication Source status of this region server.
+   */
+  repeated ReplicationLoadSource replLoadSource = 10;
+
+  /**
+   * The replicationLoadSink for the replication Sink status of this region server.
+   */
+  optional ReplicationLoadSink replLoadSink = 11;
 }
 
 message LiveServerInfo {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index fcb8c6f..c4b3456 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -130,6 +130,7 @@ import org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler;
 import org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler;
 import org.apache.hadoop.hbase.regionserver.wal.MetricsWAL;
 import org.apache.hadoop.hbase.regionserver.wal.WALActionsListener;
+import org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad;
 import org.apache.hadoop.hbase.security.UserProvider;
 import org.apache.hadoop.hbase.trace.SpanReceiverHost;
 import org.apache.hadoop.hbase.util.Addressing;
@@ -1143,6 +1144,22 @@ public class HRegionServer extends HasThread implements
     } else {
       serverLoad.setInfoServerPort(-1);
     }
+
+    // for the replicationLoad purpose. Only need to get from one service
+    // either source or sink will get the same info
+    ReplicationSourceService rsources = getReplicationSourceService();
+
+    if (rsources != null) {
+      // always refresh first to get the latest value
+      ReplicationLoad rLoad = rsources.refreshAndGetReplicationLoad();
+      if (rLoad != null) {
+        serverLoad.setReplLoadSink(rLoad.getReplicationLoadSink());
+        for (ClusterStatusProtos.ReplicationLoadSource rLS : rLoad.getReplicationLoadSourceList()) {
+          serverLoad.addReplLoadSource(rLS);
+        }
+      }
+    }
+
     return serverLoad.build();
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java
index 92ac823..25a27a9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/ReplicationService.java
@@ -22,11 +22,12 @@ import java.io.IOException;
 
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
 /**
- * Gateway to Cluster Replication.  
+ * Gateway to Cluster Replication.
  * Used by {@link org.apache.hadoop.hbase.regionserver.HRegionServer}.
  * One such application is a cross-datacenter
  * replication service that can keep two hbase clusters in sync.
@@ -52,4 +53,9 @@ public interface ReplicationService {
    * Stops replication service.
    */
   void stopReplicationService();
+
+  /**
+   * Refresh and Get ReplicationLoad
+   */
+  public ReplicationLoad refreshAndGetReplicationLoad();
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSink.java
index 0c9d016..37dc1dd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSink.java
@@ -71,4 +71,21 @@ public class MetricsSink {
     mss.incrAppliedOps(batchSize);
   }
 
+  /**
+   * Get the Age of Last Applied Op
+   * @return ageOfLastAppliedOp
+   */
+  public long getAgeOfLastAppliedOp() {
+    return mss.getLastAppliedOpAge();
+  }
+
+  /**
+   * Get the TimeStampOfLastAppliedOp. If no replication Op applied yet, the value is the timestamp
+   * at which hbase instance starts
+   * @return timeStampsOfLastAppliedOp;
+   */
+  public long getTimeStampOfLastAppliedOp() {
+    return this.lastTimestampForAge;
+  }
+
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
index a734b9c..21296a0 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/MetricsSource.java
@@ -36,6 +36,7 @@ public class MetricsSource {
 
   private long lastTimestamp = 0;
   private int lastQueueSize = 0;
+  private String id;
 
   private final MetricsReplicationSourceSource singleSourceSource;
   private final MetricsReplicationSourceSource globalSourceSource;
@@ -46,6 +47,7 @@ public class MetricsSource {
    * @param id Name of the source this class is monitoring
    */
   public MetricsSource(String id) {
+    this.id = id;
     singleSourceSource =
         CompatibilitySingletonFactory.getInstance(MetricsReplicationSourceFactory.class)
             .getSource(id);
@@ -143,4 +145,36 @@ public class MetricsSource {
     globalSourceSource.decrSizeOfLogQueue(lastQueueSize);
     lastQueueSize = 0;
   }
+
+  /**
+   * Get AgeOfLastShippedOp
+   * @return AgeOfLastShippedOp
+   */
+  public Long getAgeOfLastShippedOp() {
+    return singleSourceSource.getLastShippedAge();
+  }
+
+  /**
+   * Get the sizeOfLogQueue
+   * @return sizeOfLogQueue
+   */
+  public int getSizeOfLogQueue() {
+    return this.lastQueueSize;
+  }
+
+  /**
+   * Get the timeStampsOfLastShippedOp
+   * @return lastTimestampForAge
+   */
+  public long getTimeStampOfLastShippedOp() {
+    return lastTimestamp;
+  }
+
+  /**
+   * Get the slave peer ID
+   * @return peerID
+   */
+  public String getPeerID() {
+    return id;
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
index b30698c..5b0f469 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java
@@ -23,6 +23,7 @@ import static org.apache.hadoop.hbase.HConstants.REPLICATION_ENABLE_KEY;
 import static org.apache.hadoop.hbase.HConstants.REPLICATION_SCOPE_LOCAL;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.NavigableMap;
 import java.util.TreeMap;
@@ -65,7 +66,7 @@ import com.google.common.util.concurrent.ThreadFactoryBuilder;
  * Gateway to Replication.  Used by {@link org.apache.hadoop.hbase.regionserver.HRegionServer}.
  */
 @InterfaceAudience.Private
-public class Replication extends WALActionsListener.Base implements 
+public class Replication extends WALActionsListener.Base implements
   ReplicationSourceService, ReplicationSinkService {
   private static final Log LOG =
       LogFactory.getLog(Replication.class);
@@ -81,6 +82,8 @@ public class Replication extends WALActionsListener.Base implements
   /** Statistics thread schedule pool */
   private ScheduledExecutorService scheduleThreadPool;
   private int statsThreadPeriod;
+  // ReplicationLoad to access replication metrics
+  private ReplicationLoad replicationLoad;
 
   /**
    * Instantiate the replication management (if rep is enabled).
@@ -137,11 +140,13 @@ public class Replication extends WALActionsListener.Base implements
       this.statsThreadPeriod =
           this.conf.getInt("replication.stats.thread.period.seconds", 5 * 60);
       LOG.debug("ReplicationStatisticsThread " + this.statsThreadPeriod);
+      this.replicationLoad = new ReplicationLoad();
     } else {
       this.replicationManager = null;
       this.replicationQueues = null;
       this.replicationPeers = null;
       this.replicationTracker = null;
+      this.replicationLoad = null;
     }
   }
 
@@ -309,4 +314,29 @@ public class Replication extends WALActionsListener.Base implements
       }
     }
   }
+
+  @Override
+  public ReplicationLoad refreshAndGetReplicationLoad() {
+    if (this.replicationLoad == null) {
+      return null;
+    }
+    // always build for latest data
+    buildReplicationLoad();
+    return this.replicationLoad;
+  }
+
+  private void buildReplicationLoad() {
+    // get source
+    List<ReplicationSourceInterface> sources = this.replicationManager.getSources();
+    List<MetricsSource> sourceMetricsList = new ArrayList<MetricsSource>();
+
+    for (ReplicationSourceInterface source : sources) {
+      if (source instanceof ReplicationSource) {
+        sourceMetricsList.add(((ReplicationSource) source).getSourceMetrics());
+      }
+    }
+    // get sink
+    MetricsSink sinkMetrics = this.replicationSink.getSinkMetrics();
+    this.replicationLoad.buildReplicationLoad(sourceMetricsList, sinkMetrics);
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java
new file mode 100644
index 0000000..525efad
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.java
@@ -0,0 +1,153 @@
+/**
+ * Copyright 2014 The Apache Software Foundation Licensed to the Apache Software Foundation (ASF)
+ * under one or more contributor license agreements. See the NOTICE file distributed with this work
+ * for additional information regarding copyright ownership. The ASF licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ * http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in
+ * writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific
+ * language governing permissions and limitations under the License.
+ */
+package org.apache.hadoop.hbase.replication.regionserver;
+
+import java.util.Date;
+import java.util.List;
+import java.util.ArrayList;
+
+import org.apache.hadoop.hbase.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
+import org.apache.hadoop.hbase.replication.regionserver.MetricsSink;
+import org.apache.hadoop.hbase.replication.regionserver.MetricsSource;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.Strings;
+
+/**
+ * This class is used for exporting some of the info from replication metrics
+ */
+@InterfaceAudience.Private
+public class ReplicationLoad {
+
+  // Empty load instance.
+  public static final ReplicationLoad EMPTY_REPLICATIONLOAD = new ReplicationLoad();
+
+  private List<MetricsSource> sourceMetricsList;
+  private MetricsSink sinkMetrics;
+
+  private List<ClusterStatusProtos.ReplicationLoadSource> replicationLoadSourceList;
+  private ClusterStatusProtos.ReplicationLoadSink replicationLoadSink;
+
+  /** default constructor */
+  public ReplicationLoad() {
+    super();
+  }
+
+  /**
+   * buildReplicationLoad
+   * @param srMetricsList
+   * @param skMetrics
+   */
+
+  public void buildReplicationLoad(final List<MetricsSource> srMetricsList,
+      final MetricsSink skMetrics) {
+    this.sourceMetricsList = srMetricsList;
+    this.sinkMetrics = skMetrics;
+
+    // build the SinkLoad
+    ClusterStatusProtos.ReplicationLoadSink.Builder rLoadSinkBuild =
+        ClusterStatusProtos.ReplicationLoadSink.newBuilder();
+    rLoadSinkBuild.setAgeOfLastAppliedOp(sinkMetrics.getAgeOfLastAppliedOp());
+    rLoadSinkBuild.setTimeStampsOfLastAppliedOp(sinkMetrics.getTimeStampOfLastAppliedOp());
+    this.replicationLoadSink = rLoadSinkBuild.build();
+
+    // build the SourceLoad List
+    this.replicationLoadSourceList = new ArrayList<ClusterStatusProtos.ReplicationLoadSource>();
+    for (MetricsSource sm : this.sourceMetricsList) {
+      long ageOfLastShippedOp = sm.getAgeOfLastShippedOp();
+      int sizeOfLogQueue = sm.getSizeOfLogQueue();
+      long timeStampOfLastShippedOp = sm.getTimeStampOfLastShippedOp();
+      long replicationLag;
+      long timePassedAfterLastShippedOp =
+          EnvironmentEdgeManager.currentTime() - timeStampOfLastShippedOp;
+      if (sizeOfLogQueue != 0) {
+        // err on the large side
+        replicationLag = Math.max(ageOfLastShippedOp, timePassedAfterLastShippedOp);
+      } else if (timePassedAfterLastShippedOp < 2 * ageOfLastShippedOp) {
+        replicationLag = ageOfLastShippedOp; // last shipped happen recently
+      } else {
+        // last shipped may happen last night,
+        // so NO real lag although ageOfLastShippedOp is non-zero
+        replicationLag = 0;
+      }
+
+      ClusterStatusProtos.ReplicationLoadSource.Builder rLoadSourceBuild =
+          ClusterStatusProtos.ReplicationLoadSource.newBuilder();
+      rLoadSourceBuild.setPeerID(sm.getPeerID());
+      rLoadSourceBuild.setAgeOfLastShippedOp(ageOfLastShippedOp);
+      rLoadSourceBuild.setSizeOfLogQueue(sizeOfLogQueue);
+      rLoadSourceBuild.setTimeStampOfLastShippedOp(timeStampOfLastShippedOp);
+      rLoadSourceBuild.setReplicationLag(replicationLag);
+
+      this.replicationLoadSourceList.add(rLoadSourceBuild.build());
+    }
+
+  }
+
+  /**
+   * sourceToString
+   * @return a string contains sourceReplicationLoad information
+   */
+  public String sourceToString() {
+    if (this.sourceMetricsList == null) return null;
+
+    StringBuilder sb = new StringBuilder();
+
+    for (ClusterStatusProtos.ReplicationLoadSource rls : this.replicationLoadSourceList) {
+
+      sb = Strings.appendKeyValue(sb, "\n           PeerID", rls.getPeerID());
+      sb = Strings.appendKeyValue(sb, "AgeOfLastShippedOp", rls.getAgeOfLastShippedOp());
+      sb = Strings.appendKeyValue(sb, "SizeOfLogQueue", rls.getSizeOfLogQueue());
+      sb =
+          Strings.appendKeyValue(sb, "TimeStampsOfLastShippedOp",
+            (new Date(rls.getTimeStampOfLastShippedOp()).toString()));
+      sb = Strings.appendKeyValue(sb, "Replication Lag", rls.getReplicationLag());
+    }
+
+    return sb.toString();
+  }
+
+  /**
+   * sinkToString
+   * @return a string contains sinkReplicationLoad information
+   */
+  public String sinkToString() {
+    if (this.replicationLoadSink == null) return null;
+
+    StringBuilder sb = new StringBuilder();
+    sb =
+        Strings.appendKeyValue(sb, "AgeOfLastAppliedOp",
+          this.replicationLoadSink.getAgeOfLastAppliedOp());
+    sb =
+        Strings.appendKeyValue(sb, "TimeStampsOfLastAppliedOp",
+          (new Date(this.replicationLoadSink.getTimeStampsOfLastAppliedOp()).toString()));
+
+    return sb.toString();
+  }
+
+  public ClusterStatusProtos.ReplicationLoadSink getReplicationLoadSink() {
+    return this.replicationLoadSink;
+  }
+
+  public List<ClusterStatusProtos.ReplicationLoadSource> getReplicationLoadSourceList() {
+    return this.replicationLoadSourceList;
+  }
+
+  /**
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    return this.sourceToString() + System.getProperty("line.separator") + this.sinkToString();
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
index 9a60131..3276418 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java
@@ -254,4 +254,12 @@ public class ReplicationSink {
       "age in ms of last applied edit: " + this.metrics.refreshAgeOfLastAppliedOp() +
       ", total replicated edits: " + this.totalReplicatedEdits;
   }
+
+  /**
+   * Get replication Sink Metrics
+   * @return MetricsSink
+   */
+  public MetricsSink getSinkMetrics() {
+    return this.metrics;
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
index ee43956..714080f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
@@ -869,4 +869,12 @@ public class ReplicationSource extends Thread
       ", currently replicating from: " + this.currentPath +
       " at position: " + position;
   }
+
+  /**
+   * Get Replication Source Metrics
+   * @return sourceMetrics
+   */
+  public MetricsSource getSourceMetrics() {
+    return this.metrics;
+  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
index f0db865..2dc3c89 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
@@ -31,11 +31,15 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
+import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerLoad;
+import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
@@ -556,4 +560,45 @@ public class TestReplicationSmallTests extends TestReplicationBase {
     hadmin.close();
   }
 
+  /**
+   * Test for HBASE-9531
+   * put a few rows into htable1, which should be replicated to htable2
+   * create a ClusterStatus instance 'status' from HBaseAdmin
+   * test : status.getLoad(server).getReplicationLoadSourceList()
+   * test : status.getLoad(server).getReplicationLoadSink()
+   * * @throws Exception
+   */
+  @Test(timeout = 300000)
+  public void testReplicationStatus() throws Exception {
+    LOG.info("testReplicationStatus");
+
+    try (Admin admin = utility1.getConnection().getAdmin()) {
+
+      final byte[] qualName = Bytes.toBytes("q");
+      Put p;
+
+      for (int i = 0; i < NB_ROWS_IN_BATCH; i++) {
+        p = new Put(Bytes.toBytes("row" + i));
+        p.add(famName, qualName, Bytes.toBytes("val" + i));
+        htable1.put(p);
+      }
+
+      ClusterStatus status = admin.getClusterStatus();
+
+      for (ServerName server : status.getServers()) {
+        ServerLoad sl = status.getLoad(server);
+        List<ReplicationLoadSource> rLoadSourceList = sl.getReplicationLoadSourceList();
+        ReplicationLoadSink rLoadSink = sl.getReplicationLoadSink();
+
+        // check SourceList has at least one entry
+        assertTrue("failed to get ReplicationLoadSourceList", (rLoadSourceList.size() > 0));
+
+        // check Sink exist only as it is difficult to verify the value on the fly
+        assertTrue("failed to get ReplicationLoadSink.AgeOfLastShippedOp ",
+          (rLoadSink.getAgeOfLastAppliedOp() >= 0));
+        assertTrue("failed to get ReplicationLoadSink.TimeStampsOfLastAppliedOp ",
+          (rLoadSink.getTimeStampsOfLastAppliedOp() >= 0));
+      }
+    }
+  }
 }
diff --git a/hbase-shell/src/main/ruby/hbase/admin.rb b/hbase-shell/src/main/ruby/hbase/admin.rb
index c0ea862..35ee36c 100644
--- a/hbase-shell/src/main/ruby/hbase/admin.rb
+++ b/hbase-shell/src/main/ruby/hbase/admin.rb
@@ -608,7 +608,7 @@ module Hbase
       end
     end
 
-    def status(format)
+    def status(format, type)
       status = @admin.getClusterStatus()
       if format == "detailed"
         puts("version %s" % [ status.getHBaseVersion() ])
@@ -635,6 +635,46 @@ module Hbase
         for server in status.getDeadServerNames()
           puts("    %s" % [ server ])
         end
+      elsif format == "replication"
+        #check whether replication is enabled or not
+        if (!@admin.getConfiguration().getBoolean(org.apache.hadoop.hbase.HConstants::REPLICATION_ENABLE_KEY, 
+          org.apache.hadoop.hbase.HConstants::REPLICATION_ENABLE_DEFAULT))
+          puts("Please enable replication first.")
+        else
+          puts("version %s" % [ status.getHBaseVersion() ])
+          puts("%d live servers" % [ status.getServersSize() ])
+          for server in status.getServers()
+            sl = status.getLoad(server)
+            rSinkString   = "       SINK  :"
+            rSourceString = "       SOURCE:"
+            rLoadSink = sl.getReplicationLoadSink()
+            rSinkString << " AgeOfLastAppliedOp=" + rLoadSink.getAgeOfLastAppliedOp().to_s
+            rSinkString << ", TimeStampsOfLastAppliedOp=" + 
+			    (java.util.Date.new(rLoadSink.getTimeStampsOfLastAppliedOp())).toString()
+            rLoadSourceList = sl.getReplicationLoadSourceList()
+            index = 0
+            while index < rLoadSourceList.size()
+              rLoadSource = rLoadSourceList.get(index)
+              rSourceString << " PeerID=" + rLoadSource.getPeerID()
+              rSourceString << ", AgeOfLastShippedOp=" + rLoadSource.getAgeOfLastShippedOp().to_s
+              rSourceString << ", SizeOfLogQueue=" + rLoadSource.getSizeOfLogQueue().to_s
+              rSourceString << ", TimeStampsOfLastShippedOp=" + 
+			      (java.util.Date.new(rLoadSource.getTimeStampOfLastShippedOp())).toString()
+              rSourceString << ", Replication Lag=" + rLoadSource.getReplicationLag().to_s
+              index = index + 1
+            end
+            puts("    %s:" %
+            [ server.getHostname() ])
+            if type.casecmp("SOURCE") == 0
+              puts("%s" % rSourceString)
+            elsif type.casecmp("SINK") == 0
+              puts("%s" % rSinkString)
+            else
+              puts("%s" % rSourceString)
+              puts("%s" % rSinkString)
+            end
+          end
+        end
       elsif format == "simple"
         load = 0
         regions = 0
diff --git a/hbase-shell/src/main/ruby/shell/commands/status.rb b/hbase-shell/src/main/ruby/shell/commands/status.rb
index f72c13c..b22b272 100644
--- a/hbase-shell/src/main/ruby/shell/commands/status.rb
+++ b/hbase-shell/src/main/ruby/shell/commands/status.rb
@@ -22,18 +22,21 @@ module Shell
     class Status < Command
       def help
         return <<-EOF
-Show cluster status. Can be 'summary', 'simple', or 'detailed'. The
+Show cluster status. Can be 'summary', 'simple', 'detailed', or 'replication'. The
 default is 'summary'. Examples:
 
   hbase> status
   hbase> status 'simple'
   hbase> status 'summary'
   hbase> status 'detailed'
+  hbase> status 'replication'
+  hbase> status 'replication', 'source'
+  hbase> status 'replication', 'sink'
 EOF
       end
 
-      def command(format = 'summary')
-        admin.status(format)
+      def command(format = 'summary',type = 'both')
+        admin.status(format, type)
       end
     end
   end
diff --git a/hbase-shell/src/test/ruby/hbase/admin_test.rb b/hbase-shell/src/test/ruby/hbase/admin_test.rb
index caede3a..1925864 100644
--- a/hbase-shell/src/test/ruby/hbase/admin_test.rb
+++ b/hbase-shell/src/test/ruby/hbase/admin_test.rb
@@ -356,5 +356,17 @@ module Hbase
       assert_not_equal(nil, table)
       table.close
     end
+
+    define_test "Get replication status" do
+      replication_status("replication", "both")
+    end
+
+    define_test "Get replication source metrics information" do
+      replication_status("replication", "source")
+    end
+
+    define_test "Get replication sink metrics information" do
+      replication_status("replication", "sink")
+    end
   end
 end
diff --git a/hbase-shell/src/test/ruby/test_helper.rb b/hbase-shell/src/test/ruby/test_helper.rb
index 5579761..5dfafc5 100644
--- a/hbase-shell/src/test/ruby/test_helper.rb
+++ b/hbase-shell/src/test/ruby/test_helper.rb
@@ -94,6 +94,10 @@ module Hbase
         puts "IGNORING DROP TABLE ERROR: #{e}"
       end
     end
+
+    def replication_status(format,type)
+      return admin.status(format,type)
+    end
   end
 end
 
-- 
1.9.2.msysgit.0

