diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
index 1d4c63d..6977c7a 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -27,6 +27,7 @@ import java.util.regex.Pattern;
 import org.apache.commons.lang.ArrayUtils;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.Bytes;
 
 /**
  * HConstants holds a bunch of HBase-related constants
@@ -101,6 +102,8 @@ public final class HConstants {
   /** default host address */
   public static final String DEFAULT_HOST = "0.0.0.0";
 
+  public static final String UNKNOWN_RACK = "Unknown Rack";
+
   /** Parameter name for port master listens on. */
   public static final String MASTER_PORT = "hbase.master.port";
 
@@ -121,6 +124,12 @@ public final class HConstants {
 
   /** Name of ZooKeeper quorum configuration parameter. */
   public static final String ZOOKEEPER_QUORUM = "hbase.zookeeper.quorum";
+  
+  /** The number of favored nodes for each region */
+  public static final int FAVORED_NODES_NUM = 3;
+
+  /** The favored nodes column qualifier*/
+  public static final byte [] FAVOREDNODES_QUALIFIER = Bytes.toBytes("favorednodes");
 
   /** Name of ZooKeeper config file in conf/ directory. */
   public static final String ZOOKEEPER_CONFIG_NAME = "zoo.cfg";
diff --git a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
index 73434c1..b9623bb 100644
--- a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
+++ b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/AdminProtos.java
@@ -3118,6 +3118,16 @@ public final class AdminProtos {
       // optional uint32 versionOfOfflineNode = 2;
       boolean hasVersionOfOfflineNode();
       int getVersionOfOfflineNode();
+      
+      // repeated .ServerName favoredNodes = 3;
+      java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> 
+          getFavoredNodesList();
+      org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getFavoredNodes(int index);
+      int getFavoredNodesCount();
+      java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
+          getFavoredNodesOrBuilderList();
+      org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getFavoredNodesOrBuilder(
+          int index);
     }
     public static final class RegionOpenInfo extends
         com.google.protobuf.GeneratedMessage
@@ -3171,9 +3181,31 @@ public final class AdminProtos {
         return versionOfOfflineNode_;
       }
       
+      // repeated .ServerName favoredNodes = 3;
+      public static final int FAVOREDNODES_FIELD_NUMBER = 3;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> favoredNodes_;
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getFavoredNodesList() {
+        return favoredNodes_;
+      }
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
+          getFavoredNodesOrBuilderList() {
+        return favoredNodes_;
+      }
+      public int getFavoredNodesCount() {
+        return favoredNodes_.size();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getFavoredNodes(int index) {
+        return favoredNodes_.get(index);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getFavoredNodesOrBuilder(
+          int index) {
+        return favoredNodes_.get(index);
+      }
+      
       private void initFields() {
         region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionInfo.getDefaultInstance();
         versionOfOfflineNode_ = 0;
+        favoredNodes_ = java.util.Collections.emptyList();
       }
       private byte memoizedIsInitialized = -1;
       public final boolean isInitialized() {
@@ -3188,6 +3220,12 @@ public final class AdminProtos {
           memoizedIsInitialized = 0;
           return false;
         }
+        for (int i = 0; i < getFavoredNodesCount(); i++) {
+          if (!getFavoredNodes(i).isInitialized()) {
+            memoizedIsInitialized = 0;
+            return false;
+          }
+        }
         memoizedIsInitialized = 1;
         return true;
       }
@@ -3201,6 +3239,9 @@ public final class AdminProtos {
         if (((bitField0_ & 0x00000002) == 0x00000002)) {
           output.writeUInt32(2, versionOfOfflineNode_);
         }
+        for (int i = 0; i < favoredNodes_.size(); i++) {
+          output.writeMessage(3, favoredNodes_.get(i));
+        }
         getUnknownFields().writeTo(output);
       }
       
@@ -3218,6 +3259,10 @@ public final class AdminProtos {
           size += com.google.protobuf.CodedOutputStream
             .computeUInt32Size(2, versionOfOfflineNode_);
         }
+        for (int i = 0; i < favoredNodes_.size(); i++) {
+          size += com.google.protobuf.CodedOutputStream
+            .computeMessageSize(3, favoredNodes_.get(i));
+        }
         size += getUnknownFields().getSerializedSize();
         memoizedSerializedSize = size;
         return size;
@@ -3251,6 +3296,8 @@ public final class AdminProtos {
           result = result && (getVersionOfOfflineNode()
               == other.getVersionOfOfflineNode());
         }
+        result = result && getFavoredNodesList()
+            .equals(other.getFavoredNodesList());
         result = result &&
             getUnknownFields().equals(other.getUnknownFields());
         return result;
@@ -3268,6 +3315,10 @@ public final class AdminProtos {
           hash = (37 * hash) + VERSIONOFOFFLINENODE_FIELD_NUMBER;
           hash = (53 * hash) + getVersionOfOfflineNode();
         }
+        if (getFavoredNodesCount() > 0) {
+          hash = (37 * hash) + FAVOREDNODES_FIELD_NUMBER;
+          hash = (53 * hash) + getFavoredNodesList().hashCode();
+        }
         hash = (29 * hash) + getUnknownFields().hashCode();
         return hash;
       }
@@ -3377,6 +3428,7 @@ public final class AdminProtos {
         private void maybeForceBuilderInitialization() {
           if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
             getRegionFieldBuilder();
+            getFavoredNodesFieldBuilder();
           }
         }
         private static Builder create() {
@@ -3393,6 +3445,12 @@ public final class AdminProtos {
           bitField0_ = (bitField0_ & ~0x00000001);
           versionOfOfflineNode_ = 0;
           bitField0_ = (bitField0_ & ~0x00000002);
+          if (favoredNodesBuilder_ == null) {
+            favoredNodes_ = java.util.Collections.emptyList();
+            bitField0_ = (bitField0_ & ~0x00000004);
+          } else {
+            favoredNodesBuilder_.clear();
+          }
           return this;
         }
         
@@ -3443,6 +3501,15 @@ public final class AdminProtos {
             to_bitField0_ |= 0x00000002;
           }
           result.versionOfOfflineNode_ = versionOfOfflineNode_;
+          if (favoredNodesBuilder_ == null) {
+            if (((bitField0_ & 0x00000004) == 0x00000004)) {
+              favoredNodes_ = java.util.Collections.unmodifiableList(favoredNodes_);
+              bitField0_ = (bitField0_ & ~0x00000004);
+            }
+            result.favoredNodes_ = favoredNodes_;
+          } else {
+            result.favoredNodes_ = favoredNodesBuilder_.build();
+          }
           result.bitField0_ = to_bitField0_;
           onBuilt();
           return result;
@@ -3465,6 +3532,32 @@ public final class AdminProtos {
           if (other.hasVersionOfOfflineNode()) {
             setVersionOfOfflineNode(other.getVersionOfOfflineNode());
           }
+          if (favoredNodesBuilder_ == null) {
+            if (!other.favoredNodes_.isEmpty()) {
+              if (favoredNodes_.isEmpty()) {
+                favoredNodes_ = other.favoredNodes_;
+                bitField0_ = (bitField0_ & ~0x00000004);
+              } else {
+                ensureFavoredNodesIsMutable();
+                favoredNodes_.addAll(other.favoredNodes_);
+              }
+              onChanged();
+            }
+          } else {
+            if (!other.favoredNodes_.isEmpty()) {
+              if (favoredNodesBuilder_.isEmpty()) {
+                favoredNodesBuilder_.dispose();
+                favoredNodesBuilder_ = null;
+                favoredNodes_ = other.favoredNodes_;
+                bitField0_ = (bitField0_ & ~0x00000004);
+                favoredNodesBuilder_ = 
+                  com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                     getFavoredNodesFieldBuilder() : null;
+              } else {
+                favoredNodesBuilder_.addAllMessages(other.favoredNodes_);
+              }
+            }
+          }
           this.mergeUnknownFields(other.getUnknownFields());
           return this;
         }
@@ -3478,6 +3571,12 @@ public final class AdminProtos {
             
             return false;
           }
+          for (int i = 0; i < getFavoredNodesCount(); i++) {
+            if (!getFavoredNodes(i).isInitialized()) {
+              
+              return false;
+            }
+          }
           return true;
         }
         
@@ -3518,6 +3617,12 @@ public final class AdminProtos {
                 versionOfOfflineNode_ = input.readUInt32();
                 break;
               }
+              case 26: {
+                org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.newBuilder();
+                input.readMessage(subBuilder, extensionRegistry);
+                addFavoredNodes(subBuilder.buildPartial());
+                break;
+              }
             }
           }
         }
@@ -3635,6 +3740,192 @@ public final class AdminProtos {
           return this;
         }
         
+        // repeated .ServerName favoredNodes = 3;
+        private java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> favoredNodes_ =
+          java.util.Collections.emptyList();
+        private void ensureFavoredNodesIsMutable() {
+          if (!((bitField0_ & 0x00000004) == 0x00000004)) {
+            favoredNodes_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName>(favoredNodes_);
+            bitField0_ |= 0x00000004;
+           }
+        }
+        
+        private com.google.protobuf.RepeatedFieldBuilder<
+            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> favoredNodesBuilder_;
+        
+        public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> getFavoredNodesList() {
+          if (favoredNodesBuilder_ == null) {
+            return java.util.Collections.unmodifiableList(favoredNodes_);
+          } else {
+            return favoredNodesBuilder_.getMessageList();
+          }
+        }
+        public int getFavoredNodesCount() {
+          if (favoredNodesBuilder_ == null) {
+            return favoredNodes_.size();
+          } else {
+            return favoredNodesBuilder_.getCount();
+          }
+        }
+        public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName getFavoredNodes(int index) {
+          if (favoredNodesBuilder_ == null) {
+            return favoredNodes_.get(index);
+          } else {
+            return favoredNodesBuilder_.getMessage(index);
+          }
+        }
+        public Builder setFavoredNodes(
+            int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
+          if (favoredNodesBuilder_ == null) {
+            if (value == null) {
+              throw new NullPointerException();
+            }
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.set(index, value);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.setMessage(index, value);
+          }
+          return this;
+        }
+        public Builder setFavoredNodes(
+            int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
+          if (favoredNodesBuilder_ == null) {
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.set(index, builderForValue.build());
+            onChanged();
+          } else {
+            favoredNodesBuilder_.setMessage(index, builderForValue.build());
+          }
+          return this;
+        }
+        public Builder addFavoredNodes(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
+          if (favoredNodesBuilder_ == null) {
+            if (value == null) {
+              throw new NullPointerException();
+            }
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.add(value);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.addMessage(value);
+          }
+          return this;
+        }
+        public Builder addFavoredNodes(
+            int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName value) {
+          if (favoredNodesBuilder_ == null) {
+            if (value == null) {
+              throw new NullPointerException();
+            }
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.add(index, value);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.addMessage(index, value);
+          }
+          return this;
+        }
+        public Builder addFavoredNodes(
+            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
+          if (favoredNodesBuilder_ == null) {
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.add(builderForValue.build());
+            onChanged();
+          } else {
+            favoredNodesBuilder_.addMessage(builderForValue.build());
+          }
+          return this;
+        }
+        public Builder addFavoredNodes(
+            int index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder builderForValue) {
+          if (favoredNodesBuilder_ == null) {
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.add(index, builderForValue.build());
+            onChanged();
+          } else {
+            favoredNodesBuilder_.addMessage(index, builderForValue.build());
+          }
+          return this;
+        }
+        public Builder addAllFavoredNodes(
+            java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> values) {
+          if (favoredNodesBuilder_ == null) {
+            ensureFavoredNodesIsMutable();
+            super.addAll(values, favoredNodes_);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.addAllMessages(values);
+          }
+          return this;
+        }
+        public Builder clearFavoredNodes() {
+          if (favoredNodesBuilder_ == null) {
+            favoredNodes_ = java.util.Collections.emptyList();
+            bitField0_ = (bitField0_ & ~0x00000004);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.clear();
+          }
+          return this;
+        }
+        public Builder removeFavoredNodes(int index) {
+          if (favoredNodesBuilder_ == null) {
+            ensureFavoredNodesIsMutable();
+            favoredNodes_.remove(index);
+            onChanged();
+          } else {
+            favoredNodesBuilder_.remove(index);
+          }
+          return this;
+        }
+        public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder getFavoredNodesBuilder(
+            int index) {
+          return getFavoredNodesFieldBuilder().getBuilder(index);
+        }
+        public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder getFavoredNodesOrBuilder(
+            int index) {
+          if (favoredNodesBuilder_ == null) {
+            return favoredNodes_.get(index);  } else {
+            return favoredNodesBuilder_.getMessageOrBuilder(index);
+          }
+        }
+        public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
+             getFavoredNodesOrBuilderList() {
+          if (favoredNodesBuilder_ != null) {
+            return favoredNodesBuilder_.getMessageOrBuilderList();
+          } else {
+            return java.util.Collections.unmodifiableList(favoredNodes_);
+          }
+        }
+        public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addFavoredNodesBuilder() {
+          return getFavoredNodesFieldBuilder().addBuilder(
+              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
+        }
+        public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder addFavoredNodesBuilder(
+            int index) {
+          return getFavoredNodesFieldBuilder().addBuilder(
+              index, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.getDefaultInstance());
+        }
+        public java.util.List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder> 
+             getFavoredNodesBuilderList() {
+          return getFavoredNodesFieldBuilder().getBuilderList();
+        }
+        private com.google.protobuf.RepeatedFieldBuilder<
+            org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder> 
+            getFavoredNodesFieldBuilder() {
+          if (favoredNodesBuilder_ == null) {
+            favoredNodesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+                org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerNameOrBuilder>(
+                    favoredNodes_,
+                    ((bitField0_ & 0x00000004) == 0x00000004),
+                    getParentForChildren(),
+                    isClean());
+            favoredNodes_ = null;
+          }
+          return favoredNodesBuilder_;
+        }
+        
         // @@protoc_insertion_point(builder_scope:OpenRegionRequest.RegionOpenInfo)
       }
       
@@ -16433,71 +16724,72 @@ public final class AdminProtos {
       "er\022\016\n\006family\030\002 \003(\014\")\n\024GetStoreFileRespon",
       "se\022\021\n\tstoreFile\030\001 \003(\t\"\030\n\026GetOnlineRegion" +
       "Request\":\n\027GetOnlineRegionResponse\022\037\n\nre" +
-      "gionInfo\030\001 \003(\0132\013.RegionInfo\"\225\001\n\021OpenRegi" +
+      "gionInfo\030\001 \003(\0132\013.RegionInfo\"\270\001\n\021OpenRegi" +
       "onRequest\0223\n\010openInfo\030\001 \003(\0132!.OpenRegion" +
-      "Request.RegionOpenInfo\032K\n\016RegionOpenInfo" +
+      "Request.RegionOpenInfo\032n\n\016RegionOpenInfo" +
       "\022\033\n\006region\030\001 \002(\0132\013.RegionInfo\022\034\n\024version" +
-      "OfOfflineNode\030\002 \001(\r\"\234\001\n\022OpenRegionRespon" +
-      "se\022<\n\014openingState\030\001 \003(\0162&.OpenRegionRes" +
-      "ponse.RegionOpeningState\"H\n\022RegionOpenin" +
-      "gState\022\n\n\006OPENED\020\000\022\022\n\016ALREADY_OPENED\020\001\022\022",
-      "\n\016FAILED_OPENING\020\002\"\232\001\n\022CloseRegionReques" +
-      "t\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\034\n\024v" +
-      "ersionOfClosingNode\030\002 \001(\r\022\034\n\016transitionI" +
-      "nZK\030\003 \001(\010:\004true\022&\n\021destinationServer\030\004 \001" +
-      "(\0132\013.ServerName\"%\n\023CloseRegionResponse\022\016" +
-      "\n\006closed\030\001 \002(\010\"M\n\022FlushRegionRequest\022 \n\006" +
-      "region\030\001 \002(\0132\020.RegionSpecifier\022\025\n\rifOlde" +
-      "rThanTs\030\002 \001(\004\"=\n\023FlushRegionResponse\022\025\n\r" +
-      "lastFlushTime\030\001 \002(\004\022\017\n\007flushed\030\002 \001(\010\"J\n\022" +
-      "SplitRegionRequest\022 \n\006region\030\001 \002(\0132\020.Reg",
-      "ionSpecifier\022\022\n\nsplitPoint\030\002 \001(\014\"\025\n\023Spli" +
-      "tRegionResponse\"W\n\024CompactRegionRequest\022" +
-      " \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\r\n\005maj" +
-      "or\030\002 \001(\010\022\016\n\006family\030\003 \001(\014\"\027\n\025CompactRegio" +
-      "nResponse\"1\n\004UUID\022\024\n\014leastSigBits\030\001 \002(\004\022" +
-      "\023\n\013mostSigBits\030\002 \002(\004\"\270\003\n\010WALEntry\022\035\n\003key" +
-      "\030\001 \002(\0132\020.WALEntry.WALKey\022\037\n\004edit\030\002 \002(\0132\021" +
-      ".WALEntry.WALEdit\032~\n\006WALKey\022\031\n\021encodedRe" +
-      "gionName\030\001 \002(\014\022\021\n\ttableName\030\002 \002(\014\022\031\n\021log" +
-      "SequenceNumber\030\003 \002(\004\022\021\n\twriteTime\030\004 \002(\004\022",
-      "\030\n\tclusterId\030\005 \001(\0132\005.UUID\032\353\001\n\007WALEdit\022\025\n" +
-      "\rkeyValueBytes\030\001 \003(\014\0222\n\013familyScope\030\002 \003(" +
-      "\0132\035.WALEntry.WALEdit.FamilyScope\032M\n\013Fami" +
-      "lyScope\022\016\n\006family\030\001 \002(\014\022.\n\tscopeType\030\002 \002" +
-      "(\0162\033.WALEntry.WALEdit.ScopeType\"F\n\tScope" +
-      "Type\022\033\n\027REPLICATION_SCOPE_LOCAL\020\000\022\034\n\030REP" +
-      "LICATION_SCOPE_GLOBAL\020\001\"4\n\030ReplicateWALE" +
-      "ntryRequest\022\030\n\005entry\030\001 \003(\0132\t.WALEntry\"\033\n" +
-      "\031ReplicateWALEntryResponse\"\026\n\024RollWALWri" +
-      "terRequest\".\n\025RollWALWriterResponse\022\025\n\rr",
-      "egionToFlush\030\001 \003(\014\"#\n\021StopServerRequest\022" +
-      "\016\n\006reason\030\001 \002(\t\"\024\n\022StopServerResponse\"\026\n" +
-      "\024GetServerInfoRequest\"@\n\nServerInfo\022\037\n\ns" +
-      "erverName\030\001 \002(\0132\013.ServerName\022\021\n\twebuiPor" +
-      "t\030\002 \001(\r\"8\n\025GetServerInfoResponse\022\037\n\nserv" +
-      "erInfo\030\001 \002(\0132\013.ServerInfo2\371\005\n\014AdminServi" +
-      "ce\022>\n\rgetRegionInfo\022\025.GetRegionInfoReque" +
-      "st\032\026.GetRegionInfoResponse\022;\n\014getStoreFi" +
-      "le\022\024.GetStoreFileRequest\032\025.GetStoreFileR" +
-      "esponse\022D\n\017getOnlineRegion\022\027.GetOnlineRe",
-      "gionRequest\032\030.GetOnlineRegionResponse\0225\n" +
-      "\nopenRegion\022\022.OpenRegionRequest\032\023.OpenRe" +
-      "gionResponse\0228\n\013closeRegion\022\023.CloseRegio" +
-      "nRequest\032\024.CloseRegionResponse\0228\n\013flushR" +
-      "egion\022\023.FlushRegionRequest\032\024.FlushRegion" +
-      "Response\0228\n\013splitRegion\022\023.SplitRegionReq" +
-      "uest\032\024.SplitRegionResponse\022>\n\rcompactReg" +
-      "ion\022\025.CompactRegionRequest\032\026.CompactRegi" +
-      "onResponse\022J\n\021replicateWALEntry\022\031.Replic" +
-      "ateWALEntryRequest\032\032.ReplicateWALEntryRe",
-      "sponse\022>\n\rrollWALWriter\022\025.RollWALWriterR" +
-      "equest\032\026.RollWALWriterResponse\022>\n\rgetSer" +
-      "verInfo\022\025.GetServerInfoRequest\032\026.GetServ" +
-      "erInfoResponse\0225\n\nstopServer\022\022.StopServe" +
-      "rRequest\032\023.StopServerResponseBA\n*org.apa" +
-      "che.hadoop.hbase.protobuf.generatedB\013Adm" +
-      "inProtosH\001\210\001\001\240\001\001"
+      "OfOfflineNode\030\002 \001(\r\022!\n\014favoredNodes\030\003 \003(" +
+      "\0132\013.ServerName\"\234\001\n\022OpenRegionResponse\022<\n" +
+      "\014openingState\030\001 \003(\0162&.OpenRegionResponse" +
+      ".RegionOpeningState\"H\n\022RegionOpeningStat",
+      "e\022\n\n\006OPENED\020\000\022\022\n\016ALREADY_OPENED\020\001\022\022\n\016FAI" +
+      "LED_OPENING\020\002\"\232\001\n\022CloseRegionRequest\022 \n\006" +
+      "region\030\001 \002(\0132\020.RegionSpecifier\022\034\n\024versio" +
+      "nOfClosingNode\030\002 \001(\r\022\034\n\016transitionInZK\030\003" +
+      " \001(\010:\004true\022&\n\021destinationServer\030\004 \001(\0132\013." +
+      "ServerName\"%\n\023CloseRegionResponse\022\016\n\006clo" +
+      "sed\030\001 \002(\010\"M\n\022FlushRegionRequest\022 \n\006regio" +
+      "n\030\001 \002(\0132\020.RegionSpecifier\022\025\n\rifOlderThan" +
+      "Ts\030\002 \001(\004\"=\n\023FlushRegionResponse\022\025\n\rlastF" +
+      "lushTime\030\001 \002(\004\022\017\n\007flushed\030\002 \001(\010\"J\n\022Split",
+      "RegionRequest\022 \n\006region\030\001 \002(\0132\020.RegionSp" +
+      "ecifier\022\022\n\nsplitPoint\030\002 \001(\014\"\025\n\023SplitRegi" +
+      "onResponse\"W\n\024CompactRegionRequest\022 \n\006re" +
+      "gion\030\001 \002(\0132\020.RegionSpecifier\022\r\n\005major\030\002 " +
+      "\001(\010\022\016\n\006family\030\003 \001(\014\"\027\n\025CompactRegionResp" +
+      "onse\"1\n\004UUID\022\024\n\014leastSigBits\030\001 \002(\004\022\023\n\013mo" +
+      "stSigBits\030\002 \002(\004\"\270\003\n\010WALEntry\022\035\n\003key\030\001 \002(" +
+      "\0132\020.WALEntry.WALKey\022\037\n\004edit\030\002 \002(\0132\021.WALE" +
+      "ntry.WALEdit\032~\n\006WALKey\022\031\n\021encodedRegionN" +
+      "ame\030\001 \002(\014\022\021\n\ttableName\030\002 \002(\014\022\031\n\021logSeque",
+      "nceNumber\030\003 \002(\004\022\021\n\twriteTime\030\004 \002(\004\022\030\n\tcl" +
+      "usterId\030\005 \001(\0132\005.UUID\032\353\001\n\007WALEdit\022\025\n\rkeyV" +
+      "alueBytes\030\001 \003(\014\0222\n\013familyScope\030\002 \003(\0132\035.W" +
+      "ALEntry.WALEdit.FamilyScope\032M\n\013FamilySco" +
+      "pe\022\016\n\006family\030\001 \002(\014\022.\n\tscopeType\030\002 \002(\0162\033." +
+      "WALEntry.WALEdit.ScopeType\"F\n\tScopeType\022" +
+      "\033\n\027REPLICATION_SCOPE_LOCAL\020\000\022\034\n\030REPLICAT" +
+      "ION_SCOPE_GLOBAL\020\001\"4\n\030ReplicateWALEntryR" +
+      "equest\022\030\n\005entry\030\001 \003(\0132\t.WALEntry\"\033\n\031Repl" +
+      "icateWALEntryResponse\"\026\n\024RollWALWriterRe",
+      "quest\".\n\025RollWALWriterResponse\022\025\n\rregion" +
+      "ToFlush\030\001 \003(\014\"#\n\021StopServerRequest\022\016\n\006re" +
+      "ason\030\001 \002(\t\"\024\n\022StopServerResponse\"\026\n\024GetS" +
+      "erverInfoRequest\"@\n\nServerInfo\022\037\n\nserver" +
+      "Name\030\001 \002(\0132\013.ServerName\022\021\n\twebuiPort\030\002 \001" +
+      "(\r\"8\n\025GetServerInfoResponse\022\037\n\nserverInf" +
+      "o\030\001 \002(\0132\013.ServerInfo2\371\005\n\014AdminService\022>\n" +
+      "\rgetRegionInfo\022\025.GetRegionInfoRequest\032\026." +
+      "GetRegionInfoResponse\022;\n\014getStoreFile\022\024." +
+      "GetStoreFileRequest\032\025.GetStoreFileRespon",
+      "se\022D\n\017getOnlineRegion\022\027.GetOnlineRegionR" +
+      "equest\032\030.GetOnlineRegionResponse\0225\n\nopen" +
+      "Region\022\022.OpenRegionRequest\032\023.OpenRegionR" +
+      "esponse\0228\n\013closeRegion\022\023.CloseRegionRequ" +
+      "est\032\024.CloseRegionResponse\0228\n\013flushRegion" +
+      "\022\023.FlushRegionRequest\032\024.FlushRegionRespo" +
+      "nse\0228\n\013splitRegion\022\023.SplitRegionRequest\032" +
+      "\024.SplitRegionResponse\022>\n\rcompactRegion\022\025" +
+      ".CompactRegionRequest\032\026.CompactRegionRes" +
+      "ponse\022J\n\021replicateWALEntry\022\031.ReplicateWA",
+      "LEntryRequest\032\032.ReplicateWALEntryRespons" +
+      "e\022>\n\rrollWALWriter\022\025.RollWALWriterReques" +
+      "t\032\026.RollWALWriterResponse\022>\n\rgetServerIn" +
+      "fo\022\025.GetServerInfoRequest\032\026.GetServerInf" +
+      "oResponse\0225\n\nstopServer\022\022.StopServerRequ" +
+      "est\032\023.StopServerResponseBA\n*org.apache.h" +
+      "adoop.hbase.protobuf.generatedB\013AdminPro" +
+      "tosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -16565,7 +16857,7 @@ public final class AdminProtos {
           internal_static_OpenRegionRequest_RegionOpenInfo_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_OpenRegionRequest_RegionOpenInfo_descriptor,
-              new java.lang.String[] { "Region", "VersionOfOfflineNode", },
+              new java.lang.String[] { "Region", "VersionOfOfflineNode", "FavoredNodes", },
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest.RegionOpenInfo.class,
               org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionRequest.RegionOpenInfo.Builder.class);
           internal_static_OpenRegionResponse_descriptor =
diff --git a/hbase-protocol/src/main/protobuf/Admin.proto b/hbase-protocol/src/main/protobuf/Admin.proto
index b7d8549..67af320 100644
--- a/hbase-protocol/src/main/protobuf/Admin.proto
+++ b/hbase-protocol/src/main/protobuf/Admin.proto
@@ -69,6 +69,7 @@ message OpenRegionRequest {
   message RegionOpenInfo {
     required RegionInfo region = 1;
     optional uint32 versionOfOfflineNode = 2;
+    repeated ServerName favoredNodes = 3;
   }
 }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
index 466ee58..4fbfbc5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java
@@ -23,6 +23,7 @@ import java.net.ConnectException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Map;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -35,7 +36,9 @@ import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.master.RegionPlacement;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 
 /**
  * Writes region and assignment information to <code>.META.</code>.
@@ -58,6 +61,22 @@ public class MetaEditor {
     addRegionInfo(put, regionInfo);
     return put;
   }
+  /**
+   * Generates and returns a Put containing the region info for the catalog table
+   * and the servers
+   */
+  public static Put makePutFromRegionInfo(HRegionInfo regionInfo, List<ServerName>favoriteNodeList)
+  throws IOException {
+    Put put = makePutFromRegionInfo(regionInfo);
+    if (favoriteNodeList != null) {
+      String favoredNodes = RegionPlacement.getFavoredNodes(favoriteNodeList);
+      put.add(HConstants.CATALOG_FAMILY, HConstants.FAVOREDNODES_QUALIFIER,
+          EnvironmentEdgeManager.currentTimeMillis(), favoredNodes.getBytes());
+      LOG.info("Create the region " + regionInfo.getRegionNameAsString() +
+          " with favored nodes " + favoredNodes);
+    }
+    return put;
+  }
 
   /**
    * Adds split daughters to the Put
@@ -236,11 +255,15 @@ public class MetaEditor {
    * @throws IOException if problem connecting or updating meta
    */
   public static void addRegionsToMeta(CatalogTracker catalogTracker,
-      List<HRegionInfo> regionInfos)
+      List<HRegionInfo> regionInfos, final Map<HRegionInfo, List<ServerName>> assignmentMap)
   throws IOException {
     List<Put> puts = new ArrayList<Put>();
     for (HRegionInfo regionInfo : regionInfos) {
-      puts.add(makePutFromRegionInfo(regionInfo));
+      if (assignmentMap != null) {
+        puts.add(makePutFromRegionInfo(regionInfo, assignmentMap.get(regionInfo)));
+      } else {
+        puts.add(makePutFromRegionInfo(regionInfo));
+      }
     }
     putsToMetaTable(catalogTracker, puts);
     LOG.info("Added " + puts.size() + " regions in META");
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentDomain.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentDomain.java
new file mode 100644
index 0000000..4323361
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentDomain.java
@@ -0,0 +1,208 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.ServerName;
+
+public class AssignmentDomain {
+  protected static final Log LOG =
+    LogFactory.getLog(AssignmentDomain.class.getClass());
+  private Map<String, List<ServerName>> rackToRegionServerMap;
+  private List<String> uniqueRackList;
+  private RackManager rackManager;
+  private Map<ServerName, String> regionServerToRackMap;
+  private Random random;
+
+  public AssignmentDomain(Configuration conf) {
+    rackToRegionServerMap = new HashMap<String, List<ServerName>>();
+    regionServerToRackMap = new HashMap<ServerName, String>();
+    uniqueRackList = new ArrayList<String>();
+    rackManager = new RackManager(conf);
+    random = new Random();
+  }
+
+  /**
+   * Set the random seed
+   * @param seed
+   */
+  public void setRandomSeed(long seed) {
+    random.setSeed(seed);
+  }
+
+  /**
+   * Get the rack name in this domain for the server.
+   * @param server
+   * @return
+   */
+  public String getRack(ServerName server) {
+    if (server == null)
+      return null;
+    return regionServerToRackMap.get(server);
+  }
+
+  /**
+   * Get a random rack except for the current rack
+   * @param skipRackSet
+   * @return the random rack except for any Rack from the skipRackSet
+   * @throws IOException
+   */
+  public String getOneRandomRack(Set<String> skipRackSet) throws IOException {
+    if (skipRackSet == null || uniqueRackList.size() <= skipRackSet.size()) {
+      throw new IOException("Cannot randomly pick another random server");
+    }
+
+    String randomRack;
+    do {
+      int randomIndex = random.nextInt(this.uniqueRackList.size());
+      randomRack = this.uniqueRackList.get(randomIndex);
+    } while (skipRackSet.contains(randomRack));
+
+    return randomRack;
+  }
+
+  /**
+   * Get one random server from the rack
+   * @param rack
+   * @return
+   * @throws IOException
+   */
+  public ServerName getOneRandomServer(String rack) throws IOException {
+    return this.getOneRandomServer(rack, null);
+  }
+
+  /**
+   * Get a random server from the rack except for the servers in the skipServerSet
+   * @param skipServerSet
+   * @return the random server except for any servers from the skipServerSet
+   * @throws IOException
+   */
+  public ServerName getOneRandomServer(String rack,
+      Set<ServerName> skipServerSet) throws IOException {
+    if(rack == null) return null;
+    List<ServerName> serverList = this.rackToRegionServerMap.get(rack);
+    if (serverList == null) return null;
+
+    // Get a random server except for any servers from the skip set
+    if (skipServerSet != null && serverList.size() <= skipServerSet.size()) {
+      throw new IOException("Cannot randomly pick another random server");
+    }
+
+    ServerName randomServer;
+    do {
+      int randomIndex = random.nextInt(serverList.size());
+      randomServer = serverList.get(randomIndex);
+    } while (skipServerSet != null && skipServerSet.contains(randomServer));
+
+    return randomServer;
+  }
+
+  /**
+   * @return the total number of unique rack in the domain.
+   */
+  public int getTotalRackNum() {
+    return this.uniqueRackList.size();
+  }
+
+  /**
+   * Get the list of region severs in the rack
+   * @param rack
+   * @return the list of region severs in the rack
+   */
+  public List<ServerName> getServersFromRack(String rack) {
+    return this.rackToRegionServerMap.get(rack);
+  }
+
+  /**
+   * Add a server to the assignment domain
+   * @param server
+   */
+  public void addServer(ServerName server) {
+    // For a new server
+    String rackName = this.rackManager.getRack(server);
+    List<ServerName> serverList = this.rackToRegionServerMap.get(rackName);
+    if (serverList == null) {
+      serverList = new ArrayList<ServerName>();
+      // Add the current rack to the unique rack list
+      this.uniqueRackList.add(rackName);
+    }
+    if (!serverList.contains(server)) {
+      serverList.add(server);
+      this.rackToRegionServerMap.put(rackName, serverList);
+      this.regionServerToRackMap.put(server, rackName);
+    }
+  }
+
+  /**
+   * Add a list of servers to the assignment domain
+   * @param servers
+   */
+  public void addServers(List<ServerName> servers) {
+    for (ServerName server : servers) {
+      this.addServer(server);
+    }
+  }
+
+  public Set<ServerName> getAllServers() {
+    return regionServerToRackMap.keySet();
+  }
+  
+  /**
+   * Get the region server to rack map
+   */
+  public Map<ServerName, String> getRegionServerToRackMap() {
+    return this.regionServerToRackMap;
+  }
+
+  /**
+   * Get the rack to region server map
+   */
+  public Map<String, List<ServerName>> getRackToRegionServerMap() {
+    return this.rackToRegionServerMap;
+  }
+
+  /**
+   * @return true if there is no rack in the assignment domain
+   */
+  public boolean isEmpty() {
+    return uniqueRackList.isEmpty();
+  }
+
+  /**
+   * @return true if can place the favored nodes
+   */
+  public boolean canPlaceFavoredNodes() {
+    int serverSize = this.regionServerToRackMap.keySet().size();
+    if (serverSize < HConstants.FAVORED_NODES_NUM)
+      return false;
+    return true;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 36dc2fd..7eede8f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -72,6 +72,7 @@ import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.KeyLocker;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
+import org.apache.hadoop.hbase.util.Triple;
 import org.apache.hadoop.hbase.zookeeper.RootRegionTracker;
 import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.hadoop.hbase.zookeeper.ZKTable;
@@ -1342,7 +1343,7 @@ public class AssignmentManager extends ZooKeeperListener {
    * @return true if successful
    */
   boolean assign(final ServerName destination,
-      final List<HRegionInfo> regions) {
+      final List<HRegionInfo> regions, final Map<HRegionInfo, List<ServerName>> assignmentMap) {
     int regionCount = regions.size();
     if (regionCount == 0) {
       return true;
@@ -1401,8 +1402,8 @@ public class AssignmentManager extends ZooKeeperListener {
       // that unnecessary timeout on RIT is reduced.
       this.addPlans(plans);
 
-      List<Pair<HRegionInfo, Integer>> regionOpenInfos =
-        new ArrayList<Pair<HRegionInfo, Integer>>(states.size());
+      List<Triple<HRegionInfo, Integer, List<ServerName>>> regionOpenInfos =
+        new ArrayList<Triple<HRegionInfo, Integer, List<ServerName>>>(states.size());
       for (RegionState state: states) {
         HRegionInfo region = state.getRegion();
         String encodedRegionName = region.getEncodedName();
@@ -1415,8 +1416,8 @@ public class AssignmentManager extends ZooKeeperListener {
         } else {
           regionStates.updateRegionState(region,
             RegionState.State.PENDING_OPEN, destination);
-          regionOpenInfos.add(new Pair<HRegionInfo, Integer>(
-            region, nodeVersion));
+          regionOpenInfos.add(new Triple<HRegionInfo, Integer, List<ServerName>>(
+            region, nodeVersion, assignmentMap.get(region)));
         }
       }
 
@@ -2203,9 +2204,42 @@ public class AssignmentManager extends ZooKeeperListener {
     assign(regions.size(), servers.size(),
       "round-robin=true", bulkPlan);
   }
+  
+  public void assign(List<HRegionInfo> regions, 
+      final Map<HRegionInfo, List<ServerName>> assignmentMap) 
+          throws InterruptedException, IOException {
+    if (assignmentMap == null) {
+      assign(regions);
+    }
+    if (regions == null || regions.isEmpty()) {
+      return;
+    }
+
+    Map <ServerName, List<HRegionInfo>> plan = convertAssignmentMapToAssignmentPlan(assignmentMap);
+
+    assign(regions.size(), plan.size(),
+        "favored-nodes=true", plan, assignmentMap);
+  }
+
+  private Map<ServerName, List<HRegionInfo>> convertAssignmentMapToAssignmentPlan(
+      final Map<HRegionInfo, List<ServerName>> assignmentMap) {
+    Map <ServerName, List<HRegionInfo>> plan = new HashMap<ServerName, List<HRegionInfo>>();
+
+    for (Map.Entry<HRegionInfo, List<ServerName>> entry : assignmentMap.entrySet()) {
+      List<HRegionInfo> regionsForServer;
+      if ((regionsForServer = plan.get(entry.getValue().get(0))) == null) {
+        regionsForServer = new ArrayList<HRegionInfo>();
+        //the 0'th element in the list of servers is the primary RS for a region
+        plan.put(entry.getValue().get(0), regionsForServer); 
+      }
+      regionsForServer.add(entry.getKey()); 
+    }
+    return plan;
+  }
 
   private void assign(int regions, int totalServers,
-      String message, Map<ServerName, List<HRegionInfo>> bulkPlan)
+      String message, Map<ServerName, List<HRegionInfo>> bulkPlan,
+      Map<HRegionInfo, List<ServerName>> assignmentMap)
           throws InterruptedException, IOException {
 
     int servers = bulkPlan.size();
@@ -2216,9 +2250,10 @@ public class AssignmentManager extends ZooKeeperListener {
       // cluster, especially mini cluster for testing, so that tests won't time out
       LOG.info("Not use bulk assigning since we are assigning only "
         + regions + " region(s) to " + servers + " server(s)");
-
       for (Map.Entry<ServerName, List<HRegionInfo>> plan: bulkPlan.entrySet()) {
-        assign(plan.getKey(), plan.getValue());
+        Map<HRegionInfo, List<ServerName>> subAssignmentMap = 
+            getFavoredNodesForRegions(plan.getValue(), assignmentMap);
+        assign(plan.getKey(), plan.getValue(), subAssignmentMap);
       }
     } else {
       LOG.info("Bulk assigning " + regions + " region(s) across "
@@ -2226,12 +2261,29 @@ public class AssignmentManager extends ZooKeeperListener {
 
       // Use fixed count thread pool assigning.
       BulkAssigner ba = new GeneralBulkAssigner(
-        this.server, bulkPlan, this, bulkAssignWaitTillAllAssigned);
+        this.server, bulkPlan, this, bulkAssignWaitTillAllAssigned, assignmentMap);
       ba.bulkAssign();
       LOG.info("Bulk assigning done");
     }
   }
 
+  /** Return the favored nodes map for the regions hosted on the passed servername */
+  static Map<HRegionInfo, List<ServerName>> getFavoredNodesForRegions(List<HRegionInfo> regions,
+      Map<HRegionInfo, List<ServerName>> assignmentMap) {
+    Map<HRegionInfo, List<ServerName>> subAssignmentMap = 
+        new HashMap<HRegionInfo, List<ServerName>>();
+    for (HRegionInfo region : regions) {
+      subAssignmentMap.put(region, assignmentMap.get(region));
+    }
+    return subAssignmentMap;
+  }
+
+  private void assign(int regions, int totalServers,
+      String message, Map<ServerName, List<HRegionInfo>> bulkPlan)
+          throws InterruptedException, IOException {
+    assign(regions, totalServers, message, bulkPlan, null);
+  }
+
   /**
    * Assigns all user regions, if any exist.  Used during cluster startup.
    * <p>
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentPlan.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentPlan.java
new file mode 100644
index 0000000..5e01254
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentPlan.java
@@ -0,0 +1,224 @@
+/**
+ * Copyright 2012 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.util.Pair;
+
+/**
+ * AssignmentPlan is a writable object for the region assignment plan.
+ * It contains the mapping information between each region and
+ * its favored region server list.
+ *
+ * All the access to this class is thread-safe.
+ */
+public class AssignmentPlan {
+  protected static final Log LOG = LogFactory.getLog(
+      AssignmentPlan.class.getName());
+
+
+  /** the map between each region and its favored region server list */
+  private Map<HRegionInfo, List<ServerName>> assignmentMap;
+
+  /** the map between each region and its lasted favored server list update
+   * time stamp
+  */
+  private Map<HRegionInfo, Long> assignmentUpdateTS;
+
+  public static enum POSITION {
+    PRIMARY,
+    SECONDARY,
+    TERTIARY;
+  };
+
+  public AssignmentPlan() {
+    assignmentMap = new HashMap<HRegionInfo, List<ServerName>>();
+    assignmentUpdateTS = new HashMap<HRegionInfo, Long>();
+  }
+
+  /**
+   * Initialize the assignment plan with the existing primary region server map
+   * and the existing secondary/tertiary region server map
+   *
+   * if any regions cannot find the proper secondary / tertiary region server
+   * for whatever reason, just do NOT update the assignment plan for this region
+   * @param primaryRSMap
+   * @param secondaryAndTiteraryRSMap
+   */
+  public void initialize(Map<HRegionInfo, ServerName> primaryRSMap,
+      Map<HRegionInfo, Pair<ServerName, ServerName>> secondaryAndTertiaryRSMap) {
+    for (Map.Entry<HRegionInfo, Pair<ServerName, ServerName>> entry :
+      secondaryAndTertiaryRSMap.entrySet()) {
+      // Get the region info and their secondary/tertiary region server
+      HRegionInfo regionInfo = entry.getKey();
+      Pair<ServerName, ServerName> secondaryAndTertiaryPair =
+        entry.getValue();
+
+      // Get the primary region server
+      ServerName primaryRS = primaryRSMap.get(regionInfo);
+      if (primaryRS == null) {
+        LOG.error("No primary region server for region " +
+            regionInfo.getRegionNameAsString());
+        continue;
+      }
+
+      // Update the assignment plan with the favored nodes
+      List<ServerName> serverList = new ArrayList<ServerName>();
+      serverList.add(POSITION.PRIMARY.ordinal(), primaryRS);
+      serverList.add(POSITION.SECONDARY.ordinal(),
+          secondaryAndTertiaryPair.getFirst());
+      serverList.add(POSITION.TERTIARY.ordinal(),
+          secondaryAndTertiaryPair.getSecond());
+      this.updateAssignmentPlan(regionInfo, serverList);
+    }
+  }
+
+  /**
+   * Add an assignment to the plan
+   * @param region
+   * @param servers
+   * @param ts
+   */
+  public synchronized void updateAssignmentPlan(HRegionInfo region,
+      List<ServerName> servers, long ts) {
+    if (region == null || servers == null || servers.size() ==0)
+      return;
+    this.assignmentUpdateTS.put(region, Long.valueOf(ts));
+    this.assignmentMap.put(region, servers);
+    LOG.info("Update the assignment plan for region " +
+        region.getRegionNameAsString() + " to favored nodes " +
+        RegionPlacement.getFavoredNodes(servers)
+        + " at time stamp " + ts);
+  }
+
+  /**
+   * Add an assignment to the plan
+   * @param region
+   * @param servers
+   */
+  public synchronized void updateAssignmentPlan(HRegionInfo region,
+      List<ServerName> servers) {
+    if (region == null || servers == null || servers.size() ==0)
+      return;
+    this.assignmentMap.put(region, servers);
+    LOG.info("Update the assignment plan for region " +
+        region.getRegionNameAsString() + " ; favored nodes " +
+        RegionPlacement.getFavoredNodes(servers));
+  }
+
+  /**
+   * Remove one assignment from the plan
+   * @param region
+   */
+  public synchronized void removeAssignment(HRegionInfo region) {
+    this.assignmentMap.remove(region);
+    this.assignmentUpdateTS.remove(region);
+  }
+
+  /**
+   * @param region
+   * @return true if there is an assignment plan for the particular region.
+   */
+  public synchronized boolean hasAssignment(HRegionInfo region) {
+    return assignmentMap.containsKey(region);
+  }
+
+  /**
+   * @param region
+   * @return the list of favored region server for this region based on the plan
+   */
+  public synchronized List<ServerName> getAssignment(HRegionInfo region) {
+    return assignmentMap.get(region);
+  }
+
+  /**
+   * @param region
+   * @return the last update time stamp for the region in the plan
+   */
+  public synchronized long getAssignmentUpdateTS(HRegionInfo region) {
+    Long updateTS = assignmentUpdateTS.get(region);
+    if (updateTS == null)
+      return Long.MIN_VALUE;
+    else
+      return updateTS.longValue();
+  }
+
+  /**
+   * @return the mapping between each region to its favored region server list
+   */
+  public synchronized Map<HRegionInfo, List<ServerName>> getAssignmentMap() {
+    return this.assignmentMap;
+  }
+
+ @Override
+ public boolean equals(Object o) {
+   if (this == o) {
+     return true;
+   }
+   if (o == null) {
+     return false;
+   }
+   if (getClass() != o.getClass()) {
+     return false;
+   }
+   // To compare the map from objec o is identical to current assignment map.
+   Map<HRegionInfo, List<ServerName>> comparedMap=
+     ((AssignmentPlan)o).getAssignmentMap();
+
+   // compare the size
+   if (comparedMap.size() != this.assignmentMap.size())
+     return false;
+
+   // compare each element in the assignment map
+   for (Map.Entry<HRegionInfo, List<ServerName>> entry :
+     comparedMap.entrySet()) {
+     List<ServerName> serverList = this.assignmentMap.get(entry.getKey());
+     if (serverList == null && entry.getValue() != null) {
+       return false;
+     } else if (!serverList.equals(entry.getValue())) {
+       return false;
+     }
+   }
+   return true;
+ }
+
+ public static AssignmentPlan.POSITION getFavoredServerPosition(
+     List<ServerName> favoredNodes, ServerName server) {
+   if (favoredNodes == null || server == null ||
+       favoredNodes.size() != HConstants.FAVORED_NODES_NUM) {
+     return null;
+   }
+   for (AssignmentPlan.POSITION p : AssignmentPlan.POSITION.values()) {
+     if (favoredNodes.get(p.ordinal()).equals(server)) {
+       return p;
+     }
+   }
+   return null;
+ }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
index ab82cda..87be116 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/GeneralBulkAssigner.java
@@ -50,15 +50,18 @@ public class GeneralBulkAssigner extends BulkAssigner {
 
   final Map<ServerName, List<HRegionInfo>> bulkPlan;
   final AssignmentManager assignmentManager;
+  final Map<HRegionInfo, List<ServerName>> assignmentMap;
   final boolean waitTillAllAssigned;
 
   GeneralBulkAssigner(final Server server,
       final Map<ServerName, List<HRegionInfo>> bulkPlan,
-      final AssignmentManager am, final boolean waitTillAllAssigned) {
+      final AssignmentManager am, final boolean waitTillAllAssigned, 
+      final Map<HRegionInfo, List<ServerName>> assignmentMap) {
     super(server);
     this.bulkPlan = bulkPlan;
     this.assignmentManager = am;
     this.waitTillAllAssigned = waitTillAllAssigned;
+    this.assignmentMap = assignmentMap;
   }
 
   @Override
@@ -70,8 +73,10 @@ public class GeneralBulkAssigner extends BulkAssigner {
   protected void populatePool(ExecutorService pool) {
     this.pool = pool; // shut it down later in case some assigner hangs
     for (Map.Entry<ServerName, List<HRegionInfo>> e: this.bulkPlan.entrySet()) {
+      Map<HRegionInfo, List<ServerName>> subAssignmentMap = 
+          AssignmentManager.getFavoredNodesForRegions(e.getValue(), assignmentMap);
       pool.execute(new SingleServerBulkAssigner(e.getKey(), e.getValue(),
-        this.assignmentManager, this.failedPlans));
+        this.assignmentManager, this.failedPlans, subAssignmentMap));
     }
   }
 
@@ -209,20 +214,23 @@ public class GeneralBulkAssigner extends BulkAssigner {
     private final List<HRegionInfo> regions;
     private final AssignmentManager assignmentManager;
     private final Map<ServerName, List<HRegionInfo>> failedPlans;
+    private final Map<HRegionInfo, List<ServerName>> assignmentMap;
 
     SingleServerBulkAssigner(final ServerName regionserver,
         final List<HRegionInfo> regions, final AssignmentManager am,
-        final Map<ServerName, List<HRegionInfo>> failedPlans) {
+        final Map<ServerName, List<HRegionInfo>> failedPlans,
+        final Map<HRegionInfo, List<ServerName>> assignmentMap) {
       this.regionserver = regionserver;
       this.regions = regions;
       this.assignmentManager = am;
       this.failedPlans = failedPlans;
+      this.assignmentMap = assignmentMap;
     }
 
     @Override
     public void run() {
       try {
-       if (!assignmentManager.assign(regionserver, regions)) {
+       if (!assignmentManager.assign(regionserver, regions, assignmentMap)) {
          failedPlans.put(regionserver, regions);
        }
       } catch (Throwable t) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index c074799..c9f2f54 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -30,6 +30,7 @@ import java.util.Comparator;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Random;
 import java.util.Set;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
@@ -84,6 +85,7 @@ import org.apache.hadoop.hbase.ipc.HBaseServerRPC;
 import org.apache.hadoop.hbase.ipc.RpcServer;
 import org.apache.hadoop.hbase.ipc.ServerRpcController;
 import org.apache.hadoop.hbase.ipc.UnknownProtocolException;
+import org.apache.hadoop.hbase.master.balancer.AssignmentLoadBalancer;
 import org.apache.hadoop.hbase.master.balancer.BalancerChore;
 import org.apache.hadoop.hbase.master.balancer.ClusterStatusChore;
 import org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory;
@@ -299,6 +301,7 @@ Server {
   private volatile boolean initialized = false;
   // flag set after we complete assignRootAndMeta.
   private volatile boolean serverShutdownHandlerEnabled = false;
+  public RegionPlacementPolicy regionPlacement;
 
   // Instance of the hbase executor service.
   ExecutorService executorService;
@@ -429,6 +432,8 @@ Server {
     if (isHealthCheckerConfigured()) {
       healthCheckChore = new HealthCheckChore(sleepTime, this, getConfiguration());
     }
+
+    regionPlacement = new RegionPlacement(conf);
   }
 
   /**
@@ -616,6 +621,8 @@ Server {
   // Check if we should stop every 100ms
   private Sleeper stopSleeper = new Sleeper(100, this);
 
+  private boolean shouldAssignRegionsWithFavoredNodes;
+
   private void loop() {
     long lastMsgTs = 0l;
     long now = 0l;
@@ -693,6 +700,10 @@ Server {
 
     status.setStatus("Initializing ZK system trackers");
     initializeZKBasedSystemTrackers();
+    
+    // Only read favored nodes if using the assignment-based load balancer.
+    this.shouldAssignRegionsWithFavoredNodes = balancer.getClass().equals(
+        AssignmentLoadBalancer.class);
 
     if (!masterRecovery) {
       // initialize master side coprocessors before we start handling requests
@@ -1516,10 +1527,23 @@ Server {
     if (cpHost != null) {
       cpHost.preCreateTable(hTableDescriptor, newRegions);
     }
-
+    AssignmentPlan assignmentPlan = null;
+    String tableName = hTableDescriptor.getNameAsString();
+    if (this.shouldAssignRegionsWithFavoredNodes) {
+      // Get the assignment domain for this table
+      AssignmentDomain domain = this.getAssignmentDomain(tableName);
+      // Get the assignment plan for the new regions
+      assignmentPlan =
+        regionPlacement.getNewAssignmentPlan(newRegions, domain);
+    }
+    if (assignmentPlan != null) {
+      LOG.info("Generated the assignment plan for new table " + tableName);
+    } else {
+      LOG.info("NO assignment plan for new table " + tableName);
+    }
     this.executorService.submit(new CreateTableHandler(this,
       this.fileSystemManager, hTableDescriptor, conf,
-      newRegions, this).prepare());
+      newRegions, this, assignmentManager, assignmentPlan.getAssignmentMap()).prepare());
     if (cpHost != null) {
       cpHost.postCreateTable(hTableDescriptor, newRegions);
     }
@@ -1580,6 +1604,31 @@ Server {
            Bytes.equals(tableName, HConstants.META_TABLE_NAME);
   }
 
+  /**
+   * Get the assignment domain for the table.
+   * Currently the domain would be generated by shuffling all the online
+   * region servers.
+   * 
+   * It would be easy to extend for the multi-tenancy in the future.
+   * @param tableName
+   * @return the assignment domain for the table.
+   */
+  private AssignmentDomain getAssignmentDomain(String tableName) {
+    // Get all the online region servers
+    List<ServerName> onlineRSList =
+      this.serverManager.createDestinationServersList();
+
+    // Shuffle the server list based on the tableName
+    Random random = new Random(tableName.hashCode());
+    Collections.shuffle(onlineRSList, random);
+
+    // Add the shuffled server list into the assignment domain
+    AssignmentDomain domain = new AssignmentDomain(this.conf);
+    domain.addServers(onlineRSList);
+
+    return domain;
+  }
+
   @Override
   public void deleteTable(final byte[] tableName) throws IOException {
     checkInitialized();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java
new file mode 100644
index 0000000..d9597d6
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RackManager.java
@@ -0,0 +1,43 @@
+package org.apache.hadoop.hbase.master;
+
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.util.ReflectionUtils;
+import org.apache.hadoop.net.DNSToSwitchMapping;
+import org.apache.hadoop.net.ScriptBasedMapping;
+
+public class RackManager {
+  static final Log LOG = LogFactory.getLog(RackManager.class);
+  private DNSToSwitchMapping switchMapping;
+
+  public RackManager(Configuration conf) {
+    switchMapping = ReflectionUtils.instantiateWithCustomCtor(
+        conf.getClass("hbase.util.ip.to.rack.determiner", ScriptBasedMapping.class,
+             DNSToSwitchMapping.class).getName(), new Class<?>[]{Configuration.class},new Object[]{conf});
+  }
+
+  /**
+   * Get the name of the rack containing a server, according to the DNS to
+   * switch mapping.
+   * @param server the server for which to get the rack name
+   * @return the rack name of the server
+   */
+  public String getRack(ServerName server) {
+    if (server == null)
+      return HConstants.UNKNOWN_RACK;
+
+    List<String> racks = switchMapping.resolve(Arrays.asList(
+        new String[]{server.getHostAndPort()}));
+    if (racks != null && racks.size() > 0) {
+      return racks.get(0);
+    }
+
+    return HConstants.UNKNOWN_RACK;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionAssignmentSnapshot.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionAssignmentSnapshot.java
new file mode 100644
index 0000000..ae85625
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionAssignmentSnapshot.java
@@ -0,0 +1,202 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.MetaScanner;
+import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class RegionAssignmentSnapshot {
+  private static final Log LOG = LogFactory.getLog(RegionAssignmentSnapshot.class
+      .getName());
+
+  private Configuration conf;
+
+  /** the table name to region map */
+  private final Map<String, List<HRegionInfo>> tableToRegionMap;
+  /** the region to region server map */
+  private final Map<HRegionInfo, ServerName> regionToRegionServerMap;
+  /** the region name to region info map */
+  private final Map<String, HRegionInfo> regionNameToRegionInfoMap;
+
+  /** the regionServer to region map */
+  private final Map<ServerName, List<HRegionInfo>> regionServerToRegionMap;
+  /** the existing assignment plan in the META region */
+  private final AssignmentPlan existingAssignmentPlan;
+  /** The rack view for the current region server */
+  private final AssignmentDomain globalAssignmentDomain;
+
+  public RegionAssignmentSnapshot(Configuration conf) {
+    this.conf = conf;
+    tableToRegionMap = new HashMap<String, List<HRegionInfo>>();
+    regionToRegionServerMap = new HashMap<HRegionInfo, ServerName>();
+    regionServerToRegionMap = new HashMap<ServerName, List<HRegionInfo>>();
+    regionNameToRegionInfoMap = new TreeMap<String, HRegionInfo>();
+    existingAssignmentPlan = new AssignmentPlan();
+    globalAssignmentDomain = new AssignmentDomain(conf);
+  }
+
+  /**
+   * Initialize the region assignment snapshot by scanning the META table
+   * @throws IOException
+   */
+  public void initialize() throws IOException {
+    LOG.info("Start to scan the META for the current region assignment " +
+		"snappshot");
+
+    // Add all the online region servers
+    HBaseAdmin admin  = new HBaseAdmin(conf);
+    Collection<ServerName> servers = admin.getClusterStatus().getServers();
+    for (ServerName serverInfo : servers) {
+      globalAssignmentDomain.addServer(serverInfo);
+    }
+    
+    MetaScannerVisitor visitor = new MetaScannerVisitor() {
+      public boolean processRow(Result result) throws IOException {
+        try {
+          byte[] region = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.REGIONINFO_QUALIFIER);
+          byte[] server = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.SERVER_QUALIFIER);
+          byte[] startCode = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.STARTCODE_QUALIFIER);
+          // Process the region info
+          if (region == null) return true;
+          HRegionInfo regionInfo = HRegionInfo.getHRegionInfo(result);
+          if (regionInfo == null || regionInfo.isSplit()) {
+            return true;
+          }
+          addRegion(regionInfo);
+
+          // Process the region server
+          if (server == null) return true;
+          ServerName regionServer = new ServerName(Bytes.toString(server), 
+              Bytes.toLong(startCode));
+
+          // Add the current assignment to the snapshot
+          addAssignment(regionInfo, regionServer);
+
+          // Process the assignment plan
+          byte[] favoredNodes = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.FAVOREDNODES_QUALIFIER);
+          if (favoredNodes == null) return true;
+          // Add the favored nodes into assignment plan
+          List<ServerName> favoredServerList =
+            RegionPlacement.getFavoredNodesList(favoredNodes);
+          existingAssignmentPlan.updateAssignmentPlan(regionInfo,
+              favoredServerList);
+          return true;
+        } catch (RuntimeException e) {
+          LOG.error("Catche remote exception " + e.getMessage() +
+              " when processing" + result);
+          throw e;
+        }
+      }
+
+      @Override
+      public void close() throws IOException {
+        // TODO Auto-generated method stub
+        
+      }
+    };
+
+    // Scan .META. to pick up user regions
+    MetaScanner.metaScan(conf, visitor);
+    LOG.info("Finished to scan the META for the current region assignment" +
+      "snapshot");
+  }
+
+  private void addRegion(HRegionInfo regionInfo) {
+    if (regionInfo == null)
+      return;
+    // Process the region name to region info map
+    regionNameToRegionInfoMap.put(regionInfo.getRegionNameAsString(), regionInfo);
+
+    // Process the table to region map
+    String tableName = regionInfo.getTableNameAsString();
+    List<HRegionInfo> regionList = tableToRegionMap.get(tableName);
+    if (regionList == null) {
+      regionList = new ArrayList<HRegionInfo>();
+    }
+    // Add the current region info into the tableToRegionMap
+    regionList.add(regionInfo);
+    tableToRegionMap.put(tableName, regionList);
+  }
+
+  private void addAssignment(HRegionInfo regionInfo, ServerName server) {
+    if (server != null && regionInfo != null) {
+      // Process the region to region server map
+      regionToRegionServerMap.put(regionInfo, server);
+
+      // Process the region server to region map
+      List<HRegionInfo> regionList = regionServerToRegionMap.get(server);
+      if (regionList == null) {
+        regionList = new ArrayList<HRegionInfo>();
+      }
+      regionList.add(regionInfo);
+      regionServerToRegionMap.put(server, regionList);
+    }
+  }
+
+  public Map<String, HRegionInfo> getRegionNameToRegionInfoMap() {
+    return this.regionNameToRegionInfoMap;
+  }
+
+  public Map<String, List<HRegionInfo>> getTableToRegionMap() {
+    return tableToRegionMap;
+  }
+
+  public Map<HRegionInfo, ServerName> getRegionToRegionServerMap() {
+    return regionToRegionServerMap;
+  }
+
+  public Map<ServerName, List<HRegionInfo>> getRegionServerToRegionMap() {
+    return regionServerToRegionMap;
+  }
+
+  public AssignmentPlan getExistingAssignmentPlan() {
+    return this.existingAssignmentPlan;
+  }
+
+  public AssignmentDomain getGlobalAssignmentDomain() {
+    return this.globalAssignmentDomain;
+  }
+
+  public Set<String> getTableSet() {
+    return this.tableToRegionMap.keySet();
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacement.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacement.java
new file mode 100644
index 0000000..e02433d
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacement.java
@@ -0,0 +1,526 @@
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+
+public class RegionPlacement implements RegionPlacementPolicy{
+  private static final Log LOG = LogFactory.getLog(RegionPlacement.class
+      .getName());
+
+  final static String SERVER_NAME_SEPARATOR = ";";
+  private Configuration conf;
+
+  public RegionPlacement(Configuration conf) throws IOException {
+    this.conf = conf;
+  }
+
+  @Override
+  public AssignmentPlan getNewAssignmentPlan(HRegionInfo[] regions,
+      AssignmentDomain domain) throws IOException {
+    if (regions == null || regions.length == 0 ||
+        domain == null || domain.isEmpty() || !domain.canPlaceFavoredNodes())
+      return null;
+
+    try {
+      // Place the primary region server based on the regions and servers
+      Map<HRegionInfo, ServerName> primaryRSMap =
+        this.placePrimaryRSAsRoundRobin(regions, domain);
+
+      // Place the secondary and tertiary region server
+      Map<HRegionInfo, Pair<ServerName, ServerName>>
+        secondaryAndTertiaryRSMap =
+        this.placeSecondaryAndTertiaryRS(primaryRSMap, domain);
+
+      // Get the assignment plan by initialization with the primaryRSMap and the
+      // secondaryAndTertiaryRSMap
+      AssignmentPlan plan = new AssignmentPlan();
+      plan.initialize(primaryRSMap, secondaryAndTertiaryRSMap);
+      return plan;
+    } catch (Exception e) {
+      LOG.debug("Cannot generate the assignment plan because " + e);
+      return null;
+    }
+  }
+
+  /**
+   * Place the primary region server in the round robin way.
+   * @param regions
+   * @param domain
+   * @return the map between regions and its primary region server
+   * @throws IOException
+   */
+  private Map<HRegionInfo, ServerName> placePrimaryRSAsRoundRobin(
+      HRegionInfo[] regions, AssignmentDomain domain) throws IOException {
+
+    // Get the rack to region server map from the assignment domain
+    Map<String, List<ServerName>> rackToRegionServerMap=
+      domain.getRackToRegionServerMap();
+
+    List<String> rackList = new ArrayList<String>();
+    rackList.addAll(rackToRegionServerMap.keySet());
+    Map<String, Integer> currentProcessIndexMap = new HashMap<String, Integer>();
+    int rackIndex = 0;
+
+    // Place the region with its primary region sever in a round robin way.
+    Map<HRegionInfo, ServerName> primaryRSMap =
+      new HashMap<HRegionInfo, ServerName>();
+    for (HRegionInfo regionInfo : regions) {
+      String rackName = rackList.get(rackIndex);
+      // Initialize the current processing host index.
+      int serverIndex = 0;
+
+      // Restore the current process index from the currentProcessIndexMap
+      Integer currentProcessIndex = currentProcessIndexMap.get(rackName);
+      if (currentProcessIndex != null) {
+        serverIndex = currentProcessIndex.intValue();
+      }
+      // Get the server list for the current rack
+      List<ServerName> currentServerList = rackToRegionServerMap.get(rackName);
+
+      // Get the current process region server
+      ServerName currentServer = currentServerList.get(serverIndex);
+
+      // Place the current region with the current primary region server
+      primaryRSMap.put(regionInfo, currentServer);
+
+      // Set the next processing index
+      if ((++serverIndex) >= currentServerList.size()) {
+        // Reset the server index for the current rack
+        serverIndex = 0;
+      }
+      // Keep track of the next processing index
+      currentProcessIndexMap.put(rackName, serverIndex);
+      if ((++rackIndex) >= rackList.size()) {
+        rackIndex = 0; // reset the rack index to 0
+      }
+    }
+
+    return primaryRSMap;
+  }
+
+  /**
+   * Place the secondary and tertiary region server. Best effort to place the
+   * secondary and tertiary into the different rack as the primary region server.
+   * Also best effort to place the secondary and tertiary into the same rack.
+   *
+   * There are more than 3 region server for the placement.
+   * @param primaryRSMap
+   * @param domain
+   * @return
+   * @throws IOException
+   */
+  private Map<HRegionInfo, Pair<ServerName,ServerName>> placeSecondaryAndTertiaryRS(
+      Map<HRegionInfo, ServerName> primaryRSMap, AssignmentDomain domain)
+      throws IOException {
+    Map<HRegionInfo, Pair<ServerName,ServerName>> secondaryAndTertiaryMap
+      = new HashMap<HRegionInfo, Pair<ServerName, ServerName>>();
+
+    for (Map.Entry<HRegionInfo, ServerName> entry : primaryRSMap.entrySet()) {
+      // Get the target region and its primary region server rack
+      HRegionInfo regionInfo = entry.getKey();
+      ServerName primaryRS = entry.getValue();
+
+      // Set the random seed in the assignment domain
+      domain.setRandomSeed(regionInfo.hashCode());
+      try {
+        // Create the secondary and tertiary region server pair object.
+        Pair<ServerName, ServerName> pair;
+        // Get the rack for the primary region server
+        String primaryRack = domain.getRack(primaryRS);
+
+        if (domain.getTotalRackNum() == 1) {
+          // Single rack case: have to pick the secondary and tertiary
+          // from the same rack
+          List<ServerName> serverList = domain.getServersFromRack(primaryRack);
+          if (serverList.size() <= 2) {
+            // Single region server case: cannot not place the favored nodes
+            // on any server; !domain.canPlaceFavoredNodes()
+            break;
+          } else {
+            // Randomly select two region servers from the server list and make sure
+            // they are not overlap with the primary region server;
+           Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+           serverSkipSet.add(primaryRS);
+
+           // Place the secondary RS
+           ServerName secondaryRS =
+             domain.getOneRandomServer(primaryRack, serverSkipSet);
+           // Skip the secondary for the tertiary placement
+           serverSkipSet.add(secondaryRS);
+
+           // Place the tertiary RS
+           ServerName tertiaryRS =
+             domain.getOneRandomServer(primaryRack, serverSkipSet);
+
+           if (secondaryRS == null || tertiaryRS == null) {
+             LOG.error("Cannot place the secondary and terinary" +
+                 "region server for region " +
+                 regionInfo.getRegionNameAsString());
+           }
+           // Create the secondary and tertiary pair
+           pair = new Pair<ServerName, ServerName>();
+           pair.setFirst(secondaryRS);
+           pair.setSecond(tertiaryRS);
+          }
+        } else {
+          // Random to choose the secondary and tertiary region server
+          // from another rack to place the secondary and tertiary
+
+          // Random to choose one rack except for the current rack
+          Set<String> rackSkipSet = new HashSet<String>();
+          rackSkipSet.add(primaryRack);
+          String secondaryRack = domain.getOneRandomRack(rackSkipSet);
+          List<ServerName> serverList = domain.getServersFromRack(secondaryRack);
+          if (serverList.size() >= 2) {
+            // Randomly pick up two servers from this secondary rack
+
+            // Place the secondary RS
+            ServerName secondaryRS =
+              domain.getOneRandomServer(secondaryRack);
+
+            // Skip the secondary for the tertiary placement
+            Set<ServerName> skipServerSet = new HashSet<ServerName>();
+            skipServerSet.add(secondaryRS);
+            // Place the tertiary RS
+            ServerName tertiaryRS =
+              domain.getOneRandomServer(secondaryRack, skipServerSet);
+
+            if (secondaryRS == null || tertiaryRS == null) {
+              LOG.error("Cannot place the secondary and terinary" +
+                  "region server for region " +
+                  regionInfo.getRegionNameAsString());
+            }
+            // Create the secondary and tertiary pair
+            pair = new Pair<ServerName, ServerName>();
+            pair.setFirst(secondaryRS);
+            pair.setSecond(tertiaryRS);
+          } else {
+            // Pick the secondary rs from this secondary rack
+            // and pick the tertiary from another random rack
+            pair = new Pair<ServerName, ServerName>();
+            ServerName secondary = domain.getOneRandomServer(secondaryRack);
+            pair.setFirst(secondary);
+
+            // Pick the tertiary
+            if (domain.getTotalRackNum() == 2) {
+              // Pick the tertiary from the same rack of the primary RS
+              Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+              serverSkipSet.add(primaryRS);
+              ServerName tertiary =
+                domain.getOneRandomServer(primaryRack, serverSkipSet);
+              pair.setSecond(tertiary);
+            } else {
+              // Pick the tertiary from another rack
+              rackSkipSet.add(secondaryRack);
+              String tertiaryRandomRack = domain.getOneRandomRack(rackSkipSet);
+              ServerName tertinary =
+                domain.getOneRandomServer(tertiaryRandomRack);
+              pair.setSecond(tertinary);
+            }
+          }
+        }
+        if (pair != null) {
+          secondaryAndTertiaryMap.put(regionInfo, pair);
+          LOG.debug("Place the secondary and tertiary region server for region "
+              + regionInfo.getRegionNameAsString());
+        }
+      } catch (Exception e) {
+        LOG.warn("Cannot place the favored nodes for region " +
+            regionInfo.getRegionNameAsString() + " because " + e);
+        continue;
+      }
+    }
+    return secondaryAndTertiaryMap;
+  }
+
+  // For regions that share the primary, avoid placing the secondary and tertiary on a same RS
+  public Map<HRegionInfo, Pair<ServerName, ServerName>> placeSecondaryAndTertiaryWithRestrictions(
+      Map<HRegionInfo, ServerName> primaryRSMap, AssignmentDomain domain)
+      throws IOException {
+    Map<ServerName, String> mapServerToRack = domain
+        .getRegionServerToRackMap();
+    Map<ServerName, Set<HRegionInfo>> serverToPrimaries =
+        mapRSToPrimaries(primaryRSMap);
+    Map<HRegionInfo, Pair<ServerName, ServerName>> secondaryAndTertiaryMap =
+        new HashMap<HRegionInfo, Pair<ServerName, ServerName>>();
+
+    for (Entry<HRegionInfo, ServerName> entry : primaryRSMap.entrySet()) {
+      // Get the target region and its primary region server rack
+      HRegionInfo regionInfo = entry.getKey();
+      ServerName primaryRS = entry.getValue();
+
+      // Set the random seed in the assignment domain
+      domain.setRandomSeed(regionInfo.hashCode());
+      try {
+        // Create the secondary and tertiary region server pair object.
+        Pair<ServerName, ServerName> pair;
+        // Get the rack for the primary region server
+        String primaryRack = domain.getRack(primaryRS);
+
+        if (domain.getTotalRackNum() == 1) {
+          // Single rack case: have to pick the secondary and tertiary
+          // from the same rack
+          List<ServerName> serverList = domain
+              .getServersFromRack(primaryRack);
+          if (serverList.size() <= 2) {
+            // Single region server case: cannot not place the favored nodes
+            // on any server; !domain.canPlaceFavoredNodes()
+            continue;
+          } else {
+            // Randomly select two region servers from the server list and make
+            // sure
+            // they are not overlap with the primary region server;
+            Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+            serverSkipSet.add(primaryRS);
+
+            // Place the secondary RS
+            ServerName secondaryRS = domain.getOneRandomServer(primaryRack,
+                serverSkipSet);
+            // Skip the secondary for the tertiary placement
+            serverSkipSet.add(secondaryRS);
+
+            // Place the tertiary RS
+            ServerName tertiaryRS = domain.getOneRandomServer(primaryRack,
+                serverSkipSet);
+
+            if (secondaryRS == null || tertiaryRS == null) {
+              LOG.error("Cannot place the secondary and terinary"
+                  + "region server for region "
+                  + regionInfo.getRegionNameAsString());
+            }
+            // Create the secondary and tertiary pair
+            pair = new Pair<ServerName, ServerName>();
+            pair.setFirst(secondaryRS);
+            pair.setSecond(tertiaryRS);
+          }
+        } else {
+          // Random to choose the secondary and tertiary region server
+          // from another rack to place the secondary and tertiary
+          // Random to choose one rack except for the current rack
+          Set<String> rackSkipSet = new HashSet<String>();
+          rackSkipSet.add(primaryRack);
+          String secondaryRack = domain.getOneRandomRack(rackSkipSet);
+          List<ServerName> serverList = domain
+              .getServersFromRack(secondaryRack);
+          Set<ServerName> serverSet = new HashSet<ServerName>();
+          serverSet.addAll(serverList);
+
+          if (serverList.size() >= 2) {
+
+            // Randomly pick up two servers from this secondary rack
+            // Skip the secondary for the tertiary placement
+            // skip the servers which share the primary already
+            Set<HRegionInfo> primaries = serverToPrimaries.get(primaryRS);
+            Set<ServerName> skipServerSet = new HashSet<ServerName>();
+            while (true) {
+              Pair<ServerName, ServerName> secondaryAndTertiary = null;
+              if (primaries.size() > 1) {
+                // check where his tertiary and secondary are
+                for (HRegionInfo primary : primaries) {
+                  secondaryAndTertiary = secondaryAndTertiaryMap.get(primary);
+                  if (secondaryAndTertiary != null) {
+                    if (mapServerToRack.get(secondaryAndTertiary.getFirst())
+                        .equals(secondaryRack)) {
+                      skipServerSet.add(secondaryAndTertiary.getFirst());
+                    }
+                    if (mapServerToRack.get(secondaryAndTertiary.getSecond())
+                        .equals(secondaryRack)) {
+                      skipServerSet.add(secondaryAndTertiary.getSecond());
+                    }
+                  }
+                }
+              }
+              if (skipServerSet.size() + 2 <= serverSet.size())
+                break;
+              skipServerSet.clear();
+              rackSkipSet.add(secondaryRack);
+              // we used all racks
+              if (rackSkipSet.size() == domain.getTotalRackNum()) {
+                // remove the last two added and break
+                skipServerSet.remove(secondaryAndTertiary.getFirst());
+                skipServerSet.remove(secondaryAndTertiary.getSecond());
+                break;
+              }
+              secondaryRack = domain.getOneRandomRack(rackSkipSet);
+              serverList = domain.getServersFromRack(secondaryRack);
+              serverSet = new HashSet<ServerName>();
+              serverSet.addAll(serverList);
+            }
+
+            // Place the secondary RS
+            ServerName secondaryRS = domain.getOneRandomServer(
+                secondaryRack, skipServerSet);
+            skipServerSet.add(secondaryRS);
+            // Place the tertiary RS
+            ServerName tertiaryRS = domain.getOneRandomServer(
+                secondaryRack, skipServerSet);
+
+            if (secondaryRS == null || tertiaryRS == null) {
+              LOG.error("Cannot place the secondary and tertiary"
+                  + " region server for region "
+                  + regionInfo.getRegionNameAsString());
+            }
+            // Create the secondary and tertiary pair
+            pair = new Pair<ServerName, ServerName>();
+            pair.setFirst(secondaryRS);
+            pair.setSecond(tertiaryRS);
+          } else {
+            // Pick the secondary rs from this secondary rack
+            // and pick the tertiary from another random rack
+            pair = new Pair<ServerName, ServerName>();
+            ServerName secondary = domain.getOneRandomServer(secondaryRack);
+            pair.setFirst(secondary);
+
+            // Pick the tertiary
+            if (domain.getTotalRackNum() == 2) {
+              // Pick the tertiary from the same rack of the primary RS
+              Set<ServerName> serverSkipSet = new HashSet<ServerName>();
+              serverSkipSet.add(primaryRS);
+              ServerName tertiary = domain.getOneRandomServer(primaryRack,
+                  serverSkipSet);
+              pair.setSecond(tertiary);
+            } else {
+              // Pick the tertiary from another rack
+              rackSkipSet.add(secondaryRack);
+              String tertiaryRandomRack = domain.getOneRandomRack(rackSkipSet);
+              ServerName tertinary = domain
+                  .getOneRandomServer(tertiaryRandomRack);
+              pair.setSecond(tertinary);
+            }
+          }
+        }
+        if (pair != null) {
+          secondaryAndTertiaryMap.put(regionInfo, pair);
+          LOG.debug("Place the secondary and tertiary region server for region "
+              + regionInfo.getRegionNameAsString());
+        }
+      } catch (Exception e) {
+        LOG.warn("Cannot place the favored nodes for region "
+            + regionInfo.getRegionNameAsString() + " because " + e);
+        continue;
+      }
+    }
+    return secondaryAndTertiaryMap;
+  }
+
+  public Map<ServerName, Set<HRegionInfo>> mapRSToPrimaries(
+      Map<HRegionInfo, ServerName> primaryRSMap) {
+    Map<ServerName, Set<HRegionInfo>> primaryServerMap =
+        new HashMap<ServerName, Set<HRegionInfo>>();
+    for (Entry<HRegionInfo, ServerName> e : primaryRSMap.entrySet()) {
+      Set<HRegionInfo> currentSet = primaryServerMap.get(e.getValue());
+      if (currentSet == null) {
+        currentSet = new HashSet<HRegionInfo>();
+      }
+      currentSet.add(e.getKey());
+      primaryServerMap.put(e.getValue(), currentSet);
+    }
+    return primaryServerMap;
+  }
+
+  /**
+   * @param serverList
+   * @return string the favoredNodes generated by the server list.
+   */
+  public static String getFavoredNodes(List<ServerName> serverAddrList) {
+    String favoredNodes = "";
+    if (serverAddrList != null) {
+      for (int i = 0 ; i < serverAddrList.size(); i++) {
+        favoredNodes += serverAddrList.get(i).getServerName();
+        if (i != serverAddrList.size() - 1 ) {
+          favoredNodes += SERVER_NAME_SEPARATOR;
+        }
+      }
+    }
+    return favoredNodes;
+  }
+
+  /**
+   * @param favoredNodes The bytes of favored nodes
+   * @return the list of HServerAddress for the byte array of favored nodes.
+   */
+  public static List<ServerName> getFavoredNodesList(byte[] favoredNodes) {
+    String favoredNodesStr = Bytes.toString(favoredNodes);
+    return getFavoredNodeList(favoredNodesStr);
+  }
+
+  /**
+   * @param favoredNodes The Stromg of favored nodes
+   * @return the list of HServerAddress for the byte array of favored nodes.
+   */
+  public static List<ServerName> getFavoredNodeList(String favoredNodesStr) {
+    String[] favoredNodesArray = StringUtils.split(favoredNodesStr, SERVER_NAME_SEPARATOR);
+    if (favoredNodesArray == null)
+      return null;
+
+    List<ServerName> serverList = new ArrayList<ServerName>();
+    for (String hostNameAndPort : favoredNodesArray) {
+      serverList.add(new ServerName(hostNameAndPort));
+    }
+    return serverList;
+  }
+  /**
+   * @param favoredNodes The byte array of the favored nodes
+   * @return string the favoredNodes generated by the byte array of favored nodes.
+   */
+  public static String getFavoredNodes(byte[] favoredNodes) {
+    List<ServerName> serverList = getFavoredNodesList(favoredNodes);
+    String favoredNodesStr = getFavoredNodes(serverList);
+    return favoredNodesStr;
+  }
+
+  @Override
+  public AssignmentPlan getExistingAssignmentPlan() throws IOException {
+    RegionAssignmentSnapshot snapshot = this.getRegionAssignmentSnapshot();
+    return snapshot.getExistingAssignmentPlan();
+  }
+
+  /**
+   * @return the new RegionAssignmentSnapshot
+   * @throws IOException
+   */
+  public RegionAssignmentSnapshot getRegionAssignmentSnapshot()
+  throws IOException {
+    RegionAssignmentSnapshot currentAssignmentShapshot =
+      new RegionAssignmentSnapshot(this.conf);
+    currentAssignmentShapshot.initialize();
+    return currentAssignmentShapshot;
+  }
+
+  /**
+   * Print the assignment plan to the system output stream
+   * @param plan
+   */
+  public static void printAssignmentPlan(AssignmentPlan plan) {
+    if (plan == null) return;
+    LOG.info("========== Start to print the assignment plan ================");
+    // sort the map based on region info
+    Map<HRegionInfo, List<ServerName>> assignmentMap =
+      new TreeMap<HRegionInfo, List<ServerName>>(plan.getAssignmentMap());
+
+    for (Map.Entry<HRegionInfo, List<ServerName>> entry :
+      assignmentMap.entrySet()) {
+      String serverList = RegionPlacement.getFavoredNodes(entry.getValue());
+      String regionName = entry.getKey().getRegionNameAsString();
+      LOG.info("Region: " + regionName );
+      LOG.info("Its favored nodes: " + serverList);
+    }
+    LOG.info("========== Finish to print the assignment plan ================");
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementPolicy.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementPolicy.java
new file mode 100644
index 0000000..4ba1f4a
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionPlacementPolicy.java
@@ -0,0 +1,26 @@
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.HRegionInfo;
+
+public interface RegionPlacementPolicy {
+
+/**
+ * Get the assignment plan for the new regions
+ * @param regions
+ * @param servers
+ * @return the favored assignment plan for the regions.
+ * @throws IOException
+ */
+  public AssignmentPlan getNewAssignmentPlan(HRegionInfo[] regions,
+      AssignmentDomain domain) throws IOException;
+
+  /**
+   * Get the existing assignment plan for all the regions
+   * @return the existing favored assignment plan for all the regions
+   * @throws IOException
+   */
+  public AssignmentPlan getExistingAssignmentPlan()
+  throws IOException;
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
index 50ee2a4..4da6b84 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ServerManager.java
@@ -61,6 +61,7 @@ import org.apache.hadoop.hbase.protobuf.generated.AdminProtos.OpenRegionResponse
 import org.apache.hadoop.hbase.regionserver.RegionOpeningState;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Triple;
 
 import com.google.protobuf.ServiceException;
 
@@ -619,7 +620,7 @@ public class ServerManager {
    * @return a list of region opening states
    */
   public List<RegionOpeningState> sendRegionOpen(ServerName server,
-      List<Pair<HRegionInfo, Integer>> regionOpenInfos)
+      List<Triple<HRegionInfo, Integer, List<ServerName>>> regionOpenInfos)
   throws IOException {
     AdminProtocol admin = getServerConnection(server);
     if (admin == null) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/AssignmentLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/AssignmentLoadBalancer.java
new file mode 100644
index 0000000..7b1e4f6
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/AssignmentLoadBalancer.java
@@ -0,0 +1,38 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.balancer;
+
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.master.RegionPlan;
+
+public class AssignmentLoadBalancer extends BaseLoadBalancer {
+
+  AssignmentLoadBalancer() {
+    super();
+  }
+
+  @Override
+  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterMap) {
+    return null; //TODO fix this 
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
index d34d747..3aff290 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java
@@ -22,6 +22,7 @@ import java.io.IOException;
 import java.io.InterruptedIOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.Callable;
 import java.util.concurrent.CompletionService;
 import java.util.concurrent.ExecutionException;
@@ -41,6 +42,7 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException;
 import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaEditor;
@@ -72,11 +74,15 @@ public class CreateTableHandler extends EventHandler {
   private final CatalogTracker catalogTracker;
   private final TableLockManager tableLockManager;
   private final HRegionInfo [] newRegions;
+
+  private final Map<HRegionInfo, List<ServerName>> assignmentMap;
   private final TableLock tableLock;
 
   public CreateTableHandler(Server server, MasterFileSystem fileSystemManager,
       HTableDescriptor hTableDescriptor, Configuration conf, HRegionInfo [] newRegions,
-      MasterServices masterServices) {
+      MasterServices masterServices, AssignmentManager assignmentManager, 
+      Map<HRegionInfo, List<ServerName>> assignmentMap)
+          throws NotAllMetaRegionsOnlineException, TableExistsException, IOException {
     super(server, EventType.C_M_CREATE_TABLE);
 
     this.fileSystemManager = fileSystemManager;
@@ -86,7 +92,7 @@ public class CreateTableHandler extends EventHandler {
     this.catalogTracker = masterServices.getCatalogTracker();
     this.assignmentManager = masterServices.getAssignmentManager();
     this.tableLockManager = masterServices.getTableLockManager();
-
+    this.assignmentMap = assignmentMap;
     this.tableLock = this.tableLockManager.writeLock(this.hTableDescriptor.getName()
         , EventType.C_M_CREATE_TABLE.toString());
   }
@@ -212,12 +218,12 @@ public class CreateTableHandler extends EventHandler {
 
     if (regionInfos != null && regionInfos.size() > 0) {
       // 4. Add regions to META
-      MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos);
+      MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos, assignmentMap);
 
       // 5. Trigger immediate assignment of the regions in round-robin fashion
       try {
         assignmentManager.getRegionStates().createRegionStates(regionInfos);
-        assignmentManager.assign(regionInfos);
+        assignmentManager.assign(regionInfos, assignmentMap);
       } catch (InterruptedException e) {
         LOG.error("Caught " + e + " during round-robin assignment");
         InterruptedIOException ie = new InterruptedIOException(e.getMessage());
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
index b03c99e..78c96fc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/snapshot/CloneSnapshotHandler.java
@@ -66,8 +66,8 @@ public class CloneSnapshotHandler extends CreateTableHandler implements Snapshot
       final SnapshotDescription snapshot, final HTableDescriptor hTableDescriptor)
       throws NotAllMetaRegionsOnlineException, TableExistsException, IOException {
     super(masterServices, masterServices.getMasterFileSystem(), hTableDescriptor,
-      masterServices.getConfiguration(), null, masterServices);
-
+        masterServices.getConfiguration(), null, masterServices,
+      masterServices.getAssignmentManager(), null);
     // Snapshot information
     this.snapshot = snapshot;
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
index 9683a97..1d50335 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
@@ -106,6 +106,7 @@ import org.apache.hadoop.hbase.regionserver.wal.HLogKey;
 import org.apache.hadoop.hbase.regionserver.wal.WALEdit;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Triple;
 
 import com.google.protobuf.ByteString;
 
@@ -601,13 +602,14 @@ public final class RequestConverter {
   * @return a protocol buffer OpenRegionRequest
   */
  public static OpenRegionRequest
-     buildOpenRegionRequest(final List<Pair<HRegionInfo, Integer>> regionOpenInfos) {
+     buildOpenRegionRequest(final List<Triple<HRegionInfo, Integer, 
+         List<ServerName>>> regionOpenInfos) {
    OpenRegionRequest.Builder builder = OpenRegionRequest.newBuilder();
-   for (Pair<HRegionInfo, Integer> regionOpenInfo: regionOpenInfos) {
+   for (Triple<HRegionInfo, Integer, List<ServerName>> regionOpenInfo: regionOpenInfos) {
      Integer second = regionOpenInfo.getSecond();
      int versionOfOfflineNode = second == null ? -1 : second.intValue();
      builder.addOpenInfo(buildRegionOpenInfo(
-       regionOpenInfo.getFirst(), versionOfOfflineNode));
+       regionOpenInfo.getFirst(), versionOfOfflineNode, regionOpenInfo.getThird()));
    }
    return builder.build();
  }
@@ -1191,12 +1193,23 @@ public final class RequestConverter {
    * Create a RegionOpenInfo based on given region info and version of offline node
    */
   private static RegionOpenInfo buildRegionOpenInfo(
-      final HRegionInfo region, final int versionOfOfflineNode) {
+      final HRegionInfo region, final int versionOfOfflineNode, 
+      final List<ServerName> favoredNodes) {
     RegionOpenInfo.Builder builder = RegionOpenInfo.newBuilder();
     builder.setRegion(HRegionInfo.convert(region));
     if (versionOfOfflineNode >= 0) {
       builder.setVersionOfOfflineNode(versionOfOfflineNode);
     }
+    if (favoredNodes != null) {
+      for (ServerName server : favoredNodes) {
+        builder.addFavoredNodes(ProtobufUtil.toServerName(server));
+      }
+    }
     return builder.build();
   }
+
+  private static RegionOpenInfo buildRegionOpenInfo(
+      final HRegionInfo region, final int versionOfOfflineNode) {
+    return buildRegionOpenInfo(region, versionOfOfflineNode, null);
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 5ae8bc1..eeb5b30 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -24,6 +24,7 @@ import java.io.IOException;
 import java.io.InterruptedIOException;
 import java.io.UnsupportedEncodingException;
 import java.lang.reflect.Constructor;
+import java.net.InetSocketAddress;
 import java.text.ParseException;
 import java.util.AbstractList;
 import java.util.ArrayList;
@@ -84,6 +85,7 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.RegionTooBusyException;
+import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.backup.HFileArchiver;
 import org.apache.hadoop.hbase.client.Append;
@@ -133,6 +135,7 @@ import org.apache.hadoop.hbase.util.HashedBytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.Threads;
 import org.apache.hadoop.io.MultipleIOException;
+import org.apache.hadoop.io.SetFile;
 import org.apache.hadoop.util.StringUtils;
 import org.cliffc.high_scale_lib.Counter;
 
@@ -200,6 +203,10 @@ public class HRegion implements HeapSize { // , Writable{
 
   protected long completeSequenceId = -1L;
 
+  // When writing store files for this region, replicas will preferrably be
+  // placed on these nodes, if non-null.
+  private InetSocketAddress[] favoredNodes = null;
+
   //////////////////////////////////////////////////////////////////////////////
   // Members
   //////////////////////////////////////////////////////////////////////////////
@@ -1792,9 +1799,28 @@ public class HRegion implements HeapSize { // , Writable{
     }
   }
 
+  /**
+   * @return the nodes on which to place replicas of all store files, or null if
+   * there are no favored nodes.
+   */
+  public InetSocketAddress[] getFavoredNodes() {
+    return this.favoredNodes;
+  }
+
   //////////////////////////////////////////////////////////////////////////////
   // set() methods for client use.
   //////////////////////////////////////////////////////////////////////////////
+
+  /**
+   * Set the favored nodes on which to place replicas of all store files. The
+   * array can be null to set no preference for favored nodes, but elements of
+   * the array must not be null. Placement of replicas on favored nodes is best-
+   * effort only and the filesystem may choose different nodes.
+   * @param favoredNodes the favored nodes, or null
+   */
+  public void setFavoredNodes(InetSocketAddress[] favoredNodes) {
+    this.favoredNodes = favoredNodes;
+  }
   /**
    * @param delete delete object
    * @param writeToWAL append to the write ahead lock or not
@@ -3955,6 +3981,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @param hlog shared HLog
    * @param initialize - true to initialize the region
    * @param ignoreHLog - true to skip generate new hlog if it is null, mostly for createTable
+   * @param favoredNodes - the list of favored nodes for this region
    * @return new HRegion
    * @throws IOException
    */
@@ -3982,6 +4009,7 @@ public class HRegion implements HeapSize { // , Writable{
     }
     HRegion region = HRegion.newHRegion(tableDir,
         effectiveHLog, fs, conf, info, hTableDescriptor, null);
+    
     if (initialize) {
       region.initialize();
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index eef00f1..9b6e66a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -3493,8 +3493,10 @@ public class HRegionServer implements ClientProtocol,
             this.service.submit(new OpenMetaHandler(this, this, region, htd,
                 versionOfOfflineNode));
           } else {
+            List<org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName> serverPBList =
+                regionOpenInfo.getFavoredNodesList();
             this.service.submit(new OpenRegionHandler(this, this, region, htd,
-                versionOfOfflineNode));
+                versionOfOfflineNode, serverPBList));
           }
         }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
index 1f361a0..275d139 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java
@@ -19,6 +19,8 @@
 package org.apache.hadoop.hbase.regionserver.handler;
 
 import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.commons.logging.Log;
@@ -28,10 +30,10 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.executor.EventHandler;
+import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.ServerName;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.RegionServerAccounting;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
-import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.util.CancelableProgressable;
 import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.zookeeper.KeeperException;
@@ -56,6 +58,7 @@ public class OpenRegionHandler extends EventHandler {
   private volatile int version = -1;
   //version of the offline node that was set by the master
   private volatile int versionOfOfflineNode = -1;
+  private final List<ServerName> favoredNodes;
 
   public OpenRegionHandler(final Server server,
       final RegionServerServices rsServices, HRegionInfo regionInfo,
@@ -69,15 +72,30 @@ public class OpenRegionHandler extends EventHandler {
         versionOfOfflineNode);
   }
 
+  public OpenRegionHandler(final Server server,
+      final RegionServerServices rsServices, HRegionInfo regionInfo,
+      HTableDescriptor htd, int versionOfOfflineNode, List<ServerName> servers) {
+    this(server, rsServices, regionInfo, htd, EventType.M_RS_OPEN_REGION,
+        versionOfOfflineNode, servers);
+  }
+
   protected OpenRegionHandler(final Server server,
       final RegionServerServices rsServices, final HRegionInfo regionInfo,
       final HTableDescriptor htd, EventType eventType,
-      final int versionOfOfflineNode) {
+      final int versionOfOfflineNode, final List<ServerName> favoredNodes) {
     super(server, eventType);
     this.rsServices = rsServices;
     this.regionInfo = regionInfo;
     this.htd = htd;
     this.versionOfOfflineNode = versionOfOfflineNode;
+    this.favoredNodes = favoredNodes;
+  }
+
+  protected OpenRegionHandler(final Server server,
+      final RegionServerServices rsServices, final HRegionInfo regionInfo,
+      final HTableDescriptor htd, EventType eventType,
+      final int versionOfOfflineNode) {
+    this(server, rsServices, regionInfo, htd, eventType, versionOfOfflineNode, null);
   }
 
   public HRegionInfo getRegionInfo() {
@@ -442,6 +460,14 @@ public class OpenRegionHandler extends EventHandler {
             return tickleOpening("open_region_progress");
           }
         });
+      if (favoredNodes != null) {
+        InetSocketAddress[] addr = new InetSocketAddress[favoredNodes.size()];
+        for (int i = 0; i < favoredNodes.size(); i++) {
+          addr[i] = InetSocketAddress.createUnresolved(favoredNodes.get(i).getHostName(), 
+              favoredNodes.get(i).getPort()); //TODO: does it matter to have it unresolved
+        }
+        region.setFavoredNodes(addr);
+      }
     } catch (Throwable t) {
       // We failed open. Our caller will see the 'null' return value
       // and transition the node back to FAILED_OPEN. If that fails,
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
new file mode 100644
index 0000000..cac1eec
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestRegionPlacement.java
@@ -0,0 +1,278 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.MetaScanner;
+import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.master.AssignmentPlan.POSITION;
+import org.apache.hadoop.hbase.master.balancer.AssignmentLoadBalancer;
+import org.apache.hadoop.hbase.regionserver.HRegion;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestRegionPlacement {
+  final static Log LOG = LogFactory.getLog(TestRegionPlacement.class);
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private final static int SLAVES = 4;
+  private static HBaseAdmin admin;
+  private static RegionPlacement rp;
+  private static POSITION[] positions = AssignmentPlan.POSITION.values();
+  private int lastRegionOnPrimaryRSCount = 0;
+  private int REGION_NUM = 10;
+
+  @BeforeClass
+  public static void setupBeforeClass() throws Exception {
+    Configuration conf = TEST_UTIL.getConfiguration();
+    // Enable the favored nodes based load balancer
+    conf.setClass(HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+        AssignmentLoadBalancer.class, LoadBalancer.class);
+
+    conf.setInt("hbase.master.meta.thread.rescanfrequency", 5000);
+    conf.setInt("hbase.regionserver.msginterval", 1000);
+    conf.setLong("hbase.regionserver.transientAssignment.regionHoldPeriod", 2000);
+    TEST_UTIL.startMiniCluster(SLAVES);
+    admin = new HBaseAdmin(conf);
+    rp = new RegionPlacement(conf);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Test(timeout = 180000)
+  public void testRegionPlacement() throws Exception {
+    AssignmentPlan currentPlan;
+    // ONLY meta regions, ROOT and META, are assigned at beginning.
+    //verifyRegionMovementNum(META_REGION_NUM);
+
+    // Create a table with REGION_NUM regions.
+    createTable("testRegionAssignment", REGION_NUM);
+    
+    admin.isTableAvailable("testRegionAssignment");
+
+    // Test case1: Verify the region assignment for the exiting table
+    // is consistent with the assignment plan and all the region servers get
+    // correctly favored nodes updated.
+
+    // Get the assignment plan from scanning the META table
+    currentPlan = rp.getExistingAssignmentPlan();
+    rp.printAssignmentPlan(currentPlan);
+    // Verify the plan from the META has covered all the user regions
+    assertEquals(REGION_NUM, currentPlan.getAssignmentMap().keySet().size());
+
+    // Verify all the user regions are assigned to the primary region server
+    // based on the plan
+    verifyRegionOnPrimaryRS(REGION_NUM);
+
+    // Verify all the region server are update with the latest favored nodes
+    verifyRegionServerUpdated(currentPlan);
+    rp.printAssignmentPlan(currentPlan);
+  }
+
+  /**
+   * Verify the number of user regions is assigned to the primary
+   * region server based on the plan is expected
+   * @param expectedNum.
+   * @throws IOException
+   */
+  private void verifyRegionOnPrimaryRS(int expectedNum)
+  throws IOException {
+    this.lastRegionOnPrimaryRSCount = getNumRegionisOnPrimaryRS();
+    assertEquals("Only " +  expectedNum + " of user regions running " +
+        "on the primary region server", expectedNum ,
+        lastRegionOnPrimaryRSCount);
+  }
+
+  /**
+   * Verify all the online region servers has been updated to the
+   * latest assignment plan
+   * @param plan
+   * @throws IOException
+   */
+  private void verifyRegionServerUpdated(AssignmentPlan plan) throws IOException {
+    // Verify all region servers contain the correct favored nodes information
+    MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
+    for (int i = 0; i < SLAVES; i++) {
+      HRegionServer rs = cluster.getRegionServer(i);
+      for (HRegion region: rs.getOnlineRegions(Bytes.toBytes("testRegionAssignment"))) {
+        InetSocketAddress[] favoredSockedAddress = region.getFavoredNodes();
+        List<ServerName> favoredServerList =
+          plan.getAssignment(region.getRegionInfo());
+
+        // All regions are supposed to have favored nodes,
+        // except for META and ROOT
+        if (favoredServerList == null) {
+          HTableDescriptor desc = region.getTableDesc();
+          // Verify they are ROOT and META regions since no favored nodes
+          assertNull(favoredSockedAddress);
+          assertTrue("User region " +
+              region.getTableDesc().getNameAsString() +
+              " should have favored nodes",
+              (desc.isRootRegion() || desc.isMetaRegion()));
+        } else {
+          // For user region, the favored nodes in the region server should be
+          // identical to favored nodes in the assignmentPlan
+          assertTrue(favoredSockedAddress.length == favoredServerList.size());
+          assertTrue(favoredServerList.size() > 0);
+          for (int j = 0; j < favoredServerList.size(); j++) {
+            InetSocketAddress addrFromRS = favoredSockedAddress[j];
+            InetSocketAddress addrFromPlan = InetSocketAddress.createUnresolved(
+              favoredServerList.get(j).getHostname(), favoredServerList.get(j).getPort()); 
+
+            assertNotNull(addrFromRS);
+            assertNotNull(addrFromPlan);
+            assertTrue("Region server " + rs.getServerName().getHostAndPort()
+                 + " has the " + positions[j] +
+                 " for region " + region.getRegionNameAsString() + " is " +
+                 addrFromRS + " which is inconsistent with the plan "
+                 + addrFromPlan, addrFromRS.equals(addrFromPlan));
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * Check whether regions are assigned to servers consistent with the explicit
+   * hints that are persisted in the META table.
+   * Also keep track of the number of the regions are assigned to the
+   * primary region server.
+   * @return the number of regions are assigned to the primary region server
+   * @throws IOException
+   */
+  private int getNumRegionisOnPrimaryRS() throws IOException {
+    final AtomicInteger regionOnPrimaryNum = new AtomicInteger(0);
+    final AtomicInteger totalRegionNum = new AtomicInteger(0);
+    LOG.info("The start of region placement verification");
+    MetaScannerVisitor visitor = new MetaScannerVisitor() {
+      public boolean processRow(Result result) throws IOException {
+        try {
+          HRegionInfo info = MetaScanner.getHRegionInfo(result);
+          byte[] server = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.SERVER_QUALIFIER);
+          byte[] startCode = result.getValue(HConstants.CATALOG_FAMILY,
+              HConstants.STARTCODE_QUALIFIER);
+          byte[] favoredNodes = result.getValue(HConstants.CATALOG_FAMILY,
+              "favorednodes".getBytes());
+          POSITION[] positions = AssignmentPlan.POSITION.values();
+          if (info != null) {
+            totalRegionNum.incrementAndGet();
+            if (server != null) {
+              String serverString = 
+                  new ServerName(Bytes.toString(server),Bytes.toLong(startCode)).toString();
+              if (favoredNodes != null) {
+                String[] splits = 
+                    new String(favoredNodes).split(RegionPlacement.SERVER_NAME_SEPARATOR);
+                String placement = "[NOT FAVORED NODE]";
+                for (int i = 0; i < splits.length; i++) {
+                  if (splits[i].equals(serverString)) {
+                    placement = positions[i].toString();
+                    if (i == AssignmentPlan.POSITION.PRIMARY.ordinal()) {
+                      regionOnPrimaryNum.incrementAndGet();
+                    }
+                    break;
+                  }
+                }
+                LOG.info(info.getRegionNameAsString() + " on " +
+                    serverString + " " + placement);
+              } else {
+                LOG.info(info.getRegionNameAsString() + " running on " +
+                    serverString + " but there is no favored region server");
+              }
+            } else {
+              LOG.info(info.getRegionNameAsString() +
+                  " not assigned to any server");
+            }
+          }
+          return true;
+        } catch (RuntimeException e) {
+          LOG.error("Result=" + result);
+          throw e;
+        }
+      }
+
+      @Override
+      public void close() throws IOException {
+        // TODO Auto-generated method stub
+        
+      }
+    };
+    MetaScanner.metaScan(TEST_UTIL.getConfiguration(), visitor);
+    LOG.info("There are " + regionOnPrimaryNum.intValue() + " out of " +
+        totalRegionNum.intValue() + " regions running on the primary" +
+            " region servers" );
+    return regionOnPrimaryNum.intValue() ;
+  }
+
+  /**
+   * Create a table with specified table name and region number.
+   * @param table
+   * @param regionNum
+   * @return
+   * @throws IOException
+   */
+   private static void createTable(String table, int regionNum)
+   throws IOException {
+    byte[] tableName = Bytes.toBytes(table);
+    int expectedRegions = regionNum;
+    byte[][] splitKeys = new byte[expectedRegions - 1][];
+    for (int i = 1; i < expectedRegions; i++) {
+      byte splitKey = (byte) i;
+      splitKeys[i - 1] = new byte[] { splitKey, splitKey, splitKey };
+    }
+
+    HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
+    admin.createTable(desc, splitKeys);
+
+    HTable ht = new HTable(TEST_UTIL.getConfiguration(), tableName);
+    Map<HRegionInfo, ServerName> regions = ht.getRegionLocations();
+    assertEquals("Tried to create " + expectedRegions + " regions "
+        + "but only found " + regions.size(), expectedRegions, regions.size());
+   }
+}
