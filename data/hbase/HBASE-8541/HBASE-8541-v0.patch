diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
index a77f89f..86649bf 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
@@ -68,9 +68,9 @@ class DefaultStoreFileManager implements StoreFileManager {
   }
 
   @Override
-  public void insertNewFile(StoreFile sf) {
+  public void insertNewFiles(Collection<StoreFile> sfs) throws IOException {
     ArrayList<StoreFile> newFiles = new ArrayList<StoreFile>(storefiles);
-    newFiles.add(sf);
+    newFiles.addAll(sfs);
     sortAndSetStoreFiles(newFiles);
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
index 6d6aa7e..37541fa 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
@@ -603,7 +603,7 @@ public class HStore implements Store {
     // Append the new storefile into the list
     this.lock.writeLock().lock();
     try {
-      this.storeEngine.getStoreFileManager().insertNewFile(sf);
+      this.storeEngine.getStoreFileManager().insertNewFiles(Lists.newArrayList(sf));
     } finally {
       // We need the lock, as long as we are updating the storeFiles
       // or changing the memstore. Let us release it before calling
@@ -822,9 +822,7 @@ public class HStore implements Store {
       final List<StoreFile> sfs, final SortedSet<KeyValue> set) throws IOException {
     this.lock.writeLock().lock();
     try {
-      for (StoreFile sf : sfs) {
-        this.storeEngine.getStoreFileManager().insertNewFile(sf);
-      }
+      this.storeEngine.getStoreFileManager().insertNewFiles(sfs);
       this.memstore.clearSnapshot(set);
     } finally {
       // We need the lock, as long as we are updating the storeFiles
@@ -1089,15 +1087,15 @@ public class HStore implements Store {
       .append(" to execute.");
     LOG.info(message.toString());
     if (LOG.isTraceEnabled()) {
-     int fileCount = storeEngine.getStoreFileManager().getStorefileCount();
-     long resultSize = 0;
-     for (StoreFile sf : sfs) {
-       resultSize += sf.getReader().length();
-     }
-     String traceMessage = "COMPACTION start,end,size out,files in,files out,store size,"
-       + "store files [" + compactionStartTime + "," + now + "," + resultSize + ","
-         + cr.getFiles().size() + "," + sfs.size() + "," +  storeSize + "," + fileCount + "]";
-     LOG.trace(traceMessage);
+      int fileCount = storeEngine.getStoreFileManager().getStorefileCount();
+      long resultSize = 0;
+      for (StoreFile sf : sfs) {
+        resultSize += sf.getReader().length();
+      }
+      String traceMessage = "COMPACTION start,end,size out,files in,files out,store size,"
+        + "store files [" + compactionStartTime + "," + now + "," + resultSize + ","
+          + cr.getFiles().size() + "," + sfs.size() + "," +  storeSize + "," + fileCount + "]";
+      LOG.trace(traceMessage);
     }
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
index 779be6f..6347ff5 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
@@ -49,8 +49,9 @@ public interface StoreFileManager {
   /**
    * Adds new file, either for from MemStore flush or bulk insert, into the structure.
    * @param sf New store file.
+   * @throws IOException 
    */
-  public abstract void insertNewFile(StoreFile sf);
+  public abstract void insertNewFiles(Collection<StoreFile> sfs) throws IOException;
 
   /**
    * Adds compaction results into the structure.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
index 1659095..85eb6dc 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
@@ -45,6 +45,9 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
   /** Source scanner that is tracking KV count; may be null if source is not StoreScanner */
   protected StoreScanner sourceScanner;
 
+  /** Whether to write stripe metadata */
+  private boolean doWriteMetadata = true;
+
   public interface WriterFactory {
     public StoreFile.Writer createWriter() throws IOException;
   }
@@ -56,10 +59,16 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
     this.comparator = comparator;
   }
 
+  public void setNoMetadata() {
+    this.doWriteMetadata = false;
+  }
+
   public void finalizeWriters() throws IOException {
     finalizeWritersInternal();
-    LOG.debug("Writing out metadata for " + this.existingWriters.size() + " writers");
+    LOG.debug((this.doWriteMetadata ? "W" : "Not w")
+        + "riting metadata for " + this.existingWriters.size() + " writers");
     assert this.boundaries.size() == (this.existingWriters.size() + 1);
+    if (!this.doWriteMetadata)  return;
     for (int i = 0; i < this.existingWriters.size(); ++i) {
       StoreFile.Writer writer = this.existingWriters.get(i);
       if (writer != null) {
@@ -77,8 +86,8 @@ public abstract class StripeMultiFileWriter implements Compactor.CellSink {
       byte[] left, byte[] row, int rowOffset, int rowLength) throws IOException {
     if (StripeStoreFileManager.OPEN_KEY != left &&
         comparator.compareRows(row, rowOffset, rowLength, left, 0, left.length) < 0) {
-      String error = "The first row is lower than the left boundary of ["
-        + Bytes.toString(left) + "]: [" + Bytes.toString(row, rowOffset, rowLength) + "]";
+      String error = "The first row is lower than the left boundary of [" + Bytes.toString(left)
+        + "]: [" + Bytes.toString(row, rowOffset, rowLength) + "]";
       LOG.error(error);
       throw new IOException(error);
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java
index 3a53b60..6eddaa9 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java
@@ -35,12 +35,11 @@ public class StripeStoreConfig {
   static final Log LOG = LogFactory.getLog(StripeStoreConfig.class);
   public static final String STRIPES_TYPE_KEY = "hbase.store.stripe.type";
 
-  public static final String ASSUME_ORDERING_KEY =
-      "hbase.store.stripe.compaction.assume.ordering";
   public static final String MAX_FILES_KEY = "hbase.store.stripe.compaction.maxFiles";
   public static final String MIN_FILES_KEY = "hbase.store.stripe.compaction.minFiles";
   public static final String MAX_SPLIT_IMBALANCE_KEY = "hbase.store.stripe.split.max.imbalance";
   public static final String MIN_FILES_L0_KEY = "hbase.store.stripe.compaction.minFilesL0";
+  public static final String FLUSH_TO_L0_KEY = "hbase.store.stripe.compaction.useL0";
 
   public static enum StripeType {
     FIXED_COUNT,
@@ -64,7 +63,7 @@ public class StripeStoreConfig {
   private final int level0CompactMinFiles;
   private final int stripeCompactMinFiles;
   private final int stripeCompactMaxFiles;
-  private final boolean assumeOrdering;
+  private final boolean flushIntoL0;
 
   public static StripeType getType(Configuration conf) {
     return StripeType.fromString(conf.get(STRIPES_TYPE_KEY, StripeType.COUNT_STRING));
@@ -75,8 +74,8 @@ public class StripeStoreConfig {
     this.level0CompactMinFiles = config.getInt(MIN_FILES_L0_KEY, 4);
     this.stripeCompactMinFiles = config.getInt(MIN_FILES_KEY, 3);
     this.stripeCompactMaxFiles = config.getInt(MAX_FILES_KEY, 10);
-    this.assumeOrdering = config.getBoolean(ASSUME_ORDERING_KEY, false);
     this.maxSplitImbalance = config.getFloat(MAX_SPLIT_IMBALANCE_KEY, 1.5f);
+    this.flushIntoL0 = config.getBoolean(FLUSH_TO_L0_KEY, false);
     if (this.maxSplitImbalance == 0) {
       this.maxSplitImbalance = 1.5f;
     }
@@ -123,12 +122,10 @@ public class StripeStoreConfig {
   }
 
   /**
-   * @return Whether we can increase the risk of ignoring out of order puts for the purpose
-   *         of dropping deletes during compaction, while decreasing the needless data rewriting
-   *         and compaction size a bit (see discussion in HBASE-7902).
+   * @return Whether we should flush data into L0 before rewriting it into stripes.
    */
-  public boolean isAssumeOrdering() {
-    return assumeOrdering;
+  public boolean doFlushToL0() {
+    return flushIntoL0;
   }
 
   public static class SizeBased extends StripeStoreConfig {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
index 0c9710e..7af75a0 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
@@ -28,6 +28,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig.StripeType;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.compactions.CountBasedStripeCompactionPolicy;
@@ -41,7 +42,7 @@ import com.google.common.base.Preconditions;
  * The storage engine that implements the stripe-based store/compaction scheme.
  */
 @InterfaceAudience.Private
-public class StripeStoreEngine extends StoreEngine<DefaultStoreFlusher,
+public class StripeStoreEngine extends StoreEngine<StripeStoreFlusher,
   StripeCompactionPolicy<?>, StripeCompactor, StripeStoreFileManager> {
   static final Log LOG = LogFactory.getLog(StripeStoreEngine.class);
   private StripeStoreConfig config;
@@ -75,8 +76,10 @@ public class StripeStoreEngine extends StoreEngine<DefaultStoreFlusher,
       }
       default: assert false;
     }
-    this.storeFlusher = new DefaultStoreFlusher(conf, store);
+
     this.storeFileManager = new StripeStoreFileManager(comparator, conf, this.config);
+    this.storeFlusher = new StripeStoreFlusher(
+        conf, store, this.compactionPolicy, this.storeFileManager);
     this.compactor = new StripeCompactor(conf, store);
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
index 9372cd8..0702827 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
@@ -24,8 +24,10 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.TreeMap;
 
 import org.apache.commons.logging.Log;
@@ -41,6 +43,8 @@ import org.apache.hadoop.hbase.util.ConcatenatedLists;
 
 import com.google.common.collect.ImmutableCollection;
 import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Lists;
+
 
 /**
  * Stripe implementation of StoreFileManager.
@@ -135,15 +139,10 @@ public class StripeStoreFileManager
   }
 
   @Override
-  public void insertNewFile(StoreFile sf) {
-    LOG.debug("New level 0 file: " + sf);
-    ArrayList<StoreFile> newFiles = new ArrayList<StoreFile>(state.level0Files);
-    insertFileIntoStripe(newFiles, sf);
-    ensureLevel0Metadata(sf);
-    this.state.level0Files = ImmutableList.copyOf(newFiles);
-    ArrayList<StoreFile> newAllFiles = new ArrayList<StoreFile>(state.allFilesCached);
-    newAllFiles.add(sf);
-    this.state.allFilesCached = ImmutableList.copyOf(newAllFiles);
+  public void insertNewFiles(Collection<StoreFile> sfs) throws IOException {
+    CompactionOrFlushMergeCopy cmc = new CompactionOrFlushMergeCopy(true);
+    cmc.mergeResults(null, sfs);
+    debugDumpState("Added new files");
   }
 
   @Override
@@ -301,7 +300,7 @@ public class StripeStoreFileManager
         + " files replaced by " + results.size());
     // In order to be able to fail in the middle of the operation, we'll operate on lazy
     // copies and apply the result at the end.
-    CompactionResultsMergeCopy cmc = new CompactionResultsMergeCopy();
+    CompactionOrFlushMergeCopy cmc = new CompactionOrFlushMergeCopy(false);
     cmc.mergeResults(compactedFiles, results);
     debugDumpState("Merged compaction results");
   }
@@ -629,7 +628,7 @@ public class StripeStoreFileManager
    * Since we want to merge them atomically (more or less), it operates on lazy copies, and
    * then applies copies to real lists as necessary.
    */
-  private class CompactionResultsMergeCopy {
+  private class CompactionOrFlushMergeCopy {
     private ArrayList<List<StoreFile>> stripeFiles = null;
     private ArrayList<StoreFile> level0Files = null;
     private ArrayList<byte[]> stripeEndRows = null;
@@ -638,11 +637,13 @@ public class StripeStoreFileManager
     private Collection<StoreFile> results = null;
 
     private List<StoreFile> l0Results = new ArrayList<StoreFile>();
+    private final boolean isFlush;
 
-    public CompactionResultsMergeCopy() {
+    public CompactionOrFlushMergeCopy(boolean isFlush) {
       // Create a lazy mutable copy (other fields are so lazy they start out as nulls).
       this.stripeFiles = new ArrayList<List<StoreFile>>(
           StripeStoreFileManager.this.state.stripeFiles);
+      this.isFlush = isFlush;
     }
 
     public void mergeResults(Collection<StoreFile> compactedFiles, Collection<StoreFile> results)
@@ -651,7 +652,7 @@ public class StripeStoreFileManager
       this.compactedFiles = compactedFiles;
       this.results = results;
       // Do logical processing.
-      removeCompactedFiles();
+      if (!isFlush) removeCompactedFiles();
       TreeMap<byte[], StoreFile> newStripes = processCompactionResults();
       if (newStripes != null) {
         processNewCandidateStripes(newStripes);
@@ -678,7 +679,7 @@ public class StripeStoreFileManager
       }
 
       List<StoreFile> newAllFiles = new ArrayList<StoreFile>(oldState.allFilesCached);
-      newAllFiles.removeAll(compactedFiles);
+      if (!isFlush) newAllFiles.removeAll(compactedFiles);
       newAllFiles.addAll(results);
       newState.allFilesCached = ImmutableList.copyOf(newAllFiles);
       return newState;
@@ -686,12 +687,16 @@ public class StripeStoreFileManager
 
     private void updateMetadataMaps() {
       StripeStoreFileManager parent = StripeStoreFileManager.this;
-      for (StoreFile sf : this.compactedFiles) {
-        parent.fileStarts.remove(sf);
-        parent.fileEnds.remove(sf);
+      if (!isFlush) {
+        for (StoreFile sf : this.compactedFiles) {
+          parent.fileStarts.remove(sf);
+          parent.fileEnds.remove(sf);
+        }
       }
-      for (StoreFile sf : this.l0Results) {
-        parent.ensureLevel0Metadata(sf);
+      if (this.l0Results != null) {
+        for (StoreFile sf : this.l0Results) {
+          parent.ensureLevel0Metadata(sf);
+        }
       }
     }
 
@@ -731,7 +736,9 @@ public class StripeStoreFileManager
       for (StoreFile sf : this.results) {
         byte[] startRow = startOf(sf), endRow = endOf(sf);
         if (isInvalid(endRow) || isInvalid(startRow)) {
-          LOG.warn("The newly compacted files doesn't have stripe rows set: " + sf.getPath());
+          if (!isFlush) {
+            LOG.warn("The newly compacted file doesn't have stripes set: " + sf.getPath());
+          }
           insertFileIntoStripe(getLevel0Copy(), sf);
           this.l0Results.add(sf);
           continue;
@@ -772,7 +779,7 @@ public class StripeStoreFileManager
           int stripeIndex = findStripeIndexByEndRow(oldEndRow);
           if (stripeIndex < 0) {
             throw new IOException("An allegedly compacted file [" + oldFile + "] does not belong"
-                + " to a known stripe (end row + [" + Bytes.toString(oldEndRow) + "])");
+                + " to a known stripe (end row - [" + Bytes.toString(oldEndRow) + "])");
           }
           source = getStripeCopy(stripeIndex);
         }
@@ -800,6 +807,8 @@ public class StripeStoreFileManager
         throw new IOException("Newly created stripes do not cover the entire key space.");
       }
 
+      boolean canAddNewStripes = true;
+      Collection<StoreFile> filesForL0 = null;
       if (hasStripes) {
         // Determine which stripes will need to be removed because they conflict with new stripes.
         // The new boundaries should match old stripe boundaries, so we should get exact matches.
@@ -812,20 +821,49 @@ public class StripeStoreFileManager
         }
         int removeTo = findStripeIndexByEndRow(lastEndRow);
         if (removeTo < 0) throw new IOException("Compaction is trying to add a bad range.");
-        // Remove old empty stripes.
-        int originalCount = this.stripeFiles.size();
+        // See if there are files in the stripes we are trying to replace.
+        ArrayList<StoreFile> conflictingFiles = new ArrayList<StoreFile>();
         for (int removeIndex = removeTo; removeIndex >= removeFrom; --removeIndex) {
-          if (!this.stripeFiles.get(removeIndex).isEmpty()) {
-            throw new IOException("Compaction intends to create a new stripe that replaces an"
-                + " existing one, but the latter contains some files.");
+          conflictingFiles.addAll(this.stripeFiles.get(removeIndex));
+        }
+        if (!conflictingFiles.isEmpty()) {
+          // This can be caused by two things - concurrent flush into stripes, or a bug.
+          // Unfortunately, we cannot tell them apart without looking at timing or something
+          // like that. We will assume we are dealing with a flush and dump it into L0.
+          if (isFlush) {
+            long newSize = StripeCompactionPolicy.getTotalFileSize(newStripes.values());
+            LOG.error("Stripes were created by a flush, but results of size " + newSize
+                + " cannot be added due to a concurrent flush also creating stripes");
+            canAddNewStripes = false;
+            filesForL0 = newStripes.values();
+          } else {
+            long oldSize = StripeCompactionPolicy.getTotalFileSize(conflictingFiles);
+            LOG.info(conflictingFiles.size() + " conflicting files (likely created by a flush) "
+                + " of size " + oldSize + " are moved to L0 due to concurrent stripe change");
+            filesForL0 = conflictingFiles;
           }
-          if (removeIndex != originalCount - 1) {
-            this.stripeEndRows.remove(removeIndex);
+          if (filesForL0 != null) {
+            for (StoreFile sf : filesForL0) {
+              insertFileIntoStripe(getLevel0Copy(), sf);
+            }
+            l0Results.addAll(filesForL0);
+          }
+        }
+
+        if (canAddNewStripes) {
+          // Remove old empty stripes.
+          int originalCount = this.stripeFiles.size();
+          for (int removeIndex = removeTo; removeIndex >= removeFrom; --removeIndex) {
+            if (removeIndex != originalCount - 1) {
+              this.stripeEndRows.remove(removeIndex);
+            }
+            this.stripeFiles.remove(removeIndex);
           }
-          this.stripeFiles.remove(removeIndex);
         }
       }
 
+      if (!canAddNewStripes) return; // Files were already put into L0.
+
       // Now, insert new stripes. The total ranges match, so we can insert where we removed.
       byte[] previousEndRow = null;
       int insertAt = removeFrom;
@@ -835,7 +873,8 @@ public class StripeStoreFileManager
           assert !isOpen(previousEndRow);
           byte[] startRow = startOf(newStripe.getValue());
           if (!rowEquals(previousEndRow, startRow)) {
-            throw new IOException("The new stripes produced by compaction are not contiguous");
+            throw new IOException("The new stripes produced by "
+                + (isFlush ? "flush" : "compaction") + " are not contiguous");
           }
         }
         // Add the new stripe.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
new file mode 100644
index 0000000..4a1399a
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
@@ -0,0 +1,188 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.SortedSet;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.monitoring.MonitoredTask;
+import org.apache.hadoop.hbase.regionserver.StoreFile.Writer;
+import org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
+import org.apache.hadoop.hbase.util.CollectionBackedScanner;
+
+import com.google.common.annotations.VisibleForTesting;
+
+/**
+ * Stripe implementation of StoreFlusher. Flushes files either into L0 file w/o metadata, or 
+ * into separate striped files, avoiding L0.
+ */
+public class StripeStoreFlusher extends StoreFlusher {
+  private static final Log LOG = LogFactory.getLog(DefaultStoreFlusher.class);
+  private final Object flushLock = new Object();
+  private final StripeCompactionPolicy<?> policy;
+  private final StripeCompactionPolicy.StripeInformationProvider stripes;
+
+  public StripeStoreFlusher(Configuration conf, Store store, StripeCompactionPolicy<?> policy,
+      StripeStoreFileManager stripes) {
+    super(conf, store);
+    this.policy = policy;
+    this.stripes = stripes;
+  }
+
+  @Override
+  public List<Path> flushSnapshot(SortedSet<KeyValue> snapshot, long cacheFlushSeqNum,
+      final TimeRangeTracker tracker, AtomicLong flushedSize, MonitoredTask status)
+          throws IOException {
+    List<Path> result = new ArrayList<Path>();
+    int kvCount = snapshot.size();
+    if (kvCount == 0) return result; // don't flush if there are no entries
+
+    long smallestReadPoint = store.getSmallestReadPoint();
+    InternalScanner scanner = createScanner(snapshot, smallestReadPoint);
+    if (scanner == null) return result; // coprocessor override.
+
+    // Let policy select flush method.
+    StripeFlushRequest req = this.policy.selectFlush(this.stripes, kvCount);
+
+    long flushed = 0;
+    boolean success = false;
+    StripeMultiFileWriter mw = null;
+    try {
+      mw = req.createWriter(); // Writer according to the policy.
+      StripeMultiFileWriter.WriterFactory factory = createWriterFactory(tracker, kvCount);
+      StoreScanner storeScanner = (scanner instanceof StoreScanner) ? (StoreScanner)scanner : null;
+      mw.prepare(storeScanner, factory, store.getComparator());
+
+      synchronized (flushLock) {
+        flushed = performFlush(scanner, mw, smallestReadPoint);
+        mw.finalizeWriters();
+        for (StoreFile.Writer writer : mw.getAllWriters()) {
+          if (writer == null) continue;
+          finalizeWriter(writer, cacheFlushSeqNum, status);
+          result.add(writer.getPath());
+        }
+        success = true;
+      }
+    } finally {
+      if (!success && (mw != null)) {
+        result.clear();
+        for (StoreFile.Writer writer : mw.getAllWriters()) {
+          if (writer == null) continue;
+          writer.close();
+          store.getFileSystem().delete(writer.getPath(), false);
+        }
+      }
+      flushedSize.set(flushed);
+      try {
+        scanner.close();
+      } catch (IOException ex) {
+        LOG.warn("Failed to close flush scanner, ignoring", ex);
+      }
+    }
+    return result;
+  }
+
+  private InternalScanner createScanner(SortedSet<KeyValue> snapshot,
+      long smallestReadPoint) throws IOException {
+    KeyValueScanner memstoreScanner =
+        new CollectionBackedScanner(snapshot, store.getComparator());
+    InternalScanner scanner = preCreateCoprocScanner(memstoreScanner);
+    if (scanner == null) {
+      scanner = createStoreScanner(smallestReadPoint, memstoreScanner);
+    }
+    assert scanner != null;
+    scanner = postCreateCoprocScanner(scanner);
+    if (scanner == null) {
+      return null; // NULL from coprocessor here means skip normal processing
+    }
+    return scanner;
+  }
+
+  private StripeMultiFileWriter.WriterFactory createWriterFactory(
+      final TimeRangeTracker tracker, final long kvCount) {
+    return new StripeMultiFileWriter.WriterFactory() {
+      @Override
+      public Writer createWriter() throws IOException {
+        StoreFile.Writer writer = store.createWriterInTmp(
+            kvCount, store.getFamily().getCompression(), false, true);
+        writer.setTimeRangeTracker(tracker);
+        return writer;
+      }
+    };
+  }
+
+  /** Stripe flush request wrapper that writes a non-striped file. */
+  public static class StripeFlushRequest {
+    @VisibleForTesting
+    public StripeMultiFileWriter createWriter() throws IOException {
+      StripeMultiFileWriter writer =
+          new StripeMultiFileWriter.SizeMultiWriter(1, Long.MAX_VALUE, OPEN_KEY, OPEN_KEY);
+      writer.setNoMetadata();
+      return writer;
+    }
+  }
+
+  /** Stripe flush request wrapper based on boundaries. */
+  public static class BoundaryStripeFlushRequest extends StripeFlushRequest {
+    private final List<byte[]> targetBoundaries;
+
+    /** @param targetBoundaries New files should be written with these boundaries. */
+    public BoundaryStripeFlushRequest(List<byte[]> targetBoundaries) {
+      this.targetBoundaries = targetBoundaries;
+    }
+
+    @Override
+    public StripeMultiFileWriter createWriter() throws IOException {
+      return new StripeMultiFileWriter.BoundaryMultiWriter(targetBoundaries, null, null);
+    }
+  }
+
+  /** Stripe flush request wrapper based on size. */
+  public static class SizeStripeFlushRequest extends StripeFlushRequest {
+    private final int targetCount;
+    private final long targetKvs;
+
+    /**
+     * @param targetCount The maximum number of stripes to flush into.
+     * @param targetKvs The KV count of each segment. If targetKvs*targetCount is less than
+     *                  total number of kvs, all the overflow data goes into the last stripe.
+     */
+    public SizeStripeFlushRequest(int targetCount, long targetKvs) {
+      this.targetCount = targetCount;
+      this.targetKvs = targetKvs;
+    }
+
+    @Override
+    public StripeMultiFileWriter createWriter() throws IOException {
+      return new StripeMultiFileWriter.SizeMultiWriter(
+          this.targetCount, this.targetKvs, OPEN_KEY, OPEN_KEY);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CountBasedStripeCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CountBasedStripeCompactionPolicy.java
index ba59c38..c3d4718 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CountBasedStripeCompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CountBasedStripeCompactionPolicy.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.StoreUtils;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig.CountBased;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ConcatenatedLists;
 
@@ -86,6 +87,22 @@ public class CountBasedStripeCompactionPolicy extends
   }
 
   @Override
+  public StripeStoreFlusher.StripeFlushRequest selectFlush(
+      StripeInformationProvider si, int kvCount) {
+    if (this.config.doFlushToL0()) {
+      return new StripeStoreFlusher.StripeFlushRequest(); // L0 is used, return dumb request
+    }
+    if (si.getStripeCount() == 0) {
+      // No stripes - create them.
+      int targetStripeCount = this.config.getStripeCount();
+      long targetKvs = kvCount / targetStripeCount;
+      return new StripeStoreFlusher.SizeStripeFlushRequest(targetStripeCount, targetKvs);
+    }
+    // There are stripes - flush according to the boundaries.
+    return new StripeStoreFlusher.BoundaryStripeFlushRequest(si.getStripeBoundaries());
+  }
+
+  @Override
   public StripeCompactionRequest selectCompaction(StripeInformationProvider si,
       List<StoreFile> filesCompacting, boolean isOffpeak) throws IOException {
     // TODO: first cut - no parallel compactions. To have more fine grained control we 
@@ -127,7 +144,7 @@ public class CountBasedStripeCompactionPolicy extends
     }
 
     // If we need to compact L0, see if we can add a full stripe to it, and drop deletes.
-    boolean canDropDeletesNoL0 = this.config.isAssumeOrdering() || (l0Files.size() == 0);
+    boolean canDropDeletesNoL0 = l0Files.size() == 0;
     if (this.config.getLevel0MinFiles() <= l0Files.size()) {
       if (!canDropDeletesNoL0) {
         StripeCompactionRequest request =
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SizeBasedStripeCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SizeBasedStripeCompactionPolicy.java
index 337f4bb..7e9972f 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SizeBasedStripeCompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/SizeBasedStripeCompactionPolicy.java
@@ -34,6 +34,7 @@ import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.StoreUtils;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig.SizeBased;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ConcatenatedLists;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -79,6 +80,18 @@ public class SizeBasedStripeCompactionPolicy extends
           || needsSingleStripeCompaction(si));
   }
 
+  @Override
+  public StripeStoreFlusher.StripeFlushRequest selectFlush(StripeInformationProvider si, int kvCount) {
+    if (this.config.doFlushToL0()) {
+      return new StripeStoreFlusher.StripeFlushRequest(); // L0 is used, return dumb request.
+    }
+    if (si.getStripeCount() == 0) {
+      // No stripes - start with just one.
+      return new StripeStoreFlusher.SizeStripeFlushRequest(1, Long.MAX_VALUE);
+    }
+    // There are stripes - do according to the boundaries.
+    return new StripeStoreFlusher.BoundaryStripeFlushRequest(si.getStripeBoundaries());
+  }
 
   /**
    * @param si StoreFileManager.
@@ -128,7 +141,7 @@ public class SizeBasedStripeCompactionPolicy extends
       return selectNewStripesCompaction(si);
     }
 
-    boolean canDropDeletesNoL0 = this.config.isAssumeOrdering() || (l0Files.size() == 0);
+    boolean canDropDeletesNoL0 = l0Files.size() == 0;
     boolean tooManyStripes = this.config.getMaxNumberOfStripes() <= stripeCount;
     // If we need to compact L0, see if we can add something to it, and drop deletes.
     if (shouldCompactL0) {
@@ -258,4 +271,4 @@ public class SizeBasedStripeCompactionPolicy extends
     double reverseNumberOfTargets = (double)this.config.getSizeTarget() / getTotalFileSize(files);
     return (long)(getTotalKvCount(files) * reverseNumberOfTargets);
   }
-}
\ No newline at end of file
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
index a914bf9..d74f152 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ConcatenatedLists;
 
@@ -76,6 +77,9 @@ public abstract class StripeCompactionPolicy<ConfigType extends StripeStoreConfi
   public abstract boolean needsCompactions(
       StripeInformationProvider sfm, List<StoreFile> filesCompacting);
 
+  public abstract StripeStoreFlusher.StripeFlushRequest selectFlush(
+      StripeInformationProvider si, int kvCount);
+
   @Override
   public boolean isMajorCompaction(Collection<StoreFile> filesToCompact) throws IOException {
     return false; // there's never a major compaction!
@@ -191,7 +195,7 @@ public abstract class StripeCompactionPolicy<ConfigType extends StripeStoreConfi
     return totalSize;
   }
 
-  protected static long getTotalFileSize(final Collection<StoreFile> candidates) {
+  public static long getTotalFileSize(final Collection<StoreFile> candidates) {
     long totalSize = 0;
     for (StoreFile storeFile : candidates) {
       totalSize += storeFile.getReader().length();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java
index a264b83..d1a4d50 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java
@@ -122,7 +122,7 @@ public class TestStripeCompactor {
     StripeCompactor sc = createCompactor(writers, input);
     List<Path> paths =
         sc.compact(createDummyRequest(), Arrays.asList(boundaries), majorFrom, majorTo);
-    writers.verifyKvs(output, allFiles);
+    writers.verifyKvs(output, allFiles, true);
     if (allFiles) {
       assertEquals(output.length, paths.size());
       writers.verifyBoundaries(boundaries);
@@ -159,7 +159,7 @@ public class TestStripeCompactor {
     List<Path> paths = sc.compact(
         createDummyRequest(), targetCount, targetSize, left, right, null, null);
     assertEquals(output.length, paths.size());
-    writers.verifyKvs(output, true);
+    writers.verifyKvs(output, true, true);
     List<byte[]> boundaries = new ArrayList<byte[]>();
     boundaries.add(left);
     for (int i = 1; i < output.length; ++i) {
@@ -239,7 +239,8 @@ public class TestStripeCompactor {
   }
 
   // StoreFile.Writer has private ctor and is unwieldy, so this has to be convoluted.
-  private static class StoreFileWritersCapture implements Answer<StoreFile.Writer> {
+  public static class StoreFileWritersCapture implements
+    Answer<StoreFile.Writer>, StripeMultiFileWriter.WriterFactory {
     public static class Writer {
       public ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
       public TreeMap<byte[], byte[]> data = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
@@ -248,7 +249,7 @@ public class TestStripeCompactor {
     private List<Writer> writers = new ArrayList<Writer>();
 
     @Override
-    public StoreFile.Writer answer(InvocationOnMock invocation) throws Throwable {
+    public StoreFile.Writer createWriter() throws IOException {
       final Writer realWriter = new Writer();
       writers.add(realWriter);
       StoreFile.Writer writer = mock(StoreFile.Writer.class);
@@ -264,7 +265,12 @@ public class TestStripeCompactor {
       return writer;
     }
 
-    public void verifyKvs(KeyValue[][] kvss, boolean allFiles) {
+    @Override
+    public StoreFile.Writer answer(InvocationOnMock invocation) throws Throwable {
+      return createWriter();
+    }
+
+    public void verifyKvs(KeyValue[][] kvss, boolean allFiles, boolean requireMetadata) {
       if (allFiles) {
         assertEquals(kvss.length, writers.size());
       }
@@ -273,8 +279,13 @@ public class TestStripeCompactor {
         KeyValue[] kvs = kvss[i];
         if (kvs != null) {
           Writer w = writers.get(i - skippedWriters);
-          assertNotNull(w.data.get(STRIPE_START_KEY));
-          assertNotNull(w.data.get(STRIPE_END_KEY));
+          if (requireMetadata) {
+            assertNotNull(w.data.get(STRIPE_START_KEY));
+            assertNotNull(w.data.get(STRIPE_END_KEY));
+          } else {
+            assertNull(w.data.get(STRIPE_START_KEY));
+            assertNull(w.data.get(STRIPE_END_KEY));
+          }
           assertEquals(kvs.length, w.kvs.size());
           for (int j = 0; j < kvs.length; ++j) {
             assertEquals(kvs[j], w.kvs.get(j));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
index 9e524d2..7a2bcc1 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
@@ -82,7 +82,7 @@ public class TestStripeStoreFileManager {
   public void testInsertFilesIntoL0() throws Exception {
     StripeStoreFileManager manager = createManager();
     MockStoreFile sf = createFile();
-    manager.insertNewFile(sf);
+    manager.insertNewFiles(al(sf));
     assertEquals(1, manager.getStorefileCount());
     Collection<StoreFile> filesForGet = manager.getFilesForScanOrGet(true, KEY_A, KEY_A);
     assertEquals(1, filesForGet.size());
@@ -98,8 +98,8 @@ public class TestStripeStoreFileManager {
   @Test
   public void testClearFiles() throws Exception {
     StripeStoreFileManager manager = createManager();
-    manager.insertNewFile(createFile());
-    manager.insertNewFile(createFile());
+    manager.insertNewFiles(al(createFile()));
+    manager.insertNewFiles(al(createFile()));
     manager.addCompactionResults(al(), al(createFile(OPEN_KEY, KEY_B),
         createFile(KEY_B, OPEN_KEY)));
     assertEquals(4, manager.getStorefileCount());
@@ -119,8 +119,8 @@ public class TestStripeStoreFileManager {
   public void testRowKeyBefore() throws Exception {
     StripeStoreFileManager manager = createManager();
     StoreFile l0File = createFile(), l0File2 = createFile();
-    manager.insertNewFile(l0File);
-    manager.insertNewFile(l0File2);
+    manager.insertNewFiles(al(l0File));
+    manager.insertNewFiles(al(l0File2));
     // Get candidate files.
     Iterator<StoreFile> sfs = manager.getCandidateFilesForRowKeyBefore(KV_B);
     sfs.next();
@@ -173,8 +173,8 @@ public class TestStripeStoreFileManager {
     // If there are no stripes, should pick midpoint from the biggest file in L0.
     MockStoreFile sf5 = createFile(5, 0);
     sf5.splitPoint = new byte[1];
-    manager.insertNewFile(sf5);
-    manager.insertNewFile(createFile(1, 0));
+    manager.insertNewFiles(al(sf5));
+    manager.insertNewFiles(al(createFile(1, 0)));
     assertEquals(sf5.splitPoint, manager.getSplitPoint());
 
     // Same if there's one stripe but the biggest file is still in L0.
@@ -258,7 +258,7 @@ public class TestStripeStoreFileManager {
 
     // Populate one L0 file.
     MockStoreFile sf0 = createFile();
-    manager.insertNewFile(sf0);
+    manager.insertNewFiles(al(sf0));
     verifyGetAndScanScenario(manager, null, null,   sf0);
     verifyGetAndScanScenario(manager, null, KEY_C,  sf0);
     verifyGetAndScanScenario(manager, KEY_B, null,  sf0);
@@ -355,14 +355,11 @@ public class TestStripeStoreFileManager {
   }
 
   @Test
-  @SuppressWarnings("unchecked")
   public void testAddingCompactionResults() throws Exception {
     StripeStoreFileManager manager = createManager();
     // First, add some L0 files and "compact" one with new stripe creation.
-    StoreFile sf_L0_0a = createFile();
-    StoreFile sf_L0_0b = createFile();
-    manager.insertNewFile(sf_L0_0a);
-    manager.insertNewFile(sf_L0_0b);
+    StoreFile sf_L0_0a = createFile(), sf_L0_0b = createFile();
+    manager.insertNewFiles(al(sf_L0_0a, sf_L0_0b));
 
     // Try compacting with invalid new branches (gaps, overlaps) - no effect.
     verifyInvalidCompactionScenario(manager, al(sf_L0_0a), al(createFile(OPEN_KEY, KEY_B)));
@@ -383,7 +380,7 @@ public class TestStripeStoreFileManager {
     StoreFile sf_L0_1 = createFile();
     StoreFile sf_i2B_1 = createFile(OPEN_KEY, KEY_B);
     StoreFile sf_B2C_1 = createFile(KEY_B, KEY_C);
-    manager.insertNewFile(sf_L0_1);
+    manager.insertNewFiles(al(sf_L0_1));
     manager.addCompactionResults(al(sf_L0_0b, sf_L0_1), al(sf_i2B_1, sf_B2C_1));
     verifyAllFiles(manager, al(sf_i2B_0, sf_B2C_0, sf_C2i_0, sf_i2B_1, sf_B2C_1));
 
@@ -399,27 +396,21 @@ public class TestStripeStoreFileManager {
     manager.addCompactionResults(al(sf_i2B_0, sf_i2B_1), al(sf_i2B_3));
     verifyAllFiles(manager, al(sf_B2C_0, sf_C2i_0, sf_B2C_1, sf_i2B_3));
 
-    // Try to rebalance two stripes, but don't take all files from them - no effect.
+    // Rebalance two stripes.
     StoreFile sf_B2D_4 = createFile(KEY_B, KEY_D);
     StoreFile sf_D2i_4 = createFile(KEY_D, OPEN_KEY);
-    ArrayList<StoreFile> compacted3 = al();
-    verifyInvalidCompactionScenario(manager, al(sf_B2C_0, sf_C2i_0), al(sf_B2D_4, sf_D2i_4));
-
-    // Rebalance two stripes correctly.
     manager.addCompactionResults(al(sf_B2C_0, sf_C2i_0, sf_B2C_1), al(sf_B2D_4, sf_D2i_4));
     verifyAllFiles(manager, al(sf_i2B_3, sf_B2D_4, sf_D2i_4));
 
     // Split the first stripe.
     StoreFile sf_i2A_5 = createFile(OPEN_KEY, KEY_A);
     StoreFile sf_A2B_5 = createFile(KEY_A, KEY_B);
-    ArrayList<StoreFile> compacted4 = al(createFile(OPEN_KEY, KEY_A), createFile(KEY_A, KEY_B));
     manager.addCompactionResults(al(sf_i2B_3), al(sf_i2A_5, sf_A2B_5));
     verifyAllFiles(manager, al(sf_B2D_4, sf_D2i_4, sf_i2A_5, sf_A2B_5));
 
     // Split the middle stripe.
     StoreFile sf_B2C_6 = createFile(KEY_B, KEY_C);
     StoreFile sf_C2D_6 = createFile(KEY_C, KEY_D);
-    ArrayList<StoreFile> compacted5 = al(createFile(KEY_B, KEY_C), createFile(KEY_C, KEY_D));
     manager.addCompactionResults(al(sf_B2D_4), al(sf_B2C_6, sf_C2D_6));
     verifyAllFiles(manager, al(sf_D2i_4, sf_i2A_5, sf_A2B_5, sf_B2C_6, sf_C2D_6));
 
@@ -428,14 +419,6 @@ public class TestStripeStoreFileManager {
     manager.addCompactionResults(al(sf_A2B_5, sf_B2C_6), al(sf_A2C_7));
     verifyAllFiles(manager, al(sf_D2i_4, sf_i2A_5, sf_C2D_6, sf_A2C_7));
 
-    // Try various range mismatch cases in replaced and new data - no effect.
-    ArrayList<StoreFile> tmp = al(sf_A2C_7, sf_C2D_6); // [A, C)
-    verifyInvalidCompactionScenario(manager, tmp, al(createFile(KEY_B, KEY_C)));
-    verifyInvalidCompactionScenario(manager, tmp, al(createFile(OPEN_KEY, KEY_D)));
-    verifyInvalidCompactionScenario(manager, tmp, al(createFile(KEY_A, OPEN_KEY)));
-    verifyInvalidCompactionScenario(manager, tmp, al(createFile(KEY_A, KEY_B)));
-    verifyInvalidCompactionScenario(manager, tmp, al(createFile(KEY_B, keyAfter(KEY_B))));
-
     // Merge lower half.
     StoreFile sf_i2C_8 = createFile(OPEN_KEY, KEY_C);
     manager.addCompactionResults(al(sf_i2A_5, sf_A2C_7), al(sf_i2C_8));
@@ -448,13 +431,39 @@ public class TestStripeStoreFileManager {
   }
 
   @Test
+  public void testCompactionAndFlushConflict() throws Exception {
+    // Add file flush into stripes
+    StripeStoreFileManager sfm = createManager();
+    assertEquals(0, sfm.getStripeCount());
+    StoreFile sf_i2c = createFile(OPEN_KEY, KEY_C), sf_c2i = createFile(KEY_C, OPEN_KEY);
+    sfm.insertNewFiles(al(sf_i2c, sf_c2i));
+    assertEquals(2, sfm.getStripeCount());
+    // Now try to add conflicting flush - should go to L0.
+    StoreFile sf_i2d = createFile(OPEN_KEY, KEY_D), sf_d2i = createFile(KEY_D, OPEN_KEY);
+    sfm.insertNewFiles(al(sf_i2d, sf_d2i));
+    assertEquals(2, sfm.getStripeCount());
+    assertEquals(2, sfm.getLevel0Files().size());
+    verifyGetAndScanScenario(sfm, KEY_C, KEY_C, sf_i2d, sf_d2i, sf_c2i);
+    // Remove these files.
+    sfm.addCompactionResults(al(sf_i2d, sf_d2i), al());
+    assertEquals(0, sfm.getLevel0Files().size());
+    // Add another file to stripe; then "rebalance" stripes w/o it - the file, which was
+    // presumably flushed during compaction, should go to L0.
+    StoreFile sf_i2c_2 = createFile(OPEN_KEY, KEY_C);
+    sfm.insertNewFiles(al(sf_i2c_2));
+    sfm.addCompactionResults(al(sf_i2c, sf_c2i), al(sf_i2d, sf_d2i));
+    assertEquals(1, sfm.getLevel0Files().size());
+    verifyGetAndScanScenario(sfm, KEY_C, KEY_C, sf_i2d, sf_i2c_2);
+  }
+
+  @Test
   public void testEmptyResultsForStripes() throws Exception {
     // Test that we can compact L0 into a subset of stripes.
     StripeStoreFileManager manager = createManager();
     StoreFile sf0a = createFile();
     StoreFile sf0b = createFile();
-    manager.insertNewFile(sf0a);
-    manager.insertNewFile(sf0b);
+    manager.insertNewFiles(al(sf0a));
+    manager.insertNewFiles(al(sf0b));
     ArrayList<StoreFile> compacted = al(createFile(OPEN_KEY, KEY_B),
         createFile(KEY_B, KEY_C), createFile(KEY_C, OPEN_KEY));
     manager.addCompactionResults(al(sf0a), compacted);
@@ -490,7 +499,7 @@ public class TestStripeStoreFileManager {
     conf.setInt("hbase.hstore.blockingStoreFiles", limit);
     StripeStoreFileManager sfm = createManager(al(), conf);
     for (int i = 0; i < l0Files; ++i) {
-      sfm.insertNewFile(createFile());
+      sfm.insertNewFiles(al(createFile()));
     }
     for (int i = 0; i < filesInStripe; ++i) {
       ArrayList<StoreFile> stripe = new ArrayList<StoreFile>();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicyTestBase.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicyTestBase.java
index 45bd292..25a02dc 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicyTestBase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicyTestBase.java
@@ -32,13 +32,19 @@ import java.util.List;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
 import org.apache.hadoop.hbase.regionserver.StripeStoreFileManager;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
+import org.apache.hadoop.hbase.regionserver.TestStripeCompactor.StoreFileWritersCapture;
 import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
 import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.StripeInformationProvider;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ConcatenatedLists;
+import org.junit.Test;
 import org.mockito.ArgumentMatcher;
 
 import com.google.common.base.Function;
@@ -53,6 +59,12 @@ public class StripeCompactionPolicyTestBase {
   protected static final byte[] KEY_D = Bytes.toBytes("ddd");
   protected static final byte[] KEY_E = Bytes.toBytes("eee");
 
+  protected static final KeyValue KV_A = new KeyValue(KEY_A, 0L);
+  protected static final KeyValue KV_B = new KeyValue(KEY_B, 0L);
+  protected static final KeyValue KV_C = new KeyValue(KEY_C, 0L);
+  protected static final KeyValue KV_D = new KeyValue(KEY_D, 0L);
+  protected static final KeyValue KV_E = new KeyValue(KEY_E, 0L);
+
   protected static ArrayList<StoreFile> al(StoreFile... sfs) {
     return new ArrayList<StoreFile>(Arrays.asList(sfs));
   }
@@ -72,6 +84,29 @@ public class StripeCompactionPolicyTestBase {
     return scr;
   }
 
+  public void testNoStripesFromFlush(
+      Function<Configuration, StripeCompactionPolicy<?>> policyMaker) throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setBoolean(StripeStoreConfig.FLUSH_TO_L0_KEY, true);
+    StripeCompactionPolicy<?> policy = policyMaker.apply(conf);
+    StripeInformationProvider si = createStripesL0Only(0, 0);
+
+    KeyValue[] input = new KeyValue[] { KV_A, KV_B, KV_C, KV_D, KV_E };
+    KeyValue[][] expected = new KeyValue[][] { input };
+    verifyFlush(policy, si, input, expected, null);
+  }
+
+  public void testOldStripesFromFlush(
+      Function<Configuration, StripeCompactionPolicy<?>> policyMaker) throws Exception {
+    StripeCompactionPolicy<?> policy = policyMaker.apply(HBaseConfiguration.create());
+    StripeInformationProvider si = createStripes(0, KEY_C, KEY_D);
+
+    KeyValue[] input = new KeyValue[] { KV_B, KV_C, KV_C, KV_D, KV_E };
+    KeyValue[][] expected = new KeyValue[][] { new KeyValue[] { KV_B },
+        new KeyValue[] { KV_C, KV_C }, new KeyValue[] {  KV_D, KV_E } };
+    verifyFlush(policy, si, input, expected, new byte[][] { OPEN_KEY, KEY_C, KEY_D, OPEN_KEY });
+  }
+
   protected void testSingleStripeDropDeletes(
       Function<Configuration, StripeCompactionPolicy<?>> policyMaker) throws Exception {
     Configuration conf = HBaseConfiguration.create();
@@ -94,11 +129,6 @@ public class StripeCompactionPolicyTestBase {
     si = createStripesWithSizes(6, 2,
         new Long[][] { new Long[] { 10L, 1L, 1L, 1L, 1L }, new Long[] { 12L } });
     verifyCompaction(policy, si, si.getLevel0Files(), null, null, si.getStripeBoundaries());
-    // We can also drop deletes despite L0 if config says it's safe.
-    conf.setBoolean(StripeStoreConfig.ASSUME_ORDERING_KEY, true);
-    policy = policyMaker.apply(conf);
-    si = createStripesWithSizes(2, 2, stripes);
-    verifySingleStripeCompaction(policy, si, 0, true);
   }
 
   protected void testParallelCompactions(StripeCompactionPolicy<?> policy) throws Exception {
@@ -198,6 +228,24 @@ public class StripeCompactionPolicyTestBase {
         dropDeletesMatcher(dropDeletes, start), dropDeletesMatcher(dropDeletes, end));
   }
 
+  /** Verify arbitrary flush. */
+  protected void verifyFlush(StripeCompactionPolicy<?> policy, StripeInformationProvider si,
+      KeyValue[] input, KeyValue[][] expected, byte[][] boundaries) throws IOException {
+    StoreFileWritersCapture writers = new StoreFileWritersCapture();
+    StripeStoreFlusher.StripeFlushRequest req = policy.selectFlush(si, input.length);
+    StripeMultiFileWriter mw = req.createWriter();
+    mw.prepare(null, writers, new KVComparator());
+    for (KeyValue kv : input) {
+      mw.append(kv);
+    }
+    boolean hasMetadata = boundaries != null;
+    mw.finalizeWriters();
+    writers.verifyKvs(expected, true, hasMetadata);
+    if (hasMetadata) {
+      writers.verifyBoundaries(boundaries);
+    }
+  }
+
   private byte[] dropDeletesMatcher(Boolean dropDeletes, byte[] value) {
     return dropDeletes == null ? any(byte[].class)
             : (dropDeletes.booleanValue() ? aryEq(value) : isNull(byte[].class));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCountBasedStripeCompactionPolicy.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCountBasedStripeCompactionPolicy.java
index c32eee5..ad03e52 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCountBasedStripeCompactionPolicy.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestCountBasedStripeCompactionPolicy.java
@@ -29,6 +29,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
@@ -79,6 +80,26 @@ public class TestCountBasedStripeCompactionPolicy extends StripeCompactionPolicy
         policy, si, si.getStorefiles(), true, defaultStripeCount, null, OPEN_KEY, OPEN_KEY);
   }
 
+  @Test
+  public void testNoStripesFromFlush() throws Exception {
+    testNoStripesFromFlush(createPolicyMaker(defaultStripeCount));
+  }
+
+  @Test
+  public void testOldStripesFromFlush() throws Exception {
+    testOldStripesFromFlush(createPolicyMaker(3));
+  }
+
+  @Test
+  public void testNewStripesFromFlush() throws Exception {
+    StripeCompactionPolicy<?> policy = createPolicy(HBaseConfiguration.create());
+    StripeInformationProvider si = createStripesL0Only(0, 0);
+
+    KeyValue[] input = new KeyValue[] { KV_B, KV_C, KV_C, KV_D, KV_E };
+    KeyValue[][] expected = new KeyValue[][] { new KeyValue[] { KV_B, KV_C, KV_C },
+        new KeyValue[] {  KV_D, KV_E } };
+    verifyFlush(policy, si, input, expected, new byte[][] { OPEN_KEY, KEY_D, OPEN_KEY });
+  }
 
   @Test
   public void testExistingStripesFromL0() throws Exception {
@@ -132,10 +153,6 @@ public class TestCountBasedStripeCompactionPolicy extends StripeCompactionPolicy
     // But cannot be dropped if there are.
     si = createStripesWithSizes(2, 2, 1, 7, 2);
     verifyWholeStripesCompaction(policy, si, 0, 1,    false, 2, 4L);
-    // Unless of course the config says it's safe.
-    conf.setBoolean(StripeStoreConfig.ASSUME_ORDERING_KEY, true);
-    policy = createPolicy(conf, 3);
-    verifyWholeStripesCompaction(policy, si, 0, 1,    true, 2, 4L);
   }
 
   @Test
@@ -192,17 +209,21 @@ public class TestCountBasedStripeCompactionPolicy extends StripeCompactionPolicy
 
   @Test
   public void testSingleStripeDropDeletes() throws Exception {
-    testSingleStripeDropDeletes(new Function<Configuration, StripeCompactionPolicy<?>>() {
+    testSingleStripeDropDeletes(createPolicyMaker(2));
+  }
+
+  private Function<Configuration, StripeCompactionPolicy<?>> createPolicyMaker(
+      final int stripeCount) {
+    return new Function<Configuration, StripeCompactionPolicy<?>>() {
       @Override
       public StripeCompactionPolicy<?> apply(Configuration conf) {
-        conf.setInt(StripeStoreConfig.CountBased.FIXED_COUNT_KEY, 2);
         try {
-          return createPolicy(conf);
+          return createPolicy(conf, stripeCount);
         } catch (Exception e) {
           return null;
         }
       }
-    });
+    };
   }
 
   private static CountBasedStripeCompactionPolicy createPolicy(Configuration conf)
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestSizeBasedStripeCompactionPolicy.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestSizeBasedStripeCompactionPolicy.java
index 553c8a2..b6f88de 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestSizeBasedStripeCompactionPolicy.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestSizeBasedStripeCompactionPolicy.java
@@ -31,15 +31,13 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
 import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
-import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.SplitStripeCompactionRequest;
 import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.StripeInformationProvider;
-import org.apache.hadoop.hbase.util.ConcatenatedLists;
-import org.apache.hadoop.hbase.util.EnvironmentEdge;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
 import org.apache.hadoop.hbase.util.ManualEnvironmentEdge;
 import org.junit.Test;
@@ -152,9 +150,6 @@ public class TestSizeBasedStripeCompactionPolicy extends StripeCompactionPolicyT
     // But cannot be dropped if there are.
     si = createStripesWithSizes(2, 2, noSplit, toSplit);
     verifyWholeStripesCompaction(policy, si, 1, 1,    false, null, defaultTargetSize);
-    // Unless of course the config says it's safe.
-    conf.setBoolean(StripeStoreConfig.ASSUME_ORDERING_KEY, true);
-    verifyWholeStripesCompaction(createPolicy(conf), si, 1, 1, true, null, defaultTargetSize);
   }
 
   @SuppressWarnings("unchecked")
@@ -204,8 +199,32 @@ public class TestSizeBasedStripeCompactionPolicy extends StripeCompactionPolicyT
   }
 
   @Test
+  public void testNoStripesFromFlush() throws Exception {
+    testNoStripesFromFlush(createPolicyMaker());
+  }
+
+  @Test
+  public void testOldStripesFromFlush() throws Exception {
+    testOldStripesFromFlush(createPolicyMaker());
+  }
+
+  @Test
+  public void testNewStripesFromFlush() throws Exception {
+    StripeCompactionPolicy<?> policy = createPolicy(HBaseConfiguration.create());
+    StripeInformationProvider si = createStripesL0Only(0, 0);
+    KeyValue[] input = new KeyValue[] { KV_B, KV_C, KV_C, KV_D, KV_E };
+    // Starts with one stripe; unlike flush results, must have metadata
+    KeyValue[][] expected = new KeyValue[][] { input };
+    verifyFlush(policy, si, input, expected, new byte[][] { OPEN_KEY, OPEN_KEY });
+  }
+
+  @Test
   public void testSingleStripeDropDeletes() throws Exception {
-    testSingleStripeDropDeletes(new Function<Configuration, StripeCompactionPolicy<?>>() {
+    testSingleStripeDropDeletes(createPolicyMaker());
+  }
+
+  protected Function<Configuration, StripeCompactionPolicy<?>> createPolicyMaker() {
+    return new Function<Configuration, StripeCompactionPolicy<?>>() {
       @Override
       public StripeCompactionPolicy<?> apply(Configuration conf) {
         try {
@@ -214,7 +233,7 @@ public class TestSizeBasedStripeCompactionPolicy extends StripeCompactionPolicyT
           return null;
         }
       }
-    });
+    };
   }
 
   private static SizeBasedStripeCompactionPolicy createPolicy(Configuration conf)
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
index ba59b54..a48b20a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
@@ -31,13 +31,11 @@ import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher.StripeFlushRequest;
 import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.StripeInformationProvider;
-import org.apache.hadoop.hbase.util.ConcatenatedLists;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
-import com.google.common.collect.ImmutableList;
-
 /**
  * Doesn't test the ratio algorithm as such; that is done in other tests.
  * Only relation between the stripes with the simplest cases inside.
@@ -112,6 +110,11 @@ public class TestStripeCompactionPolicy extends StripeCompactionPolicyTestBase {
     }
 
     @Override
+    public StripeFlushRequest selectFlush(StripeInformationProvider si, int kvCount) {
+      throw new NotImplementedException();
+    }
+
+    @Override
     public boolean needsCompactions(StripeInformationProvider si, List<StoreFile> filesCompacting) {
       if (!filesCompacting.isEmpty()) return false;
       return needsSingleStripeCompaction(si);
