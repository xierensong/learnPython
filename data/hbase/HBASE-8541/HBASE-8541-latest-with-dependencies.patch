diff --git hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index 06675b2..309ff2a 100644
--- hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -2345,10 +2345,25 @@ public class KeyValue implements Cell, HeapSize, Cloneable {
     private boolean matchingRows(final KeyValue left, final short lrowlength,
         final KeyValue right, final short rrowlength) {
       return lrowlength == rrowlength &&
-          Bytes.equals(left.getBuffer(), left.getRowOffset(), lrowlength,
+          matchingRows(left.getBuffer(), left.getRowOffset(), lrowlength,
               right.getBuffer(), right.getRowOffset(), rrowlength);
     }
 
+    /**
+     * Compare rows. Just calls Bytes.equals, but it's good to have this encapsulated.
+     * @param left Left row array.
+     * @param loffset Left row offset.
+     * @param llength Left row length.
+     * @param right Right row array.
+     * @param roffset Right row offset.
+     * @param rlength Right row length.
+     * @return Whether rows are the same row.
+     */
+    public boolean matchingRows(final byte [] left, final int loffset, final int llength,
+        final byte [] right, final int roffset, final int rlength) {
+      return Bytes.equals(left, loffset, llength, right, roffset, rlength);
+    }
+
     public byte[] calcIndexKey(byte[] lastKeyOfPreviousBlock, byte[] firstKeyInBlock) {
       byte[] fakeKey = getShortMidpointKey(lastKeyOfPreviousBlock, firstKeyInBlock);
       if (compareFlatKey(fakeKey, firstKeyInBlock) > 0) {
diff --git hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java
new file mode 100644
index 0000000..67a0b85
--- /dev/null
+++ hbase-common/src/main/java/org/apache/hadoop/hbase/util/ConcatenatedLists.java
@@ -0,0 +1,166 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.lang.reflect.Array;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.NoSuchElementException;
+
+/**
+ * A collection class that contains multiple sub-lists, which allows us to not copy lists.
+ * This class does not support modification. The derived classes that add modifications are
+ * not thread-safe.
+ * NOTE: Doesn't implement list as it is not necessary for current usage, feel free to add.
+ */
+public class ConcatenatedLists<T> implements Collection<T> {
+  protected final ArrayList<List<T>> components = new ArrayList<List<T>>();
+  protected int size = 0;
+
+  public void addAllSublists(List<? extends List<T>> items) {
+    for (List<T> list : items) {
+      addSublist(list);
+    }
+  }
+
+  public void addSublist(List<T> items) {
+    if (!items.isEmpty()) {
+      this.components.add(items);
+      this.size += items.size();
+    }
+  }
+
+  @Override
+  public int size() {
+    return this.size;
+  }
+
+  @Override
+  public boolean isEmpty() {
+    return this.size == 0;
+  }
+
+  @Override
+  public boolean contains(Object o) {
+    for (List<T> component : this.components) {
+      if (component.contains(o)) return true;
+    }
+    return false;
+  }
+
+  @Override
+  public boolean containsAll(Collection<?> c) {
+    for (Object o : c) {
+      if (!contains(o)) return false;
+    }
+    return true;
+  }
+
+  @Override
+  public Object[] toArray() {
+    return toArray((Object[])Array.newInstance(Object.class, this.size));
+  }
+
+  @Override
+  @SuppressWarnings("unchecked")
+  public <U> U[] toArray(U[] a) {
+    U[] result = (a.length == this.size()) ? a
+        : (U[])Array.newInstance(a.getClass().getComponentType(), this.size);
+    int i = 0;
+    for (List<T> component : this.components) {
+      for (T t : component) {
+        result[i] = (U)t;
+        ++i;
+      }
+    }
+    return result;
+  }
+
+  @Override
+  public boolean add(T e) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean remove(Object o) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean addAll(Collection<? extends T> c) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean removeAll(Collection<?> c) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public boolean retainAll(Collection<?> c) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public void clear() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public java.util.Iterator<T> iterator() {
+    return new Iterator();
+  }
+
+  public class Iterator implements java.util.Iterator<T> {
+    protected int currentComponent = 0;
+    protected int indexWithinComponent = -1;
+    protected boolean nextWasCalled = false;
+
+    @Override
+    public boolean hasNext() {
+      return (currentComponent + 1) < components.size()
+          || ((currentComponent + 1) == components.size()
+              && ((indexWithinComponent + 1) < components.get(currentComponent).size()));
+    }
+
+    @Override
+    public T next() {
+      if (!components.isEmpty()) {
+        this.nextWasCalled = true;
+        List<T> src = components.get(currentComponent);
+        if (++indexWithinComponent < src.size()) return src.get(indexWithinComponent);
+        if (++currentComponent < components.size()) {
+          indexWithinComponent = 0;
+          src = components.get(currentComponent);
+          assert src.size() > 0;
+          return src.get(indexWithinComponent);
+        }
+      }
+      this.nextWasCalled = false;
+      throw new NoSuchElementException();
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+}
diff --git hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java
new file mode 100644
index 0000000..ce1a616
--- /dev/null
+++ hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestConcatenatedLists.java
@@ -0,0 +1,140 @@
+/*
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Iterator;
+import java.util.List;
+import java.util.NoSuchElementException;
+
+import org.apache.hadoop.hbase.SmallTests;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import static org.junit.Assert.*;
+
+@Category(SmallTests.class)
+public class TestConcatenatedLists {
+  @Test
+  public void testUnsupportedOps() {
+    // If adding support, add tests.
+    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    c.addSublist(Arrays.asList(0L, 1L));
+    try {
+      c.add(2L);
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    try {
+      c.addAll(Arrays.asList(2L, 3L));
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    try {
+      c.remove(0L);
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    try {
+      c.removeAll(Arrays.asList(0L, 1L));
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    try {
+      c.clear();
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    try {
+      c.retainAll(Arrays.asList(0L, 1L));
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+    Iterator<Long> iter = c.iterator();
+    iter.next();
+    try {
+      iter.remove();
+      fail("Should throw");
+    } catch (UnsupportedOperationException ex) {
+    }
+  }
+
+  @Test
+  public void testEmpty() {
+    verify(new ConcatenatedLists<Long>(), -1);
+  }
+
+  @Test
+  public void testOneOne() {
+    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    c.addSublist(Arrays.asList(0L));
+    verify(c, 0);
+  }
+
+  @Test
+  public void testOneMany() {
+    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    c.addSublist(Arrays.asList(0L, 1L, 2L));
+    verify(c, 2);
+  }
+
+  @Test
+  @SuppressWarnings("unchecked")
+  public void testManyOne() {
+    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    c.addSublist(Arrays.asList(0L));
+    c.addAllSublists(Arrays.asList(Arrays.asList(1L), Arrays.asList(2L)));
+    verify(c, 2);
+  }
+
+  @Test
+  @SuppressWarnings("unchecked")
+  public void testManyMany() {
+    ConcatenatedLists<Long> c = new ConcatenatedLists<Long>();
+    c.addAllSublists(Arrays.asList(Arrays.asList(0L, 1L)));
+    c.addSublist(Arrays.asList(2L, 3L, 4L));
+    c.addAllSublists(Arrays.asList(Arrays.asList(5L), Arrays.asList(6L, 7L)));
+    verify(c, 7);
+  }
+
+  private void verify(ConcatenatedLists<Long> c, int last) {
+    assertEquals((last == -1), c.isEmpty());
+    assertEquals(last + 1, c.size());
+    assertTrue(c.containsAll(c));
+    Long[] array = c.toArray(new Long[0]);
+    List<Long> all = new ArrayList<Long>();
+    Iterator<Long> iter = c.iterator();
+    for (Long i = 0L; i <= last; ++i) {
+      assertTrue(iter.hasNext());
+      assertEquals(i, iter.next());
+      assertEquals(i, array[i.intValue()]);
+      assertTrue(c.contains(i));
+      assertTrue(c.containsAll(Arrays.asList(i)));
+      all.add(i);
+    }
+    assertTrue(c.containsAll(all));
+    assertFalse(iter.hasNext());
+    try {
+      iter.next();
+      fail("Should have thrown");
+    } catch (NoSuchElementException nsee) {
+    }
+  }
+}
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
index fbd9070..4e3a5f7 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngest.java
@@ -62,6 +62,10 @@ public class IntegrationTestIngest extends IntegrationTestBase {
     loadTool.setConf(util.getConfiguration());
     // Initialize load test tool before we start breaking things;
     // LoadTestTool init, even when it is a no-op, is very fragile.
+    initTable();
+  }
+
+  protected void initTable() throws IOException {
     int ret = loadTool.run(new String[] { "-tn", getTablename(), "-init_only" });
     Assert.assertEquals("Failed to initialize LoadTestTool", 0, ret);
   }
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestStripeCompactions.java hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestStripeCompactions.java
new file mode 100644
index 0000000..1ac2834
--- /dev/null
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestIngestStripeCompactions.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.regionserver.HStore;
+import org.apache.hadoop.hbase.regionserver.StoreEngine;
+import org.apache.hadoop.hbase.regionserver.StripeStoreEngine;
+import org.apache.hadoop.hbase.util.LoadTestTool;
+import org.junit.experimental.categories.Category;
+
+/**
+ * A test class that does the same things as IntegrationTestIngest but with stripe
+ * compactions. Can be used with ChaosMonkey in the same manner.
+ */
+@Category(IntegrationTests.class)
+public class IntegrationTestIngestStripeCompactions extends IntegrationTestIngest {
+  @Override
+  protected void initTable() throws IOException {
+    // Do the same as the LoadTestTool does, but with different table configuration.
+    HTableDescriptor htd = new HTableDescriptor(getTablename());
+    htd.setConfiguration(StoreEngine.STORE_ENGINE_CLASS_KEY, StripeStoreEngine.class.getName());
+    htd.setConfiguration(HStore.BLOCKING_STOREFILES_KEY, "100");
+    HColumnDescriptor hcd = new HColumnDescriptor(LoadTestTool.COLUMN_FAMILY);
+    HBaseTestingUtility.createPreSplitLoadTestTable(util.getConfiguration(), htd, hcd);
+  }
+}
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/StripeCompactionsPerformanceEvaluation.java hbase-it/src/test/java/org/apache/hadoop/hbase/StripeCompactionsPerformanceEvaluation.java
new file mode 100644
index 0000000..ec0f149
--- /dev/null
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/StripeCompactionsPerformanceEvaluation.java
@@ -0,0 +1,351 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.commons.cli.CommandLine;
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy;
+import org.apache.hadoop.hbase.regionserver.HStore;
+import org.apache.hadoop.hbase.regionserver.StoreEngine;
+import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
+import org.apache.hadoop.hbase.regionserver.StripeStoreEngine;
+import org.apache.hadoop.hbase.util.AbstractHBaseTool;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.LoadTestTool;
+import org.apache.hadoop.hbase.util.MultiThreadedAction;
+import org.apache.hadoop.hbase.util.MultiThreadedReader;
+import org.apache.hadoop.hbase.util.MultiThreadedWriter;
+import org.apache.hadoop.hbase.util.RegionSplitter;
+import org.apache.hadoop.hbase.util.test.LoadTestDataGenerator;
+import org.apache.hadoop.hbase.util.test.LoadTestKVGenerator;
+import org.junit.Assert;
+
+
+/**
+ * A perf test which does large data ingestion using stripe compactions and regular compactions.
+ */
+@InterfaceAudience.Private
+public class StripeCompactionsPerformanceEvaluation extends AbstractHBaseTool {
+  private static final Log LOG = LogFactory.getLog(StripeCompactionsPerformanceEvaluation.class);
+  private static final String TABLE_NAME =
+      StripeCompactionsPerformanceEvaluation.class.getSimpleName();
+  private static final byte[] COLUMN_FAMILY = Bytes.toBytes("CF");
+  private static final int MIN_NUM_SERVERS = 1;
+
+  // Option names.
+  private static final String DATAGEN_KEY = "datagen";
+  private static final String ITERATIONS_KEY = "iters";
+  private static final String PRELOAD_COUNT_KEY = "pwk";
+  private static final String WRITE_COUNT_KEY = "wk";
+  private static final String WRITE_THREADS_KEY = "wt";
+  private static final String READ_THREADS_KEY = "rt";
+  private static final String INITIAL_STRIPE_COUNT_KEY = "initstripes";
+  private static final String SPLIT_SIZE_KEY = "splitsize";
+  private static final String SPLIT_PARTS_KEY = "splitparts";
+  private static final String VALUE_SIZE_KEY = "valsize";
+  private static final String SEQ_SHARDS_PER_SERVER_KEY = "seqshards";
+
+  // Option values.
+  private LoadTestDataGenerator dataGen;
+  private int iterationCount;
+  private long preloadKeys;
+  private long writeKeys;
+  private int writeThreads;
+  private int readThreads;
+  private Long initialStripeCount;
+  private Long splitSize;
+  private Long splitParts;
+
+  private static final String VALUE_SIZE_DEFAULT = "512:4096";
+
+  protected IntegrationTestingUtility util = new IntegrationTestingUtility();
+
+  @Override
+  protected void addOptions() {
+    addOptWithArg(DATAGEN_KEY, "Type of data generator to use (default or sequential)");
+    addOptWithArg(SEQ_SHARDS_PER_SERVER_KEY, "Sequential generator will shard the data into many"
+        + " sequences. The number of such shards per server is specified (default is 1).");
+    addOptWithArg(ITERATIONS_KEY, "Number of iterations to run to compare");
+    addOptWithArg(PRELOAD_COUNT_KEY, "Number of keys to preload, per server");
+    addOptWithArg(WRITE_COUNT_KEY, "Number of keys to write, per server");
+    addOptWithArg(WRITE_THREADS_KEY, "Number of threads to use for writing");
+    addOptWithArg(READ_THREADS_KEY, "Number of threads to use for reading");
+    addOptWithArg(INITIAL_STRIPE_COUNT_KEY, "Number of stripes to split regions into initially");
+    addOptWithArg(SPLIT_SIZE_KEY, "Size at which a stripe will split into more stripes");
+    addOptWithArg(SPLIT_PARTS_KEY, "Number of stripes to split a stripe into when it splits");
+    addOptWithArg(VALUE_SIZE_KEY, "Value size; either a number, or a colon-separated range;"
+        + " default " + VALUE_SIZE_DEFAULT);
+  }
+
+  @Override
+  protected void processOptions(CommandLine cmd) {
+    int minValueSize = 0, maxValueSize = 0;
+    String valueSize = cmd.getOptionValue(VALUE_SIZE_KEY, VALUE_SIZE_DEFAULT);
+    if (valueSize.contains(":")) {
+      String[] valueSizes = valueSize.split(":");
+      if (valueSize.length() != 2) throw new RuntimeException("Invalid value size: " + valueSize);
+      minValueSize = Integer.parseInt(valueSizes[0]);
+      maxValueSize = Integer.parseInt(valueSizes[1]);
+    } else {
+      minValueSize = maxValueSize = Integer.parseInt(valueSize);
+    }
+    String datagen = cmd.getOptionValue(DATAGEN_KEY, "default").toLowerCase();
+    if ("default".equals(datagen)) {
+      dataGen = new MultiThreadedAction.DefaultDataGenerator(
+          minValueSize, maxValueSize, 1, 1, new byte[][] { COLUMN_FAMILY });
+    } else if ("sequential".equals(datagen)) {
+      int shards = Integer.parseInt(cmd.getOptionValue(SEQ_SHARDS_PER_SERVER_KEY, "1"));
+      dataGen = new SeqShardedDataGenerator(minValueSize, maxValueSize, shards);
+    } else {
+      throw new RuntimeException("Unknown " + DATAGEN_KEY + ": " + datagen);
+    }
+    iterationCount = Integer.parseInt(cmd.getOptionValue(ITERATIONS_KEY, "1"));
+    preloadKeys = Long.parseLong(cmd.getOptionValue(PRELOAD_COUNT_KEY, "1000000"));
+    writeKeys = Long.parseLong(cmd.getOptionValue(WRITE_COUNT_KEY, "1000000"));
+    writeThreads = Integer.parseInt(cmd.getOptionValue(WRITE_THREADS_KEY, "10"));
+    readThreads = Integer.parseInt(cmd.getOptionValue(READ_THREADS_KEY, "20"));
+    initialStripeCount = getLongOrNull(cmd, INITIAL_STRIPE_COUNT_KEY);
+    splitSize = getLongOrNull(cmd, SPLIT_SIZE_KEY);
+    splitParts = getLongOrNull(cmd, SPLIT_PARTS_KEY);
+  }
+
+  private Long getLongOrNull(CommandLine cmd, String option) {
+    if (!cmd.hasOption(option)) return null;
+    return Long.parseLong(cmd.getOptionValue(option));
+  }
+
+  @Override
+  public Configuration getConf() {
+    Configuration c = super.getConf();
+    if (c == null && util != null) {
+      conf = util.getConfiguration();
+      c = conf;
+    }
+    return c;
+  }
+
+  @Override
+  protected int doWork() throws Exception {
+    setUp();
+    try {
+      boolean isStripe = true;
+      for (int i = 0; i < iterationCount * 2; ++i) {
+        createTable(isStripe);
+        runOneTest((isStripe ? "Stripe" : "Default") + i, conf);
+        isStripe = !isStripe;
+      }
+      return 0;
+    } finally {
+      tearDown();
+    }
+  }
+
+
+  private void setUp() throws Exception {
+    this.util = new IntegrationTestingUtility();
+    LOG.debug("Initializing/checking cluster has " + MIN_NUM_SERVERS + " servers");
+    util.initializeCluster(MIN_NUM_SERVERS);
+    LOG.debug("Done initializing/checking cluster");
+  }
+
+  protected void deleteTable() throws Exception {
+    if (util.getHBaseAdmin().tableExists(TABLE_NAME)) {
+      LOG.info("Deleting table");
+      if (!util.getHBaseAdmin().isTableDisabled(TABLE_NAME)) {
+        util.getHBaseAdmin().disableTable(TABLE_NAME);
+      }
+      util.getHBaseAdmin().deleteTable(TABLE_NAME);
+      LOG.info("Deleted table");
+    }
+  }
+
+  private void createTable(boolean isStripe) throws Exception {
+    createTable(createHtd(isStripe));
+  }
+
+  private void tearDown() throws Exception {
+    deleteTable();
+    LOG.info("Restoring the cluster");
+    util.restoreCluster();
+    LOG.info("Done restoring the cluster");
+  }
+
+  private void runOneTest(String description, Configuration conf) throws Exception {
+    int numServers = util.getHBaseClusterInterface().getClusterStatus().getServersSize();
+    long startKey = (long)preloadKeys * numServers;
+    long endKey = startKey + (long)writeKeys * numServers;
+    status(String.format("%s test starting on %d servers; preloading 0 to %d and writing to %d",
+        description, numServers, startKey, endKey));
+
+    TableName tn = TableName.valueOf(TABLE_NAME);
+    if (preloadKeys > 0) {
+      MultiThreadedWriter preloader = new MultiThreadedWriter(dataGen, conf, tn);
+      long time = System.currentTimeMillis();
+      preloader.start(0, startKey, writeThreads, false, 0, 0);
+      preloader.waitForFinish();
+      if (preloader.getNumWriteFailures() > 0) {
+        throw new IOException("Preload failed");
+      }
+      int waitTime = (int)Math.min(preloadKeys / 100, 30000); // arbitrary
+      status(description + " preload took " + (System.currentTimeMillis()-time)/1000
+          + "sec; sleeping for " + waitTime/1000 + "sec for store to stabilize");
+      Thread.sleep(waitTime);
+    }
+
+    MultiThreadedWriter writer = new MultiThreadedWriter(dataGen, conf, tn);
+    MultiThreadedReader reader = new MultiThreadedReader(dataGen, conf, tn, 100);
+    // reader.getMetrics().enable();
+    reader.linkToWriter(writer);
+
+    long testStartTime = System.currentTimeMillis();
+    writer.start(startKey, endKey, writeThreads, false, 0, 0);
+    reader.start(startKey, endKey, readThreads, /* rdmReadThreads, Long.MAX_VALUE, */ false, 0, 0);
+    writer.waitForFinish();
+    reader.waitForFinish();
+    // reader.waitForVerification(300000);
+    // reader.abortAndWaitForFinish();
+    status("Readers and writers stopped for test " + description);
+
+    boolean success = writer.getNumWriteFailures() == 0;
+    if (!success) {
+      LOG.error("Write failed");
+    } else {
+      success = reader.getNumReadErrors() == 0 && reader.getNumReadFailures() == 0;
+      if (!success) {
+        LOG.error("Read failed");
+      }
+    }
+
+    // Dump perf regardless of the result.
+    /*StringBuilder perfDump = new StringBuilder();
+    for (Pair<Long, Long> pt : reader.getMetrics().getCombinedCdf()) {
+      perfDump.append(String.format(
+          "csvread,%s,%d,%d%n", description, pt.getFirst(), pt.getSecond()));
+    }
+    if (dumpTimePerf) {
+      Iterator<Triple<Long, Double, Long>> timePerf = reader.getMetrics().getCombinedTimeSeries();
+      while (timePerf.hasNext()) {
+        Triple<Long, Double, Long> pt = timePerf.next();
+        perfDump.append(String.format("csvtime,%s,%d,%d,%.4f%n",
+            description, pt.getFirst(), pt.getThird(), pt.getSecond()));
+      }
+    }
+    LOG.info("Performance data dump for " + description + " test: \n" + perfDump.toString());*/
+    status(description + " test took " + (System.currentTimeMillis()-testStartTime)/1000 + "sec");
+    Assert.assertTrue(success);
+  }
+
+  private static void status(String s) {
+    LOG.info("STATUS " + s);
+    System.out.println(s);
+  }
+
+  private HTableDescriptor createHtd(boolean isStripe) throws Exception {
+    HTableDescriptor htd = new HTableDescriptor(TABLE_NAME);
+    htd.addFamily(new HColumnDescriptor(COLUMN_FAMILY));
+    String noSplitsPolicy = DisabledRegionSplitPolicy.class.getName();
+    htd.setConfiguration(HConstants.HBASE_REGION_SPLIT_POLICY_KEY, noSplitsPolicy);
+    if (isStripe) {
+      htd.setConfiguration(StoreEngine.STORE_ENGINE_CLASS_KEY, StripeStoreEngine.class.getName());
+      if (initialStripeCount != null) {
+        htd.setConfiguration(
+            StripeStoreConfig.INITIAL_STRIPE_COUNT_KEY, initialStripeCount.toString());
+        htd.setConfiguration(
+            HStore.BLOCKING_STOREFILES_KEY, Long.toString(10 * initialStripeCount));
+      } else {
+        htd.setConfiguration(HStore.BLOCKING_STOREFILES_KEY, "500");
+      }
+      if (splitSize != null) {
+        htd.setConfiguration(StripeStoreConfig.SIZE_TO_SPLIT_KEY, splitSize.toString());
+      }
+      if (splitParts != null) {
+        htd.setConfiguration(StripeStoreConfig.SPLIT_PARTS_KEY, splitParts.toString());
+      }
+    } else {
+      htd.setConfiguration(HStore.BLOCKING_STOREFILES_KEY, "10"); // default
+    }
+    return htd;
+  }
+
+  protected void createTable(HTableDescriptor htd) throws Exception {
+    deleteTable();
+    if (util.getHBaseClusterInterface() instanceof MiniHBaseCluster) {
+      LOG.warn("Test does not make a lot of sense for minicluster. Will set flush size low.");
+      htd.setConfiguration(HConstants.HREGION_MEMSTORE_FLUSH_SIZE, "1048576");
+    }
+    byte[][] splits = new RegionSplitter.HexStringSplit().split(
+        util.getHBaseClusterInterface().getClusterStatus().getServersSize());
+    util.getHBaseAdmin().createTable(htd, splits);
+  }
+
+  public static class SeqShardedDataGenerator extends LoadTestDataGenerator {
+    private static final byte[][] COLUMN_NAMES = new byte[][] { Bytes.toBytes("col1") };
+    private static final int PAD_TO = 10;
+    private static final int PREFIX_PAD_TO = 7;
+
+    private final int numPartitions;
+
+    public SeqShardedDataGenerator(int minValueSize, int maxValueSize, int numPartitions) {
+      super(minValueSize, maxValueSize);
+      this.numPartitions = numPartitions;
+    }
+
+    @Override
+    public byte[] getDeterministicUniqueKey(long keyBase) {
+      String num = StringUtils.leftPad(String.valueOf(keyBase), PAD_TO, "0");
+      return Bytes.toBytes(getPrefix(keyBase) + num);
+    }
+
+    private String getPrefix(long i) {
+      return StringUtils.leftPad(String.valueOf((int)(i % numPartitions)), PREFIX_PAD_TO, "0");
+    }
+
+    @Override
+    public byte[][] getColumnFamilies() {
+      return new byte[][] { COLUMN_FAMILY };
+    }
+
+    @Override
+    public byte[][] generateColumnsForCf(byte[] rowKey, byte[] cf) {
+      return COLUMN_NAMES;
+    }
+
+    @Override
+    public byte[] generateValue(byte[] rowKey, byte[] cf, byte[] column) {
+      return kvGenerator.generateRandomSizeValue(rowKey, cf, column);
+    }
+
+    @Override
+    public boolean verify(byte[] rowKey, byte[] cf, byte[] column, byte[] value) {
+      return LoadTestKVGenerator.verify(value, rowKey, cf, column);
+    }
+
+    @Override
+    public boolean verify(byte[] rowKey, byte[] cf, Set<byte[]> columnSet) {
+      return true;
+    }
+  };
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.java
index e7784ab..f2a0b06 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.java
@@ -54,6 +54,12 @@ public class DefaultStoreEngine extends StoreEngine<
     DEFAULT_COMPACTION_POLICY_CLASS = ExploringCompactionPolicy.class;
 
   @Override
+  public boolean needsCompaction(List<StoreFile> filesCompacting) {
+    return compactionPolicy.needsCompaction(
+        this.storeFileManager.getStorefiles(), filesCompacting);
+  }
+
+  @Override
   protected void createComponents(
       Configuration conf, Store store, KVComparator kvComparator) throws IOException {
     storeFileManager = new DefaultStoreFileManager(kvComparator, conf);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
index a77f89f..86649bf 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.java
@@ -68,9 +68,9 @@ class DefaultStoreFileManager implements StoreFileManager {
   }
 
   @Override
-  public void insertNewFile(StoreFile sf) {
+  public void insertNewFiles(Collection<StoreFile> sfs) throws IOException {
     ArrayList<StoreFile> newFiles = new ArrayList<StoreFile>(storefiles);
-    newFiles.add(sf);
+    newFiles.addAll(sfs);
     sortAndSetStoreFiles(newFiles);
   }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
index 79f9379..a5837c2 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.java
@@ -53,13 +53,7 @@ public class DefaultStoreFlusher extends StoreFlusher {
 
     // Use a store scanner to find which rows to flush.
     long smallestReadPoint = store.getSmallestReadPoint();
-    KeyValueScanner memstoreScanner =
-        new CollectionBackedScanner(snapshot, store.getComparator());
-    InternalScanner scanner = preCreateCoprocScanner(memstoreScanner);
-    if (scanner == null) {
-      scanner = createStoreScanner(smallestReadPoint, memstoreScanner);
-    }
-    scanner = postCreateCoprocScanner(scanner);
+    InternalScanner scanner = createScanner(snapshot, smallestReadPoint);
     if (scanner == null) {
       return result; // NULL scanner returned from coprocessor hooks means skip normal processing
     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
index c02cdcb..4ea3812 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
@@ -71,6 +71,8 @@ import org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
+import org.apache.hadoop.hbase.regionserver.compactions.Compactor;
+import org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor;
 import org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours;
 import org.apache.hadoop.hbase.regionserver.wal.HLogUtil;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -609,7 +611,7 @@ public class HStore implements Store {
     // Append the new storefile into the list
     this.lock.writeLock().lock();
     try {
-      this.storeEngine.getStoreFileManager().insertNewFile(sf);
+      this.storeEngine.getStoreFileManager().insertNewFiles(Lists.newArrayList(sf));
     } finally {
       // We need the lock, as long as we are updating the storeFiles
       // or changing the memstore. Let us release it before calling
@@ -621,6 +623,12 @@ public class HStore implements Store {
     notifyChangedReadersObservers();
     LOG.info("Successfully loaded store file " + srcPath
         + " into store " + this + " (new location: " + dstPath + ")");
+    if (LOG.isTraceEnabled()) {
+      String traceMessage = "BULK LOAD time,size,store size,store files ["
+          + EnvironmentEdgeManager.currentTimeMillis() + "," + r.length() + "," + storeSize
+          + "," + storeEngine.getStoreFileManager().getStorefileCount() + "]";
+      LOG.trace(traceMessage);
+    }
   }
 
   @Override
@@ -843,9 +851,7 @@ public class HStore implements Store {
       final List<StoreFile> sfs, final SortedSet<KeyValue> set) throws IOException {
     this.lock.writeLock().lock();
     try {
-      for (StoreFile sf : sfs) {
-        this.storeEngine.getStoreFileManager().insertNewFile(sf);
-      }
+      this.storeEngine.getStoreFileManager().insertNewFiles(sfs);
       this.memstore.clearSnapshot(set);
     } finally {
       // We need the lock, as long as we are updating the storeFiles
@@ -859,6 +865,16 @@ public class HStore implements Store {
     // Tell listeners of the change in readers.
     notifyChangedReadersObservers();
 
+    if (LOG.isTraceEnabled()) {
+      long totalSize = 0;
+      for (StoreFile sf : sfs) {
+        totalSize += sf.getReader().length();
+      }
+      String traceMessage = "FLUSH time,count,size,store size,store files ["
+          + EnvironmentEdgeManager.currentTimeMillis() + "," + sfs.size() + "," + totalSize
+          + "," + storeSize + "," + storeEngine.getStoreFileManager().getStorefileCount() + "]";
+      LOG.trace(traceMessage);
+    }
     return needsCompaction();
   }
 
@@ -1102,6 +1118,17 @@ public class HStore implements Store {
       .append(", and took ").append(StringUtils.formatTimeDiff(now, compactionStartTime))
       .append(" to execute.");
     LOG.info(message.toString());
+    if (LOG.isTraceEnabled()) {
+      int fileCount = storeEngine.getStoreFileManager().getStorefileCount();
+      long resultSize = 0;
+      for (StoreFile sf : sfs) {
+        resultSize += sf.getReader().length();
+      }
+      String traceMessage = "COMPACTION start,end,size out,files in,files out,store size,"
+        + "store files [" + compactionStartTime + "," + now + "," + resultSize + ","
+          + cr.getFiles().size() + "," + sfs.size() + "," +  storeSize + "," + fileCount + "]";
+      LOG.trace(traceMessage);
+    }
   }
 
   /**
@@ -1194,8 +1221,8 @@ public class HStore implements Store {
 
     try {
       // Ready to go. Have list of files to compact.
-      List<Path> newFiles =
-          this.storeEngine.getCompactor().compactForTesting(filesToCompact, isMajor);
+      List<Path> newFiles = ((DefaultCompactor)this.storeEngine.getCompactor())
+          .compactForTesting(filesToCompact, isMajor);
       for (Path newFile: newFiles) {
         // Move the compaction into place.
         StoreFile sf = moveFileIntoPlace(newFile);
@@ -1881,8 +1908,7 @@ public class HStore implements Store {
 
   @Override
   public boolean needsCompaction() {
-    return storeEngine.getCompactionPolicy().needsCompaction(
-        this.storeEngine.getStoreFileManager().getStorefiles(), filesCompacting);
+    return this.storeEngine.needsCompaction(this.filesCompacting);
   }
 
   @Override
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreConfigInformation.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreConfigInformation.java
index 62cef1b..768b6cf 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreConfigInformation.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreConfigInformation.java
@@ -31,7 +31,6 @@ import org.apache.hadoop.classification.InterfaceStability;
 @InterfaceStability.Unstable
 public interface StoreConfigInformation {
   /**
-   * TODO: remove after HBASE-7252 is fixed.
    * @return Gets the Memstore flush size for the region that this store works with.
    */
   long getMemstoreFlushSize();
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java
index 39140f5..4a7b1c8 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreEngine.java
@@ -20,6 +20,7 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
+import java.util.List;
 
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
@@ -80,6 +81,12 @@ public abstract class StoreEngine<SF extends StoreFlusher,
   }
 
   /**
+   * @param filesCompacting Files currently compacting
+   * @return whether a compaction selection is possible
+   */
+  public abstract boolean needsCompaction(List<StoreFile> filesCompacting);
+
+  /**
    * Creates an instance of a compaction context specific to this engine.
    * Doesn't actually select or start a compaction. See CompactionContext class comment.
    * @return New CompactionContext object.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
index 5979230..5372fa0 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
@@ -285,6 +285,10 @@ public class StoreFile {
     return modificationTimeStamp;
   }
 
+  public byte[] getMetadataValue(byte[] key) {
+    return metadataMap.get(key);
+  }
+
   /**
    * Return the largest memstoreTS found across all storefiles in
    * the given list. Store files that were created by a mapreduce
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
index 54ca48c..85be8ae 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileManager.java
@@ -47,10 +47,10 @@ public interface StoreFileManager {
   void loadFiles(List<StoreFile> storeFiles);
 
   /**
-   * Adds new file, either for from MemStore flush or bulk insert, into the structure.
+   * Adds new files, either for from MemStore flush or bulk insert, into the structure.
    * @param sf New store file.
    */
-  void insertNewFile(StoreFile sf);
+  void insertNewFiles(Collection<StoreFile> sfs) throws IOException;
 
   /**
    * Adds compaction results into the structure.
@@ -58,8 +58,7 @@ public interface StoreFileManager {
    * @param results The resulting files for the compaction.
    */
   void addCompactionResults(
-    Collection<StoreFile> compactedFiles, Collection<StoreFile> results
-  );
+      Collection<StoreFile> compactedFiles, Collection<StoreFile> results) throws IOException;
 
   /**
    * Clears all the files currently in use and returns them.
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
index ba515eb..149c0fe 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreFlusher.java
@@ -37,6 +37,7 @@ import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.regionserver.compactions.Compactor;
+import org.apache.hadoop.hbase.util.CollectionBackedScanner;
 
 /**
  * Store flusher interface. Turns a snapshot of memstore into a set of store files (usually one).
@@ -77,31 +78,27 @@ abstract class StoreFlusher {
     writer.close();
   }
 
-  /** Calls coprocessor to create a flush scanner based on memstore scanner */
-  protected InternalScanner preCreateCoprocScanner(
-      KeyValueScanner memstoreScanner) throws IOException {
-    if (store.getCoprocessorHost() != null) {
-      return store.getCoprocessorHost().preFlushScannerOpen(store, memstoreScanner);
-    }
-    return null;
-  }
-
-  /** Creates the default flush scanner based on memstore scanner */
-  protected InternalScanner createStoreScanner(long smallestReadPoint,
-      KeyValueScanner memstoreScanner) throws IOException {
-    Scan scan = new Scan();
-    scan.setMaxVersions(store.getScanInfo().getMaxVersions());
-    return new StoreScanner(store, store.getScanInfo(), scan,
-        Collections.singletonList(memstoreScanner), ScanType.COMPACT_RETAIN_DELETES,
-        smallestReadPoint, HConstants.OLDEST_TIMESTAMP);
-  }
 
   /**
-   * Calls coprocessor to create a scanner based on default flush scanner
-   * @return new or default scanner; if null, flush should not proceed.
+   * Creates the scanner for flushing snapshot. Also calls coprocessors.
+   * @return The scanner; null if coprocessor is canceling the flush.
    */
-  protected  InternalScanner postCreateCoprocScanner(InternalScanner scanner)
-      throws IOException {
+  protected InternalScanner createScanner(SortedSet<KeyValue> snapshot,
+      long smallestReadPoint) throws IOException {
+    KeyValueScanner memstoreScanner =
+        new CollectionBackedScanner(snapshot, store.getComparator());
+    InternalScanner scanner = null;
+    if (store.getCoprocessorHost() != null) {
+      scanner = store.getCoprocessorHost().preFlushScannerOpen(store, memstoreScanner);
+    }
+    if (scanner == null) {
+      Scan scan = new Scan();
+      scan.setMaxVersions(store.getScanInfo().getMaxVersions());
+      scanner = new StoreScanner(store, store.getScanInfo(), scan,
+          Collections.singletonList(memstoreScanner), ScanType.COMPACT_RETAIN_DELETES,
+          smallestReadPoint, HConstants.OLDEST_TIMESTAMP);
+    }
+    assert scanner != null;
     if (store.getCoprocessorHost() != null) {
       return store.getCoprocessorHost().preFlush(store, scanner);
     }
@@ -114,7 +111,7 @@ abstract class StoreFlusher {
    * @param sink Sink to write data to. Could be StoreFile.Writer.
    * @param smallestReadPoint Smallest read point used for the flush.
    * @return Bytes flushed.
-s   */
+   */
   protected long performFlush(InternalScanner scanner,
       Compactor.CellSink sink, long smallestReadPoint) throws IOException {
     int compactionKVMax =
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
index 54e8926..1c947a1 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
@@ -81,6 +81,7 @@ public class StoreScanner extends NonLazyKeyValueScanner
    * KVs skipped via seeking to next row/column. TODO: estimate them?
    */
   private long kvsScanned = 0;
+  private KeyValue prevKV = null;
 
   /** We don't ever expect to change this, the constant is just for clarity. */
   static final boolean LAZY_SEEK_ENABLED_BY_DEFAULT = true;
@@ -411,7 +412,6 @@ public class StoreScanner extends NonLazyKeyValueScanner
     }
 
     KeyValue kv;
-    KeyValue prevKV = null;
 
     // Only do a sanity-check if store and comparator are available.
     KeyValue.KVComparator comparator =
@@ -419,7 +419,7 @@ public class StoreScanner extends NonLazyKeyValueScanner
 
     int count = 0;
     LOOP: while((kv = this.heap.peek()) != null) {
-      ++kvsScanned;
+      if (prevKV != kv) ++kvsScanned; // Do object compare - we set prevKV from the same heap.
       // Check that the heap gives us KVs in an increasing order.
       assert prevKV == null || comparator == null || comparator.compare(prevKV, kv) <= 0 :
         "Key " + prevKV + " followed by a " + "smaller key " + kv + " in cf " + store;
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
new file mode 100644
index 0000000..588ccc0
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.java
@@ -0,0 +1,389 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.regionserver.compactions.Compactor;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Base class for cell sink that separates the provided cells into multiple files.
+ */
+public abstract class StripeMultiFileWriter implements Compactor.CellSink {
+  private static final Log LOG = LogFactory.getLog(StripeMultiFileWriter.class);
+
+  /** Factory that is used to produce single StoreFile.Writer-s */
+  protected WriterFactory writerFactory;
+  protected KVComparator comparator;
+
+  protected List<StoreFile.Writer> existingWriters;
+  protected List<byte[]> boundaries;
+  /** Source scanner that is tracking KV count; may be null if source is not StoreScanner */
+  protected StoreScanner sourceScanner;
+
+  /** Whether to write stripe metadata */
+  private boolean doWriteStripeMetadata = true;
+
+  public interface WriterFactory {
+    public StoreFile.Writer createWriter() throws IOException;
+  }
+
+  /**
+   * Initializes multi-writer before usage.
+   * @param sourceScanner Optional store scanner to obtain the information about read progress.
+   * @param factory Factory used to produce individual file writers.
+   * @param comparator Comparator used to compare rows.
+   */
+  public void init(StoreScanner sourceScanner, WriterFactory factory, KVComparator comparator)
+      throws IOException {
+    this.writerFactory = factory;
+    this.sourceScanner = sourceScanner;
+    this.comparator = comparator;
+  }
+
+  public void setNoStripeMetadata() {
+    this.doWriteStripeMetadata = false;
+  }
+
+  public List<Path> commitWriters(long maxSeqId, boolean isMajor) throws IOException {
+    assert this.existingWriters != null;
+    commitWritersInternal();
+    assert this.boundaries.size() == (this.existingWriters.size() + 1);
+    LOG.debug((this.doWriteStripeMetadata ? "W" : "Not w")
+      + "riting out metadata for " + this.existingWriters.size() + " writers");
+    List<Path> paths = new ArrayList<Path>();
+    for (int i = 0; i < this.existingWriters.size(); ++i) {
+      StoreFile.Writer writer = this.existingWriters.get(i);
+      if (writer == null) continue; // writer was skipped due to 0 KVs
+      if (doWriteStripeMetadata) {
+        writer.appendFileInfo(StripeStoreFileManager.STRIPE_START_KEY, this.boundaries.get(i));
+        writer.appendFileInfo(StripeStoreFileManager.STRIPE_END_KEY, this.boundaries.get(i + 1));
+      }
+      writer.appendMetadata(maxSeqId, isMajor);
+      paths.add(writer.getPath());
+      writer.close();
+    }
+    this.existingWriters = null;
+    return paths;
+  }
+
+  public List<Path> abortWriters() {
+    assert this.existingWriters != null;
+    List<Path> paths = new ArrayList<Path>();
+    for (StoreFile.Writer writer : this.existingWriters) {
+      try {
+        paths.add(writer.getPath());
+        writer.close();
+      } catch (Exception ex) {
+        LOG.error("Failed to close the writer after an unfinished compaction.", ex);
+      }
+    }
+    this.existingWriters = null;
+    return paths;
+  }
+
+  /**
+   * Subclasses can call this method to make sure the first KV is within multi-writer range.
+   * @param left The left boundary of the writer.
+   * @param row The row to check.
+   * @param rowOffset Offset for row.
+   * @param rowLength Length for row.
+   */
+  protected void sanityCheckLeft(
+      byte[] left, byte[] row, int rowOffset, int rowLength) throws IOException {
+    if (StripeStoreFileManager.OPEN_KEY != left &&
+        comparator.compareRows(row, rowOffset, rowLength, left, 0, left.length) < 0) {
+      String error = "The first row is lower than the left boundary of [" + Bytes.toString(left)
+        + "]: [" + Bytes.toString(row, rowOffset, rowLength) + "]";
+      LOG.error(error);
+      throw new IOException(error);
+    }
+  }
+
+  /**
+   * Subclasses can call this method to make sure the last KV is within multi-writer range.
+   * @param right The right boundary of the writer.
+   * @param row The row to check.
+   * @param rowOffset Offset for row.
+   * @param rowLength Length for row.
+   */
+  protected void sanityCheckRight(
+      byte[] right, byte[] row, int rowOffset, int rowLength) throws IOException {
+    if (StripeStoreFileManager.OPEN_KEY != right &&
+        comparator.compareRows(row, rowOffset, rowLength, right, 0, right.length) >= 0) {
+      String error = "The last row is higher or equal than the right boundary of ["
+          + Bytes.toString(right) + "]: [" + Bytes.toString(row, rowOffset, rowLength) + "]";
+      LOG.error(error);
+      throw new IOException(error);
+    }
+  }
+
+  /**
+   * Subclasses override this method to be called at the end of a successful sequence of
+   * append; all appends are processed before this method is called.
+   */
+  protected abstract void commitWritersInternal() throws IOException;
+
+  /**
+   * MultiWriter that separates the cells based on fixed row-key boundaries.
+   * All the KVs between each pair of neighboring boundaries from the list supplied to ctor
+   * will end up in one file, and separate from all other such pairs.
+   */
+  public static class BoundaryMultiWriter extends StripeMultiFileWriter {
+    private StoreFile.Writer currentWriter;
+    private byte[] currentWriterEndKey;
+
+    private KeyValue lastKv;
+    private long kvsInCurrentWriter = 0;
+    private int majorRangeFromIndex = -1, majorRangeToIndex = -1;
+    private boolean hasAnyWriter = false;
+
+    /**
+     * @param targetBoundaries The boundaries on which writers/files are separated.
+     * @param majorRangeFrom Major range is the range for which at least one file should be
+     *                       written (because all files are included in compaction).
+     *                       majorRangeFrom is the left boundary.
+     * @param majorRangeTo The right boundary of majorRange (see majorRangeFrom).
+     */
+    public BoundaryMultiWriter(List<byte[]> targetBoundaries,
+        byte[] majorRangeFrom, byte[] majorRangeTo) throws IOException {
+      super();
+      this.boundaries = targetBoundaries;
+      this.existingWriters = new ArrayList<StoreFile.Writer>(this.boundaries.size() - 1);
+      // "major" range (range for which all files are included) boundaries, if any,
+      // must match some target boundaries, let's find them.
+      assert  (majorRangeFrom == null) == (majorRangeTo == null);
+      if (majorRangeFrom != null) {
+        majorRangeFromIndex = (majorRangeFrom == StripeStoreFileManager.OPEN_KEY) ? 0
+          : Collections.binarySearch(this.boundaries, majorRangeFrom, Bytes.BYTES_COMPARATOR);
+        majorRangeToIndex = (majorRangeTo == StripeStoreFileManager.OPEN_KEY) ? boundaries.size()
+          : Collections.binarySearch(this.boundaries, majorRangeTo, Bytes.BYTES_COMPARATOR);
+        if (this.majorRangeFromIndex < 0 || this.majorRangeToIndex < 0) {
+          throw new IOException("Major range does not match writer boundaries: [" +
+              Bytes.toString(majorRangeFrom) + "] [" + Bytes.toString(majorRangeTo) + "]; from "
+              + majorRangeFromIndex + " to " + majorRangeToIndex);
+        }
+      }
+    }
+
+    @Override
+    public void append(KeyValue kv) throws IOException {
+      if (currentWriter == null && existingWriters.isEmpty()) {
+        // First append ever, do a sanity check.
+        sanityCheckLeft(this.boundaries.get(0),
+            kv.getRowArray(), kv.getRowOffset(), kv.getRowLength());
+      }
+      prepareWriterFor(kv);
+      currentWriter.append(kv);
+      lastKv = kv; // for the sanity check
+      ++kvsInCurrentWriter;
+    }
+
+    private boolean isKvAfterCurrentWriter(KeyValue kv) {
+      return ((currentWriterEndKey != StripeStoreFileManager.OPEN_KEY) &&
+            (comparator.compareRows(kv.getRowArray(), kv.getRowOffset(), kv.getRowLength(),
+                currentWriterEndKey, 0, currentWriterEndKey.length) >= 0));
+    }
+
+    @Override
+    protected void commitWritersInternal() throws IOException {
+      stopUsingCurrentWriter();
+      while (existingWriters.size() < boundaries.size() - 1) {
+        createEmptyWriter();
+      }
+      if (lastKv != null) {
+        sanityCheckRight(boundaries.get(boundaries.size() - 1),
+            lastKv.getRowArray(), lastKv.getRowOffset(), lastKv.getRowLength());
+      }
+    }
+
+    private void prepareWriterFor(KeyValue kv) throws IOException {
+      if (currentWriter != null && !isKvAfterCurrentWriter(kv)) return; // Use same writer.
+
+      stopUsingCurrentWriter();
+      // See if KV will be past the writer we are about to create; need to add another one.
+      while (isKvAfterCurrentWriter(kv)) {
+        checkCanCreateWriter();
+        createEmptyWriter();
+      }
+      checkCanCreateWriter();
+      hasAnyWriter = true;
+      currentWriter = writerFactory.createWriter();
+      existingWriters.add(currentWriter);
+    }
+
+    /**
+     * Called if there are no cells for some stripe.
+     * We need to have something in the writer list for this stripe, so that writer-boundary
+     * list indices correspond to each other. We can insert null in the writer list for that
+     * purpose, except in the following cases where we actually need a file:
+     * 1) If we are in range for which we are compacting all the files, we need to create an
+     * empty file to preserve stripe metadata.
+     * 2) If we have not produced any file at all for this compactions, and this is the
+     * last chance (the last stripe), we need to preserve last seqNum (see also HBASE-6059).
+     */
+    private void createEmptyWriter() throws IOException {
+      int index = existingWriters.size();
+      boolean isInMajorRange = (index >= majorRangeFromIndex) && (index < majorRangeToIndex);
+      // Stripe boundary count = stripe count + 1, so last stripe index is (#boundaries minus 2)
+      boolean isLastWriter = !hasAnyWriter && (index == (boundaries.size() - 2));
+      boolean needEmptyFile = isInMajorRange || isLastWriter;
+      existingWriters.add(needEmptyFile ? writerFactory.createWriter() : null);
+      hasAnyWriter |= needEmptyFile;
+      currentWriterEndKey = (existingWriters.size() + 1 == boundaries.size())
+          ? null : boundaries.get(existingWriters.size() + 1);
+    }
+
+    private void checkCanCreateWriter() throws IOException {
+      int maxWriterCount =  boundaries.size() - 1;
+      assert existingWriters.size() <= maxWriterCount;
+      if (existingWriters.size() >= maxWriterCount) {
+        throw new IOException("Cannot create any more writers (created " + existingWriters.size()
+            + " out of " + maxWriterCount + " - row might be out of range of all valid writers");
+      }
+    }
+
+    private void stopUsingCurrentWriter() {
+      if (currentWriter != null) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Stopping to use a writer after [" + Bytes.toString(currentWriterEndKey)
+              + "] row; wrote out " + kvsInCurrentWriter + " kvs");
+        }
+        kvsInCurrentWriter = 0;
+      }
+      currentWriter = null;
+      currentWriterEndKey = (existingWriters.size() + 1 == boundaries.size())
+          ? null : boundaries.get(existingWriters.size() + 1);
+    }
+  }
+
+  /**
+   * MultiWriter that separates the cells based on target cell number per file and file count.
+   * New file is started every time the target number of KVs is reached, unless the fixed
+   * count of writers has already been created (in that case all the remaining KVs go into
+   * the last writer).
+   */
+  public static class SizeMultiWriter extends StripeMultiFileWriter {
+    private int targetCount;
+    private long targetKvs;
+    private byte[] left;
+    private byte[] right;
+
+    private KeyValue lastKv;
+    private StoreFile.Writer currentWriter;
+    protected byte[] lastRowInCurrentWriter = null;
+    private long kvsInCurrentWriter = 0;
+    private long kvsSeen = 0;
+    private long kvsSeenInPrevious = 0;
+
+    /**
+     * @param targetCount The maximum count of writers that can be created.
+     * @param targetKvs The number of KVs to read from source before starting each new writer.
+     * @param left The left boundary of the first writer.
+     * @param right The right boundary of the last writer.
+     */
+    public SizeMultiWriter(int targetCount, long targetKvs, byte[] left, byte[] right) {
+      super();
+      this.targetCount = targetCount;
+      this.targetKvs = targetKvs;
+      this.left = left;
+      this.right = right;
+      int preallocate = Math.min(this.targetCount, 64);
+      this.existingWriters = new ArrayList<StoreFile.Writer>(preallocate);
+      this.boundaries = new ArrayList<byte[]>(preallocate + 1);
+    }
+
+    @Override
+    public void append(KeyValue kv) throws IOException {
+      // If we are waiting for opportunity to close and we started writing different row,
+      // discard the writer and stop waiting.
+      boolean doCreateWriter = false;
+      if (currentWriter == null) {
+        // First append ever, do a sanity check.
+        sanityCheckLeft(left, kv.getRowArray(), kv.getRowOffset(), kv.getRowLength());
+        doCreateWriter = true;
+      } else if (lastRowInCurrentWriter != null
+          && !comparator.matchingRows(kv.getRowArray(), kv.getRowOffset(), kv.getRowLength(),
+              lastRowInCurrentWriter, 0, lastRowInCurrentWriter.length)) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Stopping to use a writer after [" + Bytes.toString(lastRowInCurrentWriter)
+              + "] row; wrote out "  + kvsInCurrentWriter + " kvs");
+        }
+        lastRowInCurrentWriter = null;
+        kvsInCurrentWriter = 0;
+        kvsSeenInPrevious += kvsSeen;
+        doCreateWriter = true;
+      }
+      if (doCreateWriter) {
+        byte[] boundary = existingWriters.isEmpty() ? left : kv.getRow(); // make a copy
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Creating new writer starting at [" + Bytes.toString(boundary) + "]");
+        }
+        currentWriter = writerFactory.createWriter();
+        boundaries.add(boundary);
+        existingWriters.add(currentWriter);
+      }
+
+      currentWriter.append(kv);
+      lastKv = kv; // for the sanity check
+      ++kvsInCurrentWriter;
+      kvsSeen = kvsInCurrentWriter;
+      if (this.sourceScanner != null) {
+        kvsSeen = Math.max(kvsSeen,
+            this.sourceScanner.getEstimatedNumberOfKvsScanned() - kvsSeenInPrevious);
+      }
+
+      // If we are not already waiting for opportunity to close, start waiting if we can
+      // create any more writers and if the current one is too big.
+      if (lastRowInCurrentWriter == null
+          && existingWriters.size() < targetCount
+          && kvsSeen >= targetKvs) {
+        lastRowInCurrentWriter = kv.getRow(); // make a copy
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Preparing to start a new writer after [" + Bytes.toString(
+              lastRowInCurrentWriter) + "] row; observed " + kvsSeen + " kvs and wrote out "
+              + kvsInCurrentWriter + " kvs");
+        }
+      }
+    }
+
+    @Override
+    protected void commitWritersInternal() throws IOException {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Stopping with "  + kvsInCurrentWriter + " kvs in last writer" +
+            ((this.sourceScanner == null) ? "" : ("; observed estimated "
+                + this.sourceScanner.getEstimatedNumberOfKvsScanned() + " KVs total")));
+      }
+      if (lastKv != null) {
+        sanityCheckRight(
+            right, lastKv.getRowArray(), lastKv.getRowOffset(), lastKv.getRowLength());
+      }
+      this.boundaries.add(right);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java
new file mode 100644
index 0000000..e74a065
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreConfig.java
@@ -0,0 +1,162 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+
+/**
+ * Configuration class for stripe store and compactions.
+ * See {@link StripeStoreFileManager} for general documentation.
+ * See getters for the description of each setting.
+ */
+@InterfaceAudience.Private
+public class StripeStoreConfig {
+  static final Log LOG = LogFactory.getLog(StripeStoreConfig.class);
+
+  /** The maximum number of files to compact within a stripe; same as for regular compaction. */
+  public static final String MAX_FILES_KEY = "hbase.store.stripe.compaction.maxFiles";
+  /** The minimum number of files to compact within a stripe; same as for regular compaction. */
+  public static final String MIN_FILES_KEY = "hbase.store.stripe.compaction.minFiles";
+
+  /**  The minimum number of files to compact when compacting L0; same as minFiles for regular
+   * compaction. Given that L0 causes unnecessary overwriting of the data, should be higher than
+   * regular minFiles. */
+  public static final String MIN_FILES_L0_KEY = "hbase.store.stripe.compaction.minFilesL0";
+
+  /** The size the stripe should achieve to be considered for splitting into multiple stripes.
+   Stripe will be split when it can be fully compacted, and it is above this size. */
+  public static final String SIZE_TO_SPLIT_KEY = "hbase.store.stripe.sizeToSplit";
+  /** The target count of new stripes to produce when splitting a stripe. A floating point
+   number, default is 2. Values less than 1 will be converted to 1/x. Non-whole numbers will
+   produce unbalanced splits, which may be good for some cases. In this case the "smaller" of
+   the new stripes will always be the rightmost one. If the stripe is bigger than sizeToSplit
+   when splitting, this will be adjusted by a whole increment. */
+  public static final String SPLIT_PARTS_KEY = "hbase.store.stripe.splitPartCount";
+  /** The initial stripe count to create. If the row distribution is roughly the same over time,
+   it's good to set this to a count of stripes that is expected to be achieved in most regions,
+   to get this count from the outset and prevent unnecessary splitting. */
+  public static final String INITIAL_STRIPE_COUNT_KEY = "hbase.store.stripe.initialStripeCount";
+
+  /** Whether to flush memstore to L0 files, or directly to stripes. */
+  public static final String FLUSH_TO_L0_KEY = "hbase.store.stripe.compaction.flushToL0";
+
+  /** When splitting region, the maximum size imbalance to allow in an attempt to split at a
+   stripe boundary, so that no files go to both regions. Most users won't need to change that. */
+  public static final String MAX_REGION_SPLIT_IMBALANCE_KEY =
+      "hbase.store.stripe.region.split.max.imbalance";
+
+
+  private final float maxRegionSplitImbalance;
+  private final int level0CompactMinFiles;
+  private final int stripeCompactMinFiles;
+  private final int stripeCompactMaxFiles;
+
+  private final int initialCount;
+  private final long sizeToSplitAt;
+  private final float splitPartCount;
+  private final boolean flushIntoL0;
+  private final long splitPartSize; // derived from sizeToSplitAt and splitPartCount
+
+  private static final double EPSILON = 0.001; // good enough for this, not a real epsilon.
+  public StripeStoreConfig(Configuration config, StoreConfigInformation sci) {
+    this.level0CompactMinFiles = config.getInt(MIN_FILES_L0_KEY, 4);
+    this.stripeCompactMinFiles = config.getInt(MIN_FILES_KEY, 3);
+    this.stripeCompactMaxFiles = config.getInt(MAX_FILES_KEY, 10);
+    this.maxRegionSplitImbalance = getFloat(config, MAX_REGION_SPLIT_IMBALANCE_KEY, 1.5f, true);
+    this.flushIntoL0 = config.getBoolean(FLUSH_TO_L0_KEY, false);
+
+    float splitPartCount = getFloat(config, SPLIT_PARTS_KEY, 2f, true);
+    if (Math.abs(splitPartCount - 1.0) < EPSILON) {
+      LOG.error("Split part count cannot be 1 (" + this.splitPartCount + "), using the default");
+      splitPartCount = 2f;
+    }
+    this.splitPartCount = splitPartCount;
+    // Arbitrary default split size - 4 times the size of one L0 compaction.
+    // If we flush into L0 there's no split compaction, but for default value it is ok.
+    double flushSize = sci.getMemstoreFlushSize();
+    if (flushSize == 0) {
+      flushSize = 128 * 1024 * 1024;
+    }
+    long defaultSplitSize = (long)(flushSize * getLevel0MinFiles() * 4 * splitPartCount);
+    this.sizeToSplitAt = config.getLong(SIZE_TO_SPLIT_KEY, defaultSplitSize);
+    int initialCount = config.getInt(INITIAL_STRIPE_COUNT_KEY, 1);
+    if (initialCount == 0) {
+      LOG.error("Initial stripe count is 0, using the default");
+      initialCount = 1;
+    }
+    this.initialCount = initialCount;
+    this.splitPartSize = (long)(this.sizeToSplitAt / this.splitPartCount);
+  }
+
+  private static float getFloat(
+      Configuration config, String key, float defaultValue, boolean moreThanOne) {
+    float value = config.getFloat(key, defaultValue);
+    if (value < EPSILON) {
+      LOG.warn(String.format(
+          "%s is set to 0 or negative; using default value of %f", key, defaultValue));
+      value = defaultValue;
+    } else if ((value > 1f) != moreThanOne) {
+      value = 1f / value;
+    }
+    return value;
+  }
+
+  public float getMaxSplitImbalance() {
+    return this.maxRegionSplitImbalance;
+  }
+
+  public int getLevel0MinFiles() {
+    return level0CompactMinFiles;
+  }
+
+  public int getStripeCompactMinFiles() {
+    return stripeCompactMinFiles;
+  }
+
+  public int getStripeCompactMaxFiles() {
+    return stripeCompactMaxFiles;
+  }
+
+  public boolean isUsingL0Flush() {
+    return flushIntoL0;
+  }
+
+  public long getSplitSize() {
+    return sizeToSplitAt;
+  }
+
+  public int getInitialCount() {
+    return initialCount;
+  }
+
+  public float getSplitCount() {
+    return splitPartCount;
+  }
+
+  /**
+   * @return the desired size of the target stripe when splitting, in bytes.
+   *         Derived from {@link #getSplitSize()} and {@link #getSplitCount()}.
+   */
+  public long getSplitPartSize() {
+    return splitPartSize;
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
new file mode 100644
index 0000000..fe54a93
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreEngine.java
@@ -0,0 +1,105 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor;
+
+import com.google.common.base.Preconditions;
+
+/**
+ * The storage engine that implements the stripe-based store/compaction scheme.
+ */
+@InterfaceAudience.Private
+public class StripeStoreEngine extends StoreEngine<StripeStoreFlusher,
+  StripeCompactionPolicy, StripeCompactor, StripeStoreFileManager> {
+  static final Log LOG = LogFactory.getLog(StripeStoreEngine.class);
+  private StripeStoreConfig config;
+
+  @Override
+  public boolean needsCompaction(List<StoreFile> filesCompacting) {
+    return this.compactionPolicy.needsCompactions(this.storeFileManager, filesCompacting);
+  }
+
+  @Override
+  public CompactionContext createCompaction() {
+    return new StripeCompaction();
+  }
+
+  @Override
+  protected void createComponents(
+      Configuration conf, Store store, KVComparator comparator) throws IOException {
+    this.config = new StripeStoreConfig(conf, store);
+    this.compactionPolicy = new StripeCompactionPolicy(conf, store, config);
+    this.storeFileManager = new StripeStoreFileManager(comparator, conf, this.config);
+    this.storeFlusher = new StripeStoreFlusher(
+      conf, store, this.compactionPolicy, this.storeFileManager);
+    this.compactor = new StripeCompactor(conf, store);
+  }
+
+  /**
+   * Represents one instance of stripe compaction, with the necessary context and flow.
+   */
+  private class StripeCompaction extends CompactionContext {
+    private StripeCompactionPolicy.StripeCompactionRequest stripeRequest = null;
+
+    @Override
+    public List<StoreFile> preSelect(List<StoreFile> filesCompacting) {
+      return compactionPolicy.preSelectFilesForCoprocessor(storeFileManager, filesCompacting);
+    }
+
+    @Override
+    public boolean select(List<StoreFile> filesCompacting, boolean isUserCompaction,
+        boolean mayUseOffPeak, boolean forceMajor) throws IOException {
+      this.stripeRequest = compactionPolicy.selectCompaction(
+          storeFileManager, filesCompacting, mayUseOffPeak);
+      this.request = (this.stripeRequest == null)
+          ? new CompactionRequest(new ArrayList<StoreFile>()) : this.stripeRequest.getRequest();
+      return this.stripeRequest != null;
+    }
+
+    @Override
+    public void forceSelect(CompactionRequest request) {
+      super.forceSelect(request);
+      if (this.stripeRequest != null) {
+        this.stripeRequest.setRequest(this.request);
+      } else {
+        LOG.warn("Stripe store is forced to take an arbitrary file list and compact it.");
+        this.stripeRequest = compactionPolicy.createEmptyRequest(storeFileManager, this.request);
+      }
+    }
+
+    @Override
+    public List<Path> compact() throws IOException {
+      Preconditions.checkArgument(this.stripeRequest != null, "Cannot compact without selection");
+      return this.stripeRequest.execute(compactor);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
new file mode 100644
index 0000000..a2ac96e
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.java
@@ -0,0 +1,922 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.ConcatenatedLists;
+
+import com.google.common.collect.ImmutableCollection;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Lists;
+
+
+/**
+ * Stripe implementation of StoreFileManager.
+ * Not thread safe - relies on external locking (in HStore). Collections that this class
+ * returns are immutable or unique to the call, so they should be safe.
+ * Stripe store splits the key space of the region into non-overlapping stripes, as well as
+ * some recent files that have all the keys (level 0). Each stripe contains a set of files.
+ * When L0 is compacted, it's split into the files corresponding to existing stripe boundaries,
+ * that can thus be added to stripes.
+ * When scan or get happens, it only has to read the files from the corresponding stripes.
+ * See StripeCompationPolicy on how the stripes are determined; this class doesn't care.
+ *
+ * This class should work together with StripeCompactionPolicy and StripeCompactor.
+ * With regard to how they work, we make at least the following (reasonable) assumptions:
+ *  - Compaction produces one file per new stripe (if any); that is easy to change.
+ *  - Compaction has one contiguous set of stripes both in and out, except if L0 is involved.
+ */
+@InterfaceAudience.Private
+public class StripeStoreFileManager
+  implements StoreFileManager, StripeCompactionPolicy.StripeInformationProvider {
+  static final Log LOG = LogFactory.getLog(StripeStoreFileManager.class);
+
+  /**
+   * The file metadata fields that contain the stripe information.
+   */
+  public static final byte[] STRIPE_START_KEY = Bytes.toBytes("STRIPE_START_KEY");
+  public static final byte[] STRIPE_END_KEY = Bytes.toBytes("STRIPE_END_KEY");
+
+  private final static Bytes.RowEndKeyComparator MAP_COMPARATOR = new Bytes.RowEndKeyComparator();
+
+  /**
+   * The key value used for range boundary, indicating that the boundary is open (i.e. +-inf).
+   */
+  public final static byte[] OPEN_KEY = HConstants.EMPTY_BYTE_ARRAY;
+  final static byte[] INVALID_KEY = null;
+
+  /**
+   * The state class. Used solely to replace results atomically during
+   * compactions and avoid complicated error handling.
+   */
+  private static class State {
+    /**
+     * The end rows of each stripe. The last stripe end is always open-ended, so it's not stored
+     * here. It is invariant that the start row of the stripe is the end row of the previous one
+     * (and is an open boundary for the first one).
+     */
+    public byte[][] stripeEndRows = new byte[0][];
+
+    /**
+     * Files by stripe. Each element of the list corresponds to stripeEndRow element with the
+     * same index, except the last one. Inside each list, the files are in reverse order by
+     * seqNum. Note that the length of this is one higher than that of stripeEndKeys.
+     */
+    public ArrayList<ImmutableList<StoreFile>> stripeFiles
+      = new ArrayList<ImmutableList<StoreFile>>();
+    /** Level 0. The files are in reverse order by seqNum. */
+    public ImmutableList<StoreFile> level0Files = ImmutableList.<StoreFile>of();
+
+    /** Cached list of all files in the structure, to return from some calls */
+    public ImmutableList<StoreFile> allFilesCached = ImmutableList.<StoreFile>of();
+  }
+  private State state = null;
+
+  /** Cached file metadata (or overrides as the case may be) */
+  private HashMap<StoreFile, byte[]> fileStarts = new HashMap<StoreFile, byte[]>();
+  private HashMap<StoreFile, byte[]> fileEnds = new HashMap<StoreFile, byte[]>();
+  /** Normally invalid key is null, but in the map null is the result for "no key"; so use
+   * the following constant value in these maps instead. Note that this is a constant and
+   * we use it to compare by reference when we read from the map. */
+  private static final byte[] INVALID_KEY_IN_MAP = new byte[0];
+
+  private final KVComparator kvComparator;
+  private StripeStoreConfig config;
+
+  private final int blockingFileCount;
+
+  public StripeStoreFileManager(
+      KVComparator kvComparator, Configuration conf, StripeStoreConfig config) {
+    this.kvComparator = kvComparator;
+    this.config = config;
+    this.blockingFileCount = conf.getInt(
+        HStore.BLOCKING_STOREFILES_KEY, HStore.DEFAULT_BLOCKING_STOREFILE_COUNT);
+  }
+
+  @Override
+  public void loadFiles(List<StoreFile> storeFiles) {
+    loadUnclassifiedStoreFiles(storeFiles);
+  }
+
+  @Override
+  public Collection<StoreFile> getStorefiles() {
+    return state.allFilesCached;
+  }
+
+  @Override
+  public void insertNewFiles(Collection<StoreFile> sfs) throws IOException {
+    CompactionOrFlushMergeCopy cmc = new CompactionOrFlushMergeCopy(true);
+    cmc.mergeResults(null, sfs);
+    debugDumpState("Added new files");
+  }
+
+  @Override
+  public ImmutableCollection<StoreFile> clearFiles() {
+    ImmutableCollection<StoreFile> result = state.allFilesCached;
+    this.state = new State();
+    this.fileStarts.clear();
+    this.fileEnds.clear();
+    return result;
+  }
+
+  @Override
+  public int getStorefileCount() {
+    return state.allFilesCached.size();
+  }
+
+  /** See {@link StoreFileManager#getCandidateFilesForRowKeyBefore(KeyValue)}
+   * for details on this methods. */
+  @Override
+  public Iterator<StoreFile> getCandidateFilesForRowKeyBefore(final KeyValue targetKey) {
+    KeyBeforeConcatenatedLists result = new KeyBeforeConcatenatedLists();
+    // Order matters for this call.
+    result.addSublist(state.level0Files);
+    if (!state.stripeFiles.isEmpty()) {
+      int lastStripeIndex = findStripeForRow(targetKey.getRow(), false);
+      for (int stripeIndex = lastStripeIndex; stripeIndex >= 0; --stripeIndex) {
+        result.addSublist(state.stripeFiles.get(stripeIndex));
+      }
+    }
+    return result.iterator();
+  }
+
+  /** See {@link StoreFileManager#getCandidateFilesForRowKeyBefore(KeyValue)} and
+   * {@link StoreFileManager#updateCandidateFilesForRowKeyBefore(Iterator, KeyValue, KeyValue)}
+   * for details on this methods. */
+  @Override
+  public Iterator<StoreFile> updateCandidateFilesForRowKeyBefore(
+      Iterator<StoreFile> candidateFiles, final KeyValue targetKey, final KeyValue candidate) {
+    KeyBeforeConcatenatedLists.Iterator original =
+        (KeyBeforeConcatenatedLists.Iterator)candidateFiles;
+    assert original != null;
+    ArrayList<List<StoreFile>> components = original.getComponents();
+    for (int firstIrrelevant = 0; firstIrrelevant < components.size(); ++firstIrrelevant) {
+      StoreFile sf = components.get(firstIrrelevant).get(0);
+      byte[] endKey = endOf(sf);
+      // Entries are ordered as such: L0, then stripes in reverse order. We never remove
+      // level 0; we remove the stripe, and all subsequent ones, as soon as we find the
+      // first one that cannot possibly have better candidates.
+      if (!isInvalid(endKey) && !isOpen(endKey)
+          && (nonOpenRowCompare(endKey, targetKey.getRow()) <= 0)) {
+        original.removeComponents(firstIrrelevant);
+        break;
+      }
+    }
+    return original;
+  }
+
+  @Override
+  /**
+   * Override of getSplitPoint that determines the split point as the boundary between two
+   * stripes, unless it causes significant imbalance between split sides' sizes. In that
+   * case, the split boundary will be chosen from the middle of one of the stripes to
+   * minimize imbalance.
+   * @return The split point, or null if no split is possible.
+   */
+  public byte[] getSplitPoint() throws IOException {
+    if (this.getStorefileCount() == 0) return null;
+    if (state.stripeFiles.size() <= 1) {
+      return getSplitPointFromAllFiles();
+    }
+    int leftIndex = -1, rightIndex = state.stripeFiles.size();
+    long leftSize = 0, rightSize = 0;
+    long lastLeftSize = 0, lastRightSize = 0;
+    while (rightIndex - 1 != leftIndex) {
+      if (leftSize >= rightSize) {
+        --rightIndex;
+        lastRightSize = getStripeFilesSize(rightIndex);
+        rightSize += lastRightSize;
+      } else {
+        ++leftIndex;
+        lastLeftSize = getStripeFilesSize(leftIndex);
+        leftSize += lastLeftSize;
+      }
+    }
+    if (leftSize == 0 || rightSize == 0) {
+      String errMsg = String.format("Cannot split on a boundary - left index %d size %d, "
+          + "right index %d size %d", leftIndex, leftSize, rightIndex, rightSize);
+      debugDumpState(errMsg);
+      LOG.warn(errMsg);
+      return getSplitPointFromAllFiles();
+    }
+    double ratio = (double)rightSize / leftSize;
+    if (ratio < 1) {
+      ratio = 1 / ratio;
+    }
+    if (config.getMaxSplitImbalance() > ratio) return state.stripeEndRows[leftIndex];
+
+    // If the difference between the sides is too large, we could get the proportional key on
+    // the a stripe to equalize the difference, but there's no proportional key method at the
+    // moment, and it's not extremely important.
+    // See if we can achieve better ratio if we split the bigger side in half.
+    boolean isRightLarger = rightSize >= leftSize;
+    double newRatio = isRightLarger
+        ? getMidStripeSplitRatio(leftSize, rightSize, lastRightSize)
+        : getMidStripeSplitRatio(rightSize, leftSize, lastLeftSize);
+    if (newRatio < 1) {
+      newRatio = 1 / newRatio;
+    }
+    if (newRatio >= ratio)  return state.stripeEndRows[leftIndex];
+    LOG.debug("Splitting the stripe - ratio w/o split " + ratio + ", ratio with split "
+        + newRatio + " configured ratio " + config.getMaxSplitImbalance());
+    // Ok, we may get better ratio, get it.
+    return StoreUtils.getLargestFile(state.stripeFiles.get(
+        isRightLarger ? rightIndex : leftIndex)).getFileSplitPoint(this.kvComparator);
+  }
+
+  private byte[] getSplitPointFromAllFiles() throws IOException {
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    sfs.addSublist(state.level0Files);
+    sfs.addAllSublists(state.stripeFiles);
+    if (sfs.isEmpty()) return null;
+    return StoreUtils.getLargestFile(sfs).getFileSplitPoint(this.kvComparator);
+  }
+
+  private double getMidStripeSplitRatio(long smallerSize, long largerSize, long lastLargerSize) {
+    return (double)(largerSize - lastLargerSize / 2f) / (smallerSize + lastLargerSize / 2f);
+  }
+
+  @Override
+  public Collection<StoreFile> getFilesForScanOrGet(
+      boolean isGet, byte[] startRow, byte[] stopRow) {
+    if (state.stripeFiles.isEmpty()) {
+      return state.level0Files; // There's just L0.
+    }
+
+    int firstStripe = findStripeForRow(startRow, true);
+    int lastStripe = findStripeForRow(stopRow, false);
+    assert firstStripe <= lastStripe;
+    if (firstStripe == lastStripe && state.level0Files.isEmpty()) {
+      return state.stripeFiles.get(firstStripe); // There's just one stripe we need.
+    }
+    if (firstStripe == 0 && lastStripe == (state.stripeFiles.size() - 1)) {
+      return state.allFilesCached; // We need to read all files.
+    }
+
+    ConcatenatedLists<StoreFile> result = new ConcatenatedLists<StoreFile>();
+    result.addAllSublists(state.stripeFiles.subList(firstStripe, lastStripe + 1));
+    result.addSublist(state.level0Files);
+    return result;
+  }
+
+  @Override
+  public void addCompactionResults(
+    Collection<StoreFile> compactedFiles, Collection<StoreFile> results) throws IOException {
+    // See class comment for the assumptions we make here.
+    LOG.debug("Attempting to merge compaction results: " + compactedFiles.size()
+        + " files replaced by " + results.size());
+    // In order to be able to fail in the middle of the operation, we'll operate on lazy
+    // copies and apply the result at the end.
+    CompactionOrFlushMergeCopy cmc = new CompactionOrFlushMergeCopy(false);
+    cmc.mergeResults(compactedFiles, results);
+    debugDumpState("Merged compaction results");
+  }
+
+  @Override
+  public int getStoreCompactionPriority() {
+    // If there's only L0, do what the default store does.
+    // If we are in critical priority, do the same - we don't want to trump all stores all
+    // the time due to how many files we have.
+    int fc = getStorefileCount();
+    if (state.stripeFiles.isEmpty() || (this.blockingFileCount <= fc)) {
+      return this.blockingFileCount - fc;
+    }
+    // If we are in good shape, we don't want to be trumped by all other stores due to how
+    // many files we have, so do an approximate mapping to normal priority range; L0 counts
+    // for all stripes.
+    int l0 = state.level0Files.size(), sc = state.stripeFiles.size();
+    int priority = (int)Math.ceil(((double)(this.blockingFileCount - fc + l0) / sc) - l0);
+    return (priority <= HStore.PRIORITY_USER) ? (HStore.PRIORITY_USER + 1) : priority;
+  }
+
+  /**
+   * Gets the total size of all files in the stripe.
+   * @param stripeIndex Stripe index.
+   * @return Size.
+   */
+  private long getStripeFilesSize(int stripeIndex) {
+    long result = 0;
+    for (StoreFile sf : state.stripeFiles.get(stripeIndex)) {
+      result += sf.getReader().length();
+    }
+    return result;
+  }
+
+  /**
+   * Loads initial store files that were picked up from some physical location pertaining to
+   * this store (presumably). Unlike adding files after compaction, assumes empty initial
+   * sets, and is forgiving with regard to stripe constraints - at worst, many/all files will
+   * go to level 0.
+   * @param storeFiles Store files to add.
+   */
+  private void loadUnclassifiedStoreFiles(List<StoreFile> storeFiles) {
+    LOG.debug("Attempting to load " + storeFiles.size() + " store files.");
+    TreeMap<byte[], ArrayList<StoreFile>> candidateStripes =
+        new TreeMap<byte[], ArrayList<StoreFile>>(MAP_COMPARATOR);
+    ArrayList<StoreFile> level0Files = new ArrayList<StoreFile>();
+    // Separate the files into tentative stripes; then validate. Currently, we rely on metadata.
+    // If needed, we could dynamically determine the stripes in future.
+    for (StoreFile sf : storeFiles) {
+      byte[] startRow = startOf(sf), endRow = endOf(sf);
+      // Validate the range and put the files into place.
+      if (isInvalid(startRow) || isInvalid(endRow)) {
+        insertFileIntoStripe(level0Files, sf); // No metadata - goes to L0.
+        ensureLevel0Metadata(sf);
+      } else if (!isOpen(startRow) && !isOpen(endRow) &&
+          nonOpenRowCompare(startRow, endRow) >= 0) {
+        LOG.error("Unexpected metadata - start row [" + Bytes.toString(startRow) + "], end row ["
+          + Bytes.toString(endRow) + "] in file [" + sf.getPath() + "], pushing to L0");
+        insertFileIntoStripe(level0Files, sf); // Bad metadata - goes to L0 also.
+        ensureLevel0Metadata(sf);
+      } else {
+        ArrayList<StoreFile> stripe = candidateStripes.get(endRow);
+        if (stripe == null) {
+          stripe = new ArrayList<StoreFile>();
+          candidateStripes.put(endRow, stripe);
+        }
+        insertFileIntoStripe(stripe, sf);
+      }
+    }
+    // Possible improvement - for variable-count stripes, if all the files are in L0, we can
+    // instead create single, open-ended stripe with all files.
+
+    boolean hasOverlaps = false;
+    byte[] expectedStartRow = null; // first stripe can start wherever
+    Iterator<Map.Entry<byte[], ArrayList<StoreFile>>> entryIter =
+        candidateStripes.entrySet().iterator();
+    while (entryIter.hasNext()) {
+      Map.Entry<byte[], ArrayList<StoreFile>> entry = entryIter.next();
+      ArrayList<StoreFile> files = entry.getValue();
+      // Validate the file start rows, and remove the bad ones to level 0.
+      for (int i = 0; i < files.size(); ++i) {
+        StoreFile sf = files.get(i);
+        byte[] startRow = startOf(sf);
+        if (expectedStartRow == null) {
+          expectedStartRow = startRow; // ensure that first stripe is still consistent
+        } else if (!rowEquals(expectedStartRow, startRow)) {
+          hasOverlaps = true;
+          LOG.warn("Store file doesn't fit into the tentative stripes - expected to start at ["
+              + Bytes.toString(expectedStartRow) + "], but starts at [" + Bytes.toString(startRow)
+              + "], to L0 it goes");
+          StoreFile badSf = files.remove(i);
+          insertFileIntoStripe(level0Files, badSf);
+          ensureLevel0Metadata(badSf);
+          --i;
+        }
+      }
+      // Check if any files from the candidate stripe are valid. If so, add a stripe.
+      byte[] endRow = entry.getKey();
+      if (!files.isEmpty()) {
+        expectedStartRow = endRow; // Next stripe must start exactly at that key.
+      } else {
+        entryIter.remove();
+      }
+    }
+
+    // In the end, there must be open ends on two sides. If not, and there were no errors i.e.
+    // files are consistent, they might be coming from a split. We will treat the boundaries
+    // as open keys anyway, and log the message.
+    // If there were errors, we'll play it safe and dump everything into L0.
+    if (!candidateStripes.isEmpty()) {
+      StoreFile firstFile = candidateStripes.firstEntry().getValue().get(0);
+      boolean isOpen = isOpen(startOf(firstFile)) && isOpen(candidateStripes.lastKey());
+      if (!isOpen) {
+        LOG.warn("The range of the loaded files does not cover full key space: from ["
+            + Bytes.toString(startOf(firstFile)) + "], to ["
+            + Bytes.toString(candidateStripes.lastKey()) + "]");
+        if (!hasOverlaps) {
+          ensureEdgeStripeMetadata(candidateStripes.firstEntry().getValue(), true);
+          ensureEdgeStripeMetadata(candidateStripes.lastEntry().getValue(), false);
+        } else {
+          LOG.warn("Inconsistent files, everything goes to L0.");
+          for (ArrayList<StoreFile> files : candidateStripes.values()) {
+            for (StoreFile sf : files) {
+              insertFileIntoStripe(level0Files, sf);
+              ensureLevel0Metadata(sf);
+            }
+          }
+          candidateStripes.clear();
+        }
+      }
+    }
+
+    // Copy the results into the fields.
+    State state = new State();
+    state.level0Files = ImmutableList.copyOf(level0Files);
+    state.stripeFiles = new ArrayList<ImmutableList<StoreFile>>(candidateStripes.size());
+    state.stripeEndRows = new byte[Math.max(0, candidateStripes.size() - 1)][];
+    ArrayList<StoreFile> newAllFiles = new ArrayList<StoreFile>(level0Files);
+    int i = candidateStripes.size() - 1;
+    for (Map.Entry<byte[], ArrayList<StoreFile>> entry : candidateStripes.entrySet()) {
+      state.stripeFiles.add(ImmutableList.copyOf(entry.getValue()));
+      newAllFiles.addAll(entry.getValue());
+      if (i > 0) {
+        state.stripeEndRows[state.stripeFiles.size() - 1] = entry.getKey();
+      }
+      --i;
+    }
+    state.allFilesCached = ImmutableList.copyOf(newAllFiles);
+    this.state = state;
+    debugDumpState("Files loaded");
+  }
+
+  private void ensureEdgeStripeMetadata(ArrayList<StoreFile> stripe, boolean isFirst) {
+    HashMap<StoreFile, byte[]> targetMap = isFirst ? fileStarts : fileEnds;
+    for (StoreFile sf : stripe) {
+      targetMap.put(sf, OPEN_KEY);
+    }
+  }
+
+  private void ensureLevel0Metadata(StoreFile sf) {
+    if (!isInvalid(startOf(sf))) this.fileStarts.put(sf, INVALID_KEY_IN_MAP);
+    if (!isInvalid(endOf(sf))) this.fileEnds.put(sf, INVALID_KEY_IN_MAP);
+  }
+
+  private void debugDumpState(String string) {
+    if (!LOG.isDebugEnabled()) return;
+    StringBuilder sb = new StringBuilder();
+    sb.append("\n" + string + "; current stripe state is as such:");
+    sb.append("\n level 0 with ").append(state.level0Files.size()).append(" files;");
+    for (int i = 0; i < state.stripeFiles.size(); ++i) {
+      String endRow = (i == state.stripeEndRows.length)
+          ? "(end)" : "[" + Bytes.toString(state.stripeEndRows[i]) + "]";
+      sb.append("\n stripe ending in ").append(endRow).append(" with ")
+        .append(state.stripeFiles.get(i).size()).append(" files;");
+    }
+    sb.append("\n").append(getStorefileCount()).append(" files total.");
+    LOG.debug(sb.toString());
+  }
+
+  /**
+   * Checks whether the key indicates an open interval boundary (i.e. infinity).
+   */
+  private static final boolean isOpen(byte[] key) {
+    return key != null && key.length == 0;
+  }
+
+  /**
+   * Checks whether the key is invalid (e.g. from an L0 file, or non-stripe-compacted files).
+   */
+  private static final boolean isInvalid(byte[] key) {
+    return key == INVALID_KEY;
+  }
+
+  /**
+   * Compare two keys for equality.
+   */
+  private final boolean rowEquals(byte[] k1, byte[] k2) {
+    return kvComparator.matchingRows(k1, 0, k1.length, k2, 0, k2.length);
+  }
+
+  /**
+   * Compare two keys. Keys must not be open (isOpen(row) == false).
+   */
+  private final int nonOpenRowCompare(byte[] k1, byte[] k2) {
+    assert !isOpen(k1) && !isOpen(k2);
+    return kvComparator.compareRows(k1, 0, k1.length, k2, 0, k2.length);
+  }
+
+  /**
+   * Finds the stripe index by end row.
+   */
+  private final int findStripeIndexByEndRow(byte[] endRow) {
+    assert !isInvalid(endRow);
+    if (isOpen(endRow)) return state.stripeEndRows.length;
+    return Arrays.binarySearch(state.stripeEndRows, endRow, Bytes.BYTES_COMPARATOR);
+  }
+
+  /**
+   * Finds the stripe index for the stripe containing a row provided externally for get/scan.
+   */
+  private final int findStripeForRow(byte[] row, boolean isStart) {
+    if (isStart && row == HConstants.EMPTY_START_ROW) return 0;
+    if (!isStart && row == HConstants.EMPTY_END_ROW) return state.stripeFiles.size() - 1;
+    // If there's an exact match below, a stripe ends at "row". Stripe right boundary is
+    // exclusive, so that means the row is in the next stripe; thus, we need to add one to index.
+    // If there's no match, the return value of binarySearch is (-(insertion point) - 1), where
+    // insertion point is the index of the next greater element, or list size if none. The
+    // insertion point happens to be exactly what we need, so we need to add one to the result.
+    return Math.abs(Arrays.binarySearch(state.stripeEndRows, row, Bytes.BYTES_COMPARATOR) + 1);
+  }
+
+  @Override
+  public final byte[] getStartRow(int stripeIndex) {
+    return (stripeIndex == 0  ? OPEN_KEY : state.stripeEndRows[stripeIndex - 1]);
+  }
+
+  @Override
+  public final byte[] getEndRow(int stripeIndex) {
+    return (stripeIndex == state.stripeEndRows.length
+        ? OPEN_KEY : state.stripeEndRows[stripeIndex]);
+  }
+
+
+  private byte[] startOf(StoreFile sf) {
+    byte[] result = this.fileStarts.get(sf);
+    return result == null ? sf.getMetadataValue(STRIPE_START_KEY)
+        : (result == INVALID_KEY_IN_MAP ? INVALID_KEY : result);
+  }
+
+  private byte[] endOf(StoreFile sf) {
+    byte[] result = this.fileEnds.get(sf);
+    return result == null ? sf.getMetadataValue(STRIPE_END_KEY)
+        : (result == INVALID_KEY_IN_MAP ? INVALID_KEY : result);
+  }
+
+  /**
+   * Inserts a file in the correct place (by seqnum) in a stripe copy.
+   * @param stripe Stripe copy to insert into.
+   * @param sf File to insert.
+   */
+  private static void insertFileIntoStripe(ArrayList<StoreFile> stripe, StoreFile sf) {
+    // The only operation for which sorting of the files matters is KeyBefore. Therefore,
+    // we will store the file in reverse order by seqNum from the outset.
+    for (int insertBefore = 0; ; ++insertBefore) {
+      if (insertBefore == stripe.size()
+          || (StoreFile.Comparators.SEQ_ID.compare(sf, stripe.get(insertBefore)) >= 0)) {
+        stripe.add(insertBefore, sf);
+        break;
+      }
+    }
+  }
+
+  /**
+   * An extension of ConcatenatedLists that has several peculiar properties.
+   * First, one can cut the tail of the logical list by removing last several sub-lists.
+   * Second, items can be removed thru iterator.
+   * Third, if the sub-lists are immutable, they are replaced with mutable copies when needed.
+   * On average KeyBefore operation will contain half the stripes as potential candidates,
+   * but will quickly cut down on them as it finds something in the more likely ones; thus,
+   * the above allow us to avoid unnecessary copying of a bunch of lists.
+   */
+  private static class KeyBeforeConcatenatedLists extends ConcatenatedLists<StoreFile> {
+    @Override
+    public java.util.Iterator<StoreFile> iterator() {
+      return new Iterator();
+    }
+
+    public class Iterator extends ConcatenatedLists<StoreFile>.Iterator {
+      public ArrayList<List<StoreFile>> getComponents() {
+        return components;
+      }
+
+      public void removeComponents(int startIndex) {
+        List<List<StoreFile>> subList = components.subList(startIndex, components.size());
+        for (List<StoreFile> entry : subList) {
+          size -= entry.size();
+        }
+        assert size >= 0;
+        subList.clear();
+      }
+
+      @Override
+      public void remove() {
+        if (!this.nextWasCalled) {
+          throw new IllegalStateException("No element to remove");
+        }
+        this.nextWasCalled = false;
+        List<StoreFile> src = components.get(currentComponent);
+        if (src instanceof ImmutableList<?>) {
+          src = new ArrayList<StoreFile>(src);
+          components.set(currentComponent, src);
+        }
+        src.remove(indexWithinComponent);
+        --size;
+        --indexWithinComponent;
+        if (src.isEmpty()) {
+          components.remove(currentComponent); // indexWithinComponent is already -1 here.
+        }
+      }
+    }
+  }
+
+  /**
+   * Non-static helper class for merging compaction or flush results.
+   * Since we want to merge them atomically (more or less), it operates on lazy copies,
+   * then creates a new state object and puts it in place.
+   */
+  private class CompactionOrFlushMergeCopy {
+    private ArrayList<List<StoreFile>> stripeFiles = null;
+    private ArrayList<StoreFile> level0Files = null;
+    private ArrayList<byte[]> stripeEndRows = null;
+
+    private Collection<StoreFile> compactedFiles = null;
+    private Collection<StoreFile> results = null;
+
+    private List<StoreFile> l0Results = new ArrayList<StoreFile>();
+    private final boolean isFlush;
+
+    public CompactionOrFlushMergeCopy(boolean isFlush) {
+      // Create a lazy mutable copy (other fields are so lazy they start out as nulls).
+      this.stripeFiles = new ArrayList<List<StoreFile>>(
+          StripeStoreFileManager.this.state.stripeFiles);
+      this.isFlush = isFlush;
+    }
+
+    public void mergeResults(Collection<StoreFile> compactedFiles, Collection<StoreFile> results)
+        throws IOException {
+      assert this.compactedFiles == null && this.results == null;
+      this.compactedFiles = compactedFiles;
+      this.results = results;
+      // Do logical processing.
+      if (!isFlush) removeCompactedFiles();
+      TreeMap<byte[], StoreFile> newStripes = processResults();
+      if (newStripes != null) {
+        processNewCandidateStripes(newStripes);
+      }
+      // Create new state and update parent.
+      State state = createNewState();
+      StripeStoreFileManager.this.state = state;
+      updateMetadataMaps();
+    }
+
+    private State createNewState() {
+      State oldState = StripeStoreFileManager.this.state;
+      // Stripe count should be the same unless the end rows changed.
+      assert oldState.stripeFiles.size() == this.stripeFiles.size() || this.stripeEndRows != null;
+      State newState = new State();
+      newState.level0Files = (this.level0Files == null) ? oldState.level0Files
+          : ImmutableList.copyOf(this.level0Files);
+      newState.stripeEndRows = (this.stripeEndRows == null) ? oldState.stripeEndRows
+          : this.stripeEndRows.toArray(new byte[this.stripeEndRows.size()][]);
+      newState.stripeFiles = new ArrayList<ImmutableList<StoreFile>>(this.stripeFiles.size());
+      for (List<StoreFile> newStripe : this.stripeFiles) {
+        newState.stripeFiles.add(newStripe instanceof ImmutableList<?>
+            ? (ImmutableList<StoreFile>)newStripe : ImmutableList.copyOf(newStripe));
+      }
+
+      List<StoreFile> newAllFiles = new ArrayList<StoreFile>(oldState.allFilesCached);
+      if (!isFlush) newAllFiles.removeAll(compactedFiles);
+      newAllFiles.addAll(results);
+      newState.allFilesCached = ImmutableList.copyOf(newAllFiles);
+      return newState;
+    }
+
+    private void updateMetadataMaps() {
+      StripeStoreFileManager parent = StripeStoreFileManager.this;
+      if (!isFlush) {
+        for (StoreFile sf : this.compactedFiles) {
+          parent.fileStarts.remove(sf);
+          parent.fileEnds.remove(sf);
+        }
+      }
+      if (this.l0Results != null) {
+        for (StoreFile sf : this.l0Results) {
+          parent.ensureLevel0Metadata(sf);
+        }
+      }
+    }
+
+    /**
+     * @param index Index of the stripe we need.
+     * @return A lazy stripe copy from current stripes.
+     */
+    private final ArrayList<StoreFile> getStripeCopy(int index) {
+      List<StoreFile> stripeCopy = this.stripeFiles.get(index);
+      ArrayList<StoreFile> result = null;
+      if (stripeCopy instanceof ImmutableList<?>) {
+        result = new ArrayList<StoreFile>(stripeCopy);
+        this.stripeFiles.set(index, result);
+      } else {
+        result = (ArrayList<StoreFile>)stripeCopy;
+      }
+      return result;
+    }
+
+    /**
+     * @return A lazy L0 copy from current state.
+     */
+    private final ArrayList<StoreFile> getLevel0Copy() {
+      if (this.level0Files == null) {
+        this.level0Files = new ArrayList<StoreFile>(StripeStoreFileManager.this.state.level0Files);
+      }
+      return this.level0Files;
+    }
+
+    /**
+     * Process new files, and add them either to the structure of existing stripes,
+     * or to the list of new candidate stripes.
+     * @return New candidate stripes.
+     */
+    private TreeMap<byte[], StoreFile> processResults() throws IOException {
+      TreeMap<byte[], StoreFile> newStripes = null;
+      for (StoreFile sf : this.results) {
+        byte[] startRow = startOf(sf), endRow = endOf(sf);
+        if (isInvalid(endRow) || isInvalid(startRow)) {
+          if (!isFlush) {
+            LOG.warn("The newly compacted file doesn't have stripes set: " + sf.getPath());
+          }
+          insertFileIntoStripe(getLevel0Copy(), sf);
+          this.l0Results.add(sf);
+          continue;
+        }
+        if (!this.stripeFiles.isEmpty()) {
+          int stripeIndex = findStripeIndexByEndRow(endRow);
+          if ((stripeIndex >= 0) && rowEquals(getStartRow(stripeIndex), startRow)) {
+            // Simple/common case - add file to an existing stripe.
+            insertFileIntoStripe(getStripeCopy(stripeIndex), sf);
+            continue;
+          }
+        }
+
+        // Make a new candidate stripe.
+        if (newStripes == null) {
+          newStripes = new TreeMap<byte[], StoreFile>(MAP_COMPARATOR);
+        }
+        StoreFile oldSf = newStripes.put(endRow, sf);
+        if (oldSf != null) {
+          throw new IOException("Compactor has produced multiple files for the stripe ending in ["
+              + Bytes.toString(endRow) + "], found " + sf.getPath() + " and " + oldSf.getPath());
+        }
+      }
+      return newStripes;
+    }
+
+    /**
+     * Remove compacted files.
+     * @param compactedFiles Compacted files.
+     */
+    private void removeCompactedFiles() throws IOException {
+      for (StoreFile oldFile : this.compactedFiles) {
+        byte[] oldEndRow = endOf(oldFile);
+        List<StoreFile> source = null;
+        if (isInvalid(oldEndRow)) {
+          source = getLevel0Copy();
+        } else {
+          int stripeIndex = findStripeIndexByEndRow(oldEndRow);
+          if (stripeIndex < 0) {
+            throw new IOException("An allegedly compacted file [" + oldFile + "] does not belong"
+                + " to a known stripe (end row - [" + Bytes.toString(oldEndRow) + "])");
+          }
+          source = getStripeCopy(stripeIndex);
+        }
+        if (!source.remove(oldFile)) {
+          throw new IOException("An allegedly compacted file [" + oldFile + "] was not found");
+        }
+      }
+    }
+
+    /**
+     * See {@link #addCompactionResults(Collection, Collection)} - updates the stripe list with
+     * new candidate stripes/removes old stripes; produces new set of stripe end rows.
+     * @param newStripes  New stripes - files by end row.
+     */
+    private void processNewCandidateStripes(
+        TreeMap<byte[], StoreFile> newStripes) throws IOException {
+      // Validate that the removed and added aggregate ranges still make for a full key space.
+      boolean hasStripes = !this.stripeFiles.isEmpty();
+      this.stripeEndRows = new ArrayList<byte[]>(
+          Arrays.asList(StripeStoreFileManager.this.state.stripeEndRows));
+      int removeFrom = 0;
+      byte[] firstStartRow = startOf(newStripes.firstEntry().getValue());
+      byte[] lastEndRow = newStripes.lastKey();
+      if (!hasStripes && (!isOpen(firstStartRow) || !isOpen(lastEndRow))) {
+        throw new IOException("Newly created stripes do not cover the entire key space.");
+      }
+
+      boolean canAddNewStripes = true;
+      Collection<StoreFile> filesForL0 = null;
+      if (hasStripes) {
+        // Determine which stripes will need to be removed because they conflict with new stripes.
+        // The new boundaries should match old stripe boundaries, so we should get exact matches.
+        if (isOpen(firstStartRow)) {
+          removeFrom = 0;
+        } else {
+          removeFrom = findStripeIndexByEndRow(firstStartRow);
+          if (removeFrom < 0) throw new IOException("Compaction is trying to add a bad range.");
+          ++removeFrom;
+        }
+        int removeTo = findStripeIndexByEndRow(lastEndRow);
+        if (removeTo < 0) throw new IOException("Compaction is trying to add a bad range.");
+        // See if there are files in the stripes we are trying to replace.
+        ArrayList<StoreFile> conflictingFiles = new ArrayList<StoreFile>();
+        for (int removeIndex = removeTo; removeIndex >= removeFrom; --removeIndex) {
+          conflictingFiles.addAll(this.stripeFiles.get(removeIndex));
+        }
+        if (!conflictingFiles.isEmpty()) {
+          // This can be caused by two things - concurrent flush into stripes, or a bug.
+          // Unfortunately, we cannot tell them apart without looking at timing or something
+          // like that. We will assume we are dealing with a flush and dump it into L0.
+          if (isFlush) {
+            long newSize = StripeCompactionPolicy.getTotalFileSize(newStripes.values());
+            LOG.warn("Stripes were created by a flush, but results of size " + newSize
+                + " cannot be added due to a concurrent flush also creating stripes");
+            canAddNewStripes = false;
+            filesForL0 = newStripes.values();
+          } else {
+            long oldSize = StripeCompactionPolicy.getTotalFileSize(conflictingFiles);
+            LOG.info(conflictingFiles.size() + " conflicting files (likely created by a flush) "
+                + " of size " + oldSize + " are moved to L0 due to concurrent stripe change");
+            filesForL0 = conflictingFiles;
+          }
+          if (filesForL0 != null) {
+            for (StoreFile sf : filesForL0) {
+              insertFileIntoStripe(getLevel0Copy(), sf);
+            }
+            l0Results.addAll(filesForL0);
+          }
+        }
+
+        if (canAddNewStripes) {
+          // Remove old empty stripes.
+          int originalCount = this.stripeFiles.size();
+          for (int removeIndex = removeTo; removeIndex >= removeFrom; --removeIndex) {
+            if (removeIndex != originalCount - 1) {
+              this.stripeEndRows.remove(removeIndex);
+            }
+            this.stripeFiles.remove(removeIndex);
+          }
+        }
+      }
+
+      if (!canAddNewStripes) return; // Files were already put into L0.
+
+      // Now, insert new stripes. The total ranges match, so we can insert where we removed.
+      byte[] previousEndRow = null;
+      int insertAt = removeFrom;
+      for (Map.Entry<byte[], StoreFile> newStripe : newStripes.entrySet()) {
+        if (previousEndRow != null) {
+          // Validate that the ranges are contiguous.
+          assert !isOpen(previousEndRow);
+          byte[] startRow = startOf(newStripe.getValue());
+          if (!rowEquals(previousEndRow, startRow)) {
+            throw new IOException("The new stripes produced by "
+                + (isFlush ? "flush" : "compaction") + " are not contiguous");
+          }
+        }
+        // Add the new stripe.
+        ArrayList<StoreFile> tmp = new ArrayList<StoreFile>();
+        tmp.add(newStripe.getValue());
+        stripeFiles.add(insertAt, tmp);
+        previousEndRow = newStripe.getKey();
+        if (!isOpen(previousEndRow)) {
+          stripeEndRows.add(insertAt, previousEndRow);
+        }
+        ++insertAt;
+      }
+    }
+  }
+
+  @Override
+  public List<StoreFile> getLevel0Files() {
+    return this.state.level0Files;
+  }
+
+  @Override
+  public List<byte[]> getStripeBoundaries() {
+    if (this.state.stripeFiles.isEmpty()) return new ArrayList<byte[]>();
+    ArrayList<byte[]> result = new ArrayList<byte[]>(this.state.stripeEndRows.length + 2);
+    result.add(OPEN_KEY);
+    for (int i = 0; i < this.state.stripeEndRows.length; ++i) {
+      result.add(this.state.stripeEndRows[i]);
+    }
+    result.add(OPEN_KEY);
+    return result;
+  }
+
+  @Override
+  public ArrayList<ImmutableList<StoreFile>> getStripes() {
+    return this.state.stripeFiles;
+  }
+
+  @Override
+  public int getStripeCount() {
+    return this.state.stripeFiles.size();
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
new file mode 100644
index 0000000..cbfd07a
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.java
@@ -0,0 +1,171 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.SortedSet;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.monitoring.MonitoredTask;
+import org.apache.hadoop.hbase.regionserver.StoreFile.Writer;
+import org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
+import org.apache.hadoop.hbase.util.CollectionBackedScanner;
+
+import com.google.common.annotations.VisibleForTesting;
+
+/**
+ * Stripe implementation of StoreFlusher. Flushes files either into L0 file w/o metadata, or
+ * into separate striped files, avoiding L0.
+ */
+public class StripeStoreFlusher extends StoreFlusher {
+  private static final Log LOG = LogFactory.getLog(StripeStoreFlusher.class);
+  private final Object flushLock = new Object();
+  private final StripeCompactionPolicy policy;
+  private final StripeCompactionPolicy.StripeInformationProvider stripes;
+
+  public StripeStoreFlusher(Configuration conf, Store store,
+      StripeCompactionPolicy policy, StripeStoreFileManager stripes) {
+    super(conf, store);
+    this.policy = policy;
+    this.stripes = stripes;
+  }
+
+  @Override
+  public List<Path> flushSnapshot(SortedSet<KeyValue> snapshot, long cacheFlushSeqNum,
+      final TimeRangeTracker tracker, AtomicLong flushedSize, MonitoredTask status)
+          throws IOException {
+    List<Path> result = null;
+    int kvCount = snapshot.size();
+    if (kvCount == 0) return result; // don't flush if there are no entries
+
+    long smallestReadPoint = store.getSmallestReadPoint();
+    InternalScanner scanner = createScanner(snapshot, smallestReadPoint);
+    if (scanner == null) {
+      return result; // NULL scanner returned from coprocessor hooks means skip normal processing
+    }
+
+    // Let policy select flush method.
+    StripeFlushRequest req = this.policy.selectFlush(this.stripes, kvCount);
+
+    long flushedBytes = 0;
+    boolean success = false;
+    StripeMultiFileWriter mw = null;
+    try {
+      mw = req.createWriter(); // Writer according to the policy.
+      StripeMultiFileWriter.WriterFactory factory = createWriterFactory(tracker, kvCount);
+      StoreScanner storeScanner = (scanner instanceof StoreScanner) ? (StoreScanner)scanner : null;
+      mw.init(storeScanner, factory, store.getComparator());
+
+      synchronized (flushLock) {
+        flushedBytes = performFlush(scanner, mw, smallestReadPoint);
+        result = mw.commitWriters(cacheFlushSeqNum, false);
+        success = true;
+      }
+    } finally {
+      if (!success && (mw != null)) {
+        result.clear();
+        for (Path leftoverFile : mw.abortWriters()) {
+          try {
+            store.getFileSystem().delete(leftoverFile, false);
+          } catch (Exception e) {
+            LOG.error("Failed to delete a file after failed flush: " + e);
+          }
+        }
+      }
+      flushedSize.set(flushedBytes);
+      try {
+        scanner.close();
+      } catch (IOException ex) {
+        LOG.warn("Failed to close flush scanner, ignoring", ex);
+      }
+    }
+    return result;
+  }
+
+  private StripeMultiFileWriter.WriterFactory createWriterFactory(
+      final TimeRangeTracker tracker, final long kvCount) {
+    return new StripeMultiFileWriter.WriterFactory() {
+      @Override
+      public Writer createWriter() throws IOException {
+        StoreFile.Writer writer = store.createWriterInTmp(
+            kvCount, store.getFamily().getCompression(), false, true, true);
+        writer.setTimeRangeTracker(tracker);
+        return writer;
+      }
+    };
+  }
+
+  /** Stripe flush request wrapper that writes a non-striped file. */
+  public static class StripeFlushRequest {
+    @VisibleForTesting
+    public StripeMultiFileWriter createWriter() throws IOException {
+      StripeMultiFileWriter writer =
+          new StripeMultiFileWriter.SizeMultiWriter(1, Long.MAX_VALUE, OPEN_KEY, OPEN_KEY);
+      writer.setNoStripeMetadata();
+      return writer;
+    }
+  }
+
+  /** Stripe flush request wrapper based on boundaries. */
+  public static class BoundaryStripeFlushRequest extends StripeFlushRequest {
+    private final List<byte[]> targetBoundaries;
+
+    /** @param targetBoundaries New files should be written with these boundaries. */
+    public BoundaryStripeFlushRequest(List<byte[]> targetBoundaries) {
+      this.targetBoundaries = targetBoundaries;
+    }
+
+    @Override
+    public StripeMultiFileWriter createWriter() throws IOException {
+      return new StripeMultiFileWriter.BoundaryMultiWriter(targetBoundaries, null, null);
+    }
+  }
+
+  /** Stripe flush request wrapper based on size. */
+  public static class SizeStripeFlushRequest extends StripeFlushRequest {
+    private final int targetCount;
+    private final long targetKvs;
+
+    /**
+     * @param targetCount The maximum number of stripes to flush into.
+     * @param targetKvs The KV count of each segment. If targetKvs*targetCount is less than
+     *                  total number of kvs, all the overflow data goes into the last stripe.
+     */
+    public SizeStripeFlushRequest(int targetCount, long targetKvs) {
+      this.targetCount = targetCount;
+      this.targetKvs = targetKvs;
+    }
+
+    @Override
+    public StripeMultiFileWriter createWriter() throws IOException {
+      return new StripeMultiFileWriter.SizeMultiWriter(
+          this.targetCount, this.targetKvs, OPEN_KEY, OPEN_KEY);
+    }
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionConfiguration.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionConfiguration.java
index 5e3e39f..7b833cc 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionConfiguration.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionConfiguration.java
@@ -47,6 +47,7 @@ public class CompactionConfiguration {
   static final Log LOG = LogFactory.getLog(CompactionConfiguration.class);
 
   private static final String CONFIG_PREFIX = "hbase.hstore.compaction.";
+  public static final String RATIO_KEY = CONFIG_PREFIX + "ratio";
 
   Configuration conf;
   StoreConfigInformation storeConfigInfo;
@@ -72,7 +73,7 @@ public class CompactionConfiguration {
     minFilesToCompact = Math.max(2, conf.getInt(CONFIG_PREFIX + "min",
           /*old name*/ conf.getInt("hbase.hstore.compactionThreshold", 3)));
     maxFilesToCompact = conf.getInt(CONFIG_PREFIX + "max", 10);
-    compactionRatio = conf.getFloat(CONFIG_PREFIX + "ratio", 1.2F);
+    compactionRatio = conf.getFloat(RATIO_KEY, 1.2F);
     offPeekCompactionRatio = conf.getFloat(CONFIG_PREFIX + "ratio.offpeak", 5.0F);
 
     throttlePoint =  conf.getLong("hbase.regionserver.thread.compaction.throttle",
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.java
index 018e497..ce54485 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.java
@@ -56,14 +56,6 @@ public abstract class CompactionPolicy {
   public abstract boolean throttleCompaction(long compactionSize);
 
   /**
-   * @param storeFiles Current store files.
-   * @param filesCompacting files currently compacting.
-   * @return whether a compactionSelection is possible
-   */
-  public abstract boolean needsCompaction(final Collection<StoreFile> storeFiles,
-      final List<StoreFile> filesCompacting);
-
-  /**
    * Inform the policy that some configuration has been change,
    * so cached value should be updated it any.
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
index 24e6701..7f49a40 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/Compactor.java
@@ -33,7 +33,6 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.client.Scan;
-import org.apache.hadoop.hbase.io.CellOutputStream;
 import org.apache.hadoop.hbase.io.compress.Compression;
 import org.apache.hadoop.hbase.io.hfile.HFile.FileInfo;
 import org.apache.hadoop.hbase.io.hfile.HFileWriterV2;
@@ -79,32 +78,6 @@ public abstract class Compactor {
     void append(KeyValue kv) throws IOException;
   }
 
-  /**
-   * Do a minor/major compaction on an explicit set of storefiles from a Store.
-   * @param request the requested compaction
-   * @return Product of compaction or an empty list if all cells expired or deleted and nothing made
-   *         it through the compaction.
-   * @throws IOException
-   */
-  public abstract List<Path> compact(final CompactionRequest request) throws IOException;
-
-  /**
-   * Compact a list of files for testing. Creates a fake {@link CompactionRequest} to pass to
-   * {@link #compact(CompactionRequest)};
-   * @param filesToCompact the files to compact. These are used as the compactionSelection for the
-   *          generated {@link CompactionRequest}.
-   * @param isMajor true to major compact (prune all deletes, max versions, etc)
-   * @return Product of compaction or an empty list if all cells expired or deleted and nothing made
-   *         it through the compaction.
-   * @throws IOException
-   */
-  public List<Path> compactForTesting(final Collection<StoreFile> filesToCompact, boolean isMajor)
-      throws IOException {
-    CompactionRequest cr = new CompactionRequest(filesToCompact);
-    cr.setIsMajor(isMajor);
-    return this.compact(cr);
-  }
-
   public CompactionProgress getProgress() {
     return this.progress;
   }
@@ -123,6 +96,12 @@ public abstract class Compactor {
     public int maxTagsLength = 0;
   }
 
+  /**
+   * Extracts some details about the files to compact that are commonly needed by compactors.
+   * @param filesToCompact Files.
+   * @param calculatePutTs Whether earliest put TS is needed.
+   * @return The result.
+   */
   protected FileDetails getFileDetails(
       Collection<StoreFile> filesToCompact, boolean calculatePutTs) throws IOException {
     FileDetails fd = new FileDetails();
@@ -177,6 +156,11 @@ public abstract class Compactor {
     return fd;
   }
 
+  /**
+   * Creates file scanners for compaction.
+   * @param filesToCompact Files.
+   * @return Scanners.
+   */
   protected List<StoreFileScanner> createFileScanners(
       final Collection<StoreFile> filesToCompact, long smallestReadPoint) throws IOException {
     return StoreFileScanner.getScannersForStoreFiles(filesToCompact, false, false, true,
@@ -187,6 +171,14 @@ public abstract class Compactor {
     return store.getSmallestReadPoint();
   }
 
+  /**
+   * Calls coprocessor, if any, to create compaction scanner - before normal scanner creation.
+   * @param request Compaction request.
+   * @param scanType Scan type.
+   * @param earliestPutTs Earliest put ts.
+   * @param scanners File scanners for compaction files.
+   * @return Scanner override by coprocessor; null if not overriding.
+   */
   protected InternalScanner preCreateCoprocScanner(final CompactionRequest request,
       ScanType scanType, long earliestPutTs,  List<StoreFileScanner> scanners) throws IOException {
     if (store.getCoprocessorHost() == null) return null;
@@ -194,13 +186,27 @@ public abstract class Compactor {
         .preCompactScannerOpen(store, scanners, scanType, earliestPutTs, request);
   }
 
-  protected InternalScanner postCreateCoprocScanner(final CompactionRequest request,
+  /**
+   * Calls coprocessor, if any, to create scanners - after normal scanner creation.
+   * @param request Compaction request.
+   * @param scanType Scan type.
+   * @param scanner The default scanner created for compaction.
+   * @return Scanner scanner to use (usually the default); null if compaction should not proceed.
+   */
+   protected InternalScanner postCreateCoprocScanner(final CompactionRequest request,
       ScanType scanType, InternalScanner scanner) throws IOException {
     if (store.getCoprocessorHost() == null) return scanner;
     return store.getCoprocessorHost().preCompact(store, scanner, scanType, request);
   }
 
   @SuppressWarnings("deprecation")
+  /**
+   * Performs the compaction.
+   * @param scanner Where to read from.
+   * @param writer Where to write to.
+   * @param smallestReadPoint Smallest read point.
+   * @return Whether compaction ended; false if it was interrupted for some reason.
+   */
   protected boolean performCompaction(InternalScanner scanner,
       CellSink writer, long smallestReadPoint) throws IOException {
     int bytesWritten = 0;
@@ -239,12 +245,8 @@ public abstract class Compactor {
     return true;
   }
 
-  protected void abortWriter(final StoreFile.Writer writer) throws IOException {
-    writer.close();
-    store.getFileSystem().delete(writer.getPath(), false);
-  }
-
   /**
+   * @param store store
    * @param scanners Store file scanners.
    * @param scanType Scan type.
    * @param smallestReadPoint Smallest MVCC read point.
@@ -258,4 +260,20 @@ public abstract class Compactor {
     return new StoreScanner(store, store.getScanInfo(), scan, scanners,
         scanType, smallestReadPoint, earliestPutTs);
   }
+
+  /**
+   * @param scanners Store file scanners.
+   * @param scanType Scan type.
+   * @param smallestReadPoint Smallest MVCC read point.
+   * @param earliestPutTs Earliest put across all files.
+   * @return A compaction scanner.
+   */
+  protected InternalScanner createScanner(Store store, List<StoreFileScanner> scanners,
+     long smallestReadPoint, long earliestPutTs, byte[] dropDeletesFromRow,
+     byte[] dropDeletesToRow) throws IOException {
+    Scan scan = new Scan();
+    scan.setMaxVersions(store.getFamily().getMaxVersions());
+    return new StoreScanner(store, store.getScanInfo(), scan, scanners, smallestReadPoint,
+        earliestPutTs, dropDeletesFromRow, dropDeletesToRow);
+  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.java
index 5757443..7a479d8 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.java
@@ -20,6 +20,7 @@ package org.apache.hadoop.hbase.regionserver.compactions;
 import java.io.IOException;
 import java.io.InterruptedIOException;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 import org.apache.hadoop.classification.InterfaceAudience;
@@ -74,7 +75,8 @@ public class DefaultCompactor extends Compactor {
             fd.maxMVCCReadpoint >= smallestReadPoint, fd.maxTagsLength > 0);
         boolean finished = performCompaction(scanner, writer, smallestReadPoint);
         if (!finished) {
-          abortWriter(writer);
+          writer.close();
+          store.getFileSystem().delete(writer.getPath(), false);
           writer = null;
           throw new InterruptedIOException( "Aborting compaction of store " + store +
               " in region " + store.getRegionInfo().getRegionNameAsString() +
@@ -94,4 +96,21 @@ public class DefaultCompactor extends Compactor {
     }
     return newFiles;
   }
+
+  /**
+   * Compact a list of files for testing. Creates a fake {@link CompactionRequest} to pass to
+   * {@link #compact(CompactionRequest)};
+   * @param filesToCompact the files to compact. These are used as the compactionSelection for
+   *          the generated {@link CompactionRequest}.
+   * @param isMajor true to major compact (prune all deletes, max versions, etc)
+   * @return Product of compaction or an empty list if all cells expired or deleted and nothing \
+   *         made it through the compaction.
+   * @throws IOException
+   */
+  public List<Path> compactForTesting(final Collection<StoreFile> filesToCompact, boolean isMajor)
+      throws IOException {
+    CompactionRequest cr = new CompactionRequest(filesToCompact);
+    cr.setIsMajor(isMajor);
+    return this.compact(cr);
+  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
index 7230ed0..bdf46ae 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.java
@@ -53,9 +53,19 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
   @Override
   final ArrayList<StoreFile> applyCompactionPolicy(final ArrayList<StoreFile> candidates,
     final boolean mayUseOffPeak, final boolean mightBeStuck) throws IOException {
+    return new ArrayList<StoreFile>(applyCompactionPolicy(candidates, mightBeStuck,
+        mayUseOffPeak, comConf.getMinFilesToCompact(), comConf.getMaxFilesToCompact()));
+  }
+
+  public List<StoreFile> applyCompactionPolicy(final List<StoreFile> candidates,
+       boolean mightBeStuck, boolean mayUseOffPeak, int minFiles, int maxFiles) {
+
+    final double currentRatio = mayUseOffPeak
+        ? comConf.getCompactionRatioOffPeak() : comConf.getCompactionRatio();
+
     // Start off choosing nothing.
     List<StoreFile> bestSelection = new ArrayList<StoreFile>(0);
-    List<StoreFile> smallest = new ArrayList<StoreFile>(0);
+    List<StoreFile> smallest = mightBeStuck ? new ArrayList<StoreFile>(0) : null;
     long bestSize = 0;
     long smallestSize = Long.MAX_VALUE;
 
@@ -63,15 +73,15 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
     // Consider every starting place.
     for (int start = 0; start < candidates.size(); start++) {
       // Consider every different sub list permutation in between start and end with min files.
-      for (int currentEnd = start + comConf.getMinFilesToCompact() - 1;
+      for (int currentEnd = start + minFiles - 1;
           currentEnd < candidates.size(); currentEnd++) {
         List<StoreFile> potentialMatchFiles = candidates.subList(start, currentEnd + 1);
 
         // Sanity checks
-        if (potentialMatchFiles.size() < comConf.getMinFilesToCompact()) {
+        if (potentialMatchFiles.size() < minFiles) {
           continue;
         }
-        if (potentialMatchFiles.size() > comConf.getMaxFilesToCompact()) {
+        if (potentialMatchFiles.size() > maxFiles) {
           continue;
         }
 
@@ -81,7 +91,7 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
 
         // Store the smallest set of files.  This stored set of files will be used
         // if it looks like the algorithm is stuck.
-        if (size < smallestSize) {
+        if (mightBeStuck && size < smallestSize) {
           smallest = potentialMatchFiles;
           smallestSize = size;
         }
@@ -92,7 +102,7 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
 
         ++opts;
         if (size >= comConf.getMinCompactSize()
-            && !filesInRatio(potentialMatchFiles, mayUseOffPeak)) {
+            && !filesInRatio(potentialMatchFiles, currentRatio)) {
           continue;
         }
 
@@ -150,15 +160,13 @@ public class ExploringCompactionPolicy extends RatioBasedCompactionPolicy {
    *      FileSize(i) <= ( Sum(0,N,FileSize(_)) - FileSize(i) ) * Ratio.
    *
    * @param files List of store files to consider as a compaction candidate.
-   * @param isOffPeak should the offPeak compaction ratio be used ?
+   * @param currentRatio The ratio to use.
    * @return a boolean if these files satisfy the ratio constraints.
    */
-  private boolean filesInRatio(final List<StoreFile> files, final boolean isOffPeak) {
+  private boolean filesInRatio(final List<StoreFile> files, final double currentRatio) {
     if (files.size() < 2) {
-      return  true;
+      return true;
     }
-    final double currentRatio =
-        isOffPeak ? comConf.getCompactionRatioOffPeak() : comConf.getCompactionRatio();
 
     long totalFileSize = getTotalStoreSize(files);
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java
index 74fa9e6..bc99226 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.java
@@ -388,7 +388,6 @@ public class RatioBasedCompactionPolicy extends CompactionPolicy {
     return compactionSize > comConf.getThrottlePoint();
   }
 
-  @Override
   public boolean needsCompaction(final Collection<StoreFile> storeFiles,
       final List<StoreFile> filesCompacting) {
     int numCandidates = storeFiles.size() - filesCompacting.size();
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
new file mode 100644
index 0000000..9b84530
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.java
@@ -0,0 +1,579 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver.compactions;
+
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreUtils;
+import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.ConcatenatedLists;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.Pair;
+
+import com.google.common.collect.ImmutableList;
+
+/**
+ * Stripe store implementation of compaction policy.
+ */
+@InterfaceAudience.Private
+public class StripeCompactionPolicy extends CompactionPolicy {
+  private final static Log LOG = LogFactory.getLog(StripeCompactionPolicy.class);
+  // Policy used to compact individual stripes.
+  private ExploringCompactionPolicy stripePolicy = null;
+
+  private StripeStoreConfig config;
+
+  public StripeCompactionPolicy(
+      Configuration conf, StoreConfigInformation storeConfigInfo, StripeStoreConfig config) {
+    super(conf, storeConfigInfo);
+    this.config = config;
+    stripePolicy = new ExploringCompactionPolicy(conf, storeConfigInfo);
+  }
+
+  public List<StoreFile> preSelectFilesForCoprocessor(StripeInformationProvider si,
+      List<StoreFile> filesCompacting) {
+    // We sincerely hope nobody is messing with us with their coprocessors.
+    // If they do, they are very likely to shoot themselves in the foot.
+    // We'll just exclude all the filesCompacting from the list.
+    ArrayList<StoreFile> candidateFiles = new ArrayList<StoreFile>(si.getStorefiles());
+    candidateFiles.removeAll(filesCompacting);
+    return candidateFiles;
+  }
+
+  public StripeCompactionRequest createEmptyRequest(
+      StripeInformationProvider si, CompactionRequest request) {
+    // Treat as L0-ish compaction with fixed set of files, and hope for the best.
+    if (si.getStripeCount() > 0) {
+      return new BoundaryStripeCompactionRequest(request, si.getStripeBoundaries());
+    }
+    Pair<Long, Integer> targetKvsAndCount = estimateTargetKvs(
+        request.getFiles(), this.config.getInitialCount());
+    return new SplitStripeCompactionRequest(
+        request, OPEN_KEY, OPEN_KEY, targetKvsAndCount.getSecond(), targetKvsAndCount.getFirst());
+  }
+
+  public StripeStoreFlusher.StripeFlushRequest selectFlush(
+      StripeInformationProvider si, int kvCount) {
+    if (this.config.isUsingL0Flush()) {
+      return new StripeStoreFlusher.StripeFlushRequest(); // L0 is used, return dumb request.
+    }
+    if (si.getStripeCount() == 0) {
+      // No stripes - start with the requisite count, derive KVs per stripe.
+      int initialCount = this.config.getInitialCount();
+      return new StripeStoreFlusher.SizeStripeFlushRequest(initialCount, kvCount / initialCount);
+    }
+    // There are stripes - do according to the boundaries.
+    return new StripeStoreFlusher.BoundaryStripeFlushRequest(si.getStripeBoundaries());
+  }
+
+  public StripeCompactionRequest selectCompaction(StripeInformationProvider si,
+      List<StoreFile> filesCompacting, boolean isOffpeak) throws IOException {
+    // TODO: first cut - no parallel compactions. To have more fine grained control we
+    //       probably need structure more sophisticated than a list.
+    if (!filesCompacting.isEmpty()) {
+      LOG.debug("Not selecting compaction: " + filesCompacting.size() + " files compacting");
+      return null;
+    }
+
+    // We are going to do variations of compaction in strict order of preference.
+    // A better/more advanced approach is to use a heuristic to see which one is "more
+    // necessary" at current time.
+
+    // This can happen due to region split. We can skip it later; for now preserve
+    // compact-all-things behavior.
+    Collection<StoreFile> allFiles = si.getStorefiles();
+    if (StoreUtils.hasReferences(allFiles)) {
+      LOG.debug("There are references in the store; compacting all files");
+      long targetKvs = estimateTargetKvs(allFiles, config.getSplitCount()).getFirst();
+      SplitStripeCompactionRequest request = new SplitStripeCompactionRequest(
+          allFiles, OPEN_KEY, OPEN_KEY, targetKvs);
+      request.setMajorRangeFull();
+      return request;
+    }
+
+    int stripeCount = si.getStripeCount();
+    List<StoreFile> l0Files = si.getLevel0Files();
+
+    // See if we need to make new stripes.
+    boolean shouldCompactL0 = (this.config.getLevel0MinFiles() <= l0Files.size());
+    if (stripeCount == 0) {
+      if (!shouldCompactL0) return null; // nothing to do.
+      return selectNewStripesCompaction(si);
+    }
+
+    boolean canDropDeletesNoL0 = l0Files.size() == 0;
+    if (shouldCompactL0) {
+      if (!canDropDeletesNoL0) {
+        // If we need to compact L0, see if we can add something to it, and drop deletes.
+        StripeCompactionRequest result = selectSingleStripeCompaction(
+            si, true, canDropDeletesNoL0, isOffpeak);
+        if (result != null) return result;
+      }
+      LOG.debug("Selecting L0 compaction with " + l0Files.size() + " files");
+      return new BoundaryStripeCompactionRequest(l0Files, si.getStripeBoundaries());
+    }
+
+    // Try to delete fully expired stripes
+    StripeCompactionRequest result = selectExpiredMergeCompaction(si, canDropDeletesNoL0);
+    if (result != null) return result;
+
+    // Ok, nothing special here, let's see if we need to do a common compaction.
+    // This will also split the stripes that are too big if needed.
+    return selectSingleStripeCompaction(si, false, canDropDeletesNoL0, isOffpeak);
+  }
+
+  public boolean needsCompactions(StripeInformationProvider si, List<StoreFile> filesCompacting) {
+    // Approximation on whether we need compaction.
+    return filesCompacting.isEmpty()
+        && (StoreUtils.hasReferences(si.getStorefiles())
+          || (si.getLevel0Files().size() >= this.config.getLevel0MinFiles())
+          || needsSingleStripeCompaction(si));
+  }
+
+  @Override
+  public boolean isMajorCompaction(Collection<StoreFile> filesToCompact) throws IOException {
+    return false; // there's never a major compaction!
+  }
+
+  @Override
+  public boolean throttleCompaction(long compactionSize) {
+    return compactionSize > comConf.getThrottlePoint();
+  }
+
+  /**
+   * @param si StoreFileManager.
+   * @return Whether any stripe potentially needs compaction.
+   */
+  protected boolean needsSingleStripeCompaction(StripeInformationProvider si) {
+    int minFiles = this.config.getStripeCompactMinFiles();
+    for (List<StoreFile> stripe : si.getStripes()) {
+      if (stripe.size() >= minFiles) return true;
+    }
+    return false;
+  }
+
+  protected StripeCompactionRequest selectSingleStripeCompaction(StripeInformationProvider si,
+      boolean includeL0, boolean canDropDeletesWithoutL0, boolean isOffpeak) throws IOException {
+    ArrayList<ImmutableList<StoreFile>> stripes = si.getStripes();
+
+    int bqIndex = -1;
+    List<StoreFile> bqSelection = null;
+    int stripeCount = stripes.size();
+    long bqTotalSize = -1;
+    for (int i = 0; i < stripeCount; ++i) {
+      // If we want to compact L0 to drop deletes, we only want whole-stripe compactions.
+      // So, pass includeL0 as 2nd parameter to indicate that.
+      List<StoreFile> selection = selectSimpleCompaction(stripes.get(i),
+          !canDropDeletesWithoutL0 && includeL0, isOffpeak);
+      if (selection.isEmpty()) continue;
+      long size = 0;
+      for (StoreFile sf : selection) {
+        size += sf.getReader().length();
+      }
+      if (bqSelection == null || selection.size() > bqSelection.size() ||
+          (selection.size() == bqSelection.size() && size < bqTotalSize)) {
+        bqSelection = selection;
+        bqIndex = i;
+        bqTotalSize = size;
+      }
+    }
+    if (bqSelection == null) {
+      LOG.debug("No good compaction is possible in any stripe");
+      return null;
+    }
+    List<StoreFile> filesToCompact = new ArrayList<StoreFile>(bqSelection);
+    // See if we can, and need to, split this stripe.
+    int targetCount = 1;
+    long targetKvs = Long.MAX_VALUE;
+    boolean hasAllFiles = filesToCompact.size() == stripes.get(bqIndex).size();
+    String splitString = "";
+    if (hasAllFiles && bqTotalSize >= config.getSplitSize()) {
+      if (includeL0) {
+        // We want to avoid the scenario where we compact a stripe w/L0 and then split it.
+        // So, if we might split, don't compact the stripe with L0.
+        return null;
+      }
+      Pair<Long, Integer> kvsAndCount = estimateTargetKvs(filesToCompact, config.getSplitCount());
+      targetKvs = kvsAndCount.getFirst();
+      targetCount = kvsAndCount.getSecond();
+      splitString = "; the stripe will be split into at most "
+          + targetCount + " stripes with " + targetKvs + " target KVs";
+    }
+
+    LOG.debug("Found compaction in a stripe with end key ["
+        + Bytes.toString(si.getEndRow(bqIndex)) + "], with "
+        + filesToCompact.size() + " files of total size " + bqTotalSize + splitString);
+
+    // See if we can drop deletes.
+    StripeCompactionRequest req;
+    if (includeL0) {
+      assert hasAllFiles;
+      List<StoreFile> l0Files = si.getLevel0Files();
+      LOG.debug("Adding " + l0Files.size() + " files to compaction to be able to drop deletes");
+      ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+      sfs.addSublist(filesToCompact);
+      sfs.addSublist(l0Files);
+      req = new BoundaryStripeCompactionRequest(sfs, si.getStripeBoundaries());
+    } else {
+      req = new SplitStripeCompactionRequest(
+          filesToCompact, si.getStartRow(bqIndex), si.getEndRow(bqIndex), targetCount, targetKvs);
+    }
+    if (canDropDeletesWithoutL0 || includeL0) {
+      req.setMajorRange(si.getStartRow(bqIndex), si.getEndRow(bqIndex));
+    }
+    req.getRequest().setOffPeak(isOffpeak);
+    return req;
+  }
+
+  /**
+   * Selects the compaction of a single stripe using default policy.
+   * @param sfs Files.
+   * @param allFilesOnly Whether a compaction of all-or-none files is needed.
+   * @return The resulting selection.
+   */
+  private List<StoreFile> selectSimpleCompaction(
+      List<StoreFile> sfs, boolean allFilesOnly, boolean isOffpeak) {
+    int minFilesLocal = Math.max(
+        allFilesOnly ? sfs.size() : 0, this.config.getStripeCompactMinFiles());
+    int maxFilesLocal = Math.max(this.config.getStripeCompactMaxFiles(), minFilesLocal);
+    return stripePolicy.applyCompactionPolicy(sfs, isOffpeak, false, minFilesLocal, maxFilesLocal);
+  }
+
+  /**
+   * Selects the compaction that compacts all files (to be removed later).
+   * @param si StoreFileManager.
+   * @param targetStripeCount Target stripe count.
+   * @param targetSize Target stripe size.
+   * @return The compaction.
+   */
+  private StripeCompactionRequest selectCompactionOfAllFiles(StripeInformationProvider si,
+      int targetStripeCount, long targetSize) {
+    Collection<StoreFile> allFiles = si.getStorefiles();
+    SplitStripeCompactionRequest request = new SplitStripeCompactionRequest(
+        allFiles, OPEN_KEY, OPEN_KEY, targetStripeCount, targetSize);
+    request.setMajorRangeFull();
+    LOG.debug("Selecting a compaction that includes all " + allFiles.size() + " files");
+    return request;
+  }
+
+  private StripeCompactionRequest selectNewStripesCompaction(StripeInformationProvider si) {
+    List<StoreFile> l0Files = si.getLevel0Files();
+    Pair<Long, Integer> kvsAndCount = estimateTargetKvs(l0Files, config.getInitialCount());
+    LOG.debug("Creating " + kvsAndCount.getSecond() + " initial stripes with "
+        + kvsAndCount.getFirst() + " kvs each via L0 compaction of " + l0Files.size() + " files");
+    SplitStripeCompactionRequest request = new SplitStripeCompactionRequest(
+        si.getLevel0Files(), OPEN_KEY, OPEN_KEY, kvsAndCount.getSecond(), kvsAndCount.getFirst());
+    request.setMajorRangeFull(); // L0 only, can drop deletes.
+    return request;
+  }
+
+  private StripeCompactionRequest selectExpiredMergeCompaction(
+      StripeInformationProvider si, boolean canDropDeletesNoL0) {
+    long cfTtl = this.storeConfigInfo.getStoreFileTtl();
+    if (cfTtl == Long.MAX_VALUE) {
+      return null; // minversion might be set, cannot delete old files
+    }
+    long timestampCutoff = EnvironmentEdgeManager.currentTimeMillis() - cfTtl;
+    // Merge the longest sequence of stripes where all files have expired, if any.
+    int start = -1, bestStart = -1, length = 0, bestLength = 0;
+    ArrayList<ImmutableList<StoreFile>> stripes = si.getStripes();
+    OUTER: for (int i = 0; i < stripes.size(); ++i) {
+      for (StoreFile storeFile : stripes.get(i)) {
+        if (storeFile.getReader().getMaxTimestamp() < timestampCutoff) continue;
+        // Found non-expired file, this stripe has to stay.
+        if (length > bestLength) {
+          bestStart = start;
+          bestLength = length;
+        }
+        start = -1;
+        length = 0;
+        continue OUTER;
+      }
+      if (start == -1) {
+        start = i;
+      }
+      ++length;
+    }
+    if (length > bestLength) {
+      bestStart = start;
+      bestLength = length;
+    }
+    if (bestLength == 0) return null;
+    if (bestLength == 1) {
+      // This is currently inefficient. If only one stripe expired, we will rewrite some
+      // entire stripe just to delete some expired files because we rely on metadata and it
+      // cannot simply be updated in an old file. When we either determine stripe dynamically
+      // or move metadata to manifest, we can just drop the "expired stripes".
+      if (bestStart == (stripes.size() - 1)) return null;
+      ++bestLength;
+    }
+    LOG.debug("Merging " + bestLength + " stripes to delete expired store files");
+    int endIndex = bestStart + bestLength - 1;
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    sfs.addAllSublists(stripes.subList(bestStart, endIndex + 1));
+    SplitStripeCompactionRequest result = new SplitStripeCompactionRequest(sfs,
+        si.getStartRow(bestStart), si.getEndRow(endIndex), 1, Long.MAX_VALUE);
+    if (canDropDeletesNoL0) {
+      result.setMajorRangeFull();
+    }
+    return result;
+  }
+
+  private static long getTotalKvCount(final Collection<StoreFile> candidates) {
+    long totalSize = 0;
+    for (StoreFile storeFile : candidates) {
+      totalSize += storeFile.getReader().getEntries();
+    }
+    return totalSize;
+  }
+
+  public static long getTotalFileSize(final Collection<StoreFile> candidates) {
+    long totalSize = 0;
+    for (StoreFile storeFile : candidates) {
+      totalSize += storeFile.getReader().length();
+    }
+    return totalSize;
+  }
+
+  private Pair<Long, Integer> estimateTargetKvs(Collection<StoreFile> files, double splitCount) {
+    // If the size is larger than what we target, we don't want to split into proportionally
+    // larger parts and then have to split again very soon. So, we will increase the multiplier
+    // by one until we get small enough parts. E.g. 5Gb stripe that should have been split into
+    // 2 parts when it was 3Gb will be split into 3x1.67Gb parts, rather than 2x2.5Gb parts.
+    long totalSize = getTotalFileSize(files);
+    long targetPartSize = config.getSplitPartSize();
+    assert targetPartSize > 0 && splitCount > 0;
+    double ratio = totalSize / (splitCount * targetPartSize); // ratio of real to desired size
+    while (ratio > 1.0) {
+      // Ratio of real to desired size if we increase the multiplier.
+      double newRatio = totalSize / ((splitCount + 1.0) * targetPartSize);
+      if ((1.0 / newRatio) >= ratio) break; // New ratio is < 1.0, but further than the last one.
+      ratio = newRatio;
+      splitCount += 1.0;
+    }
+    long kvCount = (long)(getTotalKvCount(files) / splitCount);
+    return new Pair<Long, Integer>(kvCount, (int)Math.ceil(splitCount));
+  }
+
+  /** Stripe compaction request wrapper. */
+  public abstract static class StripeCompactionRequest {
+    protected CompactionRequest request;
+    protected byte[] majorRangeFromRow = null, majorRangeToRow = null;
+
+    /**
+     * Executes the request against compactor (essentially, just calls correct overload of
+     * compact method), to simulate more dynamic dispatch.
+     * @param compactor Compactor.
+     * @return result of compact(...)
+     */
+    public abstract List<Path> execute(StripeCompactor compactor) throws IOException;
+
+    public StripeCompactionRequest(CompactionRequest request) {
+      this.request = request;
+    }
+
+    /**
+     * Sets compaction "major range". Major range is the key range for which all
+     * the files are included, so they can be treated like major-compacted files.
+     * @param startRow Left boundary, inclusive.
+     * @param endRow Right boundary, exclusive.
+     */
+    public void setMajorRange(byte[] startRow, byte[] endRow) {
+      this.majorRangeFromRow = startRow;
+      this.majorRangeToRow = endRow;
+    }
+
+    public CompactionRequest getRequest() {
+      return this.request;
+    }
+
+    public void setRequest(CompactionRequest request) {
+      assert request != null;
+      this.request = request;
+      this.majorRangeFromRow = this.majorRangeToRow = null;
+    }
+  }
+
+  /**
+   * Request for stripe compactor that will cause it to split the source files into several
+   * separate files at the provided boundaries.
+   */
+  private static class BoundaryStripeCompactionRequest extends StripeCompactionRequest {
+    private final List<byte[]> targetBoundaries;
+
+    /**
+     * @param request Original request.
+     * @param targetBoundaries New files should be written with these boundaries.
+     */
+    public BoundaryStripeCompactionRequest(CompactionRequest request,
+        List<byte[]> targetBoundaries) {
+      super(request);
+      this.targetBoundaries = targetBoundaries;
+    }
+
+    public BoundaryStripeCompactionRequest(Collection<StoreFile> files,
+        List<byte[]> targetBoundaries) {
+      this(new CompactionRequest(files), targetBoundaries);
+    }
+
+    @Override
+    public List<Path> execute(StripeCompactor compactor) throws IOException {
+      return compactor.compact(
+          this.request, this.targetBoundaries, this.majorRangeFromRow, this.majorRangeToRow);
+    }
+  }
+
+  /**
+   * Request for stripe compactor that will cause it to split the source files into several
+   * separate files into based on key-value count, as well as file count limit.
+   * Most of the files will be roughly the same size. The last file may be smaller or larger
+   * depending on the interplay of the amount of data and maximum number of files allowed.
+   */
+  private static class SplitStripeCompactionRequest extends StripeCompactionRequest {
+    private final byte[] startRow, endRow;
+    private final int targetCount;
+    private final long targetKvs;
+
+    /**
+     * @param request Original request.
+     * @param startRow Left boundary of the range to compact, inclusive.
+     * @param endRow Right boundary of the range to compact, exclusive.
+     * @param targetCount The maximum number of stripe to compact into.
+     * @param targetKvs The KV count of each segment. If targetKvs*targetCount is less than
+     *                  total number of kvs, all the overflow data goes into the last stripe.
+     */
+    public SplitStripeCompactionRequest(CompactionRequest request,
+        byte[] startRow, byte[] endRow, int targetCount, long targetKvs) {
+      super(request);
+      this.startRow = startRow;
+      this.endRow = endRow;
+      this.targetCount = targetCount;
+      this.targetKvs = targetKvs;
+    }
+
+    public SplitStripeCompactionRequest(
+        CompactionRequest request, byte[] startRow, byte[] endRow, long targetKvs) {
+      this(request, startRow, endRow, Integer.MAX_VALUE, targetKvs);
+    }
+
+    public SplitStripeCompactionRequest(
+        Collection<StoreFile> files, byte[] startRow, byte[] endRow, long targetKvs) {
+      this(files, startRow, endRow, Integer.MAX_VALUE, targetKvs);
+    }
+
+    public SplitStripeCompactionRequest(Collection<StoreFile> files,
+        byte[] startRow, byte[] endRow, int targetCount, long targetKvs) {
+      this(new CompactionRequest(files), startRow, endRow, targetCount, targetKvs);
+    }
+
+    @Override
+    public List<Path> execute(StripeCompactor compactor) throws IOException {
+      return compactor.compact(this.request, this.targetCount, this.targetKvs,
+          this.startRow, this.endRow, this.majorRangeFromRow, this.majorRangeToRow);
+    }
+
+    /** Set major range of the compaction to the entire compaction range.
+     * See {@link #setMajorRange(byte[], byte[])}. */
+    public void setMajorRangeFull() {
+      setMajorRange(this.startRow, this.endRow);
+    }
+  }
+
+  /** Helper class used to calculate size related things */
+  private static class StripeSizes {
+    public final ArrayList<Long> kvCounts;
+    public final ArrayList<Long> fileSizes;
+    public double avgKvCount = 0;
+    public long minKvCount = Long.MAX_VALUE, maxKvCount = Long.MIN_VALUE;
+    public int minIndex = -1, maxIndex = -1;
+
+    public StripeSizes(List<ImmutableList<StoreFile>> stripes) {
+      assert !stripes.isEmpty();
+      kvCounts = new ArrayList<Long>(stripes.size());
+      fileSizes = new ArrayList<Long>(stripes.size());
+      for (int i = 0; i < stripes.size(); ++i) {
+        long kvCount = getTotalKvCount(stripes.get(i));
+        fileSizes.add(getTotalFileSize(stripes.get(i)));
+        kvCounts.add(kvCount);
+        avgKvCount += (double)(kvCount - avgKvCount) / (i + 1);
+        if (minKvCount > kvCount) {
+          minIndex = i;
+          minKvCount = kvCount;
+        }
+        if (maxKvCount < kvCount) {
+          maxIndex = i;
+          maxKvCount = kvCount;
+        }
+      }
+    }
+  }
+
+  /** The information about stripes that the policy needs to do its stuff */
+  public static interface StripeInformationProvider {
+    public Collection<StoreFile> getStorefiles();
+
+    /**
+     * Gets the start row for a given stripe.
+     * @param stripeIndex Stripe index.
+     * @return Start row. May be an open key.
+     */
+    public byte[] getStartRow(int stripeIndex);
+
+    /**
+     * Gets the end row for a given stripe.
+     * @param stripeIndex Stripe index.
+     * @return End row. May be an open key.
+     */
+    public byte[] getEndRow(int stripeIndex);
+
+    /**
+     * @return Level 0 files.
+     */
+    public List<StoreFile> getLevel0Files();
+
+    /**
+     * @return All stripe boundaries; including the open ones on both ends.
+     */
+    public List<byte[]> getStripeBoundaries();
+
+    /**
+     * @return The stripes.
+     */
+    public ArrayList<ImmutableList<StoreFile>> getStripes();
+
+    /**
+     * @return Stripe count.
+     */
+    public int getStripeCount();
+  }
+}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java
new file mode 100644
index 0000000..11556e5
--- /dev/null
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.java
@@ -0,0 +1,156 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver.compactions;
+
+import java.io.IOException;
+import java.io.InterruptedIOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.compress.Compression;
+import org.apache.hadoop.hbase.regionserver.InternalScanner;
+import org.apache.hadoop.hbase.regionserver.ScanType;
+import org.apache.hadoop.hbase.regionserver.Store;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreFileScanner;
+import org.apache.hadoop.hbase.regionserver.StoreScanner;
+import org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFileManager;
+import org.apache.hadoop.hbase.regionserver.StoreFile.Writer;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * This is the placeholder for stripe compactor. The implementation,
+ * as well as the proper javadoc, will be added in HBASE-7967.
+ */
+public class StripeCompactor extends Compactor {
+  private static final Log LOG = LogFactory.getLog(StripeCompactor.class);
+  public StripeCompactor(Configuration conf, Store store) {
+    super(conf, store);
+  }
+
+  public List<Path> compact(CompactionRequest request, List<byte[]> targetBoundaries,
+      byte[] majorRangeFromRow, byte[] majorRangeToRow) throws IOException {
+    if (LOG.isDebugEnabled()) {
+      StringBuilder sb = new StringBuilder();
+      sb.append("Executing compaction with " + targetBoundaries.size() + " boundaries:");
+      for (byte[] tb : targetBoundaries) {
+        sb.append(" [").append(Bytes.toString(tb)).append("]");
+      }
+      LOG.debug(sb.toString());
+    }
+    StripeMultiFileWriter writer = new StripeMultiFileWriter.BoundaryMultiWriter(
+        targetBoundaries, majorRangeFromRow, majorRangeToRow);
+    return compactInternal(writer, request, majorRangeFromRow, majorRangeToRow);
+  }
+
+  public List<Path> compact(CompactionRequest request, int targetCount, long targetSize,
+      byte[] left, byte[] right, byte[] majorRangeFromRow, byte[] majorRangeToRow)
+      throws IOException {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Executing compaction with " + targetSize
+          + " target file size, no more than " + targetCount + " files, in ["
+          + Bytes.toString(left) + "] [" + Bytes.toString(right) + "] range");
+    }
+    StripeMultiFileWriter writer = new StripeMultiFileWriter.SizeMultiWriter(
+        targetCount, targetSize, left, right);
+    return compactInternal(writer, request, majorRangeFromRow, majorRangeToRow);
+  }
+
+  private List<Path> compactInternal(StripeMultiFileWriter mw, CompactionRequest request,
+      byte[] majorRangeFromRow, byte[] majorRangeToRow) throws IOException {
+    final Collection<StoreFile> filesToCompact = request.getFiles();
+    final FileDetails fd = getFileDetails(filesToCompact, request.isMajor());
+    this.progress = new CompactionProgress(fd.maxKeyCount);
+
+    long smallestReadPoint = getSmallestReadPoint();
+    List<StoreFileScanner> scanners = createFileScanners(filesToCompact, smallestReadPoint);
+
+    boolean finished = false;
+    InternalScanner scanner = null;
+    try {
+      // Get scanner to use.
+      ScanType coprocScanType = ScanType.COMPACT_RETAIN_DELETES;
+      scanner = preCreateCoprocScanner(request, coprocScanType, fd.earliestPutTs, scanners);
+      if (scanner == null) {
+        scanner = (majorRangeFromRow == null)
+            ? createScanner(store, scanners,
+                ScanType.COMPACT_RETAIN_DELETES, smallestReadPoint, fd.earliestPutTs)
+            : createScanner(store, scanners,
+                smallestReadPoint, fd.earliestPutTs, majorRangeFromRow, majorRangeToRow);
+      }
+      scanner = postCreateCoprocScanner(request, coprocScanType, scanner);
+      if (scanner == null) {
+        // NULL scanner returned from coprocessor hooks means skip normal processing.
+        return new ArrayList<Path>();
+      }
+
+      // Create the writer factory for compactions.
+      final boolean needMvcc = fd.maxMVCCReadpoint >= smallestReadPoint;
+      final Compression.Algorithm compression = store.getFamily().getCompactionCompression();
+      StripeMultiFileWriter.WriterFactory factory = new StripeMultiFileWriter.WriterFactory() {
+        @Override
+        public Writer createWriter() throws IOException {
+          return store.createWriterInTmp(
+              fd.maxKeyCount, compression, true, needMvcc, fd.maxTagsLength > 0);
+        }
+      };
+
+      // Prepare multi-writer, and perform the compaction using scanner and writer.
+      // It is ok here if storeScanner is null.
+      StoreScanner storeScanner = (scanner instanceof StoreScanner) ? (StoreScanner)scanner : null;
+      mw.init(storeScanner, factory, store.getComparator());
+      finished = performCompaction(scanner, mw, smallestReadPoint);
+      if (!finished) {
+        throw new InterruptedIOException( "Aborting compaction of store " + store +
+            " in region " + store.getRegionInfo().getRegionNameAsString() +
+            " because it was interrupted.");
+      }
+    } finally {
+      if (scanner != null) {
+        try {
+          scanner.close();
+        } catch (Throwable t) {
+          // Don't fail the compaction if this fails.
+          LOG.error("Failed to close scanner after compaction.", t);
+        }
+      }
+      if (!finished) {
+        for (Path leftoverFile : mw.abortWriters()) {
+          try {
+            store.getFileSystem().delete(leftoverFile, false);
+          } catch (Exception ex) {
+            LOG.error("Failed to delete the leftover file after an unfinished compaction.", ex);
+          }
+        }
+      }
+    }
+
+    assert finished : "We should have exited the method on all error paths";
+    List<Path> newFiles = mw.commitWriters(fd.maxSeqId, request.isMajor());
+    assert !newFiles.isEmpty() : "Should have produced an empty file to preserve metadata.";
+    return newFiles;
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java
new file mode 100644
index 0000000..d3a36d8
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/MockStoreFile.java
@@ -0,0 +1,100 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/** A mock used so our tests don't deal with actual StoreFiles */
+public class MockStoreFile extends StoreFile {
+  long length = 0;
+  boolean isRef = false;
+  long ageInDisk;
+  long sequenceid;
+  private Map<byte[], byte[]> metadata = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+  byte[] splitPoint = null;
+
+  MockStoreFile(HBaseTestingUtility testUtil, Path testPath,
+      long length, long ageInDisk, boolean isRef, long sequenceid) throws IOException {
+    super(testUtil.getTestFileSystem(), testPath, testUtil.getConfiguration(),
+          new CacheConfig(testUtil.getConfiguration()), BloomType.NONE,
+          NoOpDataBlockEncoder.INSTANCE);
+    this.length = length;
+    this.isRef = isRef;
+    this.ageInDisk = ageInDisk;
+    this.sequenceid = sequenceid;
+  }
+
+  void setLength(long newLen) {
+    this.length = newLen;
+  }
+
+  @Override
+  byte[] getFileSplitPoint(KVComparator comparator) throws IOException {
+    return this.splitPoint;
+  }
+
+  @Override
+  public long getMaxSequenceId() {
+    return sequenceid;
+  }
+
+  @Override
+  public boolean isMajorCompaction() {
+    return false;
+  }
+
+  @Override
+  public boolean isReference() {
+    return this.isRef;
+  }
+
+  @Override
+  boolean isBulkLoadResult() {
+    return false;
+  }
+
+  @Override
+  public byte[] getMetadataValue(byte[] key) {
+    return this.metadata.get(key);
+  }
+
+  public void setMetadataValue(byte[] key, byte[] value) {
+    this.metadata.put(key, value);
+  }
+
+  @Override
+  public StoreFile.Reader getReader() {
+    final long len = this.length;
+    return new StoreFile.Reader() {
+      @Override
+      public long length() {
+        return len;
+      }
+    };
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index eda1cfe..d1bf114 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -73,6 +73,7 @@ import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
 import org.apache.hadoop.hbase.regionserver.compactions.Compactor;
+import org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor;
 import org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -644,7 +645,7 @@ public class TestCompaction {
     HStore store = (HStore) r.getStore(COLUMN_FAMILY);
 
     Collection<StoreFile> storeFiles = store.getStorefiles();
-    Compactor tool = store.storeEngine.getCompactor();
+    DefaultCompactor tool = (DefaultCompactor)store.storeEngine.getCompactor();
 
     List<Path> newFiles = tool.compactForTesting(storeFiles, false);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
index 11d7c10..1a09ad1 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestDefaultCompactSelection.java
@@ -125,64 +125,6 @@ public class TestDefaultCompactSelection extends TestCase {
     }
   }
 
-  // used so our tests don't deal with actual StoreFiles
-  static class MockStoreFile extends StoreFile {
-    long length = 0;
-    boolean isRef = false;
-    long ageInDisk;
-    long sequenceid;
-
-    MockStoreFile(long length, long ageInDisk, boolean isRef, long sequenceid) throws IOException {
-      super(TEST_UTIL.getTestFileSystem(), TEST_FILE, TEST_UTIL.getConfiguration(),
-            new CacheConfig(TEST_UTIL.getConfiguration()), BloomType.NONE,
-            NoOpDataBlockEncoder.INSTANCE);
-      this.length = length;
-      this.isRef = isRef;
-      this.ageInDisk = ageInDisk;
-      this.sequenceid = sequenceid;
-    }
-
-    void setLength(long newLen) {
-      this.length = newLen;
-    }
-
-    @Override
-    public long getMaxSequenceId() {
-      return sequenceid;
-    }
-
-    @Override
-    public boolean isMajorCompaction() {
-      return false;
-    }
-
-    @Override
-    public boolean isReference() {
-      return this.isRef;
-    }
-
-    @Override
-    public StoreFile.Reader getReader() {
-      final long len = this.length;
-      return new StoreFile.Reader() {
-        @Override
-        public long length() {
-          return len;
-        }
-      };
-    }
-
-    @Override
-    public String toString() {
-      return "MockStoreFile{" +
-          "length=" + length +
-          ", isRef=" + isRef +
-          ", ageInDisk=" + ageInDisk +
-          ", sequenceid=" + sequenceid +
-          '}';
-    }
-  }
-
   ArrayList<Long> toArrayList(long... numbers) {
     ArrayList<Long> result = new ArrayList<Long>();
     for (long i : numbers) {
@@ -216,7 +158,8 @@ public class TestDefaultCompactSelection extends TestCase {
       throws IOException {
     List<StoreFile> ret = Lists.newArrayList();
     for (int i = 0; i < sizes.size(); i++) {
-      ret.add(new MockStoreFile(sizes.get(i), ageInDisk.get(i), isReference, i));
+      ret.add(new MockStoreFile(TEST_UTIL, TEST_FILE,
+          sizes.get(i), ageInDisk.get(i), isReference, i));
     }
     return ret;
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java
new file mode 100644
index 0000000..43b8254
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeCompactor.java
@@ -0,0 +1,311 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import static org.junit.Assert.*;
+import static org.mockito.Mockito.*;
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.STRIPE_START_KEY;
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.STRIPE_END_KEY;
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.ListIterator;
+import java.util.TreeMap;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.io.compress.Compression;
+import org.apache.hadoop.hbase.io.hfile.HFile;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+
+@Category(SmallTests.class)
+public class TestStripeCompactor {
+  private static final byte[] NAME_OF_THINGS = Bytes.toBytes("foo");
+  private static final TableName TABLE_NAME = TableName.valueOf(NAME_OF_THINGS, NAME_OF_THINGS);
+
+  private static final byte[] KEY_B = Bytes.toBytes("bbb");
+  private static final byte[] KEY_C = Bytes.toBytes("ccc");
+  private static final byte[] KEY_D = Bytes.toBytes("ddd");
+
+  private static final KeyValue KV_A = kvAfter(Bytes.toBytes("aaa"));
+  private static final KeyValue KV_B = kvAfter(KEY_B);
+  private static final KeyValue KV_C = kvAfter(KEY_C);
+  private static final KeyValue KV_D = kvAfter(KEY_D);
+
+  private static KeyValue kvAfter(byte[] key) {
+    return new KeyValue(Arrays.copyOf(key, key.length + 1), 0L);
+  }
+
+  private static <T> T[] a(T... a) {
+    return a;
+  }
+
+  private static KeyValue[] e() {
+    return TestStripeCompactor.<KeyValue>a();
+  }
+
+  @Test
+  public void testBoundaryCompactions() throws Exception {
+    // General verification
+    verifyBoundaryCompaction(a(KV_A, KV_A, KV_B, KV_B, KV_C, KV_D),
+        a(OPEN_KEY, KEY_B, KEY_D, OPEN_KEY), a(a(KV_A, KV_A), a(KV_B, KV_B, KV_C), a(KV_D)));
+    verifyBoundaryCompaction(a(KV_B, KV_C), a(KEY_B, KEY_C, KEY_D), a(a(KV_B), a(KV_C)));
+    verifyBoundaryCompaction(a(KV_B, KV_C), a(KEY_B, KEY_D), new KeyValue[][] { a(KV_B, KV_C) });
+  }
+
+  @Test
+  public void testBoundaryCompactionEmptyFiles() throws Exception {
+    // No empty file if there're already files.
+    verifyBoundaryCompaction(
+        a(KV_B), a(KEY_B, KEY_C, KEY_D, OPEN_KEY), a(a(KV_B), null, null), null, null, false);
+    verifyBoundaryCompaction(a(KV_A, KV_C),
+        a(OPEN_KEY, KEY_B, KEY_C, KEY_D), a(a(KV_A), null, a(KV_C)), null, null, false);
+    // But should be created if there are no file.
+    verifyBoundaryCompaction(
+        e(), a(OPEN_KEY, KEY_B, KEY_C, OPEN_KEY), a(null, null, e()), null, null, false);
+    // In major range if there's major range.
+    verifyBoundaryCompaction(
+        e(), a(OPEN_KEY, KEY_B, KEY_C, OPEN_KEY), a(null, e(), null), KEY_B, KEY_C, false);
+    verifyBoundaryCompaction(
+        e(), a(OPEN_KEY, KEY_B, KEY_C, OPEN_KEY), a(e(), e(), null), OPEN_KEY, KEY_C, false);
+    // Major range should have files regardless of KVs.
+    verifyBoundaryCompaction(a(KV_A), a(OPEN_KEY, KEY_B, KEY_C, KEY_D, OPEN_KEY),
+        a(a(KV_A), e(), e(), null), KEY_B, KEY_D, false);
+    verifyBoundaryCompaction(a(KV_C), a(OPEN_KEY, KEY_B, KEY_C, KEY_D, OPEN_KEY),
+        a(null, null, a(KV_C), e()), KEY_C, OPEN_KEY, false);
+
+  }
+
+  public static void verifyBoundaryCompaction(
+      KeyValue[] input, byte[][] boundaries, KeyValue[][] output) throws Exception {
+    verifyBoundaryCompaction(input, boundaries, output, null, null, true);
+  }
+
+  public static void verifyBoundaryCompaction(KeyValue[] input, byte[][] boundaries,
+      KeyValue[][] output, byte[] majorFrom, byte[] majorTo, boolean allFiles)
+          throws Exception {
+    StoreFileWritersCapture writers = new StoreFileWritersCapture();
+    StripeCompactor sc = createCompactor(writers, input);
+    List<Path> paths =
+        sc.compact(createDummyRequest(), Arrays.asList(boundaries), majorFrom, majorTo);
+    writers.verifyKvs(output, allFiles, true);
+    if (allFiles) {
+      assertEquals(output.length, paths.size());
+      writers.verifyBoundaries(boundaries);
+    }
+  }
+
+  @Test
+  public void testSizeCompactions() throws Exception {
+    // General verification with different sizes.
+    verifySizeCompaction(a(KV_A, KV_A, KV_B, KV_C, KV_D), 3, 2, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A, KV_A), a(KV_B, KV_C), a(KV_D)));
+    verifySizeCompaction(a(KV_A, KV_B, KV_C, KV_D), 4, 1, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A), a(KV_B), a(KV_C), a(KV_D)));
+    verifySizeCompaction(a(KV_B, KV_C), 2, 1, KEY_B, KEY_D, a(a(KV_B), a(KV_C)));
+    // Verify row boundaries are preserved.
+    verifySizeCompaction(a(KV_A, KV_A, KV_A, KV_C, KV_D), 3, 2, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A, KV_A, KV_A), a(KV_C, KV_D)));
+    verifySizeCompaction(a(KV_A, KV_B, KV_B, KV_C), 3, 1, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A), a(KV_B, KV_B), a(KV_C)));
+    // Too much data, count limits the number of files.
+    verifySizeCompaction(a(KV_A, KV_B, KV_C, KV_D), 2, 1, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A), a(KV_B, KV_C, KV_D)));
+    verifySizeCompaction(a(KV_A, KV_B, KV_C), 1, Long.MAX_VALUE, OPEN_KEY, KEY_D,
+        new KeyValue[][] { a(KV_A, KV_B, KV_C) });
+    // Too little data/large count, no extra files.
+    verifySizeCompaction(a(KV_A, KV_B, KV_C, KV_D), Integer.MAX_VALUE, 2, OPEN_KEY, OPEN_KEY,
+        a(a(KV_A, KV_B), a(KV_C, KV_D)));
+  }
+
+  public static void verifySizeCompaction(KeyValue[] input, int targetCount, long targetSize,
+      byte[] left, byte[] right, KeyValue[][] output) throws Exception {
+    StoreFileWritersCapture writers = new StoreFileWritersCapture();
+    StripeCompactor sc = createCompactor(writers, input);
+    List<Path> paths = sc.compact(
+        createDummyRequest(), targetCount, targetSize, left, right, null, null);
+    assertEquals(output.length, paths.size());
+    writers.verifyKvs(output, true, true);
+    List<byte[]> boundaries = new ArrayList<byte[]>();
+    boundaries.add(left);
+    for (int i = 1; i < output.length; ++i) {
+      boundaries.add(output[i][0].getRow());
+    }
+    boundaries.add(right);
+    writers.verifyBoundaries(boundaries.toArray(new byte[][] {}));
+  }
+
+  private static StripeCompactor createCompactor(
+      StoreFileWritersCapture writers, KeyValue[] input) throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    final Scanner scanner = new Scanner(input);
+
+    // Create store mock that is satisfactory for compactor.
+    HColumnDescriptor col = new HColumnDescriptor(NAME_OF_THINGS);
+    ScanInfo si = new ScanInfo(col, Long.MAX_VALUE, 0, new KVComparator());
+    Store store = mock(Store.class);
+    when(store.getFamily()).thenReturn(col);
+    when(store.getScanInfo()).thenReturn(si);
+    when(store.areWritesEnabled()).thenReturn(true);
+    when(store.getFileSystem()).thenReturn(mock(FileSystem.class));
+    when(store.getRegionInfo()).thenReturn(new HRegionInfo(TABLE_NAME));
+    when(store.createWriterInTmp(anyLong(), any(Compression.Algorithm.class),
+        anyBoolean(), anyBoolean(), anyBoolean())).thenAnswer(writers);
+    when(store.getComparator()).thenReturn(new KVComparator());
+
+    return new StripeCompactor(conf, store) {
+      @Override
+      protected InternalScanner createScanner(Store store, List<StoreFileScanner> scanners,
+          long smallestReadPoint, long earliestPutTs, byte[] dropDeletesFromRow,
+          byte[] dropDeletesToRow) throws IOException {
+        return scanner;
+      }
+
+      @Override
+      protected InternalScanner createScanner(Store store, List<StoreFileScanner> scanners,
+          ScanType scanType, long smallestReadPoint, long earliestPutTs) throws IOException {
+        return scanner;
+      }
+    };
+  }
+
+  private static CompactionRequest createDummyRequest() throws Exception {
+    // "Files" are totally unused, it's Scanner class below that gives compactor fake KVs.
+    // But compaction depends on everything under the sun, so stub everything with dummies.
+    StoreFile sf = mock(StoreFile.class);
+    StoreFile.Reader r = mock(StoreFile.Reader.class);
+    when(r.length()).thenReturn(1L);
+    when(r.getBloomFilterType()).thenReturn(BloomType.NONE);
+    when(r.getHFileReader()).thenReturn(mock(HFile.Reader.class));
+    when(r.getStoreFileScanner(anyBoolean(), anyBoolean(), anyBoolean(), anyLong()))
+      .thenReturn(mock(StoreFileScanner.class));
+    when(sf.getReader()).thenReturn(r);
+    when(sf.createReader()).thenReturn(r);
+    return new CompactionRequest(Arrays.asList(sf));
+  }
+
+  private static class Scanner implements InternalScanner {
+    private final ArrayList<KeyValue> kvs;
+    public Scanner(KeyValue... kvs) {
+      this.kvs = new ArrayList<KeyValue>(Arrays.asList(kvs));
+    }
+
+    @Override
+    public boolean next(List<Cell> results) throws IOException {
+      if (kvs.isEmpty()) return false;
+      results.add(kvs.remove(0));
+      return !kvs.isEmpty();
+    }
+    @Override
+    public boolean next(List<Cell> result, int limit) throws IOException {
+      return next(result);
+    }
+    @Override
+    public void close() throws IOException {}
+  }
+
+  // StoreFile.Writer has private ctor and is unwieldy, so this has to be convoluted.
+  public static class StoreFileWritersCapture implements
+    Answer<StoreFile.Writer>, StripeMultiFileWriter.WriterFactory {
+    public static class Writer {
+      public ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
+      public TreeMap<byte[], byte[]> data = new TreeMap<byte[], byte[]>(Bytes.BYTES_COMPARATOR);
+    }
+
+    private List<Writer> writers = new ArrayList<Writer>();
+
+    @Override
+    public StoreFile.Writer createWriter() throws IOException {
+      final Writer realWriter = new Writer();
+      writers.add(realWriter);
+      StoreFile.Writer writer = mock(StoreFile.Writer.class);
+      doAnswer(new Answer<Object>() {
+        public Object answer(InvocationOnMock invocation) {
+          return realWriter.kvs.add((KeyValue)invocation.getArguments()[0]);
+        }}).when(writer).append(any(KeyValue.class));
+      doAnswer(new Answer<Object>() {
+        public Object answer(InvocationOnMock invocation) {
+          Object[] args = invocation.getArguments();
+          return realWriter.data.put((byte[])args[0], (byte[])args[1]);
+        }}).when(writer).appendFileInfo(any(byte[].class), any(byte[].class));
+      return writer;
+    }
+
+    @Override
+    public StoreFile.Writer answer(InvocationOnMock invocation) throws Throwable {
+      return createWriter();
+    }
+
+    public void verifyKvs(KeyValue[][] kvss, boolean allFiles, boolean requireMetadata) {
+      if (allFiles) {
+        assertEquals(kvss.length, writers.size());
+      }
+      int skippedWriters = 0;
+      for (int i = 0; i < kvss.length; ++i) {
+        KeyValue[] kvs = kvss[i];
+        if (kvs != null) {
+          Writer w = writers.get(i - skippedWriters);
+          if (requireMetadata) {
+            assertNotNull(w.data.get(STRIPE_START_KEY));
+            assertNotNull(w.data.get(STRIPE_END_KEY));
+          } else {
+            assertNull(w.data.get(STRIPE_START_KEY));
+            assertNull(w.data.get(STRIPE_END_KEY));
+          }
+          assertEquals(kvs.length, w.kvs.size());
+          for (int j = 0; j < kvs.length; ++j) {
+            assertEquals(kvs[j], w.kvs.get(j));
+          }
+        } else {
+          assertFalse(allFiles);
+          ++skippedWriters;
+        }
+      }
+    }
+
+    public void verifyBoundaries(byte[][] boundaries) {
+      assertEquals(boundaries.length - 1, writers.size());
+      for (int i = 0; i < writers.size(); ++i) {
+        assertArrayEquals("i = " + i, boundaries[i], writers.get(i).data.get(STRIPE_START_KEY));
+        assertArrayEquals("i = " + i, boundaries[i + 1], writers.get(i).data.get(STRIPE_END_KEY));
+      }
+    }
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java
new file mode 100644
index 0000000..2be4bf6
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreEngine.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import static org.junit.Assert.*;
+import static org.mockito.Mockito.*;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(SmallTests.class)
+public class TestStripeStoreEngine {
+
+  @Test
+  public void testCreateBasedOnConfig() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY, TestStoreEngine.class.getName());
+    StripeStoreEngine se = createEngine(conf);
+    assertTrue(se.getCompactionPolicy() instanceof StripeCompactionPolicy);
+  }
+
+  public static class TestStoreEngine extends StripeStoreEngine {
+    public void setCompactorOverride(StripeCompactor compactorOverride) {
+      this.compactor = compactorOverride;
+    }
+  }
+
+  @Test
+  public void testCompactionContextForceSelect() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    int targetCount = 2;
+    conf.setInt(StripeStoreConfig.INITIAL_STRIPE_COUNT_KEY, targetCount);
+    conf.setInt(StripeStoreConfig.MIN_FILES_L0_KEY, 2);
+    conf.set(StoreEngine.STORE_ENGINE_CLASS_KEY, TestStoreEngine.class.getName());
+    TestStoreEngine se = createEngine(conf);
+    StripeCompactor mockCompactor = mock(StripeCompactor.class);
+    se.setCompactorOverride(mockCompactor);
+    when(mockCompactor.compact(any(CompactionRequest.class), anyInt(), anyLong(),
+        any(byte[].class), any(byte[].class), any(byte[].class), any(byte[].class)))
+        .thenReturn(new ArrayList<Path>());
+
+    // Produce 3 L0 files.
+    StoreFile sf = createFile();
+    ArrayList<StoreFile> compactUs = al(sf, createFile(), createFile());
+    se.getStoreFileManager().loadFiles(compactUs);
+    // Create a compaction that would want to split the stripe.
+    CompactionContext compaction = se.createCompaction();
+    compaction.select(al(), false, false, false);
+    assertEquals(3, compaction.getRequest().getFiles().size());
+    // Override the file list. Granted, overriding this compaction in this manner will
+    // break things in real world, but we only want to verify the override.
+    compactUs.remove(sf);
+    CompactionRequest req = new CompactionRequest(compactUs);
+    compaction.forceSelect(req);
+    assertEquals(2, compaction.getRequest().getFiles().size());
+    assertFalse(compaction.getRequest().getFiles().contains(sf));
+    // Make sure the correct method it called on compactor.
+    compaction.compact();
+    verify(mockCompactor, times(1)).compact(compaction.getRequest(), targetCount, 0L,
+          StripeStoreFileManager.OPEN_KEY, StripeStoreFileManager.OPEN_KEY, null, null);
+  }
+
+  private static StoreFile createFile() throws Exception {
+    StoreFile sf = mock(StoreFile.class);
+    when(sf.getMetadataValue(any(byte[].class)))
+      .thenReturn(StripeStoreFileManager.INVALID_KEY);
+    when(sf.getReader()).thenReturn(mock(StoreFile.Reader.class));
+    when(sf.getPath()).thenReturn(new Path("moo"));
+    return sf;
+  }
+
+  private static TestStoreEngine createEngine(Configuration conf) throws Exception {
+    Store store = mock(Store.class);
+    KVComparator kvComparator = mock(KVComparator.class);
+    return (TestStoreEngine)StoreEngine.create(store, conf, kvComparator);
+  }
+
+  private static ArrayList<StoreFile> al(StoreFile... sfs) {
+    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
new file mode 100644
index 0000000..e28d0d1
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestStripeStoreFileManager.java
@@ -0,0 +1,605 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+
+import static org.junit.Assert.*;
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KVComparator;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+
+@Category(SmallTests.class)
+public class TestStripeStoreFileManager {
+  private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static final Path BASEDIR =
+      TEST_UTIL.getDataTestDir(TestStripeStoreFileManager.class.getSimpleName());
+  private static final Path CFDIR = HStore.getStoreHomedir(BASEDIR, "region", Bytes.toBytes("cf"));
+
+  private static final byte[] KEY_A = Bytes.toBytes("aaa");
+  private static final byte[] KEY_B = Bytes.toBytes("bbb");
+  private static final byte[] KEY_C = Bytes.toBytes("ccc");
+  private static final byte[] KEY_D = Bytes.toBytes("ddd");
+
+  private static final KeyValue KV_A = new KeyValue(KEY_A, 0L);
+  private static final KeyValue KV_B = new KeyValue(KEY_B, 0L);
+  private static final KeyValue KV_C = new KeyValue(KEY_C, 0L);
+  private static final KeyValue KV_D = new KeyValue(KEY_D, 0L);
+
+  @Before
+  public void setUp() throws Exception {
+    FileSystem fs = TEST_UTIL.getTestFileSystem();
+    if (!fs.mkdirs(CFDIR)) {
+      throw new IOException("Cannot create test directory " + CFDIR);
+    }
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    FileSystem fs = TEST_UTIL.getTestFileSystem();
+    if (fs.exists(CFDIR) && !fs.delete(CFDIR, true)) {
+      throw new IOException("Cannot delete test directory " + CFDIR);
+    }
+  }
+
+  @Test
+  public void testInsertFilesIntoL0() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    MockStoreFile sf = createFile();
+    manager.insertNewFiles(al(sf));
+    assertEquals(1, manager.getStorefileCount());
+    Collection<StoreFile> filesForGet = manager.getFilesForScanOrGet(true, KEY_A, KEY_A);
+    assertEquals(1, filesForGet.size());
+    assertTrue(filesForGet.contains(sf));
+
+    // Add some stripes and make sure we get this file for every stripe.
+    manager.addCompactionResults(al(), al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, OPEN_KEY)));
+    assertTrue(manager.getFilesForScanOrGet(true, KEY_A, KEY_A).contains(sf));
+    assertTrue(manager.getFilesForScanOrGet(true, KEY_C, KEY_C).contains(sf));
+  }
+
+  @Test
+  public void testClearFiles() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    manager.insertNewFiles(al(createFile()));
+    manager.insertNewFiles(al(createFile()));
+    manager.addCompactionResults(al(), al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, OPEN_KEY)));
+    assertEquals(4, manager.getStorefileCount());
+    Collection<StoreFile> allFiles = manager.clearFiles();
+    assertEquals(4, allFiles.size());
+    assertEquals(0, manager.getStorefileCount());
+    assertEquals(0, manager.getStorefiles().size());
+  }
+
+  private static ArrayList<StoreFile> dumpIterator(Iterator<StoreFile> iter) {
+    ArrayList<StoreFile> result = new ArrayList<StoreFile>();
+    for (; iter.hasNext(); result.add(iter.next()));
+    return result;
+  }
+
+  @Test
+  public void testRowKeyBefore() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    StoreFile l0File = createFile(), l0File2 = createFile();
+    manager.insertNewFiles(al(l0File));
+    manager.insertNewFiles(al(l0File2));
+    // Get candidate files.
+    Iterator<StoreFile> sfs = manager.getCandidateFilesForRowKeyBefore(KV_B);
+    sfs.next();
+    sfs.remove();
+    // Suppose we found a candidate in this file... make sure L0 file remaining is not removed.
+    sfs = manager.updateCandidateFilesForRowKeyBefore(sfs, KV_B, KV_A);
+    assertTrue(sfs.hasNext());
+    // Now add some stripes (remove L0 file too)
+    MockStoreFile stripe0a = createFile(0, 100, OPEN_KEY, KEY_B),
+        stripe1 = createFile(KEY_B, OPEN_KEY);
+    manager.addCompactionResults(al(l0File), al(stripe0a, stripe1));
+    // If we want a key <= KEY_A, we should get everything except stripe1.
+    ArrayList<StoreFile> sfsDump = dumpIterator(manager.getCandidateFilesForRowKeyBefore(KV_A));
+    assertEquals(2, sfsDump.size());
+    assertTrue(sfsDump.contains(stripe0a));
+    assertFalse(sfsDump.contains(stripe1));
+    // If we want a key <= KEY_B, we should get everything since lower bound is inclusive.
+    sfsDump = dumpIterator(manager.getCandidateFilesForRowKeyBefore(KV_B));
+    assertEquals(3, sfsDump.size());
+    assertTrue(sfsDump.contains(stripe1));
+    // For KEY_D, we should also get everything.
+    sfsDump = dumpIterator(manager.getCandidateFilesForRowKeyBefore(KV_D));
+    assertEquals(3, sfsDump.size());
+    // Suppose in the first file we found candidate with KEY_C.
+    // Then, stripe0 no longer matters and should be removed, but stripe1 should stay.
+    sfs = manager.getCandidateFilesForRowKeyBefore(KV_D);
+    sfs.next(); // Skip L0 file.
+    sfs.remove();
+    sfs = manager.updateCandidateFilesForRowKeyBefore(sfs, KV_D, KV_C);
+    assertEquals(stripe1, sfs.next());
+    assertFalse(sfs.hasNext());
+    // Add one more, later, file to stripe0, remove the last annoying L0 file.
+    // This file should be returned in preference to older L0 file; also, after we get
+    // a candidate from the first file, the old one should not be removed.
+    StoreFile stripe0b = createFile(0, 101, OPEN_KEY, KEY_B);
+    manager.addCompactionResults(al(l0File2), al(stripe0b));
+    sfs = manager.getCandidateFilesForRowKeyBefore(KV_A);
+    assertEquals(stripe0b, sfs.next());
+    sfs.remove();
+    sfs = manager.updateCandidateFilesForRowKeyBefore(sfs, KV_A, KV_A);
+    assertEquals(stripe0a, sfs.next());
+  }
+
+  @Test
+  public void testGetSplitPointEdgeCases() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    // No files => no split.
+    assertNull(manager.getSplitPoint());
+
+    // If there are no stripes, should pick midpoint from the biggest file in L0.
+    MockStoreFile sf5 = createFile(5, 0);
+    sf5.splitPoint = new byte[1];
+    manager.insertNewFiles(al(sf5));
+    manager.insertNewFiles(al(createFile(1, 0)));
+    assertEquals(sf5.splitPoint, manager.getSplitPoint());
+
+    // Same if there's one stripe but the biggest file is still in L0.
+    manager.addCompactionResults(al(), al(createFile(2, 0, OPEN_KEY, OPEN_KEY)));
+    assertEquals(sf5.splitPoint, manager.getSplitPoint());
+
+    // If the biggest file is in the stripe, should get from it.
+    MockStoreFile sf6 = createFile(6, 0, OPEN_KEY, OPEN_KEY);
+    sf6.splitPoint = new byte[1];
+    manager.addCompactionResults(al(), al(sf6));
+    assertEquals(sf6.splitPoint, manager.getSplitPoint());
+  }
+
+  @Test
+  public void testGetStripeBoundarySplits() throws Exception {
+    /* First number - split must be after this stripe; further numbers - stripes */
+    verifySplitPointScenario(5, false, 0f,     2, 1, 1, 1, 1, 1, 10);
+    verifySplitPointScenario(0, false, 0f,     6, 3, 1, 1, 2);
+    verifySplitPointScenario(2, false, 0f,     1, 1, 1, 1, 2);
+    verifySplitPointScenario(0, false, 0f,     5, 4);
+    verifySplitPointScenario(2, false, 0f,     5, 2, 5, 5, 5);
+  }
+
+  @Test
+  public void testGetUnbalancedSplits() throws Exception {
+    /* First number - split must be inside/after this stripe; further numbers - stripes */
+    verifySplitPointScenario(0, false, 2.1f,      4, 4, 4); // 8/4 is less than 2.1f
+    verifySplitPointScenario(1, true,  1.5f,      4, 4, 4); // 8/4 > 6/6
+    verifySplitPointScenario(1, false, 1.1f,      3, 4, 1, 1, 2, 2); // 7/6 < 8/5
+    verifySplitPointScenario(1, false, 1.1f,      3, 6, 1, 1, 2, 2); // 9/6 == 9/6
+    verifySplitPointScenario(1, true,  1.1f,      3, 8, 1, 1, 2, 2); // 11/6 > 10/7
+    verifySplitPointScenario(3, false, 1.1f,      2, 2, 1, 1, 4, 3); // reverse order
+    verifySplitPointScenario(4, true,  1.1f,      2, 2, 1, 1, 8, 3); // reverse order
+    verifySplitPointScenario(0, true,  1.5f,      10, 4); // 10/4 > 9/5
+    verifySplitPointScenario(0, false, 1.4f,      6, 4);  // 6/4 == 6/4
+    verifySplitPointScenario(1, true,  1.5f,      4, 10); // reverse just in case
+    verifySplitPointScenario(0, false, 1.4f,      4, 6);  // reverse just in case
+  }
+
+
+  /**
+   * Verifies scenario for finding a split point.
+   * @param splitPointAfter Stripe to expect the split point at/after.
+   * @param shouldSplitStripe If true, the split point is expected in the middle of the above
+   *                          stripe; if false, should be at the end.
+   * @param splitRatioToVerify Maximum split imbalance ratio.
+   * @param sizes Stripe sizes.
+   */
+  private void verifySplitPointScenario(int splitPointAfter, boolean shouldSplitStripe,
+      float splitRatioToVerify, int... sizes) throws Exception {
+    assertTrue(sizes.length > 1);
+    ArrayList<StoreFile> sfs = new ArrayList<StoreFile>();
+    for (int sizeIx = 0; sizeIx < sizes.length; ++sizeIx) {
+      byte[] startKey = (sizeIx == 0) ? OPEN_KEY : Bytes.toBytes(sizeIx - 1);
+      byte[] endKey = (sizeIx == sizes.length - 1) ? OPEN_KEY : Bytes.toBytes(sizeIx);
+      MockStoreFile sf = createFile(sizes[sizeIx], 0, startKey, endKey);
+      sf.splitPoint = Bytes.toBytes(-sizeIx); // set split point to the negative index
+      sfs.add(sf);
+    }
+
+    Configuration conf = HBaseConfiguration.create();
+    if (splitRatioToVerify != 0) {
+      conf.setFloat(StripeStoreConfig.MAX_REGION_SPLIT_IMBALANCE_KEY, splitRatioToVerify);
+    }
+    StripeStoreFileManager manager = createManager(al(), conf);
+    manager.addCompactionResults(al(), sfs);
+    int result = Bytes.toInt(manager.getSplitPoint());
+    // Either end key and thus positive index, or "middle" of the file and thus negative index.
+    assertEquals(splitPointAfter * (shouldSplitStripe ? -1 : 1), result);
+  }
+
+  private static byte[] keyAfter(byte[] key) {
+    return Arrays.copyOf(key, key.length + 1);
+  }
+
+  @Test
+  public void testGetFilesForGetAndScan() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    verifyGetAndScanScenario(manager, null, null);
+    verifyGetAndScanScenario(manager, KEY_B, KEY_C);
+
+    // Populate one L0 file.
+    MockStoreFile sf0 = createFile();
+    manager.insertNewFiles(al(sf0));
+    verifyGetAndScanScenario(manager, null, null,   sf0);
+    verifyGetAndScanScenario(manager, null, KEY_C,  sf0);
+    verifyGetAndScanScenario(manager, KEY_B, null,  sf0);
+    verifyGetAndScanScenario(manager, KEY_B, KEY_C, sf0);
+
+    // Populate a bunch of files for stripes, keep L0.
+    MockStoreFile sfA = createFile(OPEN_KEY, KEY_A);
+    MockStoreFile sfB = createFile(KEY_A, KEY_B);
+    MockStoreFile sfC = createFile(KEY_B, KEY_C);
+    MockStoreFile sfD = createFile(KEY_C, KEY_D);
+    MockStoreFile sfE = createFile(KEY_D, OPEN_KEY);
+    manager.addCompactionResults(al(), al(sfA, sfB, sfC, sfD, sfE));
+
+    verifyGetAndScanScenario(manager, null, null,              sf0, sfA, sfB, sfC, sfD, sfE);
+    verifyGetAndScanScenario(manager, keyAfter(KEY_A), null,   sf0, sfB, sfC, sfD, sfE);
+    verifyGetAndScanScenario(manager, null, keyAfter(KEY_C),   sf0, sfA, sfB, sfC, sfD);
+    verifyGetAndScanScenario(manager, KEY_B, null,             sf0, sfC, sfD, sfE);
+    verifyGetAndScanScenario(manager, null, KEY_C,             sf0, sfA, sfB, sfC, sfD);
+    verifyGetAndScanScenario(manager, KEY_B, keyAfter(KEY_B),  sf0, sfC);
+    verifyGetAndScanScenario(manager, keyAfter(KEY_A), KEY_B,  sf0, sfB, sfC);
+    verifyGetAndScanScenario(manager, KEY_D, KEY_D,            sf0, sfE);
+    verifyGetAndScanScenario(manager, keyAfter(KEY_B), keyAfter(KEY_C), sf0, sfC, sfD);
+  }
+
+  private void verifyGetAndScanScenario(StripeStoreFileManager manager,
+      byte[] start, byte[] end, StoreFile... results) throws Exception {
+    verifyGetOrScanScenario(manager, true, start, end, results);
+    verifyGetOrScanScenario(manager, false, start, end, results);
+  }
+
+  @Test
+  @SuppressWarnings("unchecked")
+  public void testLoadFilesWithRecoverableBadFiles() throws Exception {
+    // In L0, there will be file w/o metadata (real L0, 3 files with invalid metadata, and 3
+    // files that overlap valid stripes in various ways). Note that the 4th way to overlap the
+    // stripes will cause the structure to be mostly scraped, and is tested separately.
+    ArrayList<StoreFile> validStripeFiles = al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, KEY_C), createFile(KEY_C, OPEN_KEY),
+        createFile(KEY_C, OPEN_KEY));
+    ArrayList<StoreFile> filesToGoToL0 = al(createFile(), createFile(null, KEY_A),
+        createFile(KEY_D, null), createFile(KEY_D, KEY_A), createFile(keyAfter(KEY_A), KEY_C),
+        createFile(OPEN_KEY, KEY_D), createFile(KEY_D, keyAfter(KEY_D)));
+    ArrayList<StoreFile> allFilesToGo = flattenLists(validStripeFiles, filesToGoToL0);
+    Collections.shuffle(allFilesToGo);
+    StripeStoreFileManager manager = createManager(allFilesToGo);
+    List<StoreFile> l0Files = manager.getLevel0Files();
+    assertEquals(filesToGoToL0.size(), l0Files.size());
+    for (StoreFile sf : filesToGoToL0) {
+      assertTrue(l0Files.contains(sf));
+    }
+    verifyAllFiles(manager, allFilesToGo);
+  }
+
+  @Test
+  public void testLoadFilesWithBadStripe() throws Exception {
+    // Current "algorithm" will see the after-B key before C key, add it as valid stripe,
+    // and then fail all other stripes. So everything would end up in L0.
+    ArrayList<StoreFile> allFilesToGo = al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, KEY_C), createFile(KEY_C, OPEN_KEY),
+        createFile(KEY_B, keyAfter(KEY_B)));
+    Collections.shuffle(allFilesToGo);
+    StripeStoreFileManager manager = createManager(allFilesToGo);
+    assertEquals(allFilesToGo.size(), manager.getLevel0Files().size());
+  }
+
+  @Test
+  public void testLoadFilesWithGaps() throws Exception {
+    // Stripes must not have gaps. If they do, everything goes to L0.
+    StripeStoreFileManager manager =
+      createManager(al(createFile(OPEN_KEY, KEY_B), createFile(KEY_C, OPEN_KEY)));
+    assertEquals(2, manager.getLevel0Files().size());
+    // Just one open stripe should be ok.
+    manager = createManager(al(createFile(OPEN_KEY, OPEN_KEY)));
+    assertEquals(0, manager.getLevel0Files().size());
+    assertEquals(1, manager.getStorefileCount());
+  }
+
+  @Test
+  public void testLoadFilesAfterSplit() throws Exception {
+    // If stripes are good but have non-open ends, they must be treated as open ends.
+    MockStoreFile sf = createFile(KEY_B, KEY_C);
+    StripeStoreFileManager manager = createManager(al(createFile(OPEN_KEY, KEY_B), sf));
+    assertEquals(0, manager.getLevel0Files().size());
+    // Here, [B, C] is logically [B, inf), so we should be able to compact it to that only.
+    verifyInvalidCompactionScenario(manager, al(sf), al(createFile(KEY_B, KEY_C)));
+    manager.addCompactionResults(al(sf), al(createFile(KEY_B, OPEN_KEY)));
+    // Do the same for other variants.
+    manager = createManager(al(sf, createFile(KEY_C, OPEN_KEY)));
+    verifyInvalidCompactionScenario(manager, al(sf), al(createFile(KEY_B, KEY_C)));
+    manager.addCompactionResults(al(sf), al(createFile(OPEN_KEY, KEY_C)));
+    manager = createManager(al(sf));
+    verifyInvalidCompactionScenario(manager, al(sf), al(createFile(KEY_B, KEY_C)));
+    manager.addCompactionResults(al(sf), al(createFile(OPEN_KEY, OPEN_KEY)));
+  }
+
+  @Test
+  public void testAddingCompactionResults() throws Exception {
+    StripeStoreFileManager manager = createManager();
+    // First, add some L0 files and "compact" one with new stripe creation.
+    StoreFile sf_L0_0a = createFile(), sf_L0_0b = createFile();
+    manager.insertNewFiles(al(sf_L0_0a, sf_L0_0b));
+
+    // Try compacting with invalid new branches (gaps, overlaps) - no effect.
+    verifyInvalidCompactionScenario(manager, al(sf_L0_0a), al(createFile(OPEN_KEY, KEY_B)));
+    verifyInvalidCompactionScenario(manager, al(sf_L0_0a), al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_C, OPEN_KEY)));
+    verifyInvalidCompactionScenario(manager, al(sf_L0_0a), al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, OPEN_KEY), createFile(KEY_A, KEY_D)));
+    verifyInvalidCompactionScenario(manager, al(sf_L0_0a), al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_A, KEY_B), createFile(KEY_B, OPEN_KEY)));
+
+    StoreFile sf_i2B_0 = createFile(OPEN_KEY, KEY_B);
+    StoreFile sf_B2C_0 = createFile(KEY_B, KEY_C);
+    StoreFile sf_C2i_0 = createFile(KEY_C, OPEN_KEY);
+    manager.addCompactionResults(al(sf_L0_0a), al(sf_i2B_0, sf_B2C_0, sf_C2i_0));
+    verifyAllFiles(manager, al(sf_L0_0b, sf_i2B_0, sf_B2C_0, sf_C2i_0));
+
+    // Add another l0 file, "compact" both L0 into two stripes
+    StoreFile sf_L0_1 = createFile();
+    StoreFile sf_i2B_1 = createFile(OPEN_KEY, KEY_B);
+    StoreFile sf_B2C_1 = createFile(KEY_B, KEY_C);
+    manager.insertNewFiles(al(sf_L0_1));
+    manager.addCompactionResults(al(sf_L0_0b, sf_L0_1), al(sf_i2B_1, sf_B2C_1));
+    verifyAllFiles(manager, al(sf_i2B_0, sf_B2C_0, sf_C2i_0, sf_i2B_1, sf_B2C_1));
+
+    // Try compacting with invalid file (no metadata) - should add files to L0.
+    StoreFile sf_L0_2 = createFile(null, null);
+    manager.addCompactionResults(al(), al(sf_L0_2));
+    verifyAllFiles(manager, al(sf_i2B_0, sf_B2C_0, sf_C2i_0, sf_i2B_1, sf_B2C_1, sf_L0_2));
+    // Remove it...
+    manager.addCompactionResults(al(sf_L0_2), al());
+
+    // Do regular compaction in the first stripe.
+    StoreFile sf_i2B_3 = createFile(OPEN_KEY, KEY_B);
+    manager.addCompactionResults(al(sf_i2B_0, sf_i2B_1), al(sf_i2B_3));
+    verifyAllFiles(manager, al(sf_B2C_0, sf_C2i_0, sf_B2C_1, sf_i2B_3));
+
+    // Rebalance two stripes.
+    StoreFile sf_B2D_4 = createFile(KEY_B, KEY_D);
+    StoreFile sf_D2i_4 = createFile(KEY_D, OPEN_KEY);
+    manager.addCompactionResults(al(sf_B2C_0, sf_C2i_0, sf_B2C_1), al(sf_B2D_4, sf_D2i_4));
+    verifyAllFiles(manager, al(sf_i2B_3, sf_B2D_4, sf_D2i_4));
+
+    // Split the first stripe.
+    StoreFile sf_i2A_5 = createFile(OPEN_KEY, KEY_A);
+    StoreFile sf_A2B_5 = createFile(KEY_A, KEY_B);
+    manager.addCompactionResults(al(sf_i2B_3), al(sf_i2A_5, sf_A2B_5));
+    verifyAllFiles(manager, al(sf_B2D_4, sf_D2i_4, sf_i2A_5, sf_A2B_5));
+
+    // Split the middle stripe.
+    StoreFile sf_B2C_6 = createFile(KEY_B, KEY_C);
+    StoreFile sf_C2D_6 = createFile(KEY_C, KEY_D);
+    manager.addCompactionResults(al(sf_B2D_4), al(sf_B2C_6, sf_C2D_6));
+    verifyAllFiles(manager, al(sf_D2i_4, sf_i2A_5, sf_A2B_5, sf_B2C_6, sf_C2D_6));
+
+    // Merge two different middle stripes.
+    StoreFile sf_A2C_7 = createFile(KEY_A, KEY_C);
+    manager.addCompactionResults(al(sf_A2B_5, sf_B2C_6), al(sf_A2C_7));
+    verifyAllFiles(manager, al(sf_D2i_4, sf_i2A_5, sf_C2D_6, sf_A2C_7));
+
+    // Merge lower half.
+    StoreFile sf_i2C_8 = createFile(OPEN_KEY, KEY_C);
+    manager.addCompactionResults(al(sf_i2A_5, sf_A2C_7), al(sf_i2C_8));
+    verifyAllFiles(manager, al(sf_D2i_4, sf_C2D_6, sf_i2C_8));
+
+    // Merge all.
+    StoreFile sf_i2i_9 = createFile(OPEN_KEY, OPEN_KEY);
+    manager.addCompactionResults(al(sf_D2i_4, sf_C2D_6, sf_i2C_8), al(sf_i2i_9));
+    verifyAllFiles(manager, al(sf_i2i_9));
+  }
+
+  @Test
+  public void testCompactionAndFlushConflict() throws Exception {
+    // Add file flush into stripes
+    StripeStoreFileManager sfm = createManager();
+    assertEquals(0, sfm.getStripeCount());
+    StoreFile sf_i2c = createFile(OPEN_KEY, KEY_C), sf_c2i = createFile(KEY_C, OPEN_KEY);
+    sfm.insertNewFiles(al(sf_i2c, sf_c2i));
+    assertEquals(2, sfm.getStripeCount());
+    // Now try to add conflicting flush - should go to L0.
+    StoreFile sf_i2d = createFile(OPEN_KEY, KEY_D), sf_d2i = createFile(KEY_D, OPEN_KEY);
+    sfm.insertNewFiles(al(sf_i2d, sf_d2i));
+    assertEquals(2, sfm.getStripeCount());
+    assertEquals(2, sfm.getLevel0Files().size());
+    verifyGetAndScanScenario(sfm, KEY_C, KEY_C, sf_i2d, sf_d2i, sf_c2i);
+    // Remove these files.
+    sfm.addCompactionResults(al(sf_i2d, sf_d2i), al());
+    assertEquals(0, sfm.getLevel0Files().size());
+    // Add another file to stripe; then "rebalance" stripes w/o it - the file, which was
+    // presumably flushed during compaction, should go to L0.
+    StoreFile sf_i2c_2 = createFile(OPEN_KEY, KEY_C);
+    sfm.insertNewFiles(al(sf_i2c_2));
+    sfm.addCompactionResults(al(sf_i2c, sf_c2i), al(sf_i2d, sf_d2i));
+    assertEquals(1, sfm.getLevel0Files().size());
+    verifyGetAndScanScenario(sfm, KEY_C, KEY_C, sf_i2d, sf_i2c_2);
+  }
+
+  @Test
+  public void testEmptyResultsForStripes() throws Exception {
+    // Test that we can compact L0 into a subset of stripes.
+    StripeStoreFileManager manager = createManager();
+    StoreFile sf0a = createFile();
+    StoreFile sf0b = createFile();
+    manager.insertNewFiles(al(sf0a));
+    manager.insertNewFiles(al(sf0b));
+    ArrayList<StoreFile> compacted = al(createFile(OPEN_KEY, KEY_B),
+        createFile(KEY_B, KEY_C), createFile(KEY_C, OPEN_KEY));
+    manager.addCompactionResults(al(sf0a), compacted);
+    // Next L0 compaction only produces file for the first and last stripe.
+    ArrayList<StoreFile> compacted2 = al(createFile(OPEN_KEY, KEY_B), createFile(KEY_C, OPEN_KEY));
+    manager.addCompactionResults(al(sf0b), compacted2);
+    compacted.addAll(compacted2);
+    verifyAllFiles(manager, compacted);
+  }
+
+  @Test
+  public void testPriority() throws Exception {
+    // Expected priority, file limit, stripe count, files per stripe, l0 files.
+    testPriorityScenario(5,    5, 0, 0, 0);
+    testPriorityScenario(2,    5, 0, 0, 3);
+    testPriorityScenario(4,   25, 5, 1, 0); // example case.
+    testPriorityScenario(3,   25, 5, 1, 1); // L0 files counts for all stripes.
+    testPriorityScenario(3,   25, 5, 2, 0); // file to each stripe - same as one L0 file.
+    testPriorityScenario(2,   25, 5, 4, 0); // 1 is priority user, so 2 is returned.
+    testPriorityScenario(2,   25, 5, 4, 4); // don't return higher than user unless over limit.
+    testPriorityScenario(2,   25, 5, 1, 10); // same.
+    testPriorityScenario(0,   25, 5, 4, 5); // at limit.
+    testPriorityScenario(-5,  25, 5, 6, 0); // over limit!
+    testPriorityScenario(-1,  25, 0, 0, 26); // over limit with just L0
+  }
+
+  private void testPriorityScenario(int expectedPriority,
+      int limit, int stripes, int filesInStripe, int l0Files) throws Exception
+  {
+    final byte[][] keys = { KEY_A, KEY_B, KEY_C, KEY_D };
+    assertTrue(stripes <= keys.length + 1);
+    Configuration conf = TEST_UTIL.getConfiguration();
+    conf.setInt("hbase.hstore.blockingStoreFiles", limit);
+    StripeStoreFileManager sfm = createManager(al(), conf);
+    for (int i = 0; i < l0Files; ++i) {
+      sfm.insertNewFiles(al(createFile()));
+    }
+    for (int i = 0; i < filesInStripe; ++i) {
+      ArrayList<StoreFile> stripe = new ArrayList<StoreFile>();
+      for (int j = 0; j < stripes; ++j) {
+        stripe.add(createFile(
+            (j == 0) ? OPEN_KEY : keys[j - 1], (j == stripes - 1) ? OPEN_KEY : keys[j]));
+      }
+      sfm.addCompactionResults(al(), stripe);
+    }
+    assertEquals(expectedPriority, sfm.getStoreCompactionPriority());
+  }
+
+  private void verifyInvalidCompactionScenario(StripeStoreFileManager manager,
+      ArrayList<StoreFile> filesToCompact, ArrayList<StoreFile> filesToInsert) throws Exception {
+    Collection<StoreFile> allFiles = manager.getStorefiles();
+    try {
+       manager.addCompactionResults(filesToCompact, filesToInsert);
+       fail("Should have thrown");
+    } catch (IOException ex) {
+      // Ignore it.
+    }
+    verifyAllFiles(manager, allFiles); // must have the same files.
+  }
+
+  private void verifyGetOrScanScenario(StripeStoreFileManager manager, boolean isGet,
+      byte[] start, byte[] end, StoreFile... results) throws Exception {
+    verifyGetOrScanScenario(manager, isGet, start, end, Arrays.asList(results));
+  }
+
+  private void verifyGetOrScanScenario(StripeStoreFileManager manager, boolean isGet,
+      byte[] start, byte[] end, Collection<StoreFile> results) throws Exception {
+    start = start != null ? start : HConstants.EMPTY_START_ROW;
+    end = end != null ? end : HConstants.EMPTY_END_ROW;
+    Collection<StoreFile> sfs = manager.getFilesForScanOrGet(isGet, start, end);
+    assertEquals(results.size(), sfs.size());
+    for (StoreFile result : results) {
+      assertTrue(sfs.contains(result));
+    }
+  }
+
+  private void verifyAllFiles(
+      StripeStoreFileManager manager, Collection<StoreFile> results) throws Exception {
+    verifyGetOrScanScenario(manager, false, null, null, results);
+  }
+
+  // TODO: replace with Mockito?
+  private static MockStoreFile createFile(
+      long size, long seqNum, byte[] startKey, byte[] endKey) throws Exception {
+    FileSystem fs = TEST_UTIL.getTestFileSystem();
+    Path testFilePath = StoreFile.getUniqueFile(fs, CFDIR);
+    fs.create(testFilePath);
+    MockStoreFile sf = new MockStoreFile(TEST_UTIL, testFilePath, size, 0, false, seqNum);
+    if (startKey != null) {
+      sf.setMetadataValue(StripeStoreFileManager.STRIPE_START_KEY, startKey);
+    }
+    if (endKey != null) {
+      sf.setMetadataValue(StripeStoreFileManager.STRIPE_END_KEY, endKey);
+    }
+    return sf;
+  }
+
+  private static MockStoreFile createFile(long size, long seqNum) throws Exception {
+    return createFile(size, seqNum, null, null);
+  }
+
+  private static MockStoreFile createFile(byte[] startKey, byte[] endKey) throws Exception {
+    return createFile(0, 0, startKey, endKey);
+  }
+
+  private static MockStoreFile createFile() throws Exception {
+    return createFile(null, null);
+  }
+
+  private static StripeStoreFileManager createManager() throws Exception {
+    return createManager(new ArrayList<StoreFile>());
+  }
+
+  private static StripeStoreFileManager createManager(ArrayList<StoreFile> sfs) throws Exception {
+    return createManager(sfs, TEST_UTIL.getConfiguration());
+  }
+
+  private static StripeStoreFileManager createManager(
+      ArrayList<StoreFile> sfs, Configuration conf) throws Exception {
+    StripeStoreConfig config = new StripeStoreConfig(
+        conf, Mockito.mock(StoreConfigInformation.class));
+    StripeStoreFileManager result = new StripeStoreFileManager(new KVComparator(), conf, config);
+    result.loadFiles(sfs);
+    return result;
+  }
+
+  private static ArrayList<StoreFile> al(StoreFile... sfs) {
+    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+  }
+
+  private static ArrayList<StoreFile> flattenLists(ArrayList<StoreFile>... sfls) {
+    ArrayList<StoreFile> result = new ArrayList<StoreFile>();
+    for (ArrayList<StoreFile> sfl : sfls) {
+      result.addAll(sfl);
+    }
+    return result;
+  }
+}
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
new file mode 100644
index 0000000..e3c1f97
--- /dev/null
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/compactions/TestStripeCompactionPolicy.java
@@ -0,0 +1,645 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.regionserver.compactions;
+
+import static org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.OPEN_KEY;
+import static org.junit.Assert.*;
+import static org.mockito.AdditionalMatchers.aryEq;
+import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.anyInt;
+import static org.mockito.Matchers.anyLong;
+import static org.mockito.Matchers.argThat;
+import static org.mockito.Matchers.eq;
+import static org.mockito.Matchers.isNull;
+import static org.mockito.Mockito.*;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.commons.lang.NotImplementedException;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.regionserver.StoreConfigInformation;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter;
+import org.apache.hadoop.hbase.regionserver.StripeStoreConfig;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFileManager;
+import org.apache.hadoop.hbase.regionserver.StripeStoreFlusher;
+import org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.StripeInformationProvider;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.ConcatenatedLists;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.hbase.util.ManualEnvironmentEdge;
+import org.apache.hadoop.hbase.regionserver.TestStripeCompactor.StoreFileWritersCapture;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.ArgumentMatcher;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Lists;
+
+@Category(SmallTests.class)
+public class TestStripeCompactionPolicy {
+  private static final byte[] KEY_A = Bytes.toBytes("aaa");
+  private static final byte[] KEY_B = Bytes.toBytes("bbb");
+  private static final byte[] KEY_C = Bytes.toBytes("ccc");
+  private static final byte[] KEY_D = Bytes.toBytes("ddd");
+  private static final byte[] KEY_E = Bytes.toBytes("eee");
+  private static final KeyValue KV_A = new KeyValue(KEY_A, 0L);
+  private static final KeyValue KV_B = new KeyValue(KEY_B, 0L);
+  private static final KeyValue KV_C = new KeyValue(KEY_C, 0L);
+  private static final KeyValue KV_D = new KeyValue(KEY_D, 0L);
+  private static final KeyValue KV_E = new KeyValue(KEY_E, 0L);
+
+
+  private static long defaultSplitSize = 18;
+  private static float defaultSplitCount = 1.8F;
+  private final static int defaultInitialCount = 1;
+  private static long defaultTtl = 1000 * 1000;
+
+  @Test
+  public void testNoStripesFromFlush() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setBoolean(StripeStoreConfig.FLUSH_TO_L0_KEY, true);
+    StripeCompactionPolicy policy = createPolicy(conf);
+    StripeInformationProvider si = createStripesL0Only(0, 0);
+
+    KeyValue[] input = new KeyValue[] { KV_A, KV_B, KV_C, KV_D, KV_E };
+    KeyValue[][] expected = new KeyValue[][] { input };
+    verifyFlush(policy, si, input, expected, null);
+  }
+
+  @Test
+  public void testOldStripesFromFlush() throws Exception {
+    StripeCompactionPolicy policy = createPolicy(HBaseConfiguration.create());
+    StripeInformationProvider si = createStripes(0, KEY_C, KEY_D);
+
+    KeyValue[] input = new KeyValue[] { KV_B, KV_C, KV_C, KV_D, KV_E };
+    KeyValue[][] expected = new KeyValue[][] { new KeyValue[] { KV_B },
+        new KeyValue[] { KV_C, KV_C }, new KeyValue[] {  KV_D, KV_E } };
+    verifyFlush(policy, si, input, expected, new byte[][] { OPEN_KEY, KEY_C, KEY_D, OPEN_KEY });
+  }
+
+  @Test
+  public void testNewStripesFromFlush() throws Exception {
+    StripeCompactionPolicy policy = createPolicy(HBaseConfiguration.create());
+    StripeInformationProvider si = createStripesL0Only(0, 0);
+    KeyValue[] input = new KeyValue[] { KV_B, KV_C, KV_C, KV_D, KV_E };
+    // Starts with one stripe; unlike flush results, must have metadata
+    KeyValue[][] expected = new KeyValue[][] { input };
+    verifyFlush(policy, si, input, expected, new byte[][] { OPEN_KEY, OPEN_KEY });
+  }
+
+  @Test
+  public void testSingleStripeCompaction() throws Exception {
+    // Create a special policy that only compacts single stripes, using standard methods.
+    Configuration conf = HBaseConfiguration.create();
+    conf.setFloat(CompactionConfiguration.RATIO_KEY, 1.0F);
+    conf.setInt(StripeStoreConfig.MIN_FILES_KEY, 3);
+    conf.setInt(StripeStoreConfig.MAX_FILES_KEY, 4);
+    conf.setLong(StripeStoreConfig.SIZE_TO_SPLIT_KEY, 1000); // make sure the are no splits
+    StoreConfigInformation sci = mock(StoreConfigInformation.class);
+    StripeStoreConfig ssc = new StripeStoreConfig(conf, sci);
+    StripeCompactionPolicy policy = new StripeCompactionPolicy(conf, sci, ssc) {
+      @Override
+      public StripeCompactionRequest selectCompaction(StripeInformationProvider si,
+          List<StoreFile> filesCompacting, boolean isOffpeak) throws IOException {
+        if (!filesCompacting.isEmpty()) return null;
+        return selectSingleStripeCompaction(si, false, false, isOffpeak);
+      }
+
+      @Override
+      public boolean needsCompactions(
+          StripeInformationProvider si, List<StoreFile> filesCompacting) {
+        if (!filesCompacting.isEmpty()) return false;
+        return needsSingleStripeCompaction(si);
+      }
+    };
+
+    // No compaction due to min files or ratio
+    StripeInformationProvider si = createStripesWithSizes(0, 0,
+        new Long[] { 2L }, new Long[] { 3L, 3L }, new Long[] { 5L, 1L });
+    verifyNoCompaction(policy, si);
+    // No compaction due to min files or ratio - will report needed, but not do any.
+    si = createStripesWithSizes(0, 0,
+        new Long[] { 2L }, new Long[] { 3L, 3L }, new Long[] { 5L, 1L, 1L });
+    assertNull(policy.selectCompaction(si, al(), false));
+    assertTrue(policy.needsCompactions(si, al()));
+    // One stripe has possible compaction
+    si = createStripesWithSizes(0, 0,
+        new Long[] { 2L }, new Long[] { 3L, 3L }, new Long[] { 5L, 4L, 3L });
+    verifySingleStripeCompaction(policy, si, 2, null);
+    // Several stripes have possible compactions; choose best quality (removes most files)
+    si = createStripesWithSizes(0, 0,
+        new Long[] { 3L, 2L, 2L }, new Long[] { 2L, 2L, 1L }, new Long[] { 3L, 2L, 2L, 1L });
+    verifySingleStripeCompaction(policy, si, 2, null);
+    si = createStripesWithSizes(0, 0,
+        new Long[] { 5L }, new Long[] { 3L, 2L, 2L, 1L }, new Long[] { 3L, 2L, 2L });
+    verifySingleStripeCompaction(policy, si, 1, null);
+    // Or with smallest files, if the count is the same 
+    si = createStripesWithSizes(0, 0,
+        new Long[] { 3L, 3L, 3L }, new Long[] { 3L, 1L, 2L }, new Long[] { 3L, 2L, 2L });
+    verifySingleStripeCompaction(policy, si, 1, null);
+    // Verify max count is respected.
+    si = createStripesWithSizes(0, 0, new Long[] { 5L }, new Long[] { 5L, 4L, 4L, 4L, 4L });
+    List<StoreFile> sfs = si.getStripes().get(1).subList(1, 5);
+    verifyCompaction(policy, si, sfs, null, 1, null, si.getStartRow(1), si.getEndRow(1), true);
+    // Verify ratio is applied.
+    si = createStripesWithSizes(0, 0, new Long[] { 5L }, new Long[] { 50L, 4L, 4L, 4L, 4L });
+    sfs = si.getStripes().get(1).subList(1, 5);
+    verifyCompaction(policy, si, sfs, null, 1, null, si.getStartRow(1), si.getEndRow(1), true);
+  }
+
+  @Test
+  public void testWithParallelCompaction() throws Exception {
+    // TODO: currently only one compaction at a time per store is allowed. If this changes,
+    //       the appropriate file exclusion testing would need to be done in respective tests.
+    assertNull(createPolicy(HBaseConfiguration.create()).selectCompaction(
+        mock(StripeInformationProvider.class), al(createFile()), false));
+  }
+
+  @Test
+  public void testWithReferences() throws Exception {
+    StripeCompactionPolicy policy = createPolicy(HBaseConfiguration.create());
+    StripeCompactor sc = mock(StripeCompactor.class);
+    StoreFile ref = createFile();
+    when(ref.isReference()).thenReturn(true);
+    StripeInformationProvider si = mock(StripeInformationProvider.class);
+    Collection<StoreFile> sfs = al(ref, createFile());
+    when(si.getStorefiles()).thenReturn(sfs);
+
+    assertTrue(policy.needsCompactions(si, al()));
+    StripeCompactionPolicy.StripeCompactionRequest scr = policy.selectCompaction(si, al(), false);
+    assertEquals(si.getStorefiles(), scr.getRequest().getFiles());
+    scr.execute(sc);
+    verify(sc, only()).compact(eq(scr.getRequest()), anyInt(), anyLong(),
+        aryEq(OPEN_KEY), aryEq(OPEN_KEY), aryEq(OPEN_KEY), aryEq(OPEN_KEY));
+  }
+
+  @Test
+  public void testInitialCountFromL0() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setInt(StripeStoreConfig.MIN_FILES_L0_KEY, 2);
+    StripeCompactionPolicy policy = createPolicy(
+        conf, defaultSplitSize, defaultSplitCount, 2, false);
+    StripeCompactionPolicy.StripeInformationProvider si = createStripesL0Only(3, 8);
+    verifyCompaction(policy, si, si.getStorefiles(), true, 2, 12L, OPEN_KEY, OPEN_KEY, true);
+    si = createStripesL0Only(3, 10); // If result would be too large, split into smaller parts.
+    verifyCompaction(policy, si, si.getStorefiles(), true, 3, 10L, OPEN_KEY, OPEN_KEY, true);
+    policy = createPolicy(conf, defaultSplitSize, defaultSplitCount, 6, false);
+    verifyCompaction(policy, si, si.getStorefiles(), true, 6, 5L, OPEN_KEY, OPEN_KEY, true);
+  }
+
+  @Test
+  public void testExistingStripesFromL0() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setInt(StripeStoreConfig.MIN_FILES_L0_KEY, 3);
+    StripeCompactionPolicy.StripeInformationProvider si = createStripes(3, KEY_A);
+    verifyCompaction(
+        createPolicy(conf), si, si.getLevel0Files(), null, null, si.getStripeBoundaries());
+  }
+
+  @Test
+  public void testNothingToCompactFromL0() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setInt(StripeStoreConfig.MIN_FILES_L0_KEY, 4);
+    StripeCompactionPolicy.StripeInformationProvider si = createStripesL0Only(3, 10);
+    StripeCompactionPolicy policy = createPolicy(conf);
+    verifyNoCompaction(policy, si);
+
+    si = createStripes(3, KEY_A);
+    verifyNoCompaction(policy, si);
+  }
+
+  @Test
+  public void testSplitOffStripe() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    // First test everything with default split count of 2, then split into more.
+    conf.setInt(StripeStoreConfig.MIN_FILES_KEY, 2);
+    Long[] toSplit = new Long[] { defaultSplitSize - 2, 1L, 1L };
+    Long[] noSplit = new Long[] { defaultSplitSize - 2, 1L };
+    long splitTargetSize = (long)(defaultSplitSize / defaultSplitCount);
+    // Don't split if not eligible for compaction.
+    StripeCompactionPolicy.StripeInformationProvider si =
+        createStripesWithSizes(0, 0, new Long[] { defaultSplitSize - 2, 2L });
+    assertNull(createPolicy(conf).selectCompaction(si, al(), false));
+    // Make sure everything is eligible.
+    conf.setFloat(CompactionConfiguration.RATIO_KEY, 500f);
+    StripeCompactionPolicy policy = createPolicy(conf);
+    verifyWholeStripesCompaction(policy, si, 0, 0, null, 2, splitTargetSize);
+    // Add some extra stripes...
+    si = createStripesWithSizes(0, 0, noSplit, noSplit, toSplit);
+    verifyWholeStripesCompaction(policy, si, 2, 2, null, 2, splitTargetSize);
+    // In the middle.
+    si = createStripesWithSizes(0, 0, noSplit, toSplit, noSplit);
+    verifyWholeStripesCompaction(policy, si, 1, 1, null, 2, splitTargetSize);
+    // No split-off with different config (larger split size).
+    // However, in this case some eligible stripe will just be compacted alone.
+    StripeCompactionPolicy specPolicy = createPolicy(
+        conf, defaultSplitSize + 1, defaultSplitCount, defaultInitialCount, false);
+    verifySingleStripeCompaction(specPolicy, si, 1, null);
+  }
+
+  @Test
+  public void testSplitOffStripeDropDeletes() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    conf.setInt(StripeStoreConfig.MIN_FILES_KEY, 2);
+    StripeCompactionPolicy policy = createPolicy(conf);
+    Long[] toSplit = new Long[] { defaultSplitSize / 2, defaultSplitSize / 2 };
+    Long[] noSplit = new Long[] { 1L };
+    long splitTargetSize = (long)(defaultSplitSize / defaultSplitCount);
+
+    // Verify the deletes can be dropped if there are no L0 files.
+    StripeCompactionPolicy.StripeInformationProvider si =
+        createStripesWithSizes(0, 0, noSplit, toSplit);
+    verifyWholeStripesCompaction(policy, si, 1, 1,    true, null, splitTargetSize);
+    // But cannot be dropped if there are.
+    si = createStripesWithSizes(2, 2, noSplit, toSplit);
+    verifyWholeStripesCompaction(policy, si, 1, 1,    false, null, splitTargetSize);
+  }
+
+  @SuppressWarnings("unchecked")
+  @Test
+  public void testMergeExpiredFiles() throws Exception {
+    ManualEnvironmentEdge edge = new ManualEnvironmentEdge();
+    long now = defaultTtl + 2;
+    edge.setValue(now);
+    EnvironmentEdgeManager.injectEdge(edge);
+    try {
+      StoreFile expiredFile = createFile(), notExpiredFile = createFile();
+      when(expiredFile.getReader().getMaxTimestamp()).thenReturn(now - defaultTtl - 1);
+      when(notExpiredFile.getReader().getMaxTimestamp()).thenReturn(now - defaultTtl + 1);
+      List<StoreFile> expired = Lists.newArrayList(expiredFile, expiredFile);
+      List<StoreFile> notExpired = Lists.newArrayList(notExpiredFile, notExpiredFile);
+      List<StoreFile> mixed = Lists.newArrayList(expiredFile, notExpiredFile);
+
+      StripeCompactionPolicy policy = createPolicy(HBaseConfiguration.create(),
+          defaultSplitSize, defaultSplitCount, defaultInitialCount, true);
+      // Merge expired if there are eligible stripes.
+      StripeCompactionPolicy.StripeInformationProvider si =
+          createStripesWithFiles(expired, expired, expired);
+      verifyWholeStripesCompaction(policy, si, 0, 2, null, 1, Long.MAX_VALUE, false);
+      // Don't merge if nothing expired.
+      si = createStripesWithFiles(notExpired, notExpired, notExpired);
+      assertNull(policy.selectCompaction(si, al(), false));
+      // Merge one expired stripe with next.
+      si = createStripesWithFiles(notExpired, expired, notExpired);
+      verifyWholeStripesCompaction(policy, si, 1, 2, null, 1, Long.MAX_VALUE, false);
+      // Merge the biggest run out of multiple options.
+      // Merge one expired stripe with next.
+      si = createStripesWithFiles(notExpired, expired, notExpired, expired, expired, notExpired);
+      verifyWholeStripesCompaction(policy, si, 3, 4, null, 1, Long.MAX_VALUE, false);
+      // Stripe with a subset of expired files is not merged.
+      si = createStripesWithFiles(expired, expired, notExpired, expired, mixed);
+      verifyWholeStripesCompaction(policy, si, 0, 1, null, 1, Long.MAX_VALUE, false);
+    } finally {
+      EnvironmentEdgeManager.reset();
+    }
+  }
+
+  private static StripeCompactionPolicy.StripeInformationProvider createStripesWithFiles(
+      List<StoreFile>... stripeFiles) throws Exception {
+    return createStripesWithFiles(createBoundaries(stripeFiles.length),
+        Lists.newArrayList(stripeFiles), new ArrayList<StoreFile>());
+  }
+
+  @Test
+  public void testSingleStripeDropDeletes() throws Exception {
+    Configuration conf = HBaseConfiguration.create();
+    StripeCompactionPolicy policy = createPolicy(conf);
+    // Verify the deletes can be dropped if there are no L0 files.
+    Long[][] stripes = new Long[][] { new Long[] { 3L, 2L, 2L }, new Long[] { 6L } };
+    StripeInformationProvider si = createStripesWithSizes(0, 0, stripes);
+    verifySingleStripeCompaction(policy, si, 0, true);
+    // But cannot be dropped if there are.
+    si = createStripesWithSizes(2, 2, stripes);
+    verifySingleStripeCompaction(policy, si, 0, false);
+    // Unless there are enough to cause L0 compaction.
+    si = createStripesWithSizes(6, 2, stripes);
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    sfs.addSublist(si.getLevel0Files());
+    sfs.addSublist(si.getStripes().get(0));
+    verifyCompaction(
+        policy, si, sfs, si.getStartRow(0), si.getEndRow(0), si.getStripeBoundaries());
+    // If we cannot actually compact all files in some stripe, L0 is chosen.
+    si = createStripesWithSizes(6, 2,
+        new Long[][] { new Long[] { 10L, 1L, 1L, 1L, 1L }, new Long[] { 12L } });
+    verifyCompaction(policy, si, si.getLevel0Files(), null, null, si.getStripeBoundaries());
+  }
+
+  /********* HELPER METHODS ************/
+  private static StripeCompactionPolicy createPolicy(
+      Configuration conf) throws Exception {
+    return createPolicy(conf, defaultSplitSize, defaultSplitCount, defaultInitialCount, false);
+  }
+
+  private static StripeCompactionPolicy createPolicy(Configuration conf,
+      long splitSize, float splitCount, int initialCount, boolean hasTtl) throws Exception {
+    conf.setLong(StripeStoreConfig.SIZE_TO_SPLIT_KEY, splitSize);
+    conf.setFloat(StripeStoreConfig.SPLIT_PARTS_KEY, splitCount);
+    conf.setInt(StripeStoreConfig.INITIAL_STRIPE_COUNT_KEY, initialCount);
+    StoreConfigInformation sci = mock(StoreConfigInformation.class);
+    when(sci.getStoreFileTtl()).thenReturn(hasTtl ? defaultTtl : Long.MAX_VALUE);
+    StripeStoreConfig ssc = new StripeStoreConfig(conf, sci);
+    return new StripeCompactionPolicy(conf, sci, ssc);
+  }
+
+  private static ArrayList<StoreFile> al(StoreFile... sfs) {
+    return new ArrayList<StoreFile>(Arrays.asList(sfs));
+  }
+
+  /**
+   * Verify the compaction that includes several entire stripes.
+   * @param policy Policy to test.
+   * @param si Stripe information pre-set with stripes to test.
+   * @param from Starting stripe.
+   * @param to Ending stripe (inclusive).
+   * @param dropDeletes Whether to drop deletes from compaction range.
+   * @param count Expected # of resulting stripes, null if not checked.
+   * @param size Expected target stripe size, null if not checked.
+   */
+  private void verifyWholeStripesCompaction(StripeCompactionPolicy policy,
+      StripeInformationProvider si, int from, int to, Boolean dropDeletes,
+      Integer count, Long size, boolean needsCompaction) throws IOException {
+    verifyCompaction(policy, si, getAllFiles(si, from, to), dropDeletes,
+        count, size, si.getStartRow(from), si.getEndRow(to), needsCompaction);
+  }
+
+  private void verifyWholeStripesCompaction(StripeCompactionPolicy policy,
+      StripeInformationProvider si, int from, int to, Boolean dropDeletes,
+      Integer count, Long size) throws IOException {
+    verifyWholeStripesCompaction(policy, si, from, to, dropDeletes, count, size, true);
+  }
+
+  private void verifySingleStripeCompaction(StripeCompactionPolicy policy,
+      StripeInformationProvider si, int index, Boolean dropDeletes) throws IOException {
+    verifyWholeStripesCompaction(policy, si, index, index, dropDeletes, 1, null, true);
+  }
+
+  /**
+   * Verify no compaction is needed or selected.
+   * @param policy Policy to test.
+   * @param si Stripe information pre-set with stripes to test.
+   */
+  private void verifyNoCompaction(
+      StripeCompactionPolicy policy, StripeInformationProvider si) throws IOException {
+    assertNull(policy.selectCompaction(si, al(), false));
+    assertFalse(policy.needsCompactions(si, al()));
+  }
+
+  /**
+   * Verify arbitrary compaction.
+   * @param policy Policy to test.
+   * @param si Stripe information pre-set with stripes to test.
+   * @param sfs Files that should be compacted.
+   * @param dropDeletesFrom Row from which to drop deletes.
+   * @param dropDeletesTo Row to which to drop deletes.
+   * @param boundaries Expected target stripe boundaries.
+   */
+  private void verifyCompaction(StripeCompactionPolicy policy, StripeInformationProvider si,
+      Collection<StoreFile> sfs, byte[] dropDeletesFrom, byte[] dropDeletesTo,
+      final List<byte[]> boundaries) throws Exception {
+    StripeCompactor sc = mock(StripeCompactor.class);
+    assertTrue(policy.needsCompactions(si, al()));
+    StripeCompactionPolicy.StripeCompactionRequest scr = policy.selectCompaction(si, al(), false);
+    verifyCollectionsEqual(sfs, scr.getRequest().getFiles());
+    scr.execute(sc);
+    verify(sc, times(1)).compact(eq(scr.getRequest()), argThat(
+        new ArgumentMatcher<List<byte[]>>() {
+          @Override
+          public boolean matches(Object argument) {
+            @SuppressWarnings("unchecked")
+            List<byte[]> other = (List<byte[]>)argument;
+            if (other.size() != boundaries.size()) return false;
+            for (int i = 0; i < other.size(); ++i) {
+              if (!Bytes.equals(other.get(i), boundaries.get(i))) return false;
+            }
+            return true;
+          }
+        }),
+        dropDeletesFrom == null ? isNull(byte[].class) : aryEq(dropDeletesFrom),
+        dropDeletesTo == null ? isNull(byte[].class) : aryEq(dropDeletesTo));
+  }
+
+  /**
+   * Verify arbitrary compaction.
+   * @param policy Policy to test.
+   * @param si Stripe information pre-set with stripes to test.
+   * @param sfs Files that should be compacted.
+   * @param dropDeletes Whether to drop deletes from compaction range.
+   * @param count Expected # of resulting stripes, null if not checked.
+   * @param size Expected target stripe size, null if not checked.
+   * @param start Left boundary of the compaction.
+   * @param righr Right boundary of the compaction.
+   */
+  private void verifyCompaction(StripeCompactionPolicy policy, StripeInformationProvider si,
+      Collection<StoreFile> sfs, Boolean dropDeletes, Integer count, Long size,
+      byte[] start, byte[] end, boolean needsCompaction) throws IOException {
+    StripeCompactor sc = mock(StripeCompactor.class);
+    assertTrue(!needsCompaction || policy.needsCompactions(si, al()));
+    StripeCompactionPolicy.StripeCompactionRequest scr = policy.selectCompaction(si, al(), false);
+    verifyCollectionsEqual(sfs, scr.getRequest().getFiles());
+    scr.execute(sc);
+    verify(sc, times(1)).compact(eq(scr.getRequest()),
+        count == null ? anyInt() : eq(count.intValue()),
+        size == null ? anyLong() : eq(size.longValue()), aryEq(start), aryEq(end),
+        dropDeletesMatcher(dropDeletes, start), dropDeletesMatcher(dropDeletes, end));
+  }
+
+  /** Verify arbitrary flush. */
+  protected void verifyFlush(StripeCompactionPolicy policy, StripeInformationProvider si,
+      KeyValue[] input, KeyValue[][] expected, byte[][] boundaries) throws IOException {
+    StoreFileWritersCapture writers = new StoreFileWritersCapture();
+    StripeStoreFlusher.StripeFlushRequest req = policy.selectFlush(si, input.length);
+    StripeMultiFileWriter mw = req.createWriter();
+    mw.init(null, writers, new KeyValue.KVComparator());
+    for (KeyValue kv : input) {
+      mw.append(kv);
+    }
+    boolean hasMetadata = boundaries != null;
+    mw.commitWriters(0, false);
+    writers.verifyKvs(expected, true, hasMetadata);
+    if (hasMetadata) {
+      writers.verifyBoundaries(boundaries);
+    }
+  }
+
+
+  private byte[] dropDeletesMatcher(Boolean dropDeletes, byte[] value) {
+    return dropDeletes == null ? any(byte[].class)
+            : (dropDeletes.booleanValue() ? aryEq(value) : isNull(byte[].class));
+  }
+
+  private void verifyCollectionsEqual(Collection<StoreFile> sfs, Collection<StoreFile> scr) {
+    // Dumb.
+    assertEquals(sfs.size(), scr.size());
+    assertTrue(scr.containsAll(sfs));
+  }
+
+  private static List<StoreFile> getAllFiles(
+      StripeInformationProvider si, int fromStripe, int toStripe) {
+    ArrayList<StoreFile> expected = new ArrayList<StoreFile>();
+    for (int i = fromStripe; i <= toStripe; ++i) {
+      expected.addAll(si.getStripes().get(i));
+    }
+    return expected;
+  }
+
+  /**
+   * @param l0Count Number of L0 files.
+   * @param boundaries Target boundaries.
+   * @return Mock stripes.
+   */
+  private static StripeInformationProvider createStripes(
+      int l0Count, byte[]... boundaries) throws Exception {
+    List<Long> l0Sizes = new ArrayList<Long>();
+    for (int i = 0; i < l0Count; ++i) {
+      l0Sizes.add(5L);
+    }
+    List<List<Long>> sizes = new ArrayList<List<Long>>();
+    for (int i = 0; i <= boundaries.length; ++i) {
+      sizes.add(Arrays.asList(Long.valueOf(5)));
+    }
+    return createStripes(Arrays.asList(boundaries), sizes, l0Sizes);
+  }
+
+  /**
+   * @param l0Count Number of L0 files.
+   * @param l0Size Size of each file.
+   * @return Mock stripes.
+   */
+  private static StripeInformationProvider createStripesL0Only(
+      int l0Count, long l0Size) throws Exception {
+    List<Long> l0Sizes = new ArrayList<Long>();
+    for (int i = 0; i < l0Count; ++i) {
+      l0Sizes.add(l0Size);
+    }
+    return createStripes(null, new ArrayList<List<Long>>(), l0Sizes);
+  }
+
+  /**
+   * @param l0Count Number of L0 files.
+   * @param l0Size Size of each file.
+   * @param sizes Sizes of the files; each sub-array representing a stripe.
+   * @return Mock stripes.
+   */
+  private static StripeInformationProvider createStripesWithSizes(
+      int l0Count, long l0Size, Long[]... sizes) throws Exception {
+    ArrayList<List<Long>> sizeList = new ArrayList<List<Long>>();
+    for (Long[] size : sizes) {
+      sizeList.add(Arrays.asList(size));
+    }
+    return createStripesWithSizes(l0Count, l0Size, sizeList);
+  }
+
+  private static StripeInformationProvider createStripesWithSizes(
+      int l0Count, long l0Size, List<List<Long>> sizes) throws Exception {
+    List<byte[]> boundaries = createBoundaries(sizes.size());
+    List<Long> l0Sizes = new ArrayList<Long>();
+    for (int i = 0; i < l0Count; ++i) {
+      l0Sizes.add(l0Size);
+    }
+    return createStripes(boundaries, sizes, l0Sizes);
+  }
+
+  private static List<byte[]> createBoundaries(int stripeCount) {
+    byte[][] keys = new byte[][] { KEY_A, KEY_B, KEY_C, KEY_D, KEY_E };
+    assert stripeCount <= keys.length + 1;
+    List<byte[]> boundaries = new ArrayList<byte[]>();
+    for (int i = 0; i < stripeCount - 1; ++i) {
+      boundaries.add(keys[i]);
+    }
+    return boundaries;
+  }
+
+  private static StripeInformationProvider createStripes(List<byte[]> boundaries,
+      List<List<Long>> stripeSizes, List<Long> l0Sizes) throws Exception {
+    List<List<StoreFile>> stripeFiles = new ArrayList<List<StoreFile>>(stripeSizes.size());
+    for (List<Long> sizes : stripeSizes) {
+      List<StoreFile> sfs = new ArrayList<StoreFile>();
+      for (Long size : sizes) {
+        sfs.add(createFile(size));
+      }
+      stripeFiles.add(sfs);
+    }
+    List<StoreFile> l0Files = new ArrayList<StoreFile>();
+    for (Long size : l0Sizes) {
+      l0Files.add(createFile(size));
+    }
+    return createStripesWithFiles(boundaries, stripeFiles, l0Files);
+  }
+
+  /**
+   * This method actually does all the work.
+   */
+  private static StripeInformationProvider createStripesWithFiles(List<byte[]> boundaries,
+      List<List<StoreFile>> stripeFiles, List<StoreFile> l0Files) throws Exception {
+    ArrayList<ImmutableList<StoreFile>> stripes = new ArrayList<ImmutableList<StoreFile>>();
+    ArrayList<byte[]> boundariesList = new ArrayList<byte[]>();
+    StripeInformationProvider si = mock(StripeInformationProvider.class);
+    if (!stripeFiles.isEmpty()) {
+      assert stripeFiles.size() == (boundaries.size() + 1);
+      boundariesList.add(OPEN_KEY);
+      for (int i = 0; i <= boundaries.size(); ++i) {
+        byte[] startKey = ((i == 0) ? OPEN_KEY : boundaries.get(i - 1));
+        byte[] endKey = ((i == boundaries.size()) ? OPEN_KEY : boundaries.get(i));
+        boundariesList.add(endKey);
+        for (StoreFile sf : stripeFiles.get(i)) {
+          setFileStripe(sf, startKey, endKey);
+        }
+        stripes.add(ImmutableList.copyOf(stripeFiles.get(i)));
+        when(si.getStartRow(eq(i))).thenReturn(startKey);
+        when(si.getEndRow(eq(i))).thenReturn(endKey);
+      }
+    }
+    ConcatenatedLists<StoreFile> sfs = new ConcatenatedLists<StoreFile>();
+    sfs.addAllSublists(stripes);
+    sfs.addSublist(l0Files);
+    when(si.getStorefiles()).thenReturn(sfs);
+    when(si.getStripes()).thenReturn(stripes);
+    when(si.getStripeBoundaries()).thenReturn(boundariesList);
+    when(si.getStripeCount()).thenReturn(stripes.size());
+    when(si.getLevel0Files()).thenReturn(l0Files);
+    return si;
+  }
+
+  private static StoreFile createFile(long size) throws Exception {
+    StoreFile sf = mock(StoreFile.class);
+    when(sf.getPath()).thenReturn(new Path("moo"));
+    StoreFile.Reader r = mock(StoreFile.Reader.class);
+    when(r.getEntries()).thenReturn(size);
+    when(r.length()).thenReturn(size);
+    when(sf.getReader()).thenReturn(r);
+    return sf;
+  }
+
+  private static StoreFile createFile() throws Exception {
+    return createFile(0);
+  }
+
+  private static void setFileStripe(StoreFile sf, byte[] startKey, byte[] endKey) {
+    when(sf.getMetadataValue(StripeStoreFileManager.STRIPE_START_KEY)).thenReturn(startKey);
+    when(sf.getMetadataValue(StripeStoreFileManager.STRIPE_END_KEY)).thenReturn(endKey);
+  }
+}
