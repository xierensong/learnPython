diff --git src/main/java/org/apache/hadoop/hbase/HConstants.java src/main/java/org/apache/hadoop/hbase/HConstants.java
index e70bd3b..7bbb75b 100644
--- src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -414,6 +414,7 @@ public final class HConstants {
   public static final String NAME = "NAME";
   public static final String VERSIONS = "VERSIONS";
   public static final String IN_MEMORY = "IN_MEMORY";
+  public static final String CONFIGURATION = "CONFIGURATION";
 
   /**
    * This is a retry backoff multiplier table similar to the BSD TCP syn
diff --git src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index ba3be8e..63886f6 100644
--- src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -89,7 +89,7 @@ public class HBaseAdmin implements Abortable, Closeable {
   // want to wait a long time.
   private final int retryLongerMultiplier;
   private boolean aborted;
-  
+
   /**
    * Constructor
    *
@@ -191,7 +191,7 @@ public class HBaseAdmin implements Abortable, Closeable {
     this.aborted = true;
     throw new RuntimeException(why, e);
   }
-  
+
   @Override
   public boolean isAborted(){
     return this.aborted;
@@ -598,7 +598,7 @@ public class HBaseAdmin implements Abortable, Closeable {
         // continue
       }
     }
-    
+
     if (tableExists) {
       throw new IOException("Retries exhausted, it took too long to wait"+
         " for the table " + Bytes.toString(tableName) + " to be deleted.");
@@ -1116,7 +1116,7 @@ public class HBaseAdmin implements Abortable, Closeable {
    * servername is provided then based on the online regions in the specified
    * regionserver the specified region will be closed. The master will not be
    * informed of the close. Note that the regionname is the encoded regionname.
-   * 
+   *
    * @param encodedRegionName
    *          The encoded region name; i.e. the hash that makes up the region
    *          name suffix: e.g. if regionname is
@@ -1422,7 +1422,7 @@ public class HBaseAdmin implements Abortable, Closeable {
    * @throws MasterNotRunningException
    */
   public void move(final byte [] encodedRegionName, final byte [] destServerName)
-  throws UnknownRegionException, MasterNotRunningException, ZooKeeperConnectionException {
+  throws IOException, ZooKeeperConnectionException {
     getMaster().move(encodedRegionName, destServerName);
   }
 
@@ -1747,7 +1747,7 @@ public class HBaseAdmin implements Abortable, Closeable {
    * @param tableName the name of the table
    * @return Ordered list of {@link HRegionInfo}.
    * @throws IOException
-   */  
+   */
   public List<HRegionInfo> getTableRegions(final byte[] tableName)
   throws IOException {
     CatalogTracker ct = getCatalogTracker();
@@ -1759,7 +1759,7 @@ public class HBaseAdmin implements Abortable, Closeable {
     }
     return Regions;
   }
-  
+
   public void close() throws IOException {
     if (this.connection != null) {
       this.connection.close();
@@ -1779,14 +1779,14 @@ public class HBaseAdmin implements Abortable, Closeable {
 
   /**
    * Roll the log writer. That is, start writing log messages to a new file.
-   * 
+   *
    * @param serverName
    *          The servername of the regionserver. A server name is made of host,
    *          port and startcode. This is mandatory. Here is an example:
    *          <code> host187.example.com,60020,1289493121758</code>
    * @return If lots of logs, flush the returned regions so next time through
    * we can clean logs. Returns null if nothing to flush.  Names are actual
-   * region names as returned by {@link HRegionInfo#getEncodedName()}  
+   * region names as returned by {@link HRegionInfo#getEncodedName()}
    * @throws IOException if a remote or network exception occurs
    * @throws FailedLogCloseException
    */
diff --git src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
index 012dc0c..1c5736f 100644
--- src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
+++ src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java
@@ -269,7 +269,7 @@ public class ExecutorService {
     }
     return ret;
   }
-  
+
   /**
    * Executor instance.
    */
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java
new file mode 100644
index 0000000..7afb539
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java
@@ -0,0 +1,115 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.HRegionInfo;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+
+/**
+ * Group user API interface used between client and server.
+ */
+@InterfaceAudience.Private
+public interface GroupAdmin {
+  /**
+   * Get member tables of a group.
+   *
+   *
+   * @param groupName the name of the group
+   * @return list of table names
+   */
+  NavigableSet<String> listTablesOfGroup(String groupName) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroupInfo(String groupName) throws IOException;
+
+  /**
+   * Gets the group info of table.
+   *
+   * @param tableName the table name
+   * @return An instance of GroupInfo.
+   */
+  GroupInfo getGroupInfoOfTable(String tableName) throws IOException;
+
+  /**
+   * Move a set of serves to another group
+   *
+   *
+   * @param servers set of servers, must be in the form HOST:PORT
+   * @param targetGroup the target group
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  void moveServers(Set<String> servers, String targetGroup) throws IOException;
+
+  /**
+   * Move tables to a new group.
+   * This will unassign all of a table's region so it can be reassigned to the correct group.
+   * @param tables list of tables to move
+   * @param targetGroup target group
+   * @throws IOException
+   */
+  void moveTables(Set<String> tables, String targetGroup) throws IOException;
+
+  /**
+   * Add a new group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void addGroup(String name) throws IOException;
+
+  /**
+   * Remove a group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void removeGroup(String name) throws IOException;
+
+  /**
+   * Lists the existing groups.
+   *
+   * @return Collection of GroupInfo.
+   */
+  List<GroupInfo> listGroups() throws IOException;
+
+  /**
+   * Retrieve the GroupInfo a server is affiliated to
+   * @param hostPort
+   * @return
+   * @throws IOException
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * List servers that are currently being moved to a new group
+   * @return a map containing server=>targetGroup KV pairs
+   * @throws IOException
+   */
+  Map<String, String> listServersInTransition() throws IOException;
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java
new file mode 100644
index 0000000..ca84308
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java
@@ -0,0 +1,125 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.MasterNotRunningException;
+import org.apache.hadoop.hbase.ZooKeeperConnectionException;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+
+/**
+ * Client used for managing region server group information.
+ */
+@InterfaceAudience.Public
+public class GroupAdminClient implements GroupAdmin {
+  private GroupAdmin proxy;
+	private static final Log LOG = LogFactory.getLog(GroupAdminClient.class);
+  private int operationTimeout;
+
+  public GroupAdminClient(Configuration conf) throws ZooKeeperConnectionException, MasterNotRunningException {
+    proxy = new HBaseAdmin(conf).coprocessorProxy(GroupAdminProtocol.class);
+    operationTimeout = conf.getInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT,
+            HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT);
+  }
+
+  @Override
+  public NavigableSet<String> listTablesOfGroup(String groupName) throws IOException {
+    return proxy.listTablesOfGroup(groupName);
+  }
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+    return proxy.getGroupInfo(groupName);
+  }
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(String tableName) throws IOException {
+    return proxy.getGroupInfoOfTable(tableName);
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup) throws IOException {
+    proxy.moveServers(servers, targetGroup);
+    waitForTransitions(servers);
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    proxy.moveTables(tables, targetGroup);
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    proxy.addGroup(groupName);
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    proxy.removeGroup(name);
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    return proxy.listGroups();
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    return proxy.getGroupOfServer(hostPort);
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    return proxy.listServersInTransition();
+  }
+
+  private void waitForTransitions(Set<String> servers) throws IOException {
+    long endTime = EnvironmentEdgeManager.getDelegate().currentTimeMillis()+operationTimeout;
+    boolean found;
+    do {
+      found = false;
+      for(String server: proxy.listServersInTransition().keySet()) {
+        found = found || servers.contains(server);
+      }
+      try {
+        Thread.sleep(100);
+      } catch (InterruptedException e) {
+        LOG.debug("Sleep interrupted", e);
+
+      }
+    } while(found && EnvironmentEdgeManager.getDelegate().currentTimeMillis() <= endTime);
+    if (found) {
+      throw new DoNotRetryIOException("Timed out while Waiting for server transition to finish.");
+    }
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java
new file mode 100644
index 0000000..4de1dcb
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java
@@ -0,0 +1,254 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.constraint.ConstraintException;
+import org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.metrics.util.MBeanUtil;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingDeque;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * Service to support Region Server Grouping (HBase-6721)
+ * This should be installed as a Master CoprocessorEndpoint
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class GroupAdminEndpoint extends BaseEndpointCoprocessor
+    implements GroupAdminProtocol {
+  private static final Log LOG = LogFactory.getLog(GroupAdminEndpoint.class);
+
+  private final long threadKeepAliveTimeInMillis = 1000;
+  private int threadMax = 1;
+  private BlockingQueue<Runnable> threadQ;
+  private MasterCoprocessorEnvironment menv;
+  private MasterServices master;
+  private ExecutorService executorService;
+  //List of servers that are being moved from one group to another
+  //Key=host:port,Value=targetGroup
+  ConcurrentMap<String,String> serversInTransition =
+      new ConcurrentHashMap<String,String>();
+
+  @Override
+  public void start(CoprocessorEnvironment env) {
+    menv = (MasterCoprocessorEnvironment)env;
+    master = menv.getMasterServices();
+    threadQ = new LinkedBlockingDeque<Runnable>();
+    threadMax = menv.getConfiguration().getInt("hbase.group.executor.threads", 1);
+    executorService = new ThreadPoolExecutor(threadMax, threadMax,
+        threadKeepAliveTimeInMillis, TimeUnit.MILLISECONDS, threadQ);
+    registerMBean();
+  }
+
+  @Override
+  public void stop(CoprocessorEnvironment env) {
+    executorService.shutdown();
+  }
+
+  @Override
+  public NavigableSet<String> listTablesOfGroup(String groupName) throws IOException {
+    return getGroupInfoManager().getGroup(groupName).getTables();
+  }
+
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+    return getGroupInfoManager().getGroup(groupName);
+  }
+
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(String tableName) throws IOException {
+    return getGroupInfoManager().getGroup(getGroupInfoManager().getGroupOfTable(tableName));
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup)
+      throws IOException {
+    if (servers == null) {
+      throw new DoNotRetryIOException(
+          "The list of servers cannot be null.");
+    }
+    if (StringUtils.isEmpty(targetGroup)) {
+      throw new DoNotRetryIOException("The target group cannot be null.");
+    }
+    if(servers.size() < 1) {
+      return;
+    }
+    //check that it's a valid host and port
+    for(String server: servers) {
+      String splits[] = server.split(":",2);
+      if(splits.length < 2)
+        throw new DoNotRetryIOException("Server list contains not a valid <HOST>:<PORT> entry");
+      Integer.parseInt(splits[1]);
+    }
+
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      //we only allow a move from a single source group
+      //so this should be ok
+      GroupInfo srcGrp = manager.getGroupOfServer(servers.iterator().next());
+      //only move online servers (from default)
+      //or servers from other groups
+      //this prevents bogus servers from entering groups
+      if(GroupInfo.DEFAULT_GROUP.equals(srcGrp.getName())) {
+        Set<String> onlineServers = new HashSet<String>();
+        for(ServerName server: master.getServerManager().getOnlineServers().keySet()) {
+          onlineServers.add(server.getHostAndPort());
+        }
+        for(String el: servers) {
+          if(!onlineServers.contains(el)) {
+            throw new DoNotRetryIOException(
+                "Server "+el+" is not a member of any group.");
+          }
+        }
+      }
+
+      if(srcGrp.getServers().size() <= servers.size() &&
+          srcGrp.getTables().size() > 0) {
+        throw new DoNotRetryIOException("Cannot leave a group that contains tables without servers.");
+      }
+      GroupMoveServerWorker.MoveServerPlan plan =
+          new GroupMoveServerWorker.MoveServerPlan(servers, targetGroup);
+      GroupMoveServerWorker worker = null;
+      try {
+        worker = new GroupMoveServerWorker(master, serversInTransition, getGroupInfoManager(), plan);
+        executorService.submit(worker);
+        LOG.info("GroupMoveServerWorkerSubmitted: "+plan.getTargetGroup());
+      } catch(Exception e) {
+        LOG.error("Failed to submit GroupMoveServerWorker", e);
+        if (worker != null) {
+          worker.complete();
+        }
+        throw new DoNotRetryIOException("Failed to submit GroupMoveServerWorker",e);
+      }
+    }
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    if (tables == null) {
+      throw new DoNotRetryIOException(
+          "The list of servers cannot be null.");
+    }
+    if(tables.size() < 1) {
+      LOG.debug("moveTables() passed an empty set. Ignoring.");
+      return;
+    }
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      GroupInfo destGroup = manager.getGroup(targetGroup);
+      if(destGroup == null) {
+        throw new ConstraintException("Target group does not exist: "+targetGroup);
+      }
+      for(String table : tables) {
+        String srcGroup = manager.getGroupOfTable(table);
+        if(srcGroup.equals(targetGroup)) {
+          throw new ConstraintException("Source group is the same as target group for table "+table+" :"+srcGroup);
+        }
+      }
+      if(destGroup.getServers().size() < 1) {
+        throw new DoNotRetryIOException("Target group must have at least one server.");
+      }
+      manager.moveTables(tables, targetGroup);
+    }
+    for(String table: tables) {
+      master.getAssignmentManager().unassign(
+          master.getAssignmentManager().getRegionsOfTable(Bytes.toBytes(table)));
+    }
+  }
+
+  @Override
+  public void addGroup(String name) throws IOException {
+    getGroupInfoManager().addGroup(new GroupInfo(name));
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      GroupInfo groupInfo = getGroupInfoManager().getGroup(name);
+      if(groupInfo == null) {
+        throw new DoNotRetryIOException("Group "+name+" does not exist");
+      }
+      int tableCount = groupInfo.getTables().size();
+      if (tableCount > 0) {
+        throw new DoNotRetryIOException("Group "+name+" must have no associated tables: "+tableCount);
+      }
+      int serverCount = groupInfo.getServers().size();
+      if(serverCount > 0) {
+        throw new DoNotRetryIOException("Group "+name+" must have no associated servers: "+serverCount);
+      }
+      manager.removeGroup(name);
+    }
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    return getGroupInfoManager().listGroups();
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    return getGroupInfoManager().getGroupOfServer(hostPort);
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    return Collections.unmodifiableMap(serversInTransition);
+  }
+
+  @InterfaceAudience.Private
+  public GroupInfoManager getGroupInfoManager() throws IOException {
+    return ((GroupBasedLoadBalancer)menv.getMasterServices().getLoadBalancer()).getGroupInfoManager();
+  }
+
+  void registerMBean() {
+    org.apache.hadoop.hbase.group.MXBeanImpl mxBeanInfo =
+        org.apache.hadoop.hbase.group.MXBeanImpl.init(this, master);
+    MBeanUtil.registerMBean("Group", "Group", mxBeanInfo);
+    LOG.info("Registered Group MXBean");
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupAdminProtocol.java src/main/java/org/apache/hadoop/hbase/group/GroupAdminProtocol.java
new file mode 100644
index 0000000..5805075
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupAdminProtocol.java
@@ -0,0 +1,25 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+
+public interface GroupAdminProtocol extends GroupAdmin, CoprocessorProtocol {
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java
new file mode 100644
index 0000000..40d9421
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java
@@ -0,0 +1,409 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import com.google.common.collect.LinkedListMultimap;
+import com.google.common.collect.ListMultimap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+
+import com.google.common.collect.ArrayListMultimap;
+import org.apache.hadoop.hbase.master.DefaultLoadBalancer;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * GroupBasedLoadBalancer, used when Region Server Grouping is configured (HBase-6721)
+ * It does region balance based on a table's group membership.
+ *
+ * Most assignment methods contain two exclusive code paths: Online - when the group
+ * table is online and Offline - when it is unavailable.
+ *
+ * During Offline, assignments are made randomly irrespective of group memebership.
+ * Though during this mode, only the tables contained in SPECIAL_TABLES
+ * are given assignments to actual online servers.
+ * Once the GROUP table has been assigned, the balancer switches to Online and will then
+ * start providing appropriate assignments for user tables.
+ *
+ * An optmization has been added to cache the group information for SPECIAL_TABLES in zookeeper,
+ * thus random assignments will only occur during first time a cluster is started.
+ *
+ */
+@InterfaceAudience.Public
+public class GroupBasedLoadBalancer implements LoadBalancer {
+  /** Config for pluggable load balancers */
+  public static final String HBASE_GROUP_LOADBALANCER_CLASS = "hbase.group.grouploadbalancer.class";
+
+  private static final Log LOG = LogFactory.getLog(GroupBasedLoadBalancer.class);
+  private static final ServerName BOGUS_SERVER_NAME = ServerName.parseServerName("127.0.0.1:1");
+
+  public static final Set<String> SPECIAL_TABLES = new HashSet<String>();
+  static {
+    //security table
+    SPECIAL_TABLES.add("_acl_");
+    SPECIAL_TABLES.add(Bytes.toString(HConstants.ROOT_TABLE_NAME));
+    SPECIAL_TABLES.add(Bytes.toString(HConstants.META_TABLE_NAME));
+    SPECIAL_TABLES.add(GroupInfoManager.GROUP_TABLE_NAME);
+  }
+
+  private Configuration config;
+  private ClusterStatus clusterStatus;
+  private MasterServices masterServices;
+  private GroupInfoManager groupManager;
+  private LoadBalancer internalBalancer;
+
+  //used during reflection by LoadBalancerFactory
+  @InterfaceAudience.Private
+  public GroupBasedLoadBalancer() {
+  }
+
+  //This constructor should only be used for unit testing
+  @InterfaceAudience.Private
+  public GroupBasedLoadBalancer(GroupInfoManager groupManager) {
+    this.groupManager = groupManager;
+  }
+
+  @Override
+  public Configuration getConf() {
+    return config;
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.config = conf;
+  }
+
+  @Override
+  public void setClusterStatus(ClusterStatus st) {
+    this.clusterStatus = st;
+  }
+
+  @Override
+  public void setMasterServices(MasterServices masterServices) {
+    this.masterServices = masterServices;
+  }
+
+  @Override
+  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState) throws IOException {
+
+    if (!isOnline()) {
+      throw new IllegalStateException(GroupInfoManager.GROUP_TABLE_NAME+
+          " is not online, unable to perform balance");
+    }
+
+    Map<ServerName,List<HRegionInfo>> correctedState = correctAssignments(clusterState);
+    List<RegionPlan> regionPlans = new ArrayList<RegionPlan>();
+    try {
+      for (GroupInfo info : groupManager.listGroups()) {
+        Map<ServerName, List<HRegionInfo>> groupClusterState = new HashMap<ServerName, List<HRegionInfo>>();
+        for (String sName : info.getServers()) {
+          ServerName actual = ServerName.findServerWithSameHostnamePort(
+              clusterState.keySet(), ServerName.parseServerName(sName));
+          if (actual != null) {
+            groupClusterState.put(actual, correctedState.get(actual));
+          }
+        }
+        List<RegionPlan> groupPlans = this.internalBalancer
+            .balanceCluster(groupClusterState);
+        if (groupPlans != null) {
+          regionPlans.addAll(groupPlans);
+        }
+      }
+    } catch (IOException exp) {
+      LOG.warn("Exception while balancing cluster.", exp);
+      regionPlans.clear();
+    }
+    return regionPlans;
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment (
+      List<HRegionInfo> regions, List<ServerName> servers) throws IOException {
+    Map<ServerName, List<HRegionInfo>> assignments = Maps.newHashMap();
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(regions, servers, regionMap, serverMap);
+    for(String groupKey : regionMap.keySet()) {
+      if (regionMap.get(groupKey).size() > 0) {
+        assignments.putAll(
+            this.internalBalancer.roundRobinAssignment(
+                regionMap.get(groupKey),
+                serverMap.get(groupKey)));
+      }
+    }
+    return assignments;
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> retainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException{
+    if (!isOnline()) {
+      offlineRetainAssignment(regions, servers);
+    }
+    return onlineRetainAssignment(regions, servers);
+  }
+
+  public Map<ServerName, List<HRegionInfo>> offlineRetainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException {
+      //We will just keep assignments even if they are incorrect.
+      //Chances are most will be assigned correctly.
+      //Then we just use balance to correct the misplaced few.
+      //we need to correct catalog and group table assignment anyway.
+      return internalBalancer.retainAssignment(regions, servers);
+  }
+
+  public Map<ServerName, List<HRegionInfo>> onlineRetainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException {
+    Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+    ListMultimap<String, HRegionInfo> groupToRegion = ArrayListMultimap.create();
+    List<HRegionInfo> misplacedRegions = getMisplacedRegions(regions);
+    for (HRegionInfo region : regions.keySet()) {
+      if (!misplacedRegions.contains(region)) {
+        String groupName = groupManager.getGroupOfTable(region.getTableNameAsString());
+        groupToRegion.put(groupName, region);
+      }
+    }
+    // Now the "groupToRegion" map has only the regions which have correct
+    // assignments.
+    for (String key : groupToRegion.keys()) {
+      Map<HRegionInfo, ServerName> currentAssignmentMap = new TreeMap<HRegionInfo, ServerName>();
+      List<HRegionInfo> regionList = groupToRegion.get(key);
+      GroupInfo info = groupManager.getGroup(key);
+      List<ServerName> candidateList = filterOfflineServers(info, servers);
+      for (HRegionInfo region : regionList) {
+        currentAssignmentMap.put(region, regions.get(region));
+      }
+      assignments.putAll(this.internalBalancer.retainAssignment(
+          currentAssignmentMap, candidateList));
+    }
+
+    for (HRegionInfo region : misplacedRegions) {
+      String groupName = groupManager.getGroupOfTable(
+          region.getTableNameAsString());
+      GroupInfo info = groupManager.getGroup(groupName);
+      List<ServerName> candidateList = filterOfflineServers(info, servers);
+      ServerName server = this.internalBalancer.randomAssignment(region,
+          candidateList);
+      if (assignments.containsKey(server) == false) {
+        assignments.put(server, new ArrayList<HRegionInfo>());
+      }
+      assignments.get(server).add(region);
+    }
+    return assignments;
+  }
+
+  @Override
+  public Map<HRegionInfo, ServerName> immediateAssignment(
+      List<HRegionInfo> regions, List<ServerName> servers) throws IOException {
+    Map<HRegionInfo,ServerName> assignments = Maps.newHashMap();
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(regions, servers, regionMap, serverMap);
+    for(String groupKey : regionMap.keySet()) {
+      if (regionMap.get(groupKey).size() > 0) {
+        assignments.putAll(
+            this.internalBalancer.immediateAssignment(
+                regionMap.get(groupKey),
+                serverMap.get(groupKey)));
+      }
+    }
+    return assignments;
+  }
+
+  @Override
+  public ServerName randomAssignment(HRegionInfo region,
+      List<ServerName> servers) throws IOException {
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(Lists.newArrayList(region), servers, regionMap, serverMap);
+    List<ServerName> filteredServers = serverMap.get(regionMap.keySet().iterator().next());
+    return this.internalBalancer.randomAssignment(region, filteredServers);
+  }
+
+  private void generateGroupMaps(
+    List<HRegionInfo> regions,
+    List<ServerName> servers,
+    ListMultimap<String, HRegionInfo> regionMap,
+    ListMultimap<String, ServerName> serverMap) throws IOException {
+    if (isOnline()) {
+      for (HRegionInfo region : regions) {
+        String groupName = groupManager.getGroupOfTable(region.getTableNameAsString());
+        regionMap.put(groupName, region);
+      }
+      for (String groupKey : regionMap.keySet()) {
+        GroupInfo info = groupManager.getGroup(groupKey);
+        serverMap.putAll(groupKey, filterOfflineServers(info, servers));
+      }
+    } else {
+      String nullGroup = "_null";
+      //populate serverMap
+      for(GroupInfo groupInfo: groupManager.listGroups()) {
+        serverMap.putAll(groupInfo.getName(), filterOfflineServers(groupInfo, servers));
+      }
+      //Add bogus server, for groups that don't have special tables
+      //we assign them a bogus server to defer real assignment during online mode
+      serverMap.put(nullGroup, BOGUS_SERVER_NAME);
+      //group regions
+      for (HRegionInfo region : regions) {
+        //Even though some of the non-special tables may be part of the cached groups.
+        //We don't assign them here.
+        if(SPECIAL_TABLES.contains(region.getTableNameAsString())) {
+          regionMap.put(groupManager.getGroupOfTable(region.getTableNameAsString()),region);
+        } else {
+          regionMap.put(nullGroup,region);
+        }
+      }
+    }
+  }
+
+  private List<ServerName> filterOfflineServers(GroupInfo groupInfo,
+                                                List<ServerName> onlineServers) {
+    if (groupInfo != null) {
+      return filterServers(groupInfo.getServers(), onlineServers);
+    } else {
+      LOG.debug("Group Information found to be null. Some regions might be unassigned.");
+      return Collections.EMPTY_LIST;
+    }
+  }
+
+  /**
+   * Filter servers based on the online servers.
+   *
+   * @param servers
+   *          the servers
+   * @param onlineServers
+   *          List of servers which are online.
+   * @return the list
+   */
+  private List<ServerName> filterServers(Collection<String> servers,
+      Collection<ServerName> onlineServers) {
+    ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+    for (String server : servers) {
+      ServerName actual = ServerName.findServerWithSameHostnamePort(
+          onlineServers, ServerName.parseServerName(server));
+      if (actual != null) {
+        finalList.add(actual);
+      }
+    }
+    return finalList;
+  }
+
+  private ListMultimap<String, HRegionInfo> groupRegions(
+      List<HRegionInfo> regionList) throws IOException {
+    ListMultimap<String, HRegionInfo> regionGroup = ArrayListMultimap
+        .create();
+    for (HRegionInfo region : regionList) {
+      String groupName = groupManager.getGroupOfTable(region.getTableNameAsString());
+      regionGroup.put(groupName, region);
+    }
+    return regionGroup;
+  }
+
+  private List<HRegionInfo> getMisplacedRegions(
+      Map<HRegionInfo, ServerName> regions) throws IOException {
+    List<HRegionInfo> misplacedRegions = new ArrayList<HRegionInfo>();
+    for (HRegionInfo region : regions.keySet()) {
+      ServerName assignedServer = regions.get(region);
+      GroupInfo info = groupManager.getGroup(groupManager.getGroupOfTable(region.getTableNameAsString()));
+      if ((info == null)|| (!info.containsServer(assignedServer.getHostAndPort()))) {
+        misplacedRegions.add(region);
+      }
+    }
+    return misplacedRegions;
+  }
+
+  private Map<ServerName, List<HRegionInfo>> correctAssignments(
+       Map<ServerName, List<HRegionInfo>> existingAssignments){
+    Map<ServerName, List<HRegionInfo>> correctAssignments = new TreeMap<ServerName, List<HRegionInfo>>();
+    List<HRegionInfo> misplacedRegions = new LinkedList<HRegionInfo>();
+    for (ServerName sName : existingAssignments.keySet()) {
+      correctAssignments.put(sName, new LinkedList<HRegionInfo>());
+      List<HRegionInfo> regions = existingAssignments.get(sName);
+      for (HRegionInfo region : regions) {
+        GroupInfo info = null;
+        try {
+          info = groupManager.getGroup(groupManager.getGroupOfTable(region.getTableNameAsString()));
+        }catch(IOException exp){
+          LOG.debug("Group information null for region of table " + region.getTableNameAsString(),
+              exp);
+        }
+        if ((info == null) || (!info.containsServer(sName.getHostAndPort()))) {
+          // Misplaced region.
+          misplacedRegions.add(region);
+        } else {
+          correctAssignments.get(sName).add(region);
+        }
+      }
+    }
+
+    //unassign misplaced regions, so that they are assigned to correct groups.
+    this.masterServices.getAssignmentManager().unassign(misplacedRegions);
+    return correctAssignments;
+  }
+
+  @Override
+  public void configure() throws IOException {
+    // Create the balancer
+    Class<? extends LoadBalancer> balancerKlass = config.getClass(
+        HBASE_GROUP_LOADBALANCER_CLASS,
+        DefaultLoadBalancer.class, LoadBalancer.class);
+    internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);
+    internalBalancer.setClusterStatus(clusterStatus);
+    internalBalancer.setMasterServices(masterServices);
+    internalBalancer.setConf(config);
+    internalBalancer.configure();
+    if (groupManager == null) {
+      groupManager = new GroupInfoManagerImpl(masterServices);
+    }
+  }
+
+  public boolean isOnline() {
+    return groupManager != null && groupManager.isOnline();
+  }
+
+  @InterfaceAudience.Private
+  public GroupInfoManager getGroupInfoManager() throws IOException {
+    return groupManager;
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java
new file mode 100644
index 0000000..5ae9039
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java
@@ -0,0 +1,176 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.Serializable;
+import java.util.Collection;
+import java.util.NavigableSet;
+
+import com.google.common.collect.Sets;
+import org.codehaus.jackson.annotate.JsonCreator;
+import org.codehaus.jackson.annotate.JsonProperty;
+
+/**
+ * Stores the group information of region server groups.
+ */
+public class GroupInfo implements Serializable {
+
+  public static final String DEFAULT_GROUP = "default";
+  public static final String TABLEDESC_PROP_GROUP = "group";
+  public static final String OFFLINE_DEFAULT_GROUP = "_offline_default";
+  public static final String TRANSITION_GROUP_PREFIX = "_transition_";
+
+  private String name;
+  private NavigableSet<String> servers;
+  private NavigableSet<String> tables;
+
+  public GroupInfo(String name) {
+    this(name, Sets.<String>newTreeSet(), Sets.<String>newTreeSet());
+  }
+
+  //constructor for jackson
+  @JsonCreator
+  GroupInfo(@JsonProperty("name") String name,
+            @JsonProperty("servers") NavigableSet<String> servers,
+            @JsonProperty("tables") NavigableSet<String> tables) {
+    this.name = name;
+    this.servers = servers;
+    this.tables = tables;
+  }
+
+  public GroupInfo(GroupInfo src) {
+    name = src.getName();
+    servers = Sets.newTreeSet(src.getServers());
+    tables = Sets.newTreeSet(src.getTables());
+  }
+
+  /**
+   * Get group name.
+   *
+   * @return
+   */
+  public String getName() {
+    return name;
+  }
+
+  /**
+   * Adds the server to the group.
+   *
+   * @param hostPort the server
+   */
+  public void addServer(String hostPort){
+    servers.add(hostPort);
+  }
+
+  /**
+   * Adds a group of servers.
+   *
+   * @param hostPort the servers
+   */
+  public void addAllServers(Collection<String> hostPort){
+    servers.addAll(hostPort);
+  }
+
+  /**
+   * @param hostPort
+   * @return true, if a server with hostPort is found
+   */
+  public boolean containsServer(String hostPort) {
+    return servers.contains(hostPort);
+  }
+
+  /**
+   * Get list of servers.
+   *
+   * @return
+   */
+  public NavigableSet<String> getServers() {
+    return servers;
+  }
+
+  /**
+   * Remove a server from this group.
+   *
+   * @param hostPort
+   */
+  public boolean removeServer(String hostPort) {
+    return servers.remove(hostPort);
+  }
+
+  /**
+   * Set of tables that are members of this group
+   * @return
+   */
+  public NavigableSet<String> getTables() {
+    return tables;
+  }
+
+  public void addTable(String table) {
+    tables.add(table);
+  }
+
+  public void addAllTables(Collection<String> arg) {
+    tables.addAll(arg);
+  }
+
+  public boolean containsTable(String table) {
+    return tables.contains(table);
+  }
+
+  public boolean removeTable(String table) {
+    return tables.remove(table);
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append("GroupName:");
+    sb.append(this.name);
+    sb.append(", ");
+    sb.append(" Servers:");
+    sb.append(this.servers);
+    return sb.toString();
+
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    GroupInfo groupInfo = (GroupInfo) o;
+
+    if (!name.equals(groupInfo.name)) return false;
+    if (!servers.equals(groupInfo.servers)) return false;
+    if (!tables.equals(groupInfo.tables)) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = servers.hashCode();
+    result = 31 * result + tables.hashCode();
+    result = 31 * result + name.hashCode();
+    return result;
+  }
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java
new file mode 100644
index 0000000..ad9ec62
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java
@@ -0,0 +1,126 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+/**
+ * Interface used to manage GroupInfo storage. An implementation
+ * has the option to support offline mode.
+ * See {@link GroupBasedLoadBalancer}
+ */
+public interface GroupInfoManager {
+  //Assigned before user tables
+  public static final String GROUP_TABLE_NAME = "0group0";
+  public static final byte[] GROUP_TABLE_NAME_BYTES = Bytes.toBytes(GROUP_TABLE_NAME);
+  public static final byte[] SERVER_FAMILY_BYTES = Bytes.toBytes("servers");
+  public static final byte[] TABLE_FAMILY_BYTES = Bytes.toBytes("tables");
+  public static final byte[] ROW_KEY = {0};
+  public static final String groupZNode = "groupInfo";
+
+
+  /**
+   * Adds the group.
+   *
+   * @param groupInfo the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void addGroup(GroupInfo groupInfo) throws IOException;
+
+  /**
+   * Remove a region server group.
+   *
+   * @param groupName the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void removeGroup(String groupName) throws IOException;
+
+  /**
+   * move servers to a new group.
+   * @param hostPorts list of servers, must be part of the same group
+   * @param srcGroup
+   * @param dstGroup
+   * @return true if move was successful
+   * @throws IOException
+   */
+  boolean moveServers(Set<String> hostPorts, String srcGroup, String dstGroup) throws IOException;
+
+  /**
+   * Gets the group info of server.
+   *
+   * @param hostPort the server
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroup(String groupName) throws IOException;
+
+  /**
+   * Get the group membership of a table
+   * @param tableName
+   * @return Group name of table
+   * @throws IOException
+   */
+	String getGroupOfTable(String tableName) throws IOException;
+
+  /**
+   * Set the group membership of a set of tables
+   *
+   * @param tableNames
+   * @param groupName
+   * @throws IOException
+   */
+  void moveTables(Set<String> tableNames, String groupName) throws IOException;
+
+  /**
+   * List the groups
+   *
+   * @return list of GroupInfo
+   * @throws IOException
+   */
+  List<GroupInfo> listGroups() throws IOException;
+
+  /**
+   * Refresh/reload the group information from
+   * the persistent store
+   *
+   * @throws IOException
+   */
+  void refresh() throws IOException;
+
+  /**
+   * Whether the manager is able to fully
+   * return group metadata
+   *
+   * @return
+   */
+  boolean isOnline();
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java
new file mode 100644
index 0000000..7d98e3d
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java
@@ -0,0 +1,573 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.MetaScanner;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.RowMutations;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.handler.CreateTableHandler;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.type.TypeReference;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.NavigableSet;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+/**
+ * This is an implementation of {@link GroupInfoManager}. Which makes
+ * use of an HBase table as the persistence store for the group information.
+ * It also makes use of zookeeper to store group information needed
+ * for bootstrapping during offline mode.
+ */
+public class GroupInfoManagerImpl implements GroupInfoManager {
+	private static final Log LOG = LogFactory.getLog(GroupInfoManagerImpl.class);
+
+	//Access to this map should always be synchronized.
+	private Map<String, GroupInfo> groupMap;
+  private Map<String, String> tableMap;
+  private MasterServices master;
+  private HTable groupTable;
+  private ZooKeeperWatcher watcher;
+  private GroupStartupWorker groupStartupWorker;
+  //contains list of groups that were last flushed to persistent store
+  private Set<String> prevGroups;
+
+
+  public GroupInfoManagerImpl(MasterServices master) throws IOException {
+		this.groupMap = new HashMap<String, GroupInfo>();
+		this.tableMap = new HashMap<String, String>();
+    this.master = master;
+    this.watcher = master.getZooKeeper();
+    groupStartupWorker = new GroupStartupWorker(this, master);
+    prevGroups = new HashSet<String>();
+    refresh();
+    groupStartupWorker.start();
+  }
+
+	/**
+	 * Adds the group.
+	 *
+	 * @param groupInfo the group name
+	 */
+  @Override
+  public synchronized void addGroup(GroupInfo groupInfo) throws IOException {
+		if (groupMap.get(groupInfo.getName()) != null ||
+        groupInfo.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new DoNotRetryIOException("Group already exists: "+groupInfo.getName());
+    }
+    groupMap.put(groupInfo.getName(), groupInfo);
+    try {
+      flushConfig();
+    } catch (IOException e) {
+      groupMap.remove(groupInfo.getName());
+      refresh();
+      throw e;
+    }
+	}
+
+  @Override
+  public synchronized boolean moveServers(Set<String> hostPorts, String srcGroup, String dstGroup) throws IOException {
+    GroupInfo src = new GroupInfo(getGroup(srcGroup));
+    GroupInfo dst = new GroupInfo(getGroup(dstGroup));
+    boolean foundOne = false;
+    for(String el: hostPorts) {
+      foundOne = src.removeServer(el) || foundOne;
+      dst.addServer(el);
+    }
+
+    Map<String,GroupInfo> newGroupMap = Maps.newHashMap(groupMap);
+    if (!src.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newGroupMap.put(src.getName(), src);
+    }
+    if (!dst.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newGroupMap.put(dst.getName(), dst);
+    }
+
+    Map<String,GroupInfo> prevGroupMap = groupMap;
+    try {
+      groupMap = newGroupMap;
+      flushConfig(newGroupMap);
+    } catch(Exception e) {
+      //in case refresh fails
+      //we restored previous consistent state
+      groupMap = prevGroupMap;
+      LOG.error("Failed to update store", e);
+      refresh();
+      throw new IOException("Error while updating store", e);
+    }
+    return foundOne;
+  }
+
+  /**
+	 * Gets the group info of server.
+	 *
+	 * @param hostPort the server
+	 * @return An instance of GroupInfo.
+	 */
+  @Override
+  public synchronized GroupInfo getGroupOfServer(String hostPort) throws IOException {
+		for(GroupInfo info : groupMap.values()){
+			if (info.containsServer(hostPort)){
+				return info;
+			}
+		}
+		return getGroup(GroupInfo.DEFAULT_GROUP);
+	}
+
+	/**
+	 * Gets the group information.
+	 *
+	 * @param groupName the group name
+	 * @return An instance of GroupInfo
+	 */
+  @Override
+  public synchronized GroupInfo getGroup(String groupName) throws IOException {
+		if (groupName.equals(GroupInfo.DEFAULT_GROUP)) {
+			GroupInfo defaultInfo = new GroupInfo(GroupInfo.DEFAULT_GROUP);
+      List<ServerName> unassignedServers =
+          difference(getOnlineRS(),getAssignedServers());
+      for(ServerName serverName: unassignedServers) {
+        defaultInfo.addServer(serverName.getHostAndPort());
+      }
+      for(String tableName: master.getTableDescriptors().getAll().keySet()) {
+        if (!tableMap.containsKey(tableName)) {
+          defaultInfo.addTable(tableName);
+        }
+      }
+      for(String tableName: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+        if (!tableMap.containsKey(tableName)) {
+          defaultInfo.addTable(tableName);
+        }
+      }
+			return defaultInfo;
+		} else {
+			return this.groupMap.get(groupName);
+		}
+	}
+
+  @Override
+  public synchronized String getGroupOfTable(String tableName) throws IOException {
+    if (tableMap.containsKey(tableName)) {
+      return tableMap.get(tableName);
+    }
+    return GroupInfo.DEFAULT_GROUP;
+  }
+
+  @Override
+  public synchronized void moveTables(Set<String> tableNames, String groupName) throws IOException {
+    if (!GroupInfo.DEFAULT_GROUP.equals(groupName) && !groupMap.containsKey(groupName)) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist or is a special group");
+    }
+    Map<String,GroupInfo> newGroupMap = Maps.newHashMap(groupMap);
+    Map<String,String> newTableMap = Maps.newHashMap(tableMap);
+    for(String tableName: tableNames) {
+      if (newTableMap.containsKey(tableName)) {
+        //TODO optimize this, makes too many new objects
+        GroupInfo src = new GroupInfo(newGroupMap.get(newTableMap.get(tableName)));
+        src.removeTable(tableName);
+        newGroupMap.put(src.getName(), src);
+        newTableMap.remove(tableName);
+      }
+      if (!GroupInfo.DEFAULT_GROUP.equals(groupName)) {
+        GroupInfo dst = new GroupInfo(newGroupMap.get(groupName));
+        dst.addTable(tableName);
+        newGroupMap.put(dst.getName(), dst);
+        newTableMap.put(tableName, dst.getName());
+      }
+    }
+
+    Map<String,GroupInfo> prevGroupMap = groupMap;
+    Map<String,String> prevTableMap = tableMap;
+    try {
+      groupMap = newGroupMap;
+      tableMap = newTableMap;
+      flushConfig(newGroupMap);
+    } catch(Exception e) {
+      groupMap = prevGroupMap;
+      tableMap = prevTableMap;
+      LOG.error("Failed to update store", e);
+      refresh();
+      throw new IOException("Error while updating store", e);
+    }
+  }
+
+
+  /**
+	 * Delete a region server group.
+	 *
+	 * @param groupName the group name
+	 * @throws IOException Signals that an I/O exception has occurred.
+	 */
+  @Override
+  public synchronized void removeGroup(String groupName) throws IOException {
+    if (!groupMap.containsKey(groupName) || groupName.equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist or is a reserved group");
+    }
+    GroupInfo groupInfo = null;
+    try {
+      groupInfo = groupMap.remove(groupName);
+      flushConfig();
+    } catch(IOException e) {
+      groupMap.put(groupName, groupInfo);
+      refresh();
+      throw e;
+    }
+	}
+
+  @Override
+  public synchronized List<GroupInfo> listGroups() throws IOException {
+    List<GroupInfo> list = Lists.newLinkedList(groupMap.values());
+    list.add(getGroup(GroupInfo.DEFAULT_GROUP));
+    return list;
+  }
+
+  @Override
+  public boolean isOnline() {
+    return groupStartupWorker.isOnline();
+  }
+
+  @Override
+  public synchronized void refresh() throws IOException {
+    refresh(false);
+  }
+
+  private synchronized void refresh(boolean forceOnline) throws IOException {
+    ObjectMapper mapper = new ObjectMapper();
+    List<GroupInfo> groupList = new LinkedList<GroupInfo>();
+
+    //if online read from GROUP table
+    if (forceOnline || isOnline()) {
+      if (groupTable == null) {
+        groupTable = new HTable(master.getConfiguration(), GROUP_TABLE_NAME_BYTES);
+      }
+      Result result = groupTable.get(new Get(ROW_KEY));
+      if(!result.isEmpty()) {
+        NavigableMap<byte[],NavigableMap<byte[],byte[]>> dataMap = result.getNoVersionMap();
+        for(byte[] groupName: dataMap.get(SERVER_FAMILY_BYTES).keySet()) {
+          NavigableSet<String> servers =
+              mapper.readValue(Bytes.toString(dataMap.get(SERVER_FAMILY_BYTES).get(groupName)),
+                  new TypeReference<TreeSet<String>>() {});
+          NavigableSet<String> tables =
+              mapper.readValue(Bytes.toString(dataMap.get(TABLE_FAMILY_BYTES).get(groupName)),
+                  new TypeReference<TreeSet<String>>() {});
+          GroupInfo group = new GroupInfo(Bytes.toString(groupName), servers, tables);
+          groupList.add(group);
+        }
+      }
+    }
+    //Overwrite any info stored by table, this takes precedence
+    String groupPath = ZKUtil.joinZNode(watcher.baseZNode,groupZNode);
+    try {
+      if(ZKUtil.checkExists(watcher,groupPath) != -1) {
+        byte[] data = ZKUtil.getData(watcher, groupPath);
+        LOG.debug("Reading ZK GroupInfo:" + Bytes.toString(data));
+        groupList.addAll(
+            (List<GroupInfo>) mapper.readValue(data,new TypeReference<List<GroupInfo>>(){}));
+      }
+    } catch (KeeperException e) {
+      throw new IOException("Failed to read groupZNode",e);
+    }
+    //populate the data
+    this.groupMap.clear();
+    this.tableMap.clear();
+    for (GroupInfo group : groupList) {
+      if(!(group.getName().equals(GroupInfo.OFFLINE_DEFAULT_GROUP) && (isOnline() || forceOnline))) {
+        groupMap.put(group.getName(), group);
+        for(String table: group.getTables()) {
+          tableMap.put(table, group.getName());
+        }
+      }
+    }
+    prevGroups.clear();
+    prevGroups.addAll(groupMap.keySet());
+	}
+
+	/**
+	 * Write the configuration to HDFS.
+	 *
+	 * @throws IOException
+	 */
+	private synchronized void flushConfig() throws IOException {
+    flushConfig(groupMap);
+	}
+
+	private synchronized void flushConfig(Map<String,GroupInfo> newGroupMap) throws IOException {
+    ObjectMapper mapper = new ObjectMapper();
+    List<GroupInfo> zGroup = new LinkedList<GroupInfo>();
+    Put put = new Put(ROW_KEY);
+    Delete delete = new Delete(ROW_KEY);
+
+    //populate deletes
+    for(String groupName : prevGroups) {
+      if(!newGroupMap.containsKey(groupName)) {
+        delete.deleteColumns(TABLE_FAMILY_BYTES, Bytes.toBytes(groupName));
+        delete.deleteColumns(SERVER_FAMILY_BYTES, Bytes.toBytes(groupName));
+      }
+    }
+
+    //populate puts
+    for(GroupInfo groupInfo : newGroupMap.values()) {
+      ByteArrayOutputStream bos = new ByteArrayOutputStream();
+      mapper.writeValue(bos, groupInfo.getServers());
+      put.add(SERVER_FAMILY_BYTES,
+          Bytes.toBytes(groupInfo.getName()),
+          bos.toByteArray());
+      bos = new ByteArrayOutputStream();
+      mapper.writeValue(bos, groupInfo.getTables());
+      put.add(TABLE_FAMILY_BYTES,
+          Bytes.toBytes(groupInfo.getName()),
+          bos.toByteArray());
+      for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+        if (groupInfo.getTables().contains(special)) {
+          zGroup.add(groupInfo);
+          break;
+        }
+      }
+    }
+
+    //copy default group to offline group
+    GroupInfo defaultGroup = getGroup(GroupInfo.DEFAULT_GROUP);
+    GroupInfo offlineGroup = new GroupInfo(GroupInfo.OFFLINE_DEFAULT_GROUP);
+    offlineGroup.addAllServers(defaultGroup.getServers());
+    offlineGroup.addAllTables(defaultGroup.getTables());
+    zGroup.add(offlineGroup);
+    //Write zk data first since that's what we'll read first
+    String groupPath = ZKUtil.joinZNode(watcher.baseZNode,groupZNode);
+    try {
+      ByteArrayOutputStream bos = new ByteArrayOutputStream();
+      mapper.writeValue(bos, zGroup);
+      LOG.debug("Writing ZK GroupInfo:" + Bytes.toString(bos.toByteArray()));
+      ZKUtil.createSetData(watcher, groupPath, bos.toByteArray());
+    } catch (KeeperException e) {
+      throw new IOException("Failed to write to groupZNode",e);
+    }
+
+    RowMutations rowMutations = new RowMutations(ROW_KEY);
+    if(put.size() > 0) {
+      rowMutations.add(put);
+    }
+    if(delete.size() > 0) {
+      rowMutations.add(delete);
+    }
+    if(rowMutations.getMutations().size() > 0) {
+      groupTable.mutateRow(rowMutations);
+    }
+
+    prevGroups.clear();
+    prevGroups.addAll(newGroupMap.keySet());
+  }
+
+  private List<ServerName> getOnlineRS() throws IOException{
+    if (master != null) {
+      return master.getServerManager().getOnlineServersList();
+    }
+    try {
+      List<ServerName> servers = new LinkedList<ServerName>();
+      for (String el: ZKUtil.listChildrenNoWatch(watcher, watcher.rsZNode)) {
+        servers.add(ServerName.parseServerName(el));
+      }
+      return servers;
+    } catch (KeeperException e) {
+      throw new IOException("Failed to retrieve server list from zookeeper", e);
+    }
+  }
+
+  private List<ServerName> getAssignedServers(){
+    List<ServerName> assignedServers = Lists.newArrayList();
+    for(GroupInfo gInfo : groupMap.values()){
+      for(String hostPort: gInfo.getServers()) {
+        assignedServers.add(ServerName.parseServerName(hostPort));
+      }
+    }
+    return assignedServers;
+  }
+
+	List<ServerName> difference(Collection<ServerName> onlineServers,
+			Collection<ServerName> servers) {
+		if (servers.size() == 0){
+			return Lists.newArrayList(onlineServers);
+		} else {
+			ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+			for (ServerName olServer : onlineServers) {
+				ServerName actual = ServerName.findServerWithSameHostnamePort(
+						servers, olServer);
+				if (actual == null) {
+					finalList.add(olServer);
+				}
+			}
+			return finalList;
+		}
+	}
+
+  private static class GroupStartupWorker extends Thread {
+    private static final Log LOG = LogFactory.getLog(GroupStartupWorker.class);
+
+    private Configuration conf;
+    private volatile boolean isOnline = false;
+    private MasterServices masterServices;
+    private GroupInfoManagerImpl groupInfoManager;
+
+    public GroupStartupWorker(GroupInfoManagerImpl groupInfoManager,
+                              MasterServices masterServices) {
+      this.conf = masterServices.getConfiguration();
+      this.masterServices = masterServices;
+      this.groupInfoManager = groupInfoManager;
+      setName(GroupStartupWorker.class.getName()+"-"+masterServices.getServerName());
+      setDaemon(true);
+    }
+
+    @Override
+    public void run() {
+      if(waitForGroupTableOnline()) {
+        isOnline = true;
+        LOG.info("GroupBasedLoadBalancer is now online");
+      }
+    }
+
+    public boolean waitForGroupTableOnline() {
+      final List<HRegionInfo> foundRegions = new LinkedList<HRegionInfo>();
+      final AtomicBoolean found = new AtomicBoolean(false);
+      while (!found.get() && isMasterRunning()) {
+        foundRegions.clear();
+        found.set(true);
+        try {
+          if(masterServices.getCatalogTracker().verifyRootRegionLocation(1) &&
+              masterServices.getCatalogTracker().verifyMetaRegionLocation(1)) {
+            MetaScanner.MetaScannerVisitor visitor = new MetaScanner.MetaScannerVisitorBase() {
+              @Override
+              public boolean processRow(Result row) throws IOException {
+                byte[] value = row.getValue(HConstants.CATALOG_FAMILY,
+                    HConstants.REGIONINFO_QUALIFIER);
+                HRegionInfo info = Writables.getHRegionInfoOrNull(value);
+                if (info != null) {
+                  if (Bytes.equals(GROUP_TABLE_NAME_BYTES, info.getTableName())) {
+                    value = row.getValue(HConstants.CATALOG_FAMILY,
+                        HConstants.SERVER_QUALIFIER);
+                    if (value == null) {
+                      found.set(false);
+                    }
+                    foundRegions.add(info);
+                  }
+                }
+                return true;
+              }
+            };
+            MetaScanner.metaScan(conf, visitor);
+            List<HRegionInfo> assignedRegions
+                = masterServices.getAssignmentManager().getRegionsOfTable(GROUP_TABLE_NAME_BYTES);
+            if(assignedRegions == null) {
+              assignedRegions = new LinkedList<HRegionInfo>();
+            }
+            //if no regions in meta then we have to create the table
+            if (foundRegions.size() < 1 &&
+                !MetaReader.tableExists(masterServices.getCatalogTracker(), GROUP_TABLE_NAME)) {
+              groupInfoManager.createGroupTable(masterServices);
+            }
+            LOG.info("Group table: "+GROUP_TABLE_NAME+" isOnline: "+found.get()+", regionCount: "+foundRegions.size()+
+                ", assignCount: "+assignedRegions.size());
+            found.set(found.get() && assignedRegions.size() == foundRegions.size() && foundRegions.size() > 0);
+          } else {
+            LOG.info("Waiting for catalog tables to come online");
+            found.set(false);
+          }
+          if (found.get()) {
+            groupInfoManager.refresh(true);
+            //flush any inconsistencies between ZK and HTable
+            groupInfoManager.flushConfig();
+          }
+        } catch(Exception e) {
+          found.set(false);
+          LOG.warn("Failed to perform check", e);
+        }
+        try {
+          Thread.sleep(100);
+        } catch (InterruptedException e) {
+          LOG.info("Sleep interrupted", e);
+        }
+      }
+      return found.get();
+    }
+
+    public boolean isOnline() {
+      return isOnline;
+    }
+
+    private boolean isMasterRunning() {
+      return !masterServices.isAborted() && !masterServices.isStopped();
+    }
+  }
+
+  private void createGroupTable(MasterServices masterServices) throws IOException {
+    HTableDescriptor desc = new HTableDescriptor(GROUP_TABLE_NAME_BYTES);
+    desc.addFamily(new HColumnDescriptor(SERVER_FAMILY_BYTES));
+    desc.addFamily(new HColumnDescriptor(TABLE_FAMILY_BYTES));
+    desc.setMaxFileSize(1l << 32);
+    HRegionInfo newRegions[] = new HRegionInfo[]{
+          new HRegionInfo(desc.getName(), null, null)};
+    //we need to create the table this way to bypass
+    //checkInitialized
+    masterServices.getExecutorService()
+        .submit(new CreateTableHandler(masterServices,
+            masterServices.getMasterFileSystem(),
+            masterServices.getServerManager(),
+            desc,
+            masterServices.getConfiguration(),
+            newRegions,
+            masterServices.getCatalogTracker(),
+            masterServices.getAssignmentManager()));
+    //need this or else region won't be assigned
+    masterServices.getAssignmentManager().assign(newRegions[0], false);
+  }
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java
new file mode 100644
index 0000000..0ad5da2
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java
@@ -0,0 +1,89 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.constraint.ConstraintException;
+import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.ObserverContext;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import java.io.IOException;
+
+/**
+ * This class is a required component to enable Region Server Groups.
+ * It must be installed as a system coprocessor on the master.
+ */
+public class GroupMasterObserver extends BaseMasterObserver {
+	private static final org.apache.commons.logging.Log LOG = LogFactory.getLog(GroupMasterObserver.class);
+
+  private MasterCoprocessorEnvironment menv;
+  private GroupAdmin groupAdmin;
+
+  @Override
+  public void start(CoprocessorEnvironment ctx) throws IOException {
+    menv = (MasterCoprocessorEnvironment)ctx;
+  }
+
+  @Override
+  public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> ctx, HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+    String groupName = desc.getValue(GroupInfo.TABLEDESC_PROP_GROUP);
+    if(groupName == null) {
+      return;
+    }
+
+    GroupInfo groupInfo = getGroupAdmin().getGroupInfo(groupName);
+    if(groupInfo == null) {
+      throw new ConstraintException("Group "+groupName+" does not exist.");
+    }
+    //we remove the property since it is ephemeral
+    desc.remove(GroupInfo.TABLEDESC_PROP_GROUP);
+    getGroupAdmin().moveTables(Sets.newHashSet(desc.getNameAsString()), groupName);
+  }
+
+  @Override
+  public void postDeleteTable(ObserverContext<MasterCoprocessorEnvironment> ctx, byte[] tableName) throws IOException {
+    if(tableName.length > 0) {
+      String table = Bytes.toString(tableName);
+      GroupInfo group = getGroupAdmin().getGroupInfoOfTable(table);
+      LOG.debug("Removing deleted table from table group "+group.getName());
+      if (!GroupInfo.DEFAULT_GROUP.equals(group.getName())) {
+        getGroupAdmin().moveTables(Sets.newHashSet(table), GroupInfo.DEFAULT_GROUP);
+      }
+    }
+  }
+
+  private GroupAdmin getGroupAdmin() {
+    if(groupAdmin == null) {
+      groupAdmin = (GroupAdmin)
+          menv.getMasterServices().getCoprocessorHost().findCoprocessor(GroupAdminEndpoint.class.getName());
+      if(groupAdmin == null) {
+        groupAdmin = (GroupAdmin)
+            menv.getMasterServices().getCoprocessorHost().findCoprocessor("SecureGroupAdminEndpoint");
+      }
+    }
+    return groupAdmin;
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java
new file mode 100644
index 0000000..93f05be
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java
@@ -0,0 +1,203 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.constraint.ConstraintException;
+import org.apache.hadoop.hbase.master.MasterServices;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * This is a worker class responsible for moving a set of servers
+ * from source group to target group. Supplied servers must be part
+ * of the source group.
+ *
+ * Servers are then moved to a temporary transition group. Any
+ * regions are then unassigned from the server. Once the servers
+ * are drained of any regions the servers are then moved to the
+ * destination group.
+ */
+public class GroupMoveServerWorker implements Runnable {
+	private static final Log LOG = LogFactory.getLog(GroupMoveServerWorker.class);
+
+  private MasterServices master;
+  private MoveServerPlan plan;
+  private String transGroup;
+  private String sourceGroup;
+  private GroupInfoManager groupManager;
+  private Map<String,String> serversInTransition;
+  private volatile boolean succeeded;
+
+  public GroupMoveServerWorker(Server master, Map<String, String> serversInTransition,
+                               GroupInfoManager groupManager,
+                               MoveServerPlan plan) throws IOException {
+    this.serversInTransition = serversInTransition;
+    this.groupManager = groupManager;
+    this.master = (MasterServices)master;
+    this.plan = plan;
+
+    synchronized (serversInTransition) {
+      //check server list
+      sourceGroup = groupManager.getGroupOfServer(plan.getServers().iterator().next()).getName();
+      if(groupManager.getGroup(plan.getTargetGroup()) == null) {
+        throw new ConstraintException("Target group does not exist: "+plan.getTargetGroup());
+      }
+      for(String server: plan.getServers()) {
+        if (serversInTransition.containsKey(server)) {
+          throw new DoNotRetryIOException(
+              "Server list contains a server that is already being moved: "+server);
+        }
+        String tmpGroup = groupManager.getGroupOfServer(server).getName();
+        if (sourceGroup != null && !tmpGroup.equals(sourceGroup)) {
+          throw new DoNotRetryIOException(
+              "Move server request should only come from one source group. "+
+              "Expecting only "+sourceGroup+" but contains "+tmpGroup);
+        }
+      }
+      if(sourceGroup.equals(plan.getTargetGroup())) {
+        throw new ConstraintException(
+            "Target group is the same as source group: "+plan.getTargetGroup());
+      }
+      //update the servers as in transition
+      for(String server: plan.getServers()) {
+        serversInTransition.put(server, plan.getTargetGroup());
+      }
+      if (!sourceGroup.startsWith(GroupInfo.TRANSITION_GROUP_PREFIX)) {
+        transGroup = GroupInfo.TRANSITION_GROUP_PREFIX+
+            System.currentTimeMillis()+"_"+sourceGroup+"-"+plan.getTargetGroup();
+        groupManager.addGroup(new GroupInfo(transGroup));
+      }
+      groupManager.moveServers(plan.getServers(), sourceGroup,
+          transGroup!=null?transGroup:plan.getTargetGroup());
+    }
+  }
+
+  @Override
+  public void run() {
+    String name = "GroupMoveServer-"+transGroup+"-"+plan.getTargetGroup();
+    Thread.currentThread().setName(name);
+    try {
+      boolean found;
+      do {
+        LOG.debug(name+" is awake");
+        found = false;
+        for(String rs: plan.getServers()) {
+          List<HRegionInfo> regions = getOnlineRegions(rs);
+          LOG.info("Unassigining "+regions.size()+" regions from server "+rs);
+          if(regions.size() > 0) {
+            master.getAssignmentManager().unassign(regions);
+            found = true;
+          }
+        }
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException e) {
+          LOG.warn("Sleep interrupted", e);
+        }
+      } while(found);
+      succeeded = true;
+      LOG.info("Move server done: "+sourceGroup+"->"+plan.getTargetGroup());
+    } catch(Exception e) {
+      succeeded = false;
+      LOG.error("Caught exception while running", e);
+    }
+    try {
+      complete();
+    } catch (IOException e) {
+      succeeded = false;
+      LOG.error("Failed to complete move", e);
+    }
+  }
+
+  private List<HRegionInfo> getOnlineRegions(String hostPort) throws IOException {
+    List<HRegionInfo> regions = new LinkedList<HRegionInfo>();
+    for(Map.Entry<ServerName, List<HRegionInfo>> el:
+        master.getAssignmentManager().getAssignments().entrySet()) {
+      if (el.getKey().getHostAndPort().equals(hostPort)) {
+        regions.addAll(el.getValue());
+      }
+    }
+    return regions;
+  }
+
+  static class MoveServerPlan {
+    private Set<String> servers;
+    private String targetGroup;
+
+    public MoveServerPlan(Set<String> servers, String targetGroup) {
+      this.servers = servers;
+      this.targetGroup = targetGroup;
+    }
+
+    public Set<String> getServers() {
+      return servers;
+    }
+
+    public String getTargetGroup() {
+      return targetGroup;
+    }
+  }
+
+  public void complete() throws IOException {
+    try {
+      String tmpSourceGroup = sourceGroup;
+      if (transGroup != null) {
+        tmpSourceGroup = transGroup;
+        LOG.debug("Moving "+plan.getServers().size()+
+            " servers from transition group: "+transGroup+" to final group: "+plan.getTargetGroup());
+      }
+      if (succeeded) {
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, plan.getTargetGroup());
+        if (transGroup != null) {
+          groupManager.removeGroup(transGroup);
+          LOG.debug("Move done "+plan.getServers().size()+
+              " servers from transition group: "+transGroup+" to final group: "+plan.getTargetGroup());
+        }
+        LOG.debug("Move done "+plan.getServers().size()+
+            " servers from source group: "+sourceGroup+" to final group: "+plan.getTargetGroup());
+      } else {
+        //rollback
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, sourceGroup);
+        if (transGroup != null) {
+          groupManager.removeGroup(transGroup);
+          LOG.debug("Rollback done "+plan.getServers().size()+
+              " servers from transition group: "+transGroup+" to old group: "+sourceGroup);
+        }
+      }
+    } finally {
+      //remove servers in transition
+      synchronized(serversInTransition) {
+        for(String server: plan.getServers()) {
+          serversInTransition.remove(server);
+        }
+      }
+    }
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/MXBean.java src/main/java/org/apache/hadoop/hbase/group/MXBean.java
new file mode 100644
index 0000000..1f64aab
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/MXBean.java
@@ -0,0 +1,64 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+
+public interface MXBean {
+
+  public Map<String,List<String>> getServersByGroup() throws IOException;
+
+  public List<GroupInfoBean> getGroups() throws IOException;
+
+  public Map<String,String> getServersInTransition() throws IOException;
+
+  public static class GroupInfoBean {
+
+    private String name;
+    private List<String> servers;
+    private List<String> tables;
+
+    //Need this to convert NavigableSet to List
+    public GroupInfoBean(GroupInfo groupInfo) {
+      this.name = groupInfo.getName();
+      this.servers = new LinkedList<String>();
+      this.servers.addAll(groupInfo.getServers());
+      this.tables = new LinkedList<String>();
+      this.tables.addAll(groupInfo.getTables());
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public List<String> getServers() {
+      return servers;
+    }
+
+    public List<String> getTables() {
+      return tables;
+    }
+  }
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java
new file mode 100644
index 0000000..9f439ec
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java
@@ -0,0 +1,86 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HServerLoad;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.master.MasterServices;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+
+public class MXBeanImpl implements MXBean {
+  private static final Log LOG = LogFactory.getLog(MXBeanImpl.class);
+
+  private static MXBeanImpl instance = null;
+
+  private GroupAdmin groupAdmin;
+  private MasterServices master;
+
+  public synchronized static MXBeanImpl init(
+      final GroupAdmin groupAdmin,
+      MasterServices master) {
+    if (instance == null) {
+      instance = new MXBeanImpl(groupAdmin, master);
+    }
+    return instance;
+  }
+
+  protected MXBeanImpl(final GroupAdmin groupAdmin,
+      MasterServices master) {
+    this.groupAdmin = groupAdmin;
+    this.master = master;
+  }
+
+  @Override
+  public Map<String, List<String>> getServersByGroup() throws IOException {
+    Map<String, List<String>> data = new HashMap<String, List<String>>();
+    for (final ServerName entry :
+      master.getServerManager().getOnlineServersList()) {
+      GroupInfo groupInfo = groupAdmin.getGroupOfServer(entry.getHostAndPort());
+      if(!data.containsKey(groupInfo.getName())) {
+        data.put(groupInfo.getName(), new LinkedList<String>());
+      }
+      data.get(groupInfo.getName()).add(entry.getHostAndPort());
+    }
+    return data;
+  }
+
+  @Override
+  public List<GroupInfoBean> getGroups() throws IOException {
+    LinkedList list = new LinkedList();
+    for(GroupInfo group: groupAdmin.listGroups()) {
+      list.add(new GroupInfoBean(group));
+    }
+    return list;
+  }
+
+  @Override
+  public Map<String, String> getServersInTransition() throws IOException {
+    return groupAdmin.listServersInTransition();
+  }
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
index c17da97..3cdf96f 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
@@ -183,7 +183,7 @@ public interface HMasterInterface extends VersionedProtocol {
    * <code>encodedRegionName</code>
    */
   public void move(final byte [] encodedRegionName, final byte [] destServerName)
-  throws UnknownRegionException;
+  throws IOException;
 
   /**
    * Assign a region to a server chosen at random.
diff --git src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 1245c6f..ba5db4e 100644
--- src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -45,6 +45,7 @@ import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Chore;
 import org.apache.hadoop.hbase.HConstants;
@@ -177,7 +178,7 @@ public class AssignmentManager extends ZooKeeperListener {
 
   //Thread pool executor service for timeout monitor
   private java.util.concurrent.ExecutorService threadPoolExecutorService;
-  
+
   private List<EventType> ignoreStatesRSOffline = Arrays.asList(new EventType[]{
       EventType.RS_ZK_REGION_FAILED_OPEN, EventType.RS_ZK_REGION_CLOSED });
 
@@ -187,8 +188,8 @@ public class AssignmentManager extends ZooKeeperListener {
    */
   private volatile boolean failover = false;
 
-  // Set holding all the regions which got processed while RIT was not 
-  // populated during master failover. 
+  // Set holding all the regions which got processed while RIT was not
+  // populated during master failover.
   private Map<String, HRegionInfo> failoverProcessedRegions =
     new HashMap<String, HRegionInfo>();
 
@@ -200,7 +201,7 @@ public class AssignmentManager extends ZooKeeperListener {
    * @param catalogTracker
    * @param service
    * @throws KeeperException
-   * @throws IOException 
+   * @throws IOException
    */
   public AssignmentManager(Server master, ServerManager serverManager,
       CatalogTracker catalogTracker, final LoadBalancer balancer,
@@ -227,7 +228,7 @@ public class AssignmentManager extends ZooKeeperListener {
     this.balancer = balancer;
     this.threadPoolExecutorService = Executors.newCachedThreadPool();
   }
-  
+
   void startTimeOutMonitor() {
     Threads.setDaemonThreadRunning(timeoutMonitor.getThread(), master.getServerName()
         + ".timeoutMonitor");
@@ -286,8 +287,8 @@ public class AssignmentManager extends ZooKeeperListener {
 
   /**
    * Add a regionPlan for the specified region.
-   * @param encodedName 
-   * @param plan 
+   * @param encodedName
+   * @param plan
    */
   public void addPlan(String encodedName, RegionPlan plan) {
     synchronized (regionPlans) {
@@ -395,7 +396,7 @@ public class AssignmentManager extends ZooKeeperListener {
 
   /**
    * Process all regions that are in transition in zookeeper and also
-   * processes the list of dead servers by scanning the META. 
+   * processes the list of dead servers by scanning the META.
    * Used by master joining an cluster.
    * @param deadServers
    *          Map of dead servers and their regions. Can be null.
@@ -408,7 +409,7 @@ public class AssignmentManager extends ZooKeeperListener {
   throws KeeperException, IOException, InterruptedException {
     List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(watcher,
       watcher.assignmentZNode);
-    
+
     if (nodes == null) {
       String errorMessage = "Failed to get the children from ZK";
       master.abort(errorMessage, new IOException(errorMessage));
@@ -493,7 +494,7 @@ public class AssignmentManager extends ZooKeeperListener {
    * up in zookeeper.
    * @param encodedRegionName Region to process failover for.
    * @param regionInfo If null we'll go get it from meta table.
-   * @param deadServers Can be null 
+   * @param deadServers Can be null
    * @return True if we processed <code>regionInfo</code> as a RIT.
    * @throws KeeperException
    * @throws IOException
@@ -508,7 +509,7 @@ public class AssignmentManager extends ZooKeeperListener {
     if (data == null) return false;
     HRegionInfo hri = regionInfo;
     if (hri == null) {
-      if ((hri = getHRegionInfo(data)) == null) return false; 
+      if ((hri = getHRegionInfo(data)) == null) return false;
     }
     processRegionsInTransition(data, hri, deadServers, stat.getVersion());
     return true;
@@ -629,7 +630,7 @@ public class AssignmentManager extends ZooKeeperListener {
       }
     }
   }
-  
+
 
   /**
    * Put the region <code>hri</code> into an offline state up in zk.
@@ -836,7 +837,7 @@ public class AssignmentManager extends ZooKeeperListener {
           this.executorService.submit(new ClosedRegionHandler(master,
             this, regionState.getRegion()));
           break;
-          
+
         case RS_ZK_REGION_FAILED_OPEN:
           hri = checkIfInFailover(regionState, encodedName, data);
           if (hri != null) {
@@ -858,7 +859,7 @@ public class AssignmentManager extends ZooKeeperListener {
           // Handle this the same as if it were opened and then closed.
           regionState.update(RegionState.State.CLOSED,
               data.getStamp(), data.getOrigin());
-          // When there are more than one region server a new RS is selected as the 
+          // When there are more than one region server a new RS is selected as the
           // destination and the same is updated in the regionplan. (HBASE-5546)
           getRegionPlan(regionState, sn, true);
           this.executorService.submit(new ClosedRegionHandler(master,
@@ -866,7 +867,7 @@ public class AssignmentManager extends ZooKeeperListener {
           break;
 
         case RS_ZK_REGION_OPENING:
-          hri = checkIfInFailover(regionState, encodedName, data);       
+          hri = checkIfInFailover(regionState, encodedName, data);
           if (hri != null) {
             regionState = new RegionState(hri, RegionState.State.OPENING, data
                 .getStamp(), data.getOrigin());
@@ -941,11 +942,11 @@ public class AssignmentManager extends ZooKeeperListener {
     }
     return null;
   }
-  
+
   /**
    * Gets the HRegionInfo from the META table
    * @param  data
-   * @return HRegionInfo hri for the region 
+   * @return HRegionInfo hri for the region
    */
   private HRegionInfo getHRegionInfo(RegionTransitionData data) {
     Pair<HRegionInfo, ServerName> p = null;
@@ -1031,7 +1032,7 @@ public class AssignmentManager extends ZooKeeperListener {
 
   /**
    * Caller must hold lock on <code>this.regions</code>.
-   * @param serverName
+   * @param sn
    * @param encodedName
    * @return Found HRegionInfo or null.
    */
@@ -1246,13 +1247,13 @@ public class AssignmentManager extends ZooKeeperListener {
       ServerName oldSn = this.regions.get(regionInfo);
       if (oldSn != null) LOG.warn("Overwriting " + regionInfo.getEncodedName() +
         " on " + oldSn + " with " + sn);
-      
+
       if (isServerOnline(sn)) {
         this.regions.put(regionInfo, sn);
         addToServers(sn, regionInfo);
         this.regions.notifyAll();
       } else {
-        LOG.info("The server is not in online servers, ServerName=" + 
+        LOG.info("The server is not in online servers, ServerName=" +
           sn.getServerName() + ", region=" + regionInfo.getEncodedName());
       }
     }
@@ -1399,7 +1400,7 @@ public class AssignmentManager extends ZooKeeperListener {
    */
   public void assign(HRegionInfo region, boolean setOfflineInZK,
       boolean forceNewPlan, boolean hijack) {
-    // If hijack is true do not call disableRegionIfInRIT as 
+    // If hijack is true do not call disableRegionIfInRIT as
     // we have not yet moved the znode to OFFLINE state.
     if (!hijack && isDisabledorDisablingRegionInRIT(region)) {
       return;
@@ -1443,7 +1444,7 @@ public class AssignmentManager extends ZooKeeperListener {
           destination));
     }
     this.addPlans(plans);
-    
+
     // Presumption is that only this thread will be updating the state at this
     // time; i.e. handlers on backend won't be trying to set it to OPEN, etc.
     AtomicInteger counter = new AtomicInteger(0);
@@ -1666,11 +1667,11 @@ public class AssignmentManager extends ZooKeeperListener {
           }
         }
       }
-      
+
       if (setOfflineInZK && versionOfOfflineNode == -1) {
         return;
       }
-      
+
       if (this.master.isStopped()) {
         LOG.debug("Server stopped; skipping assign of " + state);
         return;
@@ -1683,7 +1684,7 @@ public class AssignmentManager extends ZooKeeperListener {
       }
       try {
         LOG.debug("Assigning region " + state.getRegion().getRegionNameAsString() +
-          " to " + plan.getDestination().toString());
+          " to " + plan.getDestination());
         // Transition RegionState to PENDING_OPEN
         state.update(RegionState.State.PENDING_OPEN, System.currentTimeMillis(),
             plan.getDestination());
@@ -1783,38 +1784,38 @@ public class AssignmentManager extends ZooKeeperListener {
 
   /**
    * Set region as OFFLINED up in zookeeper
-   * 
+   *
    * @param state
    * @param hijack
    *          - true if needs to be hijacked and reassigned, false otherwise.
-   * @param regionAlreadyInTransitionException  
-   *          - true if we need to retry assignment because of RegionAlreadyInTransitionException.       
+   * @param regionAlreadyInTransitionException
+   *          - true if we need to retry assignment because of RegionAlreadyInTransitionException.
    * @return the version of the offline node if setting of the OFFLINE node was
    *         successful, -1 otherwise.
    */
   int setOfflineInZooKeeper(final RegionState state, boolean hijack,
       boolean regionAlreadyInTransitionException) {
     // In case of reassignment the current state in memory need not be
-    // OFFLINE. 
+    // OFFLINE.
     if (!hijack && !state.isClosed() && !state.isOffline()) {
       if (!regionAlreadyInTransitionException ) {
         String msg = "Unexpected state : " + state + " .. Cannot transit it to OFFLINE.";
         this.master.abort(msg, new IllegalStateException(msg));
         return -1;
-      } 
+      }
       LOG.debug("Unexpected state : " + state
           + " but retrying to assign because RegionAlreadyInTransitionException.");
     }
     boolean allowZNodeCreation = false;
     // Under reassignment if the current state is PENDING_OPEN
     // or OPENING then refresh the in-memory state to PENDING_OPEN. This is
-    // important because if the region was in 
+    // important because if the region was in
     // RS_OPENING state for a long time the master will try to force the znode
     // to OFFLINE state meanwhile the RS could have opened the corresponding
     // region and the state in znode will be RS_ZK_REGION_OPENED.
     // For all other cases we can change the in-memory state to OFFLINE.
     if (hijack &&
-        (state.getState().equals(RegionState.State.PENDING_OPEN) || 
+        (state.getState().equals(RegionState.State.PENDING_OPEN) ||
             state.getState().equals(RegionState.State.OPENING))) {
       state.update(RegionState.State.PENDING_OPEN);
       allowZNodeCreation = false;
@@ -1825,7 +1826,7 @@ public class AssignmentManager extends ZooKeeperListener {
     int versionOfOfflineNode = -1;
     try {
       // get the version after setting the znode to OFFLINE
-      versionOfOfflineNode = ZKAssign.createOrForceNodeOffline(master.getZooKeeper(), 
+      versionOfOfflineNode = ZKAssign.createOrForceNodeOffline(master.getZooKeeper(),
           state.getRegion(), this.master.getServerName(),
           hijack, allowZNodeCreation);
       if (versionOfOfflineNode == -1) {
@@ -1861,7 +1862,7 @@ public class AssignmentManager extends ZooKeeperListener {
     } catch (KeeperException e) {
       if (e instanceof NodeExistsException) {
         LOG.warn("Node for " + state.getRegion() + " already exists");
-      } else { 
+      } else {
         master.abort("Unexpected ZK exception creating/setting node OFFLINE", e);
       }
       return false;
@@ -1933,8 +1934,13 @@ public class AssignmentManager extends ZooKeeperListener {
           || existingPlan.getDestination() == null
           || drainingServers.contains(existingPlan.getDestination())) {
         newPlan = true;
-        randomPlan = new RegionPlan(state.getRegion(), null, balancer
-            .randomAssignment(servers));
+        try {
+          randomPlan = new RegionPlan(state.getRegion(), null, balancer
+              .randomAssignment(state.getRegion(), servers));
+        } catch (IOException ex) {
+          LOG.warn("Failed to create new plan.",ex);
+          return null;
+        }
         this.regionPlans.put(encodedName, randomPlan);
       }
     }
@@ -2094,8 +2100,8 @@ public class AssignmentManager extends ZooKeeperListener {
         state = new RegionState(region, RegionState.State.PENDING_CLOSE);
         regionsInTransition.put(encodedName, state);
       } else if (force && (state.isPendingClose() || state.isClosing())) {
-        LOG.debug("Attempting to unassign region " + region.getRegionNameAsString() + 
-          " which is already " + state.getState()  + 
+        LOG.debug("Attempting to unassign region " + region.getRegionNameAsString() +
+          " which is already " + state.getState()  +
           " but forcing to send a CLOSE RPC again ");
         state.update(state.getState());
       } else {
@@ -2104,7 +2110,7 @@ public class AssignmentManager extends ZooKeeperListener {
           "already in transition (" + state.getState() + ", force=" + force + ")");
         return;
       }
-    } 
+    }
     // Send CLOSE RPC
     ServerName server = null;
     synchronized (this.regions) {
@@ -2178,9 +2184,9 @@ public class AssignmentManager extends ZooKeeperListener {
       // Presume retry or server will expire.
     }
   }
-  
+
   /**
-   * 
+   *
    * @param region regioninfo of znode to be deleted.
    */
   public void deleteClosingOrClosedNode(HRegionInfo region) {
@@ -2279,7 +2285,7 @@ public class AssignmentManager extends ZooKeeperListener {
 
   /**
    * Assigns all user regions to online servers. Use round-robin assignment.
-   * 
+   *
    * @param regions
    * @throws IOException
    * @throws InterruptedException
@@ -2320,7 +2326,7 @@ public class AssignmentManager extends ZooKeeperListener {
     boolean isTableEnabled = this.zkTable.isEnabledTable(tableName);
     if (!isTableEnabled) {
       setEnabledTable(tableName);
-    }    
+    }
   }
 
   /**
@@ -2436,12 +2442,18 @@ public class AssignmentManager extends ZooKeeperListener {
 
     @Override
     protected long getTimeoutOnRIT() {
-      // Guess timeout.  Multiply the number of regions on a random server
-      // by how long we thing one region takes opening.
       long perRegionOpenTimeGuesstimate =
         this.server.getConfiguration().getLong("hbase.bulk.assignment.perregion.open.time", 1000);
-      int regionsPerServer =
-        this.bulkPlan.entrySet().iterator().next().getValue().size();
+
+      int regionsPerServer = 0;
+      int serverCount = 0;
+      for(List<HRegionInfo> el: this.bulkPlan.values()) {
+        regionsPerServer += el.size();
+        if(el.size() > 0) {
+          serverCount++;
+        }
+      }
+      regionsPerServer = regionsPerServer/serverCount+1;
       long timeout = perRegionOpenTimeGuesstimate * regionsPerServer;
       LOG.debug("Timeout-on-RIT=" + timeout);
       return timeout;
@@ -2564,7 +2576,7 @@ public class AssignmentManager extends ZooKeeperListener {
     // Region assignment from META
     List<Result> results = MetaReader.fullScan(this.catalogTracker);
     // Get any new but slow to checkin region server that joined the cluster
-    Set<ServerName> onlineServers = serverManager.getOnlineServers().keySet();    
+    Set<ServerName> onlineServers = serverManager.getOnlineServers().keySet();
     // Map of offline servers and their regions to be returned
     Map<ServerName, List<Pair<HRegionInfo,Result>>> offlineServers =
       new TreeMap<ServerName, List<Pair<HRegionInfo, Result>>>();
@@ -2621,7 +2633,7 @@ public class AssignmentManager extends ZooKeeperListener {
           byte[] data = ZKUtil.getDataNoWatch(this.watcher, node, stat);
           // If znode does not exist dont consider this region
           if (data == null) {
-            LOG.debug("Region "+ regionInfo.getRegionNameAsString() + " split is completed. " 
+            LOG.debug("Region "+ regionInfo.getRegionNameAsString() + " split is completed. "
                 + "Hence need not add to regions list");
             continue;
           }
@@ -2679,14 +2691,14 @@ public class AssignmentManager extends ZooKeeperListener {
         this.enablingTables.put(tableName, new ArrayList<HRegionInfo>());
       } 
       return true;
-    } 
+    }
     return false;
   }
 
   /**
    * Recover the tables that were not fully moved to DISABLED state. These
    * tables are in DISABLING state when the master restarted/switched.
-   * 
+   *
    * @param disablingTables
    * @return
    * @throws KeeperException
@@ -2716,7 +2728,7 @@ public class AssignmentManager extends ZooKeeperListener {
   /**
    * Recover the tables that are not fully moved to ENABLED state. These tables
    * are in ENABLING state when the master restarted/switched
-   * 
+   *
    * @param enablingTables
    * @param isWatcherCreated
    * @throws KeeperException
@@ -2763,10 +2775,10 @@ public class AssignmentManager extends ZooKeeperListener {
    * Processes list of dead servers from result of META scan and regions in RIT
    * <p>
    * This is used for failover to recover the lost regions that belonged to
-   * RegionServers which failed while there was no active master or regions 
+   * RegionServers which failed while there was no active master or regions
    * that were in RIT.
    * <p>
-   * 
+   *
    * @param deadServers
    *          The list of dead servers which failed while there was no active
    *          master. Can be null.
@@ -2780,7 +2792,7 @@ public class AssignmentManager extends ZooKeeperListener {
       List<String> nodes) throws IOException, KeeperException {
     if (null != deadServers) {
       Set<ServerName> actualDeadServers = this.serverManager.getDeadServers();
-      for (Map.Entry<ServerName, List<Pair<HRegionInfo, Result>>> deadServer : 
+      for (Map.Entry<ServerName, List<Pair<HRegionInfo, Result>>> deadServer :
         deadServers.entrySet()) {
         // skip regions of dead servers because SSH will process regions during rs expiration.
         // see HBASE-5916
@@ -2804,7 +2816,7 @@ public class AssignmentManager extends ZooKeeperListener {
             // we consider that this region is being handled.
             // So we should skip it and process it in
             // processRegionsInTransition.
-            if (data != null && data.getOrigin() != null && 
+            if (data != null && data.getOrigin() != null &&
                 serverManager.isServerOnline(data.getOrigin())) {
               LOG.info("The region " + regionInfo.getEncodedName()
                   + "is being handled on " + data.getOrigin());
@@ -2940,7 +2952,7 @@ public class AssignmentManager extends ZooKeeperListener {
    */
   public List<HRegionInfo> getRegionsOfTable(byte[] tableName) {
     List<HRegionInfo> tableRegions = new ArrayList<HRegionInfo>();
-    // boundary needs to have table's name but regionID 0 so that it is sorted 
+    // boundary needs to have table's name but regionID 0 so that it is sorted
     // before all table's regions.
     HRegionInfo boundary =
       new HRegionInfo(tableName, null, null, false, 0L);
@@ -3103,7 +3115,7 @@ public class AssignmentManager extends ZooKeeperListener {
       }
     }
   }
-  
+
   private void processOpeningState(HRegionInfo regionInfo) {
     LOG.info("Region has been OPENING for too " + "long, reassigning region="
         + regionInfo.getRegionNameAsString());
@@ -3268,7 +3280,7 @@ public class AssignmentManager extends ZooKeeperListener {
    * Can't let out original since it can change and at least the loadbalancer
    * wants to iterate this exported list.  We need to synchronize on regions
    * since all access to this.servers is under a lock on this.regions.
-   * 
+   *
    * @return A clone of current assignments by table.
    */
   Map<String, Map<ServerName, List<HRegionInfo>>> getAssignmentsByTable() {
@@ -3311,13 +3323,14 @@ public class AssignmentManager extends ZooKeeperListener {
     }
     return result;
   }
-  
+
   /**
    * @return A clone of current assignments. Note, this is assignments only.
    * If a new server has come in and it has no regions, it will not be included
    * in the returned Map.
    */
-  Map<ServerName, List<HRegionInfo>> getAssignments() {
+  @InterfaceAudience.Private
+  public Map<ServerName, List<HRegionInfo>> getAssignments() {
     // This is an EXPENSIVE clone.  Cloning though is the safest thing to do.
     // Can't let out original since it can change and at least the loadbalancer
     // wants to iterate this exported list.  We need to synchronize on regions
@@ -3481,7 +3494,7 @@ public class AssignmentManager extends ZooKeeperListener {
     public boolean isSplitting() {
       return state == State.SPLITTING;
     }
- 
+
     public boolean isSplit() {
       return state == State.SPLIT;
     }
@@ -3495,12 +3508,12 @@ public class AssignmentManager extends ZooKeeperListener {
     }
 
     /**
-     * A slower (but more easy-to-read) stringification 
+     * A slower (but more easy-to-read) stringification
      */
     public String toDescriptiveString() {
       long lstamp = stamp.get();
       long relTime = System.currentTimeMillis() - lstamp;
-      
+
       return region.getRegionNameAsString()
         + " state=" + state
         + ", ts=" + new Date(lstamp) + " (" + (relTime/1000) + "s ago)"
@@ -3527,10 +3540,10 @@ public class AssignmentManager extends ZooKeeperListener {
     this.timeoutMonitor.interrupt();
     this.timerUpdater.interrupt();
   }
-  
+
   /**
    * Check whether the RegionServer is online.
-   * @param serverName 
+   * @param serverName
    * @return True if online.
    */
   public boolean isServerOnline(ServerName serverName) {
diff --git src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
index 82a33e0..a5af1e1 100644
--- src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
+++ src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
@@ -43,7 +43,6 @@ import org.apache.hadoop.hbase.HDFSBlocksDistribution;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -118,7 +117,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
 
 
    RegionInfoComparator riComparator = new RegionInfoComparator();
-   
+
    private class RegionPlanComparator implements Comparator<RegionPlan> {
     @Override
     public int compare(RegionPlan l, RegionPlan r) {
@@ -141,21 +140,21 @@ public class DefaultLoadBalancer implements LoadBalancer {
    * have either floor(average) or ceiling(average) regions.
    *
    * HBASE-3609 Modeled regionsToMove using Guava's MinMaxPriorityQueue so that
-   *   we can fetch from both ends of the queue. 
-   * At the beginning, we check whether there was empty region server 
+   *   we can fetch from both ends of the queue.
+   * At the beginning, we check whether there was empty region server
    *   just discovered by Master. If so, we alternately choose new / old
    *   regions from head / tail of regionsToMove, respectively. This alternation
    *   avoids clustering young regions on the newly discovered region server.
    *   Otherwise, we choose new regions from head of regionsToMove.
-   *   
+   *
    * Another improvement from HBASE-3609 is that we assign regions from
    *   regionsToMove to underloaded servers in round-robin fashion.
    *   Previously one underloaded server would be filled before we move onto
    *   the next underloaded server, leading to clustering of young regions.
-   *   
+   *
    * Finally, we randomly shuffle underloaded servers so that they receive
    *   offloaded regions relatively evenly across calls to balanceCluster().
-   *         
+   *
    * The algorithm is currently implemented as such:
    *
    * <ol>
@@ -289,7 +288,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
       List<HRegionInfo> regions = server.getValue();
       int numToOffload = Math.min(regionCount - max, regions.size());
       // account for the out-of-band regions which were assigned to this server
-      // after some other region server crashed 
+      // after some other region server crashed
       Collections.sort(regions, riComparator);
       int numTaken = 0;
       for (int i = 0; i <= numToOffload; ) {
@@ -587,7 +586,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
     // Group all of the old assignments by their hostname.
     // We can't group directly by ServerName since the servers all have
     // new start-codes.
-    
+
     // Group the servers by their hostname. It's possible we have multiple
     // servers on the same host on different ports.
     ArrayListMultimap<String, ServerName> serversByHostname =
@@ -595,20 +594,20 @@ public class DefaultLoadBalancer implements LoadBalancer {
     for (ServerName server : servers) {
       serversByHostname.put(server.getHostname(), server);
     }
-    
+
     // Now come up with new assignments
     Map<ServerName, List<HRegionInfo>> assignments =
       new TreeMap<ServerName, List<HRegionInfo>>();
-    
+
     for (ServerName server : servers) {
       assignments.put(server, new ArrayList<HRegionInfo>());
     }
-    
+
     // Collection of the hostnames that used to have regions
     // assigned, but for which we no longer have any RS running
     // after the cluster restart.
     Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();
-    
+
     int numRandomAssignments = 0;
     int numRetainedAssigments = 0;
     for (Map.Entry<HRegionInfo, ServerName> entry : regions.entrySet()) {
@@ -637,7 +636,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
         numRetainedAssigments++;
       }
     }
-    
+
     String randomAssignMsg = "";
     if (numRandomAssignments > 0) {
       randomAssignMsg = numRandomAssignments + " regions were assigned " +
@@ -645,7 +644,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
       		"longer present in the cluster. These hosts were:\n  " +
           Joiner.on("\n  ").join(oldHostsNoLongerPresent);
     }
-    
+
     LOG.info("Reassigned " + regions.size() + " regions. " +
         numRetainedAssigments + " retained the pre-restart assignment. " +
         randomAssignMsg);
@@ -680,7 +679,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
       LOG.debug("IOException during HDFSBlocksDistribution computation. for " +
         "region = " + region.getEncodedName() , ioe);
     }
-    
+
     return topServerNames;
   }
 
@@ -712,7 +711,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
    * order as input hosts.
    * @param hosts the list of hosts
    * @return ServerName list
-   */  
+   */
   private List<ServerName> mapHostNameToServerName(List<String> hosts) {
     if ( hosts == null || status == null) {
       return null;
@@ -768,7 +767,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
     return assignments;
   }
 
-  public ServerName randomAssignment(List<ServerName> servers) {
+  public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
     if (servers == null || servers.isEmpty()) {
       LOG.warn("Wanted to do random assignment but no servers to assign to");
       return null;
@@ -776,4 +775,8 @@ public class DefaultLoadBalancer implements LoadBalancer {
     return servers.get(RANDOM.nextInt(servers.size()));
   }
 
+  @Override
+  public void configure() throws IOException {
+  }
+
 }
diff --git src/main/java/org/apache/hadoop/hbase/master/HMaster.java src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index c680630..563fbdc 100644
--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -42,6 +42,7 @@ import java.util.concurrent.atomic.AtomicReference;
 import javax.management.ObjectName;
 
 import com.google.common.collect.ClassToInstanceMap;
+import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.collect.MutableClassToInstanceMap;
 import org.apache.commons.logging.Log;
@@ -185,7 +186,7 @@ Server {
   private CatalogTracker catalogTracker;
   // Cluster status zk tracker and local setter
   private ClusterStatusTracker clusterStatusTracker;
-  
+
   // buffer for "fatal error" notices from region servers
   // in the cluster. This is only used for assisting
   // operations/debugging.
@@ -266,7 +267,7 @@ Server {
     // Creation of a HSA will force a resolve.
     InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
     if (initialIsa.getAddress() == null) {
-      throw new IllegalArgumentException("Failed resolve of " + this.isa);
+      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
     }
     int numHandlers = conf.getInt("hbase.master.handler.count",
       conf.getInt("hbase.regionserver.handler.count", 25));
@@ -340,7 +341,7 @@ Server {
         "(Also watching cluster state node)");
       Thread.sleep(c.getInt("zookeeper.session.timeout", 180 * 1000));
     }
-    
+
   }
 
   /**
@@ -378,7 +379,7 @@ Server {
       }
     } catch (Throwable t) {
       // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility
-      if (t instanceof NoClassDefFoundError && 
+      if (t instanceof NoClassDefFoundError &&
           t.getMessage().contains("org/apache/hadoop/hdfs/protocol/FSConstants$SafeModeAction")) {
           // improved error message for this special case
           abort("HBase is having a problem with its Hadoop jars.  You may need to "
@@ -390,7 +391,7 @@ Server {
       }
     } finally {
       startupStatus.cleanup();
-      
+
       stopChores();
       // Wait for all the remaining region servers to report in IFF we were
       // running a cluster shutdown AND we were NOT aborting.
@@ -412,7 +413,7 @@ Server {
 
   /**
    * Try becoming active master.
-   * @param startupStatus 
+   * @param startupStatus
    * @return True if we could successfully become the active master.
    * @throws InterruptedException
    */
@@ -490,7 +491,7 @@ Server {
    * <li>Ensure assignment of root and meta regions<li>
    * <li>Handle either fresh cluster start or master failover</li>
    * </ol>
-   * @param masterRecovery 
+   * @param masterRecovery
    *
    * @throws IOException
    * @throws InterruptedException
@@ -527,7 +528,7 @@ Server {
 
     status.setStatus("Initializing ZK system trackers");
     initializeZKBasedSystemTrackers();
-    
+
     if (!masterRecovery) {
       // initialize master side coprocessors before we start handling requests
       status.setStatus("Initializing master coprocessors");
@@ -556,6 +557,10 @@ Server {
     status.setStatus("Splitting logs after master startup");
     splitLogAfterStartup(this.fileSystemManager);
 
+    this.balancer.setClusterStatus(getClusterStatus());
+    this.balancer.setMasterServices(this);
+    this.balancer.configure();
+
     // Make sure root and meta assigned before proceeding.
     assignRootAndMeta(status);
     enableServerShutdownHandler();
@@ -571,9 +576,6 @@ Server {
     status.setStatus("Starting assignment manager");
     this.assignmentManager.joinCluster();
 
-    this.balancer.setClusterStatus(getClusterStatus());
-    this.balancer.setMasterServices(this);
-
     // Fixing up missing daughters if any
     status.setStatus("Fixing up missing daughters");
     fixupDaughters(status);
@@ -596,7 +598,7 @@ Server {
     // removing dead server with same hostname and port of rs which is trying to check in before
     // master initialization. See HBASE-5916.
     this.serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer();
-    
+
     if (!masterRecovery) {
       if (this.cpHost != null) {
         // don't let cp initialization errors kill the master
@@ -608,11 +610,11 @@ Server {
       }
     }
   }
-  
+
   /**
    * If ServerShutdownHandler is disabled, we enable it and expire those dead
    * but not expired servers.
-   * 
+   *
    * @throws IOException
    */
   private void enableServerShutdownHandler() throws IOException {
@@ -621,7 +623,7 @@ Server {
       this.serverManager.expireDeadNotExpiredServers();
     }
   }
-  
+
   /**
    * Useful for testing purpose also where we have
    * master restart scenarios.
@@ -847,7 +849,7 @@ Server {
    *  need to install an unexpected exception handler.
    */
   private void startServiceThreads() throws IOException{
- 
+
    // Start the executor service pools
    this.executorService.startExecutorService(ExecutorType.MASTER_OPEN_REGION,
       conf.getInt("hbase.master.executor.openregion.threads", 5));
@@ -857,7 +859,7 @@ Server {
       conf.getInt("hbase.master.executor.serverops.threads", 3));
    this.executorService.startExecutorService(ExecutorType.MASTER_META_SERVER_OPERATIONS,
       conf.getInt("hbase.master.executor.serverops.threads", 5));
-   
+
    // We depend on there being only one instance of this executor running
    // at a time.  To do concurrency, would need fencing of enable/disable of
    // tables.
@@ -1061,10 +1063,17 @@ Server {
         this.assignmentManager.getAssignmentsByTable();
 
       List<RegionPlan> plans = new ArrayList<RegionPlan>();
-      for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
-        List<RegionPlan> partialPlans = this.balancer.balanceCluster(assignments);
-        if (partialPlans != null) plans.addAll(partialPlans);
+      try {
+        for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
+          List<RegionPlan> partialPlans = null;
+          partialPlans = this.balancer.balanceCluster(assignments);
+          if (partialPlans != null) plans.addAll(partialPlans);
+        }
+      } catch (IOException e) {
+        LOG.warn("Failed to retrieve balance plan", e);
+        return false;
       }
+
       int rpCount = 0;  // number of RegionPlans balanced so far
       long totalRegPlanExecTime = 0;
       balancerRan = plans != null;
@@ -1114,11 +1123,11 @@ Server {
         newValue = this.cpHost.preBalanceSwitch(newValue);
       }
       if (mode == BalanceSwitchMode.SYNC) {
-        synchronized (this.balancer) {        
+        synchronized (this.balancer) {
           this.balanceSwitch = newValue;
         }
       } else {
-        this.balanceSwitch = newValue;        
+        this.balanceSwitch = newValue;
       }
       LOG.info("BalanceSwitch=" + newValue);
       if (this.cpHost != null) {
@@ -1127,14 +1136,14 @@ Server {
     } catch (IOException ioe) {
       LOG.warn("Error flipping balance switch", ioe);
     }
-    return oldValue;    
+    return oldValue;
   }
-  
+
   @Override
   public boolean synchronousBalanceSwitch(final boolean b) {
     return switchBalancer(b, BalanceSwitchMode.SYNC);
   }
-  
+
   @Override
   public boolean balanceSwitch(final boolean b) {
     return switchBalancer(b, BalanceSwitchMode.ASYNC);
@@ -1152,7 +1161,7 @@ Server {
 
   @Override
   public void move(final byte[] encodedRegionName, final byte[] destServerName)
-  throws UnknownRegionException {
+  throws IOException {
     Pair<HRegionInfo, ServerName> p =
       this.assignmentManager.getAssignment(encodedRegionName);
     if (p == null)
@@ -1162,15 +1171,15 @@ Server {
       LOG.info("Passed destination servername is null or empty so choosing a server at random");
       List<ServerName> destServers = this.serverManager.getOnlineServersList();
       destServers.remove(p.getSecond());
-      // If i have only one RS then destination can be null.
-      dest = balancer.randomAssignment(destServers);
+      dest = balancer.randomAssignment(p.getFirst(), destServers);
     } else {
-      dest = new ServerName(Bytes.toString(destServerName));
+      ServerName candidate = new ServerName(Bytes.toString(destServerName));
+      dest = balancer.randomAssignment(p.getFirst(), Lists.newArrayList(candidate));
     }
     
     // Now we can do the move
     RegionPlan rp = new RegionPlan(p.getFirst(), p.getSecond(), dest);
-    
+
     try {
       if (this.cpHost != null) {
         if (this.cpHost.preMove(p.getFirst(), p.getSecond(), dest)) {
@@ -1257,7 +1266,7 @@ Server {
    * @return Pair indicating the number of regions updated Pair.getFirst is the
    *         regions that are yet to be updated Pair.getSecond is the total number
    *         of regions of the table
-   * @throws IOException 
+   * @throws IOException
    */
   public Pair<Integer, Integer> getAlterStatus(byte[] tableName)
   throws IOException {
@@ -1605,7 +1614,7 @@ Server {
   public AssignmentManager getAssignmentManager() {
     return this.assignmentManager;
   }
-  
+
   public MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer() {
     return rsFatals;
   }
@@ -1669,13 +1678,13 @@ Server {
   public boolean isAborted() {
     return this.abort;
   }
-  
+
   void checkInitialized() throws PleaseHoldException {
     if (!this.initialized) {
       throw new PleaseHoldException("Master is initializing");
     }
   }
-  
+
   /**
    * Report whether this master is currently the active master or not.
    * If not active master, we are parked on ZK waiting to become active.
@@ -1733,8 +1742,8 @@ Server {
       cpHost.postAssign(pair.getFirst());
     }
   }
-  
-  
+
+
 
   public void assignRegion(HRegionInfo hri) {
     assignmentManager.assign(hri, true);
@@ -1746,7 +1755,16 @@ Server {
     checkInitialized();
     Pair<HRegionInfo, ServerName> pair =
       MetaReader.getRegion(this.catalogTracker, regionName);
-    if (pair == null) throw new UnknownRegionException(Bytes.toString(regionName));
+    if (Bytes.equals(HRegionInfo.ROOT_REGIONINFO.getRegionName(),regionName)) {
+      try {
+        pair = new Pair<HRegionInfo, ServerName>(HRegionInfo.ROOT_REGIONINFO, this.catalogTracker.getRootLocation());
+      } catch (InterruptedException e) {
+        throw new IOException(e);
+      }
+    }
+    if (pair == null) {
+      throw new UnknownRegionException(Bytes.toString(regionName));
+    }
     HRegionInfo hri = pair.getFirst();
     if (cpHost != null) {
       if (cpHost.preUnassign(hri, force)) {
@@ -1765,7 +1783,7 @@ Server {
   }
 
   /**
-   * Get HTD array for given tables 
+   * Get HTD array for given tables
    * @param tableNames
    * @return HTableDescriptor[]
    */
@@ -1809,6 +1827,11 @@ Server {
   }
 
   @Override
+  public LoadBalancer getLoadBalancer() {
+    return balancer;
+  }
+
+  @Override
   public ExecResult execCoprocessor(Exec call) throws IOException {
     Class<? extends CoprocessorProtocol> protocol = call.getProtocol();
     if (protocol == null) {
diff --git src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
index 7d2dd74..c60a3ee 100644
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
 
+import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
@@ -59,11 +60,18 @@ public interface LoadBalancer extends Configurable {
   public void setMasterServices(MasterServices masterServices);
 
   /**
+   * Configure the load balancer. Must be called after setters.
+   *
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  public void configure() throws IOException;
+
+  /**
    * Perform the major balance operation
    * @param clusterState
    * @return List of plans
    */
-  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState);
+  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState) throws IOException;
 
   /**
    * Perform a Round Robin assignment of regions.
@@ -71,7 +79,8 @@ public interface LoadBalancer extends Configurable {
    * @param servers
    * @return Map of servername to regioninfos
    */
-  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions, List<ServerName> servers);
+  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions,
+                                                                 List<ServerName> servers) throws IOException;
 
   /**
    * Assign regions to the previously hosting region server
@@ -79,7 +88,8 @@ public interface LoadBalancer extends Configurable {
    * @param servers
    * @return List of plans
    */
-  public Map<ServerName, List<HRegionInfo>> retainAssignment(Map<HRegionInfo, ServerName> regions, List<ServerName> servers);
+  public Map<ServerName, List<HRegionInfo>> retainAssignment(Map<HRegionInfo, ServerName> regions,
+                                                             List<ServerName> servers) throws IOException;
 
   /**
    * Sync assign a region
@@ -87,12 +97,14 @@ public interface LoadBalancer extends Configurable {
    * @param servers
     * @return Map regioninfos to servernames
    */
-  public Map<HRegionInfo, ServerName> immediateAssignment(List<HRegionInfo> regions, List<ServerName> servers);
+  public Map<HRegionInfo, ServerName> immediateAssignment(List<HRegionInfo> regions,
+                                                          List<ServerName> servers) throws IOException;
 
   /**
-   * Get a random region server from the list
+   * Get a random region server for the region from the list
+   * @param region
    * @param servers
    * @return Servername
    */
-  public ServerName randomAssignment(List<ServerName> servers);
+  public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) throws IOException;
 }
diff --git src/main/java/org/apache/hadoop/hbase/master/MasterServices.java src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
index eb0e540..7873210 100644
--- src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
+++ src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
@@ -161,4 +161,9 @@ public interface MasterServices extends Server {
    */
   public <T extends CoprocessorProtocol> boolean registerProtocol(
       Class<T> protocol, T handler);
+
+  /**
+   * @return load balancer
+   */
+  public LoadBalancer getLoadBalancer();
 }
diff --git src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index dc0883b..fe6e4f6 100644
--- src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -20,6 +20,7 @@
 package org.apache.hadoop.hbase;
 
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 import java.io.File;
 import java.io.IOException;
@@ -30,16 +31,20 @@ import java.net.ServerSocket;
 import java.net.Socket;
 import java.net.UnknownHostException;
 import java.security.MessageDigest;
+import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
 import java.util.Random;
+import java.util.TreeMap;
 import java.util.UUID;
 
+import com.google.common.collect.Maps;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.commons.logging.impl.Jdk14Logger;
@@ -2160,4 +2165,62 @@ public class HBaseTestingUtility {
     return region;
   }
 
+  public static void waitForCondition(PrivilegedExceptionAction<Boolean> action) throws Exception {
+      waitForCondition(5*60000, action);
+  }
+
+  public static void waitForCondition(long timeout, PrivilegedExceptionAction<Boolean> action) throws Exception {
+    long sleepInterval = 100;
+    long tries = timeout/sleepInterval;
+    int i = 0;
+    while(action.run()) {
+      if(i==0) {
+        StackTraceElement el = Thread.currentThread().getStackTrace()[2];
+        if(el.getMethodName().equals("waitForCondition")) {
+          el = Thread.currentThread().getStackTrace()[3];
+        }
+        LOG.info("Waiting for method: "+el.getClassName()+"."+
+            el.getMethodName()+"("+el.getFileName()+":"+el.getLineNumber()+")");
+      }
+      Thread.sleep(sleepInterval);
+      if(tries-- < 0) {
+        fail("Timeout");
+      }
+      i = (i+1) % 10;
+    }
+  }
+
+  public Map<String,List<String>> getTableRegionMap() throws IOException {
+    Map<String,List<String>> map = Maps.newTreeMap();
+    Map<String,Map<ServerName,List<String>>> tableServerRegionMap
+        = getTableServerRegionMap();
+    for(String tableName : tableServerRegionMap.keySet()) {
+      if(!map.containsKey(tableName)) {
+        map.put(tableName, new LinkedList<String>());
+      }
+      for(List<String> subset: tableServerRegionMap.get(tableName).values()) {
+        map.get(tableName).addAll(subset);
+      }
+    }
+    return map;
+  }
+
+  public Map<String,Map<ServerName,List<String>>> getTableServerRegionMap() throws IOException {
+    Map<String,Map<ServerName,List<String>>> map = Maps.newTreeMap();
+    ClusterStatus status = getHBaseClusterInterface().getClusterStatus();
+    for(ServerName serverName : status.getServers()) {
+      for(HServerLoad.RegionLoad rl : status.getLoad(serverName).getRegionsLoad().values()) {
+        String tableName = Bytes.toString(HRegionInfo.getTableName(rl.getName()));
+        if(!map.containsKey(tableName)) {
+          map.put(tableName, new TreeMap<ServerName, List<String>>());
+        }
+        if(!map.get(tableName).containsKey(serverName)) {
+          map.get(tableName).put(serverName, new LinkedList<String>());
+        }
+        map.get(tableName).get(serverName).add(rl.getNameAsString());
+      }
+    }
+    return map;
+  }
+
 }
diff --git src/test/java/org/apache/hadoop/hbase/group/IntegrationTestGroup.java src/test/java/org/apache/hadoop/hbase/group/IntegrationTestGroup.java
new file mode 100644
index 0000000..60e38ef
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/group/IntegrationTestGroup.java
@@ -0,0 +1,402 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.IntegrationTestingUtility;
+import org.apache.hadoop.hbase.IntegrationTests;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeSet;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+@Category(IntegrationTests.class)
+public class IntegrationTestGroup extends TestGroupsBase{
+  //Integration specific
+  private final static Log LOG = LogFactory.getLog(IntegrationTestGroup.class);
+  protected final static int NUM_SLAVES_BASE = 4; //number of slaves for the smallest cluster
+
+
+  @Before
+  public void beforeMethod() throws Exception {
+    LOG.info("Setting up IntegrationTestGroup");
+    LOG.info("Initializing cluster with " + NUM_SLAVES_BASE + " servers");
+    //set shared configs
+    TEST_UTIL = new IntegrationTestingUtility();
+    ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);
+    admin = TEST_UTIL.getHBaseAdmin();
+    cluster = TEST_UTIL.getHBaseClusterInterface();
+    groupAdmin = new VerifyingGroupAdminClient(TEST_UTIL.getConfiguration());
+    //cleanup previous artifacts
+    deleteTableIfNecessary();
+    deleteGroups();
+    LOG.info("Done initializing cluster");
+  }
+
+  @After
+  public void afterMethod() throws Exception {
+    LOG.info("Restoring the cluster");
+    ((IntegrationTestingUtility)TEST_UTIL).restoreCluster();
+    LOG.info("Done restoring the cluster");
+  }
+
+  @Test
+  public void testCreateMultiRegion() throws IOException {
+    LOG.info("testCreateMultiRegion");
+    byte[] tableName = Bytes.toBytes(tablePrefix+"_multi_table");
+    byte[] end = {1,3,5,7,9};
+    byte[] start = {0,2,4,6,8};
+    byte[][] f = {Bytes.toBytes("f")};
+    TEST_UTIL.createTable(tableName, f,1,start,end,10);
+  }
+
+  @Test
+  public void testCreateAndAssign() throws Exception {
+    LOG.info("testCreateAndAssign");
+
+    final byte[] tableName = Bytes.toBytes(tablePrefix+"_pre_table");
+    GroupInfo appInfo = addGroup(groupAdmin, "appInfo", 1);
+    final HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor("f"));
+    desc.setValue(GroupInfo.TABLEDESC_PROP_GROUP, appInfo.getName());
+    admin.createTable(desc);
+    //wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getTableRegionMap().get(desc.getNameAsString()) == null;
+      }
+    });
+    ServerName targetServer = ServerName.parseServerName(appInfo.getServers().iterator().next());
+    HRegionInterface targetRS = admin.getConnection().getHRegionConnection(targetServer.getHostname(),
+        targetServer.getPort());
+    //verify it was assigned to the right group
+    assertEquals(1, targetRS.getOnlineRegions().size());
+    //verify prop was not stored as part of the schema
+    assertNull(admin.getTableDescriptor(tableName).getValue(GroupInfo.TABLEDESC_PROP_GROUP));
+  }
+
+  @Test
+  public void testSimpleRegionServerMove() throws IOException,
+      InterruptedException {
+    LOG.info("testSimpleRegionServerMove");
+
+    GroupInfo appInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+    GroupInfo adminInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+    GroupInfo dInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(3, groupAdmin.listGroups().size());
+    assertEquals(1, adminInfo.getServers().size());
+    assertEquals(1, appInfo.getServers().size());
+    assertEquals(admin.getClusterStatus().getServers().size() - 2, dInfo.getServers().size());
+    groupAdmin.moveServers(appInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(appInfo.getName());
+    groupAdmin.moveServers(adminInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(adminInfo.getName());
+    assertTrue(groupAdmin.listGroups().size() == 1);
+  }
+
+  @Test
+  public void testMoveServers() throws Exception {
+    LOG.info("testMoveServers");
+
+    //create groups and assign servers
+    addGroup(groupAdmin, "bar", 3);
+    groupAdmin.addGroup("foo");
+
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    GroupInfo fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(0, fooGroup.getServers().size());
+
+    //test fail bogus server move
+    try {
+      groupAdmin.moveServers(Sets.newHashSet("foo:9999"),"foo");
+      fail("Bogus servers shouldn't have been successfully moved.");
+    } catch(IOException ex) {
+      assertTrue(ex.getMessage().contains("Server foo:9999 is not a member of any group."));
+    }
+
+    //test success case
+    LOG.info("moving servers "+barGroup.getServers()+" to group foo");
+    groupAdmin.moveServers(barGroup.getServers(), fooGroup.getName());
+
+    barGroup = groupAdmin.getGroupInfo("bar");
+    fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(0,barGroup.getServers().size());
+    assertEquals(3,fooGroup.getServers().size());
+
+    LOG.info("moving servers "+fooGroup.getServers()+" to group default");
+    groupAdmin.moveServers(fooGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return admin.getClusterStatus().getServers().size() !=
+            groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP).getServers().size();
+      }
+    });
+
+    fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(0,fooGroup.getServers().size());
+
+    //test group removal
+    LOG.info("Remove group "+barGroup.getName());
+    groupAdmin.removeGroup(barGroup.getName());
+    assertEquals(null, groupAdmin.getGroupInfo(barGroup.getName()));
+    LOG.info("Remove group "+fooGroup.getName());
+    groupAdmin.removeGroup(fooGroup.getName());
+    assertEquals(null, groupAdmin.getGroupInfo(fooGroup.getName()));
+  }
+
+  @Test
+  public void testTableMoveAndDrop() throws Exception {
+    LOG.info("testTableMove");
+
+    final String tableName = tablePrefix + rand.nextInt();
+    final byte[] tableNameBytes = Bytes.toBytes(tableName);
+    final byte[] familyNameBytes = Bytes.toBytes("f");
+    String newGroupName = "g_" + rand.nextInt();
+    final GroupInfo newGroup = addGroup(groupAdmin, newGroupName, 2);
+
+    HTable ht = TEST_UTIL.createTable(tableNameBytes, familyNameBytes);
+    assertEquals(4,
+        TEST_UTIL.createMultiRegions(TEST_UTIL.getConfiguration(), ht, familyNameBytes, 4));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        List<String> regions = TEST_UTIL.getTableRegionMap().get(tableName);
+        if (regions == null)
+          return true;
+        return TEST_UTIL.getTableRegionMap().get(tableName).size() < 5;
+      }
+    });
+
+    GroupInfo tableGrp = groupAdmin.getGroupInfoOfTable(tableName);
+    assertTrue(tableGrp.getName().equals(GroupInfo.DEFAULT_GROUP));
+
+    //change table's group
+    LOG.info("Moving table "+tableName+" to "+newGroup.getName());
+    groupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());
+
+    //verify group change
+    assertEquals(newGroup.getName(),
+        groupAdmin.getGroupInfoOfTable(tableName).getName());
+
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        Map<ServerName, List<String>> serverMap =
+            TEST_UTIL.getTableServerRegionMap().get(tableName);
+        boolean found = true;
+        int count = 0;
+        if(serverMap != null) {
+          for (ServerName rs : serverMap.keySet()) {
+            if (newGroup.containsServer(rs.getHostAndPort())) {
+              count += serverMap.get(rs).size();
+            }
+          }
+        }
+        return count != 5;
+      }
+    });
+
+    //verify removed table is removed from group
+    TEST_UTIL.deleteTable(tableNameBytes);
+    assertEquals(0, groupAdmin.getGroupInfo(newGroup.getName()).getTables().size());
+  }
+
+  @Test
+  public void testRegionMove() throws Exception {
+    LOG.info("testRegionMove");
+
+    final GroupInfo newGroup = addGroup(groupAdmin, "g_" + rand.nextInt(), 1);
+    final String tableName = tablePrefix + rand.nextInt();
+    final byte[] tableNameBytes = Bytes.toBytes(tableName);
+    final byte[] familyNameBytes = Bytes.toBytes("f");
+    HTable ht = TEST_UTIL.createTable(tableNameBytes, familyNameBytes);
+
+    // All the regions created below will be assigned to the default group.
+    assertEquals(5, TEST_UTIL.createMultiRegions(TEST_UTIL.getConfiguration(), ht, familyNameBytes, 5));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        List<String> regions = TEST_UTIL.getTableRegionMap().get(tableName);
+        if(regions == null)
+          return true;
+        return TEST_UTIL.getTableRegionMap().get(tableName).size() < 6;
+      }
+    });
+
+    //get target region to move
+    Map<ServerName,List<String>> assignMap =
+        TEST_UTIL.getTableServerRegionMap().get(tableName);
+    String targetRegion = null;
+    for(ServerName server : assignMap.keySet()) {
+      targetRegion = assignMap.get(server).size() > 0 ? assignMap.get(server).get(0) : null;
+      if(targetRegion != null) {
+        break;
+      }
+    }
+    //get server which is not a member of new group
+    ServerName targetServer = null;
+    for(ServerName server : admin.getClusterStatus().getServers()) {
+      if(!newGroup.containsServer(server.getHostAndPort())) {
+        targetServer = server;
+        break;
+      }
+    }
+    final HRegionInterface targetRS =
+        TEST_UTIL.getHBaseAdmin().getConnection().getHRegionConnection(targetServer.getHostname(), targetServer.getPort());
+
+    //move target server to group
+    groupAdmin.moveServers(Sets.newHashSet(targetServer.getHostAndPort()), newGroup.getName());
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return targetRS.getOnlineRegions().size() > 0;
+      }
+    });
+
+    // Lets move this region to the new group.
+    TEST_UTIL.getHBaseAdmin().move(Bytes.toBytes(HRegionInfo.encodeRegionName(Bytes.toBytes(targetRegion))),
+        Bytes.toBytes(targetServer.getServerName()));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return
+            TEST_UTIL.getTableRegionMap().get(tableName) != null &&
+                TEST_UTIL.getTableRegionMap().get(tableName).size() != 6 ||
+                admin.getClusterStatus().getRegionsInTransition().size() > 0;
+      }
+    });
+
+    //verify that targetServer didn't open it
+    assertFalse(targetRS.getOnlineRegions().contains(targetRegion));
+  }
+
+  @Test
+  public void testFailRemoveGroup() throws IOException, InterruptedException {
+    LOG.info("testFailRemoveGroup");
+
+    addGroup(groupAdmin, "bar", 3);
+    String tableName = tablePrefix+"_my_table";
+    TEST_UTIL.createTable(Bytes.toBytes(tableName), Bytes.toBytes("f"));
+    groupAdmin.moveTables(Sets.newHashSet(tableName), "bar");
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    //group is not empty therefore it should fail
+    try {
+      groupAdmin.removeGroup(barGroup.getName());
+      fail("Expected remove group to fail");
+    } catch(IOException e) {
+    }
+    //group cannot lose all it's servers therefore it should fail
+    try {
+      groupAdmin.moveServers(barGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+      fail("Expected move servers to fail");
+    } catch(IOException e) {
+    }
+
+    groupAdmin.moveTables(barGroup.getTables(), GroupInfo.DEFAULT_GROUP);
+    try {
+      groupAdmin.removeGroup(barGroup.getName());
+      fail("Expected move servers to fail");
+    } catch(IOException e) {
+    }
+
+    groupAdmin.moveServers(barGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(barGroup.getName());
+
+    assertEquals(1, groupAdmin.listGroups().size());
+  }
+
+  @Test
+  public void testKillRS() throws Exception {
+    LOG.info("testKillRS");
+
+    final byte[] tableName = Bytes.toBytes(tablePrefix + "_testKillRS");
+    GroupInfo appInfo = addGroup(groupAdmin, "appInfo", 1);
+    final HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor("f"));
+    desc.setValue(GroupInfo.TABLEDESC_PROP_GROUP, appInfo.getName());
+    admin.createTable(desc);
+    //wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getTableRegionMap().get(desc.getNameAsString()) == null;
+      }
+    });
+
+    ServerName targetServer = ServerName.parseServerName(appInfo.getServers().first());
+    HRegionInterface targetRS = admin.getConnection().getHRegionConnection(targetServer.getHostname(),
+        targetServer.getPort());
+    HRegionInfo targetRegion = targetRS.getOnlineRegions().get(0);
+    assertEquals(1, targetRS.getOnlineRegions().size());
+    targetRS.stop("die");
+
+    //wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return cluster.getClusterStatus().getRegionsInTransition().size() == 0;
+      }
+    });
+    TreeSet<String> newServers = Sets.newTreeSet();
+    newServers.add(groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP).getServers().first());
+    groupAdmin.moveServers(newServers, appInfo.getName());
+    admin.assign(targetRegion.getRegionName());
+
+    //wait for region to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return cluster.getClusterStatus().getRegionsInTransition().size() != 0;
+      }
+    });
+
+    targetServer = ServerName.parseServerName(newServers.first());
+    targetRS = admin.getConnection().getHRegionConnection(targetServer.getHostname(), targetServer.getPort());
+    assertEquals(1, targetRS.getOnlineRegions().size());
+    assertEquals(Bytes.toString(tableName), targetRS.getOnlineRegions().get(0).getTableNameAsString());
+  }
+}
diff --git src/test/java/org/apache/hadoop/hbase/group/TestGroups.java src/test/java/org/apache/hadoop/hbase/group/TestGroups.java
new file mode 100644
index 0000000..d100fab
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/group/TestGroups.java
@@ -0,0 +1,219 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.security.PrivilegedExceptionAction;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import static org.mockito.Mockito.*;
+
+@Category(MediumTests.class)
+public class TestGroups extends IntegrationTestGroup {
+  protected static final Log LOG = LogFactory.getLog(TestGroups.class);
+  private static HMaster master;
+  private static GroupAdminEndpoint groupEndpoint;
+
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.getConfiguration().set(
+        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+        GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName() + "," +
+            GroupAdminEndpoint.class.getName());
+    TEST_UTIL.startMiniCluster(NUM_SLAVES_BASE);
+    TEST_UTIL.getConfiguration().set(
+        ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART,
+        ""+NUM_SLAVES_BASE);
+
+    admin = TEST_UTIL.getHBaseAdmin();
+    cluster = TEST_UTIL.getHBaseCluster();
+    master = ((MiniHBaseCluster)cluster).getMaster();
+    groupAdmin = new VerifyingGroupAdminClient(TEST_UTIL.getConfiguration());
+    groupEndpoint =
+        (GroupAdminEndpoint)master.getCoprocessorHost().findCoprocessor(GroupAdminEndpoint.class.getName());
+
+    //wait for balancer to come online
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return !master.isInitialized() ||
+            !((GroupBasedLoadBalancer) master.getLoadBalancer()).isOnline();
+      }
+    });
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Override
+  public void beforeMethod() throws Exception {
+  }
+
+  @Override
+  public void afterMethod() throws Exception {
+    int missing = NUM_SLAVES_BASE - cluster.getClusterStatus().getServers().size();
+    for(int i=0; i<missing; i++) {
+      ((MiniHBaseCluster)cluster).startRegionServer();
+    }
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return cluster.getClusterStatus().getServers().size() != NUM_SLAVES_BASE;
+      }
+    });
+    deleteTableIfNecessary();
+    deleteGroups();
+  }
+
+  @Test
+  public void testJmx() throws Exception {
+    MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();
+    Iterator<ObjectName> it = mBeanServer.queryNames(new ObjectName("hadoop:name=Group,service=Group"), null).iterator();
+    //verify it was loaded properly
+    assertEquals("hadoop:name=Group,service=Group", it.next().getCanonicalName());
+
+    final MXBeanImpl info = MXBeanImpl.init(groupAdmin, master);
+    GroupInfo defaultGroup = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(1, info.getGroups().size());
+    assertEquals(defaultGroup.getName(), info.getGroups().get(0).getName());
+    assertEquals(defaultGroup.getServers(), Sets.newTreeSet(info.getGroups().get(0).getServers()));
+    assertEquals(defaultGroup.getServers(), Sets.newTreeSet(info.getServersByGroup().get(GroupInfo.DEFAULT_GROUP)));
+    assertEquals(0, info.getServersInTransition().size());
+
+    GroupInfo barGroup = addGroup(groupAdmin, "bar", 3);
+    String tableName1 = tablePrefix+"_testJmx1";
+    String tableName2 = tablePrefix+"_testJmx2";
+    TEST_UTIL.createTable(Bytes.toBytes(tableName1), Bytes.toBytes("f"));
+    TEST_UTIL.createTable(Bytes.toBytes(tableName2), Bytes.toBytes("f"));
+    groupAdmin.moveTables(Sets.newHashSet(tableName2), barGroup.getName());
+    assertEquals(2, info.getGroups().size());
+
+    int defaultIndex = -1;
+    int barIndex = -1;
+
+    for(int i=0; i<info.getGroups().size(); i++) {
+      MXBean.GroupInfoBean bean = info.getGroups().get(i);
+      if(bean.getName().equals(defaultGroup.getName())) {
+        defaultIndex = i;
+      }
+      else if(bean.getName().equals(barGroup.getName())) {
+        barIndex = i;
+      }
+    }
+
+    defaultGroup = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(defaultGroup.getName(),
+        info.getGroups().get(defaultIndex).getName());
+    assertEquals(defaultGroup.getTables(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getTables()));
+    assertEquals(defaultGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getServers()));
+    assertEquals(defaultGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getServers()));
+
+    barGroup = groupAdmin.getGroupInfo(barGroup.getName());
+    assertEquals(barGroup.getName(),
+        info.getGroups().get(barIndex).getName());
+    assertEquals(barGroup.getTables(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getTables()));
+    assertEquals(barGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getServers()));
+    assertEquals(barGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getServers()));
+  }
+
+  @Test
+  public void testBasicStartUp() throws IOException {
+    GroupInfo defaultInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(4, defaultInfo.getServers().size());
+    // Assignment of root and meta regions.
+    int count = 0;
+    for(List<HRegionInfo> info: master.getAssignmentManager().getAssignments().values()) {
+      count += info.size();
+    }
+    assertEquals(3, count);
+  }
+
+  @Test
+  public void testMoveServerWorker() throws Exception {
+    LOG.info("testMoveServerWorker");
+
+    //create groups and assign servers
+    addGroup(groupAdmin, "bar", 3);
+    groupAdmin.addGroup("foo");
+
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    GroupInfo fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(0, fooGroup.getServers().size());
+
+    //test failure in move server worker
+    MasterServices mockedMaster = Mockito.spy(master);
+    doThrow(new RuntimeException("testMoveServerWorker")).when(mockedMaster).getAssignmentManager();
+    Map<String,String> serversInTransition = new HashMap<String,String>();
+
+    Runnable failedRunnable =
+        new GroupMoveServerWorker(mockedMaster,
+            serversInTransition,
+            groupEndpoint.getGroupInfoManager(),
+            new GroupMoveServerWorker.MoveServerPlan(barGroup.getServers(), fooGroup.getName()));
+    Thread failedMoveServerThread = new Thread(failedRunnable);
+    failedMoveServerThread.start();
+    failedMoveServerThread.join();
+    verify(mockedMaster,atLeastOnce()).getAssignmentManager();
+    barGroup = groupAdmin.getGroupInfo("bar");
+    assertEquals(0, serversInTransition.size());
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(3, groupAdmin.listGroups().size());
+  }
+}
diff --git src/test/java/org/apache/hadoop/hbase/group/TestGroupsBase.java src/test/java/org/apache/hadoop/hbase/group/TestGroupsBase.java
new file mode 100644
index 0000000..1e06f41
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/group/TestGroupsBase.java
@@ -0,0 +1,95 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.hadoop.hbase.HBaseCluster;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import java.io.IOException;
+import java.security.SecureRandom;
+import java.util.HashSet;
+import java.util.Set;
+
+import static org.junit.Assert.assertTrue;
+
+public abstract class TestGroupsBase {
+  //shared
+  protected final static String groupPrefix = "Group-";
+  protected final static String tablePrefix = "Group-";
+  protected final static SecureRandom rand = new SecureRandom();
+
+  //shared, cluster type specific
+  protected static HBaseTestingUtility TEST_UTIL;
+  protected static HBaseAdmin admin;
+  protected static HBaseCluster cluster;
+  protected static GroupAdminClient groupAdmin;
+
+
+  protected GroupInfo addGroup(GroupAdminClient gAdmin, String groupName,
+                               int serverCount) throws IOException, InterruptedException {
+    GroupInfo defaultInfo = gAdmin
+        .getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertTrue(defaultInfo != null);
+    assertTrue(defaultInfo.getServers().size() >= serverCount);
+    gAdmin.addGroup(groupName);
+
+    Set<String> set = new HashSet<String>();
+    for(String server: defaultInfo.getServers()) {
+      if(set.size() == serverCount) {
+        break;
+      }
+      set.add(server);
+    }
+    gAdmin.moveServers(set, groupName);
+    GroupInfo result = gAdmin.getGroupInfo(groupName);
+    assertTrue(result.getServers().size() >= serverCount);
+    return result;
+  }
+
+  static void removeGroup(GroupAdminClient groupAdmin, String groupName) throws IOException {
+    GroupInfo groupInfo = groupAdmin.getGroupInfo(groupName);
+    for(String table: groupInfo.getTables()) {
+      byte[] bTable = Bytes.toBytes(table);
+      groupAdmin.moveTables(groupInfo.getTables(), GroupInfo.DEFAULT_GROUP);
+      groupAdmin.moveServers(groupInfo.getServers(), GroupInfo.DEFAULT_GROUP);
+    }
+    groupAdmin.removeGroup(groupName);
+  }
+
+  protected void deleteTableIfNecessary() throws IOException {
+    for (HTableDescriptor desc : TEST_UTIL.getHBaseAdmin().listTables(tablePrefix+".*")) {
+      TEST_UTIL.deleteTable(desc.getName());
+    }
+  }
+
+  protected void deleteGroups() throws IOException {
+    GroupAdminClient groupAdmin = new GroupAdminClient(TEST_UTIL.getConfiguration());
+    for(GroupInfo group: groupAdmin.listGroups()) {
+      if(!group.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+        groupAdmin.moveTables(group.getTables(),GroupInfo.DEFAULT_GROUP);
+        groupAdmin.moveServers(group.getServers(),GroupInfo.DEFAULT_GROUP);
+        groupAdmin.removeGroup(group.getName());
+      }
+    }
+  }
+}
diff --git src/test/java/org/apache/hadoop/hbase/group/TestGroupsOfflineMode.java src/test/java/org/apache/hadoop/hbase/group/TestGroupsOfflineMode.java
new file mode 100644
index 0000000..916442c
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/group/TestGroupsOfflineMode.java
@@ -0,0 +1,170 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.security.PrivilegedExceptionAction;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+@Category(MediumTests.class)
+public class TestGroupsOfflineMode extends TestGroupsBase {
+  private static final org.apache.commons.logging.Log LOG = LogFactory.getLog(TestGroupsOfflineMode.class);
+  private static HMaster master;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.getConfiguration().set(
+        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+        GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName()+","+
+            GroupAdminEndpoint.class.getName());
+    TEST_UTIL.getConfiguration().set(
+        ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART,
+        "1");
+    TEST_UTIL.startMiniCluster(2, 3);
+    cluster = TEST_UTIL.getHBaseCluster();
+    master = ((MiniHBaseCluster)cluster).getMaster();
+    master.balanceSwitch(false);
+    admin = TEST_UTIL.getHBaseAdmin();
+    //wait till the balancer is in online mode
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return !master.isInitialized() ||
+            !((GroupBasedLoadBalancer)master.getLoadBalancer()).isOnline() ||
+            master.getServerManager().getOnlineServersList().size() < 3;
+      }
+    });
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Test
+  public void testOffline() throws Exception, InterruptedException {
+    //table should be after group table name
+    //so it gets assigned later
+    final String failoverTable = "z"+GroupInfoManager.GROUP_TABLE_NAME;
+    TEST_UTIL.createTable(Bytes.toBytes(failoverTable), Bytes.toBytes("f"));
+
+    //adding testTable to special group so it gets assigned during offline mode
+    GroupBasedLoadBalancer.SPECIAL_TABLES.add(failoverTable);
+
+    GroupAdminClient groupAdmin = new GroupAdminClient(TEST_UTIL.getConfiguration());
+
+    final HRegionServer killRS = ((MiniHBaseCluster)cluster).getRegionServer(0);
+    final HRegionServer groupRS = ((MiniHBaseCluster)cluster).getRegionServer(1);
+    final HRegionServer failoverRS = ((MiniHBaseCluster)cluster).getRegionServer(2);
+
+    String newGroup =  "my_group";
+    groupAdmin.addGroup(newGroup);
+    if(cluster.getClusterStatus().getServers().contains(failoverRS.getServerName())) {
+      for(HRegionInfo  regionInfo:
+          master.getAssignmentManager().getAssignments().get(failoverRS.getServerName())) {
+        master.move(regionInfo.getEncodedNameAsBytes(),
+            Bytes.toBytes(killRS.getServerName().getServerName()));
+      }
+      LOG.info("Waiting for region unassignments on failover RS...");
+      TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+        @Override
+        public Boolean run() throws Exception {
+          return master.getAssignmentManager().getAssignments().get(failoverRS.getServerName()).size() > 0;
+        }
+      });
+    }
+
+    //move server to group and make sure all tables are assigned
+    groupAdmin.moveServers(Sets.newHashSet(groupRS.getServerName().getHostAndPort()), newGroup);
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return groupRS.getOnlineRegions().size() > 0 ||
+            master.getAssignmentManager().getRegionsInTransition().size() > 0;
+      }
+    });
+    //move table to group and wait
+    groupAdmin.moveTables(Sets.newHashSet(GroupInfoManager.GROUP_TABLE_NAME), newGroup);
+    LOG.info("Waiting for move table...");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return groupRS.getOnlineRegions().size() < 1;
+      }
+    });
+    groupRS.stop("die");
+    //race condition here
+    TEST_UTIL.getHBaseCluster().getMaster().stopMaster();
+    LOG.info("Waiting for offline mode...");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getHBaseCluster().getMaster() == null ||
+            !TEST_UTIL.getHBaseCluster().getMaster().isActiveMaster() ||
+            !TEST_UTIL.getHBaseCluster().getMaster().isInitialized() ||
+            TEST_UTIL.getHBaseCluster().getMaster().getServerManager().getOnlineServers().size() > 2;
+      }
+    });
+
+    //make sure balancer is in offline mode, since this is what we're testing
+    assertFalse(((GroupBasedLoadBalancer)TEST_UTIL.getHBaseCluster().getMaster().getLoadBalancer()).isOnline());
+    //verify the group affiliation that's loaded from ZK instead of tables
+    assertEquals(newGroup, groupAdmin.getGroupInfoOfTable(GroupInfoManager.GROUP_TABLE_NAME).getName());
+    assertEquals(GroupInfo.OFFLINE_DEFAULT_GROUP, groupAdmin.getGroupInfoOfTable(failoverTable).getName());
+
+    //kill final regionserver to see the failover happens for all tables
+    //except GROUP table since it's group does not have any online RS
+    killRS.stop("die");
+    master = TEST_UTIL.getHBaseCluster().getMaster();
+    LOG.info("Waiting for new table assignment...");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return failoverRS.getOnlineRegions(Bytes.toBytes(failoverTable)).size() < 1;
+      }
+    });
+    assertEquals(0, failoverRS.getOnlineRegions(GroupInfoManager.GROUP_TABLE_NAME_BYTES).size());
+    //need this for minicluster to shutdown cleanly
+    master.stopMaster();
+  }
+}
diff --git src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java
new file mode 100644
index 0000000..29096fa
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java
@@ -0,0 +1,124 @@
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+import org.junit.Assert;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.MasterNotRunningException;
+import org.apache.hadoop.hbase.ZooKeeperConnectionException;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.type.TypeReference;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Set;
+import java.util.TreeSet;
+
+public class VerifyingGroupAdminClient extends GroupAdminClient {
+  private HTable table;
+  private ZooKeeperWatcher zkw;
+
+  public VerifyingGroupAdminClient(Configuration conf)
+      throws IOException {
+    super(conf);
+    table = new HTable(conf, GroupInfoManager.GROUP_TABLE_NAME_BYTES);
+    zkw = new ZooKeeperWatcher(conf, this.getClass().getSimpleName(), null);
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    super.addGroup(groupName);
+    verify();
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup) throws IOException {
+    super.moveServers(servers, targetGroup);
+    verify();
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    super.moveTables(tables, targetGroup);
+    verify();
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    super.removeGroup(name);
+    verify();
+  }
+
+  public void verify() throws IOException {
+    ObjectMapper mapper = new ObjectMapper();
+    Get get = new Get(GroupInfoManager.ROW_KEY);
+    get.addFamily(GroupInfoManager.SERVER_FAMILY_BYTES);
+    get.addFamily(GroupInfoManager.TABLE_FAMILY_BYTES);
+    Map<String, GroupInfo> groupMap = Maps.newHashMap();
+    Set<GroupInfo> specialList = Sets.newHashSet();
+    Set<GroupInfo> zList = Sets.newHashSet();
+
+    Result result = table.get(get);
+    if(!result.isEmpty()) {
+      NavigableMap<byte[],NavigableMap<byte[],byte[]>> dataMap =
+          result.getNoVersionMap();
+      for(byte[] groupNameBytes:
+          dataMap.get(GroupInfoManager.SERVER_FAMILY_BYTES).keySet()) {
+        TreeSet<String> servers =
+            mapper.readValue(
+                Bytes.toString(dataMap.get(GroupInfoManager.SERVER_FAMILY_BYTES).get(groupNameBytes)),
+                new TypeReference<TreeSet<String>>() {});
+        TreeSet<String> tables =
+            mapper.readValue(
+                Bytes.toString(dataMap.get(GroupInfoManager.TABLE_FAMILY_BYTES).get(groupNameBytes)),
+                new TypeReference<TreeSet<String>>() {});
+        GroupInfo groupInfo =
+            new GroupInfo(Bytes.toString(groupNameBytes), servers, tables);
+        groupMap.put(groupInfo.getName(), groupInfo);
+        for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+          if(tables.contains(special)) {
+            specialList.add(groupInfo);
+            break;
+          }
+        }
+      }
+    }
+    groupMap.put(GroupInfo.DEFAULT_GROUP, super.getGroupInfo(GroupInfo.DEFAULT_GROUP));
+    for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+      GroupInfo groupInfo = groupMap.get(GroupInfo.DEFAULT_GROUP);
+      if(groupInfo.getTables().contains(special)) {
+        specialList.add(groupInfo);
+        break;
+      }
+    }
+    Assert.assertEquals(Sets.newHashSet(groupMap.values()),
+        Sets.newHashSet(super.listGroups()));
+    try {
+      String data = Bytes.toString(
+          ZKUtil.getData(zkw, ZKUtil.joinZNode(zkw.baseZNode,"groupInfo")));
+      zList.addAll((List<GroupInfo>)mapper.readValue(data, new TypeReference<List<GroupInfo>>(){}));
+      Assert.assertEquals(zList.size(),specialList.size());
+      for(GroupInfo groupInfo: zList) {
+        if(groupInfo.getName().equals(GroupInfo.OFFLINE_DEFAULT_GROUP)) {
+          Assert.assertEquals(groupMap.get(GroupInfo.DEFAULT_GROUP).getServers(), groupInfo.getServers());
+          Assert.assertEquals(groupMap.get(GroupInfo.DEFAULT_GROUP).getTables(), groupInfo.getTables());
+        } else {
+          Assert.assertTrue(specialList.contains(groupInfo));
+        }
+      }
+    } catch (KeeperException e) {
+      throw new IOException("ZK verification failed", e);
+    }
+  }
+
+}
diff --git src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
index aee8537..964259b 100644
--- src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
+++ src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
@@ -962,8 +962,8 @@ public class TestAssignmentManager {
     }
 
     @Override
-    public ServerName randomAssignment(List<ServerName> servers) {
-      ServerName randomServerName = super.randomAssignment(servers);
+    public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
+      ServerName randomServerName = super.randomAssignment(region, servers);
       this.gate.set(true);
       return randomServerName;
     }
@@ -1015,7 +1015,7 @@ public class TestAssignmentManager {
       while (this.gate.get()) Threads.sleep(1);
       super.processRegionsInTransition(data, regionInfo, deadServers, expectedVersion);
     }
-    
+
     @Override
     public void assign(HRegionInfo region, boolean setOfflineInZK, boolean forceNewPlan,
         boolean hijack) {
@@ -1027,12 +1027,12 @@ public class TestAssignmentManager {
         super.assign(region, setOfflineInZK, forceNewPlan, hijack);
       }
     }
-    
+
     @Override
     public ServerName getRegionServerOfRegion(HRegionInfo hri) {
       return SERVERNAME_A;
     }
-    
+
     /** reset the watcher */
     void setWatcher(ZooKeeperWatcher watcher) {
       this.watcher = watcher;
diff --git src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index 6b17c76..8205903 100644
--- src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -308,6 +308,11 @@ public class TestCatalogJanitor {
     @Override
     public void deleteColumn(byte[] tableName, byte[] columnName) throws IOException {
     }
+
+    @Override
+    public LoadBalancer getLoadBalancer() {
+      return null;
+    }
   }
 
   @Test
diff --git src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java
new file mode 100644
index 0000000..a38d187
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java
@@ -0,0 +1,566 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.security.SecureRandom;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.group.GroupBasedLoadBalancer;
+import org.apache.hadoop.hbase.group.GroupInfo;
+import org.apache.hadoop.hbase.group.GroupInfoManager;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Lists;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+@Category(SmallTests.class)
+public class TestGroupBasedLoadBalancer {
+
+    private static final Log LOG = LogFactory.getLog(TestGroupBasedLoadBalancer.class);
+    private static GroupBasedLoadBalancer loadBalancer;
+    private static SecureRandom rand;
+
+    static String[]  groups = new String[] { GroupInfo.DEFAULT_GROUP, "dg2", "dg3",
+            "dg4" };
+    static String[] tables = new String[] { "dt1", "dt2", "dt3", "dt4" };
+    static List<ServerName> servers;
+    static Map<String, GroupInfo> groupMap;
+    static Map<String, String> tableMap;
+    static List<HTableDescriptor> tableDescs;
+    int[] regionAssignment = new int[] { 2, 5, 7, 10, 4, 3, 1 };
+    static int regionId = 0;
+
+    @BeforeClass
+    public static void beforeAllTests() throws Exception {
+        rand = new SecureRandom();
+        servers = generateServers(7);
+        groupMap = constructGroupInfo(servers, groups);
+        tableMap = new HashMap<String, String>();
+        tableDescs = constructTableDesc();
+        Configuration conf = HBaseConfiguration.create();
+        conf.set("hbase.regions.slop", "0");
+        loadBalancer = new GroupBasedLoadBalancer(getMockedGroupInfoManager());
+        loadBalancer.setMasterServices(getMockedMaster());
+        loadBalancer.setConf(conf);
+        loadBalancer.configure();
+    }
+
+    /**
+     * Test the load balancing algorithm.
+     *
+     * Invariant is that all servers of the group should be hosting either floor(average) or
+     * ceiling(average)
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testBalanceCluster() throws Exception {
+        Map<ServerName, List<HRegionInfo>> servers = mockClusterServers();
+        ArrayListMultimap<String, ServerAndLoad> list = convertToGroupBasedMap(servers);
+        LOG.info("Mock Cluster :  " + printStats(list));
+        List<RegionPlan> plans = loadBalancer.balanceCluster(servers);
+        ArrayListMultimap<String, ServerAndLoad> balancedCluster = reconcile(
+                                                    list, plans);
+        LOG.info("Mock Balance : " + printStats(balancedCluster));
+        assertClusterAsBalanced(balancedCluster);
+    }
+
+    /**
+     * Invariant is that all servers of a group have load between floor(avg) and
+     * ceiling(avg) number of regions.
+     */
+    private void assertClusterAsBalanced(
+            ArrayListMultimap<String, ServerAndLoad> groupLoadMap) {
+        for (String gName : groupLoadMap.keySet()) {
+            List<ServerAndLoad> groupLoad = groupLoadMap.get(gName);
+            int numServers = groupLoad.size();
+            int numRegions = 0;
+            int maxRegions = 0;
+            int minRegions = Integer.MAX_VALUE;
+            for (ServerAndLoad server : groupLoad) {
+                int nr = server.getLoad();
+                if (nr > maxRegions) {
+                    maxRegions = nr;
+                }
+                if (nr < minRegions) {
+                    minRegions = nr;
+                }
+                numRegions += nr;
+            }
+            if (maxRegions - minRegions < 2) {
+                // less than 2 between max and min, can't balance
+                return;
+            }
+            int min = numRegions / numServers;
+            int max = numRegions % numServers == 0 ? min : min + 1;
+
+            for (ServerAndLoad server : groupLoad) {
+                assertTrue(server.getLoad() <= max);
+                assertTrue(server.getLoad() >= min);
+            }
+        }
+    }
+
+    /**
+     * Tests immediate assignment.
+     *
+     * Invariant is that all regions have an assignment.
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testImmediateAssignment() throws Exception {
+        List<HRegionInfo> regions = randomRegions(20);
+        Map<HRegionInfo, ServerName> assignments = loadBalancer
+                .immediateAssignment(regions, servers);
+        assertImmediateAssignment(regions, servers, assignments);
+    }
+
+    /**
+     * All regions have an assignment.
+     *
+     * @param regions
+     * @param servers
+     * @param assignments
+     * @throws IOException
+     * @throws FileNotFoundException
+     */
+    private void assertImmediateAssignment(List<HRegionInfo> regions,
+            List<ServerName> servers, Map<HRegionInfo, ServerName> assignments)
+            throws FileNotFoundException, IOException {
+        for (HRegionInfo region : regions) {
+            assertTrue(assignments.containsKey(region));
+            ServerName server = assignments.get(region);
+            String tableName = region.getTableNameAsString();
+
+            String groupName =
+                loadBalancer.getGroupInfoManager().getGroupOfTable(tableName);
+            assertTrue(StringUtils.isNotEmpty(groupName));
+            GroupInfo gInfo = getMockedGroupInfoManager().getGroup(groupName);
+            assertTrue("Region is not correctly assigned to group servers.",
+                    gInfo.containsServer(server.getHostAndPort()));
+        }
+    }
+
+    /**
+     * Tests the bulk assignment used during cluster startup.
+     *
+     * Round-robin. Should yield a balanced cluster so same invariant as the
+     * load balancer holds, all servers holding either floor(avg) or
+     * ceiling(avg).
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testBulkAssignment() throws Exception {
+        List<HRegionInfo> regions = randomRegions(25);
+        Map<ServerName, List<HRegionInfo>> assignments = loadBalancer
+                .roundRobinAssignment(regions, servers);
+        assertTrue(assignments.keySet().size() == servers.size());
+        for (ServerName sn : assignments.keySet()) {
+            List<HRegionInfo> regionAssigned = assignments.get(sn);
+            for (HRegionInfo region : regionAssigned) {
+                String tableName = region.getTableNameAsString();
+                String groupName =
+                    getMockedGroupInfoManager().getGroupOfTable(tableName);
+                assertTrue(StringUtils.isNotEmpty(groupName));
+                GroupInfo gInfo = getMockedGroupInfoManager().getGroup(
+                        groupName);
+                assertTrue(
+                        "Region is not correctly assigned to group servers.",
+                        gInfo.containsServer(sn.getHostAndPort()));
+            }
+        }
+        ArrayListMultimap<String, ServerAndLoad> loadMap = convertToGroupBasedMap(assignments);
+        assertClusterAsBalanced(loadMap);
+    }
+
+    /**
+     * Test the cluster startup bulk assignment which attempts to retain
+     * assignment info.
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testRetainAssignment() throws Exception {
+        // Test simple case where all same servers are there
+        Map<ServerName, List<HRegionInfo>> currentAssignments = mockClusterServers();
+        Map<HRegionInfo, ServerName> inputForTest = new HashMap<HRegionInfo, ServerName>();
+        for (ServerName sn : currentAssignments.keySet()) {
+            for (HRegionInfo region : currentAssignments.get(sn)) {
+                inputForTest.put(region, sn);
+            }
+        }
+        Map<ServerName, List<HRegionInfo>> newAssignment = loadBalancer
+                .retainAssignment(inputForTest, servers);
+        assertRetainedAssignment(inputForTest, servers, newAssignment);
+    }
+
+    /**
+     * Asserts a valid retained assignment plan.
+     * <p>
+     * Must meet the following conditions:
+     * <ul>
+     * <li>Every input region has an assignment, and to an online server
+     * <li>If a region had an existing assignment to a server with the same
+     * address a a currently online server, it will be assigned to it
+     * </ul>
+     *
+     * @param existing
+     * @param assignment
+     * @throws IOException
+     * @throws FileNotFoundException
+     */
+    private void assertRetainedAssignment(
+            Map<HRegionInfo, ServerName> existing, List<ServerName> servers,
+            Map<ServerName, List<HRegionInfo>> assignment)
+            throws FileNotFoundException, IOException {
+        // Verify condition 1, every region assigned, and to online server
+        Set<ServerName> onlineServerSet = new TreeSet<ServerName>(servers);
+        Set<HRegionInfo> assignedRegions = new TreeSet<HRegionInfo>();
+        for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+            assertTrue(
+                    "Region assigned to server that was not listed as online",
+                    onlineServerSet.contains(a.getKey()));
+            for (HRegionInfo r : a.getValue())
+                assignedRegions.add(r);
+        }
+        assertEquals(existing.size(), assignedRegions.size());
+
+        // Verify condition 2, every region must be assigned to correct server.
+        Set<String> onlineHostNames = new TreeSet<String>();
+        for (ServerName s : servers) {
+            onlineHostNames.add(s.getHostname());
+        }
+
+        for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+            ServerName currentServer = a.getKey();
+            for (HRegionInfo r : a.getValue()) {
+                ServerName oldAssignedServer = existing.get(r);
+                String tableName = r.getTableNameAsString();
+                String groupName =
+                    getMockedGroupInfoManager().getGroupOfTable(tableName);
+                assertTrue(StringUtils.isNotEmpty(groupName));
+                GroupInfo gInfo = getMockedGroupInfoManager().getGroup(
+                        groupName);
+                assertTrue(
+                        "Region is not correctly assigned to group servers.",
+                        gInfo.containsServer(currentServer.getHostAndPort()));
+                if (oldAssignedServer != null
+                        && onlineHostNames.contains(oldAssignedServer
+                                .getHostname())) {
+                    // this region was previously assigned somewhere, and that
+                    // host is still around, then the host must have been is a
+                    // different group.
+                    if (oldAssignedServer.getHostAndPort().equals(
+                            currentServer.getHostAndPort()) == false) {
+                        assertFalse(gInfo.containsServer(oldAssignedServer
+                                .getHostAndPort()));
+                    }
+                }
+            }
+        }
+    }
+
+    private String printStats(
+            ArrayListMultimap<String, ServerAndLoad> groupBasedLoad) {
+        StringBuffer sb = new StringBuffer();
+        sb.append("\n");
+        for (String groupName : groupBasedLoad.keySet()) {
+            sb.append("Stats for group: " + groupName);
+            sb.append("\n");
+            sb.append(groupMap.get(groupName).getServers());
+            sb.append("\n");
+            List<ServerAndLoad> groupLoad = groupBasedLoad.get(groupName);
+            int numServers = groupLoad.size();
+            int totalRegions = 0;
+            sb.append("Per Server Load: \n");
+            for (ServerAndLoad sLoad : groupLoad) {
+                sb.append("Server :" + sLoad.getServerName() + " Load : "
+                        + sLoad.getLoad() + "\n");
+                totalRegions += sLoad.getLoad();
+            }
+            sb.append(" Group Statistics : \n");
+            float average = (float) totalRegions / numServers;
+            int max = (int) Math.ceil(average);
+            int min = (int) Math.floor(average);
+            sb.append("[srvr=" + numServers + " rgns=" + totalRegions + " avg="
+                    + average + " max=" + max + " min=" + min + "]");
+            sb.append("\n");
+            sb.append("===============================");
+            sb.append("\n");
+        }
+        return sb.toString();
+    }
+
+    private ArrayListMultimap<String, ServerAndLoad> convertToGroupBasedMap(
+            final Map<ServerName, List<HRegionInfo>> serversMap) throws IOException {
+        ArrayListMultimap<String, ServerAndLoad> loadMap = ArrayListMultimap
+                .create();
+        for (GroupInfo gInfo : getMockedGroupInfoManager().listGroups()) {
+            Set<String> groupServers = gInfo.getServers();
+            for (String hostAndPort : groupServers) {
+                ServerName actual = ServerName.findServerWithSameHostnamePort(
+                        servers, ServerName.parseServerName(hostAndPort));
+                List<HRegionInfo> regions = serversMap.get(actual);
+                assertTrue("No load for " + actual, regions != null);
+                loadMap.put(gInfo.getName(),
+                        new ServerAndLoad(actual, regions.size()));
+            }
+        }
+        return loadMap;
+    }
+
+  private ArrayListMultimap<String, ServerAndLoad> reconcile(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      List<RegionPlan> plans) {
+    ArrayListMultimap<String, ServerAndLoad> result = ArrayListMultimap
+        .create();
+    result.putAll(previousLoad);
+    if (plans != null) {
+      for (RegionPlan plan : plans) {
+        ServerName source = plan.getSource();
+        updateLoad(result, source, -1);
+        ServerName destination = plan.getDestination();
+        updateLoad(result, destination, +1);
+      }
+    }
+    return result;
+  }
+
+  private void updateLoad(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      final ServerName sn, final int diff) {
+    for (String groupName : previousLoad.keySet()) {
+      ServerAndLoad newSAL = null;
+      ServerAndLoad oldSAL = null;
+      for (ServerAndLoad sal : previousLoad.get(groupName)) {
+        if (ServerName.isSameHostnameAndPort(sn, sal.getServerName())) {
+          oldSAL = sal;
+          newSAL = new ServerAndLoad(sn, sal.getLoad() + diff);
+          break;
+        }
+      }
+      if (newSAL != null) {
+        previousLoad.remove(groupName, oldSAL);
+        previousLoad.put(groupName, newSAL);
+        break;
+      }
+    }
+  }
+
+    private Map<ServerName, List<HRegionInfo>> mockClusterServers() throws IOException {
+        assertTrue(servers.size() == regionAssignment.length);
+        Map<ServerName, List<HRegionInfo>> assignment = new TreeMap<ServerName, List<HRegionInfo>>();
+        for (int i = 0; i < servers.size(); i++) {
+            int numRegions = regionAssignment[i];
+            List<HRegionInfo> regions = assignedRegions(numRegions, servers.get(i));
+            assignment.put(servers.get(i), regions);
+        }
+        return assignment;
+    }
+
+    /**
+     * Generate a list of regions evenly distributed between the tables.
+     *
+     * @param numRegions The number of regions to be generated.
+     * @return List of HRegionInfo.
+     */
+    private List<HRegionInfo> randomRegions(int numRegions) {
+        List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+        byte[] start = new byte[16];
+        byte[] end = new byte[16];
+        rand.nextBytes(start);
+        rand.nextBytes(end);
+        int regionIdx = rand.nextInt(tables.length);
+        for (int i = 0; i < numRegions; i++) {
+            Bytes.putInt(start, 0, numRegions << 1);
+            Bytes.putInt(end, 0, (numRegions << 1) + 1);
+            int tableIndex = (i + regionIdx) % tables.length;
+            HRegionInfo hri = new HRegionInfo(
+                    Bytes.toBytes(tables[tableIndex]), start, end, false,
+                    regionId++);
+            regions.add(hri);
+        }
+        return regions;
+    }
+
+    /**
+     * Generate assigned regions to a given server using group information.
+     *
+     * @param numRegions the num regions to generate
+     * @param sn the servername
+     * @return the list of regions
+     * @throws IOException Signals that an I/O exception has occurred.
+     */
+    private List<HRegionInfo> assignedRegions(int numRegions, ServerName sn) throws IOException {
+      List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+      byte[] start = new byte[16];
+      byte[] end = new byte[16];
+      Bytes.putInt(start, 0, numRegions << 1);
+      Bytes.putInt(end, 0, (numRegions << 1) + 1);
+      for (int i = 0; i < numRegions; i++) {
+          String tableName = getTableName(sn);
+          HRegionInfo hri = new HRegionInfo(
+                  Bytes.toBytes(tableName), start, end, false,
+                  regionId++);
+          regions.add(hri);
+      }
+      return regions;
+  }
+
+    private static List<ServerName> generateServers(int numServers) {
+        List<ServerName> servers = new ArrayList<ServerName>(numServers);
+        for (int i = 0; i < numServers; i++) {
+            String host = "server" + rand.nextInt(100000);
+            int port = rand.nextInt(60000);
+            servers.add(new ServerName(host, port, -1));
+        }
+        return servers;
+    }
+
+    /**
+     * Construct group info, with each group having at least one server.
+     *
+     * @param servers the servers
+     * @param groups the groups
+     * @return the map
+     */
+    private static Map<String, GroupInfo> constructGroupInfo(
+            List<ServerName> servers, String[] groups) {
+        assertTrue(servers != null);
+        assertTrue(servers.size() >= groups.length);
+        int index = 0;
+        Map<String, GroupInfo> groupMap = new HashMap<String, GroupInfo>();
+        for (String grpName : groups) {
+            GroupInfo groupInfo = new GroupInfo(grpName);
+            groupInfo.addServer(servers.get(index).getHostAndPort());
+            groupMap.put(grpName, groupInfo);
+            index++;
+        }
+        while (index < servers.size()) {
+            int grpIndex = rand.nextInt(groups.length);
+            groupMap.get(groups[grpIndex]).addServer(
+                    servers.get(index).getHostAndPort());
+            index++;
+        }
+        return groupMap;
+    }
+
+    /**
+     * Construct table descriptors evenly distributed between the groups.
+     *
+     * @return the list
+     */
+    private static List<HTableDescriptor> constructTableDesc() {
+        List<HTableDescriptor> tds = Lists.newArrayList();
+        int index = rand.nextInt(groups.length);
+        for (int i = 0; i < tables.length; i++) {
+            HTableDescriptor htd = new HTableDescriptor(tables[i]);
+            int grpIndex = (i + index) % groups.length ;
+            String groupName = groups[grpIndex];
+            tableMap.put(tables[i], groupName);
+            tds.add(htd);
+        }
+        return tds;
+    }
+
+    private static MasterServices getMockedMaster() throws IOException {
+        TableDescriptors tds = Mockito.mock(TableDescriptors.class);
+        Mockito.when(tds.get(tables[0])).thenReturn(tableDescs.get(0));
+        Mockito.when(tds.get(tables[1])).thenReturn(tableDescs.get(1));
+        Mockito.when(tds.get(tables[2])).thenReturn(tableDescs.get(2));
+        Mockito.when(tds.get(tables[3])).thenReturn(tableDescs.get(3));
+        MasterServices services = Mockito.mock(HMaster.class);
+        Mockito.when(services.getTableDescriptors()).thenReturn(tds);
+        AssignmentManager am = Mockito.mock(AssignmentManager.class);
+        Mockito.when(services.getAssignmentManager()).thenReturn(am);
+        return services;
+    }
+
+    private static GroupInfoManager getMockedGroupInfoManager() throws IOException {
+        GroupInfoManager gm = Mockito.mock(GroupInfoManager.class);
+        Mockito.when(gm.getGroup(groups[0])).thenReturn(
+                groupMap.get(groups[0]));
+        Mockito.when(gm.getGroup(groups[1])).thenReturn(
+                groupMap.get(groups[1]));
+        Mockito.when(gm.getGroup(groups[2])).thenReturn(
+                groupMap.get(groups[2]));
+        Mockito.when(gm.getGroup(groups[3])).thenReturn(
+                groupMap.get(groups[3]));
+        Mockito.when(gm.listGroups()).thenReturn(
+                Lists.newLinkedList(groupMap.values()));
+        Mockito.when(gm.isOnline()).thenReturn(true);
+        Mockito.when(gm.getGroupOfTable(Mockito.anyString())).thenAnswer(new Answer<String>() {
+          @Override
+          public String answer(InvocationOnMock invocation) throws Throwable {
+            return tableMap.get(invocation.getArguments()[0]);
+          }
+        });
+        return gm;
+    }
+
+    private String getTableName(ServerName sn) throws IOException{
+      String tableName = null;
+      GroupInfoManager gm = getMockedGroupInfoManager();
+      GroupInfo groupOfServer = null;
+      for(GroupInfo gInfo : gm.listGroups()){
+        if(gInfo.containsServer(sn.getHostAndPort())){
+          groupOfServer = gInfo;
+          break;
+        }
+      }
+
+      for(HTableDescriptor desc : tableDescs){
+       if(gm.getGroupOfTable(desc.getNameAsString()).endsWith(groupOfServer.getName())){
+         tableName = desc.getNameAsString();
+       }
+      }
+      return tableName;
+    }
+}
diff --git src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
index 8a4c7ff..6c8f9e4 100644
--- src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
+++ src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
@@ -940,7 +940,7 @@ public class TestSplitTransactionOnCluster {
    */
   private int ensureTableRegionNotOnSameServerAsMeta(final HBaseAdmin admin,
       final HRegionInfo hri)
-  throws UnknownRegionException, MasterNotRunningException,
+  throws IOException,
   ZooKeeperConnectionException, InterruptedException {
     MiniHBaseCluster cluster = TESTING_UTIL.getMiniHBaseCluster();
     // Now make sure that the table region is not on same server as that hosting
