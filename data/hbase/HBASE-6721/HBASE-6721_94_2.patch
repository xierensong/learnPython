diff --git security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index b82ad5d..ddc3dc2 100644
--- security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ security/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -377,7 +377,7 @@ public class AccessController extends BaseRegionObserver
    * @throws IOException if obtaining the current user fails
    * @throws AccessDeniedException if user has no authorization
    */
-  private void requirePermission(byte[] tableName, byte[] family, byte[] qualifier,
+  public void requirePermission(byte[] tableName, byte[] family, byte[] qualifier,
       Action... permissions) throws IOException {
     User user = getActiveUser();
     AuthResult result = null;
diff --git security/src/main/java/org/apache/hadoop/hbase/security/access/SecureGroupAdminEndpoint.java security/src/main/java/org/apache/hadoop/hbase/security/access/SecureGroupAdminEndpoint.java
new file mode 100644
index 0000000..51b2739
--- /dev/null
+++ security/src/main/java/org/apache/hadoop/hbase/security/access/SecureGroupAdminEndpoint.java
@@ -0,0 +1,63 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.security.access;
+
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.master.GroupAdminEndpoint;
+import org.apache.hadoop.hbase.master.GroupAdminProtocol;
+import org.apache.hadoop.hbase.master.GroupInfo;
+
+import java.io.IOException;
+import java.util.Set;
+
+public class SecureGroupAdminEndpoint extends GroupAdminEndpoint implements GroupAdminProtocol{
+  private MasterCoprocessorEnvironment menv;
+
+  @Override
+  public void start(CoprocessorEnvironment env) {
+    super.start(env);
+    menv = (MasterCoprocessorEnvironment)env;
+  }
+
+  @Override
+  public void moveServers(Set<String> hostPorts, String dstGroup) throws IOException {
+    getAccessController().requirePermission(HConstants.ROOT_TABLE_NAME, null, null, Permission.Action.ADMIN);
+    super.moveServers(hostPorts, dstGroup);
+  }
+
+  @Override
+  public void removeGroup(String groupName) throws IOException {
+    getAccessController().requirePermission(HConstants.ROOT_TABLE_NAME, null, null, Permission.Action.ADMIN);
+    super.removeGroup(groupName);
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    getAccessController().requirePermission(HConstants.ROOT_TABLE_NAME, null, null, Permission.Action.ADMIN);
+    super.addGroup(groupName);
+  }
+
+  private AccessController getAccessController() {
+    return (AccessController)menv.getMasterServices()
+        .getCoprocessorHost().findCoprocessor(AccessController.class.getName());
+  }
+}
diff --git security/src/test/java/org/apache/hadoop/hbase/security/access/TestSecureGroupAdminEndpoint.java security/src/test/java/org/apache/hadoop/hbase/security/access/TestSecureGroupAdminEndpoint.java
new file mode 100644
index 0000000..0e67d79
--- /dev/null
+++ security/src/test/java/org/apache/hadoop/hbase/security/access/TestSecureGroupAdminEndpoint.java
@@ -0,0 +1,237 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.security.access;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Coprocessor;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException;
+import org.apache.hadoop.hbase.master.GroupAdmin;
+import org.apache.hadoop.hbase.master.GroupBasedLoadBalancer;
+import org.apache.hadoop.hbase.master.MasterCoprocessorHost;
+import org.apache.hadoop.hbase.security.AccessDeniedException;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.atomic.AtomicLong;
+
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+/**
+ * Performs authorization checks for common operations, according to different
+ * levels of authorized users.
+ */
+@Category(LargeTests.class)
+@SuppressWarnings("rawtypes")
+public class TestSecureGroupAdminEndpoint {
+  private static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private static Configuration conf;
+
+  // user with all permissions
+  private static User SUPERUSER;
+  // user granted with all global permission
+  private static User USER_ADMIN;
+  // user with rw permissions
+  private static User USER_RW;
+  // user with read-only permissions
+  private static User USER_RO;
+  // user is table owner. will have all permissions on table
+  private static User USER_OWNER;
+  // user with create table permissions alone
+  private static User USER_CREATE;
+  // user with no permissions
+  private static User USER_NONE;
+
+  private static AccessController ACCESS_CONTROLLER;
+  private static SecureGroupAdminEndpoint GROUP_ENDPOINT;
+
+  @BeforeClass
+  public static void setupBeforeClass() throws Exception {
+    // setup configuration
+    conf = TEST_UTIL.getConfiguration();
+    SecureTestUtil.enableSecurity(conf);
+		TEST_UTIL.getConfiguration().set(
+				HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+				GroupBasedLoadBalancer.class.getName());
+    conf.set("hbase.coprocessor.master.classes",
+        conf.get("hbase.coprocessor.master.classes")+","+SecureGroupAdminEndpoint.class.getName());
+
+    TEST_UTIL.startMiniCluster(1,2);
+    MasterCoprocessorHost cpHost = TEST_UTIL.getMiniHBaseCluster().getMaster().getCoprocessorHost();
+    cpHost.load(AccessController.class, Coprocessor.PRIORITY_HIGHEST, conf);
+    ACCESS_CONTROLLER = (AccessController) cpHost.findCoprocessor(AccessController.class.getName());
+    GROUP_ENDPOINT = (SecureGroupAdminEndpoint)
+        TEST_UTIL.getMiniHBaseCluster().getMaster()
+           .getCoprocessorHost().findCoprocessor(SecureGroupAdminEndpoint.class.getName());
+    // Wait for the ACL table to become available
+    TEST_UTIL.waitTableAvailable(AccessControlLists.ACL_TABLE_NAME, 5000);
+
+
+
+    // create a set of test users
+    SUPERUSER = User.createUserForTesting(conf, "admin", new String[] { "supergroup" });
+    USER_ADMIN = User.createUserForTesting(conf, "admin2", new String[0]);
+    USER_NONE = User.createUserForTesting(conf, "nouser", new String[0]);
+
+    // initialize access control
+    HTable meta = new HTable(conf, AccessControlLists.ACL_TABLE_NAME);
+    AccessControllerProtocol protocol =
+        meta.coprocessorProxy(AccessControllerProtocol.class, HConstants.EMPTY_START_ROW);
+
+    protocol.grant(new UserPermission(Bytes.toBytes(USER_ADMIN.getShortName()),
+        Permission.Action.ADMIN, Permission.Action.CREATE, Permission.Action.READ,
+        Permission.Action.WRITE));
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  public void verifyAllowed(User user, PrivilegedExceptionAction... actions) throws Exception {
+    for (PrivilegedExceptionAction action : actions) {
+      try {
+        user.runAs(action);
+      } catch (AccessDeniedException ade) {
+        fail("Expected action to pass for user '" + user.getShortName() + "' but was denied");
+      }
+    }
+  }
+
+  public void verifyAllowed(PrivilegedExceptionAction action, User... users) throws Exception {
+    for (User user : users) {
+      verifyAllowed(user, action);
+    }
+  }
+
+  public void verifyDenied(User user, PrivilegedExceptionAction... actions) throws Exception {
+    for (PrivilegedExceptionAction action : actions) {
+      try {
+        user.runAs(action);
+        fail("Expected AccessDeniedException for user '" + user.getShortName() + "'");
+      } catch (RetriesExhaustedWithDetailsException e) {
+        // in case of batch operations, and put, the client assembles a
+        // RetriesExhaustedWithDetailsException instead of throwing an
+        // AccessDeniedException
+        boolean isAccessDeniedException = false;
+        for (Throwable ex : e.getCauses()) {
+          if (ex instanceof AccessDeniedException) {
+            isAccessDeniedException = true;
+            break;
+          }
+        }
+        if (!isAccessDeniedException) {
+          fail("Not receiving AccessDeniedException for user '" + user.getShortName() + "'");
+        }
+      } catch (AccessDeniedException ade) {
+        // expected result
+      }
+    }
+  }
+
+  public void verifyDenied(PrivilegedExceptionAction action, User... users) throws Exception {
+    for (User user : users) {
+      verifyDenied(user, action);
+    }
+  }
+
+  @Test
+  public void testGetAddRemove() throws Exception {
+    final AtomicLong counter = new AtomicLong(0);
+
+    PrivilegedExceptionAction getGroup = new PrivilegedExceptionAction() {
+      public Object run() throws Exception {
+        GROUP_ENDPOINT.getGroupInfo("default");
+        return null;
+      }
+    };
+    verifyAllowed(getGroup, SUPERUSER, USER_ADMIN, USER_NONE);
+
+    PrivilegedExceptionAction addGroup = new PrivilegedExceptionAction() {
+      public Object run() throws Exception {
+        GROUP_ENDPOINT.addGroup("testGetAddRemove"+counter.incrementAndGet());
+        return null;
+      }
+    };
+    verifyDenied(addGroup, USER_NONE);
+    verifyAllowed(addGroup, SUPERUSER, USER_ADMIN);
+
+    PrivilegedExceptionAction removeGroup = new PrivilegedExceptionAction() {
+      public Object run() throws Exception {
+        GROUP_ENDPOINT.removeGroup("testGetAddRemove"+counter.getAndDecrement());
+        return null;
+      }
+    };
+    verifyAllowed(removeGroup, SUPERUSER, USER_ADMIN);
+    verifyDenied(removeGroup, USER_NONE);
+  }
+
+  @Test
+  public void testMoveServer() throws Exception {
+    final AtomicLong counter = new AtomicLong(0);
+    Set<String> servers = new TreeSet<String>();
+    for(int i=1;i<=100;i++) {
+      GROUP_ENDPOINT.addGroup("testMoveServer_"+i);
+    }
+
+    PrivilegedExceptionAction action = new PrivilegedExceptionAction() {
+      public Object run() throws Exception {
+        String hostPort;
+        Set<String> set = new TreeSet<String>();
+        hostPort = TEST_UTIL.getMiniHBaseCluster().getRegionServer(1).getServerName().getHostAndPort();
+        set.add(hostPort);
+        GROUP_ENDPOINT.moveServers(set, "testMoveServer_" + counter.incrementAndGet());
+        waitForTransitions(GROUP_ENDPOINT);
+        return null;
+      }
+    };
+    verifyAllowed(action, SUPERUSER, USER_ADMIN);
+    verifyDenied(action, USER_NONE);
+  }
+
+  @Test
+  public void testListGroups() throws Exception {
+    PrivilegedExceptionAction action = new PrivilegedExceptionAction() {
+      public Object run() throws Exception {
+        GROUP_ENDPOINT.listGroups();
+        return null;
+      }
+    };
+    verifyAllowed(action, SUPERUSER, USER_ADMIN,USER_NONE);
+  }
+
+  private static void waitForTransitions(GroupAdmin gAdmin) throws IOException, InterruptedException {
+    while(gAdmin.listServersInTransition().size()>0) {
+      Thread.sleep(1000);
+    }
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/HConstants.java src/main/java/org/apache/hadoop/hbase/HConstants.java
index ba657e0..b016603 100644
--- src/main/java/org/apache/hadoop/hbase/HConstants.java
+++ src/main/java/org/apache/hadoop/hbase/HConstants.java
@@ -407,6 +407,7 @@ public final class HConstants {
   public static final String NAME = "NAME";
   public static final String VERSIONS = "VERSIONS";
   public static final String IN_MEMORY = "IN_MEMORY";
+  public static final String CONFIG = "CONFIG";
 
   /**
    * This is a retry backoff multiplier table similar to the BSD TCP syn
diff --git src/main/java/org/apache/hadoop/hbase/client/GroupAdminClient.java src/main/java/org/apache/hadoop/hbase/client/GroupAdminClient.java
new file mode 100644
index 0000000..742bff1
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/client/GroupAdminClient.java
@@ -0,0 +1,119 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.MasterNotRunningException;
+import org.apache.hadoop.hbase.ZooKeeperConnectionException;
+import org.apache.hadoop.hbase.master.GroupAdmin;
+import org.apache.hadoop.hbase.master.GroupAdminProtocol;
+import org.apache.hadoop.hbase.master.GroupInfo;
+
+/**
+ * This class is responsible for managing region server group information.
+ */
+public class GroupAdminClient implements GroupAdmin {
+  private GroupAdmin proxy;
+	private static final Log LOG = LogFactory.getLog(GroupAdminClient.class);
+  private int operationTimeout;
+
+  public GroupAdminClient(Configuration conf) throws ZooKeeperConnectionException, MasterNotRunningException {
+    proxy = new HBaseAdmin(conf).coprocessorProxy(GroupAdminProtocol.class);
+    operationTimeout = conf.getInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT,
+            HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT);
+  }
+
+  @Override
+  public List<HRegionInfo> listOnlineRegionsOfGroup(String groupName) throws IOException {
+    return proxy.listOnlineRegionsOfGroup(groupName);
+  }
+
+  @Override
+  public Collection<String> listTablesOfGroup(String groupName) throws IOException {
+    return proxy.listTablesOfGroup(groupName);
+  }
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+    return proxy.getGroupInfo(groupName);
+  }
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(byte[] tableName) throws IOException {
+    return proxy.getGroupInfoOfTable(tableName);
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup) throws IOException, InterruptedException {
+    proxy.moveServers(servers, targetGroup);
+    waitForTransitions(servers);
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    proxy.addGroup(groupName);
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    proxy.removeGroup(name);
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    return proxy.listGroups();
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    return proxy.getGroupOfServer(hostPort);
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    return proxy.listServersInTransition();
+  }
+
+  private void waitForTransitions(Set<String> servers) throws IOException, InterruptedException {
+    long endTime = System.currentTimeMillis()+operationTimeout;
+    boolean found;
+    do {
+      found = false;
+      for(String server: proxy.listServersInTransition().keySet()) {
+        found = found || servers.contains(server);
+      }
+      Thread.sleep(1000);
+    } while(found && System.currentTimeMillis() <= endTime);
+    if(found) {
+      throw new DoNotRetryIOException("Operation timed out.");
+    }
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index 71711c1..cb9961f 100644
--- src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -22,6 +22,7 @@ package org.apache.hadoop.hbase.client;
 import java.io.Closeable;
 import java.io.IOException;
 import java.io.InterruptedIOException;
+import java.lang.reflect.Proxy;
 import java.net.SocketTimeoutException;
 import java.util.Arrays;
 import java.util.LinkedList;
@@ -55,8 +56,10 @@ import org.apache.hadoop.hbase.catalog.CatalogTracker;
 import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitorBase;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.ipc.MasterExecRPCInvoker;
 import org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.CompactionState;
 import org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException;
 import org.apache.hadoop.hbase.util.Addressing;
@@ -1814,4 +1817,20 @@ public class HBaseAdmin implements Abortable, Closeable {
     }
     return state;
   }
+
+  /**
+   * Creates and returns a proxy to the CoprocessorProtocol instance running in the
+   * master.
+   *
+   * @param protocol The class or interface defining the remote protocol
+   * @return A CoprocessorProtocol instance
+   */
+  public <T extends CoprocessorProtocol> T coprocessorProxy(
+      Class<T> protocol) {
+    return (T) Proxy.newProxyInstance(this.getClass().getClassLoader(),
+        new Class[]{protocol},
+        new MasterExecRPCInvoker(conf,
+            connection,
+            protocol));
+  }
 }
diff --git src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExec.java src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExec.java
new file mode 100644
index 0000000..66a538e
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExec.java
@@ -0,0 +1,110 @@
+/*
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client.coprocessor;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.client.Row;
+import org.apache.hadoop.hbase.io.HbaseObjectWritable;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+import org.apache.hadoop.hbase.ipc.Invocation;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Classes;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.lang.reflect.Method;
+
+/**
+ * Represents an arbitrary method invocation against a Coprocessor
+ * instance.  In order for a coprocessor implementation to be remotely callable
+ * by clients, it must define and implement a {@link org.apache.hadoop.hbase.ipc.CoprocessorProtocol}
+ * subclass.  Only methods defined in the {@code CoprocessorProtocol} interface
+ * will be callable by clients.
+ *
+ * <p>
+ * This class is used internally by
+ * {@link org.apache.hadoop.hbase.client.HBaseAdmin#coprocessorProxy(Class)}
+ * to wrap the {@code CoprocessorProtocol} method invocations requested in
+ * RPC calls.  It should not be used directly by HBase clients.
+ * </p>
+ *
+ * @see org.apache.hadoop.hbase.client.coprocessor.MasterExecResult
+ * @see org.apache.hadoop.hbase.client.HBaseAdmin#coprocessorProxy(Class)
+ */
+public class MasterExec extends Invocation {
+  private Class<? extends CoprocessorProtocol> protocol;
+  private String protocolName;
+
+  public MasterExec() {
+  }
+
+  public MasterExec(Configuration configuration,
+                    Class<? extends CoprocessorProtocol> protocol,
+                    Method method, Object[] parameters) {
+    super(method, protocol, parameters);
+    this.conf = configuration;
+    this.protocol = protocol;
+    this.protocolName = protocol.getName();
+  }
+
+  public String getProtocolName() {
+    return protocolName;
+  }
+
+  public Class<? extends CoprocessorProtocol> getProtocol() {
+    return protocol;
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    // fields for Invocation
+    out.writeUTF(this.methodName);
+    out.writeInt(parameterClasses.length);
+    for (int i = 0; i < parameterClasses.length; i++) {
+      HbaseObjectWritable.writeObject(out, parameters[i],
+          parameters[i] != null ? parameters[i].getClass() : parameterClasses[i],
+          conf);
+      out.writeUTF(parameterClasses[i].getName());
+    }
+    out.writeUTF(protocol.getName());
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    // fields for Invocation
+    methodName = in.readUTF();
+    parameters = new Object[in.readInt()];
+    parameterClasses = new Class[parameters.length];
+    HbaseObjectWritable objectWritable = new HbaseObjectWritable();
+    for (int i = 0; i < parameters.length; i++) {
+      parameters[i] = HbaseObjectWritable.readObject(in, objectWritable,
+        this.conf);
+      String parameterClassName = in.readUTF();
+      try {
+        parameterClasses[i] = Classes.extendedForName(parameterClassName);
+      } catch (ClassNotFoundException e) {
+        throw new IOException("Couldn't find class: " + parameterClassName);
+      }
+    }
+    // fields for Exec
+    protocolName = in.readUTF();
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExecResult.java src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExecResult.java
new file mode 100644
index 0000000..a8a9dc2
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/client/coprocessor/MasterExecResult.java
@@ -0,0 +1,70 @@
+/*
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.client.coprocessor;
+
+import org.apache.hadoop.hbase.io.HbaseObjectWritable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Writable;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+/**
+ * Represents the return value from a
+ * {@link MasterExec} invocation.
+ * This simply wraps the value for easier
+ * {@link org.apache.hadoop.hbase.io.HbaseObjectWritable}
+ * serialization.
+ *
+ * <p>
+ * This class is used internally by the HBaseAdmin client code to properly serialize
+ * responses from {@link org.apache.hadoop.hbase.ipc.CoprocessorProtocol}
+ * method invocations.  It should not be used directly by clients.
+ * </p>
+ *
+ * @see org.apache.hadoop.hbase.client.coprocessor.MasterExec
+ * @see org.apache.hadoop.hbase.client.HBaseAdmin#coprocessorProxy(Class)
+ */
+public class MasterExecResult implements Writable {
+  private Object value;
+
+  public MasterExecResult() {
+  }
+
+  public MasterExecResult(Object value) {
+    this.value = value;
+  }
+
+  public Object getValue() {
+    return value;
+  }
+
+  @Override
+  public void write(DataOutput out) throws IOException {
+    HbaseObjectWritable.writeObject(out, value,
+        value != null ? value.getClass() : Writable.class, null);
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    value = HbaseObjectWritable.readObject(in, null);
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
index d1d1990..d6f5327 100644
--- src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
+++ src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
@@ -63,6 +63,7 @@ import org.apache.hadoop.hbase.client.Row;
 import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExec;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.BitComparator;
 import org.apache.hadoop.hbase.filter.ColumnCountGetFilter;
@@ -266,6 +267,9 @@ public class HbaseObjectWritable implements Writable, WritableWithSize, Configur
     GENERIC_ARRAY_CODE = code++;
     addToMap(Array.class, GENERIC_ARRAY_CODE);
 
+    //master coprocessor
+    addToMap(MasterExec.class, code++);
+
     // make sure that this is the last statement in this static block
     NEXT_CLASS_CODE = code;
   }
diff --git src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
index 645b306..7878bc5 100644
--- src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
+++ src/main/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
@@ -26,6 +26,8 @@ import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.UnknownRegionException;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExec;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExecResult;
 import org.apache.hadoop.hbase.security.TokenInfo;
 import org.apache.hadoop.hbase.security.KerberosInfo;
 import org.apache.hadoop.hbase.util.Pair;
@@ -266,4 +268,22 @@ public interface HMasterInterface extends VersionedProtocol {
    * @return array of HTableDescriptor
    */
   public HTableDescriptor[] getHTableDescriptors(List<String> tableNames);
+
+  /**
+   * Executes a single {@link org.apache.hadoop.hbase.ipc.CoprocessorProtocol}
+   * method using the registered protocol handlers.
+   * {@link CoprocessorProtocol} implementations must be registered via the
+   * {@link org.apache.hadoop.hbase.master.MasterServices#registerProtocol(Class, CoprocessorProtocol)}
+   * method before they are available.
+   *
+   * @param call an {@code Exec} instance identifying the protocol, method name,
+   *     and parameters for the method invocation
+   * @return an {@code ExecResult} instance containing the region name of the
+   *     invocation and the return value
+   * @throws IOException if no registered protocol handler is found or an error
+   *     occurs during the invocation
+   * @see org.apache.hadoop.hbase.master.MasterServices#registerProtocol(Class, CoprocessorProtocol)
+   */
+  public MasterExecResult execCoprocessor(MasterExec call)
+      throws IOException;
 }
diff --git src/main/java/org/apache/hadoop/hbase/ipc/MasterExecRPCInvoker.java src/main/java/org/apache/hadoop/hbase/ipc/MasterExecRPCInvoker.java
new file mode 100644
index 0000000..f9a90a3
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/ipc/MasterExecRPCInvoker.java
@@ -0,0 +1,69 @@
+/*
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.ipc;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.client.HConnection;
+import org.apache.hadoop.hbase.client.ServerCallable;
+import org.apache.hadoop.hbase.client.coprocessor.Exec;
+import org.apache.hadoop.hbase.client.coprocessor.ExecResult;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExec;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExecResult;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import java.lang.reflect.InvocationHandler;
+import java.lang.reflect.Method;
+
+/**
+ * Backs a {@link org.apache.hadoop.hbase.ipc.CoprocessorProtocol} subclass proxy and forwards method
+ * invocations for server execution.  Note that internally this will issue a
+ * separate RPC call for each method invocation.
+ */
+public class MasterExecRPCInvoker implements InvocationHandler {
+  // LOG is NOT in hbase subpackage intentionally so that the default HBase
+  // DEBUG log level does NOT emit RPC-level logging.
+  private static final Log LOG = LogFactory.getLog("org.apache.hadoop.ipc.MasterExecRPCInvoker");
+
+  private Configuration conf;
+  private final HConnection connection;
+  private Class<? extends CoprocessorProtocol> protocol;
+
+  public MasterExecRPCInvoker(Configuration conf,
+                              HConnection connection,
+                              Class<? extends CoprocessorProtocol> protocol) {
+    this.conf = conf;
+    this.connection = connection;
+    this.protocol = protocol;
+  }
+
+  @Override
+  public Object invoke(Object instance, final Method method, final Object[] args)
+      throws Throwable {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Call: "+method.getName()+", "+(args != null ? args.length : 0));
+    }
+    MasterExec exec = new MasterExec(conf, protocol, method, args);
+    MasterExecResult result = connection.getMaster().execCoprocessor(exec);
+    LOG.debug("Master Result is value="+result.getValue());
+    return result.getValue();
+  }
+}
\ No newline at end of file
diff --git src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 503061d..d675bd3 100644
--- src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -1653,7 +1653,7 @@ public class AssignmentManager extends ZooKeeperListener {
       }
       try {
         LOG.debug("Assigning region " + state.getRegion().getRegionNameAsString() +
-          " to " + plan.getDestination().toString());
+          " to " + plan.getDestination());
         // Transition RegionState to PENDING_OPEN
         state.update(RegionState.State.PENDING_OPEN, System.currentTimeMillis(),
             plan.getDestination());
@@ -1904,7 +1904,7 @@ public class AssignmentManager extends ZooKeeperListener {
           || drainingServers.contains(existingPlan.getDestination())) {
         newPlan = true;
         randomPlan = new RegionPlan(state.getRegion(), null, balancer
-            .randomAssignment(servers));
+            .randomAssignment(state.getRegion(), servers));
         this.regionPlans.put(encodedName, randomPlan);
       }
     }
diff --git src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
index 82a33e0..a5af1e1 100644
--- src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
+++ src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java
@@ -43,7 +43,6 @@ import org.apache.hadoop.hbase.HDFSBlocksDistribution;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -768,7 +767,7 @@ public class DefaultLoadBalancer implements LoadBalancer {
     return assignments;
   }
 
-  public ServerName randomAssignment(List<ServerName> servers) {
+  public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
     if (servers == null || servers.isEmpty()) {
       LOG.warn("Wanted to do random assignment but no servers to assign to");
       return null;
@@ -776,4 +775,8 @@ public class DefaultLoadBalancer implements LoadBalancer {
     return servers.get(RANDOM.nextInt(servers.size()));
   }
 
+  @Override
+  public void configure() throws IOException {
+  }
+
 }
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupAdmin.java src/main/java/org/apache/hadoop/hbase/master/GroupAdmin.java
new file mode 100644
index 0000000..72eb9ba
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupAdmin.java
@@ -0,0 +1,110 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.hadoop.hbase.HRegionInfo;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+public interface GroupAdmin {
+  /**
+   * Get online regions of a region server group.
+   *
+   * @param groupName the name of the group
+   * @return list of online regions this group contains
+   */
+  List<HRegionInfo> listOnlineRegionsOfGroup(String groupName) throws IOException;
+
+  /**
+   * Get member tables of a group.
+   *
+   * @param groupName the name of the group
+   * @return list of table names
+   */
+  Collection<String> listTablesOfGroup(String groupName) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroupInfo(String groupName) throws IOException;
+
+  /**
+   * Gets the group info of table.
+   *
+   * @param tableName the table name
+   * @return An instance of GroupInfo.
+   */
+  GroupInfo getGroupInfoOfTable(byte[] tableName) throws IOException;
+
+  /**
+   * Move a set of serves to another group
+   *
+   * @param server the server
+   * @param targetGroup the target group
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   * @throws InterruptedException the interrupted exception
+   */
+  void moveServers(Set<String> server, String targetGroup)
+      throws IOException, InterruptedException;
+
+
+  /**
+   * Add a new group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void addGroup(String name) throws IOException;
+
+  /**
+   * Remove a new group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void removeGroup(String name) throws IOException;
+
+  /**
+   * Gets the existing groups.
+   *
+   * @return Collection of GroupInfo.
+   */
+  List<GroupInfo> listGroups() throws IOException;
+
+  /**
+   * Retrieve the GroupInfo a server is affiliated to
+   * @param hostPort
+   * @return
+   * @throws IOException
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * List servers that are currently being moved to a new group
+   * @return
+   * @throws IOException
+   */
+  Map<String, String> listServersInTransition() throws IOException;
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupAdminEndpoint.java src/main/java/org/apache/hadoop/hbase/master/GroupAdminEndpoint.java
new file mode 100644
index 0000000..f222e98
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupAdminEndpoint.java
@@ -0,0 +1,222 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingDeque;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+/**
+ * Service to support Region Server Grouping (HBase-6721)
+ * This should be installed as a Master CoprocessorEndpoint
+ */
+public class GroupAdminEndpoint extends BaseEndpointCoprocessor
+    implements GroupAdminProtocol {
+	private static final Log LOG = LogFactory.getLog(GroupAdminEndpoint.class);
+
+  private final long threadKeepAliveTimeInMillis = 1000;
+  private int threadMax = 1;
+  private BlockingQueue<Runnable> threadQ;
+  private MasterCoprocessorEnvironment menv;
+  private MasterServices master;
+  private ExecutorService executorService;
+  //List of servers that are being moved from one group to another
+  //Key=host:port,Value=targetGroup
+  private ConcurrentMap<String,String> serversInTransition =
+      new ConcurrentHashMap<String,String>();
+
+  @Override
+  public void start(CoprocessorEnvironment env) {
+    menv = (MasterCoprocessorEnvironment)env;
+    master = menv.getMasterServices();
+    threadQ = new LinkedBlockingDeque<Runnable>();
+    threadMax = menv.getConfiguration().getInt("hbase.group.executor.threads", 1);
+    executorService = new ThreadPoolExecutor(threadMax, threadMax,
+        threadKeepAliveTimeInMillis, TimeUnit.MILLISECONDS, threadQ);
+  }
+
+  @Override
+  public void stop(CoprocessorEnvironment env) {
+    executorService.shutdown();
+  }
+
+  @Override
+  public List<HRegionInfo> listOnlineRegionsOfGroup(String groupName) throws IOException {
+		if (groupName == null) {
+      throw new NullPointerException("groupName can't be null");
+    }
+
+		List<HRegionInfo> regions = new ArrayList<HRegionInfo>();
+    GroupInfo groupInfo = getGroupInfoManager().getGroup(groupName);
+    if (groupInfo == null) {
+			return null;
+		} else {
+			Set<String> servers = groupInfo.getServers();
+      Map<String,List<HRegionInfo>> assignments = getOnlineRegions();
+      for(ServerName serverName: master.getServerManager().getOnlineServersList()) {
+        String hostPort = serverName.getHostAndPort();
+        if(servers.contains(hostPort) && assignments.containsKey(hostPort)) {
+          regions.addAll(assignments.get(hostPort));
+        }
+			}
+		}
+		return regions;
+	}
+
+  @Override
+  public Collection<String> listTablesOfGroup(String groupName) throws IOException {
+		Set<String> set = new HashSet<String>();
+		if (groupName == null) {
+      throw new NullPointerException("groupName can't be null");
+    }
+
+    GroupInfo groupInfo = getGroupInfoManager().getGroup(groupName);
+    if (groupInfo == null) {
+			return null;
+		} else {
+      HTableDescriptor[] tables =
+          master.getTableDescriptors().getAll().values().toArray(new HTableDescriptor[0]);
+      for (HTableDescriptor table : tables) {
+        if(GroupInfo.getGroupProperty(table).equals(groupName))
+          set.add(table.getNameAsString());
+      }
+    }
+		return set;
+	}
+
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+			return getGroupInfoManager().getGroup(groupName);
+	}
+
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(byte[] tableName) throws IOException {
+		HTableDescriptor des;
+		GroupInfo tableRSGroup;
+    des =  master.getTableDescriptors().get(tableName);
+		String group = GroupInfo.getGroupProperty(des);
+		tableRSGroup = getGroupInfoManager().getGroup(group);
+		return tableRSGroup;
+	}
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup)
+			throws IOException {
+		if (servers == null) {
+			throw new IOException(
+					"The list of servers cannot be null.");
+		}
+    if (StringUtils.isEmpty(targetGroup)) {
+			throw new IOException(
+					"The target group cannot be null.");
+    }
+
+    GroupMoveServerWorker.MoveServerPlan plan =
+        new GroupMoveServerWorker.MoveServerPlan(servers, targetGroup);
+    GroupMoveServerWorker worker = null;
+    try {
+      worker = new GroupMoveServerWorker(master, serversInTransition, getGroupInfoManager(), plan);
+      executorService.submit(worker);
+      LOG.info("GroupMoveServerHanndlerSubmitted: "+plan.getTargetGroup());
+    } catch(Exception e) {
+      LOG.error("Failed to submit GroupMoveServerWorker", e);
+      if(worker != null) {
+        worker.complete();
+      }
+      throw new DoNotRetryIOException("Failed to submit GroupMoveServerWorker",e);
+    }
+	}
+
+  @Override
+  public void addGroup(String name) throws IOException {
+    getGroupInfoManager().addGroup(new GroupInfo(name, new HashSet<String>()));
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      if(listTablesOfGroup(name).size() > 0) {
+        throw new DoNotRetryIOException("Group "+name+" must have no associated tables.");
+      }
+      manager.removeGroup(name);
+    }
+
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    return getGroupInfoManager().listGroups();
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    return getGroupInfoManager().getGroupOfServer(hostPort);
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    return Collections.unmodifiableMap(serversInTransition);
+  }
+
+  private GroupInfoManager getGroupInfoManager() {
+    return ((GroupBasedLoadBalancer)menv.getMasterServices().getLoadBalancer()).getGroupInfoManager();
+  }
+
+  private Map<String,List<HRegionInfo>> getOnlineRegions() throws IOException {
+    Map<String,List<HRegionInfo>> result = new HashMap<String, List<HRegionInfo>>();
+    for(Map.Entry<ServerName, java.util.List<HRegionInfo>> el:
+        master.getAssignmentManager().getAssignments().entrySet()) {
+      if(!result.containsKey(el.getKey().getHostAndPort())) {
+        result.put(el.getKey().getHostAndPort(),new LinkedList<HRegionInfo>());
+      }
+      result.get(el.getKey().getHostAndPort()).addAll(el.getValue());
+    }
+    return result;
+  }
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupAdminProtocol.java src/main/java/org/apache/hadoop/hbase/master/GroupAdminProtocol.java
new file mode 100644
index 0000000..5d230e4
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupAdminProtocol.java
@@ -0,0 +1,25 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
+
+public interface GroupAdminProtocol extends GroupAdmin, CoprocessorProtocol {
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupBasedLoadBalancer.java src/main/java/org/apache/hadoop/hbase/master/GroupBasedLoadBalancer.java
new file mode 100644
index 0000000..682cebe
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupBasedLoadBalancer.java
@@ -0,0 +1,325 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import com.google.common.collect.ListMultimap;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+
+import com.google.common.collect.ArrayListMultimap;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * GroupBasedLoadBalancer, used when Region Server Grouping is configured (HBase-6721)
+ * It does region balance based on a table's group membership.
+ */
+public class GroupBasedLoadBalancer implements LoadBalancer {
+  /** Config for pluggable load balancers */
+  public static final String HBASE_GROUP_LOADBALANCER_CLASS = "hbase.group.grouploadbalancer.class";
+
+  private static final Log LOG = LogFactory
+      .getLog(GroupBasedLoadBalancer.class);
+  private Configuration config;
+  private ClusterStatus clusterStatus;
+  private MasterServices masterServices;
+  private GroupInfoManager groupManager;
+  private LoadBalancer internalBalancer;
+
+  GroupBasedLoadBalancer() {
+    this(null);
+  }
+
+  GroupBasedLoadBalancer(GroupInfoManager groupManager) {
+    this.groupManager = groupManager;
+  }
+
+  @Override
+  public Configuration getConf() {
+    return config;
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.config = conf;
+  }
+
+  @Override
+  public void setClusterStatus(ClusterStatus st) {
+    this.clusterStatus = st;
+  }
+
+  @Override
+  public void setMasterServices(MasterServices masterServices) {
+    this.masterServices = masterServices;
+  }
+
+  @Override
+  public List<RegionPlan> balanceCluster(
+      Map<ServerName, List<HRegionInfo>> clusterState) {
+    Map<ServerName,List<HRegionInfo>> correctedState = correctAssignments(clusterState);
+    List<RegionPlan> regionPlans = new ArrayList<RegionPlan>();
+    try {
+      for (GroupInfo info : groupManager.listGroups()) {
+        Map<ServerName, List<HRegionInfo>> groupClusterState = new HashMap<ServerName, List<HRegionInfo>>();
+        for (String sName : info.getServers()) {
+          ServerName actual = ServerName.findServerWithSameHostnamePort(
+              clusterState.keySet(), ServerName.parseServerName(sName));
+          if (actual != null) {
+            groupClusterState.put(actual, correctedState.get(actual));
+          }
+        }
+        List<RegionPlan> groupPlans = this.internalBalancer
+            .balanceCluster(groupClusterState);
+        if (groupPlans != null) {
+          regionPlans.addAll(groupPlans);
+        }
+      }
+    } catch (IOException exp) {
+      LOG.warn("Exception while balancing cluster.", exp);
+    }
+    return regionPlans;
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(
+      List<HRegionInfo> regions, List<ServerName> servers) {
+    try {
+      Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+      ListMultimap<String, HRegionInfo> regionGroup = groupRegions(regions);
+      for (String groupKey : regionGroup.keys()) {
+        GroupInfo info = groupManager.getGroup(groupKey);
+        assignments.putAll(this.internalBalancer.roundRobinAssignment(
+            regionGroup.get(groupKey), getServerToAssign(info, servers)));
+      }
+      return assignments;
+    } catch (IOException e) {
+      LOG.error("Failed to access group store", e);
+      throw new IllegalStateException("Failed to access group store", e);
+    }
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> retainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) {
+    try {
+      Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+      ListMultimap<String, HRegionInfo> rGroup = ArrayListMultimap.create();
+      List<HRegionInfo> misplacedRegions = getMisplacedRegions(regions);
+      for (HRegionInfo region : regions.keySet()) {
+        if (misplacedRegions.contains(region) == false) {
+          String groupName = masterServices
+              .getTableDescriptors().get(region.getTableNameAsString()).getValue(GroupInfo.GROUP_KEY);
+          rGroup.put(groupName, region);
+        }
+      }
+      // Now the "rGroup" map has only the regions which have correct
+      // assignments.
+      for (String key : rGroup.keys()) {
+        Map<HRegionInfo, ServerName> currentAssignmentMap = new TreeMap<HRegionInfo, ServerName>();
+        List<HRegionInfo> regionList = rGroup.get(key);
+        GroupInfo info = groupManager.getGroup(key);
+        List<ServerName> candidateList = getServerToAssign(info, servers);
+        for (HRegionInfo region : regionList) {
+          currentAssignmentMap.put(region, regions.get(region));
+        }
+        assignments.putAll(this.internalBalancer.retainAssignment(
+            currentAssignmentMap, candidateList));
+      }
+
+      for (HRegionInfo region : misplacedRegions) {
+          String groupName = masterServices
+              .getTableDescriptors().get(region.getTableNameAsString()).getValue(GroupInfo.GROUP_KEY);
+        GroupInfo info = groupManager.getGroup(groupName);
+        List<ServerName> candidateList = getServerToAssign(info, servers);
+        ServerName server = this.internalBalancer.randomAssignment(region,
+            candidateList);
+        if (assignments.containsKey(server) == false) {
+          assignments.put(server, new ArrayList<HRegionInfo>());
+        }
+        assignments.get(server).add(region);
+      }
+      return assignments;
+    } catch (IOException e) {
+      LOG.error("Failed to access group store", e);
+      throw new IllegalStateException("Failed to access group store", e);
+    }
+  }
+
+  @Override
+  public Map<HRegionInfo, ServerName> immediateAssignment(
+      List<HRegionInfo> regions, List<ServerName> servers) {
+    try {
+      Map<HRegionInfo, ServerName> assignments = new TreeMap<HRegionInfo, ServerName>();
+      // Need to group regions by the group and servers and then call the
+      // internal load balancer.
+      ListMultimap<String, HRegionInfo> regionGroups = groupRegions(regions);
+      for (String key : regionGroups.keys()) {
+        List<HRegionInfo> regionsOfSameGroup = regionGroups.get(key);
+        GroupInfo info = groupManager.getGroup(key);
+        List<ServerName> candidateList = getServerToAssign(info, servers);
+        assignments.putAll(this.internalBalancer.immediateAssignment(
+            regionsOfSameGroup, candidateList));
+      }
+      return assignments;
+    } catch (IOException e) {
+      LOG.error("Failed to access group store", e);
+      throw new IllegalStateException("Failed to access group store", e);
+    }
+  }
+
+  @Override
+  public ServerName randomAssignment(HRegionInfo region,
+      List<ServerName> servers) {
+    try {
+      String tableName = region.getTableNameAsString();
+      List<ServerName> candidateList;
+      GroupInfo groupInfo = groupManager.getGroup(GroupInfo
+          .getGroupProperty(masterServices.getTableDescriptors()
+              .get(tableName)));
+      candidateList = getServerToAssign(groupInfo, servers);
+      return this.internalBalancer.randomAssignment(region, candidateList);
+    } catch (IOException e) {
+      LOG.error("Failed to access group store", e);
+      throw new IllegalStateException("Failed to access group store", e);
+    }
+  }
+
+  private List<ServerName> getServerToAssign(GroupInfo groupInfo,
+      List<ServerName> onlineServers) {
+    if (groupInfo != null) {
+      return filterServers(groupInfo.getServers(), onlineServers);
+    } else {
+      LOG.debug("Group Information found to be null. Some regions might be unassigned.");
+      return new ArrayList<ServerName>();
+    }
+  }
+
+  /**
+   * Filter servers based on the online servers.
+   *
+   * @param servers
+   *          the servers
+   * @param onlineServers
+   *          List of servers which are online.
+   * @return the list
+   */
+  private List<ServerName> filterServers(Collection<String> servers,
+      Collection<ServerName> onlineServers) {
+    ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+    for (String server : servers) {
+      ServerName actual = ServerName.findServerWithSameHostnamePort(
+          onlineServers, ServerName.parseServerName(server));
+      if (actual != null) {
+        finalList.add(actual);
+      }
+    }
+    return finalList;
+  }
+
+  private ListMultimap<String, HRegionInfo> groupRegions(
+      List<HRegionInfo> regionList) throws IOException {
+    ListMultimap<String, HRegionInfo> regionGroup = ArrayListMultimap
+        .create();
+    for (HRegionInfo region : regionList) {
+      String groupName = GroupInfo.getGroupProperty(masterServices
+          .getTableDescriptors().get(region.getTableNameAsString()));
+      regionGroup.put(groupName, region);
+    }
+    return regionGroup;
+  }
+
+  private List<HRegionInfo> getMisplacedRegions(
+      Map<HRegionInfo, ServerName> regions) throws IOException {
+    List<HRegionInfo> misplacedRegions = new ArrayList<HRegionInfo>();
+    for (HRegionInfo region : regions.keySet()) {
+      ServerName assignedServer = regions.get(region);
+      GroupInfo info = groupManager.getGroup(GroupInfo
+          .getGroupProperty(masterServices.getTableDescriptors().get(
+              region.getTableNameAsString())));
+      if ((info == null)|| (!info.containsServer(assignedServer.getHostAndPort()))) {
+        misplacedRegions.add(region);
+      }
+    }
+    return misplacedRegions;
+  }
+
+  private Map<ServerName, List<HRegionInfo>> correctAssignments(
+       Map<ServerName, List<HRegionInfo>> existingAssignments){
+    Map<ServerName, List<HRegionInfo>> correctAssignments = new TreeMap<ServerName, List<HRegionInfo>>();
+    List<HRegionInfo> misplacedRegions = new ArrayList<HRegionInfo>();
+    for (ServerName sName : existingAssignments.keySet()) {
+      correctAssignments.put(sName, new ArrayList<HRegionInfo>());
+      List<HRegionInfo> regions = existingAssignments.get(sName);
+      for (HRegionInfo region : regions) {
+        GroupInfo info = null;
+        try {
+          info = groupManager.getGroup(GroupInfo.getGroupProperty(masterServices
+              .getTableDescriptors().get(region.getTableNameAsString())));
+        }catch(IOException exp){
+          LOG.debug("Group information null for region of table " + region.getTableNameAsString(),
+              exp);
+        }
+        if ((info == null) || (!info.containsServer(sName.getHostAndPort()))) {
+          // Misplaced region.
+          misplacedRegions.add(region);
+        } else {
+          correctAssignments.get(sName).add(region);
+        }
+      }
+    }
+
+    //unassign misplaced regions, so that they are assigned to correct groups.
+    this.masterServices.getAssignmentManager().unassign(misplacedRegions);
+    return correctAssignments;
+  }
+
+  GroupInfoManager getGroupInfoManager() {
+    return groupManager;
+  }
+
+  @Override
+  public void configure() throws IOException {
+    if (this.groupManager == null) {
+        this.groupManager = new GroupInfoManagerImpl(masterServices.getConfiguration(), masterServices);
+    }
+    // Create the balancer
+    Class<? extends LoadBalancer> balancerKlass = config.getClass(
+        HBASE_GROUP_LOADBALANCER_CLASS,
+        DefaultLoadBalancer.class, LoadBalancer.class);
+    internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);
+    internalBalancer.setClusterStatus(clusterStatus);
+    internalBalancer.setMasterServices(masterServices);
+    internalBalancer.setConf(config);
+    internalBalancer.configure();
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupInfo.java src/main/java/org/apache/hadoop/hbase/master/GroupInfo.java
new file mode 100644
index 0000000..3ffdf9b
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupInfo.java
@@ -0,0 +1,245 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.NavigableSet;
+import java.util.Set;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.lang.StringUtils;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Stores the group information of region server groups.
+ */
+public class GroupInfo implements Serializable {
+
+	private Set<String> servers;
+	public static final String DEFAULT_GROUP = "default";
+  public static final String TRANSITION_GROUP_PREFIX = "_transition_";
+	public static final String GROUP_KEY = "rs_group";
+
+	private String name;
+
+  public GroupInfo() {
+    this.servers = new HashSet<String>();
+  }
+
+	public GroupInfo(String name, Set<String> servers) {
+		this.name = name;
+    this.servers = servers;
+	}
+
+  public GroupInfo(GroupInfo src) {
+    servers = Sets.newTreeSet(src.getServers());
+    name = src.getName();
+  }
+
+	/**
+	 * Get group name.
+	 *
+	 * @return
+	 */
+	public String getName() {
+		return name;
+	}
+
+	/**
+	 * Adds the server to the group.
+	 *
+	 * @param hostPort the server
+	 */
+	public void addServer(String hostPort){
+		this.servers.add(hostPort);
+	}
+
+	/**
+	 * Adds a group of servers.
+	 *
+	 * @param hostPort the servers
+	 */
+	public void addAll(Collection<String> hostPort){
+		this.servers.addAll(hostPort);
+	}
+
+	public boolean containsServer(String hostPort) {
+    return servers.contains(hostPort);
+	}
+
+	/**
+	 * Checks based of equivalence of host name and port.
+	 *
+	 * @param serverList The list to check for containment.
+	 * @return true, if successful
+	 */
+	public boolean containsServer(Set<String> serverList) {
+		if (serverList.size() == 0) {
+			return false;
+		} else {
+			boolean contains = true;
+			for (String hostPort : serverList) {
+				contains = contains && this.getServers().contains(hostPort);
+				if (!contains)
+					return contains;
+			}
+			return contains;
+		}
+	}
+
+
+	/**
+	 * Get a copy of servers.
+	 *
+	 * @return
+	 */
+	public Set<String> getServers() {
+		return this.servers;
+	}
+
+	/**
+	 * Write the group out.
+	 *
+	 * @param out
+	 * @throws IOException
+	 */
+	public void write(BufferedWriter out) throws IOException {
+		StringBuffer sb = new StringBuffer();
+		sb.append(getName());
+		sb.append("\t");
+		for (String sName : servers) {
+			if (sb.length() != (getName().length() + 1)) {
+				sb.append(",");
+			}
+			sb.append(sName);
+		}
+		out.write(sb.toString());
+		out.newLine();
+	}
+
+	public boolean readFields(String line) throws IOException {
+		boolean isWellFormed = false;
+		String[] groupSplit = line.split("\t");
+		switch(groupSplit.length) {
+		case 1: this.name = groupSplit[0].trim();
+				isWellFormed = true;
+				break;
+		case 2: this.name = groupSplit[0].trim();
+				String[] hostPortPairs = groupSplit[1].trim().split(",");
+				for (String sName : hostPortPairs) {
+					if (StringUtils.isNotEmpty(sName)) {
+						this.servers.add(sName);
+					}
+				}
+				isWellFormed = true;
+				break;
+		}
+
+		return isWellFormed;
+	}
+
+	/**
+	 * Remove a server from this group.
+	 *
+	 * @param hostPort
+	 */
+	public boolean removeServer(String hostPort) {
+    return this.servers.remove(hostPort);
+	}
+
+	/**
+	 * Get group attribute from a table descriptor.
+	 *
+	 * @param des
+	 * @return The group name of the table.
+	 */
+	public static String getGroupProperty(HTableDescriptor des) {
+		String group = des.getValue(GROUP_KEY);
+		if (group!= null) {
+			return group;
+		} else {
+			return GroupInfo.DEFAULT_GROUP;
+    }
+	}
+
+
+	public static void setGroupProperty(String group, HTableDescriptor des) {
+    if(group.equals(DEFAULT_GROUP)) {
+      des.remove(group);
+    }
+    else {
+		  des.setValue(GROUP_KEY, group);
+    }
+	}
+
+	@Override
+	public String toString() {
+		StringBuffer sb = new StringBuffer();
+		sb.append("{GroupName:");
+		sb.append(this.name);
+		sb.append("-");
+		sb.append(" Severs:");
+		sb.append(this.servers+ "}");
+		return sb.toString();
+
+	}
+
+	@Override
+	public int hashCode() {
+		final int prime = 31;
+		int result = 1;
+		result = prime * result + ((name == null) ? 0 : name.hashCode());
+		result = prime * result
+				+ ((servers == null) ? 0 : servers.hashCode());
+		return result;
+	}
+
+	@Override
+	public boolean equals(Object obj) {
+		if (this == obj)
+			return true;
+		if (obj == null)
+			return false;
+		if (!(obj instanceof GroupInfo))
+			return false;
+		GroupInfo other = (GroupInfo) obj;
+		if (name == null) {
+			if (other.name != null)
+				return false;
+		} else if (!name.equals(other.name))
+			return false;
+		if (servers == null) {
+			if (other.servers != null)
+				return false;
+		} else if (servers.size() != other.getServers().size()){
+			return false;
+		}else if(!containsServer(other.getServers())){
+			return false;
+		}
+
+		return true;
+	}
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupInfoManager.java src/main/java/org/apache/hadoop/hbase/master/GroupInfoManager.java
new file mode 100644
index 0000000..4334091
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupInfoManager.java
@@ -0,0 +1,65 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master;
+
+import org.apache.hadoop.hbase.HTableDescriptor;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+public interface GroupInfoManager {
+  /**
+   * Adds the group.
+   *
+   * @param groupInfo the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void addGroup(GroupInfo groupInfo) throws IOException;
+
+  /**
+   * Remove a region server group.
+   *
+   * @param groupName the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void removeGroup(String groupName) throws IOException;
+
+  boolean moveServers(Set<String> hostPort, String srcGroup, String dstGroup) throws IOException;
+
+  /**
+   * Gets the group info of server.
+   *
+   * @param hostPort the server
+   * @return An instance of GroupInfo.
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroup(String groupName) throws IOException;
+
+  List<GroupInfo> listGroups() throws IOException;
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupInfoManagerImpl.java src/main/java/org/apache/hadoop/hbase/master/GroupInfoManagerImpl.java
new file mode 100644
index 0000000..2ecc28c
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupInfoManagerImpl.java
@@ -0,0 +1,333 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.commons.io.output.ByteArrayOutputStream;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.util.FSUtils;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+
+import java.io.BufferedReader;
+import java.io.BufferedWriter;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.io.OutputStreamWriter;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeSet;
+import java.util.concurrent.ConcurrentHashMap;
+
+public class GroupInfoManagerImpl implements GroupInfoManager {
+	private static final Log LOG = LogFactory.getLog(GroupInfoManagerImpl.class);
+
+	//Access to this map should always be synchronized.
+	private Map<String, GroupInfo> groupMap;
+  private String znodePath;
+  private ZooKeeperWatcher watcher;
+  private MasterServices master;
+  private String groupZNode = "group";
+
+  public GroupInfoManagerImpl(Configuration conf, MasterServices master) throws IOException {
+		this.groupMap = new ConcurrentHashMap<String, GroupInfo>();
+    this.master = master;
+    this.watcher = master.getZooKeeper();
+    znodePath = ZKUtil.joinZNode(watcher.baseZNode, groupZNode);
+
+    try {
+      if(ZKUtil.checkExists(watcher,znodePath) == -1) {
+        ZKUtil.createSetData(watcher, znodePath, new byte[0]);
+      }
+    } catch (KeeperException e) {
+      throw new IOException("Failed to verify group znode",e);
+    }
+    reloadConfig();
+  }
+
+	/**
+	 * Adds the group.
+	 *
+	 * @param groupInfo the group name
+	 * @throws java.io.IOException Signals that an I/O exception has occurred.
+	 */
+  @Override
+  public synchronized void addGroup(GroupInfo groupInfo) throws IOException {
+		if (groupMap.get(groupInfo.getName()) != null ||
+        groupInfo.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new DoNotRetryIOException("Group already exists: "+groupInfo.getName());
+    }
+    groupMap.put(groupInfo.getName(), groupInfo);
+    try {
+      flushConfig();
+    } catch (IOException e) {
+      groupMap.remove(groupInfo.getName());
+      throw e;
+    }
+	}
+
+  @Override
+  public synchronized boolean moveServers(Set<String> hostPort, String srcGroup, String dstGroup) throws IOException {
+    GroupInfo src = new GroupInfo(getGroup(srcGroup));
+    GroupInfo dst = new GroupInfo(getGroup(dstGroup));
+    boolean foundOne = false;
+    for(String el: hostPort) {
+      foundOne = foundOne || src.removeServer(el);
+      dst.addServer(el);
+    }
+
+    Map<String,GroupInfo> newMap = Maps.newHashMap(groupMap);
+    if(!src.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newMap.put(src.getName(), src);
+    }
+    if(!dst.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newMap.put(dst.getName(), dst);
+    }
+    flushConfig(newMap);
+    groupMap = newMap;
+    return foundOne;
+  }
+
+  /**
+	 * Gets the group info of server.
+	 *
+	 * @param hostPort the server
+	 * @return An instance of GroupInfo.
+	 */
+  @Override
+  public synchronized GroupInfo getGroupOfServer(String hostPort) throws IOException {
+		for(GroupInfo info : groupMap.values()){
+			if(info.containsServer(hostPort)){
+				return info;
+			}
+		}
+		return getGroup(GroupInfo.DEFAULT_GROUP);
+	}
+
+	/**
+	 * Gets the group information.
+	 *
+	 * @param groupName the group name
+	 * @return An instance of GroupInfo
+	 */
+  @Override
+  public synchronized GroupInfo getGroup(String groupName) throws IOException {
+		if (groupName.equalsIgnoreCase(GroupInfo.DEFAULT_GROUP)) {
+			GroupInfo defaultInfo = new GroupInfo(GroupInfo.DEFAULT_GROUP, new TreeSet<String>());
+      List<ServerName> unassignedServers =
+          difference(getOnlineRS(),getAssignedServers());
+      for(ServerName serverName: unassignedServers) {
+        defaultInfo.addServer(serverName.getHostAndPort());
+      }
+			return defaultInfo;
+		} else {
+			return this.groupMap.get(groupName);
+		}
+	}
+
+
+
+	/**
+	 * Delete a region server group.
+	 *
+	 * @param groupName the group name
+	 * @throws IOException Signals that an I/O exception has occurred.
+	 */
+  @Override
+  public synchronized void removeGroup(String groupName) throws IOException {
+    GroupInfo group = null;
+    if(!groupMap.containsKey(groupName) || groupName.equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new IllegalArgumentException("Group "+groupName+" does not exist or is default group");
+    }
+    synchronized (groupMap) {
+      try {
+        group = groupMap.remove(groupName);
+        flushConfig();
+      } catch (IOException e) {
+        groupMap.put(groupName, group);
+        throw e;
+      }
+    }
+	}
+
+  @Override
+  public synchronized List<GroupInfo> listGroups() throws IOException {
+    List<GroupInfo> list = Lists.newLinkedList(groupMap.values());
+    list.add(getGroup(GroupInfo.DEFAULT_GROUP));
+    return list;
+  }
+
+	/**
+	 * Read group configuration from HDFS.
+	 *
+	 * @throws IOException
+	 */
+	synchronized void reloadConfig() throws IOException {
+		List<GroupInfo> groupList;
+    InputStream is = null;
+    try {
+      byte[] payload = ZKUtil.getData(watcher, znodePath);
+      synchronized (groupMap) {
+        this.groupMap.clear();
+        groupList = readGroups(new ByteArrayInputStream(payload));
+        for (GroupInfo group : groupList) {
+          groupMap.put(group.getName(), group);
+        }
+      }
+    } catch (KeeperException e) {
+      throw new IOException("Failed to read group znode", e);
+    } finally {
+      if(is != null) {
+        is.close();
+      }
+    }
+	}
+
+	/**
+	 * Write the configuration to HDFS.
+	 *
+	 * @throws IOException
+	 */
+	private synchronized void flushConfig() throws IOException {
+    flushConfig(groupMap);
+	}
+
+	private synchronized void flushConfig(Map<String,GroupInfo> map) throws IOException {
+    ByteArrayOutputStream os = new ByteArrayOutputStream();
+		try {
+			List<GroupInfo> groups = Lists.newArrayList(map.values());
+			writeGroups(groups, os);
+      ZKUtil.setData(watcher, znodePath, os.toByteArray());
+    } catch (KeeperException e) {
+      throw new IOException("Failed to write to group znode", e);
+    } finally {
+			if(os != null) {
+        os.close();
+      }
+		}
+	}
+
+	/**
+	 * Read a list of GroupInfo.
+	 *
+	 * @param in
+	 *            DataInput
+	 * @return
+	 * @throws IOException
+	 */
+	private static List<GroupInfo> readGroups(final InputStream in)
+			throws IOException {
+		List<GroupInfo> groupList = new ArrayList<GroupInfo>();
+		BufferedReader br = new BufferedReader(new InputStreamReader(in));
+		String line = null;
+		try {
+			while ((line = br.readLine()) != null && (line = line.trim()).length() > 0) {
+				GroupInfo group = new GroupInfo();
+				if (group.readFields(line)) {
+					if (group.getName().equalsIgnoreCase(GroupInfo.DEFAULT_GROUP))
+            throw new IOException("Config file contains default group!");
+          groupList.add(group);
+				}
+			}
+		} finally {
+			br.close();
+		}
+		return groupList;
+	}
+
+	/**
+	 * Write a list of group information out.
+	 *
+	 * @param groups
+	 * @param out
+	 * @throws IOException
+	 */
+	private static void writeGroups(Collection<GroupInfo> groups, OutputStream out)
+			throws IOException {
+		BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(out));
+		try {
+			for (GroupInfo group : groups) {
+        if (group.getName().equalsIgnoreCase(GroupInfo.DEFAULT_GROUP))
+          throw new IOException("Config file contains default group!");
+				group.write(bw);
+			}
+		} finally {
+			bw.close();
+		}
+	}
+
+  private List<ServerName> getOnlineRS() throws IOException{
+    if(master != null) {
+      return master.getServerManager().getOnlineServersList();
+    }
+    try {
+      List<ServerName> servers = new LinkedList<ServerName>();
+      for (String el: ZKUtil.listChildrenNoWatch(watcher, watcher.rsZNode)) {
+        servers.add(ServerName.parseServerName(el));
+      }
+      return servers;
+    } catch (KeeperException e) {
+      throw new IOException("Failed to retrieve server list for zookeeper", e);
+    }
+  }
+
+  private List<ServerName> getAssignedServers(){
+    List<ServerName> assignedServers = Lists.newArrayList();
+    for(GroupInfo gInfo : groupMap.values()){
+      for(String hostPort: gInfo.getServers()) {
+        assignedServers.add(ServerName.parseServerName(hostPort));
+      }
+    }
+    return assignedServers;
+  }
+
+	List<ServerName> difference(Collection<ServerName> onlineServers,
+			Collection<ServerName> servers) {
+		if(servers.size() == 0){
+			return Lists.newArrayList(onlineServers);
+		} else {
+			ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+			for (ServerName olServer : onlineServers) {
+				ServerName actual = ServerName.findServerWithSameHostnamePort(
+						servers, olServer);
+				if (actual == null) {
+					finalList.add(olServer);
+				}
+			}
+			return finalList;
+		}
+	}
+
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupMasterObserver.java src/main/java/org/apache/hadoop/hbase/master/GroupMasterObserver.java
new file mode 100644
index 0000000..6b8b689
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupMasterObserver.java
@@ -0,0 +1,65 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.ObserverContext;
+
+import java.io.IOException;
+import java.util.List;
+
+public class GroupMasterObserver extends BaseMasterObserver {
+
+    private MasterCoprocessorEnvironment menv;
+
+  @Override
+  public void start(CoprocessorEnvironment ctx) throws IOException {
+    menv = (MasterCoprocessorEnvironment)ctx;
+  }
+
+  @Override
+  public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> ctx, HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+    String groupName = GroupInfo.getGroupProperty(desc);
+    if(getGroupInfoManager().getGroup(groupName) == null) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist.");
+    }
+  }
+
+  @Override
+  public void preModifyTable(ObserverContext<MasterCoprocessorEnvironment> ctx, byte[] tableName, HTableDescriptor htd) throws IOException {
+    MasterServices master = ctx.getEnvironment().getMasterServices();
+    String groupName = GroupInfo.getGroupProperty(htd);
+    if(getGroupInfoManager().getGroup(groupName) == null) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist.");
+    }
+
+    List<HRegionInfo> tableRegionList = master.getAssignmentManager().getRegionsOfTable(tableName);
+    master.getAssignmentManager().unassign(tableRegionList);
+  }
+
+  private GroupInfoManager getGroupInfoManager() {
+    return ((GroupBasedLoadBalancer)menv.getMasterServices().getLoadBalancer()).getGroupInfoManager();
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/GroupMoveServerWorker.java src/main/java/org/apache/hadoop/hbase/master/GroupMoveServerWorker.java
new file mode 100644
index 0000000..b9bc8c5
--- /dev/null
+++ src/main/java/org/apache/hadoop/hbase/master/GroupMoveServerWorker.java
@@ -0,0 +1,167 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerName;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeSet;
+
+public class GroupMoveServerWorker implements Runnable {
+	private static final Log LOG = LogFactory.getLog(GroupMoveServerWorker.class);
+
+  private MasterServices master;
+  private MoveServerPlan plan;
+  private String transGroup;
+  private String sourceGroup;
+  private GroupInfoManager groupManager;
+  private Map<String,String> serversInTransition;
+  private volatile boolean success;
+
+  public GroupMoveServerWorker(Server master, Map<String, String> serversInTransition,
+                               GroupInfoManager groupManager, MoveServerPlan plan) throws IOException {
+    this.serversInTransition = serversInTransition;
+    this.groupManager = groupManager;
+    this.master = (MasterServices)master;
+    this.plan = plan;
+
+    synchronized (serversInTransition) {
+      //check server list
+      sourceGroup = groupManager.getGroupOfServer(plan.getServers().iterator().next()).getName();
+      for(String server: plan.getServers()) {
+        if(serversInTransition.containsKey(server)) {
+          throw new DoNotRetryIOException("Server list contains a server that is already being moved: "+server);
+        }
+        String tmpGroup = groupManager.getGroupOfServer(server).getName();
+        if(sourceGroup != null && !tmpGroup.equals(sourceGroup)) {
+          throw new DoNotRetryIOException("Move server request should only come from one source group");
+        }
+      }
+      //update the servers as in transition
+      for(String server: plan.getServers()) {
+        serversInTransition.put(server, plan.getTargetGroup());
+      }
+    }
+
+    if(!sourceGroup.startsWith(GroupInfo.TRANSITION_GROUP_PREFIX)) {
+      transGroup = GroupInfo.TRANSITION_GROUP_PREFIX+sourceGroup+"_TO_"+plan.getTargetGroup();
+      groupManager.addGroup(new GroupInfo(transGroup, new TreeSet<String>()));
+    }
+    groupManager.moveServers(plan.getServers(), sourceGroup, transGroup);
+  }
+
+  @Override
+  public void run() {
+    String name = "GroupMoveServer-"+transGroup+"-"+plan.getTargetGroup();
+    Thread.currentThread().setName(name);
+    try {
+      boolean found;
+      do {
+        found = false;
+        for(String rs: plan.getServers()) {
+          List<HRegionInfo> regions = getOnlineRegions(rs);
+          LOG.info("Unassigining "+regions.size()+" from server "+rs);
+          master.getAssignmentManager().unassign(regions);
+          found = found || regions.size() > 0;
+        }
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException e) {
+          LOG.warn("Sleep interrupted", e);
+        }
+      } while(found);
+      success = true;
+      LOG.info("Move server done: "+sourceGroup+"->"+plan.getTargetGroup());
+    } catch(Exception e) {
+      success = false;
+      LOG.error("Caught exception while running", e);
+    }
+    if(success) {
+      try {
+        complete();
+      } catch (IOException e) {
+        success = false;
+        LOG.error("Failed to complete move", e);
+      }
+    }
+  }
+
+  private List<HRegionInfo> getOnlineRegions(String hostPort) throws IOException {
+    List<HRegionInfo> regions = new LinkedList<HRegionInfo>();
+    for(Map.Entry<ServerName, List<HRegionInfo>> el:
+        master.getAssignmentManager().getAssignments().entrySet()) {
+      if(el.getKey().getHostAndPort().equals(hostPort)) {
+        regions.addAll(el.getValue());
+      }
+    }
+    return regions;
+  }
+
+  public static class MoveServerPlan {
+    private Set<String> servers;
+    private String targetGroup;
+
+    public MoveServerPlan(Set<String> servers, String targetGroup) {
+      this.servers = servers;
+      this.targetGroup = targetGroup;
+    }
+
+    public Set<String> getServers() {
+      return servers;
+    }
+
+    public String getTargetGroup() {
+      return targetGroup;
+    }
+  }
+
+  public void complete() throws IOException {
+    String tmpSourceGroup = sourceGroup;
+    if(transGroup != null) {
+      tmpSourceGroup = transGroup;
+      LOG.debug("Moving "+plan.getServers().size()+
+          " servers from transition group: "+transGroup+" to final group: "+plan.getTargetGroup());
+    }
+    try {
+      if(success) {
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, plan.getTargetGroup());
+        if(transGroup != null) {
+          groupManager.removeGroup(transGroup);
+        }
+      }
+    } finally {
+      //remove servers in transition
+      synchronized(serversInTransition) {
+        for(String server: plan.getServers()) {
+          serversInTransition.remove(server);
+        }
+      }
+    }
+  }
+}
diff --git src/main/java/org/apache/hadoop/hbase/master/HMaster.java src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index b476638..5d89e7e 100644
--- src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -22,13 +22,13 @@ package org.apache.hadoop.hbase.master;
 import java.io.IOException;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
 import java.net.InetAddress;
 import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -41,6 +41,10 @@ import java.util.concurrent.atomic.AtomicReference;
 
 import javax.management.ObjectName;
 
+import com.google.common.collect.ClassToInstanceMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.MutableClassToInstanceMap;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -67,9 +71,13 @@ import org.apache.hadoop.hbase.client.MetaScanner;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitor;
 import org.apache.hadoop.hbase.client.MetaScanner.MetaScannerVisitorBase;
 import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.coprocessor.ExecResult;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExec;
+import org.apache.hadoop.hbase.client.coprocessor.MasterExecResult;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.executor.ExecutorService.ExecutorType;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 import org.apache.hadoop.hbase.ipc.HBaseRPC;
 import org.apache.hadoop.hbase.ipc.HBaseServer;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
@@ -88,7 +96,6 @@ import org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler;
 import org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler;
 import org.apache.hadoop.hbase.master.metrics.MasterMetrics;
-import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer;
 import org.apache.hadoop.hbase.monitoring.MonitoredTask;
 import org.apache.hadoop.hbase.monitoring.TaskMonitor;
@@ -223,6 +230,13 @@ Server {
    */
   private ObjectName mxBean = null;
 
+  // Registered master protocol handlers
+  private ClassToInstanceMap<CoprocessorProtocol>
+      protocolHandlers = MutableClassToInstanceMap.create();
+
+  private Map<String, Class<? extends CoprocessorProtocol>>
+      protocolHandlerNames = Maps.newHashMap();
+
   /**
    * Initializes the HMaster. The steps are as follows:
    * <p>
@@ -250,7 +264,7 @@ Server {
     // Creation of a HSA will force a resolve.
     InetSocketAddress initialIsa = new InetSocketAddress(hostname, port);
     if (initialIsa.getAddress() == null) {
-      throw new IllegalArgumentException("Failed resolve of " + this.isa);
+      throw new IllegalArgumentException("Failed resolve of " + initialIsa);
     }
     int numHandlers = conf.getInt("hbase.master.handler.count",
       conf.getInt("hbase.regionserver.handler.count", 25));
@@ -530,6 +544,10 @@ Server {
     status.setStatus("Splitting logs after master startup");
     splitLogAfterStartup(this.fileSystemManager);
 
+    this.balancer.setClusterStatus(getClusterStatus());
+    this.balancer.setMasterServices(this);
+    this.balancer.configure();
+
     // Make sure root and meta assigned before proceeding.
     assignRootAndMeta(status);
     enableServerShutdownHandler();
@@ -545,9 +563,6 @@ Server {
     status.setStatus("Starting assignment manager");
     this.assignmentManager.joinCluster();
 
-    this.balancer.setClusterStatus(getClusterStatus());
-    this.balancer.setMasterServices(this);
-
     // Fixing up missing daughters if any
     status.setStatus("Fixing up missing daughters");
     fixupDaughters(status);
@@ -1128,10 +1143,10 @@ Server {
       LOG.info("Passed destination servername is null or empty so choosing a server at random");
       List<ServerName> destServers = this.serverManager.getOnlineServersList();
       destServers.remove(p.getSecond());
-      // If i have only one RS then destination can be null.
-      dest = balancer.randomAssignment(destServers);
+      dest = balancer.randomAssignment(p.getFirst(), destServers);
     } else {
-      dest = new ServerName(Bytes.toString(destServerName));
+      ServerName candidate = new ServerName(Bytes.toString(destServerName));
+      dest = balancer.randomAssignment(p.getFirst(), Lists.newArrayList(candidate));
     }
     
     // Now we can do the move
@@ -1552,6 +1567,7 @@ Server {
     return zooKeeper;
   }
 
+  @Override
   public MasterCoprocessorHost getCoprocessorHost() {
     return cpHost;
   }
@@ -1711,7 +1727,15 @@ Server {
     checkInitialized();
     Pair<HRegionInfo, ServerName> pair =
       MetaReader.getRegion(this.catalogTracker, regionName);
-    if (pair == null) throw new UnknownRegionException(Bytes.toString(regionName));
+    if (Bytes.equals(HRegionInfo.ROOT_REGIONINFO.getRegionName(),regionName)) {
+      try {
+        pair = new Pair<HRegionInfo, ServerName>(HRegionInfo.ROOT_REGIONINFO, this.catalogTracker.getRootLocation());
+      } catch (InterruptedException e) {
+        throw new IOException(e);
+      }
+    }
+    if (pair == null) throw new UnknownRegionException(
+        Bytes.toString(HRegionInfo.ROOT_REGIONINFO.getRegionName())+"<-->"+Bytes.toString(regionName));
     HRegionInfo hri = pair.getFirst();
     if (cpHost != null) {
       if (cpHost.preUnassign(hri, force)) {
@@ -1751,6 +1775,100 @@ Server {
   }
 
   /**
+   * Registers a new CoprocessorProtocol subclass and instance to
+   * be available for handling
+   * {@link HMaster#execCoprocessor(org.apache.hadoop.hbase.client.coprocessor.MasterExec)} calls.
+   *
+   * <p>
+   * Only a single protocol type/handler combination may be registered per
+   * region.
+   * After the first registration, subsequent calls with the same protocol type
+   * will fail with a return value of {@code false}.
+   * </p>
+   * @param protocol a {@code CoprocessorProtocol} subinterface defining the
+   * protocol methods
+   * @param handler an instance implementing the interface
+   * @param <T> the protocol type
+   * @return {@code true} if the registration was successful, {@code false}
+   * otherwise
+   */
+  public <T extends CoprocessorProtocol> boolean registerProtocol(
+      Class<T> protocol, T handler) {
+
+    /* No stacking of protocol handlers is currently allowed.  The
+     * first to claim wins!
+     */
+    if (protocolHandlers.containsKey(protocol)) {
+      LOG.error("Protocol "+protocol.getName()+
+          " already registered, rejecting request from "+
+          handler
+      );
+      return false;
+    }
+
+    protocolHandlers.putInstance(protocol, handler);
+    protocolHandlerNames.put(protocol.getName(), protocol);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Registered master protocol handler: protocol="+protocol.getName());
+    }
+    return true;
+  }
+
+  @Override
+  public LoadBalancer getLoadBalancer() {
+    return balancer;
+  }
+
+  @Override
+  public MasterExecResult execCoprocessor(MasterExec call) throws IOException {
+    Class<? extends CoprocessorProtocol> protocol = call.getProtocol();
+    if (protocol == null) {
+      String protocolName = call.getProtocolName();
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Received dynamic protocol exec call with protocolName " + protocolName);
+      }
+      // detect the actual protocol class
+      protocol  = protocolHandlerNames.get(protocolName);
+      if (protocol == null) {
+        throw new HBaseRPC.UnknownProtocolException(protocol,
+            "No matching handler for master protocol "+protocolName);
+      }
+    }
+    if (!protocolHandlers.containsKey(protocol)) {
+      throw new HBaseRPC.UnknownProtocolException(protocol,
+          "No matching handler for protocol ");
+    }
+
+    CoprocessorProtocol handler = protocolHandlers.getInstance(protocol);
+    Object value;
+
+    try {
+      Method method = protocol.getMethod(
+          call.getMethodName(), call.getParameterClasses());
+      method.setAccessible(true);
+
+      value = method.invoke(handler, call.getParameters());
+    } catch (InvocationTargetException e) {
+      Throwable target = e.getTargetException();
+      if (target instanceof IOException) {
+        throw (IOException)target;
+      }
+      IOException ioe = new IOException(target.toString());
+      ioe.setStackTrace(target.getStackTrace());
+      throw ioe;
+    } catch (Throwable e) {
+      if (!(e instanceof IOException)) {
+        LOG.error("Unexpected throwable object ", e);
+      }
+      IOException ioe = new IOException(e.toString());
+      ioe.setStackTrace(e.getStackTrace());
+      throw ioe;
+    }
+
+    return new MasterExecResult(value);
+  }
+
+  /**
    * Get all table descriptors
    * @return All descriptors or null if none.
    */
diff --git src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
index 7d2dd74..8093378 100644
--- src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
+++ src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ServerName;
 
+import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
@@ -59,6 +60,13 @@ public interface LoadBalancer extends Configurable {
   public void setMasterServices(MasterServices masterServices);
 
   /**
+   * Configure the load balancer. Must be called after setters.
+   *
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  public void configure() throws IOException;
+
+  /**
    * Perform the major balance operation
    * @param clusterState
    * @return List of plans
@@ -91,8 +99,9 @@ public interface LoadBalancer extends Configurable {
 
   /**
    * Get a random region server from the list
+   * @param region
    * @param servers
    * @return Servername
    */
-  public ServerName randomAssignment(List<ServerName> servers);
+  public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers);
 }
diff --git src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
index 4beafb2..68ab566 100644
--- src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
+++ src/main/java/org/apache/hadoop/hbase/master/MasterCoprocessorHost.java
@@ -25,6 +25,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.*;
 import org.apache.hadoop.hbase.coprocessor.*;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 
 import java.io.IOException;
 
@@ -69,6 +70,12 @@ public class MasterCoprocessorHost
   public MasterEnvironment createEnvironment(final Class<?> implClass,
       final Coprocessor instance, final int priority, final int seq,
       final Configuration conf) {
+    for (Class c : implClass.getInterfaces()) {
+      if (CoprocessorProtocol.class.isAssignableFrom(c)) {
+        masterServices.registerProtocol(c, (CoprocessorProtocol)instance);
+        break;
+      }
+    }
     return new MasterEnvironment(implClass, instance, priority, seq, conf,
         masterServices);
   }
diff --git src/main/java/org/apache/hadoop/hbase/master/MasterServices.java src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
index 6d6d009..d091185 100644
--- src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
+++ src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
@@ -26,6 +26,7 @@ import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.TableDescriptors;
 import org.apache.hadoop.hbase.executor.EventHandler;
 import org.apache.hadoop.hbase.executor.ExecutorService;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 import org.apache.hadoop.hbase.zookeeper.RegionServerTracker;
 
 /**
@@ -78,4 +79,35 @@ public interface MasterServices extends Server {
    * @return true if master enables ServerShutdownHandler;
    */
   public boolean isServerShutdownHandlerEnabled();
+
+  /**
+   * @return returns the master coprocessor host
+   */
+  public MasterCoprocessorHost getCoprocessorHost();
+
+  /**
+   * Registers a new CoprocessorProtocol subclass and instance to
+   * be available for handling
+   * {@link HMaster#execCoprocessor(org.apache.hadoop.hbase.client.coprocessor.MasterExec)} calls.
+   *
+   * <p>
+   * Only a single protocol type/handler combination may be registered.
+   *
+   * After the first registration, subsequent calls with the same protocol type
+   * will fail with a return value of {@code false}.
+   * </p>
+   * @param protocol a {@code CoprocessorProtocol} subinterface defining the
+   * protocol methods
+   * @param handler an instance implementing the interface
+   * @param <T> the protocol type
+   * @return {@code true} if the registration was successful, {@code false}
+   * otherwise
+   */
+  public <T extends CoprocessorProtocol> boolean registerProtocol(
+      Class<T> protocol, T handler);
+
+  /**
+   * @return load balancer
+   */
+  public LoadBalancer getLoadBalancer();
 }
diff --git src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
index e38cdcd..1c596d5 100644
--- src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
+++ src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorEndpoint.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.MediumTests;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Scan;
@@ -76,6 +77,8 @@ public class TestCoprocessorEndpoint {
     conf.setStrings(CoprocessorHost.REGION_COPROCESSOR_CONF_KEY,
         "org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint",
         "org.apache.hadoop.hbase.coprocessor.GenericEndpoint");
+    conf.setStrings(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,
+        "org.apache.hadoop.hbase.coprocessor.GenericEndpoint");
 
     util.startMiniCluster(2);
     cluster = util.getMiniHBaseCluster();
@@ -136,6 +139,34 @@ public class TestCoprocessorEndpoint {
     table.close();
   }
 
+  @Test
+  public void testMasterGeneric() throws Throwable {
+    HBaseAdmin admin = new HBaseAdmin(util.getConfiguration());
+    GenericProtocol protocol = admin.coprocessorProxy(GenericProtocol.class);
+    String workResult1 = protocol.doWork("foo");
+    assertEquals("foo", workResult1);
+    byte[] workResult2 = protocol.doWork(new byte[]{1});
+    assertArrayEquals(new byte[]{1}, workResult2);
+    byte workResult3 = protocol.doWork((byte)1);
+    assertEquals((byte)1, workResult3);
+    char workResult4 = protocol.doWork('c');
+    assertEquals('c', workResult4);
+    boolean workResult5 = protocol.doWork(true);
+    assertEquals(true, workResult5);
+    short workResult6 = protocol.doWork((short)1);
+    assertEquals((short)1, workResult6);
+    int workResult7 = protocol.doWork(5);
+    assertEquals(5, workResult7);
+    long workResult8 = protocol.doWork(5l);
+    assertEquals(5l, workResult8);
+    double workResult9 = protocol.doWork(6d);
+    assertEquals(6d, workResult9, 0.01);
+    float workResult10 = protocol.doWork(6f);
+    assertEquals(6f, workResult10, 0.01);
+    Text workResult11 = protocol.doWork(new Text("foo"));
+    assertEquals(new Text("foo"), workResult11);
+  }
+
   @Ignore @Test
   public void testAggregation() throws Throwable {
     HTable table = new HTable(util.getConfiguration(), TEST_TABLE);
diff --git src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java
index 0e467ab..892dda1 100644
--- src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java
+++ src/test/java/org/apache/hadoop/hbase/io/TestHbaseObjectWritable.java
@@ -539,7 +539,7 @@ public class TestHbaseObjectWritable extends TestCase {
    * note on the test above. 
    */
   public void testGetNextObjectCode(){
-    assertEquals(82,HbaseObjectWritable.getNextClassCode());
+    assertEquals(83,HbaseObjectWritable.getNextClassCode());
   }
 
   @org.junit.Rule
diff --git src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
index cd3989a..e63b12e 100644
--- src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
+++ src/test/java/org/apache/hadoop/hbase/master/TestAssignmentManager.java
@@ -896,8 +896,8 @@ public class TestAssignmentManager {
     }
 
     @Override
-    public ServerName randomAssignment(List<ServerName> servers) {
-      ServerName randomServerName = super.randomAssignment(servers);
+    public ServerName randomAssignment(HRegionInfo region, List<ServerName> servers) {
+      ServerName randomServerName = super.randomAssignment(region, servers);
       this.gate.set(true);
       return randomServerName;
     }
diff --git src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index f48e7a5..45d40fa 100644
--- src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -58,6 +58,7 @@ import org.apache.hadoop.hbase.client.HConnectionTestingUtility;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.executor.ExecutorService;
 import org.apache.hadoop.hbase.io.Reference;
+import org.apache.hadoop.hbase.ipc.CoprocessorProtocol;
 import org.apache.hadoop.hbase.master.CatalogJanitor.SplitParentFirstComparator;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.regionserver.Store;
@@ -274,6 +275,21 @@ public class TestCatalogJanitor {
     public boolean isServerShutdownHandlerEnabled() {
       return true;
     }
+
+    @Override
+    public MasterCoprocessorHost getCoprocessorHost() {
+      return null;
+    }
+
+    @Override
+    public <T extends CoprocessorProtocol> boolean registerProtocol(Class<T> protocol, T handler) {
+      return false;
+    }
+
+    @Override
+    public LoadBalancer getLoadBalancer() {
+      return null;
+    }
   }
 
   @Test
diff --git src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java
new file mode 100644
index 0000000..eae21a0
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/master/TestGroupBasedLoadBalancer.java
@@ -0,0 +1,557 @@
+/**
+ * Copyright 2011 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Lists;
+
+@Category(MediumTests.class)
+public class TestGroupBasedLoadBalancer {
+
+    private static final Log LOG = LogFactory.getLog(TestGroupBasedLoadBalancer.class);
+    private static LoadBalancer loadBalancer;
+    private static Random rand;
+
+    static String[]  groups = new String[] { GroupInfo.DEFAULT_GROUP, "dg2", "dg3",
+            "dg4" };
+    static String[] tables = new String[] { "dt1", "dt2", "dt3", "dt4" };
+    static List<ServerName> servers;
+    static Map<String, GroupInfo> groupMap;
+    static List<HTableDescriptor> tableDescs;
+    int[] regionAssignment = new int[] { 2, 5, 7, 10, 4, 3, 1 };
+    static int regionId = 0;
+
+    @BeforeClass
+    public static void beforeAllTests() throws Exception {
+        rand = new Random();
+        servers = generatedServers(7);
+        groupMap = constructGroupInfo(servers, groups);
+        tableDescs = constructTableDesc();
+        Configuration conf = HBaseConfiguration.create();
+        conf.set("hbase.regions.slop", "0");
+        loadBalancer = new GroupBasedLoadBalancer(getMockedGroupInfoManager());
+        loadBalancer.setMasterServices(getMockedMaster());
+        loadBalancer.setConf(conf);
+    }
+
+    /**
+     * Test the load balancing algorithm.
+     *
+     * Invariant is that all servers of the group should be hosting either floor(average) or
+     * ceiling(average)
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testBalanceCluster() throws Exception {
+        Map<ServerName, List<HRegionInfo>> servers = mockClusterServers();
+        ArrayListMultimap<String, ServerAndLoad> list = convertToGroupBasedMap(servers);
+        LOG.info("Mock Cluster :  " + printStats(list));
+        List<RegionPlan> plans = loadBalancer.balanceCluster(servers);
+        ArrayListMultimap<String, ServerAndLoad> balancedCluster = reconcile(
+                                                    list, plans);
+        LOG.info("Mock Balance : " + printStats(balancedCluster));
+        assertClusterAsBalanced(balancedCluster);
+    }
+
+    /**
+     * Invariant is that all servers of a group have load between floor(avg) and
+     * ceiling(avg) number of regions.
+     */
+    private void assertClusterAsBalanced(
+            ArrayListMultimap<String, ServerAndLoad> groupLoadMap) {
+        for (String gName : groupLoadMap.keySet()) {
+            List<ServerAndLoad> groupLoad = groupLoadMap.get(gName);
+            int numServers = groupLoad.size();
+            int numRegions = 0;
+            int maxRegions = 0;
+            int minRegions = Integer.MAX_VALUE;
+            for (ServerAndLoad server : groupLoad) {
+                int nr = server.getLoad();
+                if (nr > maxRegions) {
+                    maxRegions = nr;
+                }
+                if (nr < minRegions) {
+                    minRegions = nr;
+                }
+                numRegions += nr;
+            }
+            if (maxRegions - minRegions < 2) {
+                // less than 2 between max and min, can't balance
+                return;
+            }
+            int min = numRegions / numServers;
+            int max = numRegions % numServers == 0 ? min : min + 1;
+
+            for (ServerAndLoad server : groupLoad) {
+                assertTrue(server.getLoad() <= max);
+                assertTrue(server.getLoad() >= min);
+            }
+        }
+    }
+
+    /**
+     * Tests immediate assignment.
+     *
+     * Invariant is that all regions have an assignment.
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testImmediateAssignment() throws Exception {
+        List<HRegionInfo> regions = randomRegions(20);
+        Map<HRegionInfo, ServerName> assignments = loadBalancer
+                .immediateAssignment(regions, servers);
+        assertImmediateAssignment(regions, servers, assignments);
+    }
+
+    /**
+     * All regions have an assignment.
+     *
+     * @param regions
+     * @param servers
+     * @param assignments
+     * @throws IOException
+     * @throws FileNotFoundException
+     */
+    private void assertImmediateAssignment(List<HRegionInfo> regions,
+            List<ServerName> servers, Map<HRegionInfo, ServerName> assignments)
+            throws FileNotFoundException, IOException {
+        for (HRegionInfo region : regions) {
+            assertTrue(assignments.containsKey(region));
+            ServerName server = assignments.get(region);
+            String tableName = region.getTableNameAsString();
+            String groupName =
+                GroupInfo.getGroupProperty(
+                            getMockedMaster().getTableDescriptors().get(
+                                    tableName));
+            assertTrue(StringUtils.isNotEmpty(groupName));
+            GroupInfo gInfo = getMockedGroupInfoManager().getGroup(groupName);
+            assertTrue("Region is not correctly assigned to group servers.",
+                    gInfo.containsServer(server.getHostAndPort()));
+        }
+    }
+
+    /**
+     * Tests the bulk assignment used during cluster startup.
+     *
+     * Round-robin. Should yield a balanced cluster so same invariant as the
+     * load balancer holds, all servers holding either floor(avg) or
+     * ceiling(avg).
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testBulkAssignment() throws Exception {
+        List<HRegionInfo> regions = randomRegions(25);
+        Map<ServerName, List<HRegionInfo>> assignments = loadBalancer
+                .roundRobinAssignment(regions, servers);
+        assertTrue(assignments.keySet().size() == servers.size());
+        for (ServerName sn : assignments.keySet()) {
+            List<HRegionInfo> regionAssigned = assignments.get(sn);
+            for (HRegionInfo region : regionAssigned) {
+                String tableName = region.getTableNameAsString();
+                String groupName =
+                    GroupInfo.getGroupProperty(
+                                getMockedMaster().getTableDescriptors().get(
+                                        tableName));
+                assertTrue(StringUtils.isNotEmpty(groupName));
+                GroupInfo gInfo = getMockedGroupInfoManager().getGroup(
+                        groupName);
+                assertTrue(
+                        "Region is not correctly assigned to group servers.",
+                        gInfo.containsServer(sn.getHostAndPort()));
+            }
+        }
+        ArrayListMultimap<String, ServerAndLoad> loadMap = convertToGroupBasedMap(assignments);
+        assertClusterAsBalanced(loadMap);
+    }
+
+    /**
+     * Test the cluster startup bulk assignment which attempts to retain
+     * assignment info.
+     *
+     * @throws Exception
+     */
+    @Test
+    public void testRetainAssignment() throws Exception {
+        // Test simple case where all same servers are there
+        Map<ServerName, List<HRegionInfo>> currentAssignments = mockClusterServers();
+        Map<HRegionInfo, ServerName> inputForTest = new HashMap<HRegionInfo, ServerName>();
+        for (ServerName sn : currentAssignments.keySet()) {
+            for (HRegionInfo region : currentAssignments.get(sn)) {
+                inputForTest.put(region, sn);
+            }
+        }
+        Map<ServerName, List<HRegionInfo>> newAssignment = loadBalancer
+                .retainAssignment(inputForTest, servers);
+        assertRetainedAssignment(inputForTest, servers, newAssignment);
+    }
+
+    /**
+     * Asserts a valid retained assignment plan.
+     * <p>
+     * Must meet the following conditions:
+     * <ul>
+     * <li>Every input region has an assignment, and to an online server
+     * <li>If a region had an existing assignment to a server with the same
+     * address a a currently online server, it will be assigned to it
+     * </ul>
+     *
+     * @param existing
+     * @param groupBasedLoad
+     * @param assignment
+     * @throws IOException
+     * @throws FileNotFoundException
+     */
+    private void assertRetainedAssignment(
+            Map<HRegionInfo, ServerName> existing, List<ServerName> servers,
+            Map<ServerName, List<HRegionInfo>> assignment)
+            throws FileNotFoundException, IOException {
+        // Verify condition 1, every region assigned, and to online server
+        Set<ServerName> onlineServerSet = new TreeSet<ServerName>(servers);
+        Set<HRegionInfo> assignedRegions = new TreeSet<HRegionInfo>();
+        for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+            assertTrue(
+                    "Region assigned to server that was not listed as online",
+                    onlineServerSet.contains(a.getKey()));
+            for (HRegionInfo r : a.getValue())
+                assignedRegions.add(r);
+        }
+        assertEquals(existing.size(), assignedRegions.size());
+
+        // Verify condition 2, every region must be assigned to correct server.
+        Set<String> onlineHostNames = new TreeSet<String>();
+        for (ServerName s : servers) {
+            onlineHostNames.add(s.getHostname());
+        }
+
+        for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+            ServerName currentServer = a.getKey();
+            for (HRegionInfo r : a.getValue()) {
+                ServerName oldAssignedServer = existing.get(r);
+                String tableName = r.getTableNameAsString();
+                String groupName =
+                        GroupInfo.getGroupProperty(
+                                getMockedMaster().getTableDescriptors().get(
+                                        tableName));
+                assertTrue(StringUtils.isNotEmpty(groupName));
+                GroupInfo gInfo = getMockedGroupInfoManager().getGroup(
+                        groupName);
+                assertTrue(
+                        "Region is not correctly assigned to group servers.",
+                        gInfo.containsServer(currentServer.getHostAndPort()));
+                if (oldAssignedServer != null
+                        && onlineHostNames.contains(oldAssignedServer
+                                .getHostname())) {
+                    // this region was previously assigned somewhere, and that
+                    // host is still around, then the host must have been is a
+                    // different group.
+                    if (oldAssignedServer.getHostAndPort().equals(
+                            currentServer.getHostAndPort()) == false) {
+                        assertFalse(gInfo.containsServer(oldAssignedServer
+                                .getHostAndPort()));
+                    }
+                }
+            }
+        }
+    }
+
+    private String printStats(
+            ArrayListMultimap<String, ServerAndLoad> groupBasedLoad) {
+        StringBuffer sb = new StringBuffer();
+        sb.append("\n");
+        for (String groupName : groupBasedLoad.keySet()) {
+            sb.append("Stats for group: " + groupName);
+            sb.append("\n");
+            sb.append(groupMap.get(groupName).getServers());
+            sb.append("\n");
+            List<ServerAndLoad> groupLoad = groupBasedLoad.get(groupName);
+            int numServers = groupLoad.size();
+            int totalRegions = 0;
+            sb.append("Per Server Load: \n");
+            for (ServerAndLoad sLoad : groupLoad) {
+                sb.append("Server :" + sLoad.getServerName() + " Load : "
+                        + sLoad.getLoad() + "\n");
+                totalRegions += sLoad.getLoad();
+            }
+            sb.append(" Group Statistics : \n");
+            float average = (float) totalRegions / numServers;
+            int max = (int) Math.ceil(average);
+            int min = (int) Math.floor(average);
+            sb.append("[srvr=" + numServers + " rgns=" + totalRegions + " avg="
+                    + average + " max=" + max + " min=" + min + "]");
+            sb.append("\n");
+            sb.append("===============================");
+            sb.append("\n");
+        }
+        return sb.toString();
+    }
+
+    private ArrayListMultimap<String, ServerAndLoad> convertToGroupBasedMap(
+            final Map<ServerName, List<HRegionInfo>> serversMap) throws IOException {
+        ArrayListMultimap<String, ServerAndLoad> loadMap = ArrayListMultimap
+                .create();
+        for (GroupInfo gInfo : getMockedGroupInfoManager().listGroups()) {
+            Set<String> groupServers = gInfo.getServers();
+            for (String hostAndPort : groupServers) {
+                ServerName actual = ServerName.findServerWithSameHostnamePort(
+                        servers, ServerName.parseServerName(hostAndPort));
+                List<HRegionInfo> regions = serversMap.get(actual);
+                assertTrue("No load for " + actual, regions != null);
+                loadMap.put(gInfo.getName(),
+                        new ServerAndLoad(actual, regions.size()));
+            }
+        }
+        return loadMap;
+    }
+
+  private ArrayListMultimap<String, ServerAndLoad> reconcile(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      List<RegionPlan> plans) {
+    ArrayListMultimap<String, ServerAndLoad> result = ArrayListMultimap
+        .create();
+    result.putAll(previousLoad);
+    if (plans != null) {
+      for (RegionPlan plan : plans) {
+        ServerName source = plan.getSource();
+        updateLoad(result, source, -1);
+        ServerName destination = plan.getDestination();
+        updateLoad(result, destination, +1);
+      }
+    }
+    return result;
+  }
+
+  private void updateLoad(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      final ServerName sn, final int diff) {
+    for (String groupName : previousLoad.keySet()) {
+      ServerAndLoad newSAL = null;
+      ServerAndLoad oldSAL = null;
+      for (ServerAndLoad sal : previousLoad.get(groupName)) {
+        if (ServerName.isSameHostnameAndPort(sn, sal.getServerName())) {
+          oldSAL = sal;
+          newSAL = new ServerAndLoad(sn, sal.getLoad() + diff);
+          break;
+        }
+      }
+      if (newSAL != null) {
+        previousLoad.remove(groupName, oldSAL);
+        previousLoad.put(groupName, newSAL);
+        break;
+      }
+    }
+  }
+
+    private Map<ServerName, List<HRegionInfo>> mockClusterServers() throws IOException {
+        assertTrue(servers.size() == regionAssignment.length);
+        Map<ServerName, List<HRegionInfo>> assignment = new TreeMap<ServerName, List<HRegionInfo>>();
+        for (int i = 0; i < servers.size(); i++) {
+            int numRegions = regionAssignment[i];
+            List<HRegionInfo> regions = assignedRegions(numRegions, servers.get(i));
+            assignment.put(servers.get(i), regions);
+        }
+        return assignment;
+    }
+
+    /**
+     * Generated a list of regions evenly distributed between the tables.
+     *
+     * @param numRegions The number of regions to be generated.
+     * @return List of HRegionInfo.
+     */
+    private List<HRegionInfo> randomRegions(int numRegions) {
+        List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+        byte[] start = new byte[16];
+        byte[] end = new byte[16];
+        rand.nextBytes(start);
+        rand.nextBytes(end);
+        int regionIdx = rand.nextInt(tables.length);
+        for (int i = 0; i < numRegions; i++) {
+            Bytes.putInt(start, 0, numRegions << 1);
+            Bytes.putInt(end, 0, (numRegions << 1) + 1);
+            int tableIndex = (i + regionIdx) % tables.length;
+            HRegionInfo hri = new HRegionInfo(
+                    Bytes.toBytes(tables[tableIndex]), start, end, false,
+                    regionId++);
+            regions.add(hri);
+        }
+        return regions;
+    }
+
+    /**
+     * Generated assigned regions to a given server using group information.
+     *
+     * @param numRegions the num regions to generate
+     * @param sn the servername
+     * @return the list
+     * @throws IOException Signals that an I/O exception has occurred.
+     */
+    private List<HRegionInfo> assignedRegions(int numRegions, ServerName sn) throws IOException {
+      List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+      byte[] start = new byte[16];
+      byte[] end = new byte[16];
+      for (int i = 0; i < numRegions; i++) {
+          Bytes.putInt(start, 0, numRegions << 1);
+          Bytes.putInt(end, 0, (numRegions << 1) + 1);
+          String tableName = getTableName(sn);
+          HRegionInfo hri = new HRegionInfo(
+                  Bytes.toBytes(tableName), start, end, false,
+                  regionId++);
+          regions.add(hri);
+      }
+      return regions;
+  }
+
+    private static List<ServerName> generatedServers(int numServers) {
+        List<ServerName> servers = new ArrayList<ServerName>(numServers);
+        for (int i = 0; i < numServers; i++) {
+            String host = "server" + rand.nextInt(100000);
+            int port = rand.nextInt(60000);
+            servers.add(new ServerName(host, port, -1));
+        }
+        return servers;
+    }
+
+    /**
+     * Construct group info, with each group have atleast one server.
+     *
+     * @param servers the servers
+     * @param groups the groups
+     * @return the map
+     */
+    private static Map<String, GroupInfo> constructGroupInfo(
+            List<ServerName> servers, String[] groups) {
+        assertTrue(servers != null);
+        assertTrue(servers.size() >= groups.length);
+        int index = 0;
+        Map<String, GroupInfo> groupMap = new HashMap<String, GroupInfo>();
+        for (String grpName : groups) {
+            TreeSet<String> hostAndPort = new TreeSet<String>();
+            hostAndPort.add(servers.get(index).getHostAndPort());
+            groupMap.put(grpName, new GroupInfo(grpName, hostAndPort));
+            index++;
+        }
+        while (index < servers.size()) {
+            int grpIndex = rand.nextInt(groups.length);
+            groupMap.get(groups[grpIndex]).addServer(
+                    servers.get(index).getHostAndPort());
+            index++;
+        }
+        return groupMap;
+    }
+
+    /**
+     * Construct table descriptors evenly distributed between the groups.
+     *
+     * @return the list
+     */
+    private static List<HTableDescriptor> constructTableDesc() {
+        List<HTableDescriptor> tds = Lists.newArrayList();
+        int index = rand.nextInt(groups.length);
+        for (int i = 0; i < tables.length; i++) {
+            HTableDescriptor htd = new HTableDescriptor(tables[i]);
+            int grpIndex = (i + index) % groups.length ;
+            String groupName = groups[grpIndex];
+            GroupInfo.setGroupProperty(groupName, htd);
+            tds.add(htd);
+        }
+        return tds;
+    }
+
+    private static MasterServices getMockedMaster() throws IOException {
+        TableDescriptors tds = Mockito.mock(TableDescriptors.class);
+        Mockito.when(tds.get(tables[0])).thenReturn(tableDescs.get(0));
+        Mockito.when(tds.get(tables[1])).thenReturn(tableDescs.get(1));
+        Mockito.when(tds.get(tables[2])).thenReturn(tableDescs.get(2));
+        Mockito.when(tds.get(tables[3])).thenReturn(tableDescs.get(3));
+        MasterServices services = Mockito.mock(HMaster.class);
+        Mockito.when(services.getTableDescriptors()).thenReturn(tds);
+        AssignmentManager am = Mockito.mock(AssignmentManager.class);
+        Mockito.when(services.getAssignmentManager()).thenReturn(am);
+        return services;
+    }
+
+    private static GroupInfoManager getMockedGroupInfoManager() throws IOException {
+        GroupInfoManager gm = Mockito.mock(GroupInfoManager.class);
+        Mockito.when(gm.getGroup(groups[0])).thenReturn(
+                groupMap.get(groups[0]));
+        Mockito.when(gm.getGroup(groups[1])).thenReturn(
+                groupMap.get(groups[1]));
+        Mockito.when(gm.getGroup(groups[2])).thenReturn(
+                groupMap.get(groups[2]));
+        Mockito.when(gm.getGroup(groups[3])).thenReturn(
+                groupMap.get(groups[3]));
+        Mockito.when(gm.listGroups()).thenReturn(
+                Lists.newLinkedList(groupMap.values()));
+        return gm;
+    }
+
+    private String getTableName(ServerName sn) throws IOException{
+      String tableName = null;
+      GroupInfoManager gm = getMockedGroupInfoManager();
+      GroupInfo groupOfServer = null;
+      for(GroupInfo gInfo : gm.listGroups()){
+        if(gInfo.containsServer(sn.getHostAndPort())){
+          groupOfServer = gInfo;
+          break;
+        }
+      }
+
+      for(HTableDescriptor desc : tableDescs){
+       if(GroupInfo.getGroupProperty(desc).endsWith(groupOfServer.getName())){
+         tableName = desc.getNameAsString();
+       }
+      }
+      return tableName;
+    }
+}
diff --git src/test/java/org/apache/hadoop/hbase/master/TestGroups.java src/test/java/org/apache/hadoop/hbase/master/TestGroups.java
new file mode 100644
index 0000000..afb3fe1
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/master/TestGroups.java
@@ -0,0 +1,246 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.GroupAdminClient;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.ipc.HRegionInterface;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(MediumTests.class)
+public class TestGroups {
+	private static HBaseTestingUtility TEST_UTIL;
+	private static HMaster master;
+	private static Random rand;
+  private static HBaseAdmin admin;
+	private static String groupPrefix = "Group-";
+	private static String tablePrefix = "TABLE-";
+	private static String familyPrefix = "FAMILY-";
+
+	@BeforeClass
+	public static void setUp() throws Exception {
+		TEST_UTIL = new HBaseTestingUtility();
+		TEST_UTIL.getConfiguration().set(
+				HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+				GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName()+","+
+        GroupAdminEndpoint.class.getName());
+		TEST_UTIL.startMiniCluster(4);
+		MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
+		master = cluster.getMaster();
+		rand = new Random();
+    admin = TEST_UTIL.getHBaseAdmin();
+	}
+
+	@AfterClass
+	public static void tearDown() throws Exception {
+		TEST_UTIL.shutdownMiniCluster();
+	}
+
+  @After
+  public void afterMethod() throws Exception {
+		GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+    for(GroupInfo group: groupAdmin.listGroups()) {
+      if(!group.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+        removeGroup(groupAdmin, group.getName());
+      }
+    }
+    for(HTableDescriptor table: TEST_UTIL.getHBaseAdmin().listTables()) {
+      if(!table.isMetaRegion() && !table.isRootRegion()) {
+        TEST_UTIL.deleteTable(table.getName());
+      }
+    }
+  }
+
+	@Test
+	public void testBasicStartUp() throws IOException {
+		GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+		GroupInfo defaultInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+		assertTrue(defaultInfo.getServers().size() == 4);
+		// Assignment of root and meta regions.
+		assertTrue(groupAdmin.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP)
+        .size() == 2);
+	}
+
+	@Test
+	public void testSimpleRegionServerMove() throws IOException,
+			InterruptedException {
+		GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+		GroupInfo appInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+		GroupInfo adminInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+    GroupInfo dInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+		// Force the group info manager to read group information from disk.
+		assertTrue(groupAdmin.listGroups().size() == 3);
+		assertTrue(adminInfo.getServers().size() == 1);
+		assertTrue(appInfo.getServers().size() == 1);
+		assertTrue(dInfo.getServers().size() == 2);
+		groupAdmin.moveServers(appInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+		groupAdmin.removeGroup(appInfo.getName());
+		groupAdmin.moveServers(adminInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+		groupAdmin.removeGroup(adminInfo.getName());
+		assertTrue(groupAdmin.listGroups().size() == 1);
+	}
+
+	@Test
+	public void testTableMove() throws IOException, InterruptedException {
+		String tableName = tablePrefix + rand.nextInt();
+		byte[] TABLENAME = Bytes.toBytes(tableName);
+		byte[] FAMILYNAME = Bytes.toBytes(familyPrefix + rand.nextInt());
+		GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+		GroupInfo newGroup = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 2);
+    int currMetaCount = TEST_UTIL.getMetaTableRows().size();
+		HTable ht = TEST_UTIL.createTable(TABLENAME, FAMILYNAME);
+		assertTrue(TEST_UTIL.createMultiRegions(master.getConfiguration(), ht,
+				FAMILYNAME, 4) == 4);
+		TEST_UTIL.waitUntilAllRegionsAssigned(currMetaCount+4);
+		assertTrue(master.getAssignmentManager().getZKTable()
+				.isEnabledTable(Bytes.toString(TABLENAME)));
+		List<HRegionInfo> regionList = TEST_UTIL.getHBaseAdmin()
+				.getTableRegions(TABLENAME);
+		assertTrue(regionList.size() > 0);
+		GroupInfo tableGrp = groupAdmin.getGroupInfoOfTable(Bytes.toBytes(tableName));
+		assertTrue(tableGrp.getName().equals(GroupInfo.DEFAULT_GROUP));
+
+    //change table's group
+    admin.disableTable(TABLENAME);
+    HTableDescriptor desc = admin.getTableDescriptor(TABLENAME);
+    GroupInfo.setGroupProperty(newGroup.getName(), desc);
+    admin.modifyTable(TABLENAME, desc);
+    admin.enableTable(TABLENAME);
+
+    //verify group change
+    desc = admin.getTableDescriptor(TABLENAME);
+		assertEquals(newGroup.getName(),
+        GroupInfo.getGroupProperty(desc));
+
+		Map<String, Map<ServerName, List<HRegionInfo>>> tableRegionAssignMap = master
+				.getAssignmentManager().getAssignmentsByTable();
+		assertTrue(tableRegionAssignMap.keySet().size() == 1);
+		Map<ServerName, List<HRegionInfo>> serverMap = tableRegionAssignMap
+				.get(tableName);
+		for (ServerName rs : serverMap.keySet()) {
+			if (serverMap.get(rs).size() > 0) {
+				assertTrue(newGroup.containsServer(rs.getHostAndPort()));
+			}
+		}
+    removeGroup(groupAdmin, newGroup.getName());
+		TEST_UTIL.deleteTable(TABLENAME);
+		tableRegionAssignMap = master.getAssignmentManager()
+				.getAssignmentsByTable();
+		assertEquals(tableRegionAssignMap.size(), 0);
+	}
+
+	@Test
+	public void testRegionMove() throws IOException, InterruptedException {
+		GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+		GroupInfo newGroup = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+		String tableNameOne = tablePrefix + rand.nextInt();
+		byte[] tableOneBytes = Bytes.toBytes(tableNameOne);
+		byte[] familyOneBytes = Bytes.toBytes(familyPrefix + rand.nextInt());
+    int currMetaCount = TEST_UTIL.getMetaTableRows().size();
+		HTable ht = TEST_UTIL.createTable(tableOneBytes, familyOneBytes);
+		// All the regions created below will be assigned to the default group.
+		assertTrue(TEST_UTIL.createMultiRegions(master.getConfiguration(), ht,
+				familyOneBytes, 5) == 5);
+		TEST_UTIL.waitUntilAllRegionsAssigned(currMetaCount+5);
+		List<HRegionInfo> regions = groupAdmin
+				.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP);
+		assertTrue(regions.size() + ">=" + 5, regions.size() >= 5);
+		HRegionInfo region = regions.get(regions.size()-1);
+		// Lets move this region to newGroupName group.
+		ServerName tobeAssigned =
+        ServerName.parseServerName(newGroup.getServers().iterator().next());
+		master.move(region.getEncodedNameAsBytes(),
+        Bytes.toBytes(tobeAssigned.toString()));
+
+		int tries =10;
+    while (groupAdmin.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP).size() != regions.size()
+        && tries-- > 0) {
+      Thread.sleep(100);
+    }
+    //verify that region was never assigned to the server
+		List<HRegionInfo> updatedRegions = groupAdmin.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP);
+    assertTrue(regions.size() + "!=" + updatedRegions.size(),regions.size() == updatedRegions.size());
+    HRegionInterface rs = admin.getConnection().getHRegionConnection(tobeAssigned.getHostname(),
+      tobeAssigned.getPort());
+		assertFalse(rs.getOnlineRegions().contains(region));
+	}
+
+	static GroupInfo addGroup(GroupAdminClient gAdmin, String groupName,
+			int serverCount) throws IOException, InterruptedException {
+		GroupInfo defaultInfo = gAdmin
+				.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+		assertTrue(defaultInfo != null);
+		assertTrue(defaultInfo.getServers().size() >= serverCount);
+		gAdmin.addGroup(groupName);
+
+    Set<String> set = new HashSet<String>();
+    for(String server: defaultInfo.getServers()) {
+      if(set.size() == serverCount) {
+        break;
+      }
+      set.add(server);
+    }
+    gAdmin.moveServers(set, groupName);
+    GroupInfo result = gAdmin.getGroupInfo(groupName);
+		assertTrue(result.getServers().size() >= serverCount);
+    return result;
+	}
+
+  static void removeGroup(GroupAdminClient groupAdmin, String groupName) throws IOException {
+    for(String table: groupAdmin.listTablesOfGroup(groupName)) {
+      byte[] bTable = Bytes.toBytes(table);
+      admin.disableTable(bTable);
+      HTableDescriptor desc = admin.getTableDescriptor(bTable);
+      desc.remove(GroupInfo.GROUP_KEY);
+      admin.modifyTable(bTable, desc);
+      admin.enableTable(bTable);
+    }
+    groupAdmin.removeGroup(groupName);
+  }
+
+}
diff --git src/test/java/org/apache/hadoop/hbase/master/TestGroupsWithDeadServers.java src/test/java/org/apache/hadoop/hbase/master/TestGroupsWithDeadServers.java
new file mode 100644
index 0000000..4ebf482
--- /dev/null
+++ src/test/java/org/apache/hadoop/hbase/master/TestGroupsWithDeadServers.java
@@ -0,0 +1,199 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Random;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.GroupAdminClient;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.ResultScanner;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.JVMClusterUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(LargeTests.class)
+public class TestGroupsWithDeadServers {
+	private static HBaseTestingUtility TEST_UTIL;
+	private static HMaster master;
+	private static Random rand;
+  private static HBaseAdmin admin;
+
+	@BeforeClass
+	public static void setUp() throws Exception {
+		TEST_UTIL = new HBaseTestingUtility();
+		TEST_UTIL.getConfiguration().set(
+				HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+				GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName()+","+
+        GroupAdminEndpoint.class.getName());
+		TEST_UTIL.getConfiguration().setInt(
+				"hbase.master.assignment.timeoutmonitor.period", 2000);
+		TEST_UTIL.getConfiguration().setInt(
+				"hbase.master.assignment.timeoutmonitor.timeout", 5000);
+		TEST_UTIL.startMiniCluster(4);
+		MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();
+		master = cluster.getMaster();
+		rand = new Random();
+    admin = TEST_UTIL.getHBaseAdmin();
+	}
+
+	@AfterClass
+	public static void tearDown() throws Exception {
+		TEST_UTIL.shutdownMiniCluster();
+	}
+
+	@Test
+	public void testGroupWithOnlineServers() throws IOException, InterruptedException{
+    GroupAdminClient groupAdmin = new GroupAdminClient(master.getConfiguration());
+		String newRSGroup = "group-" + rand.nextInt();
+		String tableNameTwo = "TABLE-" + rand.nextInt();
+		byte[] tableTwoBytes = Bytes.toBytes(tableNameTwo);
+		String familyName = "family" + rand.nextInt();
+		byte[] familyTwoBytes = Bytes.toBytes(familyName);
+    int baseNumRegions = TEST_UTIL.getMetaTableRows().size();
+		int NUM_REGIONS = 4;
+
+		GroupInfo defaultInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+		assertTrue(defaultInfo.getServers().size() == 4);
+		TestGroups.addGroup(groupAdmin, newRSGroup, 2);
+		defaultInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+		assertTrue(defaultInfo.getServers().size() == 2);
+		assertTrue(groupAdmin.getGroupInfo(newRSGroup).getServers().size() == 2);
+		HTable ht = TEST_UTIL.createTable(tableTwoBytes, familyTwoBytes);
+		// All the regions created below will be assigned to the default group.
+		assertTrue(TEST_UTIL.createMultiRegions(master.getConfiguration(), ht,
+				familyTwoBytes, NUM_REGIONS) == NUM_REGIONS);
+		TEST_UTIL.waitUntilAllRegionsAssigned(baseNumRegions+NUM_REGIONS);
+		List<HRegionInfo> regions = groupAdmin.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP);
+		assertTrue(regions.size() >= NUM_REGIONS);
+    //move table to new group
+    admin.disableTable(tableNameTwo);
+    HTableDescriptor desc = admin.getTableDescriptor(tableTwoBytes);
+    GroupInfo.setGroupProperty(newRSGroup, desc);
+    admin.modifyTable(tableTwoBytes, desc);
+    admin.enableTable(tableTwoBytes);
+
+		TEST_UTIL.waitUntilAllRegionsAssigned(baseNumRegions+NUM_REGIONS);
+		//Move the ROOT and META regions to default group.
+		ServerName serverForRoot =
+        ServerName.findServerWithSameHostnamePort(master.getServerManager().getOnlineServersList(),
+            ServerName.parseServerName(defaultInfo.getServers().iterator().next()));
+		master.move(HRegionInfo.ROOT_REGIONINFO.getEncodedNameAsBytes(), Bytes.toBytes(serverForRoot.toString()));
+		master.move(HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes(), Bytes.toBytes(serverForRoot.toString()));
+		while (master.getAssignmentManager().isRegionsInTransition()){
+			Thread.sleep(10);
+		}
+		List<HRegionInfo> newGrpRegions = groupAdmin.listOnlineRegionsOfGroup(newRSGroup);
+		assertTrue(newGrpRegions.size() == NUM_REGIONS);
+		MiniHBaseCluster hbaseCluster = TEST_UTIL.getHBaseCluster();
+		// Now we kill all the region servers in the new group.
+		Set<String> serverNames = groupAdmin.getGroupInfo(newRSGroup).getServers();
+		for (String sName : serverNames) {
+			int serverNumber = getServerNumber(
+					hbaseCluster.getRegionServerThreads(), sName);
+			assert (serverNumber != -1);
+			hbaseCluster.stopRegionServer(serverNumber, false);
+		}
+		//wait till all the regions come transition state.
+    int tries = 10;
+		while (groupAdmin.listOnlineRegionsOfGroup(newRSGroup).size() != 0 && tries-- > 0){
+			Thread.sleep(100);
+		}
+		newGrpRegions = groupAdmin.listOnlineRegionsOfGroup(newRSGroup);
+    assertTrue("Number of online regions in" + newRSGroup + " " + newGrpRegions.size(),
+      newGrpRegions.size() == 0);
+		regions = groupAdmin.listOnlineRegionsOfGroup(GroupInfo.DEFAULT_GROUP);
+		assertTrue(regions.size() == 2);
+		startServersAndMove(groupAdmin, 1, newRSGroup);
+		while(master.getAssignmentManager().isRegionsInTransition()){
+			Thread.sleep(5);
+		}
+		scanTableForPositiveResults(ht);
+		newGrpRegions = groupAdmin.listOnlineRegionsOfGroup(newRSGroup);
+		assertTrue(newGrpRegions.size() == NUM_REGIONS);
+		TEST_UTIL.deleteTable(tableTwoBytes);
+    groupAdmin.removeGroup(newRSGroup);
+	}
+
+	private int getServerNumber(List<JVMClusterUtil.RegionServerThread> servers, String sName){
+		int i = 0;
+		for(JVMClusterUtil.RegionServerThread rs : servers){
+			if(sName.equals(rs.getRegionServer().getServerName().getHostAndPort())){
+				return i;
+			}
+			i++;
+		}
+		return -1;
+	}
+	
+	private void scanTableForPositiveResults(HTable ht) throws IOException{
+		ResultScanner s = null;
+		try {
+			Scan scan = new Scan();
+			s = ht.getScanner(scan);
+		} finally {
+			if (s != null) {
+				s.close();
+			}
+		}
+	}
+
+	private void startServersAndMove(GroupAdminClient groupAdmin, int numServers,
+			String groupName) throws IOException, InterruptedException {
+		MiniHBaseCluster hbaseCluster = TEST_UTIL.getHBaseCluster();
+		ServerName newServer;
+		for (int i = 0; i < numServers; i++) {
+			newServer = hbaseCluster.startRegionServer().getRegionServer()
+					.getServerName();
+			// Make sure that the server manager reports the new online servers.
+			while (ServerName.findServerWithSameHostnamePort(master
+					.getServerManager().getOnlineServersList(), newServer) == null) {
+				Thread.sleep(5);
+			}
+			assertTrue(groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP)
+          .containsServer(newServer.getHostAndPort()));
+      Set<String> set = new TreeSet<String>();
+      set.add(newServer.getHostAndPort());
+			groupAdmin.moveServers(set, groupName);
+			assertTrue(groupAdmin.getGroupInfo(groupName).containsServer(
+          newServer.getHostAndPort()));
+		}
+	}
+
+}
