diff --git a/hbase-it/src/test/java/org/apache/hadoop/hbase/group/IntegrationTestRegionServerGroups.java b/hbase-it/src/test/java/org/apache/hadoop/hbase/group/IntegrationTestRegionServerGroups.java
new file mode 100644
index 0000000..8cbe1ac
--- /dev/null
+++ b/hbase-it/src/test/java/org/apache/hadoop/hbase/group/IntegrationTestRegionServerGroups.java
@@ -0,0 +1,70 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.IntegrationTestingUtility;
+import org.apache.hadoop.hbase.IntegrationTests;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.experimental.categories.Category;
+
+/**
+ * A suite of integration tests for Region Server Groups.
+ * It subclasses and therefore also runs tests defined in
+ * {@link TestRegionServerGroupsCommonCases}
+ */
+@Category(IntegrationTests.class)
+public class IntegrationTestRegionServerGroups extends TestRegionServerGroupsCommonCases {
+  private final static Log LOG = LogFactory.getLog(IntegrationTestRegionServerGroups.class);
+  protected final static int NUM_SLAVES_BASE = 4; //number of slaves for the smallest cluster
+  private static boolean initialized = false;
+
+
+  @Before
+  public void beforeMethod() throws Exception {
+    if(!initialized) {
+      LOG.info("Setting up IntegrationTestGroup");
+      LOG.info("Initializing cluster with " + NUM_SLAVES_BASE + " servers");
+      TEST_UTIL = new IntegrationTestingUtility();
+      ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);
+      //set shared configs
+      admin = TEST_UTIL.getHBaseAdmin();
+      cluster = TEST_UTIL.getHBaseClusterInterface();
+      groupAdmin = new VerifyingGroupAdminClient(TEST_UTIL.getConfiguration());
+      LOG.info("Done initializing cluster");
+      initialized = true;
+    }
+    LOG.info("Cleaning up previous test run");
+    //cleanup previous artifacts
+    deleteTableIfNecessary();
+    deleteGroups();
+    admin.setBalancerRunning(false, true);
+    LOG.info("Done cleaning up previous test run");
+  }
+
+  @After
+  public void afterMethod() throws Exception {
+    LOG.info("Restoring the cluster");
+    ((IntegrationTestingUtility)TEST_UTIL).restoreCluster();
+    LOG.info("Done restoring the cluster");
+  }
+}
diff --git a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupAdminProtos.java b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupAdminProtos.java
new file mode 100644
index 0000000..09b5799
--- /dev/null
+++ b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupAdminProtos.java
@@ -0,0 +1,10950 @@
+// Generated by the protocol buffer compiler.  DO NOT EDIT!
+// source: GroupAdmin.proto
+
+package org.apache.hadoop.hbase.protobuf.generated;
+
+public final class GroupAdminProtos {
+  private GroupAdminProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistry registry) {
+  }
+  public interface ListTablesOfGroupRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string groupName = 1;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class ListTablesOfGroupRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements ListTablesOfGroupRequestOrBuilder {
+    // Use ListTablesOfGroupRequest.newBuilder() to construct.
+    private ListTablesOfGroupRequest(Builder builder) {
+      super(builder);
+    }
+    private ListTablesOfGroupRequest(boolean noInit) {}
+    
+    private static final ListTablesOfGroupRequest defaultInstance;
+    public static ListTablesOfGroupRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListTablesOfGroupRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string groupName = 1;
+    public static final int GROUPNAME_FIELD_NUMBER = 1;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.getDefaultInstance()) return this;
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string groupName = 1;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ListTablesOfGroupRequest)
+    }
+    
+    static {
+      defaultInstance = new ListTablesOfGroupRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListTablesOfGroupRequest)
+  }
+  
+  public interface ListTablesOfGroupResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated string tableNames = 1;
+    java.util.List<String> getTableNamesList();
+    int getTableNamesCount();
+    String getTableNames(int index);
+  }
+  public static final class ListTablesOfGroupResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements ListTablesOfGroupResponseOrBuilder {
+    // Use ListTablesOfGroupResponse.newBuilder() to construct.
+    private ListTablesOfGroupResponse(Builder builder) {
+      super(builder);
+    }
+    private ListTablesOfGroupResponse(boolean noInit) {}
+    
+    private static final ListTablesOfGroupResponse defaultInstance;
+    public static ListTablesOfGroupResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListTablesOfGroupResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupResponse_fieldAccessorTable;
+    }
+    
+    // repeated string tableNames = 1;
+    public static final int TABLENAMES_FIELD_NUMBER = 1;
+    private com.google.protobuf.LazyStringList tableNames_;
+    public java.util.List<String>
+        getTableNamesList() {
+      return tableNames_;
+    }
+    public int getTableNamesCount() {
+      return tableNames_.size();
+    }
+    public String getTableNames(int index) {
+      return tableNames_.get(index);
+    }
+    
+    private void initFields() {
+      tableNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < tableNames_.size(); i++) {
+        output.writeBytes(1, tableNames_.getByteString(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      {
+        int dataSize = 0;
+        for (int i = 0; i < tableNames_.size(); i++) {
+          dataSize += com.google.protobuf.CodedOutputStream
+            .computeBytesSizeNoTag(tableNames_.getByteString(i));
+        }
+        size += dataSize;
+        size += 1 * getTableNamesList().size();
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse) obj;
+      
+      boolean result = true;
+      result = result && getTableNamesList()
+          .equals(other.getTableNamesList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getTableNamesCount() > 0) {
+        hash = (37 * hash) + TABLENAMES_FIELD_NUMBER;
+        hash = (53 * hash) + getTableNamesList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListTablesOfGroupResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        tableNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse(this);
+        int from_bitField0_ = bitField0_;
+        if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          tableNames_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              tableNames_);
+          bitField0_ = (bitField0_ & ~0x00000001);
+        }
+        result.tableNames_ = tableNames_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance()) return this;
+        if (!other.tableNames_.isEmpty()) {
+          if (tableNames_.isEmpty()) {
+            tableNames_ = other.tableNames_;
+            bitField0_ = (bitField0_ & ~0x00000001);
+          } else {
+            ensureTableNamesIsMutable();
+            tableNames_.addAll(other.tableNames_);
+          }
+          onChanged();
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              ensureTableNamesIsMutable();
+              tableNames_.add(input.readBytes());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated string tableNames = 1;
+      private com.google.protobuf.LazyStringList tableNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureTableNamesIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          tableNames_ = new com.google.protobuf.LazyStringArrayList(tableNames_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      public java.util.List<String>
+          getTableNamesList() {
+        return java.util.Collections.unmodifiableList(tableNames_);
+      }
+      public int getTableNamesCount() {
+        return tableNames_.size();
+      }
+      public String getTableNames(int index) {
+        return tableNames_.get(index);
+      }
+      public Builder setTableNames(
+          int index, String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTableNamesIsMutable();
+        tableNames_.set(index, value);
+        onChanged();
+        return this;
+      }
+      public Builder addTableNames(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTableNamesIsMutable();
+        tableNames_.add(value);
+        onChanged();
+        return this;
+      }
+      public Builder addAllTableNames(
+          java.lang.Iterable<String> values) {
+        ensureTableNamesIsMutable();
+        super.addAll(values, tableNames_);
+        onChanged();
+        return this;
+      }
+      public Builder clearTableNames() {
+        tableNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        onChanged();
+        return this;
+      }
+      void addTableNames(com.google.protobuf.ByteString value) {
+        ensureTableNamesIsMutable();
+        tableNames_.add(value);
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ListTablesOfGroupResponse)
+    }
+    
+    static {
+      defaultInstance = new ListTablesOfGroupResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListTablesOfGroupResponse)
+  }
+  
+  public interface GetGroupInfoRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string groupName = 1;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class GetGroupInfoRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupInfoRequestOrBuilder {
+    // Use GetGroupInfoRequest.newBuilder() to construct.
+    private GetGroupInfoRequest(Builder builder) {
+      super(builder);
+    }
+    private GetGroupInfoRequest(boolean noInit) {}
+    
+    private static final GetGroupInfoRequest defaultInstance;
+    public static GetGroupInfoRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupInfoRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string groupName = 1;
+    public static final int GROUPNAME_FIELD_NUMBER = 1;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.getDefaultInstance()) return this;
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string groupName = 1;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupInfoRequest)
+    }
+    
+    static {
+      defaultInstance = new GetGroupInfoRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupInfoRequest)
+  }
+  
+  public interface GetGroupInfoResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // optional .GroupInfo groupInfo = 1;
+    boolean hasGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder();
+  }
+  public static final class GetGroupInfoResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupInfoResponseOrBuilder {
+    // Use GetGroupInfoResponse.newBuilder() to construct.
+    private GetGroupInfoResponse(Builder builder) {
+      super(builder);
+    }
+    private GetGroupInfoResponse(boolean noInit) {}
+    
+    private static final GetGroupInfoResponse defaultInstance;
+    public static GetGroupInfoResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupInfoResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoResponse_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // optional .GroupInfo groupInfo = 1;
+    public static final int GROUPINFO_FIELD_NUMBER = 1;
+    private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_;
+    public boolean hasGroupInfo() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+      return groupInfo_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+      return groupInfo_;
+    }
+    
+    private void initFields() {
+      groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (hasGroupInfo()) {
+        if (!getGroupInfo().isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeMessage(1, groupInfo_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, groupInfo_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupInfo() == other.hasGroupInfo());
+      if (hasGroupInfo()) {
+        result = result && getGroupInfo()
+            .equals(other.getGroupInfo());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupInfo()) {
+        hash = (37 * hash) + GROUPINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupInfo().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getGroupInfoFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        if (groupInfoBuilder_ == null) {
+          result.groupInfo_ = groupInfo_;
+        } else {
+          result.groupInfo_ = groupInfoBuilder_.build();
+        }
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance()) return this;
+        if (other.hasGroupInfo()) {
+          mergeGroupInfo(other.getGroupInfo());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (hasGroupInfo()) {
+          if (!getGroupInfo().isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder();
+              if (hasGroupInfo()) {
+                subBuilder.mergeFrom(getGroupInfo());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setGroupInfo(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // optional .GroupInfo groupInfo = 1;
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> groupInfoBuilder_;
+      public boolean hasGroupInfo() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_;
+        } else {
+          return groupInfoBuilder_.getMessage();
+        }
+      }
+      public Builder setGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          groupInfo_ = value;
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder setGroupInfo(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = builderForValue.build();
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder mergeGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001) &&
+              groupInfo_ != org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance()) {
+            groupInfo_ =
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder(groupInfo_).mergeFrom(value).buildPartial();
+          } else {
+            groupInfo_ = value;
+          }
+          onChanged();
+        } else {
+          groupInfoBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder clearGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+          onChanged();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder getGroupInfoBuilder() {
+        bitField0_ |= 0x00000001;
+        onChanged();
+        return getGroupInfoFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+        if (groupInfoBuilder_ != null) {
+          return groupInfoBuilder_.getMessageOrBuilder();
+        } else {
+          return groupInfo_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+          getGroupInfoFieldBuilder() {
+        if (groupInfoBuilder_ == null) {
+          groupInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder>(
+                  groupInfo_,
+                  getParentForChildren(),
+                  isClean());
+          groupInfo_ = null;
+        }
+        return groupInfoBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupInfoResponse)
+    }
+    
+    static {
+      defaultInstance = new GetGroupInfoResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupInfoResponse)
+  }
+  
+  public interface GetGroupInfoOfTableRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string tableName = 1;
+    boolean hasTableName();
+    String getTableName();
+  }
+  public static final class GetGroupInfoOfTableRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupInfoOfTableRequestOrBuilder {
+    // Use GetGroupInfoOfTableRequest.newBuilder() to construct.
+    private GetGroupInfoOfTableRequest(Builder builder) {
+      super(builder);
+    }
+    private GetGroupInfoOfTableRequest(boolean noInit) {}
+    
+    private static final GetGroupInfoOfTableRequest defaultInstance;
+    public static GetGroupInfoOfTableRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupInfoOfTableRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string tableName = 1;
+    public static final int TABLENAME_FIELD_NUMBER = 1;
+    private java.lang.Object tableName_;
+    public boolean hasTableName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getTableName() {
+      java.lang.Object ref = tableName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          tableName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getTableNameBytes() {
+      java.lang.Object ref = tableName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        tableName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      tableName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasTableName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getTableNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getTableNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasTableName() == other.hasTableName());
+      if (hasTableName()) {
+        result = result && getTableName()
+            .equals(other.getTableName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasTableName()) {
+        hash = (37 * hash) + TABLENAME_FIELD_NUMBER;
+        hash = (53 * hash) + getTableName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        tableName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.tableName_ = tableName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.getDefaultInstance()) return this;
+        if (other.hasTableName()) {
+          setTableName(other.getTableName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasTableName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              tableName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string tableName = 1;
+      private java.lang.Object tableName_ = "";
+      public boolean hasTableName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getTableName() {
+        java.lang.Object ref = tableName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          tableName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setTableName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        tableName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearTableName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        tableName_ = getDefaultInstance().getTableName();
+        onChanged();
+        return this;
+      }
+      void setTableName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        tableName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupInfoOfTableRequest)
+    }
+    
+    static {
+      defaultInstance = new GetGroupInfoOfTableRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupInfoOfTableRequest)
+  }
+  
+  public interface GetGroupInfoOfTableResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // optional .GroupInfo groupInfo = 1;
+    boolean hasGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder();
+  }
+  public static final class GetGroupInfoOfTableResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupInfoOfTableResponseOrBuilder {
+    // Use GetGroupInfoOfTableResponse.newBuilder() to construct.
+    private GetGroupInfoOfTableResponse(Builder builder) {
+      super(builder);
+    }
+    private GetGroupInfoOfTableResponse(boolean noInit) {}
+    
+    private static final GetGroupInfoOfTableResponse defaultInstance;
+    public static GetGroupInfoOfTableResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupInfoOfTableResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableResponse_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // optional .GroupInfo groupInfo = 1;
+    public static final int GROUPINFO_FIELD_NUMBER = 1;
+    private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_;
+    public boolean hasGroupInfo() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+      return groupInfo_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+      return groupInfo_;
+    }
+    
+    private void initFields() {
+      groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (hasGroupInfo()) {
+        if (!getGroupInfo().isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeMessage(1, groupInfo_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, groupInfo_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupInfo() == other.hasGroupInfo());
+      if (hasGroupInfo()) {
+        result = result && getGroupInfo()
+            .equals(other.getGroupInfo());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupInfo()) {
+        hash = (37 * hash) + GROUPINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupInfo().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupInfoOfTableResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getGroupInfoFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        if (groupInfoBuilder_ == null) {
+          result.groupInfo_ = groupInfo_;
+        } else {
+          result.groupInfo_ = groupInfoBuilder_.build();
+        }
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance()) return this;
+        if (other.hasGroupInfo()) {
+          mergeGroupInfo(other.getGroupInfo());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (hasGroupInfo()) {
+          if (!getGroupInfo().isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder();
+              if (hasGroupInfo()) {
+                subBuilder.mergeFrom(getGroupInfo());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setGroupInfo(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // optional .GroupInfo groupInfo = 1;
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> groupInfoBuilder_;
+      public boolean hasGroupInfo() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_;
+        } else {
+          return groupInfoBuilder_.getMessage();
+        }
+      }
+      public Builder setGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          groupInfo_ = value;
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder setGroupInfo(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = builderForValue.build();
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder mergeGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001) &&
+              groupInfo_ != org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance()) {
+            groupInfo_ =
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder(groupInfo_).mergeFrom(value).buildPartial();
+          } else {
+            groupInfo_ = value;
+          }
+          onChanged();
+        } else {
+          groupInfoBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder clearGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+          onChanged();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder getGroupInfoBuilder() {
+        bitField0_ |= 0x00000001;
+        onChanged();
+        return getGroupInfoFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+        if (groupInfoBuilder_ != null) {
+          return groupInfoBuilder_.getMessageOrBuilder();
+        } else {
+          return groupInfo_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+          getGroupInfoFieldBuilder() {
+        if (groupInfoBuilder_ == null) {
+          groupInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder>(
+                  groupInfo_,
+                  getParentForChildren(),
+                  isClean());
+          groupInfo_ = null;
+        }
+        return groupInfoBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupInfoOfTableResponse)
+    }
+    
+    static {
+      defaultInstance = new GetGroupInfoOfTableResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupInfoOfTableResponse)
+  }
+  
+  public interface MoveServersRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string targetGroup = 1;
+    boolean hasTargetGroup();
+    String getTargetGroup();
+    
+    // repeated string servers = 2;
+    java.util.List<String> getServersList();
+    int getServersCount();
+    String getServers(int index);
+  }
+  public static final class MoveServersRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements MoveServersRequestOrBuilder {
+    // Use MoveServersRequest.newBuilder() to construct.
+    private MoveServersRequest(Builder builder) {
+      super(builder);
+    }
+    private MoveServersRequest(boolean noInit) {}
+    
+    private static final MoveServersRequest defaultInstance;
+    public static MoveServersRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public MoveServersRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string targetGroup = 1;
+    public static final int TARGETGROUP_FIELD_NUMBER = 1;
+    private java.lang.Object targetGroup_;
+    public boolean hasTargetGroup() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getTargetGroup() {
+      java.lang.Object ref = targetGroup_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          targetGroup_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getTargetGroupBytes() {
+      java.lang.Object ref = targetGroup_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        targetGroup_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    // repeated string servers = 2;
+    public static final int SERVERS_FIELD_NUMBER = 2;
+    private com.google.protobuf.LazyStringList servers_;
+    public java.util.List<String>
+        getServersList() {
+      return servers_;
+    }
+    public int getServersCount() {
+      return servers_.size();
+    }
+    public String getServers(int index) {
+      return servers_.get(index);
+    }
+    
+    private void initFields() {
+      targetGroup_ = "";
+      servers_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasTargetGroup()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getTargetGroupBytes());
+      }
+      for (int i = 0; i < servers_.size(); i++) {
+        output.writeBytes(2, servers_.getByteString(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getTargetGroupBytes());
+      }
+      {
+        int dataSize = 0;
+        for (int i = 0; i < servers_.size(); i++) {
+          dataSize += com.google.protobuf.CodedOutputStream
+            .computeBytesSizeNoTag(servers_.getByteString(i));
+        }
+        size += dataSize;
+        size += 1 * getServersList().size();
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasTargetGroup() == other.hasTargetGroup());
+      if (hasTargetGroup()) {
+        result = result && getTargetGroup()
+            .equals(other.getTargetGroup());
+      }
+      result = result && getServersList()
+          .equals(other.getServersList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasTargetGroup()) {
+        hash = (37 * hash) + TARGETGROUP_FIELD_NUMBER;
+        hash = (53 * hash) + getTargetGroup().hashCode();
+      }
+      if (getServersCount() > 0) {
+        hash = (37 * hash) + SERVERS_FIELD_NUMBER;
+        hash = (53 * hash) + getServersList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        targetGroup_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        servers_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.targetGroup_ = targetGroup_;
+        if (((bitField0_ & 0x00000002) == 0x00000002)) {
+          servers_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              servers_);
+          bitField0_ = (bitField0_ & ~0x00000002);
+        }
+        result.servers_ = servers_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.getDefaultInstance()) return this;
+        if (other.hasTargetGroup()) {
+          setTargetGroup(other.getTargetGroup());
+        }
+        if (!other.servers_.isEmpty()) {
+          if (servers_.isEmpty()) {
+            servers_ = other.servers_;
+            bitField0_ = (bitField0_ & ~0x00000002);
+          } else {
+            ensureServersIsMutable();
+            servers_.addAll(other.servers_);
+          }
+          onChanged();
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasTargetGroup()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              targetGroup_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              ensureServersIsMutable();
+              servers_.add(input.readBytes());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string targetGroup = 1;
+      private java.lang.Object targetGroup_ = "";
+      public boolean hasTargetGroup() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getTargetGroup() {
+        java.lang.Object ref = targetGroup_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          targetGroup_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setTargetGroup(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        targetGroup_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearTargetGroup() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        targetGroup_ = getDefaultInstance().getTargetGroup();
+        onChanged();
+        return this;
+      }
+      void setTargetGroup(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        targetGroup_ = value;
+        onChanged();
+      }
+      
+      // repeated string servers = 2;
+      private com.google.protobuf.LazyStringList servers_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureServersIsMutable() {
+        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+          servers_ = new com.google.protobuf.LazyStringArrayList(servers_);
+          bitField0_ |= 0x00000002;
+         }
+      }
+      public java.util.List<String>
+          getServersList() {
+        return java.util.Collections.unmodifiableList(servers_);
+      }
+      public int getServersCount() {
+        return servers_.size();
+      }
+      public String getServers(int index) {
+        return servers_.get(index);
+      }
+      public Builder setServers(
+          int index, String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureServersIsMutable();
+        servers_.set(index, value);
+        onChanged();
+        return this;
+      }
+      public Builder addServers(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureServersIsMutable();
+        servers_.add(value);
+        onChanged();
+        return this;
+      }
+      public Builder addAllServers(
+          java.lang.Iterable<String> values) {
+        ensureServersIsMutable();
+        super.addAll(values, servers_);
+        onChanged();
+        return this;
+      }
+      public Builder clearServers() {
+        servers_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        onChanged();
+        return this;
+      }
+      void addServers(com.google.protobuf.ByteString value) {
+        ensureServersIsMutable();
+        servers_.add(value);
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:MoveServersRequest)
+    }
+    
+    static {
+      defaultInstance = new MoveServersRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:MoveServersRequest)
+  }
+  
+  public interface MoveServersResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class MoveServersResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements MoveServersResponseOrBuilder {
+    // Use MoveServersResponse.newBuilder() to construct.
+    private MoveServersResponse(Builder builder) {
+      super(builder);
+    }
+    private MoveServersResponse(boolean noInit) {}
+    
+    private static final MoveServersResponse defaultInstance;
+    public static MoveServersResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public MoveServersResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersResponse_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveServersResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:MoveServersResponse)
+    }
+    
+    static {
+      defaultInstance = new MoveServersResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:MoveServersResponse)
+  }
+  
+  public interface MoveTablesRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string targetGroup = 1;
+    boolean hasTargetGroup();
+    String getTargetGroup();
+    
+    // repeated string tables = 2;
+    java.util.List<String> getTablesList();
+    int getTablesCount();
+    String getTables(int index);
+  }
+  public static final class MoveTablesRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements MoveTablesRequestOrBuilder {
+    // Use MoveTablesRequest.newBuilder() to construct.
+    private MoveTablesRequest(Builder builder) {
+      super(builder);
+    }
+    private MoveTablesRequest(boolean noInit) {}
+    
+    private static final MoveTablesRequest defaultInstance;
+    public static MoveTablesRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public MoveTablesRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string targetGroup = 1;
+    public static final int TARGETGROUP_FIELD_NUMBER = 1;
+    private java.lang.Object targetGroup_;
+    public boolean hasTargetGroup() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getTargetGroup() {
+      java.lang.Object ref = targetGroup_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          targetGroup_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getTargetGroupBytes() {
+      java.lang.Object ref = targetGroup_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        targetGroup_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    // repeated string tables = 2;
+    public static final int TABLES_FIELD_NUMBER = 2;
+    private com.google.protobuf.LazyStringList tables_;
+    public java.util.List<String>
+        getTablesList() {
+      return tables_;
+    }
+    public int getTablesCount() {
+      return tables_.size();
+    }
+    public String getTables(int index) {
+      return tables_.get(index);
+    }
+    
+    private void initFields() {
+      targetGroup_ = "";
+      tables_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasTargetGroup()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getTargetGroupBytes());
+      }
+      for (int i = 0; i < tables_.size(); i++) {
+        output.writeBytes(2, tables_.getByteString(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getTargetGroupBytes());
+      }
+      {
+        int dataSize = 0;
+        for (int i = 0; i < tables_.size(); i++) {
+          dataSize += com.google.protobuf.CodedOutputStream
+            .computeBytesSizeNoTag(tables_.getByteString(i));
+        }
+        size += dataSize;
+        size += 1 * getTablesList().size();
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasTargetGroup() == other.hasTargetGroup());
+      if (hasTargetGroup()) {
+        result = result && getTargetGroup()
+            .equals(other.getTargetGroup());
+      }
+      result = result && getTablesList()
+          .equals(other.getTablesList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasTargetGroup()) {
+        hash = (37 * hash) + TARGETGROUP_FIELD_NUMBER;
+        hash = (53 * hash) + getTargetGroup().hashCode();
+      }
+      if (getTablesCount() > 0) {
+        hash = (37 * hash) + TABLES_FIELD_NUMBER;
+        hash = (53 * hash) + getTablesList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        targetGroup_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        tables_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.targetGroup_ = targetGroup_;
+        if (((bitField0_ & 0x00000002) == 0x00000002)) {
+          tables_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              tables_);
+          bitField0_ = (bitField0_ & ~0x00000002);
+        }
+        result.tables_ = tables_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.getDefaultInstance()) return this;
+        if (other.hasTargetGroup()) {
+          setTargetGroup(other.getTargetGroup());
+        }
+        if (!other.tables_.isEmpty()) {
+          if (tables_.isEmpty()) {
+            tables_ = other.tables_;
+            bitField0_ = (bitField0_ & ~0x00000002);
+          } else {
+            ensureTablesIsMutable();
+            tables_.addAll(other.tables_);
+          }
+          onChanged();
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasTargetGroup()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              targetGroup_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              ensureTablesIsMutable();
+              tables_.add(input.readBytes());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string targetGroup = 1;
+      private java.lang.Object targetGroup_ = "";
+      public boolean hasTargetGroup() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getTargetGroup() {
+        java.lang.Object ref = targetGroup_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          targetGroup_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setTargetGroup(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        targetGroup_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearTargetGroup() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        targetGroup_ = getDefaultInstance().getTargetGroup();
+        onChanged();
+        return this;
+      }
+      void setTargetGroup(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        targetGroup_ = value;
+        onChanged();
+      }
+      
+      // repeated string tables = 2;
+      private com.google.protobuf.LazyStringList tables_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureTablesIsMutable() {
+        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
+          tables_ = new com.google.protobuf.LazyStringArrayList(tables_);
+          bitField0_ |= 0x00000002;
+         }
+      }
+      public java.util.List<String>
+          getTablesList() {
+        return java.util.Collections.unmodifiableList(tables_);
+      }
+      public int getTablesCount() {
+        return tables_.size();
+      }
+      public String getTables(int index) {
+        return tables_.get(index);
+      }
+      public Builder setTables(
+          int index, String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTablesIsMutable();
+        tables_.set(index, value);
+        onChanged();
+        return this;
+      }
+      public Builder addTables(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTablesIsMutable();
+        tables_.add(value);
+        onChanged();
+        return this;
+      }
+      public Builder addAllTables(
+          java.lang.Iterable<String> values) {
+        ensureTablesIsMutable();
+        super.addAll(values, tables_);
+        onChanged();
+        return this;
+      }
+      public Builder clearTables() {
+        tables_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000002);
+        onChanged();
+        return this;
+      }
+      void addTables(com.google.protobuf.ByteString value) {
+        ensureTablesIsMutable();
+        tables_.add(value);
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:MoveTablesRequest)
+    }
+    
+    static {
+      defaultInstance = new MoveTablesRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:MoveTablesRequest)
+  }
+  
+  public interface MoveTablesResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class MoveTablesResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements MoveTablesResponseOrBuilder {
+    // Use MoveTablesResponse.newBuilder() to construct.
+    private MoveTablesResponse(Builder builder) {
+      super(builder);
+    }
+    private MoveTablesResponse(boolean noInit) {}
+    
+    private static final MoveTablesResponse defaultInstance;
+    public static MoveTablesResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public MoveTablesResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesResponse_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_MoveTablesResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:MoveTablesResponse)
+    }
+    
+    static {
+      defaultInstance = new MoveTablesResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:MoveTablesResponse)
+  }
+  
+  public interface AddGroupRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string groupName = 1;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class AddGroupRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements AddGroupRequestOrBuilder {
+    // Use AddGroupRequest.newBuilder() to construct.
+    private AddGroupRequest(Builder builder) {
+      super(builder);
+    }
+    private AddGroupRequest(boolean noInit) {}
+    
+    private static final AddGroupRequest defaultInstance;
+    public static AddGroupRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public AddGroupRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string groupName = 1;
+    public static final int GROUPNAME_FIELD_NUMBER = 1;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.getDefaultInstance()) return this;
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string groupName = 1;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:AddGroupRequest)
+    }
+    
+    static {
+      defaultInstance = new AddGroupRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:AddGroupRequest)
+  }
+  
+  public interface AddGroupResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class AddGroupResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements AddGroupResponseOrBuilder {
+    // Use AddGroupResponse.newBuilder() to construct.
+    private AddGroupResponse(Builder builder) {
+      super(builder);
+    }
+    private AddGroupResponse(boolean noInit) {}
+    
+    private static final AddGroupResponse defaultInstance;
+    public static AddGroupResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public AddGroupResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupResponse_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_AddGroupResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:AddGroupResponse)
+    }
+    
+    static {
+      defaultInstance = new AddGroupResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:AddGroupResponse)
+  }
+  
+  public interface RemoveGroupRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string groupName = 1;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class RemoveGroupRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements RemoveGroupRequestOrBuilder {
+    // Use RemoveGroupRequest.newBuilder() to construct.
+    private RemoveGroupRequest(Builder builder) {
+      super(builder);
+    }
+    private RemoveGroupRequest(boolean noInit) {}
+    
+    private static final RemoveGroupRequest defaultInstance;
+    public static RemoveGroupRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public RemoveGroupRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string groupName = 1;
+    public static final int GROUPNAME_FIELD_NUMBER = 1;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.getDefaultInstance()) return this;
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string groupName = 1;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:RemoveGroupRequest)
+    }
+    
+    static {
+      defaultInstance = new RemoveGroupRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:RemoveGroupRequest)
+  }
+  
+  public interface RemoveGroupResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class RemoveGroupResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements RemoveGroupResponseOrBuilder {
+    // Use RemoveGroupResponse.newBuilder() to construct.
+    private RemoveGroupResponse(Builder builder) {
+      super(builder);
+    }
+    private RemoveGroupResponse(boolean noInit) {}
+    
+    private static final RemoveGroupResponse defaultInstance;
+    public static RemoveGroupResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public RemoveGroupResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupResponse_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_RemoveGroupResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:RemoveGroupResponse)
+    }
+    
+    static {
+      defaultInstance = new RemoveGroupResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:RemoveGroupResponse)
+  }
+  
+  public interface ListGroupsRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class ListGroupsRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements ListGroupsRequestOrBuilder {
+    // Use ListGroupsRequest.newBuilder() to construct.
+    private ListGroupsRequest(Builder builder) {
+      super(builder);
+    }
+    private ListGroupsRequest(boolean noInit) {}
+    
+    private static final ListGroupsRequest defaultInstance;
+    public static ListGroupsRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListGroupsRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsRequest_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:ListGroupsRequest)
+    }
+    
+    static {
+      defaultInstance = new ListGroupsRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListGroupsRequest)
+  }
+  
+  public interface ListGroupsResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated .GroupInfo groupInfo = 1;
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> 
+        getGroupInfoList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index);
+    int getGroupInfoCount();
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+        getGroupInfoOrBuilderList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+        int index);
+  }
+  public static final class ListGroupsResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements ListGroupsResponseOrBuilder {
+    // Use ListGroupsResponse.newBuilder() to construct.
+    private ListGroupsResponse(Builder builder) {
+      super(builder);
+    }
+    private ListGroupsResponse(boolean noInit) {}
+    
+    private static final ListGroupsResponse defaultInstance;
+    public static ListGroupsResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListGroupsResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsResponse_fieldAccessorTable;
+    }
+    
+    // repeated .GroupInfo groupInfo = 1;
+    public static final int GROUPINFO_FIELD_NUMBER = 1;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> groupInfo_;
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> getGroupInfoList() {
+      return groupInfo_;
+    }
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+        getGroupInfoOrBuilderList() {
+      return groupInfo_;
+    }
+    public int getGroupInfoCount() {
+      return groupInfo_.size();
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index) {
+      return groupInfo_.get(index);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+        int index) {
+      return groupInfo_.get(index);
+    }
+    
+    private void initFields() {
+      groupInfo_ = java.util.Collections.emptyList();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      for (int i = 0; i < getGroupInfoCount(); i++) {
+        if (!getGroupInfo(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < groupInfo_.size(); i++) {
+        output.writeMessage(1, groupInfo_.get(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      for (int i = 0; i < groupInfo_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, groupInfo_.get(i));
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse) obj;
+      
+      boolean result = true;
+      result = result && getGroupInfoList()
+          .equals(other.getGroupInfoList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getGroupInfoCount() > 0) {
+        hash = (37 * hash) + GROUPINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupInfoList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListGroupsResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getGroupInfoFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse(this);
+        int from_bitField0_ = bitField0_;
+        if (groupInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+            groupInfo_ = java.util.Collections.unmodifiableList(groupInfo_);
+            bitField0_ = (bitField0_ & ~0x00000001);
+          }
+          result.groupInfo_ = groupInfo_;
+        } else {
+          result.groupInfo_ = groupInfoBuilder_.build();
+        }
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance()) return this;
+        if (groupInfoBuilder_ == null) {
+          if (!other.groupInfo_.isEmpty()) {
+            if (groupInfo_.isEmpty()) {
+              groupInfo_ = other.groupInfo_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+            } else {
+              ensureGroupInfoIsMutable();
+              groupInfo_.addAll(other.groupInfo_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.groupInfo_.isEmpty()) {
+            if (groupInfoBuilder_.isEmpty()) {
+              groupInfoBuilder_.dispose();
+              groupInfoBuilder_ = null;
+              groupInfo_ = other.groupInfo_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+              groupInfoBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getGroupInfoFieldBuilder() : null;
+            } else {
+              groupInfoBuilder_.addAllMessages(other.groupInfo_);
+            }
+          }
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        for (int i = 0; i < getGroupInfoCount(); i++) {
+          if (!getGroupInfo(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder();
+              input.readMessage(subBuilder, extensionRegistry);
+              addGroupInfo(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated .GroupInfo groupInfo = 1;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> groupInfo_ =
+        java.util.Collections.emptyList();
+      private void ensureGroupInfoIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          groupInfo_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo>(groupInfo_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> groupInfoBuilder_;
+      
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> getGroupInfoList() {
+        if (groupInfoBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(groupInfo_);
+        } else {
+          return groupInfoBuilder_.getMessageList();
+        }
+      }
+      public int getGroupInfoCount() {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.size();
+        } else {
+          return groupInfoBuilder_.getCount();
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index) {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.get(index);
+        } else {
+          return groupInfoBuilder_.getMessage(index);
+        }
+      }
+      public Builder setGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.set(index, value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      public Builder setGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(value);
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(index, value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(index, value);
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addAllGroupInfo(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> values) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          super.addAll(values, groupInfo_);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addAllMessages(values);
+        }
+        return this;
+      }
+      public Builder clearGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+          onChanged();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        return this;
+      }
+      public Builder removeGroupInfo(int index) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.remove(index);
+          onChanged();
+        } else {
+          groupInfoBuilder_.remove(index);
+        }
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder getGroupInfoBuilder(
+          int index) {
+        return getGroupInfoFieldBuilder().getBuilder(index);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+          int index) {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.get(index);  } else {
+          return groupInfoBuilder_.getMessageOrBuilder(index);
+        }
+      }
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+           getGroupInfoOrBuilderList() {
+        if (groupInfoBuilder_ != null) {
+          return groupInfoBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(groupInfo_);
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder addGroupInfoBuilder() {
+        return getGroupInfoFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance());
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder addGroupInfoBuilder(
+          int index) {
+        return getGroupInfoFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance());
+      }
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder> 
+           getGroupInfoBuilderList() {
+        return getGroupInfoFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+          getGroupInfoFieldBuilder() {
+        if (groupInfoBuilder_ == null) {
+          groupInfoBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder>(
+                  groupInfo_,
+                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  getParentForChildren(),
+                  isClean());
+          groupInfo_ = null;
+        }
+        return groupInfoBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ListGroupsResponse)
+    }
+    
+    static {
+      defaultInstance = new ListGroupsResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListGroupsResponse)
+  }
+  
+  public interface GetGroupOfServerRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string hostport = 1;
+    boolean hasHostport();
+    String getHostport();
+  }
+  public static final class GetGroupOfServerRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupOfServerRequestOrBuilder {
+    // Use GetGroupOfServerRequest.newBuilder() to construct.
+    private GetGroupOfServerRequest(Builder builder) {
+      super(builder);
+    }
+    private GetGroupOfServerRequest(boolean noInit) {}
+    
+    private static final GetGroupOfServerRequest defaultInstance;
+    public static GetGroupOfServerRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupOfServerRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string hostport = 1;
+    public static final int HOSTPORT_FIELD_NUMBER = 1;
+    private java.lang.Object hostport_;
+    public boolean hasHostport() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getHostport() {
+      java.lang.Object ref = hostport_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          hostport_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getHostportBytes() {
+      java.lang.Object ref = hostport_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        hostport_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      hostport_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasHostport()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getHostportBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getHostportBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasHostport() == other.hasHostport());
+      if (hasHostport()) {
+        result = result && getHostport()
+            .equals(other.getHostport());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasHostport()) {
+        hash = (37 * hash) + HOSTPORT_FIELD_NUMBER;
+        hash = (53 * hash) + getHostport().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        hostport_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.hostport_ = hostport_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.getDefaultInstance()) return this;
+        if (other.hasHostport()) {
+          setHostport(other.getHostport());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasHostport()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              hostport_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string hostport = 1;
+      private java.lang.Object hostport_ = "";
+      public boolean hasHostport() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getHostport() {
+        java.lang.Object ref = hostport_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          hostport_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setHostport(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        hostport_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearHostport() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        hostport_ = getDefaultInstance().getHostport();
+        onChanged();
+        return this;
+      }
+      void setHostport(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        hostport_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupOfServerRequest)
+    }
+    
+    static {
+      defaultInstance = new GetGroupOfServerRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupOfServerRequest)
+  }
+  
+  public interface GetGroupOfServerResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // optional .GroupInfo groupInfo = 1;
+    boolean hasGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder();
+  }
+  public static final class GetGroupOfServerResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements GetGroupOfServerResponseOrBuilder {
+    // Use GetGroupOfServerResponse.newBuilder() to construct.
+    private GetGroupOfServerResponse(Builder builder) {
+      super(builder);
+    }
+    private GetGroupOfServerResponse(boolean noInit) {}
+    
+    private static final GetGroupOfServerResponse defaultInstance;
+    public static GetGroupOfServerResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GetGroupOfServerResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerResponse_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // optional .GroupInfo groupInfo = 1;
+    public static final int GROUPINFO_FIELD_NUMBER = 1;
+    private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_;
+    public boolean hasGroupInfo() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+      return groupInfo_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+      return groupInfo_;
+    }
+    
+    private void initFields() {
+      groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (hasGroupInfo()) {
+        if (!getGroupInfo().isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeMessage(1, groupInfo_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, groupInfo_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupInfo() == other.hasGroupInfo());
+      if (hasGroupInfo()) {
+        result = result && getGroupInfo()
+            .equals(other.getGroupInfo());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupInfo()) {
+        hash = (37 * hash) + GROUPINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupInfo().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_GetGroupOfServerResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getGroupInfoFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        if (groupInfoBuilder_ == null) {
+          result.groupInfo_ = groupInfo_;
+        } else {
+          result.groupInfo_ = groupInfoBuilder_.build();
+        }
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance()) return this;
+        if (other.hasGroupInfo()) {
+          mergeGroupInfo(other.getGroupInfo());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (hasGroupInfo()) {
+          if (!getGroupInfo().isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder();
+              if (hasGroupInfo()) {
+                subBuilder.mergeFrom(getGroupInfo());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setGroupInfo(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // optional .GroupInfo groupInfo = 1;
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> groupInfoBuilder_;
+      public boolean hasGroupInfo() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_;
+        } else {
+          return groupInfoBuilder_.getMessage();
+        }
+      }
+      public Builder setGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          groupInfo_ = value;
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder setGroupInfo(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = builderForValue.build();
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder mergeGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001) &&
+              groupInfo_ != org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance()) {
+            groupInfo_ =
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder(groupInfo_).mergeFrom(value).buildPartial();
+          } else {
+            groupInfo_ = value;
+          }
+          onChanged();
+        } else {
+          groupInfoBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000001;
+        return this;
+      }
+      public Builder clearGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+          onChanged();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder getGroupInfoBuilder() {
+        bitField0_ |= 0x00000001;
+        onChanged();
+        return getGroupInfoFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder() {
+        if (groupInfoBuilder_ != null) {
+          return groupInfoBuilder_.getMessageOrBuilder();
+        } else {
+          return groupInfo_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+          getGroupInfoFieldBuilder() {
+        if (groupInfoBuilder_ == null) {
+          groupInfoBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder>(
+                  groupInfo_,
+                  getParentForChildren(),
+                  isClean());
+          groupInfo_ = null;
+        }
+        return groupInfoBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GetGroupOfServerResponse)
+    }
+    
+    static {
+      defaultInstance = new GetGroupOfServerResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GetGroupOfServerResponse)
+  }
+  
+  public interface ServerGroupPairOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string server = 1;
+    boolean hasServer();
+    String getServer();
+    
+    // required string groupName = 2;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class ServerGroupPair extends
+      com.google.protobuf.GeneratedMessage
+      implements ServerGroupPairOrBuilder {
+    // Use ServerGroupPair.newBuilder() to construct.
+    private ServerGroupPair(Builder builder) {
+      super(builder);
+    }
+    private ServerGroupPair(boolean noInit) {}
+    
+    private static final ServerGroupPair defaultInstance;
+    public static ServerGroupPair getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ServerGroupPair getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ServerGroupPair_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ServerGroupPair_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string server = 1;
+    public static final int SERVER_FIELD_NUMBER = 1;
+    private java.lang.Object server_;
+    public boolean hasServer() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getServer() {
+      java.lang.Object ref = server_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          server_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getServerBytes() {
+      java.lang.Object ref = server_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        server_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    // required string groupName = 2;
+    public static final int GROUPNAME_FIELD_NUMBER = 2;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      server_ = "";
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasServer()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getServerBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeBytes(2, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getServerBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(2, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair) obj;
+      
+      boolean result = true;
+      result = result && (hasServer() == other.hasServer());
+      if (hasServer()) {
+        result = result && getServer()
+            .equals(other.getServer());
+      }
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasServer()) {
+        hash = (37 * hash) + SERVER_FIELD_NUMBER;
+        hash = (53 * hash) + getServer().hashCode();
+      }
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ServerGroupPair_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ServerGroupPair_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        server_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.server_ = server_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.getDefaultInstance()) return this;
+        if (other.hasServer()) {
+          setServer(other.getServer());
+        }
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasServer()) {
+          
+          return false;
+        }
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              server_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              bitField0_ |= 0x00000002;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string server = 1;
+      private java.lang.Object server_ = "";
+      public boolean hasServer() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getServer() {
+        java.lang.Object ref = server_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          server_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setServer(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        server_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearServer() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        server_ = getDefaultInstance().getServer();
+        onChanged();
+        return this;
+      }
+      void setServer(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        server_ = value;
+        onChanged();
+      }
+      
+      // required string groupName = 2;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000002;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000002);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000002;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ServerGroupPair)
+    }
+    
+    static {
+      defaultInstance = new ServerGroupPair(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ServerGroupPair)
+  }
+  
+  public interface ListServersInTransitionRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  public static final class ListServersInTransitionRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements ListServersInTransitionRequestOrBuilder {
+    // Use ListServersInTransitionRequest.newBuilder() to construct.
+    private ListServersInTransitionRequest(Builder builder) {
+      super(builder);
+    }
+    private ListServersInTransitionRequest(boolean noInit) {}
+    
+    private static final ListServersInTransitionRequest defaultInstance;
+    public static ListServersInTransitionRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListServersInTransitionRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionRequest_fieldAccessorTable;
+    }
+    
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest) obj;
+      
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest(this);
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+          }
+        }
+      }
+      
+      
+      // @@protoc_insertion_point(builder_scope:ListServersInTransitionRequest)
+    }
+    
+    static {
+      defaultInstance = new ListServersInTransitionRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListServersInTransitionRequest)
+  }
+  
+  public interface ListServersInTransitionResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated .ServerGroupPair serversInTransition = 1;
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> 
+        getServersInTransitionList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair getServersInTransition(int index);
+    int getServersInTransitionCount();
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder> 
+        getServersInTransitionOrBuilderList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder getServersInTransitionOrBuilder(
+        int index);
+  }
+  public static final class ListServersInTransitionResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements ListServersInTransitionResponseOrBuilder {
+    // Use ListServersInTransitionResponse.newBuilder() to construct.
+    private ListServersInTransitionResponse(Builder builder) {
+      super(builder);
+    }
+    private ListServersInTransitionResponse(boolean noInit) {}
+    
+    private static final ListServersInTransitionResponse defaultInstance;
+    public static ListServersInTransitionResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ListServersInTransitionResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionResponse_fieldAccessorTable;
+    }
+    
+    // repeated .ServerGroupPair serversInTransition = 1;
+    public static final int SERVERSINTRANSITION_FIELD_NUMBER = 1;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> serversInTransition_;
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> getServersInTransitionList() {
+      return serversInTransition_;
+    }
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder> 
+        getServersInTransitionOrBuilderList() {
+      return serversInTransition_;
+    }
+    public int getServersInTransitionCount() {
+      return serversInTransition_.size();
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair getServersInTransition(int index) {
+      return serversInTransition_.get(index);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder getServersInTransitionOrBuilder(
+        int index) {
+      return serversInTransition_.get(index);
+    }
+    
+    private void initFields() {
+      serversInTransition_ = java.util.Collections.emptyList();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      for (int i = 0; i < getServersInTransitionCount(); i++) {
+        if (!getServersInTransition(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < serversInTransition_.size(); i++) {
+        output.writeMessage(1, serversInTransition_.get(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      for (int i = 0; i < serversInTransition_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, serversInTransition_.get(i));
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse) obj;
+      
+      boolean result = true;
+      result = result && getServersInTransitionList()
+          .equals(other.getServersInTransitionList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getServersInTransitionCount() > 0) {
+        hash = (37 * hash) + SERVERSINTRANSITION_FIELD_NUMBER;
+        hash = (53 * hash) + getServersInTransitionList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_ListServersInTransitionResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getServersInTransitionFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (serversInTransitionBuilder_ == null) {
+          serversInTransition_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+        } else {
+          serversInTransitionBuilder_.clear();
+        }
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse(this);
+        int from_bitField0_ = bitField0_;
+        if (serversInTransitionBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+            serversInTransition_ = java.util.Collections.unmodifiableList(serversInTransition_);
+            bitField0_ = (bitField0_ & ~0x00000001);
+          }
+          result.serversInTransition_ = serversInTransition_;
+        } else {
+          result.serversInTransition_ = serversInTransitionBuilder_.build();
+        }
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance()) return this;
+        if (serversInTransitionBuilder_ == null) {
+          if (!other.serversInTransition_.isEmpty()) {
+            if (serversInTransition_.isEmpty()) {
+              serversInTransition_ = other.serversInTransition_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+            } else {
+              ensureServersInTransitionIsMutable();
+              serversInTransition_.addAll(other.serversInTransition_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.serversInTransition_.isEmpty()) {
+            if (serversInTransitionBuilder_.isEmpty()) {
+              serversInTransitionBuilder_.dispose();
+              serversInTransitionBuilder_ = null;
+              serversInTransition_ = other.serversInTransition_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+              serversInTransitionBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getServersInTransitionFieldBuilder() : null;
+            } else {
+              serversInTransitionBuilder_.addAllMessages(other.serversInTransition_);
+            }
+          }
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        for (int i = 0; i < getServersInTransitionCount(); i++) {
+          if (!getServersInTransition(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.newBuilder();
+              input.readMessage(subBuilder, extensionRegistry);
+              addServersInTransition(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated .ServerGroupPair serversInTransition = 1;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> serversInTransition_ =
+        java.util.Collections.emptyList();
+      private void ensureServersInTransitionIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          serversInTransition_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair>(serversInTransition_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder> serversInTransitionBuilder_;
+      
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> getServersInTransitionList() {
+        if (serversInTransitionBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(serversInTransition_);
+        } else {
+          return serversInTransitionBuilder_.getMessageList();
+        }
+      }
+      public int getServersInTransitionCount() {
+        if (serversInTransitionBuilder_ == null) {
+          return serversInTransition_.size();
+        } else {
+          return serversInTransitionBuilder_.getCount();
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair getServersInTransition(int index) {
+        if (serversInTransitionBuilder_ == null) {
+          return serversInTransition_.get(index);
+        } else {
+          return serversInTransitionBuilder_.getMessage(index);
+        }
+      }
+      public Builder setServersInTransition(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair value) {
+        if (serversInTransitionBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.set(index, value);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      public Builder setServersInTransition(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder builderForValue) {
+        if (serversInTransitionBuilder_ == null) {
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addServersInTransition(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair value) {
+        if (serversInTransitionBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.add(value);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.addMessage(value);
+        }
+        return this;
+      }
+      public Builder addServersInTransition(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair value) {
+        if (serversInTransitionBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.add(index, value);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.addMessage(index, value);
+        }
+        return this;
+      }
+      public Builder addServersInTransition(
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder builderForValue) {
+        if (serversInTransitionBuilder_ == null) {
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.add(builderForValue.build());
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addServersInTransition(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder builderForValue) {
+        if (serversInTransitionBuilder_ == null) {
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addAllServersInTransition(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair> values) {
+        if (serversInTransitionBuilder_ == null) {
+          ensureServersInTransitionIsMutable();
+          super.addAll(values, serversInTransition_);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.addAllMessages(values);
+        }
+        return this;
+      }
+      public Builder clearServersInTransition() {
+        if (serversInTransitionBuilder_ == null) {
+          serversInTransition_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.clear();
+        }
+        return this;
+      }
+      public Builder removeServersInTransition(int index) {
+        if (serversInTransitionBuilder_ == null) {
+          ensureServersInTransitionIsMutable();
+          serversInTransition_.remove(index);
+          onChanged();
+        } else {
+          serversInTransitionBuilder_.remove(index);
+        }
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder getServersInTransitionBuilder(
+          int index) {
+        return getServersInTransitionFieldBuilder().getBuilder(index);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder getServersInTransitionOrBuilder(
+          int index) {
+        if (serversInTransitionBuilder_ == null) {
+          return serversInTransition_.get(index);  } else {
+          return serversInTransitionBuilder_.getMessageOrBuilder(index);
+        }
+      }
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder> 
+           getServersInTransitionOrBuilderList() {
+        if (serversInTransitionBuilder_ != null) {
+          return serversInTransitionBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(serversInTransition_);
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder addServersInTransitionBuilder() {
+        return getServersInTransitionFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.getDefaultInstance());
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder addServersInTransitionBuilder(
+          int index) {
+        return getServersInTransitionFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.getDefaultInstance());
+      }
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder> 
+           getServersInTransitionBuilderList() {
+        return getServersInTransitionFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder> 
+          getServersInTransitionFieldBuilder() {
+        if (serversInTransitionBuilder_ == null) {
+          serversInTransitionBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPairOrBuilder>(
+                  serversInTransition_,
+                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  getParentForChildren(),
+                  isClean());
+          serversInTransition_ = null;
+        }
+        return serversInTransitionBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ListServersInTransitionResponse)
+    }
+    
+    static {
+      defaultInstance = new ListServersInTransitionResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ListServersInTransitionResponse)
+  }
+  
+  public interface BalanceGroupRequestOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string groupName = 1;
+    boolean hasGroupName();
+    String getGroupName();
+  }
+  public static final class BalanceGroupRequest extends
+      com.google.protobuf.GeneratedMessage
+      implements BalanceGroupRequestOrBuilder {
+    // Use BalanceGroupRequest.newBuilder() to construct.
+    private BalanceGroupRequest(Builder builder) {
+      super(builder);
+    }
+    private BalanceGroupRequest(boolean noInit) {}
+    
+    private static final BalanceGroupRequest defaultInstance;
+    public static BalanceGroupRequest getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public BalanceGroupRequest getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupRequest_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupRequest_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string groupName = 1;
+    public static final int GROUPNAME_FIELD_NUMBER = 1;
+    private java.lang.Object groupName_;
+    public boolean hasGroupName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getGroupName() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          groupName_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getGroupNameBytes() {
+      java.lang.Object ref = groupName_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        groupName_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    private void initFields() {
+      groupName_ = "";
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasGroupName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getGroupNameBytes());
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getGroupNameBytes());
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest) obj;
+      
+      boolean result = true;
+      result = result && (hasGroupName() == other.hasGroupName());
+      if (hasGroupName()) {
+        result = result && getGroupName()
+            .equals(other.getGroupName());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasGroupName()) {
+        hash = (37 * hash) + GROUPNAME_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupName().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequestOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupRequest_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupRequest_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        groupName_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.groupName_ = groupName_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.getDefaultInstance()) return this;
+        if (other.hasGroupName()) {
+          setGroupName(other.getGroupName());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasGroupName()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              groupName_ = input.readBytes();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string groupName = 1;
+      private java.lang.Object groupName_ = "";
+      public boolean hasGroupName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getGroupName() {
+        java.lang.Object ref = groupName_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          groupName_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setGroupName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearGroupName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        groupName_ = getDefaultInstance().getGroupName();
+        onChanged();
+        return this;
+      }
+      void setGroupName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        groupName_ = value;
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:BalanceGroupRequest)
+    }
+    
+    static {
+      defaultInstance = new BalanceGroupRequest(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:BalanceGroupRequest)
+  }
+  
+  public interface BalanceGroupResponseOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required bool success = 1;
+    boolean hasSuccess();
+    boolean getSuccess();
+  }
+  public static final class BalanceGroupResponse extends
+      com.google.protobuf.GeneratedMessage
+      implements BalanceGroupResponseOrBuilder {
+    // Use BalanceGroupResponse.newBuilder() to construct.
+    private BalanceGroupResponse(Builder builder) {
+      super(builder);
+    }
+    private BalanceGroupResponse(boolean noInit) {}
+    
+    private static final BalanceGroupResponse defaultInstance;
+    public static BalanceGroupResponse getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public BalanceGroupResponse getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupResponse_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupResponse_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required bool success = 1;
+    public static final int SUCCESS_FIELD_NUMBER = 1;
+    private boolean success_;
+    public boolean hasSuccess() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public boolean getSuccess() {
+      return success_;
+    }
+    
+    private void initFields() {
+      success_ = false;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasSuccess()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBool(1, success_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBoolSize(1, success_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse other = (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse) obj;
+      
+      boolean result = true;
+      result = result && (hasSuccess() == other.hasSuccess());
+      if (hasSuccess()) {
+        result = result && (getSuccess()
+            == other.getSuccess());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasSuccess()) {
+        hash = (37 * hash) + SUCCESS_FIELD_NUMBER;
+        hash = (53 * hash) + hashBoolean(getSuccess());
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponseOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupResponse_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.internal_static_BalanceGroupResponse_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        success_ = false;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse result = new org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.success_ = success_;
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance()) return this;
+        if (other.hasSuccess()) {
+          setSuccess(other.getSuccess());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasSuccess()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 8: {
+              bitField0_ |= 0x00000001;
+              success_ = input.readBool();
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required bool success = 1;
+      private boolean success_ ;
+      public boolean hasSuccess() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public boolean getSuccess() {
+        return success_;
+      }
+      public Builder setSuccess(boolean value) {
+        bitField0_ |= 0x00000001;
+        success_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearSuccess() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        success_ = false;
+        onChanged();
+        return this;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:BalanceGroupResponse)
+    }
+    
+    static {
+      defaultInstance = new BalanceGroupResponse(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:BalanceGroupResponse)
+  }
+  
+  public static abstract class GroupAdminService
+      implements com.google.protobuf.Service {
+    protected GroupAdminService() {}
+    
+    public interface Interface {
+      public abstract void addGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse> done);
+      
+      public abstract void removeGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse> done);
+      
+      public abstract void listGroups(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse> done);
+      
+      public abstract void getGroupOfServer(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse> done);
+      
+      public abstract void listServersInTransition(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse> done);
+      
+      public abstract void listTablesOfGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse> done);
+      
+      public abstract void getGroupInfo(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse> done);
+      
+      public abstract void getGroupInfoOfTable(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse> done);
+      
+      public abstract void moveServers(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse> done);
+      
+      public abstract void moveTables(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse> done);
+      
+      public abstract void balanceGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse> done);
+      
+    }
+    
+    public static com.google.protobuf.Service newReflectiveService(
+        final Interface impl) {
+      return new GroupAdminService() {
+        @java.lang.Override
+        public  void addGroup(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse> done) {
+          impl.addGroup(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void removeGroup(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse> done) {
+          impl.removeGroup(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void listGroups(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse> done) {
+          impl.listGroups(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void getGroupOfServer(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse> done) {
+          impl.getGroupOfServer(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void listServersInTransition(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse> done) {
+          impl.listServersInTransition(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void listTablesOfGroup(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse> done) {
+          impl.listTablesOfGroup(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void getGroupInfo(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse> done) {
+          impl.getGroupInfo(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void getGroupInfoOfTable(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse> done) {
+          impl.getGroupInfoOfTable(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void moveServers(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse> done) {
+          impl.moveServers(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void moveTables(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse> done) {
+          impl.moveTables(controller, request, done);
+        }
+        
+        @java.lang.Override
+        public  void balanceGroup(
+            com.google.protobuf.RpcController controller,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request,
+            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse> done) {
+          impl.balanceGroup(controller, request, done);
+        }
+        
+      };
+    }
+    
+    public static com.google.protobuf.BlockingService
+        newReflectiveBlockingService(final BlockingInterface impl) {
+      return new com.google.protobuf.BlockingService() {
+        public final com.google.protobuf.Descriptors.ServiceDescriptor
+            getDescriptorForType() {
+          return getDescriptor();
+        }
+        
+        public final com.google.protobuf.Message callBlockingMethod(
+            com.google.protobuf.Descriptors.MethodDescriptor method,
+            com.google.protobuf.RpcController controller,
+            com.google.protobuf.Message request)
+            throws com.google.protobuf.ServiceException {
+          if (method.getService() != getDescriptor()) {
+            throw new java.lang.IllegalArgumentException(
+              "Service.callBlockingMethod() given method descriptor for " +
+              "wrong service type.");
+          }
+          switch(method.getIndex()) {
+            case 0:
+              return impl.addGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest)request);
+            case 1:
+              return impl.removeGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest)request);
+            case 2:
+              return impl.listGroups(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest)request);
+            case 3:
+              return impl.getGroupOfServer(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest)request);
+            case 4:
+              return impl.listServersInTransition(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest)request);
+            case 5:
+              return impl.listTablesOfGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest)request);
+            case 6:
+              return impl.getGroupInfo(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest)request);
+            case 7:
+              return impl.getGroupInfoOfTable(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest)request);
+            case 8:
+              return impl.moveServers(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest)request);
+            case 9:
+              return impl.moveTables(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest)request);
+            case 10:
+              return impl.balanceGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest)request);
+            default:
+              throw new java.lang.AssertionError("Can't get here.");
+          }
+        }
+        
+        public final com.google.protobuf.Message
+            getRequestPrototype(
+            com.google.protobuf.Descriptors.MethodDescriptor method) {
+          if (method.getService() != getDescriptor()) {
+            throw new java.lang.IllegalArgumentException(
+              "Service.getRequestPrototype() given method " +
+              "descriptor for wrong service type.");
+          }
+          switch(method.getIndex()) {
+            case 0:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.getDefaultInstance();
+            case 1:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.getDefaultInstance();
+            case 2:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.getDefaultInstance();
+            case 3:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.getDefaultInstance();
+            case 4:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.getDefaultInstance();
+            case 5:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.getDefaultInstance();
+            case 6:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.getDefaultInstance();
+            case 7:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.getDefaultInstance();
+            case 8:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.getDefaultInstance();
+            case 9:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.getDefaultInstance();
+            case 10:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.getDefaultInstance();
+            default:
+              throw new java.lang.AssertionError("Can't get here.");
+          }
+        }
+        
+        public final com.google.protobuf.Message
+            getResponsePrototype(
+            com.google.protobuf.Descriptors.MethodDescriptor method) {
+          if (method.getService() != getDescriptor()) {
+            throw new java.lang.IllegalArgumentException(
+              "Service.getResponsePrototype() given method " +
+              "descriptor for wrong service type.");
+          }
+          switch(method.getIndex()) {
+            case 0:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance();
+            case 1:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance();
+            case 2:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance();
+            case 3:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance();
+            case 4:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance();
+            case 5:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance();
+            case 6:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance();
+            case 7:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance();
+            case 8:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance();
+            case 9:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance();
+            case 10:
+              return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance();
+            default:
+              throw new java.lang.AssertionError("Can't get here.");
+          }
+        }
+        
+      };
+    }
+    
+    public abstract void addGroup(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse> done);
+    
+    public abstract void removeGroup(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse> done);
+    
+    public abstract void listGroups(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse> done);
+    
+    public abstract void getGroupOfServer(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse> done);
+    
+    public abstract void listServersInTransition(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse> done);
+    
+    public abstract void listTablesOfGroup(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse> done);
+    
+    public abstract void getGroupInfo(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse> done);
+    
+    public abstract void getGroupInfoOfTable(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse> done);
+    
+    public abstract void moveServers(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse> done);
+    
+    public abstract void moveTables(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse> done);
+    
+    public abstract void balanceGroup(
+        com.google.protobuf.RpcController controller,
+        org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request,
+        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse> done);
+    
+    public static final
+        com.google.protobuf.Descriptors.ServiceDescriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.getDescriptor().getServices().get(0);
+    }
+    public final com.google.protobuf.Descriptors.ServiceDescriptor
+        getDescriptorForType() {
+      return getDescriptor();
+    }
+    
+    public final void callMethod(
+        com.google.protobuf.Descriptors.MethodDescriptor method,
+        com.google.protobuf.RpcController controller,
+        com.google.protobuf.Message request,
+        com.google.protobuf.RpcCallback<
+          com.google.protobuf.Message> done) {
+      if (method.getService() != getDescriptor()) {
+        throw new java.lang.IllegalArgumentException(
+          "Service.callMethod() given method descriptor for wrong " +
+          "service type.");
+      }
+      switch(method.getIndex()) {
+        case 0:
+          this.addGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse>specializeCallback(
+              done));
+          return;
+        case 1:
+          this.removeGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse>specializeCallback(
+              done));
+          return;
+        case 2:
+          this.listGroups(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse>specializeCallback(
+              done));
+          return;
+        case 3:
+          this.getGroupOfServer(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse>specializeCallback(
+              done));
+          return;
+        case 4:
+          this.listServersInTransition(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse>specializeCallback(
+              done));
+          return;
+        case 5:
+          this.listTablesOfGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse>specializeCallback(
+              done));
+          return;
+        case 6:
+          this.getGroupInfo(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse>specializeCallback(
+              done));
+          return;
+        case 7:
+          this.getGroupInfoOfTable(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse>specializeCallback(
+              done));
+          return;
+        case 8:
+          this.moveServers(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse>specializeCallback(
+              done));
+          return;
+        case 9:
+          this.moveTables(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse>specializeCallback(
+              done));
+          return;
+        case 10:
+          this.balanceGroup(controller, (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest)request,
+            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse>specializeCallback(
+              done));
+          return;
+        default:
+          throw new java.lang.AssertionError("Can't get here.");
+      }
+    }
+    
+    public final com.google.protobuf.Message
+        getRequestPrototype(
+        com.google.protobuf.Descriptors.MethodDescriptor method) {
+      if (method.getService() != getDescriptor()) {
+        throw new java.lang.IllegalArgumentException(
+          "Service.getRequestPrototype() given method " +
+          "descriptor for wrong service type.");
+      }
+      switch(method.getIndex()) {
+        case 0:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.getDefaultInstance();
+        case 1:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.getDefaultInstance();
+        case 2:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.getDefaultInstance();
+        case 3:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.getDefaultInstance();
+        case 4:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.getDefaultInstance();
+        case 5:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.getDefaultInstance();
+        case 6:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.getDefaultInstance();
+        case 7:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.getDefaultInstance();
+        case 8:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.getDefaultInstance();
+        case 9:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.getDefaultInstance();
+        case 10:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.getDefaultInstance();
+        default:
+          throw new java.lang.AssertionError("Can't get here.");
+      }
+    }
+    
+    public final com.google.protobuf.Message
+        getResponsePrototype(
+        com.google.protobuf.Descriptors.MethodDescriptor method) {
+      if (method.getService() != getDescriptor()) {
+        throw new java.lang.IllegalArgumentException(
+          "Service.getResponsePrototype() given method " +
+          "descriptor for wrong service type.");
+      }
+      switch(method.getIndex()) {
+        case 0:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance();
+        case 1:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance();
+        case 2:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance();
+        case 3:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance();
+        case 4:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance();
+        case 5:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance();
+        case 6:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance();
+        case 7:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance();
+        case 8:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance();
+        case 9:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance();
+        case 10:
+          return org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance();
+        default:
+          throw new java.lang.AssertionError("Can't get here.");
+      }
+    }
+    
+    public static Stub newStub(
+        com.google.protobuf.RpcChannel channel) {
+      return new Stub(channel);
+    }
+    
+    public static final class Stub extends org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GroupAdminService implements Interface {
+      private Stub(com.google.protobuf.RpcChannel channel) {
+        this.channel = channel;
+      }
+      
+      private final com.google.protobuf.RpcChannel channel;
+      
+      public com.google.protobuf.RpcChannel getChannel() {
+        return channel;
+      }
+      
+      public  void addGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(0),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance()));
+      }
+      
+      public  void removeGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(1),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance()));
+      }
+      
+      public  void listGroups(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(2),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance()));
+      }
+      
+      public  void getGroupOfServer(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(3),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance()));
+      }
+      
+      public  void listServersInTransition(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(4),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance()));
+      }
+      
+      public  void listTablesOfGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(5),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance()));
+      }
+      
+      public  void getGroupInfo(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(6),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance()));
+      }
+      
+      public  void getGroupInfoOfTable(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(7),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance()));
+      }
+      
+      public  void moveServers(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(8),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance()));
+      }
+      
+      public  void moveTables(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(9),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance()));
+      }
+      
+      public  void balanceGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request,
+          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse> done) {
+        channel.callMethod(
+          getDescriptor().getMethods().get(10),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance(),
+          com.google.protobuf.RpcUtil.generalizeCallback(
+            done,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.class,
+            org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance()));
+      }
+    }
+    
+    public static BlockingInterface newBlockingStub(
+        com.google.protobuf.BlockingRpcChannel channel) {
+      return new BlockingStub(channel);
+    }
+    
+    public interface BlockingInterface {
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse addGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse removeGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse listGroups(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse getGroupOfServer(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse listServersInTransition(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse listTablesOfGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse getGroupInfo(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse getGroupInfoOfTable(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse moveServers(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse moveTables(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request)
+          throws com.google.protobuf.ServiceException;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse balanceGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request)
+          throws com.google.protobuf.ServiceException;
+    }
+    
+    private static final class BlockingStub implements BlockingInterface {
+      private BlockingStub(com.google.protobuf.BlockingRpcChannel channel) {
+        this.channel = channel;
+      }
+      
+      private final com.google.protobuf.BlockingRpcChannel channel;
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse addGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(0),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse removeGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(1),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse listGroups(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(2),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse getGroupOfServer(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(3),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse listServersInTransition(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(4),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse listTablesOfGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(5),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse getGroupInfo(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(6),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse getGroupInfoOfTable(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(7),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse moveServers(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(8),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse moveTables(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(9),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.getDefaultInstance());
+      }
+      
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse balanceGroup(
+          com.google.protobuf.RpcController controller,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest request)
+          throws com.google.protobuf.ServiceException {
+        return (org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse) channel.callBlockingMethod(
+          getDescriptor().getMethods().get(10),
+          controller,
+          request,
+          org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.getDefaultInstance());
+      }
+      
+    }
+  }
+  
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListTablesOfGroupRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListTablesOfGroupRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListTablesOfGroupResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListTablesOfGroupResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupInfoRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupInfoRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupInfoResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupInfoResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupInfoOfTableRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupInfoOfTableRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupInfoOfTableResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupInfoOfTableResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_MoveServersRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_MoveServersRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_MoveServersResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_MoveServersResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_MoveTablesRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_MoveTablesRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_MoveTablesResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_MoveTablesResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_AddGroupRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_AddGroupRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_AddGroupResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_AddGroupResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_RemoveGroupRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_RemoveGroupRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_RemoveGroupResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_RemoveGroupResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListGroupsRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListGroupsRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListGroupsResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListGroupsResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupOfServerRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupOfServerRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GetGroupOfServerResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GetGroupOfServerResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ServerGroupPair_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ServerGroupPair_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListServersInTransitionRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListServersInTransitionRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ListServersInTransitionResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ListServersInTransitionResponse_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_BalanceGroupRequest_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_BalanceGroupRequest_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_BalanceGroupResponse_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_BalanceGroupResponse_fieldAccessorTable;
+  
+  public static com.google.protobuf.Descriptors.FileDescriptor
+      getDescriptor() {
+    return descriptor;
+  }
+  private static com.google.protobuf.Descriptors.FileDescriptor
+      descriptor;
+  static {
+    java.lang.String[] descriptorData = {
+      "\n\020GroupAdmin.proto\032\017GroupInfo.proto\"-\n\030L" +
+      "istTablesOfGroupRequest\022\021\n\tgroupName\030\001 \002" +
+      "(\t\"/\n\031ListTablesOfGroupResponse\022\022\n\ntable" +
+      "Names\030\001 \003(\t\"(\n\023GetGroupInfoRequest\022\021\n\tgr" +
+      "oupName\030\001 \002(\t\"5\n\024GetGroupInfoResponse\022\035\n" +
+      "\tgroupInfo\030\001 \001(\0132\n.GroupInfo\"/\n\032GetGroup" +
+      "InfoOfTableRequest\022\021\n\ttableName\030\001 \002(\t\"<\n" +
+      "\033GetGroupInfoOfTableResponse\022\035\n\tgroupInf" +
+      "o\030\001 \001(\0132\n.GroupInfo\":\n\022MoveServersReques" +
+      "t\022\023\n\013targetGroup\030\001 \002(\t\022\017\n\007servers\030\002 \003(\t\"",
+      "\025\n\023MoveServersResponse\"8\n\021MoveTablesRequ" +
+      "est\022\023\n\013targetGroup\030\001 \002(\t\022\016\n\006tables\030\002 \003(\t" +
+      "\"\024\n\022MoveTablesResponse\"$\n\017AddGroupReques" +
+      "t\022\021\n\tgroupName\030\001 \002(\t\"\022\n\020AddGroupResponse" +
+      "\"\'\n\022RemoveGroupRequest\022\021\n\tgroupName\030\001 \002(" +
+      "\t\"\025\n\023RemoveGroupResponse\"\023\n\021ListGroupsRe" +
+      "quest\"3\n\022ListGroupsResponse\022\035\n\tgroupInfo" +
+      "\030\001 \003(\0132\n.GroupInfo\"+\n\027GetGroupOfServerRe" +
+      "quest\022\020\n\010hostport\030\001 \002(\t\"9\n\030GetGroupOfSer" +
+      "verResponse\022\035\n\tgroupInfo\030\001 \001(\0132\n.GroupIn",
+      "fo\"4\n\017ServerGroupPair\022\016\n\006server\030\001 \002(\t\022\021\n" +
+      "\tgroupName\030\002 \002(\t\" \n\036ListServersInTransit" +
+      "ionRequest\"P\n\037ListServersInTransitionRes" +
+      "ponse\022-\n\023serversInTransition\030\001 \003(\0132\020.Ser" +
+      "verGroupPair\"(\n\023BalanceGroupRequest\022\021\n\tg" +
+      "roupName\030\001 \002(\t\"\'\n\024BalanceGroupResponse\022\017" +
+      "\n\007success\030\001 \002(\0102\345\005\n\021GroupAdminService\022/\n" +
+      "\010addGroup\022\020.AddGroupRequest\032\021.AddGroupRe" +
+      "sponse\0228\n\013removeGroup\022\023.RemoveGroupReque" +
+      "st\032\024.RemoveGroupResponse\0225\n\nlistGroups\022\022",
+      ".ListGroupsRequest\032\023.ListGroupsResponse\022" +
+      "G\n\020getGroupOfServer\022\030.GetGroupOfServerRe" +
+      "quest\032\031.GetGroupOfServerResponse\022\\\n\027list" +
+      "ServersInTransition\022\037.ListServersInTrans" +
+      "itionRequest\032 .ListServersInTransitionRe" +
+      "sponse\022J\n\021listTablesOfGroup\022\031.ListTables" +
+      "OfGroupRequest\032\032.ListTablesOfGroupRespon" +
+      "se\022;\n\014getGroupInfo\022\024.GetGroupInfoRequest" +
+      "\032\025.GetGroupInfoResponse\022P\n\023getGroupInfoO" +
+      "fTable\022\033.GetGroupInfoOfTableRequest\032\034.Ge",
+      "tGroupInfoOfTableResponse\0228\n\013moveServers" +
+      "\022\023.MoveServersRequest\032\024.MoveServersRespo" +
+      "nse\0225\n\nmoveTables\022\022.MoveTablesRequest\032\023." +
+      "MoveTablesResponse\022;\n\014balanceGroup\022\024.Bal" +
+      "anceGroupRequest\032\025.BalanceGroupResponseB" +
+      "F\n*org.apache.hadoop.hbase.protobuf.gene" +
+      "ratedB\020GroupAdminProtosH\001\210\001\001\240\001\001"
+    };
+    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
+      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
+        public com.google.protobuf.ExtensionRegistry assignDescriptors(
+            com.google.protobuf.Descriptors.FileDescriptor root) {
+          descriptor = root;
+          internal_static_ListTablesOfGroupRequest_descriptor =
+            getDescriptor().getMessageTypes().get(0);
+          internal_static_ListTablesOfGroupRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListTablesOfGroupRequest_descriptor,
+              new java.lang.String[] { "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest.Builder.class);
+          internal_static_ListTablesOfGroupResponse_descriptor =
+            getDescriptor().getMessageTypes().get(1);
+          internal_static_ListTablesOfGroupResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListTablesOfGroupResponse_descriptor,
+              new java.lang.String[] { "TableNames", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse.Builder.class);
+          internal_static_GetGroupInfoRequest_descriptor =
+            getDescriptor().getMessageTypes().get(2);
+          internal_static_GetGroupInfoRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupInfoRequest_descriptor,
+              new java.lang.String[] { "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest.Builder.class);
+          internal_static_GetGroupInfoResponse_descriptor =
+            getDescriptor().getMessageTypes().get(3);
+          internal_static_GetGroupInfoResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupInfoResponse_descriptor,
+              new java.lang.String[] { "GroupInfo", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse.Builder.class);
+          internal_static_GetGroupInfoOfTableRequest_descriptor =
+            getDescriptor().getMessageTypes().get(4);
+          internal_static_GetGroupInfoOfTableRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupInfoOfTableRequest_descriptor,
+              new java.lang.String[] { "TableName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest.Builder.class);
+          internal_static_GetGroupInfoOfTableResponse_descriptor =
+            getDescriptor().getMessageTypes().get(5);
+          internal_static_GetGroupInfoOfTableResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupInfoOfTableResponse_descriptor,
+              new java.lang.String[] { "GroupInfo", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse.Builder.class);
+          internal_static_MoveServersRequest_descriptor =
+            getDescriptor().getMessageTypes().get(6);
+          internal_static_MoveServersRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_MoveServersRequest_descriptor,
+              new java.lang.String[] { "TargetGroup", "Servers", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest.Builder.class);
+          internal_static_MoveServersResponse_descriptor =
+            getDescriptor().getMessageTypes().get(7);
+          internal_static_MoveServersResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_MoveServersResponse_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse.Builder.class);
+          internal_static_MoveTablesRequest_descriptor =
+            getDescriptor().getMessageTypes().get(8);
+          internal_static_MoveTablesRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_MoveTablesRequest_descriptor,
+              new java.lang.String[] { "TargetGroup", "Tables", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest.Builder.class);
+          internal_static_MoveTablesResponse_descriptor =
+            getDescriptor().getMessageTypes().get(9);
+          internal_static_MoveTablesResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_MoveTablesResponse_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse.Builder.class);
+          internal_static_AddGroupRequest_descriptor =
+            getDescriptor().getMessageTypes().get(10);
+          internal_static_AddGroupRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_AddGroupRequest_descriptor,
+              new java.lang.String[] { "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest.Builder.class);
+          internal_static_AddGroupResponse_descriptor =
+            getDescriptor().getMessageTypes().get(11);
+          internal_static_AddGroupResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_AddGroupResponse_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse.Builder.class);
+          internal_static_RemoveGroupRequest_descriptor =
+            getDescriptor().getMessageTypes().get(12);
+          internal_static_RemoveGroupRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_RemoveGroupRequest_descriptor,
+              new java.lang.String[] { "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest.Builder.class);
+          internal_static_RemoveGroupResponse_descriptor =
+            getDescriptor().getMessageTypes().get(13);
+          internal_static_RemoveGroupResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_RemoveGroupResponse_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse.Builder.class);
+          internal_static_ListGroupsRequest_descriptor =
+            getDescriptor().getMessageTypes().get(14);
+          internal_static_ListGroupsRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListGroupsRequest_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest.Builder.class);
+          internal_static_ListGroupsResponse_descriptor =
+            getDescriptor().getMessageTypes().get(15);
+          internal_static_ListGroupsResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListGroupsResponse_descriptor,
+              new java.lang.String[] { "GroupInfo", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse.Builder.class);
+          internal_static_GetGroupOfServerRequest_descriptor =
+            getDescriptor().getMessageTypes().get(16);
+          internal_static_GetGroupOfServerRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupOfServerRequest_descriptor,
+              new java.lang.String[] { "Hostport", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest.Builder.class);
+          internal_static_GetGroupOfServerResponse_descriptor =
+            getDescriptor().getMessageTypes().get(17);
+          internal_static_GetGroupOfServerResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GetGroupOfServerResponse_descriptor,
+              new java.lang.String[] { "GroupInfo", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse.Builder.class);
+          internal_static_ServerGroupPair_descriptor =
+            getDescriptor().getMessageTypes().get(18);
+          internal_static_ServerGroupPair_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ServerGroupPair_descriptor,
+              new java.lang.String[] { "Server", "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair.Builder.class);
+          internal_static_ListServersInTransitionRequest_descriptor =
+            getDescriptor().getMessageTypes().get(19);
+          internal_static_ListServersInTransitionRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListServersInTransitionRequest_descriptor,
+              new java.lang.String[] { },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest.Builder.class);
+          internal_static_ListServersInTransitionResponse_descriptor =
+            getDescriptor().getMessageTypes().get(20);
+          internal_static_ListServersInTransitionResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ListServersInTransitionResponse_descriptor,
+              new java.lang.String[] { "ServersInTransition", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse.Builder.class);
+          internal_static_BalanceGroupRequest_descriptor =
+            getDescriptor().getMessageTypes().get(21);
+          internal_static_BalanceGroupRequest_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_BalanceGroupRequest_descriptor,
+              new java.lang.String[] { "GroupName", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest.Builder.class);
+          internal_static_BalanceGroupResponse_descriptor =
+            getDescriptor().getMessageTypes().get(22);
+          internal_static_BalanceGroupResponse_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_BalanceGroupResponse_descriptor,
+              new java.lang.String[] { "Success", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse.Builder.class);
+          return null;
+        }
+      };
+    com.google.protobuf.Descriptors.FileDescriptor
+      .internalBuildGeneratedFileFrom(descriptorData,
+        new com.google.protobuf.Descriptors.FileDescriptor[] {
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.getDescriptor(),
+        }, assigner);
+  }
+  
+  // @@protoc_insertion_point(outer_class_scope)
+}
diff --git a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupInfoProtos.java b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupInfoProtos.java
new file mode 100644
index 0000000..33c2e88
--- /dev/null
+++ b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/GroupInfoProtos.java
@@ -0,0 +1,2262 @@
+// Generated by the protocol buffer compiler.  DO NOT EDIT!
+// source: GroupInfo.proto
+
+package org.apache.hadoop.hbase.protobuf.generated;
+
+public final class GroupInfoProtos {
+  private GroupInfoProtos() {}
+  public static void registerAllExtensions(
+      com.google.protobuf.ExtensionRegistry registry) {
+  }
+  public interface GroupInfoOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // required string name = 1;
+    boolean hasName();
+    String getName();
+    
+    // required .ServerList servers = 2;
+    boolean hasServers();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList getServers();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder getServersOrBuilder();
+    
+    // required .TableList tables = 3;
+    boolean hasTables();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList getTables();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder getTablesOrBuilder();
+  }
+  public static final class GroupInfo extends
+      com.google.protobuf.GeneratedMessage
+      implements GroupInfoOrBuilder {
+    // Use GroupInfo.newBuilder() to construct.
+    private GroupInfo(Builder builder) {
+      super(builder);
+    }
+    private GroupInfo(boolean noInit) {}
+    
+    private static final GroupInfo defaultInstance;
+    public static GroupInfo getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GroupInfo getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfo_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfo_fieldAccessorTable;
+    }
+    
+    private int bitField0_;
+    // required string name = 1;
+    public static final int NAME_FIELD_NUMBER = 1;
+    private java.lang.Object name_;
+    public boolean hasName() {
+      return ((bitField0_ & 0x00000001) == 0x00000001);
+    }
+    public String getName() {
+      java.lang.Object ref = name_;
+      if (ref instanceof String) {
+        return (String) ref;
+      } else {
+        com.google.protobuf.ByteString bs = 
+            (com.google.protobuf.ByteString) ref;
+        String s = bs.toStringUtf8();
+        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
+          name_ = s;
+        }
+        return s;
+      }
+    }
+    private com.google.protobuf.ByteString getNameBytes() {
+      java.lang.Object ref = name_;
+      if (ref instanceof String) {
+        com.google.protobuf.ByteString b = 
+            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
+        name_ = b;
+        return b;
+      } else {
+        return (com.google.protobuf.ByteString) ref;
+      }
+    }
+    
+    // required .ServerList servers = 2;
+    public static final int SERVERS_FIELD_NUMBER = 2;
+    private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList servers_;
+    public boolean hasServers() {
+      return ((bitField0_ & 0x00000002) == 0x00000002);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList getServers() {
+      return servers_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder getServersOrBuilder() {
+      return servers_;
+    }
+    
+    // required .TableList tables = 3;
+    public static final int TABLES_FIELD_NUMBER = 3;
+    private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList tables_;
+    public boolean hasTables() {
+      return ((bitField0_ & 0x00000004) == 0x00000004);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList getTables() {
+      return tables_;
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder getTablesOrBuilder() {
+      return tables_;
+    }
+    
+    private void initFields() {
+      name_ = "";
+      servers_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance();
+      tables_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      if (!hasName()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasServers()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      if (!hasTables()) {
+        memoizedIsInitialized = 0;
+        return false;
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        output.writeBytes(1, getNameBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        output.writeMessage(2, servers_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        output.writeMessage(3, tables_);
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      if (((bitField0_ & 0x00000001) == 0x00000001)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(1, getNameBytes());
+      }
+      if (((bitField0_ & 0x00000002) == 0x00000002)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(2, servers_);
+      }
+      if (((bitField0_ & 0x00000004) == 0x00000004)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(3, tables_);
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo other = (org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo) obj;
+      
+      boolean result = true;
+      result = result && (hasName() == other.hasName());
+      if (hasName()) {
+        result = result && getName()
+            .equals(other.getName());
+      }
+      result = result && (hasServers() == other.hasServers());
+      if (hasServers()) {
+        result = result && getServers()
+            .equals(other.getServers());
+      }
+      result = result && (hasTables() == other.hasTables());
+      if (hasTables()) {
+        result = result && getTables()
+            .equals(other.getTables());
+      }
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (hasName()) {
+        hash = (37 * hash) + NAME_FIELD_NUMBER;
+        hash = (53 * hash) + getName().hashCode();
+      }
+      if (hasServers()) {
+        hash = (37 * hash) + SERVERS_FIELD_NUMBER;
+        hash = (53 * hash) + getServers().hashCode();
+      }
+      if (hasTables()) {
+        hash = (37 * hash) + TABLES_FIELD_NUMBER;
+        hash = (53 * hash) + getTables().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfo_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfo_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getServersFieldBuilder();
+          getTablesFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        name_ = "";
+        bitField0_ = (bitField0_ & ~0x00000001);
+        if (serversBuilder_ == null) {
+          servers_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance();
+        } else {
+          serversBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000002);
+        if (tablesBuilder_ == null) {
+          tables_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance();
+        } else {
+          tablesBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000004);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo result = new org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo(this);
+        int from_bitField0_ = bitField0_;
+        int to_bitField0_ = 0;
+        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
+          to_bitField0_ |= 0x00000001;
+        }
+        result.name_ = name_;
+        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
+          to_bitField0_ |= 0x00000002;
+        }
+        if (serversBuilder_ == null) {
+          result.servers_ = servers_;
+        } else {
+          result.servers_ = serversBuilder_.build();
+        }
+        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
+          to_bitField0_ |= 0x00000004;
+        }
+        if (tablesBuilder_ == null) {
+          result.tables_ = tables_;
+        } else {
+          result.tables_ = tablesBuilder_.build();
+        }
+        result.bitField0_ = to_bitField0_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance()) return this;
+        if (other.hasName()) {
+          setName(other.getName());
+        }
+        if (other.hasServers()) {
+          mergeServers(other.getServers());
+        }
+        if (other.hasTables()) {
+          mergeTables(other.getTables());
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        if (!hasName()) {
+          
+          return false;
+        }
+        if (!hasServers()) {
+          
+          return false;
+        }
+        if (!hasTables()) {
+          
+          return false;
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              bitField0_ |= 0x00000001;
+              name_ = input.readBytes();
+              break;
+            }
+            case 18: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.newBuilder();
+              if (hasServers()) {
+                subBuilder.mergeFrom(getServers());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setServers(subBuilder.buildPartial());
+              break;
+            }
+            case 26: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.newBuilder();
+              if (hasTables()) {
+                subBuilder.mergeFrom(getTables());
+              }
+              input.readMessage(subBuilder, extensionRegistry);
+              setTables(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // required string name = 1;
+      private java.lang.Object name_ = "";
+      public boolean hasName() {
+        return ((bitField0_ & 0x00000001) == 0x00000001);
+      }
+      public String getName() {
+        java.lang.Object ref = name_;
+        if (!(ref instanceof String)) {
+          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
+          name_ = s;
+          return s;
+        } else {
+          return (String) ref;
+        }
+      }
+      public Builder setName(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00000001;
+        name_ = value;
+        onChanged();
+        return this;
+      }
+      public Builder clearName() {
+        bitField0_ = (bitField0_ & ~0x00000001);
+        name_ = getDefaultInstance().getName();
+        onChanged();
+        return this;
+      }
+      void setName(com.google.protobuf.ByteString value) {
+        bitField0_ |= 0x00000001;
+        name_ = value;
+        onChanged();
+      }
+      
+      // required .ServerList servers = 2;
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList servers_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder> serversBuilder_;
+      public boolean hasServers() {
+        return ((bitField0_ & 0x00000002) == 0x00000002);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList getServers() {
+        if (serversBuilder_ == null) {
+          return servers_;
+        } else {
+          return serversBuilder_.getMessage();
+        }
+      }
+      public Builder setServers(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList value) {
+        if (serversBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          servers_ = value;
+          onChanged();
+        } else {
+          serversBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000002;
+        return this;
+      }
+      public Builder setServers(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder builderForValue) {
+        if (serversBuilder_ == null) {
+          servers_ = builderForValue.build();
+          onChanged();
+        } else {
+          serversBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000002;
+        return this;
+      }
+      public Builder mergeServers(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList value) {
+        if (serversBuilder_ == null) {
+          if (((bitField0_ & 0x00000002) == 0x00000002) &&
+              servers_ != org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance()) {
+            servers_ =
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.newBuilder(servers_).mergeFrom(value).buildPartial();
+          } else {
+            servers_ = value;
+          }
+          onChanged();
+        } else {
+          serversBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000002;
+        return this;
+      }
+      public Builder clearServers() {
+        if (serversBuilder_ == null) {
+          servers_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance();
+          onChanged();
+        } else {
+          serversBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000002);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder getServersBuilder() {
+        bitField0_ |= 0x00000002;
+        onChanged();
+        return getServersFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder getServersOrBuilder() {
+        if (serversBuilder_ != null) {
+          return serversBuilder_.getMessageOrBuilder();
+        } else {
+          return servers_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder> 
+          getServersFieldBuilder() {
+        if (serversBuilder_ == null) {
+          serversBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder>(
+                  servers_,
+                  getParentForChildren(),
+                  isClean());
+          servers_ = null;
+        }
+        return serversBuilder_;
+      }
+      
+      // required .TableList tables = 3;
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList tables_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance();
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder> tablesBuilder_;
+      public boolean hasTables() {
+        return ((bitField0_ & 0x00000004) == 0x00000004);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList getTables() {
+        if (tablesBuilder_ == null) {
+          return tables_;
+        } else {
+          return tablesBuilder_.getMessage();
+        }
+      }
+      public Builder setTables(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList value) {
+        if (tablesBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          tables_ = value;
+          onChanged();
+        } else {
+          tablesBuilder_.setMessage(value);
+        }
+        bitField0_ |= 0x00000004;
+        return this;
+      }
+      public Builder setTables(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder builderForValue) {
+        if (tablesBuilder_ == null) {
+          tables_ = builderForValue.build();
+          onChanged();
+        } else {
+          tablesBuilder_.setMessage(builderForValue.build());
+        }
+        bitField0_ |= 0x00000004;
+        return this;
+      }
+      public Builder mergeTables(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList value) {
+        if (tablesBuilder_ == null) {
+          if (((bitField0_ & 0x00000004) == 0x00000004) &&
+              tables_ != org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance()) {
+            tables_ =
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.newBuilder(tables_).mergeFrom(value).buildPartial();
+          } else {
+            tables_ = value;
+          }
+          onChanged();
+        } else {
+          tablesBuilder_.mergeFrom(value);
+        }
+        bitField0_ |= 0x00000004;
+        return this;
+      }
+      public Builder clearTables() {
+        if (tablesBuilder_ == null) {
+          tables_ = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance();
+          onChanged();
+        } else {
+          tablesBuilder_.clear();
+        }
+        bitField0_ = (bitField0_ & ~0x00000004);
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder getTablesBuilder() {
+        bitField0_ |= 0x00000004;
+        onChanged();
+        return getTablesFieldBuilder().getBuilder();
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder getTablesOrBuilder() {
+        if (tablesBuilder_ != null) {
+          return tablesBuilder_.getMessageOrBuilder();
+        } else {
+          return tables_;
+        }
+      }
+      private com.google.protobuf.SingleFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder> 
+          getTablesFieldBuilder() {
+        if (tablesBuilder_ == null) {
+          tablesBuilder_ = new com.google.protobuf.SingleFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder>(
+                  tables_,
+                  getParentForChildren(),
+                  isClean());
+          tables_ = null;
+        }
+        return tablesBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GroupInfo)
+    }
+    
+    static {
+      defaultInstance = new GroupInfo(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GroupInfo)
+  }
+  
+  public interface ServerListOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated string server = 2;
+    java.util.List<String> getServerList();
+    int getServerCount();
+    String getServer(int index);
+  }
+  public static final class ServerList extends
+      com.google.protobuf.GeneratedMessage
+      implements ServerListOrBuilder {
+    // Use ServerList.newBuilder() to construct.
+    private ServerList(Builder builder) {
+      super(builder);
+    }
+    private ServerList(boolean noInit) {}
+    
+    private static final ServerList defaultInstance;
+    public static ServerList getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public ServerList getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_ServerList_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_ServerList_fieldAccessorTable;
+    }
+    
+    // repeated string server = 2;
+    public static final int SERVER_FIELD_NUMBER = 2;
+    private com.google.protobuf.LazyStringList server_;
+    public java.util.List<String>
+        getServerList() {
+      return server_;
+    }
+    public int getServerCount() {
+      return server_.size();
+    }
+    public String getServer(int index) {
+      return server_.get(index);
+    }
+    
+    private void initFields() {
+      server_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < server_.size(); i++) {
+        output.writeBytes(2, server_.getByteString(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      {
+        int dataSize = 0;
+        for (int i = 0; i < server_.size(); i++) {
+          dataSize += com.google.protobuf.CodedOutputStream
+            .computeBytesSizeNoTag(server_.getByteString(i));
+        }
+        size += dataSize;
+        size += 1 * getServerList().size();
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList other = (org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList) obj;
+      
+      boolean result = true;
+      result = result && getServerList()
+          .equals(other.getServerList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getServerCount() > 0) {
+        hash = (37 * hash) + SERVER_FIELD_NUMBER;
+        hash = (53 * hash) + getServerList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerListOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_ServerList_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_ServerList_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        server_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList result = new org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList(this);
+        int from_bitField0_ = bitField0_;
+        if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          server_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              server_);
+          bitField0_ = (bitField0_ & ~0x00000001);
+        }
+        result.server_ = server_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.getDefaultInstance()) return this;
+        if (!other.server_.isEmpty()) {
+          if (server_.isEmpty()) {
+            server_ = other.server_;
+            bitField0_ = (bitField0_ & ~0x00000001);
+          } else {
+            ensureServerIsMutable();
+            server_.addAll(other.server_);
+          }
+          onChanged();
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 18: {
+              ensureServerIsMutable();
+              server_.add(input.readBytes());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated string server = 2;
+      private com.google.protobuf.LazyStringList server_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureServerIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          server_ = new com.google.protobuf.LazyStringArrayList(server_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      public java.util.List<String>
+          getServerList() {
+        return java.util.Collections.unmodifiableList(server_);
+      }
+      public int getServerCount() {
+        return server_.size();
+      }
+      public String getServer(int index) {
+        return server_.get(index);
+      }
+      public Builder setServer(
+          int index, String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureServerIsMutable();
+        server_.set(index, value);
+        onChanged();
+        return this;
+      }
+      public Builder addServer(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureServerIsMutable();
+        server_.add(value);
+        onChanged();
+        return this;
+      }
+      public Builder addAllServer(
+          java.lang.Iterable<String> values) {
+        ensureServerIsMutable();
+        super.addAll(values, server_);
+        onChanged();
+        return this;
+      }
+      public Builder clearServer() {
+        server_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        onChanged();
+        return this;
+      }
+      void addServer(com.google.protobuf.ByteString value) {
+        ensureServerIsMutable();
+        server_.add(value);
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:ServerList)
+    }
+    
+    static {
+      defaultInstance = new ServerList(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:ServerList)
+  }
+  
+  public interface TableListOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated string table = 3;
+    java.util.List<String> getTableList();
+    int getTableCount();
+    String getTable(int index);
+  }
+  public static final class TableList extends
+      com.google.protobuf.GeneratedMessage
+      implements TableListOrBuilder {
+    // Use TableList.newBuilder() to construct.
+    private TableList(Builder builder) {
+      super(builder);
+    }
+    private TableList(boolean noInit) {}
+    
+    private static final TableList defaultInstance;
+    public static TableList getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public TableList getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_TableList_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_TableList_fieldAccessorTable;
+    }
+    
+    // repeated string table = 3;
+    public static final int TABLE_FIELD_NUMBER = 3;
+    private com.google.protobuf.LazyStringList table_;
+    public java.util.List<String>
+        getTableList() {
+      return table_;
+    }
+    public int getTableCount() {
+      return table_.size();
+    }
+    public String getTable(int index) {
+      return table_.get(index);
+    }
+    
+    private void initFields() {
+      table_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < table_.size(); i++) {
+        output.writeBytes(3, table_.getByteString(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      {
+        int dataSize = 0;
+        for (int i = 0; i < table_.size(); i++) {
+          dataSize += com.google.protobuf.CodedOutputStream
+            .computeBytesSizeNoTag(table_.getByteString(i));
+        }
+        size += dataSize;
+        size += 1 * getTableList().size();
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList other = (org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList) obj;
+      
+      boolean result = true;
+      result = result && getTableList()
+          .equals(other.getTableList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getTableCount() > 0) {
+        hash = (37 * hash) + TABLE_FIELD_NUMBER;
+        hash = (53 * hash) + getTableList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableListOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_TableList_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_TableList_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        table_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList result = new org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList(this);
+        int from_bitField0_ = bitField0_;
+        if (((bitField0_ & 0x00000001) == 0x00000001)) {
+          table_ = new com.google.protobuf.UnmodifiableLazyStringList(
+              table_);
+          bitField0_ = (bitField0_ & ~0x00000001);
+        }
+        result.table_ = table_;
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.getDefaultInstance()) return this;
+        if (!other.table_.isEmpty()) {
+          if (table_.isEmpty()) {
+            table_ = other.table_;
+            bitField0_ = (bitField0_ & ~0x00000001);
+          } else {
+            ensureTableIsMutable();
+            table_.addAll(other.table_);
+          }
+          onChanged();
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 26: {
+              ensureTableIsMutable();
+              table_.add(input.readBytes());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated string table = 3;
+      private com.google.protobuf.LazyStringList table_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+      private void ensureTableIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          table_ = new com.google.protobuf.LazyStringArrayList(table_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      public java.util.List<String>
+          getTableList() {
+        return java.util.Collections.unmodifiableList(table_);
+      }
+      public int getTableCount() {
+        return table_.size();
+      }
+      public String getTable(int index) {
+        return table_.get(index);
+      }
+      public Builder setTable(
+          int index, String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTableIsMutable();
+        table_.set(index, value);
+        onChanged();
+        return this;
+      }
+      public Builder addTable(String value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  ensureTableIsMutable();
+        table_.add(value);
+        onChanged();
+        return this;
+      }
+      public Builder addAllTable(
+          java.lang.Iterable<String> values) {
+        ensureTableIsMutable();
+        super.addAll(values, table_);
+        onChanged();
+        return this;
+      }
+      public Builder clearTable() {
+        table_ = com.google.protobuf.LazyStringArrayList.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00000001);
+        onChanged();
+        return this;
+      }
+      void addTable(com.google.protobuf.ByteString value) {
+        ensureTableIsMutable();
+        table_.add(value);
+        onChanged();
+      }
+      
+      // @@protoc_insertion_point(builder_scope:TableList)
+    }
+    
+    static {
+      defaultInstance = new TableList(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:TableList)
+  }
+  
+  public interface GroupInfoListOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+    
+    // repeated .GroupInfo groupInfo = 1;
+    java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> 
+        getGroupInfoList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index);
+    int getGroupInfoCount();
+    java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+        getGroupInfoOrBuilderList();
+    org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+        int index);
+  }
+  public static final class GroupInfoList extends
+      com.google.protobuf.GeneratedMessage
+      implements GroupInfoListOrBuilder {
+    // Use GroupInfoList.newBuilder() to construct.
+    private GroupInfoList(Builder builder) {
+      super(builder);
+    }
+    private GroupInfoList(boolean noInit) {}
+    
+    private static final GroupInfoList defaultInstance;
+    public static GroupInfoList getDefaultInstance() {
+      return defaultInstance;
+    }
+    
+    public GroupInfoList getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+    
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfoList_descriptor;
+    }
+    
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfoList_fieldAccessorTable;
+    }
+    
+    // repeated .GroupInfo groupInfo = 1;
+    public static final int GROUPINFO_FIELD_NUMBER = 1;
+    private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> groupInfo_;
+    public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> getGroupInfoList() {
+      return groupInfo_;
+    }
+    public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+        getGroupInfoOrBuilderList() {
+      return groupInfo_;
+    }
+    public int getGroupInfoCount() {
+      return groupInfo_.size();
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index) {
+      return groupInfo_.get(index);
+    }
+    public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+        int index) {
+      return groupInfo_.get(index);
+    }
+    
+    private void initFields() {
+      groupInfo_ = java.util.Collections.emptyList();
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+      
+      for (int i = 0; i < getGroupInfoCount(); i++) {
+        if (!getGroupInfo(i).isInitialized()) {
+          memoizedIsInitialized = 0;
+          return false;
+        }
+      }
+      memoizedIsInitialized = 1;
+      return true;
+    }
+    
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      for (int i = 0; i < groupInfo_.size(); i++) {
+        output.writeMessage(1, groupInfo_.get(i));
+      }
+      getUnknownFields().writeTo(output);
+    }
+    
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+    
+      size = 0;
+      for (int i = 0; i < groupInfo_.size(); i++) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeMessageSize(1, groupInfo_.get(i));
+      }
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+    
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+    
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList other = (org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList) obj;
+      
+      boolean result = true;
+      result = result && getGroupInfoList()
+          .equals(other.getGroupInfoList());
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+    
+    @java.lang.Override
+    public int hashCode() {
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      if (getGroupInfoCount() > 0) {
+        hash = (37 * hash) + GROUPINFO_FIELD_NUMBER;
+        hash = (53 * hash) + getGroupInfoList().hashCode();
+      }
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      return hash;
+    }
+    
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return newBuilder().mergeFrom(data, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      Builder builder = newBuilder();
+      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
+        return builder.buildParsed();
+      } else {
+        return null;
+      }
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input).buildParsed();
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return newBuilder().mergeFrom(input, extensionRegistry)
+               .buildParsed();
+    }
+    
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+    
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoListOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfoList_descriptor;
+      }
+      
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.internal_static_GroupInfoList_fieldAccessorTable;
+      }
+      
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+      
+      private Builder(BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+          getGroupInfoFieldBuilder();
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+      
+      public Builder clear() {
+        super.clear();
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        return this;
+      }
+      
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+      
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.getDescriptor();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.getDefaultInstance();
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList build() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+      
+      private org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList buildParsed()
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(
+            result).asInvalidProtocolBufferException();
+        }
+        return result;
+      }
+      
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList result = new org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList(this);
+        int from_bitField0_ = bitField0_;
+        if (groupInfoBuilder_ == null) {
+          if (((bitField0_ & 0x00000001) == 0x00000001)) {
+            groupInfo_ = java.util.Collections.unmodifiableList(groupInfo_);
+            bitField0_ = (bitField0_ & ~0x00000001);
+          }
+          result.groupInfo_ = groupInfo_;
+        } else {
+          result.groupInfo_ = groupInfoBuilder_.build();
+        }
+        onBuilt();
+        return result;
+      }
+      
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+      
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.getDefaultInstance()) return this;
+        if (groupInfoBuilder_ == null) {
+          if (!other.groupInfo_.isEmpty()) {
+            if (groupInfo_.isEmpty()) {
+              groupInfo_ = other.groupInfo_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+            } else {
+              ensureGroupInfoIsMutable();
+              groupInfo_.addAll(other.groupInfo_);
+            }
+            onChanged();
+          }
+        } else {
+          if (!other.groupInfo_.isEmpty()) {
+            if (groupInfoBuilder_.isEmpty()) {
+              groupInfoBuilder_.dispose();
+              groupInfoBuilder_ = null;
+              groupInfo_ = other.groupInfo_;
+              bitField0_ = (bitField0_ & ~0x00000001);
+              groupInfoBuilder_ = 
+                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
+                   getGroupInfoFieldBuilder() : null;
+            } else {
+              groupInfoBuilder_.addAllMessages(other.groupInfo_);
+            }
+          }
+        }
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+      
+      public final boolean isInitialized() {
+        for (int i = 0; i < getGroupInfoCount(); i++) {
+          if (!getGroupInfo(i).isInitialized()) {
+            
+            return false;
+          }
+        }
+        return true;
+      }
+      
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder(
+            this.getUnknownFields());
+        while (true) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              this.setUnknownFields(unknownFields.build());
+              onChanged();
+              return this;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                this.setUnknownFields(unknownFields.build());
+                onChanged();
+                return this;
+              }
+              break;
+            }
+            case 10: {
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.newBuilder();
+              input.readMessage(subBuilder, extensionRegistry);
+              addGroupInfo(subBuilder.buildPartial());
+              break;
+            }
+          }
+        }
+      }
+      
+      private int bitField0_;
+      
+      // repeated .GroupInfo groupInfo = 1;
+      private java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> groupInfo_ =
+        java.util.Collections.emptyList();
+      private void ensureGroupInfoIsMutable() {
+        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
+          groupInfo_ = new java.util.ArrayList<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo>(groupInfo_);
+          bitField0_ |= 0x00000001;
+         }
+      }
+      
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> groupInfoBuilder_;
+      
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> getGroupInfoList() {
+        if (groupInfoBuilder_ == null) {
+          return java.util.Collections.unmodifiableList(groupInfo_);
+        } else {
+          return groupInfoBuilder_.getMessageList();
+        }
+      }
+      public int getGroupInfoCount() {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.size();
+        } else {
+          return groupInfoBuilder_.getCount();
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo getGroupInfo(int index) {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.get(index);
+        } else {
+          return groupInfoBuilder_.getMessage(index);
+        }
+      }
+      public Builder setGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.set(index, value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(index, value);
+        }
+        return this;
+      }
+      public Builder setGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.set(index, builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.setMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addGroupInfo(org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(value);
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo value) {
+        if (groupInfoBuilder_ == null) {
+          if (value == null) {
+            throw new NullPointerException();
+          }
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(index, value);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(index, value);
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addGroupInfo(
+          int index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder builderForValue) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.add(index, builderForValue.build());
+          onChanged();
+        } else {
+          groupInfoBuilder_.addMessage(index, builderForValue.build());
+        }
+        return this;
+      }
+      public Builder addAllGroupInfo(
+          java.lang.Iterable<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo> values) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          super.addAll(values, groupInfo_);
+          onChanged();
+        } else {
+          groupInfoBuilder_.addAllMessages(values);
+        }
+        return this;
+      }
+      public Builder clearGroupInfo() {
+        if (groupInfoBuilder_ == null) {
+          groupInfo_ = java.util.Collections.emptyList();
+          bitField0_ = (bitField0_ & ~0x00000001);
+          onChanged();
+        } else {
+          groupInfoBuilder_.clear();
+        }
+        return this;
+      }
+      public Builder removeGroupInfo(int index) {
+        if (groupInfoBuilder_ == null) {
+          ensureGroupInfoIsMutable();
+          groupInfo_.remove(index);
+          onChanged();
+        } else {
+          groupInfoBuilder_.remove(index);
+        }
+        return this;
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder getGroupInfoBuilder(
+          int index) {
+        return getGroupInfoFieldBuilder().getBuilder(index);
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder getGroupInfoOrBuilder(
+          int index) {
+        if (groupInfoBuilder_ == null) {
+          return groupInfo_.get(index);  } else {
+          return groupInfoBuilder_.getMessageOrBuilder(index);
+        }
+      }
+      public java.util.List<? extends org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+           getGroupInfoOrBuilderList() {
+        if (groupInfoBuilder_ != null) {
+          return groupInfoBuilder_.getMessageOrBuilderList();
+        } else {
+          return java.util.Collections.unmodifiableList(groupInfo_);
+        }
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder addGroupInfoBuilder() {
+        return getGroupInfoFieldBuilder().addBuilder(
+            org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance());
+      }
+      public org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder addGroupInfoBuilder(
+          int index) {
+        return getGroupInfoFieldBuilder().addBuilder(
+            index, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.getDefaultInstance());
+      }
+      public java.util.List<org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder> 
+           getGroupInfoBuilderList() {
+        return getGroupInfoFieldBuilder().getBuilderList();
+      }
+      private com.google.protobuf.RepeatedFieldBuilder<
+          org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder> 
+          getGroupInfoFieldBuilder() {
+        if (groupInfoBuilder_ == null) {
+          groupInfoBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder, org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoOrBuilder>(
+                  groupInfo_,
+                  ((bitField0_ & 0x00000001) == 0x00000001),
+                  getParentForChildren(),
+                  isClean());
+          groupInfo_ = null;
+        }
+        return groupInfoBuilder_;
+      }
+      
+      // @@protoc_insertion_point(builder_scope:GroupInfoList)
+    }
+    
+    static {
+      defaultInstance = new GroupInfoList(true);
+      defaultInstance.initFields();
+    }
+    
+    // @@protoc_insertion_point(class_scope:GroupInfoList)
+  }
+  
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GroupInfo_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GroupInfo_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ServerList_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ServerList_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_TableList_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_TableList_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_GroupInfoList_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_GroupInfoList_fieldAccessorTable;
+  
+  public static com.google.protobuf.Descriptors.FileDescriptor
+      getDescriptor() {
+    return descriptor;
+  }
+  private static com.google.protobuf.Descriptors.FileDescriptor
+      descriptor;
+  static {
+    java.lang.String[] descriptorData = {
+      "\n\017GroupInfo.proto\"S\n\tGroupInfo\022\014\n\004name\030\001" +
+      " \002(\t\022\034\n\007servers\030\002 \002(\0132\013.ServerList\022\032\n\006ta" +
+      "bles\030\003 \002(\0132\n.TableList\"\034\n\nServerList\022\016\n\006" +
+      "server\030\002 \003(\t\"\032\n\tTableList\022\r\n\005table\030\003 \003(\t" +
+      "\".\n\rGroupInfoList\022\035\n\tgroupInfo\030\001 \003(\0132\n.G" +
+      "roupInfoBB\n*org.apache.hadoop.hbase.prot" +
+      "obuf.generatedB\017GroupInfoProtosH\001\240\001\001"
+    };
+    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
+      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
+        public com.google.protobuf.ExtensionRegistry assignDescriptors(
+            com.google.protobuf.Descriptors.FileDescriptor root) {
+          descriptor = root;
+          internal_static_GroupInfo_descriptor =
+            getDescriptor().getMessageTypes().get(0);
+          internal_static_GroupInfo_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GroupInfo_descriptor,
+              new java.lang.String[] { "Name", "Servers", "Tables", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfo.Builder.class);
+          internal_static_ServerList_descriptor =
+            getDescriptor().getMessageTypes().get(1);
+          internal_static_ServerList_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ServerList_descriptor,
+              new java.lang.String[] { "Server", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.ServerList.Builder.class);
+          internal_static_TableList_descriptor =
+            getDescriptor().getMessageTypes().get(2);
+          internal_static_TableList_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_TableList_descriptor,
+              new java.lang.String[] { "Table", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.TableList.Builder.class);
+          internal_static_GroupInfoList_descriptor =
+            getDescriptor().getMessageTypes().get(3);
+          internal_static_GroupInfoList_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_GroupInfoList_descriptor,
+              new java.lang.String[] { "GroupInfo", },
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.class,
+              org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos.GroupInfoList.Builder.class);
+          return null;
+        }
+      };
+    com.google.protobuf.Descriptors.FileDescriptor
+      .internalBuildGeneratedFileFrom(descriptorData,
+        new com.google.protobuf.Descriptors.FileDescriptor[] {
+        }, assigner);
+  }
+  
+  // @@protoc_insertion_point(outer_class_scope)
+}
diff --git a/hbase-protocol/src/main/protobuf/GroupAdmin.proto b/hbase-protocol/src/main/protobuf/GroupAdmin.proto
new file mode 100644
index 0000000..cb55a86
--- /dev/null
+++ b/hbase-protocol/src/main/protobuf/GroupAdmin.proto
@@ -0,0 +1,151 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This file contains protocol buffers that are used for group admin service.
+
+option java_package = "org.apache.hadoop.hbase.protobuf.generated";
+option java_outer_classname = "GroupAdminProtos";
+option java_generic_services = true;
+option java_generate_equals_and_hash = true;
+option optimize_for = SPEED;
+
+import 'GroupInfo.proto';
+
+message ListTablesOfGroupRequest {
+  required string groupName = 1;
+}
+
+message ListTablesOfGroupResponse {
+  repeated string tableNames = 1;
+}
+
+message GetGroupInfoRequest {
+  required string groupName = 1;
+}
+
+message GetGroupInfoResponse {
+  optional GroupInfo groupInfo = 1;
+}
+
+message GetGroupInfoOfTableRequest {
+  required string tableName = 1;
+}
+
+message GetGroupInfoOfTableResponse {
+  optional GroupInfo groupInfo = 1;
+}
+
+message MoveServersRequest {
+  required string targetGroup = 1;
+  repeated string servers = 2;
+}
+
+message MoveServersResponse {
+}
+
+message MoveTablesRequest {
+  required string targetGroup = 1;
+  repeated string tables = 2;
+}
+
+message MoveTablesResponse {
+}
+
+message AddGroupRequest {
+ required string groupName = 1;
+}
+
+message AddGroupResponse {
+}
+
+message RemoveGroupRequest {
+ required string groupName = 1;
+}
+
+message RemoveGroupResponse {
+}
+
+message ListGroupsRequest {
+}
+
+message ListGroupsResponse {
+ repeated GroupInfo groupInfo = 1;
+}
+
+message GetGroupOfServerRequest {
+ required string hostport = 1;
+}
+
+message GetGroupOfServerResponse {
+ optional GroupInfo groupInfo = 1;
+}
+
+message ServerGroupPair {
+  required string server = 1;
+  required string groupName = 2;
+}
+
+message ListServersInTransitionRequest {
+}
+
+message ListServersInTransitionResponse {
+  repeated ServerGroupPair serversInTransition = 1;
+}
+
+message BalanceGroupRequest {
+  required string groupName = 1;
+}
+
+message BalanceGroupResponse {
+  required bool success = 1;
+}
+
+service GroupAdminService {
+    rpc addGroup(AddGroupRequest)
+      returns (AddGroupResponse);
+
+    rpc removeGroup(RemoveGroupRequest)
+      returns (RemoveGroupResponse);
+
+    rpc listGroups(ListGroupsRequest)
+      returns (ListGroupsResponse);
+
+   rpc getGroupOfServer(GetGroupOfServerRequest)
+	returns (GetGroupOfServerResponse);
+
+   rpc listServersInTransition(ListServersInTransitionRequest)
+      returns (ListServersInTransitionResponse);
+
+   rpc listTablesOfGroup(ListTablesOfGroupRequest)
+      returns (ListTablesOfGroupResponse);
+
+   rpc getGroupInfo(GetGroupInfoRequest)
+      returns (GetGroupInfoResponse);
+  
+   rpc getGroupInfoOfTable(GetGroupInfoOfTableRequest)
+	returns (GetGroupInfoOfTableResponse);
+
+   rpc moveServers(MoveServersRequest)
+	returns (MoveServersResponse);
+
+  rpc moveTables(MoveTablesRequest)
+	returns (MoveTablesResponse);
+
+  rpc balanceGroup(BalanceGroupRequest)
+	returns (BalanceGroupResponse);
+}
diff --git a/hbase-protocol/src/main/protobuf/GroupInfo.proto b/hbase-protocol/src/main/protobuf/GroupInfo.proto
new file mode 100644
index 0000000..4b98ef1
--- /dev/null
+++ b/hbase-protocol/src/main/protobuf/GroupInfo.proto
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// This file contains protocol buffers that are shared throughout HBase
+
+option java_package = "org.apache.hadoop.hbase.protobuf.generated";
+option java_outer_classname = "GroupInfoProtos";
+option java_generate_equals_and_hash = true;
+option optimize_for = SPEED;
+
+/**
+ * Content of a region server group.
+ */
+message GroupInfo {
+  required string name = 1;
+  required ServerList servers = 2;
+  required TableList tables = 3;
+}
+
+/**
+ * List of servers
+ */
+message ServerList {
+  repeated string server = 2;
+}
+
+/**
+ * List of tables
+ */
+message TableList {
+  repeated string table = 3;
+}
+
+/**
+ * List of GroupInfo
+ */
+message GroupInfoList {
+  repeated GroupInfo groupInfo = 1;
+}
diff --git a/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon b/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
index cc0343b..7e5fef2 100644
--- a/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
+++ b/hbase-server/src/main/jamon/org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.jamon
@@ -22,6 +22,7 @@ HBaseAdmin admin;
 Map<String, Integer> frags = null;
 ServerName metaLocation = null;
 List<ServerName> servers = null;
+List<GroupInfo> groups = null;
 Set<ServerName> deadServers = null;
 boolean showAppendWarning = false;
 boolean catalogJanitorEnabled = true;
@@ -42,6 +43,8 @@ org.apache.hadoop.hbase.client.HBaseAdmin;
 org.apache.hadoop.hbase.client.HConnectionManager;
 org.apache.hadoop.hbase.HTableDescriptor;
 org.apache.hadoop.hbase.HBaseConfiguration;
+org.apache.hadoop.hbase.group.GroupInfo;
+com.google.common.base.Joiner;
 org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 </%import>
 <%if format.equals("json") %>
@@ -306,6 +309,9 @@ org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
     <p><% tables.length %> table(s) in set. [<a href=tablesDetailed.jsp>Details</a>]</p>
 </table>
 </%if>
+<%if (groups != null) %>
+<& groupInfos &>
+</%if>
 </%def>
 
 <%def userSnapshots>
@@ -332,6 +338,26 @@ org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription;
 </%def>
 
 
+<%def groupInfos>
+<h2>Region Server Groups</h2>
+<%if (groups != null && groups.size() > 0)%>
+<table class="table table-striped">
+<tr><th>Group Name</th><th>Servers</th><th>Tables</th></tr>
+<%java>
+  for(GroupInfo group: groups){
+    String servers = Joiner.on(", ").join(group.getServers());
+    String tables = Joiner.on(", ").join(group.getTables());
+    String groupName = group.getName();
+</%java>    
+<tr><td><% groupName %></td><td><% servers %></td><td><% tables %></td></tr>
+<%java>
+  }
+</%java>
+<p><% groups.size() %> group(s) in set.</p>
+</table>
+</%if>
+</%def>
+
 <%def deadRegionServers>
 
 <%if (deadServers != null && deadServers.size() > 0)%>
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java
new file mode 100644
index 0000000..1069638
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdmin.java
@@ -0,0 +1,131 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * Group user API interface used between client and server.
+ */
+@InterfaceAudience.Private
+public interface GroupAdmin {
+  /**
+   * Get member tables of a group.
+   *
+   *
+   * @param groupName the name of the group
+   * @return list of table names
+   */
+  NavigableSet<String> listTablesOfGroup(String groupName) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroupInfo(String groupName) throws IOException;
+
+  /**
+   * Gets the group info of table.
+   *
+   * @param tableName the table name
+   * @return An instance of GroupInfo.
+   */
+  GroupInfo getGroupInfoOfTable(String tableName) throws IOException;
+
+  /**
+   * Move a set of serves to another group
+   *
+   *
+   * @param servers set of servers, must be in the form HOST:PORT
+   * @param targetGroup the target group
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  void moveServers(Set<String> servers, String targetGroup) throws IOException;
+
+  /**
+   * Move a table to a new group.
+   * This will unassign all of a table's region so it can be reassigned to the correct group.
+   * @param tableName the name of the table to move
+   * @param targetGroup target group
+   * @throws IOException
+   */
+  void moveTable(String tableName, String targetGroup) throws IOException;
+
+  /**
+   * Move tables to a new group.
+   * This will unassign all of a table's region so it can be reassigned to the correct group.
+   * @param tables list of tables to move
+   * @param targetGroup target group
+   * @throws IOException
+   */
+  void moveTables(Set<String> tables, String targetGroup) throws IOException;
+
+  /**
+   * Add a new group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void addGroup(String name) throws IOException;
+
+  /**
+   * Remove a group
+   * @param name name of the group
+   * @throws IOException
+   */
+  void removeGroup(String name) throws IOException;
+
+  /**
+   * Lists the existing groups.
+   *
+   * @return Collection of GroupInfo.
+   */
+  List<GroupInfo> listGroups() throws IOException;
+
+  /**
+   * Retrieve the GroupInfo a server is affiliated to
+   * @param hostPort
+   * @return
+   * @throws IOException
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * List servers that are currently being moved to a new group
+   * @return a map containing server=>targetGroup KV pairs
+   * @throws IOException
+   */
+  Map<String, String> listServersInTransition() throws IOException;
+
+  /**
+   * Balance the regions in a group
+   * @param name the name of the group to balance
+   * @return
+   * @throws IOException
+   */
+  boolean balanceGroup(String name) throws IOException;
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java
new file mode 100644
index 0000000..0431fac
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminClient.java
@@ -0,0 +1,270 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.Set;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.exceptions.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel;
+import org.apache.hadoop.hbase.ipc.ServerRpcController;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GroupAdminService;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair;
+import org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
+import com.google.protobuf.ServiceException;
+
+/**
+ * Client used for managing region server group information.
+ */
+@InterfaceAudience.Public
+public class GroupAdminClient implements GroupAdmin {
+  private HBaseAdmin admin;
+  private GroupAdminService.BlockingInterface service;
+  private static final Log LOG = LogFactory.getLog(GroupAdminClient.class);
+  private int operationTimeout;
+  private ServerRpcController controller;
+  
+  public GroupAdminClient(Configuration conf) throws IOException {
+    admin = new HBaseAdmin(conf);
+    controller = new ServerRpcController();
+    CoprocessorRpcChannel channel = admin.coprocessorService();
+    service = GroupAdminService.newBlockingStub(channel);
+    operationTimeout = conf.getInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT,
+            HConstants.DEFAULT_HBASE_CLIENT_OPERATION_TIMEOUT);
+  }
+
+  @Override
+  public NavigableSet<String> listTablesOfGroup(String groupName) throws IOException {
+    try {
+      ListTablesOfGroupRequest.Builder request = GroupAdminProtos.ListTablesOfGroupRequest
+          .newBuilder();
+      request.setGroupName(groupName);
+      ListTablesOfGroupResponse response = service.listTablesOfGroup(controller, request.build());
+      return Sets.newTreeSet(response.getTableNamesList());
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+    try {
+      GetGroupInfoRequest.Builder request = GroupAdminProtos.GetGroupInfoRequest.newBuilder();
+      request.setGroupName(groupName);
+      GetGroupInfoResponse response = service.getGroupInfo(controller, request.build());
+      if(response.hasGroupInfo()) {
+        return GroupInfo.convert(response.getGroupInfo());
+      }
+      return null;
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(String tableName) throws IOException {
+    try {
+      GetGroupInfoOfTableRequest.Builder request = GetGroupInfoOfTableRequest
+          .newBuilder();
+      request.setTableName(tableName);
+      GetGroupInfoOfTableResponse response =
+          service.getGroupInfoOfTable(controller, request.build());
+      if(response.hasGroupInfo()) {
+        return GroupInfo.convert(response.getGroupInfo());
+      }
+      return null;
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup) throws IOException {
+    try {
+      MoveServersRequest.Builder request = MoveServersRequest.newBuilder();
+      request.setTargetGroup(targetGroup);
+      for (String server : servers){
+        request.addServers(server);   
+      }
+      service.moveServers(controller, request.build());
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+    waitForTransitions(servers);
+  }
+
+  @Override
+  public void moveTable(String tableName, String targetGroup) throws IOException {
+    moveTables(Sets.newHashSet(tableName), targetGroup);
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    try {
+      MoveTablesRequest.Builder request = MoveTablesRequest.newBuilder();
+      request.setTargetGroup(targetGroup);
+      for(String table : tables){
+        request.addTables(table);
+      }
+      service.moveTables(controller, request.build());
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    try {
+      AddGroupRequest.Builder request = AddGroupRequest.newBuilder();
+      request.setGroupName(groupName);
+      service.addGroup(controller, request.build());
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+   try {
+      RemoveGroupRequest.Builder request = RemoveGroupRequest.newBuilder();
+      request.setGroupName(name);
+      service.removeGroup(controller, request.build());
+   } catch (ServiceException se) {
+     throw ProtobufUtil.getRemoteException(se);
+   }
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    try {
+      ListGroupsRequest.Builder request = ListGroupsRequest.newBuilder();
+      ListGroupsResponse response = service.listGroups(controller, request.build());
+      List<GroupInfo> groupInfos = Lists.newArrayList();
+      for(GroupInfoProtos.GroupInfo info : response.getGroupInfoList()){
+        groupInfos.add(GroupInfo.convert(info));
+      }
+      return groupInfos;
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    try {
+      GetGroupOfServerRequest.Builder request = GetGroupOfServerRequest.newBuilder();
+      request.setHostport(hostPort);
+      GetGroupOfServerResponse response = service.getGroupOfServer(controller, request.build());
+      if(response.hasGroupInfo()) {
+        return GroupInfo.convert(response.getGroupInfo());
+      }
+      return null;
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    try {
+      ListServersInTransitionRequest.Builder request = ListServersInTransitionRequest.newBuilder();
+      ListServersInTransitionResponse response = service.listServersInTransition(controller,
+        request.build());
+      Map<String,String> serversInTransition = new HashMap<String,String>();
+      for (ServerGroupPair pair : response.getServersInTransitionList()){
+        serversInTransition.put(pair.getServer(), pair.getGroupName());
+      }
+      return serversInTransition;
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+  private void waitForTransitions(Set<String> servers) throws IOException {
+    long endTime = EnvironmentEdgeManager.getDelegate().currentTimeMillis()+operationTimeout;
+    boolean found;
+    do {
+      found = false;
+      for (String server: listServersInTransition().keySet()) {
+        found = found || servers.contains(server);
+      }
+      try {
+        Thread.sleep(100);
+      } catch (InterruptedException e) {
+        LOG.debug("Sleep interrupted", e);
+        throw new DoNotRetryIOException("Sleep interrupted", e);
+      }
+    } while(found && EnvironmentEdgeManager.getDelegate().currentTimeMillis() <= endTime);
+    if (found) {
+      throw new DoNotRetryIOException("Timed out while Waiting for server transition to finish.");
+    }
+  }
+
+  public void close() throws IOException {
+    admin.close();
+  }
+
+  @Override
+  public boolean balanceGroup(String name) throws IOException {
+    try {
+      BalanceGroupRequest.Builder request = BalanceGroupRequest.newBuilder();
+      request.setGroupName(name);
+      BalanceGroupResponse response = service.balanceGroup(controller, request.build());
+      return response.getSuccess();
+    } catch (ServiceException se) {
+      throw ProtobufUtil.getRemoteException(se);
+    }
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java
new file mode 100644
index 0000000..2094042
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupAdminEndpoint.java
@@ -0,0 +1,651 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.NavigableMap;
+import java.util.NavigableSet;
+import java.util.Set;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingDeque;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.Coprocessor;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.exceptions.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.MetaScanner;
+import org.apache.hadoop.hbase.exceptions.ConstraintException;
+import org.apache.hadoop.hbase.coprocessor.CoprocessorService;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.master.RegionState;
+import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.protobuf.ResponseConverter;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.AddGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.BalanceGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoOfTableResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupInfoResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GetGroupOfServerResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.GroupAdminService;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListGroupsResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListServersInTransitionResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ListTablesOfGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveServersResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.MoveTablesResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupRequest;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.RemoveGroupResponse;
+import org.apache.hadoop.hbase.protobuf.generated.GroupAdminProtos.ServerGroupPair;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.security.access.AccessControlLists;
+import org.apache.hadoop.hbase.security.access.AccessController;
+import org.apache.hadoop.hbase.security.access.Permission;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.metrics.util.MBeanUtil;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+import com.google.protobuf.RpcCallback;
+import com.google.protobuf.RpcController;
+import com.google.protobuf.Service;
+
+/**
+ * Service to support Region Server Grouping (HBase-6721)
+ * This should be installed as a Master CoprocessorEndpoint and
+ * {@link org.apache.hadoop.hbase.HConstants.ZOOKEEPER_USEMULTI}
+ * must be enabled.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class GroupAdminEndpoint extends GroupAdminService
+    implements GroupAdmin, CoprocessorService, Coprocessor {
+  private static final Log LOG = LogFactory.getLog(GroupAdminEndpoint.class);
+
+  private final long threadKeepAliveTimeInMillis = 1000;
+  private int threadMax = 1;
+  private static AccessController accessController;
+  private BlockingQueue<Runnable> threadQ;
+  private MasterCoprocessorEnvironment menv;
+  private MasterServices master;
+  private ExecutorService executorService;
+  //List of servers that are being moved from one group to another
+  //Key=host:port,Value=targetGroup
+  ConcurrentMap<String,String> serversInTransition =
+      new ConcurrentHashMap<String,String>();
+
+  @Override
+  public void start(CoprocessorEnvironment env) {
+    menv = (MasterCoprocessorEnvironment)env;
+    master = menv.getMasterServices();
+    threadQ = new LinkedBlockingDeque<Runnable>();
+    threadMax = menv.getConfiguration().getInt("hbase.group.executor.threads", 1);
+    executorService = new ThreadPoolExecutor(threadMax, threadMax,
+        threadKeepAliveTimeInMillis, TimeUnit.MILLISECONDS, threadQ);
+    registerMBean();
+  }
+
+  @Override
+  public void stop(CoprocessorEnvironment env) {
+    executorService.shutdown();
+  }
+
+  @Override
+  public NavigableSet<String> listTablesOfGroup(String groupName) throws IOException {
+    return getGroupInfoManager().getGroup(groupName).getTables();
+  }
+
+
+  @Override
+  public GroupInfo getGroupInfo(String groupName) throws IOException {
+    return getGroupInfoManager().getGroup(groupName);
+  }
+
+
+  @Override
+  public GroupInfo getGroupInfoOfTable(String tableName) throws IOException {
+    GroupInfo groupInfo = getGroupInfoManager().getGroupOfTable(tableName);
+    //There is a case where we pre-assign membership of a non-existent table
+    //let's honor that
+    if(GroupInfo.DEFAULT_GROUP.equals(groupInfo.getName()) &&
+       master.getTableDescriptors().get(tableName) == null) {
+      throw new ConstraintException("Table "+tableName+" does not exist");
+    }
+    return groupInfo;
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup)
+      throws IOException {
+    if (servers == null) {
+      throw new DoNotRetryIOException("The list of servers cannot be null.");
+    }
+    if (StringUtils.isEmpty(targetGroup)) {
+      throw new DoNotRetryIOException("The target group cannot be null.");
+    }
+    if (servers.size() < 1) {
+      return;
+    }
+    //check that it's a valid host and port
+    for (String server: servers) {
+      String splits[] = server.split(":",2);
+      if (splits.length < 2)
+        throw new DoNotRetryIOException("Server list contains invalid <HOST>:<PORT> entry");
+      Integer.parseInt(splits[1]);
+    }
+
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      //we only allow a move from a single source group
+      //so this should be ok
+      GroupInfo srcGrp = manager.getGroupOfServer(servers.iterator().next());
+      //only move online servers (from default)
+      //or servers from other groups
+      //this prevents bogus servers from entering groups
+      if (GroupInfo.DEFAULT_GROUP.equals(srcGrp.getName())) {
+        Set<String> onlineServers = new HashSet<String>();
+        for (ServerName server: master.getServerManager().getOnlineServers().keySet()) {
+          onlineServers.add(server.getHostAndPort());
+        }
+        for (String el: servers) {
+          if (!onlineServers.contains(el)) {
+            throw new DoNotRetryIOException("Server " + el + " is not a member of any group.");
+          }
+        }
+      }
+
+      if (srcGrp.getServers().size() <= servers.size() &&
+          srcGrp.getTables().size() > 0) {
+        throw new DoNotRetryIOException("Cannot leave a group that contains tables " +
+            "without servers.");
+      }
+      GroupMoveServerWorker.MoveServerPlan plan =
+          new GroupMoveServerWorker.MoveServerPlan(servers, targetGroup);
+      GroupMoveServerWorker worker = null;
+      try {
+        worker = new GroupMoveServerWorker(master, serversInTransition,
+            getGroupInfoManager(), plan);
+        executorService.submit(worker);
+        LOG.info("GroupMoveServerWorkerSubmitted: " + plan.getTargetGroup());
+      } catch (Exception e) {
+        LOG.error("Failed to submit GroupMoveServerWorker", e);
+        if (worker != null) {
+          worker.complete();
+        }
+        throw new DoNotRetryIOException("Failed to submit GroupMoveServerWorker",e);
+      }
+    }
+  }
+
+  @Override
+  public void moveTable(String tableName, String targetGroup) throws IOException {
+    moveTables(Sets.newHashSet(tableName), targetGroup);
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    if (tables == null) {
+      throw new DoNotRetryIOException(
+          "The list of tables cannot be null.");
+    }
+    if(tables.size() < 1) {
+      LOG.debug("moveTables() passed an empty set. Ignoring.");
+      return;
+    }
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      GroupInfo destGroup = manager.getGroup(targetGroup);
+      if(destGroup == null) {
+        throw new ConstraintException("Target group does not exist: "+targetGroup);
+      }
+      for(String table : tables) {
+        GroupInfo srcGroup = manager.getGroupOfTable(table);
+        if(srcGroup.getName().equals(targetGroup)) {
+          throw new ConstraintException("Source group is the same as target group for table "+table+" :"+srcGroup);
+        }
+      }
+      if(destGroup.getServers().size() < 1) {
+        throw new ConstraintException("Target group must have at least one server.");
+      }
+      manager.moveTables(tables, targetGroup);
+    }
+    for (String table : tables) {
+      NavigableMap<HRegionInfo, ServerName> regionMap = MetaScanner.allTableRegions(master
+          .getConfiguration(), Bytes.toBytes(table), false);
+      //don't unassign regions if they're already there
+      List<HRegionInfo> unassignList = new LinkedList<HRegionInfo>();
+      for(Map.Entry<HRegionInfo, ServerName> entry: regionMap.entrySet()) {
+        if(entry.getValue() != null &&
+           !manager.getGroupOfServer(entry.getValue().getHostAndPort()).getName()
+            .equals(targetGroup)) {
+          unassignList.add(entry.getKey());
+        }
+      }
+      LOG.debug("Unassigning "+unassignList.size()+
+          " regions for reassignment to "+targetGroup);
+      master.getAssignmentManager().unassign(unassignList);
+    }
+  }
+
+  @Override
+  public void addGroup(String name) throws IOException {
+    getGroupInfoManager().addGroup(new GroupInfo(name));
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    GroupInfoManager manager = getGroupInfoManager();
+    synchronized (manager) {
+      GroupInfo groupInfo = getGroupInfoManager().getGroup(name);
+      if(groupInfo == null) {
+        throw new DoNotRetryIOException("Group "+name+" does not exist");
+      }
+      int tableCount = groupInfo.getTables().size();
+      if (tableCount > 0) {
+        throw new DoNotRetryIOException("Group "+name+" must have no associated tables: "+tableCount);
+      }
+      int serverCount = groupInfo.getServers().size();
+      if(serverCount > 0) {
+        throw new DoNotRetryIOException("Group "+name+" must have no associated servers: "+serverCount);
+      }
+      manager.removeGroup(name);
+    }
+  }
+
+  @Override
+  public List<GroupInfo> listGroups() throws IOException {
+    return getGroupInfoManager().listGroups();
+  }
+
+  @Override
+  public GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    return getGroupInfoManager().getGroupOfServer(hostPort);
+  }
+
+  @Override
+  public Map<String, String> listServersInTransition() throws IOException {
+    return Collections.unmodifiableMap(serversInTransition);
+  }
+
+  @InterfaceAudience.Private
+  public GroupInfoManager getGroupInfoManager() throws IOException {
+    return ((GroupBasedLoadBalancer)menv.getMasterServices().getLoadBalancer()).getGroupInfoManager();
+  }
+
+  void registerMBean() {
+    org.apache.hadoop.hbase.group.MXBeanImpl mxBeanInfo =
+        org.apache.hadoop.hbase.group.MXBeanImpl.init(this, master);
+    MBeanUtil.registerMBean("Group", "Group", mxBeanInfo);
+    LOG.info("Registered Group MXBean");
+  }
+
+  @Override
+  public Service getService() {
+    return this;
+  }
+
+  @Override
+  public void addGroup(RpcController controller, AddGroupRequest request,
+                       RpcCallback<AddGroupResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("addGroup");
+      }
+      this.addGroup(request.getGroupName());
+      done.run(AddGroupResponse.newBuilder().build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void removeGroup(RpcController controller, RemoveGroupRequest request,
+                          RpcCallback<RemoveGroupResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("removeGroup");
+      }
+      this.removeGroup(request.getGroupName());
+      done.run(RemoveGroupResponse.newBuilder().build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void listGroups(RpcController controller, ListGroupsRequest request,
+                         RpcCallback<ListGroupsResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("listGroups");
+      }
+      List<GroupInfo> groups = this.listGroups();
+      ListGroupsResponse.Builder resp = ListGroupsResponse.newBuilder();
+      for(GroupInfo info : groups){
+        resp.addGroupInfo(info.convert());
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void getGroupOfServer(RpcController controller, GetGroupOfServerRequest request,
+                               RpcCallback<GetGroupOfServerResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("getGroupOfServer");
+      }
+      GroupInfo info = this.getGroupOfServer(request.getHostport());
+      GetGroupOfServerResponse.Builder resp = GetGroupOfServerResponse.newBuilder();
+      if(info != null) {
+        resp.setGroupInfo(info.convert());
+      } else {
+        resp.clearGroupInfo();
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void listServersInTransition(RpcController controller,
+                                      ListServersInTransitionRequest request,
+                                      RpcCallback<ListServersInTransitionResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("listServersInTransition");
+      }
+      Map<String,String> servers = this.listServersInTransition();
+      ListServersInTransitionResponse.Builder resp = ListServersInTransitionResponse.newBuilder();
+      for(Entry<String, String> entry : servers.entrySet()){
+        ServerGroupPair pair = ServerGroupPair.newBuilder().setServer(entry.getKey())
+            .setGroupName(entry.getValue()).build();
+        resp.addServersInTransition(pair);
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void listTablesOfGroup(RpcController controller, ListTablesOfGroupRequest request,
+                                RpcCallback<ListTablesOfGroupResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("listTablesOfGroup");
+      }
+      NavigableSet<String> tables = this.listTablesOfGroup(request.getGroupName());
+      ListTablesOfGroupResponse.Builder resp = ListTablesOfGroupResponse.newBuilder();
+      int i = 0;
+      for (String table : tables) {
+        resp.setTableNames(i, table);
+        i++;
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void getGroupInfo(RpcController controller, GetGroupInfoRequest request,
+                           RpcCallback<GetGroupInfoResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("getGroupInfo");
+      }
+      GroupInfo info = this.getGroupInfo(request.getGroupName());
+      GetGroupInfoResponse.Builder resp = GetGroupInfoResponse.newBuilder();
+      if(info != null) {
+        resp.setGroupInfo(info.convert());
+      } else {
+        resp.clearGroupInfo();
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void getGroupInfoOfTable(RpcController controller, GetGroupInfoOfTableRequest request,
+                                  RpcCallback<GetGroupInfoOfTableResponse> done) {
+    try {
+      GroupInfo info = this.getGroupInfoOfTable(request.getTableName());
+      GetGroupInfoOfTableResponse.Builder resp = GetGroupInfoOfTableResponse.newBuilder();
+      if(info != null) {
+        resp.setGroupInfo(info.convert());
+      } else {
+        resp.clearGroupInfo();
+      }
+      done.run(resp.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void moveServers(RpcController controller, MoveServersRequest request,
+                          RpcCallback<MoveServersResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("moveServers");
+      }
+      this.moveServers(Sets.newHashSet(request.getServersList()), request.getTargetGroup());
+      done.run(MoveServersResponse.newBuilder().build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public void moveTables(RpcController controller, MoveTablesRequest request,
+                         RpcCallback<MoveTablesResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("moveTables");
+      }
+      this.moveTables(Sets.newHashSet(request.getTablesList()), request.getTargetGroup());
+      done.run(MoveTablesResponse.newBuilder().build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  @Override
+  public boolean balanceGroup(String groupName) throws IOException {
+    ServerManager serverManager = master.getServerManager();
+    AssignmentManager assignmentManager = master.getAssignmentManager();
+    LoadBalancer balancer = master.getLoadBalancer();
+
+    boolean balancerRan;
+    synchronized (balancer) {
+      // Only allow one balance run at at time.
+      Map<String, RegionState> groupRIT = groupGetRegionsInTransition(groupName);
+      if (groupRIT.size() > 0) {
+        LOG.debug("Not running balancer because "
+            + groupRIT.size()
+            + " region(s) in transition: "
+            + org.apache.commons.lang.StringUtils.abbreviate(master.getAssignmentManager()
+            .getRegionStates().getRegionsInTransition().toString(), 256));
+        return false;
+      }
+      if (serverManager.areDeadServersInProgress()) {
+        LOG.debug("Not running balancer because processing dead regionserver(s): "
+            + serverManager.getDeadServers());
+        return false;
+      }
+
+      // We balance per group instead of per table
+      List<RegionPlan> plans = new ArrayList<RegionPlan>();
+      for (Entry<String, Map<ServerName, List<HRegionInfo>>> tableMap :
+          getGroupAssignmentsByTable(groupName).entrySet()) {
+        LOG.info("Creating partial plan for table " + tableMap.getKey() + ": "
+            + tableMap.getValue());
+        List<RegionPlan> partialPlans = balancer.balanceCluster(tableMap.getValue());
+        LOG.info("Partial plan for table " + tableMap.getKey() + ": " + partialPlans);
+        if (partialPlans != null) {
+          plans.addAll(partialPlans);
+        }
+      }
+      long startTime = System.currentTimeMillis();
+      balancerRan = plans != null;
+      if (plans != null && !plans.isEmpty()) {
+        LOG.info("Group balance " + groupName + " starting with plan count: " + plans.size());
+        for (RegionPlan plan : plans) {
+          LOG.info("balance " + plan);
+          assignmentManager.balance(plan);
+        }
+        LOG.info("Group balance " + groupName + " completed after "
+            + (System.currentTimeMillis() - startTime) + " seconds");
+      }
+    }
+    return balancerRan;
+  }
+
+  private Map<String, RegionState> groupGetRegionsInTransition(String groupName) throws IOException {
+    Map<String, RegionState> rit = Maps.newTreeMap();
+    AssignmentManager am = master.getAssignmentManager();
+    GroupInfo groupInfo = getGroupInfo(groupName);
+    for (String tableName : groupInfo.getTables()) {
+      for (HRegionInfo regionInfo : am.getRegionStates()
+          .getRegionsOfTable(Bytes.toBytes(tableName))) {
+        RegionState state = master.getAssignmentManager().getRegionStates()
+            .getRegionTransitionState(regionInfo);
+        if (state != null) {
+          rit.put(regionInfo.getEncodedName(), state);
+        }
+      }
+    }
+    return rit;
+  }
+
+  private Map<String, Map<ServerName, List<HRegionInfo>>> getGroupAssignmentsByTable(String groupName) throws
+      IOException {
+    Map<String, Map<ServerName, List<HRegionInfo>>> result = Maps.newHashMap();
+    GroupInfo groupInfo = getGroupInfo(groupName);
+
+    Map<ServerName, List<HRegionInfo>> serverMap = Maps.newHashMap();
+    for(ServerName serverName: master.getServerManager().getOnlineServers().keySet()) {
+      if(groupInfo.getServers().contains(serverName.getHostAndPort())) {
+        serverMap.put(serverName, Collections.EMPTY_LIST);
+      }
+    }
+
+    //add all tables that are members of the group
+    for(String tableName : groupInfo.getTables()) {
+      result.put(tableName, new HashMap<ServerName, List<HRegionInfo>>());
+      result.get(tableName).putAll(serverMap);
+      for(Map.Entry<HRegionInfo, ServerName> entry :
+        master.getAssignmentManager().getRegionStates()
+            .getRegionAssignmentsByTable(Bytes.toBytes(tableName))
+            .entrySet()) {
+        List<HRegionInfo> regions =
+            result.get(tableName).get(entry.getValue());
+        if(regions == null || regions == Collections.EMPTY_LIST) {
+          regions = new LinkedList<HRegionInfo>();
+          result.get(tableName).put(entry.getValue(), regions);
+        }
+        regions.add(entry.getKey());
+      }
+      LOG.debug("Added assignments for "+tableName+": "+result.get(tableName));
+    }
+    return result;
+  }
+
+  @Override
+  public void balanceGroup(RpcController controller, BalanceGroupRequest request,
+                           RpcCallback<BalanceGroupResponse> done) {
+    try {
+      if (isSecurityEnabled()) {
+        requireAdmin("balanceGroup");
+      }
+      boolean success = this.balanceGroup(request.getGroupName());
+      BalanceGroupResponse.Builder response = BalanceGroupResponse.newBuilder();
+      response.setSuccess(success);
+      done.run(response.build());
+    } catch (IOException e) {
+      ResponseConverter.setControllerException(controller, e);
+    }
+    done.run(null);
+  }
+
+  private AccessController getAccessController() {
+    if(accessController == null) {
+      accessController = (AccessController)menv.getMasterServices()
+          .getCoprocessorHost().findCoprocessor(AccessController.class.getName());
+    }
+    return accessController;
+  }
+
+  private void requireAdmin(String method) throws IOException {
+    getAccessController().requirePermission(method, AccessControlLists.ACL_TABLE_NAME, null, null,
+        Permission.Action.ADMIN);
+  }
+
+  private boolean isSecurityEnabled(){
+    return (User.isSecurityEnabled() && User.isHBaseSecurityEnabled(this.menv.getConfiguration()));
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java
new file mode 100644
index 0000000..c1b7bdb
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupBasedLoadBalancer.java
@@ -0,0 +1,433 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import com.google.common.collect.LinkedListMultimap;
+import com.google.common.collect.ListMultimap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.ServerName;
+
+import com.google.common.collect.ArrayListMultimap;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.master.balancer.DefaultLoadBalancer;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * GroupBasedLoadBalancer, used when Region Server Grouping is configured (HBase-6721)
+ * It does region balance based on a table's group membership.
+ *
+ * Most assignment methods contain two exclusive code paths: Online - when the group
+ * table is online and Offline - when it is unavailable.
+ *
+ * During Offline, assignments are made randomly irrespective of group memebership.
+ * Though during this mode, only the tables contained in SPECIAL_TABLES
+ * are given assignments to actual online servers.
+ * Once the GROUP table has been assigned, the balancer switches to Online and will then
+ * start providing appropriate assignments for user tables.
+ *
+ * An optmization has been added to cache the group information for SPECIAL_TABLES in zookeeper,
+ * thus random assignments will only occur during first time a cluster is started.
+ *
+ */
+@InterfaceAudience.Public
+public class GroupBasedLoadBalancer implements LoadBalancer {
+  /** Config for pluggable load balancers */
+  public static final String HBASE_GROUP_LOADBALANCER_CLASS = "hbase.group.grouploadbalancer.class";
+
+  private static final Log LOG = LogFactory.getLog(GroupBasedLoadBalancer.class);
+  private static final ServerName BOGUS_SERVER_NAME = ServerName.parseServerName("127.0.0.1:1");
+
+  public static final Set<String> SPECIAL_TABLES = new HashSet<String>();
+  static {
+    //security table
+    SPECIAL_TABLES.add("_acl_");
+    SPECIAL_TABLES.add(Bytes.toString(HConstants.ROOT_TABLE_NAME));
+    SPECIAL_TABLES.add(Bytes.toString(HConstants.META_TABLE_NAME));
+    SPECIAL_TABLES.add(GroupInfoManager.GROUP_TABLE_NAME);
+  }
+
+  private Configuration config;
+  private ClusterStatus clusterStatus;
+  private MasterServices masterServices;
+  private GroupInfoManager groupManager;
+  private LoadBalancer internalBalancer;
+
+  //used during reflection by LoadBalancerFactory
+  @InterfaceAudience.Private
+  public GroupBasedLoadBalancer() {
+  }
+
+  //This constructor should only be used for unit testing
+  @InterfaceAudience.Private
+  public GroupBasedLoadBalancer(GroupInfoManager groupManager) {
+    this.groupManager = groupManager;
+  }
+
+  @Override
+  public Configuration getConf() {
+    return config;
+  }
+
+  @Override
+  public void setConf(Configuration conf) {
+    this.config = conf;
+  }
+
+  @Override
+  public void setClusterStatus(ClusterStatus st) {
+    this.clusterStatus = st;
+  }
+
+  @Override
+  public void setMasterServices(MasterServices masterServices) {
+    this.masterServices = masterServices;
+  }
+
+  @Override
+  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState) throws IOException {
+
+    if (!isOnline()) {
+      throw new IllegalStateException(GroupInfoManager.GROUP_TABLE_NAME+
+          " is not online, unable to perform balance");
+    }
+
+    Map<ServerName,List<HRegionInfo>> correctedState = correctAssignments(clusterState);
+    List<RegionPlan> regionPlans = new ArrayList<RegionPlan>();
+    try {
+      for (GroupInfo info : groupManager.listGroups()) {
+        Map<ServerName, List<HRegionInfo>> groupClusterState = new HashMap<ServerName, List<HRegionInfo>>();
+        for (String sName : info.getServers()) {
+          ServerName actual = ServerName.findServerWithSameHostnamePort(
+              clusterState.keySet(), ServerName.parseServerName(sName));
+          if (actual != null) {
+            groupClusterState.put(actual, correctedState.get(actual));
+          }
+        }
+        List<RegionPlan> groupPlans = this.internalBalancer
+            .balanceCluster(groupClusterState);
+        if (groupPlans != null) {
+          regionPlans.addAll(groupPlans);
+        }
+      }
+    } catch (IOException exp) {
+      LOG.warn("Exception while balancing cluster.", exp);
+      regionPlans.clear();
+    }
+    return regionPlans;
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment (
+      List<HRegionInfo> regions, List<ServerName> servers) throws IOException {
+    Map<ServerName, List<HRegionInfo>> assignments = Maps.newHashMap();
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(regions, servers, regionMap, serverMap);
+    for(String groupKey : regionMap.keySet()) {
+      if (regionMap.get(groupKey).size() > 0) {
+        Map<ServerName, List<HRegionInfo>> result =
+            this.internalBalancer.roundRobinAssignment(
+                regionMap.get(groupKey),
+                serverMap.get(groupKey));
+        if(result != null) {
+          assignments.putAll(result);
+        }
+      }
+    }
+    return assignments;
+  }
+
+  @Override
+  public Map<ServerName, List<HRegionInfo>> retainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException{
+    if (!isOnline()) {
+      return offlineRetainAssignment(regions, servers);
+    }
+    return onlineRetainAssignment(regions, servers);
+  }
+
+  public Map<ServerName, List<HRegionInfo>> offlineRetainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException {
+      //We will just keep assignments even if they are incorrect.
+      //Chances are most will be assigned correctly.
+      //Then we just use balance to correct the misplaced few.
+      //we need to correct catalog and group table assignment anyway.
+      return internalBalancer.retainAssignment(regions, servers);
+  }
+
+  public Map<ServerName, List<HRegionInfo>> onlineRetainAssignment(
+      Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws IOException {
+    Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
+    ListMultimap<String, HRegionInfo> groupToRegion = ArrayListMultimap.create();
+    List<HRegionInfo> misplacedRegions = getMisplacedRegions(regions);
+    for (HRegionInfo region : regions.keySet()) {
+      if (!misplacedRegions.contains(region)) {
+        GroupInfo groupInfo = groupManager.getGroupOfTable(region.getTableNameAsString());
+        groupToRegion.put(groupInfo.getName(), region);
+      }
+    }
+    // Now the "groupToRegion" map has only the regions which have correct
+    // assignments.
+    for (String key : groupToRegion.keys()) {
+      Map<HRegionInfo, ServerName> currentAssignmentMap = new TreeMap<HRegionInfo, ServerName>();
+      List<HRegionInfo> regionList = groupToRegion.get(key);
+      GroupInfo info = groupManager.getGroup(key);
+      List<ServerName> candidateList = filterOfflineServers(info, servers);
+      for (HRegionInfo region : regionList) {
+        currentAssignmentMap.put(region, regions.get(region));
+      }
+      assignments.putAll(this.internalBalancer.retainAssignment(
+          currentAssignmentMap, candidateList));
+    }
+
+    for (HRegionInfo region : misplacedRegions) {
+      GroupInfo info = groupManager.getGroupOfTable(region.getTableNameAsString());
+      List<ServerName> candidateList = filterOfflineServers(info, servers);
+      ServerName server = this.internalBalancer.randomAssignment(region,
+          candidateList);
+      if (server != null) {
+        if(!assignments.containsKey(server)) {
+          assignments.put(server, new ArrayList<HRegionInfo>());
+        }
+        assignments.get(server).add(region);
+      } else {
+        LOG.warn("No random server found for group "+info.getName()+
+            " from candidate list: "+candidateList);
+        //if not server is available assign to bogus so it ends up in RIT
+        if(!assignments.containsKey(BOGUS_SERVER_NAME)) {
+          assignments.put(BOGUS_SERVER_NAME, new ArrayList<HRegionInfo>());
+        }
+        assignments.get(BOGUS_SERVER_NAME).add(region);
+      }
+    }
+    return assignments;
+  }
+
+  @Override
+  public Map<HRegionInfo, ServerName> immediateAssignment(
+      List<HRegionInfo> regions, List<ServerName> servers) throws IOException {
+    Map<HRegionInfo,ServerName> assignments = Maps.newHashMap();
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(regions, servers, regionMap, serverMap);
+    for(String groupKey : regionMap.keySet()) {
+      if (regionMap.get(groupKey).size() > 0) {
+        assignments.putAll(
+            this.internalBalancer.immediateAssignment(
+                regionMap.get(groupKey),
+                serverMap.get(groupKey)));
+      }
+    }
+    return assignments;
+  }
+
+  @Override
+  public ServerName randomAssignment(HRegionInfo region,
+      List<ServerName> servers) throws IOException {
+    ListMultimap<String,HRegionInfo> regionMap = LinkedListMultimap.create();
+    ListMultimap<String,ServerName> serverMap = LinkedListMultimap.create();
+    generateGroupMaps(Lists.newArrayList(region), servers, regionMap, serverMap);
+    List<ServerName> filteredServers = serverMap.get(regionMap.keySet().iterator().next());
+    return this.internalBalancer.randomAssignment(region, filteredServers);
+  }
+
+  private void generateGroupMaps(
+    List<HRegionInfo> regions,
+    List<ServerName> servers,
+    ListMultimap<String, HRegionInfo> regionMap,
+    ListMultimap<String, ServerName> serverMap) throws IOException {
+    if (isOnline()) {
+      for (HRegionInfo region : regions) {
+        GroupInfo groupInfo = groupManager.getGroupOfTable(region.getTableNameAsString());
+        regionMap.put(groupInfo.getName(), region);
+      }
+      for (String groupKey : regionMap.keySet()) {
+        GroupInfo info = groupManager.getGroup(groupKey);
+        serverMap.putAll(groupKey, filterOfflineServers(info, servers));
+      }
+    } else {
+      String nullGroup = "_null";
+      //populate serverMap
+      for(GroupInfo groupInfo: groupManager.listGroups()) {
+        serverMap.putAll(groupInfo.getName(), filterOfflineServers(groupInfo, servers));
+      }
+      //add DEFAULT group to capture new special tables
+      serverMap.putAll(GroupInfo.DEFAULT_GROUP, serverMap.get(GroupInfo.OFFLINE_DEFAULT_GROUP));
+      //Add bogus server, for groups that don't have special tables
+      //we assign them a bogus server to defer real assignment during online mode
+      serverMap.put(nullGroup, BOGUS_SERVER_NAME);
+      //group regions
+      for (HRegionInfo region : regions) {
+        //Even though some of the non-special tables may be part of the cached groups.
+        //We don't assign them here.
+        if(SPECIAL_TABLES.contains(region.getTableNameAsString())) {
+          regionMap.put(groupManager.getGroupOfTable(region.getTableNameAsString()).getName(),
+              region);
+        } else {
+          regionMap.put(nullGroup,region);
+        }
+      }
+    }
+  }
+
+  private List<ServerName> filterOfflineServers(GroupInfo groupInfo,
+                                                List<ServerName> onlineServers) {
+    if (groupInfo != null) {
+      return filterServers(groupInfo.getServers(), onlineServers);
+    } else {
+      LOG.debug("Group Information found to be null. Some regions might be unassigned.");
+      return Collections.EMPTY_LIST;
+    }
+  }
+
+  /**
+   * Filter servers based on the online servers.
+   *
+   * @param servers
+   *          the servers
+   * @param onlineServers
+   *          List of servers which are online.
+   * @return the list
+   */
+  private List<ServerName> filterServers(Collection<String> servers,
+      Collection<ServerName> onlineServers) {
+    ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+    for (String server : servers) {
+      ServerName actual = ServerName.findServerWithSameHostnamePort(
+          onlineServers, ServerName.parseServerName(server));
+      if (actual != null) {
+        finalList.add(actual);
+      }
+    }
+    return finalList;
+  }
+
+  private ListMultimap<String, HRegionInfo> groupRegions(
+      List<HRegionInfo> regionList) throws IOException {
+    ListMultimap<String, HRegionInfo> regionGroup = ArrayListMultimap
+        .create();
+    for (HRegionInfo region : regionList) {
+      regionGroup.put(groupManager.getGroupOfTable(region.getTableNameAsString()).getName(),
+          region);
+    }
+    return regionGroup;
+  }
+
+  private List<HRegionInfo> getMisplacedRegions(
+      Map<HRegionInfo, ServerName> regions) throws IOException {
+    List<HRegionInfo> misplacedRegions = new ArrayList<HRegionInfo>();
+    for (HRegionInfo region : regions.keySet()) {
+      ServerName assignedServer = regions.get(region);
+      GroupInfo info = groupManager.getGroupOfTable(region.getTableNameAsString());
+      if (assignedServer != null &&
+          (info == null || !info.containsServer(assignedServer.getHostAndPort()))) {
+        LOG.warn("Found misplaced region: "+region.getRegionNameAsString()+
+            " on server: "+assignedServer+
+            " found in group: "+groupManager.getGroupOfServer(assignedServer.getHostAndPort())+
+            " outside of group: "+info.getName());
+        misplacedRegions.add(region);
+      }
+    }
+    return misplacedRegions;
+  }
+
+  private Map<ServerName, List<HRegionInfo>> correctAssignments(
+       Map<ServerName, List<HRegionInfo>> existingAssignments){
+    Map<ServerName, List<HRegionInfo>> correctAssignments =
+        new TreeMap<ServerName, List<HRegionInfo>>();
+    List<HRegionInfo> misplacedRegions = new LinkedList<HRegionInfo>();
+    for (ServerName sName : existingAssignments.keySet()) {
+      correctAssignments.put(sName, new LinkedList<HRegionInfo>());
+      List<HRegionInfo> regions = existingAssignments.get(sName);
+      for (HRegionInfo region : regions) {
+        GroupInfo info = null;
+        try {
+          info = groupManager.getGroupOfTable(region.getTableNameAsString());
+        }catch(IOException exp){
+          LOG.debug("Group information null for region of table " + region.getTableNameAsString(),
+              exp);
+        }
+        if ((info == null) || (!info.containsServer(sName.getHostAndPort()))) {
+          // Misplaced region.
+          misplacedRegions.add(region);
+        } else {
+          correctAssignments.get(sName).add(region);
+        }
+      }
+    }
+
+    //unassign misplaced regions, so that they are assigned to correct groups.
+    this.masterServices.getAssignmentManager().unassign(misplacedRegions);
+    return correctAssignments;
+  }
+
+  @Override
+  public void configure() throws IOException {
+    if(!config.getBoolean(HConstants.ZOOKEEPER_USEMULTI, false)) {
+      LOG.error("hbase.zookeeper.useMulti must be enabled.");
+      throw new IllegalStateException("hbase.zookeeper.useMulti must be enabled.");
+    }
+    // Create the balancer
+    Class<? extends LoadBalancer> balancerKlass = config.getClass(
+        HBASE_GROUP_LOADBALANCER_CLASS,
+        DefaultLoadBalancer.class, LoadBalancer.class);
+    internalBalancer = ReflectionUtils.newInstance(balancerKlass, config);
+    internalBalancer.setClusterStatus(clusterStatus);
+    internalBalancer.setMasterServices(masterServices);
+    internalBalancer.setConf(config);
+    internalBalancer.configure();
+    if (groupManager == null) {
+      groupManager = new GroupInfoManagerImpl(masterServices);
+    }
+  }
+
+  public boolean isOnline() {
+    return groupManager != null && groupManager.isOnline();
+  }
+
+  @InterfaceAudience.Private
+  public GroupInfoManager getGroupInfoManager() throws IOException {
+    return groupManager;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java
new file mode 100644
index 0000000..b49f554
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfo.java
@@ -0,0 +1,194 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.util.Collection;
+import java.util.NavigableSet;
+
+import org.apache.hadoop.hbase.exceptions.DeserializationException;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos;
+import org.codehaus.jackson.annotate.JsonCreator;
+import org.codehaus.jackson.annotate.JsonProperty;
+
+import com.google.common.base.Objects;
+import com.google.common.collect.Sets;
+import com.google.protobuf.InvalidProtocolBufferException;
+
+/**
+ * Stores the group information of region server groups.
+ */
+public class GroupInfo {
+  public static final String DEFAULT_GROUP = "default";
+  public static final String TABLEDESC_PROP_GROUP = "hbase.rsgroup.name";
+  public static final String OFFLINE_DEFAULT_GROUP = "_offline_default";
+  public static final String TRANSITION_GROUP_PREFIX = "_transition_";
+
+  private String name;
+  private NavigableSet<String> servers;
+  private NavigableSet<String> tables;
+
+  public GroupInfo(String name) {
+    this(name, Sets.<String>newTreeSet(), Sets.<String>newTreeSet());
+  }
+
+  GroupInfo(String name,
+            NavigableSet<String> servers,
+            NavigableSet<String> tables) {
+    this.name = name;
+    this.servers = servers;
+    this.tables = tables;
+  }
+
+  public GroupInfo(GroupInfo src) {
+    name = src.getName();
+    servers = Sets.newTreeSet(src.getServers());
+    tables = Sets.newTreeSet(src.getTables());
+  }
+
+  /**
+   * Get group name.
+   *
+   * @return
+   */
+  public String getName() {
+    return name;
+  }
+
+  /**
+   * Adds the server to the group.
+   *
+   * @param hostPort the server
+   */
+  public void addServer(String hostPort){
+    servers.add(hostPort);
+  }
+
+  /**
+   * Adds a group of servers.
+   *
+   * @param hostPort the servers
+   */
+  public void addAllServers(Collection<String> hostPort){
+    servers.addAll(hostPort);
+  }
+
+  /**
+   * @param hostPort
+   * @return true, if a server with hostPort is found
+   */
+  public boolean containsServer(String hostPort) {
+    return servers.contains(hostPort);
+  }
+
+  /**
+   * Get list of servers.
+   *
+   * @return
+   */
+  public NavigableSet<String> getServers() {
+    return servers;
+  }
+
+  /**
+   * Remove a server from this group.
+   *
+   * @param hostPort
+   */
+  public boolean removeServer(String hostPort) {
+    return servers.remove(hostPort);
+  }
+
+  /**
+   * Set of tables that are members of this group
+   * @return
+   */
+  public NavigableSet<String> getTables() {
+    return tables;
+  }
+
+  public void addTable(String table) {
+    tables.add(table);
+  }
+
+  public void addAllTables(Collection<String> arg) {
+    tables.addAll(arg);
+  }
+
+  public boolean containsTable(String table) {
+    return tables.contains(table);
+  }
+
+  public boolean removeTable(String table) {
+    return tables.remove(table);
+  }
+
+  @Override
+  public String toString() {   
+    return Objects.toStringHelper(this).add("Name", this.name).add("Servers", this.servers)
+        .add("tables", this.tables).toString();
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    GroupInfo groupInfo = (GroupInfo) o;
+
+    if (!name.equals(groupInfo.name)) return false;
+    if (!servers.equals(groupInfo.servers)) return false;
+    if (!tables.equals(groupInfo.tables)) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result = servers.hashCode();
+    result = 31 * result + tables.hashCode();
+    result = 31 * result + name.hashCode();
+    return result;
+  }
+
+  GroupInfoProtos.GroupInfo convert() {
+    GroupInfoProtos.GroupInfo.Builder builder = GroupInfoProtos.GroupInfo.newBuilder();
+    builder.setName(name);
+    builder.setServers(GroupInfoProtos.ServerList.newBuilder().addAllServer(servers));
+    builder.setTables(GroupInfoProtos.TableList.newBuilder().addAllTable(tables));
+    return builder.build();
+  }
+
+  public byte [] toByteArray() {
+    return ProtobufUtil.prependPBMagic(convert().toByteArray());
+  }
+
+  static GroupInfo convert(final GroupInfoProtos.GroupInfo gInfo) {
+    GroupInfo info = new GroupInfo(gInfo.getName());
+    info.getServers().addAll(gInfo.getServers().getServerList());
+    info.getTables().addAll(gInfo.getTables().getTableList());
+    return info;
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java
new file mode 100644
index 0000000..d30d903
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManager.java
@@ -0,0 +1,126 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Set;
+
+/**
+ * Interface used to manage GroupInfo storage. An implementation
+ * has the option to support offline mode.
+ * See {@link GroupBasedLoadBalancer}
+ */
+public interface GroupInfoManager {
+  //Assigned before user tables
+  public static final String GROUP_TABLE_NAME = "0group0";
+  public static final byte[] GROUP_TABLE_NAME_BYTES = Bytes.toBytes(GROUP_TABLE_NAME);
+  public static final byte[] SERVER_FAMILY_BYTES = Bytes.toBytes("servers");
+  public static final byte[] TABLE_FAMILY_BYTES = Bytes.toBytes("tables");
+  public static final byte[] ROW_KEY = {0};
+  public static final String groupZNode = "groupInfo";
+
+
+  /**
+   * Adds the group.
+   *
+   * @param groupInfo the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void addGroup(GroupInfo groupInfo) throws IOException;
+
+  /**
+   * Remove a region server group.
+   *
+   * @param groupName the group name
+   * @throws java.io.IOException Signals that an I/O exception has occurred.
+   */
+  void removeGroup(String groupName) throws IOException;
+
+  /**
+   * move servers to a new group.
+   * @param hostPorts list of servers, must be part of the same group
+   * @param srcGroup
+   * @param dstGroup
+   * @return true if move was successful
+   * @throws IOException
+   */
+  boolean moveServers(Set<String> hostPorts, String srcGroup, String dstGroup) throws IOException;
+
+  /**
+   * Gets the group info of server.
+   *
+   * @param hostPort the server
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroupOfServer(String hostPort) throws IOException;
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  GroupInfo getGroup(String groupName) throws IOException;
+
+  /**
+   * Get the group membership of a table
+   * @param tableName
+   * @return Group name of table
+   * @throws IOException
+   */
+	GroupInfo getGroupOfTable(String tableName) throws IOException;
+
+  /**
+   * Set the group membership of a set of tables
+   *
+   * @param tableNames
+   * @param groupName
+   * @throws IOException
+   */
+  void moveTables(Set<String> tableNames, String groupName) throws IOException;
+
+  /**
+   * List the groups
+   *
+   * @return list of GroupInfo
+   * @throws IOException
+   */
+  List<GroupInfo> listGroups() throws IOException;
+
+  /**
+   * Refresh/reload the group information from
+   * the persistent store
+   *
+   * @throws IOException
+   */
+  void refresh() throws IOException;
+
+  /**
+   * Whether the manager is able to fully
+   * return group metadata
+   *
+   * @return
+   */
+  boolean isOnline();
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java
new file mode 100644
index 0000000..2faaf58
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupInfoManagerImpl.java
@@ -0,0 +1,587 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.exceptions.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.ClientProtocol;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.HConnection;
+import org.apache.hadoop.hbase.client.HConnectionManager;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.MetaScanner;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.RowMutations;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.handler.CreateTableHandler;
+import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
+import org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.zookeeper.ZKTable;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+
+/**
+ * This is an implementation of {@link GroupInfoManager}. Which makes
+ * use of an HBase table as the persistence store for the group information.
+ * It also makes use of zookeeper to store group information needed
+ * for bootstrapping during offline mode.
+ */
+public class GroupInfoManagerImpl implements GroupInfoManager {
+  private static final Log LOG = LogFactory.getLog(GroupInfoManagerImpl.class);
+
+  //Access to this map should always be synchronized.
+  private Map<String, GroupInfo> groupMap;
+  private Map<String, String> tableMap;
+  private MasterServices master;
+  private HTable groupTable;
+  private ZooKeeperWatcher watcher;
+  private GroupStartupWorker groupStartupWorker;
+  //contains list of groups that were last flushed to persistent store
+  private Set<String> prevGroups;
+
+
+  public GroupInfoManagerImpl(MasterServices master) throws IOException {
+    this.groupMap = new HashMap<String, GroupInfo>();
+    this.tableMap = new HashMap<String, String>();
+    this.master = master;
+    this.watcher = master.getZooKeeper();
+    groupStartupWorker = new GroupStartupWorker(this, master);
+    prevGroups = new HashSet<String>();
+    refresh();
+    groupStartupWorker.start();
+  }
+
+  /**
+   * Adds the group.
+   *
+   * @param groupInfo the group name
+   */
+  @Override
+  public synchronized void addGroup(GroupInfo groupInfo) throws IOException {
+    if (groupMap.get(groupInfo.getName()) != null ||
+        groupInfo.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new DoNotRetryIOException("Group already exists: "+groupInfo.getName());
+    }
+    groupMap.put(groupInfo.getName(), groupInfo);
+    try {
+      flushConfig();
+    } catch (IOException e) {
+      groupMap.remove(groupInfo.getName());
+      refresh();
+      throw e;
+    }
+  }
+
+  @Override
+  public synchronized boolean moveServers(Set<String> hostPorts, String srcGroup, String dstGroup) throws IOException {
+    GroupInfo src = new GroupInfo(getGroup(srcGroup));
+    GroupInfo dst = new GroupInfo(getGroup(dstGroup));
+    boolean foundOne = false;
+    for(String el: hostPorts) {
+      foundOne = src.removeServer(el) || foundOne;
+      dst.addServer(el);
+    }
+
+    Map<String,GroupInfo> newGroupMap = Maps.newHashMap(groupMap);
+    if (!src.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newGroupMap.put(src.getName(), src);
+    }
+    if (!dst.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+      newGroupMap.put(dst.getName(), dst);
+    }
+
+    Map<String,GroupInfo> prevGroupMap = groupMap;
+    try {
+      groupMap = newGroupMap;
+      flushConfig(newGroupMap);
+    } catch(Exception e) {
+      //in case refresh fails
+      //we restored previous consistent state
+      groupMap = prevGroupMap;
+      LOG.error("Failed to update store", e);
+      refresh();
+      throw new IOException("Error while updating store", e);
+    }
+    return foundOne;
+  }
+
+  /**
+   * Gets the group info of server.
+   *
+   * @param hostPort the server
+   * @return An instance of GroupInfo.
+   */
+  @Override
+  public synchronized GroupInfo getGroupOfServer(String hostPort) throws IOException {
+    for(GroupInfo info : groupMap.values()){
+      if (info.containsServer(hostPort)){
+        return info;
+      }
+    }
+    return getGroup(GroupInfo.DEFAULT_GROUP);
+  }
+
+  /**
+   * Gets the group information.
+   *
+   * @param groupName the group name
+   * @return An instance of GroupInfo
+   */
+  @Override
+  public synchronized GroupInfo getGroup(String groupName) throws IOException {
+    if (groupName.equals(GroupInfo.DEFAULT_GROUP)) {
+      GroupInfo defaultInfo = new GroupInfo(GroupInfo.DEFAULT_GROUP);
+      List<ServerName> unassignedServers =
+          difference(getOnlineRS(),getAssignedServers());
+      for(ServerName serverName: unassignedServers) {
+        defaultInfo.addServer(serverName.getHostAndPort());
+      }
+      for(String tableName: master.getTableDescriptors().getAll().keySet()) {
+        if (!tableMap.containsKey(tableName)) {
+          defaultInfo.addTable(tableName);
+        }
+      }
+      for(String tableName: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+        if (!tableMap.containsKey(tableName)) {
+          defaultInfo.addTable(tableName);
+        }
+      }
+      return defaultInfo;
+    } else {
+      return this.groupMap.get(groupName);
+    }
+  }
+
+  @Override
+  public synchronized GroupInfo getGroupOfTable(String tableName) throws IOException {
+    if (tableMap.containsKey(tableName)) {
+      return getGroup(tableMap.get(tableName));
+    }
+    return getGroup(GroupInfo.DEFAULT_GROUP);
+  }
+
+  @Override
+  public synchronized void moveTables(Set<String> tableNames, String groupName) throws IOException {
+    if (!GroupInfo.DEFAULT_GROUP.equals(groupName) && !groupMap.containsKey(groupName)) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist or is a special group");
+    }
+    Map<String,GroupInfo> newGroupMap = Maps.newHashMap(groupMap);
+    Map<String,String> newTableMap = Maps.newHashMap(tableMap);
+    for(String tableName: tableNames) {
+      if (newTableMap.containsKey(tableName)) {
+        //TODO optimize this, makes too many new objects
+        GroupInfo src = new GroupInfo(newGroupMap.get(newTableMap.get(tableName)));
+        src.removeTable(tableName);
+        newGroupMap.put(src.getName(), src);
+        newTableMap.remove(tableName);
+      }
+      if (!GroupInfo.DEFAULT_GROUP.equals(groupName)) {
+        GroupInfo dst = new GroupInfo(newGroupMap.get(groupName));
+        dst.addTable(tableName);
+        newGroupMap.put(dst.getName(), dst);
+        newTableMap.put(tableName, dst.getName());
+      }
+    }
+
+    Map<String,GroupInfo> prevGroupMap = groupMap;
+    Map<String,String> prevTableMap = tableMap;
+    try {
+      groupMap = newGroupMap;
+      tableMap = newTableMap;
+      flushConfig(newGroupMap);
+    } catch(Exception e) {
+      groupMap = prevGroupMap;
+      tableMap = prevTableMap;
+      LOG.error("Failed to update store", e);
+      refresh();
+      throw new IOException("Error while updating store", e);
+    }
+  }
+
+
+  /**
+   * Delete a region server group.
+   *
+   * @param groupName the group name
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  @Override
+  public synchronized void removeGroup(String groupName) throws IOException {
+    if (!groupMap.containsKey(groupName) || groupName.equals(GroupInfo.DEFAULT_GROUP)) {
+      throw new DoNotRetryIOException("Group "+groupName+" does not exist or is a reserved group");
+    }
+    GroupInfo groupInfo = null;
+    try {
+      groupInfo = groupMap.remove(groupName);
+      flushConfig();
+    } catch(IOException e) {
+      groupMap.put(groupName, groupInfo);
+      refresh();
+      throw e;
+    }
+  }
+
+  @Override
+  public synchronized List<GroupInfo> listGroups() throws IOException {
+    List<GroupInfo> list = Lists.newLinkedList(groupMap.values());
+    list.add(getGroup(GroupInfo.DEFAULT_GROUP));
+    return list;
+  }
+
+  @Override
+  public boolean isOnline() {
+    return groupStartupWorker.isOnline();
+  }
+
+  @Override
+  public synchronized void refresh() throws IOException {
+    refresh(false);
+  }
+
+  private synchronized void refresh(boolean forceOnline) throws IOException {
+    List<GroupInfo> groupList = new LinkedList<GroupInfo>();
+
+    //if online read from GROUP table
+    if (forceOnline || isOnline()) {
+      if (groupTable == null) {
+        groupTable = new HTable(master.getConfiguration(), GROUP_TABLE_NAME_BYTES);
+      }
+      Result result = groupTable.get(new Get(ROW_KEY));
+      if(!result.isEmpty()) {
+        NavigableMap<byte[],NavigableMap<byte[],byte[]>> dataMap = result.getNoVersionMap();
+        for(byte[] groupName: dataMap.get(SERVER_FAMILY_BYTES).keySet()) {
+          GroupInfoProtos.ServerList serverList =
+              GroupInfoProtos.ServerList.parseFrom(dataMap.get(SERVER_FAMILY_BYTES).get(groupName));
+          GroupInfoProtos.TableList tableList =
+              GroupInfoProtos.TableList.parseFrom(dataMap.get(TABLE_FAMILY_BYTES).get(groupName));
+          GroupInfo group = new GroupInfo(Bytes.toString(groupName));
+          group.addAllServers(serverList.getServerList());
+          group.addAllTables(tableList.getTableList());
+          groupList.add(group);
+        }
+      }
+    }
+    //Overwrite any info stored by table, this takes precedence
+    String groupBasePath = ZKUtil.joinZNode(watcher.baseZNode,groupZNode);
+    try {
+      List<String> groupChildren = ZKUtil.listChildrenNoWatch(watcher, groupBasePath);
+      if(groupChildren != null) {
+        for(String zNode: groupChildren) {
+          String groupPath = ZKUtil.joinZNode(groupBasePath, zNode);
+          byte[] data = ZKUtil.getData(watcher, groupPath);
+          GroupInfo info = GroupInfo.convert(GroupInfoProtos.GroupInfo.parseFrom(data));
+          groupList.add(info);
+          LOG.debug("Reading GroupInfo:" + info);
+        }
+      }
+    } catch (KeeperException e) {
+      throw new IOException("Failed to read groupZNode",e);
+    }
+    //populate the data
+    this.groupMap.clear();
+    this.tableMap.clear();
+    for (GroupInfo group : groupList) {
+      if(!(group.getName().equals(GroupInfo.OFFLINE_DEFAULT_GROUP) && (isOnline() || forceOnline))) {
+        groupMap.put(group.getName(), group);
+        for(String table: group.getTables()) {
+          tableMap.put(table, group.getName());
+        }
+      }
+    }
+    prevGroups.clear();
+    prevGroups.addAll(groupMap.keySet());
+  }
+
+  /**
+   * Write the configuration to HDFS.
+   *
+   * @throws IOException
+   */
+  private synchronized void flushConfig() throws IOException {
+    flushConfig(groupMap);
+  }
+
+  private synchronized void flushConfig(Map<String,GroupInfo> newGroupMap) throws IOException {
+    List<GroupInfo> zGroup = new LinkedList<GroupInfo>();
+    Put put = new Put(ROW_KEY);
+    Delete delete = new Delete(ROW_KEY);
+
+    //populate deletes
+    for(String groupName : prevGroups) {
+      if(!newGroupMap.containsKey(groupName)) {
+        delete.deleteColumns(TABLE_FAMILY_BYTES, Bytes.toBytes(groupName));
+        delete.deleteColumns(SERVER_FAMILY_BYTES, Bytes.toBytes(groupName));
+      }
+    }
+
+    //populate puts
+    for(GroupInfo groupInfo : newGroupMap.values()) {
+      put.add(SERVER_FAMILY_BYTES,
+          Bytes.toBytes(groupInfo.getName()),
+          GroupInfoProtos.ServerList.newBuilder().addAllServer(groupInfo.getServers())
+              .build().toByteArray());
+      put.add(TABLE_FAMILY_BYTES,
+          Bytes.toBytes(groupInfo.getName()),
+          GroupInfoProtos.TableList.newBuilder().addAllTable(groupInfo.getTables())
+              .build().toByteArray());
+      for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+        if (groupInfo.getTables().contains(special)) {
+          zGroup.add(groupInfo);
+          break;
+        }
+      }
+    }
+
+    //copy default group to offline group
+    GroupInfo defaultGroup = getGroup(GroupInfo.DEFAULT_GROUP);
+    GroupInfo offlineGroup = new GroupInfo(GroupInfo.OFFLINE_DEFAULT_GROUP);
+    offlineGroup.addAllServers(defaultGroup.getServers());
+    offlineGroup.addAllTables(defaultGroup.getTables());
+    zGroup.add(offlineGroup);
+    //Write zk data first since that's what we'll read first
+    String groupBasePath = ZKUtil.joinZNode(watcher.baseZNode, groupZNode);
+    try {
+      List<ZKUtil.ZKUtilOp> ops = new LinkedList<ZKUtil.ZKUtilOp>();
+      ZKUtil.createAndFailSilent(watcher, groupBasePath);
+      for(String zNode: ZKUtil.listChildrenNoWatch(watcher, groupBasePath)) {
+        String groupPath = ZKUtil.joinZNode(groupBasePath, zNode);
+        ops.add(ZKUtil.ZKUtilOp.deleteNodeFailSilent(groupPath));
+      }
+      for(GroupInfo groupInfo: zGroup) {
+        LOG.debug("Preparing GroupInfo for ZK write:" + groupInfo);
+        String groupPath = ZKUtil.joinZNode(groupBasePath, groupInfo.getName());
+        ops.add(ZKUtil.ZKUtilOp.createAndFailSilent(groupPath,
+            groupInfo.convert().toByteArray()));
+      }
+      ZKUtil.multiOrSequential(watcher, ops, false);
+      LOG.debug("GroupInfo ZK update done");
+    } catch (KeeperException e) {
+      throw new IOException("Failed to write to groupZNode",e);
+    }
+
+    RowMutations rowMutations = new RowMutations(ROW_KEY);
+    if(put.size() > 0) {
+      rowMutations.add(put);
+    }
+    if(delete.size() > 0) {
+      rowMutations.add(delete);
+    }
+    if(rowMutations.getMutations().size() > 0) {
+      groupTable.mutateRow(rowMutations);
+    }
+
+    prevGroups.clear();
+    prevGroups.addAll(newGroupMap.keySet());
+  }
+
+  private List<ServerName> getOnlineRS() throws IOException{
+    if (master != null) {
+      return master.getServerManager().getOnlineServersList();
+    }
+    try {
+      List<ServerName> servers = new LinkedList<ServerName>();
+      for (String el: ZKUtil.listChildrenNoWatch(watcher, watcher.rsZNode)) {
+        servers.add(ServerName.parseServerName(el));
+      }
+      return servers;
+    } catch (KeeperException e) {
+      throw new IOException("Failed to retrieve server list from zookeeper", e);
+    }
+  }
+
+  private List<ServerName> getAssignedServers(){
+    List<ServerName> assignedServers = Lists.newArrayList();
+    for(GroupInfo gInfo : groupMap.values()){
+      for(String hostPort: gInfo.getServers()) {
+        assignedServers.add(ServerName.parseServerName(hostPort));
+      }
+    }
+    return assignedServers;
+  }
+
+  List<ServerName> difference(Collection<ServerName> onlineServers,
+                              Collection<ServerName> servers) {
+    if (servers.size() == 0){
+      return Lists.newArrayList(onlineServers);
+    } else {
+      ArrayList<ServerName> finalList = new ArrayList<ServerName>();
+      for (ServerName olServer : onlineServers) {
+        ServerName actual = ServerName.findServerWithSameHostnamePort(
+            servers, olServer);
+        if (actual == null) {
+          finalList.add(olServer);
+        }
+      }
+      return finalList;
+    }
+  }
+
+  private static class GroupStartupWorker extends Thread {
+    private static final Log LOG = LogFactory.getLog(GroupStartupWorker.class);
+
+    private Configuration conf;
+    private volatile boolean isOnline = false;
+    private MasterServices masterServices;
+    private GroupInfoManagerImpl groupInfoManager;
+
+    public GroupStartupWorker(GroupInfoManagerImpl groupInfoManager,
+                              MasterServices masterServices) {
+      this.conf = masterServices.getConfiguration();
+      this.masterServices = masterServices;
+      this.groupInfoManager = groupInfoManager;
+      setName(GroupStartupWorker.class.getName()+"-"+masterServices.getServerName());
+      setDaemon(true);
+    }
+
+    @Override
+    public void run() {
+      if(waitForGroupTableOnline()) {
+        isOnline = true;
+        LOG.info("GroupBasedLoadBalancer is now online");
+      }
+    }
+
+    public boolean waitForGroupTableOnline() {
+      final List<HRegionInfo> foundRegions = new LinkedList<HRegionInfo>();
+      final List<HRegionInfo> assignedRegions = new LinkedList<HRegionInfo>();
+      final AtomicBoolean found = new AtomicBoolean(false);
+      boolean createSent = false;
+      while (!found.get() && isMasterRunning()) {
+        foundRegions.clear();
+        assignedRegions.clear();
+        found.set(true);
+        try {
+          boolean rootMetaFound = masterServices.getCatalogTracker().verifyMetaRegionLocation(1);
+          if (rootMetaFound) {
+            final HConnection conn = HConnectionManager.getConnection(conf);
+            final ZKTable zkTable = new ZKTable(masterServices.getZooKeeper());
+            MetaScanner.MetaScannerVisitor visitor = new MetaScanner.MetaScannerVisitorBase() {
+              @Override
+              public boolean processRow(Result row) throws IOException {
+                HRegionInfo info = HRegionInfo.getHRegionInfo(row);
+                if (info != null) {
+                  if (Bytes.equals(GROUP_TABLE_NAME_BYTES, info.getTableName())) {
+                    ServerName sn = HRegionInfo.getServerName(row);
+                    if (sn == null) {
+                      found.set(false);
+                    } else if (zkTable.isEnabledTable(GROUP_TABLE_NAME)) {
+                      try {
+                        ClientProtocol client = conn.getClient(sn.getHostname(),sn.getPort());
+                        ProtobufUtil.get(client, info.getRegionName(), new Get(ROW_KEY));
+                        assignedRegions.add(info);
+                      } catch(Exception ex) {
+                        LOG.debug("Caught exception while verifying group region", ex);
+                      }
+                    }
+                    foundRegions.add(info);
+                  }
+                }
+                return true;
+              }
+            };
+            MetaScanner.metaScan(conf, visitor);
+            // if no regions in meta then we have to create the table
+            if (foundRegions.size() < 1 && rootMetaFound && !createSent) {
+              groupInfoManager.createGroupTable(masterServices);
+              createSent = true;
+            }
+            LOG.info("Group table: " + GROUP_TABLE_NAME + " isOnline: " + found.get()
+                + ", regionCount: " + foundRegions.size() + ", assignCount: "
+                + assignedRegions.size());
+            found.set(found.get() && assignedRegions.size() == foundRegions.size()
+                && foundRegions.size() > 0);
+          } else {
+            LOG.info("Waiting for catalog tables to come online");
+            found.set(false);
+          }
+          if (found.get()) {
+            groupInfoManager.refresh(true);
+            //flush any inconsistencies between ZK and HTable
+            groupInfoManager.flushConfig();
+          }
+        } catch(Exception e) {
+          found.set(false);
+          LOG.warn("Failed to perform check", e);
+        }
+        try {
+          Thread.sleep(100);
+        } catch (InterruptedException e) {
+          LOG.info("Sleep interrupted", e);
+        }
+      }
+      return found.get();
+    }
+
+    public boolean isOnline() {
+      return isOnline;
+    }
+
+    private boolean isMasterRunning() {
+      return !masterServices.isAborted() && !masterServices.isStopped();
+    }
+  }
+
+  private void createGroupTable(MasterServices masterServices) throws IOException {
+    HTableDescriptor desc = new HTableDescriptor(GROUP_TABLE_NAME_BYTES);
+    desc.addFamily(new HColumnDescriptor(SERVER_FAMILY_BYTES));
+    desc.addFamily(new HColumnDescriptor(TABLE_FAMILY_BYTES));
+    desc.setMaxFileSize(1l << 32);
+    HRegionInfo newRegions[] = new HRegionInfo[]{
+        new HRegionInfo(desc.getName(), null, null)};
+    //we need to create the table this way to bypass
+    //checkInitialized
+    masterServices.getExecutorService()
+        .submit(new CreateTableHandler(masterServices,
+            masterServices.getMasterFileSystem(),
+            desc,
+            masterServices.getConfiguration(),
+            newRegions,
+            masterServices));
+    //need this or else region won't be assigned
+    masterServices.getAssignmentManager().assign(newRegions[0], false);
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java
new file mode 100644
index 0000000..e527281
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMasterObserver.java
@@ -0,0 +1,94 @@
+/**
+ * Copyright The Apache Software Foundation Licensed to the Apache Software Foundation (ASF) under
+ * one or more contributor license agreements. See the NOTICE file distributed with this work for
+ * additional information regarding copyright ownership. The ASF licenses this file to you under the
+ * Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
+ * Unless required by applicable law or agreed to in writing, software distributed under the License
+ * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
+ * or implied. See the License for the specific language governing permissions and limitations under
+ * the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.CoprocessorEnvironment;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.exceptions.ConstraintException;
+import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.ObserverContext;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.collect.Sets;
+
+/**
+ * This class is a required component to enable Region Server Groups. It must be installed as a
+ * system coprocessor on the master.
+ */
+@InterfaceAudience.Private
+public class GroupMasterObserver extends BaseMasterObserver {
+  private static final org.apache.commons.logging.Log LOG = LogFactory
+      .getLog(GroupMasterObserver.class);
+
+  private MasterCoprocessorEnvironment menv;
+  private GroupAdmin groupAdmin;
+
+  @Override
+  public void start(CoprocessorEnvironment ctx) throws IOException {
+    menv = (MasterCoprocessorEnvironment) ctx;
+  }
+
+  @Override
+  public void preCreateTable(ObserverContext<MasterCoprocessorEnvironment> ctx,
+      HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+    String groupName = desc.getValue(GroupInfo.TABLEDESC_PROP_GROUP);
+    if (groupName == null) {
+      return;
+    }
+
+    GroupInfo groupInfo = getGroupAdmin().getGroupInfo(groupName);
+    if (groupInfo == null) {
+      throw new ConstraintException("Group " + groupName + " does not exist.");
+    }
+    // we remove the property since it is ephemeral
+    desc.remove(GroupInfo.TABLEDESC_PROP_GROUP);
+    getGroupAdmin().moveTables(Sets.newHashSet(desc.getNameAsString()), groupName);
+  }
+
+  @Override
+  public void postDeleteTable(ObserverContext<MasterCoprocessorEnvironment> ctx, byte[] tableName)
+      throws IOException {
+    if (tableName.length > 0) {
+      String table = Bytes.toString(tableName);
+      try {
+        GroupInfo group = getGroupAdmin().getGroupInfoOfTable(table);
+        LOG.debug("Removing deleted table from table group " + group.getName());
+        if (!GroupInfo.DEFAULT_GROUP.equals(group.getName())) {
+          getGroupAdmin().moveTables(Sets.newHashSet(table), GroupInfo.DEFAULT_GROUP);
+        }
+      } catch (ConstraintException ex) {
+        // this is normal
+        LOG.trace("Failed to perform group information cleanup for table: " + table, ex);
+      } catch (IOException ex) {
+        LOG.debug("Failed to perform group information cleanup for table: " + table, ex);
+      }
+    }
+  }
+
+  private GroupAdmin getGroupAdmin() {
+    if (groupAdmin == null) {
+      groupAdmin = (GroupAdmin) menv.getMasterServices().getCoprocessorHost()
+          .findCoprocessor(GroupAdminEndpoint.class.getName());
+      if (groupAdmin == null) {
+        groupAdmin = (GroupAdmin) menv.getMasterServices().getCoprocessorHost()
+            .findCoprocessor("SecureGroupAdminEndpoint");
+      }
+    }
+    return groupAdmin;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java
new file mode 100644
index 0000000..7568b15
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/GroupMoveServerWorker.java
@@ -0,0 +1,206 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.exceptions.DoNotRetryIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.Server;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.exceptions.ConstraintException;
+import org.apache.hadoop.hbase.master.MasterServices;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+/**
+ * This is a worker class responsible for moving a set of servers
+ * from source group to target group. Supplied servers must be part
+ * of the source group.
+ *
+ * Servers are then moved to a temporary transition group. Any
+ * regions are then unassigned from the server. Once the servers
+ * are drained of any regions the servers are then moved to the
+ * destination group.
+ */
+@InterfaceAudience.Private
+public class GroupMoveServerWorker implements Runnable {
+	private static final Log LOG = LogFactory.getLog(GroupMoveServerWorker.class);
+
+  private MasterServices master;
+  private MoveServerPlan plan;
+  private String transGroup;
+  private String sourceGroup;
+  private GroupInfoManager groupManager;
+  private Map<String,String> serversInTransition;
+  private volatile boolean succeeded;
+
+  public GroupMoveServerWorker(Server master, Map<String, String> serversInTransition,
+                               GroupInfoManager groupManager,
+                               MoveServerPlan plan) throws IOException {
+    this.serversInTransition = serversInTransition;
+    this.groupManager = groupManager;
+    this.master = (MasterServices)master;
+    this.plan = plan;
+
+    synchronized (serversInTransition) {
+      //check server list
+      sourceGroup = groupManager.getGroupOfServer(plan.getServers().iterator().next()).getName();
+      if(groupManager.getGroup(plan.getTargetGroup()) == null) {
+        throw new ConstraintException("Target group does not exist: "+plan.getTargetGroup());
+      }
+      for(String server: plan.getServers()) {
+        if (serversInTransition.containsKey(server)) {
+          throw new DoNotRetryIOException(
+              "Server list contains a server that is already being moved: "+server);
+        }
+        String tmpGroup = groupManager.getGroupOfServer(server).getName();
+        if (sourceGroup != null && !tmpGroup.equals(sourceGroup)) {
+          throw new DoNotRetryIOException(
+              "Move server request should only come from one source group. "+
+              "Expecting only "+sourceGroup+" but contains "+tmpGroup);
+        }
+      }
+      if(sourceGroup.equals(plan.getTargetGroup())) {
+        throw new ConstraintException(
+            "Target group is the same as source group: "+plan.getTargetGroup());
+      }
+      //update the servers as in transition
+      for(String server: plan.getServers()) {
+        serversInTransition.put(server, plan.getTargetGroup());
+      }
+      if (!sourceGroup.startsWith(GroupInfo.TRANSITION_GROUP_PREFIX)) {
+        transGroup = GroupInfo.TRANSITION_GROUP_PREFIX+
+            System.currentTimeMillis()+"_"+sourceGroup+"-"+plan.getTargetGroup();
+        groupManager.addGroup(new GroupInfo(transGroup));
+      }
+      groupManager.moveServers(plan.getServers(), sourceGroup,
+          transGroup!=null?transGroup:plan.getTargetGroup());
+    }
+  }
+
+  @Override
+  public void run() {
+    String name = "GroupMoveServer-"+transGroup+"-"+plan.getTargetGroup();
+    Thread.currentThread().setName(name);
+    try {
+      boolean found;
+      do {
+        LOG.debug(name+" is awake");
+        found = false;
+        for(String rs: plan.getServers()) {
+          List<HRegionInfo> regions = getOnlineRegions(rs);
+          LOG.info("Unassigining "+regions.size()+" regions from server "+rs);
+          if(regions.size() > 0) {
+            master.getAssignmentManager().unassign(regions);
+            found = true;
+          }
+        }
+        try {
+          Thread.sleep(1000);
+        } catch (InterruptedException e) {
+          LOG.warn("Sleep interrupted", e);
+        }
+      } while(found);
+      succeeded = true;
+      LOG.info("Move server done: "+sourceGroup+"->"+plan.getTargetGroup());
+    } catch(Exception e) {
+      succeeded = false;
+      LOG.error("Caught exception while running", e);
+    }
+    try {
+      complete();
+    } catch (IOException e) {
+      succeeded = false;
+      LOG.error("Failed to complete move", e);
+    }
+  }
+
+  private List<HRegionInfo> getOnlineRegions(String hostPort) throws IOException {
+    List<HRegionInfo> regions = new LinkedList<HRegionInfo>();
+    for(Entry<HRegionInfo, ServerName> el:
+        master.getAssignmentManager().getRegionStates().getRegionAssignments().entrySet()) {
+      if (el.getValue().getHostAndPort().equals(hostPort)) {
+        regions.add(el.getKey());
+      }
+    }
+    return regions;
+  }
+
+  static class MoveServerPlan {
+    private Set<String> servers;
+    private String targetGroup;
+
+    public MoveServerPlan(Set<String> servers, String targetGroup) {
+      this.servers = servers;
+      this.targetGroup = targetGroup;
+    }
+
+    public Set<String> getServers() {
+      return servers;
+    }
+
+    public String getTargetGroup() {
+      return targetGroup;
+    }
+  }
+
+  public void complete() throws IOException {
+    try {
+      String tmpSourceGroup = sourceGroup;
+      if (transGroup != null) {
+        tmpSourceGroup = transGroup;
+        LOG.debug("Moving "+plan.getServers().size()+
+            " servers from transition group: "+transGroup+" to final group: "+plan.getTargetGroup());
+      }
+      if (succeeded) {
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, plan.getTargetGroup());
+        if (transGroup != null) {
+          groupManager.removeGroup(transGroup);
+          LOG.debug("Move done "+plan.getServers().size()+
+              " servers from transition group: "+transGroup+" to final group: "+plan.getTargetGroup());
+        }
+        LOG.debug("Move done "+plan.getServers().size()+
+            " servers from source group: "+sourceGroup+" to final group: "+plan.getTargetGroup());
+      } else {
+        //rollback
+        groupManager.moveServers(plan.getServers(), tmpSourceGroup, sourceGroup);
+        if (transGroup != null) {
+          groupManager.removeGroup(transGroup);
+          LOG.debug("Rollback done "+plan.getServers().size()+
+              " servers from transition group: "+transGroup+" to old group: "+sourceGroup);
+        }
+      }
+    } finally {
+      //remove servers in transition
+      synchronized(serversInTransition) {
+        for(String server: plan.getServers()) {
+          serversInTransition.remove(server);
+        }
+      }
+    }
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBean.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBean.java
new file mode 100644
index 0000000..1f64aab
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBean.java
@@ -0,0 +1,64 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+
+public interface MXBean {
+
+  public Map<String,List<String>> getServersByGroup() throws IOException;
+
+  public List<GroupInfoBean> getGroups() throws IOException;
+
+  public Map<String,String> getServersInTransition() throws IOException;
+
+  public static class GroupInfoBean {
+
+    private String name;
+    private List<String> servers;
+    private List<String> tables;
+
+    //Need this to convert NavigableSet to List
+    public GroupInfoBean(GroupInfo groupInfo) {
+      this.name = groupInfo.getName();
+      this.servers = new LinkedList<String>();
+      this.servers.addAll(groupInfo.getServers());
+      this.tables = new LinkedList<String>();
+      this.tables.addAll(groupInfo.getTables());
+    }
+
+    public String getName() {
+      return name;
+    }
+
+    public List<String> getServers() {
+      return servers;
+    }
+
+    public List<String> getTables() {
+      return tables;
+    }
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java
new file mode 100644
index 0000000..61dc8ab
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/group/MXBeanImpl.java
@@ -0,0 +1,85 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.master.MasterServices;
+
+public class MXBeanImpl implements MXBean {
+  private static final Log LOG = LogFactory.getLog(MXBeanImpl.class);
+
+  private static MXBeanImpl instance = null;
+
+  private GroupAdmin groupAdmin;
+  private MasterServices master;
+
+  public synchronized static MXBeanImpl init(
+      final GroupAdmin groupAdmin,
+      MasterServices master) {
+    if (instance == null) {
+      instance = new MXBeanImpl(groupAdmin, master);
+    }
+    return instance;
+  }
+
+  protected MXBeanImpl(final GroupAdmin groupAdmin,
+      MasterServices master) {
+    this.groupAdmin = groupAdmin;
+    this.master = master;
+  }
+
+  @Override
+  public Map<String, List<String>> getServersByGroup() throws IOException {
+    Map<String, List<String>> data = new HashMap<String, List<String>>();
+    for (final ServerName entry :
+      master.getServerManager().getOnlineServersList()) {
+      GroupInfo groupInfo = groupAdmin.getGroupOfServer(entry.getHostAndPort());
+      if(!data.containsKey(groupInfo.getName())) {
+        data.put(groupInfo.getName(), new LinkedList<String>());
+      }
+      data.get(groupInfo.getName()).add(entry.getHostAndPort());
+    }
+    return data;
+  }
+
+  @Override
+  public List<GroupInfoBean> getGroups() throws IOException {
+    LinkedList<GroupInfoBean> list = new LinkedList<GroupInfoBean>();
+    for(GroupInfo group: groupAdmin.listGroups()) {
+      list.add(new GroupInfoBean(group));
+    }
+    return list;
+  }
+
+  @Override
+  public Map<String, String> getServersInTransition() throws IOException {
+    return groupAdmin.listServersInTransition();
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
index 1579f16..a9cb05a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java
@@ -2025,8 +2025,13 @@ public class AssignmentManager extends ZooKeeperListener {
           || existingPlan.getDestination() == null
           || !destServers.contains(existingPlan.getDestination())) {
         newPlan = true;
-        randomPlan = new RegionPlan(region, null,
-            balancer.randomAssignment(region, destServers));
+        try {
+          randomPlan = new RegionPlan(region, null,
+              balancer.randomAssignment(region, destServers));
+        } catch (IOException ex) {
+          LOG.warn("Failed to create new plan.",ex);
+          return null;
+        }
         this.regionPlans.put(encodedName, randomPlan);
       }
     }
@@ -3021,6 +3026,7 @@ public class AssignmentManager extends ZooKeeperListener {
   /**
    * @param plan Plan to execute.
    */
+  @InterfaceAudience.LimitedPrivate("coprocessor")
   public void balance(final RegionPlan plan) {
     synchronized (this.regionPlans) {
       this.regionPlans.put(plan.getRegionName(), plan);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index ab62a08..77ef9b3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -726,7 +726,7 @@ Server {
 
     status.setStatus("Initializing ZK system trackers");
     initializeZKBasedSystemTrackers();
-
+    
     if (!masterRecovery) {
       // initialize master side coprocessors before we start handling requests
       status.setStatus("Initializing master coprocessors");
@@ -760,6 +760,9 @@ Server {
     status.setStatus("Splitting logs after master startup");
     splitLogAfterStartup(this.fileSystemManager);
 
+    this.balancer.setClusterStatus(getClusterStatus());
+    this.balancer.setMasterServices(this);
+    this.balancer.configure();
     // Make sure meta assigned before proceeding.
     if (!assignMeta(status)) return;
     enableServerShutdownHandler();
@@ -771,13 +774,10 @@ Server {
     org.apache.hadoop.hbase.catalog.MetaMigrationConvertingToPB
       .updateMetaIfNecessary(this);
 
-    this.balancer.setMasterServices(this);
     // Fix up assignment manager status
     status.setStatus("Starting assignment manager");
     this.assignmentManager.joinCluster();
 
-    this.balancer.setClusterStatus(getClusterStatus());
-
     if (!masterRecovery) {
       // Start balancer and meta catalog janitor after meta and regions have
       // been assigned.
@@ -1291,9 +1291,14 @@ Server {
       List<RegionPlan> plans = new ArrayList<RegionPlan>();
       //Give the balancer the current cluster state.
       this.balancer.setClusterStatus(getClusterStatus());
-      for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
-        List<RegionPlan> partialPlans = this.balancer.balanceCluster(assignments);
-        if (partialPlans != null) plans.addAll(partialPlans);
+      try {
+        for (Map<ServerName, List<HRegionInfo>> assignments : assignmentsByTable.values()) {
+          List<RegionPlan> partialPlans = this.balancer.balanceCluster(assignments);
+          if (partialPlans != null) plans.addAll(partialPlans);
+        }
+      } catch (IOException e) {
+        LOG.warn("Failed to retrieve balance plan", e);
+        return false;
       }
       long cutoffTime = System.currentTimeMillis() + maximumBalanceTime;
       int rpCount = 0;  // number of RegionPlans balanced so far
@@ -1489,7 +1494,11 @@ Server {
         "choosing a server at random");
       final List<ServerName> destServers = this.serverManager.createDestinationServersList(
         regionState.getServerName());
-      dest = balancer.randomAssignment(hri, destServers);
+      try {
+        dest = balancer.randomAssignment(hri, destServers);
+      } catch (IOException e) {
+        throw new HBaseIOException("Could not obtain a destination server", e);
+      }
     } else {
       dest = new ServerName(Bytes.toString(destServerName));
       if (dest.equals(regionState.getServerName())) {
@@ -2654,4 +2663,9 @@ Server {
     String healthScriptLocation = this.conf.get(HConstants.HEALTH_SCRIPT_LOC);
     return org.apache.commons.lang.StringUtils.isNotBlank(healthScriptLocation);
   }
+
+  @Override
+  public LoadBalancer getLoadBalancer() {
+    return this.balancer;
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
index e5252fe..f8f1b4a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/LoadBalancer.java
@@ -18,6 +18,7 @@
  */
 package org.apache.hadoop.hbase.master;
 
+import java.io.IOException;
 import java.util.List;
 import java.util.Map;
 
@@ -64,7 +65,8 @@ public interface LoadBalancer extends Configurable {
    * @param clusterState
    * @return List of plans
    */
-  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState);
+  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState)
+      throws IOException;
 
   /**
    * Perform a Round Robin assignment of regions.
@@ -72,7 +74,8 @@ public interface LoadBalancer extends Configurable {
    * @param servers
    * @return Map of servername to regioninfos
    */
-  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions, List<ServerName> servers);
+  public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions,
+      List<ServerName> servers) throws IOException;
 
   /**
    * Assign regions to the previously hosting region server
@@ -80,15 +83,17 @@ public interface LoadBalancer extends Configurable {
    * @param servers
    * @return List of plans
    */
-  public Map<ServerName, List<HRegionInfo>> retainAssignment(Map<HRegionInfo, ServerName> regions, List<ServerName> servers);
+  public Map<ServerName, List<HRegionInfo>> retainAssignment(Map<HRegionInfo, ServerName> regions,
+      List<ServerName> servers) throws IOException;
 
   /**
    * Sync assign a region
    * @param regions
    * @param servers
-    * @return Map regioninfos to servernames
+   * @return Map regioninfos to servernames
    */
-  public Map<HRegionInfo, ServerName> immediateAssignment(List<HRegionInfo> regions, List<ServerName> servers);
+  public Map<HRegionInfo, ServerName> immediateAssignment(List<HRegionInfo> regions,
+      List<ServerName> servers) throws IOException;
 
   /**
    * Get a random region server from the list
@@ -96,6 +101,12 @@ public interface LoadBalancer extends Configurable {
    * @param servers
    * @return Servername
    */
-  public ServerName randomAssignment(HRegionInfo regionInfo, 
-		  List<ServerName> servers);
+  public ServerName randomAssignment(HRegionInfo regionInfo,
+      List<ServerName> servers) throws IOException;
+
+  /**
+   * Configure the load balancer. Must be called after setters.
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  public void configure() throws IOException;
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
index f204566..d040134 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterServices.java
@@ -184,4 +184,8 @@ public interface MasterServices extends Server {
   public void dispatchMergingRegions(final HRegionInfo region_a,
       final HRegionInfo region_b, final boolean forcible) throws IOException;
 
+  /**
+   * @return load balancer
+   */
+  public LoadBalancer getLoadBalancer();
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
index 3669cc0..f285b0f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterStatusServlet.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hbase.master;
 
 import java.io.IOException;
-
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -34,6 +33,8 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.group.GroupBasedLoadBalancer;
+import org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl;
@@ -76,6 +77,10 @@ public class MasterStatusServlet extends HttpServlet {
       .setDeadServers(deadServers)
       .setCatalogJanitorEnabled(master.isCatalogJanitorEnabled(null,
           RequestConverter.buildIsCatalogJanitorEnabledRequest()).getValue());
+      if (master.getLoadBalancer() instanceof GroupBasedLoadBalancer) {
+        tmpl.setGroups(((GroupBasedLoadBalancer) master.getLoadBalancer())
+            .getGroupInfoManager().listGroups());
+      }
     } catch (ServiceException s) {
       throw new IOException(s);
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
index 4824ca6..6717b60 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java
@@ -401,6 +401,21 @@ public class RegionStates {
     }
     return tableRegions;
   }
+  
+  /**
+   * Gets the online region assignments by table.
+   *
+   * @param tableName 
+   * @return Online region assignments by table
+   */
+  public Map<HRegionInfo, ServerName> getRegionAssignmentsByTable(byte[] tableName){
+    List<HRegionInfo> regions = this.getRegionsOfTable(tableName);
+    Map<HRegionInfo, ServerName> assignments = new TreeMap<HRegionInfo, ServerName>();
+    for(HRegionInfo rInfo : regions){
+      assignments.put(rInfo, regionAssignments.get(rInfo));
+    }
+    return assignments;
+  }
 
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
index 6d8bd48..87a08fd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
@@ -17,7 +17,9 @@
  */
 package org.apache.hadoop.hbase.master.balancer;
 
+import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -330,13 +332,13 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
    *
    * @param regions all regions
    * @param servers all servers
-   * @return map of server to the regions it should take, or null if no
+   * @return map of server to the regions it should take, or EMPTY_MAP if no
    *         assignment is possible (ie. no regions or no servers)
    */
   public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions,
       List<ServerName> servers) {
     if (regions.isEmpty() || servers.isEmpty()) {
-      return null;
+      return Collections.EMPTY_MAP;
     }
     Map<ServerName, List<HRegionInfo>> assignments = new TreeMap<ServerName, List<HRegionInfo>>();
     int numRegions = regions.size();
@@ -481,4 +483,7 @@ public abstract class BaseLoadBalancer implements LoadBalancer {
     return assignments;
   }
 
+  @Override
+  public void configure() throws IOException {
+  }
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/DefaultLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/DefaultLoadBalancer.java
index 42aa6e1..b6641e5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/DefaultLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/DefaultLoadBalancer.java
@@ -17,6 +17,7 @@
  */
 package org.apache.hadoop.hbase.master.balancer;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
index d2e7698..617d2d6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessController.java
@@ -30,6 +30,7 @@ import com.google.protobuf.RpcController;
 import com.google.protobuf.Service;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CoprocessorEnvironment;
@@ -327,7 +328,8 @@ public class AccessController extends BaseRegionObserver
    * @throws IOException if obtaining the current user fails
    * @throws AccessDeniedException if user has no authorization
    */
-  private void requirePermission(String request, byte[] tableName, byte[] family, byte[] qualifier,
+  @InterfaceAudience.LimitedPrivate("regionserver-groups")
+   public void requirePermission(String request, byte[] tableName, byte[] family, byte[] qualifier,
       Action... permissions) throws IOException {
     User user = getActiveUser();
     AuthResult result = null;
diff --git a/hbase-server/src/main/ruby/hbase/group_admin.rb b/hbase-server/src/main/ruby/hbase/group_admin.rb
new file mode 100644
index 0000000..510151c
--- /dev/null
+++ b/hbase-server/src/main/ruby/hbase/group_admin.rb
@@ -0,0 +1,145 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+include Java
+java_import org.apache.hadoop.hbase.util.Pair
+
+# Wrapper for org.apache.hadoop.hbase.group.GroupAdminClient
+# Which is an API to manage region server groups
+
+module Hbase
+  class GroupAdmin
+    include HBaseConstants
+
+    def initialize(configuration, formatter)
+      @admin = org.apache.hadoop.hbase.group.GroupAdminClient.new(configuration)
+      @conf = configuration
+      @formatter = formatter
+    end
+
+    #----------------------------------------------------------------------------------------------
+    # Returns a list of groups in hbase
+    def listGroups
+      @admin.listGroups.map { |g| g.getName }
+    end
+    #----------------------------------------------------------------------------------------------
+    # get a group's information
+    def getGroup(group_name)
+      @admin.getGroupInfo(group_name)
+    end
+    #----------------------------------------------------------------------------------------------
+    # add a group
+    def addGroup(group_name)
+      @admin.addGroup(group_name)
+    end
+    #----------------------------------------------------------------------------------------------
+    # remove a group
+    def removeGroup(group_name)
+      @admin.removeGroup(group_name)
+    end
+    #----------------------------------------------------------------------------------------------
+    # balance a group
+    def balanceGroup(group_name)
+      @admin.balanceGroup(group_name)
+    end
+    #----------------------------------------------------------------------------------------------
+    # move server to a group
+    def moveServers(dest, *args)
+      servers = java.util.HashSet.new()
+      args[0].each do |s|
+        servers.add(s)
+      end
+      @admin.moveServers(servers, dest)
+    end
+    #----------------------------------------------------------------------------------------------
+    # move server to a group
+    def moveTables(dest, *args)
+      tables = java.util.HashSet.new();
+      args[0].each do |s|
+        tables.add(s)
+      end
+      @admin.moveTables(tables,dest)
+    end
+    #----------------------------------------------------------------------------------------------
+    # get group of server
+    def getGroupOfServer(server)
+      @admin.getGroupOfServer(server)
+    end
+    #----------------------------------------------------------------------------------------------
+    # get group of server
+    def getGroupOfTable(table)
+      @admin.getGroupInfoOfTable(table)
+    end
+    #----------------------------------------------------------------------------------------------
+    # get list tables of groups
+    def listTablesOfGroup(group_name)
+      @admin.listTablesOfGroup(group_name).map { |g| g }
+    end
+    #----------------------------------------------------------------------------------------------
+    # list servers in transition
+    def listServersInTransition()
+      iter = @admin.listServersInTransition.entrySet.iterator
+      while iter.hasNext
+        entry = iter.next
+        if block_given?
+          yield(entry.getKey, entry.getValue)
+        else
+          res[entry.getKey] = entry.getValue
+        end
+      end
+    end
+    #----------------------------------------------------------------------------------------------
+    # print group info
+    def printGroupInfo(group)
+      res = {}
+      if block_given?
+        yield("Name:")
+        yield(group.getName)
+      else
+        res += "Name:"
+        res += group.getName
+      end
+      if block_given?
+        yield("Servers:")
+      else
+        res += v
+      end
+      group.getServers.each do |v|
+        if block_given?
+          yield(v)
+        else
+          res += v
+        end
+      end
+      if block_given?
+        yield("Tables:")
+      else
+        res += v
+      end
+      group.getTables.each do |v|
+        if block_given?
+          yield(v)
+        else
+          res += v
+        end
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/hbase/hbase.rb b/hbase-server/src/main/ruby/hbase/hbase.rb
index 64482c5..eb06d98 100644
--- a/hbase-server/src/main/ruby/hbase/hbase.rb
+++ b/hbase-server/src/main/ruby/hbase/hbase.rb
@@ -21,6 +21,7 @@ include Java
 
 require 'hbase/admin'
 require 'hbase/table'
+require 'hbase/group_admin'
 require 'hbase/security'
 
 module Hbase
@@ -43,6 +44,10 @@ module Hbase
       ::Hbase::Admin.new(configuration, formatter)
     end
 
+    def group_admin(formatter)
+      ::Hbase::GroupAdmin.new(configuration, formatter)
+    end
+
     # Create new one each time
     def table(table, shell)
       ::Hbase::Table.new(configuration, table, shell)
diff --git a/hbase-server/src/main/ruby/shell.rb b/hbase-server/src/main/ruby/shell.rb
index 1ba9e02..be2bdbd 100644
--- a/hbase-server/src/main/ruby/shell.rb
+++ b/hbase-server/src/main/ruby/shell.rb
@@ -78,6 +78,10 @@ module Shell
       @hbase_admin ||= hbase.admin(formatter)
     end
 
+    def group_admin
+	@group_admin ||= hbase.group_admin(formatter)
+     end
+
     def hbase_table(name)
       hbase.table(name, self)
     end
@@ -333,3 +337,23 @@ Shell.load_command_group(
   ]
 )
 
+Shell.load_command_group(
+   'group',
+   :full_name => 'Groups',
+   :comment => "NOTE: Above commands are only applicable if running with the Groups setup",
+   :commands => %w[
+     group_list
+     group_get
+     group_add
+     group_remove
+     group_balance
+     group_move_servers
+     group_move_tables
+     group_of_server
+     group_of_table
+     group_list_tables
+     group_list_transitions
+   ]
+)
+
+
diff --git a/hbase-server/src/main/ruby/shell/commands.rb b/hbase-server/src/main/ruby/shell/commands.rb
index fba5f1d..036a05c 100644
--- a/hbase-server/src/main/ruby/shell/commands.rb
+++ b/hbase-server/src/main/ruby/shell/commands.rb
@@ -46,6 +46,10 @@ module Shell
         @shell.hbase_admin
       end
 
+      def group_admin
+        @shell.group_admin
+      end
+
       def table(name)
         @shell.hbase_table(name)
       end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_add.rb b/hbase-server/src/main/ruby/shell/commands/group_add.rb
new file mode 100644
index 0000000..9244df3
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_add.rb
@@ -0,0 +1,39 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupAdd < Command
+      def help
+        return <<-EOF
+Create a new region server group.
+
+Example:
+
+  hbase> group_add 'my_group'
+EOF
+      end
+
+      def command(group_name)
+        group_admin.addGroup(group_name)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_balance.rb b/hbase-server/src/main/ruby/shell/commands/group_balance.rb
new file mode 100644
index 0000000..63433fd
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_balance.rb
@@ -0,0 +1,37 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupBalance < Command
+      def help
+        return <<-EOF
+Balance a region server group
+
+  hbase> group_balance 'my_group'
+EOF
+      end
+
+      def command(group_name)
+        group_admin.balanceGroup(group_name)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_get.rb b/hbase-server/src/main/ruby/shell/commands/group_get.rb
new file mode 100644
index 0000000..dc252e3
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_get.rb
@@ -0,0 +1,44 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupGet < Command
+      def help
+        return <<-EOF
+Get a region server group's information.
+
+Example:
+
+  hbase> group_get 'default'
+EOF
+      end
+
+      def command(group_name)
+        now = Time.now
+        formatter.header([ "Group Information" ])
+        group_admin.printGroupInfo group_admin.getGroup(group_name) do |s|
+          formatter.row([ s ])
+        end
+        formatter.footer(now)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_list.rb b/hbase-server/src/main/ruby/shell/commands/group_list.rb
new file mode 100644
index 0000000..71ed52b
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_list.rb
@@ -0,0 +1,50 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupList < Command
+      def help
+        return <<-EOF
+List all region server groups. Optional regular expression parameter could
+be used to filter the output.
+
+Example:
+
+  hbase> group_list
+  hbase> group_list 'abc.*'
+EOF
+      end
+
+      def command(regex = ".*")
+        now = Time.now
+        formatter.header([ "GROUPS" ])
+
+        regex = /#{regex}/ unless regex.is_a?(Regexp)
+        list = group_admin.listGroups.grep(regex)
+        list.each do |group|
+          formatter.row([ group ])
+        end
+
+        formatter.footer(now, list.size)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_list_tables.rb b/hbase-server/src/main/ruby/shell/commands/group_list_tables.rb
new file mode 100644
index 0000000..47ddb22
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_list_tables.rb
@@ -0,0 +1,45 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupListTables < Command
+      def help
+        return <<-EOF
+List member tables of a given region server group in hbase.
+
+Example:
+
+  hbase> group_list_tables 'default'
+EOF
+      end
+
+      def command(group_name)
+        now = Time.now
+        formatter.header([ "TABLES" ])
+        list = group_admin.listTablesOfGroup(group_name)
+        list.each do |table|
+          formatter.row([ table ])
+        end
+        formatter.footer(now, list.size)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_list_transitions.rb b/hbase-server/src/main/ruby/shell/commands/group_list_transitions.rb
new file mode 100644
index 0000000..6bfa922
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_list_transitions.rb
@@ -0,0 +1,44 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+#TODO make this command name sho
+module Shell
+  module Commands
+    class GroupListTransitions < Command
+      def help
+        return <<-EOF
+List region servers in transition.
+
+Example:
+
+  hbase> group_list_transitions
+EOF
+      end
+      def command()
+        now = Time.now
+        formatter.header(["Server", "Destination"])
+        count = group_admin.listServersInTransition do |server, dest|
+          formatter.row([ server, dest ])
+        end
+        formatter.footer(now, count)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_move_servers.rb b/hbase-server/src/main/ruby/shell/commands/group_move_servers.rb
new file mode 100644
index 0000000..84c0893
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_move_servers.rb
@@ -0,0 +1,37 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupMoveServers < Command
+      def help
+        return <<-EOF
+Reassign a region server from one group to another.
+
+  hbase> group_move_servers 'dest',['server1:port','server2:port']
+EOF
+      end
+
+      def command(dest, *servers)
+        group_admin.moveServers(dest, *servers)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_move_tables.rb b/hbase-server/src/main/ruby/shell/commands/group_move_tables.rb
new file mode 100644
index 0000000..e5f07be
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_move_tables.rb
@@ -0,0 +1,37 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupMoveTables < Command
+      def help
+        return <<-EOF
+Reassign tables from one group to another.
+
+  hbase> group_move_tables 'dest',['table1','table2']
+EOF
+      end
+
+      def command(dest, *servers)
+        group_admin.moveTables(dest, *servers)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_of_server.rb b/hbase-server/src/main/ruby/shell/commands/group_of_server.rb
new file mode 100644
index 0000000..6cd2be2
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_of_server.rb
@@ -0,0 +1,42 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupOfServer < Command
+      def help
+        return <<-EOF
+Get the group name the given region server is a member of.
+
+  hbase> group_of_server 'server1:port1'
+EOF
+      end
+
+      def command(server)
+        now = Time.now
+        formatter.header([ "Group Information" ])
+        group_admin.printGroupInfo group_admin.getGroupOfServer(server) do |s|
+          formatter.row([ s ])
+        end
+        formatter.footer(now, 1)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_of_table.rb b/hbase-server/src/main/ruby/shell/commands/group_of_table.rb
new file mode 100644
index 0000000..84f2012
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_of_table.rb
@@ -0,0 +1,42 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupOfTable < Command
+      def help
+        return <<-EOF
+Get the group name the given table is a member of.
+
+  hbase> group_of_table 'myTable'
+EOF
+      end
+
+      def command(table)
+        now = Time.now
+        formatter.header([ "Group Information" ])
+        group_admin.printGroupInfo group_admin.getGroupOfTable(table) do |s|
+          formatter.row([ s ])
+        end
+        formatter.footer(now, 1)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/main/ruby/shell/commands/group_remove.rb b/hbase-server/src/main/ruby/shell/commands/group_remove.rb
new file mode 100644
index 0000000..f87e489
--- /dev/null
+++ b/hbase-server/src/main/ruby/shell/commands/group_remove.rb
@@ -0,0 +1,37 @@
+#
+# Copyright The Apache Software Foundation
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+module Shell
+  module Commands
+    class GroupRemove < Command
+      def help
+        return <<-EOF
+Remove a group.
+
+  hbase> group_remove 'my_group'
+EOF
+      end
+
+      def command(group_name)
+        group_admin.removeGroup(group_name)
+      end
+    end
+  end
+end
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
index accf181..3fdd248 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -1,5 +1,3 @@
-
-
 /**
  *
  * Licensed to the Apache Software Foundation (ASF) under one
@@ -33,16 +31,19 @@ import java.net.ServerSocket;
 import java.net.Socket;
 import java.net.UnknownHostException;
 import java.security.MessageDigest;
+import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashSet;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.NavigableSet;
 import java.util.Random;
 import java.util.Set;
+import java.util.TreeMap;
 import java.util.UUID;
 
 import org.apache.commons.logging.Log;
@@ -111,6 +112,8 @@ import org.apache.zookeeper.WatchedEvent;
 import org.apache.zookeeper.ZooKeeper;
 import org.apache.zookeeper.ZooKeeper.States;
 
+import com.google.common.collect.Maps;
+
 /**
  * Facility for testing HBase. Replacement for
  * old HBaseTestCase and HBaseClusterTestCase functionality.
@@ -1478,7 +1481,7 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
     Path tableDir = new Path(getDefaultRootDirPath().toString()
         + System.getProperty("file.separator") + htd.getNameAsString()
         + System.getProperty("file.separator") + regionToDeleteInFS);
-    getDFSCluster().getFileSystem().delete(tableDir);
+    FileSystem.get(conf).delete(tableDir);
     // flush cache of regions
     HConnection conn = table.getConnection();
     conn.clearRegionCache();
@@ -2723,4 +2726,62 @@ public class HBaseTestingUtility extends HBaseCommonTestingUtility {
     }
     return supportedAlgos.toArray(new Compression.Algorithm[0]);
   }
+  
+  public static void waitForCondition(PrivilegedExceptionAction<Boolean> action) throws Exception {
+    waitForCondition(5 * 60000, action);
+  }
+
+  public static void waitForCondition(long timeout, PrivilegedExceptionAction<Boolean> action)
+      throws Exception {
+    long sleepInterval = 100;
+    long tries = timeout / sleepInterval;
+    int i = 0;
+    while (action.run()) {
+      if (i == 0) {
+        StackTraceElement el = Thread.currentThread().getStackTrace()[2];
+        if (el.getMethodName().equals("waitForCondition")) {
+          el = Thread.currentThread().getStackTrace()[3];
+        }
+        LOG.info("Waiting for method: " + el.getClassName() + "." + el.getMethodName() + "("
+            + el.getFileName() + ":" + el.getLineNumber() + ")");
+      }
+      Thread.sleep(sleepInterval);
+      if (tries-- < 0) {
+        fail("Timeout");
+      }
+      i = (i + 1) % 10;
+    }
+  }
+
+  public Map<String, List<String>> getTableRegionMap() throws IOException {
+    Map<String, List<String>> map = Maps.newTreeMap();
+    Map<String, Map<ServerName, List<String>>> tableServerRegionMap = getTableServerRegionMap();
+    for (String tableName : tableServerRegionMap.keySet()) {
+      if (!map.containsKey(tableName)) {
+        map.put(tableName, new LinkedList<String>());
+      }
+      for (List<String> subset : tableServerRegionMap.get(tableName).values()) {
+        map.get(tableName).addAll(subset);
+      }
+    }
+    return map;
+  }
+
+  public Map<String, Map<ServerName, List<String>>> getTableServerRegionMap() throws IOException {
+    Map<String, Map<ServerName, List<String>>> map = Maps.newTreeMap();
+    ClusterStatus status = getHBaseClusterInterface().getClusterStatus();
+    for (ServerName serverName : status.getServers()) {
+      for (RegionLoad rl : status.getLoad(serverName).getRegionsLoad().values()) {
+        String tableName = Bytes.toString(HRegionInfo.getTableName(rl.getName()));
+        if (!map.containsKey(tableName)) {
+          map.put(tableName, new TreeMap<ServerName, List<String>>());
+        }
+        if (!map.get(tableName).containsKey(serverName)) {
+          map.get(tableName).put(serverName, new LinkedList<String>());
+        }
+        map.get(tableName).get(serverName).add(rl.getNameAsString());
+      }
+    }
+    return map;
+  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroups.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroups.java
new file mode 100644
index 0000000..05e878f
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroups.java
@@ -0,0 +1,224 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+import static org.mockito.Mockito.atLeastOnce;
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.verify;
+
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.security.PrivilegedExceptionAction;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.common.collect.Sets;
+
+/**
+ * Unit test suite for testing region server groups
+ */
+@Category(MediumTests.class)
+public class TestRegionServerGroups extends TestRegionServerGroupsCommonCases {
+  protected static final Log LOG = LogFactory.getLog(TestRegionServerGroups.class);
+  private static HMaster master;
+  private static GroupAdminEndpoint groupEndpoint;
+  protected final static int NUM_SLAVES_BASE = 4;
+
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.getConfiguration().set(
+        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+        GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName() + "," +
+            GroupAdminEndpoint.class.getName());
+    TEST_UTIL.getConfiguration().setBoolean(HConstants.ZOOKEEPER_USEMULTI, true);
+    TEST_UTIL.startMiniCluster(NUM_SLAVES_BASE);
+    TEST_UTIL.getConfiguration().set(
+        ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART,
+        ""+NUM_SLAVES_BASE);
+
+    admin = TEST_UTIL.getHBaseAdmin();
+    cluster = TEST_UTIL.getHBaseCluster();
+    master = ((MiniHBaseCluster)cluster).getMaster();
+    //wait for balancer to come online
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return !master.isInitialized() ||
+            !((GroupBasedLoadBalancer) master.getLoadBalancer()).isOnline();
+      }
+    });
+    admin.setBalancerRunning(false,true);
+    groupAdmin = new VerifyingGroupAdminClient(TEST_UTIL.getConfiguration());
+    groupEndpoint =
+        (GroupAdminEndpoint)master.getCoprocessorHost().findCoprocessor(GroupAdminEndpoint.class.getName());
+    LOG.info("Done initializing cluster");
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+ @After
+  public void afterMethod() throws Exception {
+    int missing = NUM_SLAVES_BASE - cluster.getClusterStatus().getServers().size();
+    for(int i=0; i<missing; i++) {
+      ((MiniHBaseCluster)cluster).startRegionServer();
+    }
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return cluster.getClusterStatus().getServers().size() != NUM_SLAVES_BASE;
+      }
+    });
+    deleteTableIfNecessary();
+    deleteGroups();
+  }
+
+  @Test
+  public void testJmx() throws Exception {
+    MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer();
+    Iterator<ObjectName> it = mBeanServer.queryNames(new ObjectName("hadoop:name=Group,service=Group"), null).iterator();
+    //verify it was loaded properly
+    assertEquals("hadoop:name=Group,service=Group", it.next().getCanonicalName());
+
+    final MXBeanImpl info = MXBeanImpl.init(groupAdmin, master);
+    GroupInfo defaultGroup = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(1, info.getGroups().size());
+    assertEquals(defaultGroup.getName(), info.getGroups().get(0).getName());
+    assertEquals(defaultGroup.getServers(), Sets.newTreeSet(info.getGroups().get(0).getServers()));
+    assertEquals(defaultGroup.getServers(), Sets.newTreeSet(info.getServersByGroup().get(GroupInfo.DEFAULT_GROUP)));
+    assertEquals(0, info.getServersInTransition().size());
+
+    GroupInfo barGroup = addGroup(groupAdmin, "bar", 3);
+    String tableName1 = tablePrefix+"_testJmx1";
+    String tableName2 = tablePrefix+"_testJmx2";
+    TEST_UTIL.createTable(Bytes.toBytes(tableName1), Bytes.toBytes("f"));
+    TEST_UTIL.createTable(Bytes.toBytes(tableName2), Bytes.toBytes("f"));
+    groupAdmin.moveTables(Sets.newHashSet(tableName2), barGroup.getName());
+    assertEquals(2, info.getGroups().size());
+
+    int defaultIndex = -1;
+    int barIndex = -1;
+
+    for(int i=0; i<info.getGroups().size(); i++) {
+      MXBean.GroupInfoBean bean = info.getGroups().get(i);
+      if(bean.getName().equals(defaultGroup.getName())) {
+        defaultIndex = i;
+      }
+      else if(bean.getName().equals(barGroup.getName())) {
+        barIndex = i;
+      }
+    }
+
+    defaultGroup = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(defaultGroup.getName(),
+        info.getGroups().get(defaultIndex).getName());
+    assertEquals(defaultGroup.getTables(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getTables()));
+    assertEquals(defaultGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getServers()));
+    assertEquals(defaultGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(defaultIndex).getServers()));
+
+    barGroup = groupAdmin.getGroupInfo(barGroup.getName());
+    assertEquals(barGroup.getName(),
+        info.getGroups().get(barIndex).getName());
+    assertEquals(barGroup.getTables(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getTables()));
+    assertEquals(barGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getServers()));
+    assertEquals(barGroup.getServers(),
+        Sets.newTreeSet(info.getGroups().get(barIndex).getServers()));
+  }
+
+  @Test
+  public void testBasicStartUp() throws IOException {
+    GroupInfo defaultInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(4, defaultInfo.getServers().size());
+    // Assignment of root and meta regions.
+    int count = master.getAssignmentManager().getRegionStates().getRegionAssignments().keySet()
+        .size();
+    assertEquals(2, count);
+  }
+
+  @Test
+  public void testMoveServerWorker() throws Exception {
+    LOG.info("testMoveServerWorker");
+
+    //create groups and assign servers
+    addGroup(groupAdmin, "bar", 3);
+    groupAdmin.addGroup("foo");
+
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    GroupInfo fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(0, fooGroup.getServers().size());
+
+    //test failure in move server worker
+    MasterServices mockedMaster = Mockito.spy(master);
+    doThrow(new RuntimeException("testMoveServerWorker")).when(mockedMaster).getAssignmentManager();
+    Map<String,String> serversInTransition = new HashMap<String,String>();
+
+    Runnable failedRunnable =
+        new GroupMoveServerWorker(mockedMaster,
+            serversInTransition,
+            groupEndpoint.getGroupInfoManager(),
+            new GroupMoveServerWorker.MoveServerPlan(barGroup.getServers(), fooGroup.getName()));
+    Thread failedMoveServerThread = new Thread(failedRunnable);
+    failedMoveServerThread.start();
+    failedMoveServerThread.join();
+    verify(mockedMaster,atLeastOnce()).getAssignmentManager();
+    barGroup = groupAdmin.getGroupInfo("bar");
+    assertEquals(0, serversInTransition.size());
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(3, groupAdmin.listGroups().size());
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsBase.java
new file mode 100644
index 0000000..3f5ca4a
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsBase.java
@@ -0,0 +1,95 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.security.SecureRandom;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.hbase.HBaseCluster;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.util.Bytes;
+
+public abstract class TestRegionServerGroupsBase {
+  //shared
+  protected final static String groupPrefix = "Group-";
+  protected final static String tablePrefix = "Group-";
+  protected final static SecureRandom rand = new SecureRandom();
+ 
+  //shared, cluster type specific
+  protected static HBaseTestingUtility TEST_UTIL;
+  protected static HBaseAdmin admin;
+  protected static HBaseCluster cluster;
+  protected static GroupAdminClient groupAdmin;
+
+
+  protected GroupInfo addGroup(GroupAdminClient gAdmin, String groupName,
+                               int serverCount) throws IOException, InterruptedException {
+    GroupInfo defaultInfo = gAdmin
+        .getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertTrue(defaultInfo != null);
+    assertTrue(defaultInfo.getServers().size() >= serverCount);
+    gAdmin.addGroup(groupName);
+
+    Set<String> set = new HashSet<String>();
+    for(String server: defaultInfo.getServers()) {
+      if(set.size() == serverCount) {
+        break;
+      }
+      set.add(server);
+    }
+    gAdmin.moveServers(set, groupName);
+    GroupInfo result = gAdmin.getGroupInfo(groupName);
+    assertTrue(result.getServers().size() >= serverCount);
+    return result;
+  }
+
+  static void removeGroup(GroupAdminClient groupAdmin, String groupName) throws IOException {
+    GroupInfo groupInfo = groupAdmin.getGroupInfo(groupName);
+    for(String table: groupInfo.getTables()) {
+      byte[] bTable = Bytes.toBytes(table);
+      groupAdmin.moveTables(groupInfo.getTables(), GroupInfo.DEFAULT_GROUP);
+      groupAdmin.moveServers(groupInfo.getServers(), GroupInfo.DEFAULT_GROUP);
+    }
+    groupAdmin.removeGroup(groupName);
+  }
+
+  protected void deleteTableIfNecessary() throws IOException {
+    for (HTableDescriptor desc : TEST_UTIL.getHBaseAdmin().listTables(tablePrefix+".*")) {
+      TEST_UTIL.deleteTable(desc.getName());
+    }
+  }
+
+  protected void deleteGroups() throws IOException {
+    GroupAdminClient groupAdmin = new GroupAdminClient(TEST_UTIL.getConfiguration());
+    for(GroupInfo group: groupAdmin.listGroups()) {
+      if(!group.getName().equals(GroupInfo.DEFAULT_GROUP)) {
+        groupAdmin.moveTables(group.getTables(),GroupInfo.DEFAULT_GROUP);
+        groupAdmin.moveServers(group.getServers(),GroupInfo.DEFAULT_GROUP);
+        groupAdmin.removeGroup(group.getName());
+      }
+    }
+  } 
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsCommonCases.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsCommonCases.java
new file mode 100644
index 0000000..fabb4bc
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsCommonCases.java
@@ -0,0 +1,474 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.security.PrivilegedExceptionAction;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeSet;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.Test;
+
+import com.google.common.collect.Sets;
+
+/**
+ * A common set of Region Server Group test cases
+ * which will be run as part of the unit
+ * and integration test suite through subclassed
+ * implementations of this class
+ */
+public abstract class TestRegionServerGroupsCommonCases extends TestRegionServerGroupsBase {
+  
+  private final static Log LOG = LogFactory.getLog(TestRegionServerGroupsCommonCases.class);
+  
+  @Test
+  public void testCreateMultiRegion() throws IOException {
+    LOG.info("testCreateMultiRegion");
+    byte[] tableName = Bytes.toBytes(tablePrefix + "_multi_table");
+    byte[] end = { 1, 3, 5, 7, 9 };
+    byte[] start = { 0, 2, 4, 6, 8 };
+    byte[][] f = { Bytes.toBytes("f") };
+    TEST_UTIL.createTable(tableName, f, 1, start, end, 10);
+  }
+
+  @Test
+  public void testCreateAndAssign() throws Exception {
+    LOG.info("testCreateAndAssign");
+
+    final byte[] tableName = Bytes.toBytes(tablePrefix + "_pre_table");
+    GroupInfo appInfo = addGroup(groupAdmin, "appInfo", 1);
+    final HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor("f"));
+    desc.setValue(GroupInfo.TABLEDESC_PROP_GROUP, appInfo.getName());
+    admin.createTable(desc);
+    // wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getTableRegionMap().get(desc.getNameAsString()) == null;
+      }
+    });
+    ServerName targetServer = ServerName.parseServerName(appInfo.getServers().iterator().next());
+    // verify it was assigned to the right group
+    assertEquals(1, admin.getOnlineRegions(targetServer).size());
+    // verify prop was not stored as part of the schema
+    assertNull(admin.getTableDescriptor(tableName).getValue(GroupInfo.TABLEDESC_PROP_GROUP));
+  }
+
+  @Test
+  public void testCreateAndDrop() throws Exception {
+    LOG.info("testCreateAndDrop");
+
+    final byte[] tableName = Bytes.toBytes(tablePrefix+"_testCreateAndDrop");
+    TEST_UTIL.createTable(tableName, Bytes.toBytes("cf"));
+    //wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getTableRegionMap().get(Bytes.toString(tableName)) == null;
+      }
+    });
+    TEST_UTIL.deleteTable(tableName);
+  }
+
+  @Test
+  public void testSimpleRegionServerMove() throws IOException,
+      InterruptedException {
+    LOG.info("testSimpleRegionServerMove");
+
+    GroupInfo appInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+    GroupInfo adminInfo = addGroup(groupAdmin, groupPrefix + rand.nextInt(), 1);
+    GroupInfo dInfo = groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP);
+    assertEquals(3, groupAdmin.listGroups().size());
+    assertEquals(1, adminInfo.getServers().size());
+    assertEquals(1, appInfo.getServers().size());
+    assertEquals(admin.getClusterStatus().getServers().size() - 2, dInfo.getServers().size());
+    groupAdmin.moveServers(appInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(appInfo.getName());
+    groupAdmin.moveServers(adminInfo.getServers(),
+        GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(adminInfo.getName());
+    assertTrue(groupAdmin.listGroups().size() == 1);
+  }
+
+  @Test
+  public void testMoveServers() throws Exception {
+    LOG.info("testMoveServers");
+
+    //create groups and assign servers
+    addGroup(groupAdmin, "bar", 3);
+    groupAdmin.addGroup("foo");
+
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    GroupInfo fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(3, barGroup.getServers().size());
+    assertEquals(0, fooGroup.getServers().size());
+
+    //test fail bogus server move
+    try {
+      groupAdmin.moveServers(Sets.newHashSet("foo:9999"),"foo");
+      fail("Bogus servers shouldn't have been successfully moved.");
+    } catch(Throwable ex) {
+      do {
+        if(ex.getMessage().contains("Server foo:9999 is not a member of any group.")) {
+          break;
+        }
+        ex = ex.getCause();
+      } while (ex != null);
+      if(ex == null) {
+        fail("Didn't get the correct exception for bogus server move: "+ex);
+      }
+    }
+
+    //test success case
+    LOG.info("moving servers "+barGroup.getServers()+" to group foo");
+    groupAdmin.moveServers(barGroup.getServers(), fooGroup.getName());
+
+    barGroup = groupAdmin.getGroupInfo("bar");
+    fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(0,barGroup.getServers().size());
+    assertEquals(3,fooGroup.getServers().size());
+
+    LOG.info("moving servers "+fooGroup.getServers()+" to group default");
+    groupAdmin.moveServers(fooGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return admin.getClusterStatus().getServers().size() !=
+            groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP).getServers().size();
+      }
+    });
+
+    fooGroup = groupAdmin.getGroupInfo("foo");
+    assertEquals(0,fooGroup.getServers().size());
+
+    //test group removal
+    LOG.info("Remove group "+barGroup.getName());
+    groupAdmin.removeGroup(barGroup.getName());
+    assertEquals(null, groupAdmin.getGroupInfo(barGroup.getName()));
+    LOG.info("Remove group "+fooGroup.getName());
+    groupAdmin.removeGroup(fooGroup.getName());
+    assertEquals(null, groupAdmin.getGroupInfo(fooGroup.getName()));
+  }
+
+  @Test
+  public void testTableMoveAndDrop() throws Exception {
+    LOG.info("testTableMove");
+
+    final String tableName = tablePrefix + rand.nextInt();
+    final byte[] tableNameBytes = Bytes.toBytes(tableName);
+    final byte[] familyNameBytes = Bytes.toBytes("f");
+    String newGroupName = "g_" + rand.nextInt();
+    final GroupInfo newGroup = addGroup(groupAdmin, newGroupName, 2);
+
+    HTable ht = TEST_UTIL.createTable(tableNameBytes, familyNameBytes);
+    assertEquals(4,
+        TEST_UTIL.createMultiRegions(TEST_UTIL.getConfiguration(), ht, familyNameBytes, 4));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        List<String> regions = TEST_UTIL.getTableRegionMap().get(tableName);
+        if (regions == null)
+          return true;
+        return TEST_UTIL.getTableRegionMap().get(tableName).size() < 4;
+      }
+    });
+
+    GroupInfo tableGrp = groupAdmin.getGroupInfoOfTable(tableName);
+    assertTrue(tableGrp.getName().equals(GroupInfo.DEFAULT_GROUP));
+
+    //change table's group
+    LOG.info("Moving table "+tableName+" to "+newGroup.getName());
+    groupAdmin.moveTables(Sets.newHashSet(tableName), newGroup.getName());
+
+    //verify group change
+    assertEquals(newGroup.getName(),
+        groupAdmin.getGroupInfoOfTable(tableName).getName());
+
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        Map<ServerName, List<String>> serverMap =
+            TEST_UTIL.getTableServerRegionMap().get(tableName);
+        boolean found = true;
+        int count = 0;
+        if(serverMap != null) {
+          for (ServerName rs : serverMap.keySet()) {
+            if (newGroup.containsServer(rs.getHostAndPort())) {
+              count += serverMap.get(rs).size();
+            }
+          }
+        }
+        return count != 4;
+      }
+    });
+
+    //verify removed table is removed from group
+    TEST_UTIL.deleteTable(tableNameBytes);
+    assertEquals(0, groupAdmin.getGroupInfo(newGroup.getName()).getTables().size());
+  }
+
+  @Test
+  public void testGroupBalance() throws Exception {
+    LOG.info("testGroupBalance");
+
+    final String tableName = tablePrefix+"_testGroupBalance";
+    final byte[] tableNameBytes = Bytes.toBytes(tableName);
+    final byte[] familyNameBytes = Bytes.toBytes("f");
+    String newGroupName = "g_" + rand.nextInt();
+    final GroupInfo newGroup = addGroup(groupAdmin, newGroupName, 3);
+    final HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor("f"));
+    desc.setValue(GroupInfo.TABLEDESC_PROP_GROUP, newGroup.getName());
+    byte [] startKey = Bytes.toBytes("aaaaa");
+    byte [] endKey = Bytes.toBytes("zzzzz");
+    admin.createTable(desc, startKey, endKey, 6);
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        List<String> regions = TEST_UTIL.getTableRegionMap().get(tableName);
+        if(regions == null) {
+          return true;
+        }
+        return regions.size() < 6;
+      }
+    });
+
+
+    //make assignment uneven, move all regions to one server
+    Map<ServerName,List<String>> assignMap =
+        TEST_UTIL.getTableServerRegionMap().get(tableName);
+    final ServerName first = assignMap.entrySet().iterator().next().getKey();
+    for(HRegionInfo region: admin.getTableRegions(tableNameBytes)) {
+      if(!assignMap.get(first).contains(region)) {
+        admin.move(region.getEncodedNameAsBytes(), Bytes.toBytes(first.getServerName()));
+      }
+    }
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        Map<ServerName,List<String>> map = TEST_UTIL.getTableServerRegionMap().get(tableName);
+        if(map == null) {
+          return true;
+        }
+        List<String> regions = map.get(first);
+        if(regions == null) {
+          return true;
+        }
+        return regions.size() < 6;
+      }
+    });
+
+    //balance the other group and make sure it doesn't affect the new group
+    groupAdmin.balanceGroup(GroupInfo.DEFAULT_GROUP);
+    assertEquals(6, TEST_UTIL.getTableServerRegionMap().get(tableName).get(first).size());
+
+    groupAdmin.balanceGroup(newGroupName);
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        for(List<String> regions: TEST_UTIL.getTableServerRegionMap().get(tableName).values()) {
+          if(2 != regions.size()) {
+            return true;
+          }
+        }
+        return false;
+      }
+    });
+  }
+
+  @Test
+  public void testRegionMove() throws Exception {
+    LOG.info("testRegionMove");
+
+    final GroupInfo newGroup = addGroup(groupAdmin, "g_" + rand.nextInt(), 1);
+    final String tableName = tablePrefix + rand.nextInt();
+    final byte[] tableNameBytes = Bytes.toBytes(tableName);
+    final byte[] familyNameBytes = Bytes.toBytes("f");
+    HTable ht = TEST_UTIL.createTable(tableNameBytes, familyNameBytes);
+
+    // All the regions created below will be assigned to the default group.
+    assertEquals(5, TEST_UTIL.createMultiRegions(TEST_UTIL.getConfiguration(), ht, familyNameBytes, 5));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        List<String> regions = TEST_UTIL.getTableRegionMap().get(tableName);
+        if(regions == null)
+          return true;
+        return TEST_UTIL.getTableRegionMap().get(tableName).size() < 6;
+      }
+    });
+
+    //get target region to move
+    Map<ServerName,List<String>> assignMap =
+        TEST_UTIL.getTableServerRegionMap().get(tableName);
+    String targetRegion = null;
+    for(ServerName server : assignMap.keySet()) {
+      targetRegion = assignMap.get(server).size() > 0 ? assignMap.get(server).get(0) : null;
+      if(targetRegion != null) {
+        break;
+      }
+    }
+    //get server which is not a member of new group
+    ServerName temp = null;
+    for(ServerName server : admin.getClusterStatus().getServers()) {
+      if(!newGroup.containsServer(server.getHostAndPort())) {
+        temp = server;
+        break;
+      }
+    }
+    final ServerName targetServer = temp;
+        //move target server to group
+    groupAdmin.moveServers(Sets.newHashSet(targetServer.getHostAndPort()), newGroup.getName());
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return admin.getOnlineRegions(targetServer).size() > 0;
+      }
+    });
+
+    // Lets move this region to the new group.
+    TEST_UTIL.getHBaseAdmin().move(Bytes.toBytes(HRegionInfo.encodeRegionName(Bytes.toBytes(targetRegion))),
+        Bytes.toBytes(targetServer.getServerName()));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return
+            TEST_UTIL.getTableRegionMap().get(tableName) != null &&
+                TEST_UTIL.getTableRegionMap().get(tableName).size() != 6 ||
+                admin.getClusterStatus().getRegionsInTransition().size() > 0;
+      }
+    });
+
+    //verify that targetServer didn't open it
+    assertFalse(admin.getOnlineRegions(targetServer).contains(targetRegion));
+  }
+
+  @Test
+  public void testFailRemoveGroup() throws IOException, InterruptedException {
+    LOG.info("testFailRemoveGroup");
+
+    addGroup(groupAdmin, "bar", 3);
+    String tableName = tablePrefix+"_my_table";
+    TEST_UTIL.createTable(Bytes.toBytes(tableName), Bytes.toBytes("f"));
+    groupAdmin.moveTables(Sets.newHashSet(tableName), "bar");
+    GroupInfo barGroup = groupAdmin.getGroupInfo("bar");
+    //group is not empty therefore it should fail
+    try {
+      groupAdmin.removeGroup(barGroup.getName());
+      fail("Expected remove group to fail");
+    } catch(IOException e) {
+    }
+    //group cannot lose all it's servers therefore it should fail
+    try {
+      groupAdmin.moveServers(barGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+      fail("Expected move servers to fail");
+    } catch(IOException e) {
+    }
+
+    groupAdmin.moveTables(barGroup.getTables(), GroupInfo.DEFAULT_GROUP);
+    try {
+      groupAdmin.removeGroup(barGroup.getName());
+      fail("Expected move servers to fail");
+    } catch(IOException e) {
+    }
+
+    groupAdmin.moveServers(barGroup.getServers(), GroupInfo.DEFAULT_GROUP);
+    groupAdmin.removeGroup(barGroup.getName());
+
+    assertEquals(1, groupAdmin.listGroups().size());
+  }
+
+  @Test
+  public void testKillRS() throws Exception {
+    LOG.info("testKillRS");
+
+    final String tableNameString = tablePrefix + "_testKillRS";
+    final byte[] tableName = Bytes.toBytes(tableNameString);
+
+    GroupInfo appInfo = addGroup(groupAdmin, "appInfo", 1);
+    final HTableDescriptor desc = new HTableDescriptor(tableName);
+    desc.addFamily(new HColumnDescriptor("f"));
+    desc.setValue(GroupInfo.TABLEDESC_PROP_GROUP, appInfo.getName());
+    admin.createTable(desc);
+    //wait for created table to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getTableRegionMap().get(desc.getNameAsString()) == null;
+      }
+    });
+
+    ServerName targetServer = ServerName.parseServerName(appInfo.getServers().first());
+    List<HRegionInfo> onlineRegions = admin.getOnlineRegions(targetServer);
+    HRegionInfo targetRegion = onlineRegions.get(0);
+    assertEquals(1, onlineRegions.size());
+    try {
+      admin.stopRegionServer(targetServer.getHostAndPort());
+    } catch(IOException ex) {
+    }
+
+    //wait for created table to be unassigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        Map<String,List<String>> map = TEST_UTIL.getTableRegionMap();
+        return map.get(tableNameString) != null &&
+            map.get(tableNameString).size() > 0;
+      }
+    });
+    TreeSet<String> newServers = Sets.newTreeSet();
+    newServers.add(groupAdmin.getGroupInfo(GroupInfo.DEFAULT_GROUP).getServers().first());
+    groupAdmin.moveServers(newServers, appInfo.getName());
+    admin.assign(targetRegion.getRegionName());
+
+    //wait for region to be assigned
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        Map<String,List<String>> map = TEST_UTIL.getTableRegionMap();
+        return map.get(tableNameString) == null ||
+            map.get(tableNameString).size() == 0;
+      }
+    });
+
+    targetServer = ServerName.parseServerName(newServers.first());
+    assertEquals(1, admin.getOnlineRegions(targetServer).size());
+    assertEquals(Bytes.toString(tableName), admin.getOnlineRegions(targetServer).get(0).getTableNameAsString());
+  }
+
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsOfflineMode.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsOfflineMode.java
new file mode 100644
index 0000000..2c98442
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/TestRegionServerGroupsOfflineMode.java
@@ -0,0 +1,196 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.group;
+
+import com.google.common.collect.Sets;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.MediumTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.ServerManager;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.zookeeper.MasterAddressTracker;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import java.security.PrivilegedExceptionAction;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+/**
+ * Unit test suite for testing Region Server Groups'
+ * offline mode.
+ */
+@Category(MediumTests.class)
+public class TestRegionServerGroupsOfflineMode extends TestRegionServerGroupsBase {
+  private static final org.apache.commons.logging.Log LOG = LogFactory.getLog(TestRegionServerGroupsOfflineMode.class);
+  private static HMaster master;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    TEST_UTIL = new HBaseTestingUtility();
+    TEST_UTIL.getConfiguration().set(
+        HConstants.HBASE_MASTER_LOADBALANCER_CLASS,
+        GroupBasedLoadBalancer.class.getName());
+    TEST_UTIL.getConfiguration().set("hbase.coprocessor.master.classes",
+        GroupMasterObserver.class.getName()+","+
+            GroupAdminEndpoint.class.getName());
+    TEST_UTIL.getConfiguration().set(
+        ServerManager.WAIT_ON_REGIONSERVERS_MINTOSTART,
+        "1");
+    TEST_UTIL.getConfiguration().setBoolean(HConstants.ZOOKEEPER_USEMULTI, true);
+    TEST_UTIL.startMiniCluster(2, 3);
+    cluster = TEST_UTIL.getHBaseCluster();
+    master = ((MiniHBaseCluster)cluster).getMaster();
+    master.balanceSwitch(false);
+    admin = TEST_UTIL.getHBaseAdmin();
+    //wait till the balancer is in online mode
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return !master.isInitialized() ||
+            !((GroupBasedLoadBalancer)master.getLoadBalancer()).isOnline() ||
+            master.getServerManager().getOnlineServersList().size() < 3;
+      }
+    });
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @Test
+  public void testOffline() throws Exception, InterruptedException {
+    //table should be after group table name
+    //so it gets assigned later
+    final String failoverTable = "z"+GroupInfoManager.GROUP_TABLE_NAME;
+    TEST_UTIL.createTable(Bytes.toBytes(failoverTable), Bytes.toBytes("f"));
+
+    //adding testTable to special group so it gets assigned during offline mode
+    GroupBasedLoadBalancer.SPECIAL_TABLES.add(failoverTable);
+
+    GroupAdminClient groupAdmin = new GroupAdminClient(TEST_UTIL.getConfiguration());
+
+    final HRegionServer killRS = ((MiniHBaseCluster)cluster).getRegionServer(0);
+    final HRegionServer groupRS = ((MiniHBaseCluster)cluster).getRegionServer(1);
+    final HRegionServer failoverRS = ((MiniHBaseCluster)cluster).getRegionServer(2);
+
+    String newGroup =  "my_group";
+    groupAdmin.addGroup(newGroup);
+    if(cluster.getClusterStatus().getServers().contains(failoverRS.getServerName())) {
+      for(HRegionInfo  regionInfo:
+        TEST_UTIL.getHBaseAdmin().getOnlineRegions(failoverRS.getServerName())) {
+        TEST_UTIL.getHBaseAdmin().move(regionInfo.getEncodedNameAsBytes(),
+            Bytes.toBytes(killRS.getServerName().getServerName()));
+      }
+      LOG.info("Waiting for region unassignments on failover RS...");
+      TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+        @Override
+        public Boolean run() throws Exception {
+          return TEST_UTIL.getHBaseAdmin().getOnlineRegions(failoverRS.getServerName()).size() > 0;
+        }
+      });
+    }
+
+    //move server to group and make sure all tables are assigned
+    groupAdmin.moveServers(Sets.newHashSet(groupRS.getServerName().getHostAndPort()), newGroup);
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getHBaseAdmin().getOnlineRegions(groupRS.getServerName()).size() > 0 ||
+            master.getAssignmentManager().getRegionStates().getRegionsInTransition().size() > 0;
+      }
+    });
+    //move table to group and wait
+    groupAdmin.moveTables(Sets.newHashSet(GroupInfoManager.GROUP_TABLE_NAME), newGroup);
+    LOG.info("Waiting for move table...");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getHBaseAdmin().getOnlineRegions(groupRS.getServerName()).size() < 1;
+      }
+    });
+    groupRS.stop("die");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return admin.getClusterStatus().getServers().contains(groupRS.getServerName());
+      }
+    });
+    //race condition here
+    groupAdmin.close();
+
+    ServerName currMaster = MasterAddressTracker.getMasterAddress(TEST_UTIL.getZooKeeperWatcher());
+    TEST_UTIL.getHBaseCluster().stopMaster(currMaster);
+    TEST_UTIL.getHBaseCluster().waitForMasterToStop(currMaster,2*60000);
+    LOG.info("Waiting for offline mode...");
+    assertTrue (TEST_UTIL.getHBaseCluster().waitForActiveAndReadyMaster());
+    final HMaster newActive = TEST_UTIL.getHBaseCluster().getMaster();
+    assertTrue(newActive != null);
+    assertFalse(master == newActive);
+    LOG.info("Current New Master " + MasterAddressTracker.getMasterAddress(TEST_UTIL.getZooKeeperWatcher()));
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return TEST_UTIL.getHBaseCluster().getMaster() == null ||
+            !TEST_UTIL.getHBaseCluster().getMaster().isActiveMaster() ||
+            !TEST_UTIL.getHBaseCluster().getMaster().isInitialized() ||
+            TEST_UTIL.getHBaseCluster().getMaster().getServerManager().getOnlineServers().size() > 2;
+      }
+    });
+
+    //make sure balancer is in offline mode, since this is what we're testing
+    assertFalse(((GroupBasedLoadBalancer)TEST_UTIL.getHBaseCluster().getMaster().getLoadBalancer()).isOnline());
+    //verify the group affiliation that's loaded from ZK instead of tables
+    assertEquals(newGroup, groupAdmin.getGroupInfoOfTable(GroupInfoManager.GROUP_TABLE_NAME).getName());
+    assertEquals(GroupInfo.OFFLINE_DEFAULT_GROUP, groupAdmin.getGroupInfoOfTable(failoverTable).getName());
+
+    //kill final regionserver to see the failover happens for all tables
+    //except GROUP table since it's group does not have any online RS
+    killRS.stop("die");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return admin.getClusterStatus().getServers().contains(killRS.getServerName());
+      }
+    });
+    master = TEST_UTIL.getHBaseCluster().getMaster();
+    LOG.info("Waiting for new table assignment...");
+    TEST_UTIL.waitForCondition(new PrivilegedExceptionAction<Boolean>() {
+      @Override
+      public Boolean run() throws Exception {
+        return failoverRS.getOnlineRegions(Bytes.toBytes(failoverTable)).size() < 1;
+      }
+    });
+    assertEquals(0, failoverRS.getOnlineRegions(GroupInfoManager.GROUP_TABLE_NAME_BYTES).size());
+    //need this for minicluster to shutdown cleanly
+    master.stopMaster();
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java
new file mode 100644
index 0000000..8f7888c
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/group/VerifyingGroupAdminClient.java
@@ -0,0 +1,128 @@
+package org.apache.hadoop.hbase.group;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.protobuf.generated.GroupInfoProtos;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.type.TypeReference;
+import org.junit.Assert;
+
+import com.google.common.collect.Maps;
+import com.google.common.collect.Sets;
+
+public class VerifyingGroupAdminClient extends GroupAdminClient {
+  private HTable table;
+  private ZooKeeperWatcher zkw;
+
+  public VerifyingGroupAdminClient(Configuration conf)
+      throws IOException {
+    super(conf);
+    table = new HTable(conf, GroupInfoManager.GROUP_TABLE_NAME_BYTES);
+    zkw = new ZooKeeperWatcher(conf, this.getClass().getSimpleName(), null);
+  }
+
+  @Override
+  public void addGroup(String groupName) throws IOException {
+    super.addGroup(groupName);
+    verify();
+  }
+
+  @Override
+  public void moveServers(Set<String> servers, String targetGroup) throws IOException {
+    super.moveServers(servers, targetGroup);
+    verify();
+  }
+
+  @Override
+  public void moveTable(String tableName, String targetGroup) throws IOException {
+    moveTables(Sets.newHashSet(tableName), targetGroup);
+  }
+
+  @Override
+  public void moveTables(Set<String> tables, String targetGroup) throws IOException {
+    super.moveTables(tables, targetGroup);
+    verify();
+  }
+
+  @Override
+  public void removeGroup(String name) throws IOException {
+    super.removeGroup(name);
+    verify();
+  }
+
+  public void verify() throws IOException {
+    ObjectMapper mapper = new ObjectMapper();
+    Get get = new Get(GroupInfoManager.ROW_KEY);
+    get.addFamily(GroupInfoManager.SERVER_FAMILY_BYTES);
+    get.addFamily(GroupInfoManager.TABLE_FAMILY_BYTES);
+    Map<String, GroupInfo> groupMap = Maps.newHashMap();
+    Set<GroupInfo> specialList = Sets.newHashSet();
+    Set<GroupInfo> zList = Sets.newHashSet();
+
+    Result result = table.get(get);
+    if(!result.isEmpty()) {
+      NavigableMap<byte[],NavigableMap<byte[],byte[]>> dataMap =
+          result.getNoVersionMap();
+      for(byte[] groupNameBytes:
+          dataMap.get(GroupInfoManager.SERVER_FAMILY_BYTES).keySet()) {
+        GroupInfoProtos.ServerList serverList =
+            GroupInfoProtos.ServerList.parseFrom(dataMap.get(GroupInfoManager.SERVER_FAMILY_BYTES).get(groupNameBytes));
+        GroupInfoProtos.TableList tableList =
+            GroupInfoProtos.TableList.parseFrom(dataMap.get(GroupInfoManager.TABLE_FAMILY_BYTES).get(groupNameBytes));
+        GroupInfo groupInfo = new GroupInfo(Bytes.toString(groupNameBytes));
+        groupInfo.addAllServers(serverList.getServerList());
+        groupInfo.addAllTables(tableList.getTableList());
+        groupMap.put(groupInfo.getName(), groupInfo);
+        for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+          if(groupInfo.getTables().contains(special)) {
+            specialList.add(groupInfo);
+            break;
+          }
+        }
+      }
+    }
+    groupMap.put(GroupInfo.DEFAULT_GROUP, super.getGroupInfo(GroupInfo.DEFAULT_GROUP));
+    for(String special: GroupBasedLoadBalancer.SPECIAL_TABLES) {
+      GroupInfo groupInfo = groupMap.get(GroupInfo.DEFAULT_GROUP);
+      if(groupInfo.getTables().contains(special)) {
+        specialList.add(groupInfo);
+        break;
+      }
+    }
+    Assert.assertEquals(Sets.newHashSet(groupMap.values()),
+        Sets.newHashSet(super.listGroups()));
+    try {
+      String groupBasePath = ZKUtil.joinZNode(zkw.baseZNode, "groupInfo");
+      for(String zNode: ZKUtil.listChildrenNoWatch(zkw, groupBasePath)) {
+        String groupPath = ZKUtil.joinZNode(groupBasePath, zNode);
+        byte[] data = ZKUtil.getData(zkw, groupPath);
+        zList.add(GroupInfo.convert(GroupInfoProtos.GroupInfo.parseFrom(data)));
+      }
+      Assert.assertEquals(zList.size(), specialList.size());
+      for(GroupInfo groupInfo: zList) {
+        if(groupInfo.getName().equals(GroupInfo.OFFLINE_DEFAULT_GROUP)) {
+          Assert.assertEquals(groupMap.get(GroupInfo.DEFAULT_GROUP).getServers(), groupInfo.getServers());
+          Assert.assertEquals(groupMap.get(GroupInfo.DEFAULT_GROUP).getTables(), groupInfo.getTables());
+        } else {
+          Assert.assertTrue(specialList.contains(groupInfo));
+        }
+      }
+    } catch (KeeperException e) {
+      throw new IOException("ZK verification failed", e);
+    }
+  }
+
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index e8cccd8..c60894d 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -301,6 +301,11 @@ public class TestCatalogJanitor {
     }
 
     @Override
+    public LoadBalancer getLoadBalancer() {
+      return null;
+    }
+
+    @Override
     public void deleteTable(byte[] tableName) throws IOException { }
 
     @Override
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
index 0264f56..401c7b9 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestBaseLoadBalancer.java
@@ -20,6 +20,7 @@ package org.apache.hadoop.hbase.master.balancer;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -67,6 +68,10 @@ public class TestBaseLoadBalancer extends BalancerTestBase {
       return null;
     }
 
+    @Override
+    public void configure() throws IOException {
+    }
+
   }
 
   /**
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestGroupBasedLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestGroupBasedLoadBalancer.java
new file mode 100644
index 0000000..80570e2
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestGroupBasedLoadBalancer.java
@@ -0,0 +1,577 @@
+/**
+ * Copyright The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master.balancer;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.security.SecureRandom;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.group.GroupBasedLoadBalancer;
+import org.apache.hadoop.hbase.group.GroupInfo;
+import org.apache.hadoop.hbase.group.GroupInfoManager;
+import org.apache.hadoop.hbase.master.AssignmentManager;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.Mockito;
+
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.Lists;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+/**
+ * Unit test for Region Server Groups' Balancer
+ */
+@Category(SmallTests.class)
+public class TestGroupBasedLoadBalancer {
+
+  private static final Log LOG = LogFactory.getLog(TestGroupBasedLoadBalancer.class);
+  private static GroupBasedLoadBalancer loadBalancer;
+  private static SecureRandom rand;
+
+  static String[]  groups = new String[] { GroupInfo.DEFAULT_GROUP, "dg2", "dg3",
+      "dg4" };
+  static String[] tables = new String[] { "dt1", "dt2", "dt3", "dt4" };
+  static List<ServerName> servers;
+  static Map<String, GroupInfo> groupMap;
+  static Map<String, String> tableMap;
+  static List<HTableDescriptor> tableDescs;
+  int[] regionAssignment = new int[] { 2, 5, 7, 10, 4, 3, 1 };
+  static int regionId = 0;
+
+  @BeforeClass
+  public static void beforeAllTests() throws Exception {
+    rand = new SecureRandom();
+    servers = generateServers(7);
+    groupMap = constructGroupInfo(servers, groups);
+    tableMap = new HashMap<String, String>();
+    tableDescs = constructTableDesc();
+    Configuration conf = HBaseConfiguration.create();
+    conf.set("hbase.regions.slop", "0");
+    conf.setBoolean(HConstants.ZOOKEEPER_USEMULTI, true);
+    loadBalancer = new GroupBasedLoadBalancer(getMockedGroupInfoManager());
+    loadBalancer.setMasterServices(getMockedMaster());
+    loadBalancer.setConf(conf);
+    loadBalancer.configure();
+  }
+
+  /**
+   * Test the load balancing algorithm.
+   *
+   * Invariant is that all servers of the group should be hosting either floor(average) or
+   * ceiling(average)
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testBalanceCluster() throws Exception {
+    Map<ServerName, List<HRegionInfo>> servers = mockClusterServers();
+    ArrayListMultimap<String, ServerAndLoad> list = convertToGroupBasedMap(servers);
+    LOG.info("Mock Cluster :  " + printStats(list));
+    List<RegionPlan> plans = loadBalancer.balanceCluster(servers);
+    ArrayListMultimap<String, ServerAndLoad> balancedCluster = reconcile(
+        list, plans);
+    LOG.info("Mock Balance : " + printStats(balancedCluster));
+    assertClusterAsBalanced(balancedCluster);
+  }
+
+  /**
+   * Invariant is that all servers of a group have load between floor(avg) and
+   * ceiling(avg) number of regions.
+   */
+  private void assertClusterAsBalanced(
+      ArrayListMultimap<String, ServerAndLoad> groupLoadMap) {
+    for (String gName : groupLoadMap.keySet()) {
+      List<ServerAndLoad> groupLoad = groupLoadMap.get(gName);
+      int numServers = groupLoad.size();
+      int numRegions = 0;
+      int maxRegions = 0;
+      int minRegions = Integer.MAX_VALUE;
+      for (ServerAndLoad server : groupLoad) {
+        int nr = server.getLoad();
+        if (nr > maxRegions) {
+          maxRegions = nr;
+        }
+        if (nr < minRegions) {
+          minRegions = nr;
+        }
+        numRegions += nr;
+      }
+      if (maxRegions - minRegions < 2) {
+        // less than 2 between max and min, can't balance
+        return;
+      }
+      int min = numRegions / numServers;
+      int max = numRegions % numServers == 0 ? min : min + 1;
+
+      for (ServerAndLoad server : groupLoad) {
+        assertTrue(server.getLoad() <= max);
+        assertTrue(server.getLoad() >= min);
+      }
+    }
+  }
+
+  /**
+   * Tests immediate assignment.
+   *
+   * Invariant is that all regions have an assignment.
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testImmediateAssignment() throws Exception {
+    List<HRegionInfo> regions = randomRegions(20);
+    Map<HRegionInfo, ServerName> assignments = loadBalancer
+        .immediateAssignment(regions, servers);
+    assertImmediateAssignment(regions, servers, assignments);
+  }
+
+  /**
+   * All regions have an assignment.
+   *
+   * @param regions
+   * @param servers
+   * @param assignments
+   * @throws IOException
+   * @throws FileNotFoundException
+   */
+  private void assertImmediateAssignment(List<HRegionInfo> regions,
+                                         List<ServerName> servers, Map<HRegionInfo, ServerName> assignments)
+      throws FileNotFoundException, IOException {
+    for (HRegionInfo region : regions) {
+      assertTrue(assignments.containsKey(region));
+      ServerName server = assignments.get(region);
+      String tableName = region.getTableNameAsString();
+      GroupInfo gInfo =
+          loadBalancer.getGroupInfoManager().getGroupOfTable(tableName);
+      assertNotNull(gInfo);
+      assertTrue("Region is not correctly assigned to group servers.",
+          gInfo.containsServer(server.getHostAndPort()));
+    }
+  }
+
+  /**
+   * Tests the bulk assignment used during cluster startup.
+   *
+   * Round-robin. Should yield a balanced cluster so same invariant as the
+   * load balancer holds, all servers holding either floor(avg) or
+   * ceiling(avg).
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testBulkAssignment() throws Exception {
+    List<HRegionInfo> regions = randomRegions(25);
+    Map<ServerName, List<HRegionInfo>> assignments = loadBalancer
+        .roundRobinAssignment(regions, servers);
+    //test empty region/servers scenario
+    //this should not throw an NPE
+    loadBalancer.roundRobinAssignment(regions,
+        Collections.EMPTY_LIST);
+    //test regular scenario
+    assertTrue(assignments.keySet().size() == servers.size());
+    for (ServerName sn : assignments.keySet()) {
+      List<HRegionInfo> regionAssigned = assignments.get(sn);
+      for (HRegionInfo region : regionAssigned) {
+        String tableName = region.getTableNameAsString();
+        GroupInfo gInfo =
+            getMockedGroupInfoManager().getGroupOfTable(tableName);
+        assertNotNull(gInfo);
+        assertTrue(
+            "Region is not correctly assigned to group servers.",
+            gInfo.containsServer(sn.getHostAndPort()));
+      }
+    }
+    ArrayListMultimap<String, ServerAndLoad> loadMap = convertToGroupBasedMap(assignments);
+    assertClusterAsBalanced(loadMap);
+  }
+
+  /**
+   * Test the cluster startup bulk assignment which attempts to retain
+   * assignment info.
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testRetainAssignment() throws Exception {
+    // Test simple case where all same servers are there
+    Map<ServerName, List<HRegionInfo>> currentAssignments = mockClusterServers();
+    Map<HRegionInfo, ServerName> inputForTest = new HashMap<HRegionInfo, ServerName>();
+    for (ServerName sn : currentAssignments.keySet()) {
+      for (HRegionInfo region : currentAssignments.get(sn)) {
+        inputForTest.put(region, sn);
+      }
+    }
+    //verify region->null server assignment is handled
+    inputForTest.put(randomRegions(1).get(0), null);
+    Map<ServerName, List<HRegionInfo>> newAssignment = loadBalancer
+        .retainAssignment(inputForTest, servers);
+    assertRetainedAssignment(inputForTest, servers, newAssignment);
+  }
+
+  /**
+   * Asserts a valid retained assignment plan.
+   * <p>
+   * Must meet the following conditions:
+   * <ul>
+   * <li>Every input region has an assignment, and to an online server
+   * <li>If a region had an existing assignment to a server with the same
+   * address a a currently online server, it will be assigned to it
+   * </ul>
+   *
+   * @param existing
+   * @param assignment
+   * @throws IOException
+   * @throws FileNotFoundException
+   */
+  private void assertRetainedAssignment(
+      Map<HRegionInfo, ServerName> existing, List<ServerName> servers,
+      Map<ServerName, List<HRegionInfo>> assignment)
+      throws FileNotFoundException, IOException {
+    // Verify condition 1, every region assigned, and to online server
+    Set<ServerName> onlineServerSet = new TreeSet<ServerName>(servers);
+    Set<HRegionInfo> assignedRegions = new TreeSet<HRegionInfo>();
+    for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+      assertTrue(
+          "Region assigned to server that was not listed as online",
+          onlineServerSet.contains(a.getKey()));
+      for (HRegionInfo r : a.getValue())
+        assignedRegions.add(r);
+    }
+    assertEquals(existing.size(), assignedRegions.size());
+
+    // Verify condition 2, every region must be assigned to correct server.
+    Set<String> onlineHostNames = new TreeSet<String>();
+    for (ServerName s : servers) {
+      onlineHostNames.add(s.getHostname());
+    }
+
+    for (Map.Entry<ServerName, List<HRegionInfo>> a : assignment.entrySet()) {
+      ServerName currentServer = a.getKey();
+      for (HRegionInfo r : a.getValue()) {
+        ServerName oldAssignedServer = existing.get(r);
+        String tableName = r.getTableNameAsString();
+        GroupInfo gInfo =
+            getMockedGroupInfoManager().getGroupOfTable(tableName);
+        assertNotNull(gInfo);
+        assertTrue(
+            "Region is not correctly assigned to group servers.",
+            gInfo.containsServer(currentServer.getHostAndPort()));
+        if (oldAssignedServer != null
+            && onlineHostNames.contains(oldAssignedServer
+            .getHostname())) {
+          // this region was previously assigned somewhere, and that
+          // host is still around, then the host must have been is a
+          // different group.
+          if (oldAssignedServer.getHostAndPort().equals(
+              currentServer.getHostAndPort()) == false) {
+            assertFalse(gInfo.containsServer(oldAssignedServer
+                .getHostAndPort()));
+          }
+        }
+      }
+    }
+  }
+
+  private String printStats(
+      ArrayListMultimap<String, ServerAndLoad> groupBasedLoad) {
+    StringBuffer sb = new StringBuffer();
+    sb.append("\n");
+    for (String groupName : groupBasedLoad.keySet()) {
+      sb.append("Stats for group: " + groupName);
+      sb.append("\n");
+      sb.append(groupMap.get(groupName).getServers());
+      sb.append("\n");
+      List<ServerAndLoad> groupLoad = groupBasedLoad.get(groupName);
+      int numServers = groupLoad.size();
+      int totalRegions = 0;
+      sb.append("Per Server Load: \n");
+      for (ServerAndLoad sLoad : groupLoad) {
+        sb.append("Server :" + sLoad.getServerName() + " Load : "
+            + sLoad.getLoad() + "\n");
+        totalRegions += sLoad.getLoad();
+      }
+      sb.append(" Group Statistics : \n");
+      float average = (float) totalRegions / numServers;
+      int max = (int) Math.ceil(average);
+      int min = (int) Math.floor(average);
+      sb.append("[srvr=" + numServers + " rgns=" + totalRegions + " avg="
+          + average + " max=" + max + " min=" + min + "]");
+      sb.append("\n");
+      sb.append("===============================");
+      sb.append("\n");
+    }
+    return sb.toString();
+  }
+
+  private ArrayListMultimap<String, ServerAndLoad> convertToGroupBasedMap(
+      final Map<ServerName, List<HRegionInfo>> serversMap) throws IOException {
+    ArrayListMultimap<String, ServerAndLoad> loadMap = ArrayListMultimap
+        .create();
+    for (GroupInfo gInfo : getMockedGroupInfoManager().listGroups()) {
+      Set<String> groupServers = gInfo.getServers();
+      for (String hostAndPort : groupServers) {
+        ServerName actual = ServerName.findServerWithSameHostnamePort(
+            servers, ServerName.parseServerName(hostAndPort));
+        List<HRegionInfo> regions = serversMap.get(actual);
+        assertTrue("No load for " + actual, regions != null);
+        loadMap.put(gInfo.getName(),
+            new ServerAndLoad(actual, regions.size()));
+      }
+    }
+    return loadMap;
+  }
+
+  private ArrayListMultimap<String, ServerAndLoad> reconcile(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      List<RegionPlan> plans) {
+    ArrayListMultimap<String, ServerAndLoad> result = ArrayListMultimap
+        .create();
+    result.putAll(previousLoad);
+    if (plans != null) {
+      for (RegionPlan plan : plans) {
+        ServerName source = plan.getSource();
+        updateLoad(result, source, -1);
+        ServerName destination = plan.getDestination();
+        updateLoad(result, destination, +1);
+      }
+    }
+    return result;
+  }
+
+  private void updateLoad(
+      ArrayListMultimap<String, ServerAndLoad> previousLoad,
+      final ServerName sn, final int diff) {
+    for (String groupName : previousLoad.keySet()) {
+      ServerAndLoad newSAL = null;
+      ServerAndLoad oldSAL = null;
+      for (ServerAndLoad sal : previousLoad.get(groupName)) {
+        if (ServerName.isSameHostnameAndPort(sn, sal.getServerName())) {
+          oldSAL = sal;
+          newSAL = new ServerAndLoad(sn, sal.getLoad() + diff);
+          break;
+        }
+      }
+      if (newSAL != null) {
+        previousLoad.remove(groupName, oldSAL);
+        previousLoad.put(groupName, newSAL);
+        break;
+      }
+    }
+  }
+
+  private Map<ServerName, List<HRegionInfo>> mockClusterServers() throws IOException {
+    assertTrue(servers.size() == regionAssignment.length);
+    Map<ServerName, List<HRegionInfo>> assignment = new TreeMap<ServerName, List<HRegionInfo>>();
+    for (int i = 0; i < servers.size(); i++) {
+      int numRegions = regionAssignment[i];
+      List<HRegionInfo> regions = assignedRegions(numRegions, servers.get(i));
+      assignment.put(servers.get(i), regions);
+    }
+    return assignment;
+  }
+
+  /**
+   * Generate a list of regions evenly distributed between the tables.
+   *
+   * @param numRegions The number of regions to be generated.
+   * @return List of HRegionInfo.
+   */
+  private List<HRegionInfo> randomRegions(int numRegions) {
+    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    byte[] start = new byte[16];
+    byte[] end = new byte[16];
+    rand.nextBytes(start);
+    rand.nextBytes(end);
+    int regionIdx = rand.nextInt(tables.length);
+    for (int i = 0; i < numRegions; i++) {
+      Bytes.putInt(start, 0, numRegions << 1);
+      Bytes.putInt(end, 0, (numRegions << 1) + 1);
+      int tableIndex = (i + regionIdx) % tables.length;
+      HRegionInfo hri = new HRegionInfo(
+          Bytes.toBytes(tables[tableIndex]), start, end, false,
+          regionId++);
+      regions.add(hri);
+    }
+    return regions;
+  }
+
+  /**
+   * Generate assigned regions to a given server using group information.
+   *
+   * @param numRegions the num regions to generate
+   * @param sn the servername
+   * @return the list of regions
+   * @throws IOException Signals that an I/O exception has occurred.
+   */
+  private List<HRegionInfo> assignedRegions(int numRegions, ServerName sn) throws IOException {
+    List<HRegionInfo> regions = new ArrayList<HRegionInfo>(numRegions);
+    byte[] start = new byte[16];
+    byte[] end = new byte[16];
+    Bytes.putInt(start, 0, numRegions << 1);
+    Bytes.putInt(end, 0, (numRegions << 1) + 1);
+    for (int i = 0; i < numRegions; i++) {
+      String tableName = getTableName(sn);
+      HRegionInfo hri = new HRegionInfo(
+          Bytes.toBytes(tableName), start, end, false,
+          regionId++);
+      regions.add(hri);
+    }
+    return regions;
+  }
+
+  private static List<ServerName> generateServers(int numServers) {
+    List<ServerName> servers = new ArrayList<ServerName>(numServers);
+    for (int i = 0; i < numServers; i++) {
+      String host = "server" + rand.nextInt(100000);
+      int port = rand.nextInt(60000);
+      servers.add(new ServerName(host, port, -1));
+    }
+    return servers;
+  }
+
+  /**
+   * Construct group info, with each group having at least one server.
+   *
+   * @param servers the servers
+   * @param groups the groups
+   * @return the map
+   */
+  private static Map<String, GroupInfo> constructGroupInfo(
+      List<ServerName> servers, String[] groups) {
+    assertTrue(servers != null);
+    assertTrue(servers.size() >= groups.length);
+    int index = 0;
+    Map<String, GroupInfo> groupMap = new HashMap<String, GroupInfo>();
+    for (String grpName : groups) {
+      GroupInfo groupInfo = new GroupInfo(grpName);
+      groupInfo.addServer(servers.get(index).getHostAndPort());
+      groupMap.put(grpName, groupInfo);
+      index++;
+    }
+    while (index < servers.size()) {
+      int grpIndex = rand.nextInt(groups.length);
+      groupMap.get(groups[grpIndex]).addServer(
+          servers.get(index).getHostAndPort());
+      index++;
+    }
+    return groupMap;
+  }
+
+  /**
+   * Construct table descriptors evenly distributed between the groups.
+   *
+   * @return the list
+   */
+  private static List<HTableDescriptor> constructTableDesc() {
+    List<HTableDescriptor> tds = Lists.newArrayList();
+    int index = rand.nextInt(groups.length);
+    for (int i = 0; i < tables.length; i++) {
+      HTableDescriptor htd = new HTableDescriptor(tables[i]);
+      int grpIndex = (i + index) % groups.length ;
+      String groupName = groups[grpIndex];
+      tableMap.put(tables[i], groupName);
+      tds.add(htd);
+    }
+    return tds;
+  }
+
+  private static MasterServices getMockedMaster() throws IOException {
+    TableDescriptors tds = Mockito.mock(TableDescriptors.class);
+    Mockito.when(tds.get(tables[0])).thenReturn(tableDescs.get(0));
+    Mockito.when(tds.get(tables[1])).thenReturn(tableDescs.get(1));
+    Mockito.when(tds.get(tables[2])).thenReturn(tableDescs.get(2));
+    Mockito.when(tds.get(tables[3])).thenReturn(tableDescs.get(3));
+    MasterServices services = Mockito.mock(HMaster.class);
+    Mockito.when(services.getTableDescriptors()).thenReturn(tds);
+    AssignmentManager am = Mockito.mock(AssignmentManager.class);
+    Mockito.when(services.getAssignmentManager()).thenReturn(am);
+    return services;
+  }
+
+  private static GroupInfoManager getMockedGroupInfoManager() throws IOException {
+    GroupInfoManager gm = Mockito.mock(GroupInfoManager.class);
+    Mockito.when(gm.getGroup(groups[0])).thenReturn(
+        groupMap.get(groups[0]));
+    Mockito.when(gm.getGroup(groups[1])).thenReturn(
+        groupMap.get(groups[1]));
+    Mockito.when(gm.getGroup(groups[2])).thenReturn(
+        groupMap.get(groups[2]));
+    Mockito.when(gm.getGroup(groups[3])).thenReturn(
+        groupMap.get(groups[3]));
+    Mockito.when(gm.listGroups()).thenReturn(
+        Lists.newLinkedList(groupMap.values()));
+    Mockito.when(gm.isOnline()).thenReturn(true);
+    Mockito.when(gm.getGroupOfTable(Mockito.anyString())).thenAnswer(new Answer<GroupInfo>() {
+      @Override
+      public GroupInfo answer(InvocationOnMock invocation) throws Throwable {
+        return groupMap.get(tableMap.get(invocation.getArguments()[0]));
+      }
+    });
+    return gm;
+  }
+
+  private String getTableName(ServerName sn) throws IOException{
+    String tableName = null;
+    GroupInfoManager gm = getMockedGroupInfoManager();
+    GroupInfo groupOfServer = null;
+    for(GroupInfo gInfo : gm.listGroups()){
+      if(gInfo.containsServer(sn.getHostAndPort())){
+        groupOfServer = gInfo;
+        break;
+      }
+    }
+
+    for(HTableDescriptor desc : tableDescs){
+      if(gm.getGroupOfTable(desc.getNameAsString()).getName().endsWith(groupOfServer.getName())){
+        tableName = desc.getNameAsString();
+      }
+    }
+    return tableName;
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
index 1e8a798..dc08b54 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSplitTransactionOnCluster.java
@@ -876,8 +876,7 @@ public class TestSplitTransactionOnCluster {
    */
   private int ensureTableRegionNotOnSameServerAsMeta(final HBaseAdmin admin,
       final HRegionInfo hri)
-  throws HBaseIOException, MasterNotRunningException,
-  ZooKeeperConnectionException, InterruptedException {
+      throws HBaseIOException, MasterNotRunningException, ZooKeeperConnectionException, InterruptedException {
     // Now make sure that the table region is not on same server as that hosting
     // .META.  We don't want .META. replay polluting our test when we later crash
     // the table region serving server.
