diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
index 7436b71..bf297ef 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.java
@@ -304,7 +304,7 @@
   private IOEngine getIOEngineFromName(String ioEngineName, long capacity)
       throws IOException {
     if (ioEngineName.startsWith("file:")) {
-      return new FileIOEngine(ioEngineName.substring(5), capacity);
+      return new FileIOEngine(ioEngineName.substring(5).split(","), capacity);
     } else if (ioEngineName.startsWith("offheap")) {
       return new ByteBufferIOEngine(capacity, true);
     } else if (ioEngineName.startsWith("heap")) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java
index aaf5cf9..c3d796f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.java
@@ -22,6 +22,8 @@
 import java.io.RandomAccessFile;
 import java.nio.ByteBuffer;
 import java.nio.channels.FileChannel;
+import java.util.ArrayList;
+import java.util.List;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -39,38 +41,65 @@
 @InterfaceAudience.Private
 public class FileIOEngine implements IOEngine {
   private static final Log LOG = LogFactory.getLog(FileIOEngine.class);
-  private final RandomAccessFile raf;
-  private final FileChannel fileChannel;
-  private final String path;
-  private long size;
+  private final List<RandomAccessFile> rafs;
+  private final List<FileChannel> fileChannels;
+  private final List<String> paths;
+  private final int numFiles;
+  private final long[] fileEndSizes;
+  private final long fileSize;
+  private long totalSize;
 
-  public FileIOEngine(String filePath, long fileSize) throws IOException {
-    this.path = filePath;
-    this.size = fileSize;
-    try {
-      raf = new RandomAccessFile(filePath, "rw");
-    } catch (java.io.FileNotFoundException fex) {
-      LOG.error("Can't create bucket cache file " + filePath, fex);
-      throw fex;
-    }
-
-    try {
+  public FileIOEngine(String [] filePaths, long totalSize) throws IOException {
+    this.totalSize = totalSize;
+    this.numFiles = filePaths.length;
+    this.fileSize = totalSize / numFiles;
+    this.fileEndSizes = new long[numFiles];
+    rafs = new ArrayList<RandomAccessFile>(numFiles);
+    fileChannels = new ArrayList<FileChannel>(numFiles);
+    paths = new ArrayList<String>(numFiles);
+    for (int i = 0; i < numFiles; i++) {
+      fileEndSizes[i] = fileSize * (i + 1);
+      filePaths[i] = filePaths[i].trim();
+      this.paths.add(filePaths[i]);
+      RandomAccessFile raf;
+      try {
+        raf = new RandomAccessFile(filePaths[i], "rw");
+      } catch (java.io.FileNotFoundException fex) {
+        LOG.error("Can't create bucket cache file " + filePaths[i], fex);
+        throw fex;
+      }
+      
+      try {
       raf.setLength(fileSize);
-    } catch (IOException ioex) {
-      LOG.error("Can't extend bucket cache file; insufficient space for "
-          + StringUtils.byteDesc(fileSize), ioex);
-      raf.close();
-      throw ioex;
-    }
-
-    fileChannel = raf.getChannel();
-    LOG.info("Allocating " + StringUtils.byteDesc(fileSize) + ", on the path:" + filePath);
+      } catch (IOException ioex) {
+        LOG.error("Can't extend bucket cache file " + filePaths[i] + "; insufficient space for "
+            + StringUtils.byteDesc(fileSize), ioex);
+        raf.close();
+        throw ioex;
+      }
+      this.rafs.add(raf);
+      FileChannel fileChannel = raf.getChannel();
+      this.fileChannels.add(fileChannel);
+      LOG.info("Allocating " + StringUtils.byteDesc(fileSize) + ", on the path:" + filePaths[i]);
+     }
   }
-
+  
   @Override
   public String toString() {
-    return "ioengine=" + this.getClass().getSimpleName() + ", path=" + this.path +
-      ", size=" + String.format("%,d", this.size);
+    return "ioengine=" + this.getClass().getSimpleName() + ", path=" + this.paths +
+      ", size=" + String.format("%,d", this.totalSize);
+  }
+  
+  /**
+  * Obtain the fileChannel corresponding to the given offset.
+  * @param offset The offset in the total size of IOEngine
+  * @return fileChannel
+  */
+  public FileChannel getFileChannel(long offset) {
+    for (int i = 0; i < numFiles; ++i) 
+        if (offset < fileEndSizes[i])
+           return fileChannels.get(i);
+    return null;
   }
 
   /**
@@ -84,7 +113,7 @@
 
   /**
    * Transfers data from file to the given byte buffer
-   * @param offset The offset in the file where the first byte to be read
+   * @param offset The offset in the total size of IOEngine where the first byte to be read
    * @param length The length of buffer that should be allocated for reading
    *               from the file channel
    * @return number of bytes read
@@ -94,7 +123,8 @@
   public Cacheable read(long offset, int length, CacheableDeserializer<Cacheable> deserializer)
       throws IOException {
     ByteBuffer dstBuffer = ByteBuffer.allocate(length);
-    fileChannel.read(dstBuffer, offset);
+    long fileOffset = offset % fileSize;
+    getFileChannel(offset).read(dstBuffer, fileOffset);
     // The buffer created out of the fileChannel is formed by copying the data from the file
     // Hence in this case there is no shared memory that we point to. Even if the BucketCache evicts
     // this buffer from the file the data is already copied and there is no need to ensure that
@@ -109,12 +139,13 @@
   /**
    * Transfers data from the given byte buffer to file
    * @param srcBuffer the given byte buffer from which bytes are to be read
-   * @param offset The offset in the file where the first byte to be written
+   * @param offset The offset in the total size of IOEngine where the first byte to be written
    * @throws IOException
    */
   @Override
   public void write(ByteBuffer srcBuffer, long offset) throws IOException {
-    fileChannel.write(srcBuffer, offset);
+    long fileOffset = offset % fileSize;
+    getFileChannel(offset).write(srcBuffer, fileOffset);
   }
 
   /**
@@ -123,7 +154,8 @@
    */
   @Override
   public void sync() throws IOException {
-    fileChannel.force(true);
+    for (int i = 0; i < numFiles; i++)
+      fileChannels.get(i).force(true);
   }
 
   /**
@@ -131,15 +163,17 @@
    */
   @Override
   public void shutdown() {
-    try {
-      fileChannel.close();
-    } catch (IOException ex) {
-      LOG.error("Can't shutdown cleanly", ex);
-    }
-    try {
-      raf.close();
-    } catch (IOException ex) {
-      LOG.error("Can't shutdown cleanly", ex);
+    for (int i = 0; i < numFiles; i++) {
+      try {
+        fileChannels.get(i).close();
+      } catch (IOException ex) {
+        LOG.error("Can't shutdown cleanly", ex);
+      }
+      try {
+        rafs.get(i).close();
+      } catch (IOException ex) {
+        LOG.error("Can't shutdown cleanly", ex);
+      }
     }
   }
 
@@ -147,7 +181,9 @@
   public void write(ByteBuff srcBuffer, long offset) throws IOException {
     // When caching block into BucketCache there will be single buffer backing for this HFileBlock.
     assert srcBuffer.hasArray();
-    fileChannel.write(
-        ByteBuffer.wrap(srcBuffer.array(), srcBuffer.arrayOffset(), srcBuffer.remaining()), offset);
+    long fileOffset = offset % fileSize;
+    getFileChannel(offset).write(
+      ByteBuffer.wrap(srcBuffer.array(), srcBuffer.arrayOffset(), srcBuffer.remaining()),
+      fileOffset);
   }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestFileIOEngine.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestFileIOEngine.java
index 93f4cf7..abb00a6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestFileIOEngine.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/bucket/TestFileIOEngine.java
@@ -39,7 +39,7 @@
   @Test
   public void testFileIOEngine() throws IOException {
     int size = 2 * 1024 * 1024; // 2 MB
-    String filePath = "testFileIOEngine";
+    String [] filePath = {"testFileIOEngine"};
     try {
       FileIOEngine fileIOEngine = new FileIOEngine(filePath, size);
       for (int i = 0; i < 50; i++) {
@@ -58,11 +58,45 @@
         }
       }
     } finally {
-      File file = new File(filePath);
+      File file = new File(filePath[0]);
       if (file.exists()) {
         file.delete();
       }
     }
 
   }
+  
+  @Test
+  public void testMultiFileIOEngine() throws IOException {
+    int size = 2 * 1024 * 1024; // 2 MB
+    String [] filePath = {"testFileIOEngine1","testFileIOEngine2"};
+    try {
+      FileIOEngine fileIOEngine = new FileIOEngine(filePath, size);
+      for (int i = 0; i < 50; i++) {
+        int len = (int) Math.floor(Math.random() * 100);
+        long offset = (long) Math.floor(Math.random() * size % (size - len));
+        byte[] data1 = new byte[len];
+        for (int j = 0; j < data1.length; ++j) {
+          data1[j] = (byte) (Math.random() * 255);
+        }
+        fileIOEngine.write(ByteBuffer.wrap(data1), offset);
+        BufferGrabbingDeserializer deserializer = new BufferGrabbingDeserializer();
+        fileIOEngine.read(offset, len, deserializer);
+        ByteBuff data2 = deserializer.getDeserializedByteBuff();
+        for (int j = 0; j < data1.length; ++j) {
+          assertTrue(data1[j] == data2.get(j));
+        }
+      }
+    } finally {
+      File file1 = new File(filePath[0]);
+      if (file1.exists()) {
+        file1.delete();
+      }
+      File file2 = new File(filePath[1]);
+      if (file2.exists()) {
+        file2.delete();
+      }
+    }
+
+  }
 }
