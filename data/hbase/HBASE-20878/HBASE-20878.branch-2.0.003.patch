diff --git a/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto b/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto
index c2c6dce..8181e0c 100644
--- a/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto
+++ b/hbase-protocol-shaded/src/main/protobuf/MasterProcedure.proto
@@ -266,6 +266,7 @@ enum MergeTableRegionsState {
   MERGE_TABLE_REGIONS_POST_MERGE_COMMIT_OPERATION = 9;
   MERGE_TABLE_REGIONS_OPEN_MERGED_REGION = 10;
   MERGE_TABLE_REGIONS_POST_OPERATION = 11;
+  MERGE_TABLE_REGIONS_CHECK_CLOSED_REGIONS = 12;
 }
 
 message MergeTableRegionsStateData {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
index 57e71f8..d0a0601 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java
@@ -23,6 +23,7 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
+import java.util.NavigableSet;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -54,6 +55,7 @@ import org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil;
 import org.apache.hadoop.hbase.procedure2.ProcedureMetrics;
 import org.apache.hadoop.hbase.procedure2.ProcedureStateSerializer;
 import org.apache.hadoop.hbase.quotas.QuotaExceededException;
+import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.HRegionFileSystem;
 import org.apache.hadoop.hbase.regionserver.HStoreFile;
 import org.apache.hadoop.hbase.regionserver.StoreFileInfo;
@@ -219,7 +221,7 @@ public class MergeTableRegionsProcedure
 
   @Override
   protected Flow executeFromState(final MasterProcedureEnv env,
-      final MergeTableRegionsState state) {
+      MergeTableRegionsState state) {
     LOG.trace("{} execute state={}", this, state);
     try {
       switch (state) {
@@ -236,6 +238,10 @@ public class MergeTableRegionsProcedure
           break;
         case MERGE_TABLE_REGIONS_CLOSE_REGIONS:
           addChildProcedure(createUnassignProcedures(env, getRegionReplication(env)));
+          setNextState(MergeTableRegionsState.MERGE_TABLE_REGIONS_CHECK_CLOSED_REGIONS);
+          break;
+        case MERGE_TABLE_REGIONS_CHECK_CLOSED_REGIONS:
+          checkClosedRegions(env);
           setNextState(MergeTableRegionsState.MERGE_TABLE_REGIONS_CREATE_MERGED_REGION);
           break;
         case MERGE_TABLE_REGIONS_CREATE_MERGED_REGION:
@@ -315,6 +321,7 @@ public class MergeTableRegionsProcedure
           cleanupMergedRegion(env);
           break;
         case MERGE_TABLE_REGIONS_CLOSE_REGIONS:
+        case MERGE_TABLE_REGIONS_CHECK_CLOSED_REGIONS:
           rollbackCloseRegionsForMerge(env);
           break;
         case MERGE_TABLE_REGIONS_PRE_MERGE_OPERATION:
@@ -462,6 +469,45 @@ public class MergeTableRegionsProcedure
   }
 
   /**
+   * if the closed regions are on servers not online, that means the region might not
+   * closed properly, we need to abort this merge procedure, and online those regions.
+   * So that the region's WAL can be replayed. Otherwise, data will loss.
+   * @param env the master env
+   * @throws IOException IOException
+   */
+  private void checkClosedRegions(final MasterProcedureEnv env) throws IOException {
+    checkRecoveredEdit(env, regionsToMerge[0]);
+    checkRecoveredEdit(env, regionsToMerge[1]);
+  }
+
+  /**
+   * Check whether there is recovered.edits in the closed region
+   * If any, that means this region is not closed property, we need
+   * to abort region merge to prevent data loss
+   * @param env the master env
+   * @param regionInfo the region to check
+   * @throws IOException IOException
+   */
+  private void checkRecoveredEdit(final MasterProcedureEnv env,
+      final RegionInfo regionInfo) throws IOException {
+    Path rootDir = FSUtils.getRootDir(env.getMasterConfiguration());
+    //Only default replica region can be merged, so we can use regioninfo
+    //directly without converting it to default replica's regioninfo.
+    Path regionDir = HRegion.getRegionDir(rootDir, regionInfo);
+    NavigableSet<Path> files = WALSplitter
+        .getSplitEditFilesSorted(env.getMasterServices().getFileSystem(),
+            regionDir);
+    if (files == null || files.size() == 0) {
+      //There is no recovered.edit in the region dir, we can safely proceed.
+    } else {
+      throw new IOException("Recovered.edits are found in Region: " + regionInfo
+          + ", abort merge to prevent data loss");
+    }
+
+
+  }
+
+  /**
    * Prepare merge and do some check
    * @param env MasterProcedureEnv
    * @throws IOException
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMergeTableRegionsWhileRSCrash.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMergeTableRegionsWhileRSCrash.java
new file mode 100644
index 0000000..b979bb8
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/TestMergeTableRegionsWhileRSCrash.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.master;
+
+import java.util.List;
+import java.util.concurrent.CountDownLatch;
+
+import org.apache.hadoop.hbase.HBaseClassTestRule;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.RegionInfo;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.ResultScanner;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure;
+import org.apache.hadoop.hbase.master.assignment.UnassignProcedure;
+import org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv;
+import org.apache.hadoop.hbase.procedure2.ProcedureExecutor;
+import org.apache.hadoop.hbase.testclassification.MasterTests;
+import org.apache.hadoop.hbase.testclassification.MediumTests;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+
+@Category({MasterTests.class, MediumTests.class})
+public class TestMergeTableRegionsWhileRSCrash {
+
+  @ClassRule
+  public static final HBaseClassTestRule CLASS_RULE =
+      HBaseClassTestRule.forClass(TestMergeTableRegionsWhileRSCrash.class);
+
+  private static final Logger LOG = LoggerFactory
+      .getLogger(TestMergeTableRegionsWhileRSCrash.class);
+
+  protected static final HBaseTestingUtility UTIL = new HBaseTestingUtility();
+  private static TableName TABLE_NAME = TableName.valueOf("test");
+  private static Admin admin;
+  private static byte[] CF = Bytes.toBytes("cf");
+  private static byte[] SPLITKEY = Bytes.toBytes("row5");
+  private static CountDownLatch mergeCommitArrive = new CountDownLatch(1);
+  private static Table TABLE;
+
+
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    UTIL.startMiniCluster(1);
+    admin = UTIL.getHBaseAdmin();
+    byte[][] splitKeys = new byte[1][];
+    splitKeys[0] = SPLITKEY;
+    TABLE = UTIL.createTable(TABLE_NAME, CF, splitKeys);
+    UTIL.waitTableAvailable(TABLE_NAME);
+  }
+
+  @AfterClass
+  public static void cleanupTest() throws Exception {
+    try {
+      UTIL.shutdownMiniCluster();
+    } catch (Exception e) {
+      LOG.warn("failure shutting down cluster", e);
+    }
+  }
+
+  @Test
+  public void test() throws Exception {
+    //write some rows to the table
+    for (int i = 0; i < 10; i++) {
+      byte[] row = Bytes.toBytes("row" + i);
+      Put put = new Put(row);
+      put.addColumn(CF, CF, CF);
+      TABLE.put(put);
+    }
+    MasterProcedureEnv env = UTIL.getMiniHBaseCluster().getMaster()
+        .getMasterProcedureExecutor().getEnvironment();
+    final ProcedureExecutor<MasterProcedureEnv> executor = UTIL.getMiniHBaseCluster()
+        .getMaster().getMasterProcedureExecutor();
+    List<RegionInfo> regionInfos = admin.getRegions(TABLE_NAME);
+    MergeTableRegionsProcedure mergeTableRegionsProcedure = new MergeTableRegionsProcedure(
+        env, regionInfos.get(0), regionInfos.get(1));
+    executor.submitProcedure(mergeTableRegionsProcedure);
+    UTIL.waitFor(30000, () -> executor.getProcedures().stream()
+        .filter(p -> p instanceof UnassignProcedure)
+        .map(p -> (UnassignProcedure) p)
+        .anyMatch(p -> TABLE_NAME.equals(p.getTableName())));
+    UTIL.getMiniHBaseCluster().killRegionServer(
+        UTIL.getMiniHBaseCluster().getRegionServer(0).getServerName());
+    UTIL.getMiniHBaseCluster().startRegionServer();
+    UTIL.waitUntilNoRegionsInTransition();
+    Scan scan = new Scan();
+    ResultScanner results = TABLE.getScanner(scan);
+    int count = 0;
+    Result result = null;
+    while ((result = results.next()) != null) {
+      count++;
+    }
+    Assert.assertEquals("There should be 10 rows!", 10, count);
+
+
+
+
+  }
+
+
+
+}
