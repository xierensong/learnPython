diff --git a/.gitignore b/.gitignore
index 446e37b..9bb1f61 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,3 +4,6 @@
 /.settings
 /build
 /logs
+*.patch
+*.rej
+*.orig
diff --git a/TODO.txt b/TODO.txt
new file mode 100644
index 0000000..c56e88c
--- /dev/null
+++ b/TODO.txt
@@ -0,0 +1,10 @@
+We're using jackson 1.0.1 because it'll work with stargate.  Update to jackson 1.4.
+One problem is that jackson deserializes a ColumnFamilyDescriptor as a LinkedListHashMap.
+
+Is having ObjectMapper per object right? I moved it to the toString for now.
+
+Renamed HCD as CFD and HTD as TD.
+
+Serialization in shell will now be JSON not ruby.
+
+READ-ONLY is a state change.  Need to add it back into state.
diff --git a/ivy.xml b/ivy.xml
index 4a1a9bc..4c87d4c 100644
--- a/ivy.xml
+++ b/ivy.xml
@@ -18,9 +18,9 @@
 <ivy-module version="1.0">
   <info organisation="org.apache.hadoop" module="${ant.project.name}" revision="${version}">
     <license name="Apache 2.0"/>
-    <ivyauthor name="Apache Hadoop Team" url="http://hadoop.apache.org"/>
+    <ivyauthor name="Apache HBase Team" url="http://hbase.org"/>
     <description>
-        Hadoop Core
+        HBase
     </description>
   </info>
   <configurations defaultconfmapping="default">
@@ -67,6 +67,8 @@
     <conf name="httpclient" visibility="private" extends="commons-logging"/>
     <conf name="log4j" visibility="private"/>
     <conf name="lucene" visibility="private"/>
+    <conf name="jackson-mapping-asl" visibility="private"/>
+    <conf name="jackson-core-asl" visibility="private"/>
     <conf name="jdiff" visibility="private" extends="log4j,s3-client,jetty,server"/>
     <conf name="checkstyle" visibility="private"/>
 
@@ -100,6 +102,10 @@
    <dependency org="thrift" name="libthrift" 
                rev="${thrift.version}" conf="common->default" />
    -->
+   <dependency org="org.codehaus.jackson" name="jackson-core-asl" 
+               rev="${jackson.version}" conf="common->default"/> 
+   <dependency org="org.codehaus.jackson" name="jackson-mapper-asl" 
+               rev="${jackson.version}" conf="common->default"/> 
 
    <!--  Test  -->
    <dependency org="org.apache.hadoop" name="hadoop-core-test" 
diff --git a/ivy/libraries.properties b/ivy/libraries.properties
index c56b99e..82c687f 100644
--- a/ivy/libraries.properties
+++ b/ivy/libraries.properties
@@ -43,5 +43,4 @@ protobuf.version=2.1.0
 
 jruby.version=1.3.1
 
-
-
+jackson.version=1.0.1
diff --git a/lib/libthrift-r771587.jar b/lib/libthrift-r771587.jar
deleted file mode 100644
index 3988da7..0000000
Binary files a/lib/libthrift-r771587.jar and /dev/null differ
diff --git a/lib/zookeeper-3.2.2.jar b/lib/zookeeper-3.2.2.jar
deleted file mode 100644
index de99db4..0000000
Binary files a/lib/zookeeper-3.2.2.jar and /dev/null differ
diff --git a/src/java/org/apache/hadoop/hbase/ColumnFamilyDefinition.java b/src/java/org/apache/hadoop/hbase/ColumnFamilyDefinition.java
new file mode 100644
index 0000000..7cb9c1a
--- /dev/null
+++ b/src/java/org/apache/hadoop/hbase/ColumnFamilyDefinition.java
@@ -0,0 +1,424 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import java.util.Collection;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.hadoop.hbase.io.hfile.Compression;
+import org.apache.hadoop.hbase.io.hfile.HFile;
+import org.apache.hadoop.hbase.util.Attributes;
+import org.apache.hadoop.hbase.util.JSONing;
+
+/**
+ * Column family schema descriptor.  Has name and family attributes such as
+ * number of versions, compression settings, etc.
+ *
+ * <p>Not thread-safe.
+ * 
+ * <p>TODO: Certain attributes are immutable once this CFS has been constructed.
+ * Enforce.  For examle, name (But check must not get in way of deserialzation)
+ * <p>TODO: Creating an objectmapper on each serialization is ok?
+ * <p>TODO: Its ok having no version on this class?  Its easy adding attributes
+ * so do we need a version?  Deserializing, we can just check for attribute?
+ * Same for {@link TableDefinition}?  Or should I use AVRO?  The schema needs to
+ * be humanly readable though.  This alone would eliminate binary serializations?
+ * <p>TODO: Don't use a Map; use explicit serializer with version
+ */
+public class ColumnFamilyDefinition implements Map<String, Object> {
+  // Backing Map of attributes.  Keep differences only rather than all
+  // attributes. Make it sorted so items come out same every time.
+  private final Map<String, Object> delegatee = 
+    new TreeMap<String, Object>();
+
+  // Keys for attributes
+  public static final String COMPRESSION = "COMPRESSION";
+  public static final String BLOCKCACHE = "BLOCKCACHE";
+  public static final String BLOCKSIZE = "BLOCKSIZE";
+  public static final String LENGTH = "LENGTH";
+  public static final String TTL = "TTL";
+
+  // Signifies the longest TTL.
+  public static final String FOREVER = "FOREVER";
+
+  /**
+   * Default compression type.
+   */
+  public static final String DEFAULT_COMPRESSION =
+    Compression.Algorithm.NONE.getName();
+
+  /**
+   * Default number of versions of a record to keep.
+   */
+  public static final int DEFAULT_VERSIONS = 3;
+
+  /**
+   * Default setting for whether to serve from memory or not.
+   */
+  public static final boolean DEFAULT_IN_MEMORY = false;
+
+  /**
+   * Default setting for whether to use a block cache or not.
+   */
+  public static final boolean DEFAULT_BLOCKCACHE = true;
+
+  /**
+   * Default size of blocks in files store to the filesytem.  Use smaller for
+   * faster random-access at expense of larger indices (more memory consumption).
+   */
+  public static final int DEFAULT_BLOCKSIZE = HFile.DEFAULT_BLOCKSIZE;
+
+  /**
+   * Default time to live of cell contents.
+   */
+  public static final int DEFAULT_TTL = HConstants.FOREVER;
+
+  /*
+   * Cache here the blocksize value.
+   * TODO: its OK to cache since when we're re-enable, we create a new CFD?
+   */
+  private volatile int cachedBlocksize = -1;
+
+  /*
+   * Cache the max versions rather than calculate it every time.
+   */
+  private volatile int cachedMaxVersions = -1;
+
+  /**
+   * Default constructor. Used by deserializer.
+   */
+  public ColumnFamilyDefinition() {
+    super();
+  }
+
+  /**
+   * Construct a column descriptor specifying only the family name 
+   * The other attributes are defaulted.
+   * 
+   * @param familyName Column family name. Must be 'printable' -- digit or
+   * letter -- and may not contain a <code>:<code>
+   */
+  public ColumnFamilyDefinition(final String familyName) {
+    this.delegatee.put(HConstants.NAME, familyName);
+  }
+
+  /**
+   * Constructor.
+   * Copies attributes from the passed <code>cfs</code>.
+   * @param cfs What to copy.
+   */
+  public ColumnFamilyDefinition(final ColumnFamilyDefinition cfs) {
+    isLegalFamilyName(cfs.getName());
+    this.delegatee.putAll(cfs);
+  }
+
+  /**
+   * Constructor
+   * @param familyName Column family name. Must be 'printable' -- digit or
+   * letter -- and may not contain a <code>:<code>
+   * @param maxVersions Maximum number of versions to keep
+   * @param compression Compression type
+   * @param inMemory If true, column data should be kept in an HRegionServer's
+   * cache
+   * @param blockCacheEnabled If true, MapFile blocks should be cached
+   * @param blocksize
+   * @param timeToLive Time-to-live of cell contents, in seconds
+   * (use HConstants.FOREVER for unlimited TTL)
+   * 
+   * @throws IllegalArgumentException if passed a family name that is made of 
+   * other than 'word' characters: i.e. <code>[a-zA-Z_0-9]</code> or contains
+   * a <code>:</code>
+   * @throws IllegalArgumentException if the number of versions is &lt;= 0
+   */
+  public ColumnFamilyDefinition(final String familyName, final int maxVersions,
+      final String compression, final boolean inMemory,
+      final boolean blockCacheEnabled, final int blocksize,
+      final int timeToLive) {
+    isLegalFamilyName(familyName);
+    this.delegatee.put(HConstants.NAME, familyName);
+    setMaxVersions(maxVersions);
+    setInMemory(inMemory);
+    setBlockCacheEnabled(blockCacheEnabled);
+    setTimeToLive(timeToLive);
+    setCompressionType(Compression.Algorithm.valueOf(compression.toUpperCase()));
+    setBlocksize(blocksize);
+  }
+
+  /**
+   * @param b Family name.
+   * @return <code>b</code>
+   * @throws IllegalArgumentException If not null and not a legitimate family
+   * name: i.e. 'printable' and ends in a ':' (Null passes are allowed because
+   * <code>b</code> can be null when deserializing).  Cannot start with a '.'
+   * either.
+   */
+  public static String isLegalFamilyName(final String b) {
+    if (b == null) {
+      return b;
+    }
+    if (b.charAt(0) == '.') {
+      throw new IllegalArgumentException("Family names cannot start with a " +
+        "period: " + b);
+    }
+    for (int i = 0; i < b.length(); i++) {
+      char c = b.charAt(i);
+      if (Character.isISOControl(c) || c == ':') {
+        throw new IllegalArgumentException("Illegal character <" + c +
+          ">. Family names cannot contain control characters or colons: " + b);
+      }
+    }
+    return b;
+  }
+
+  /**
+   * @return Name of this column family.
+   */
+  public String getName() {
+    return (String)this.delegatee.get(HConstants.NAME);
+  }
+
+  /** @return compression type being used for the column family */
+  public Compression.Algorithm getCompression() {
+    Object v = this.delegatee.get(COMPRESSION);
+    return v == null? Compression.Algorithm.NONE:
+      Compression.Algorithm.valueOf(((String)v).toUpperCase());
+  }
+  
+  /** @return maximum number of versions */
+  public int getMaxVersions() {
+    if (this.cachedMaxVersions == -1) {
+      this.cachedMaxVersions =
+        Attributes.getInt(this.delegatee, HConstants.VERSIONS, DEFAULT_VERSIONS);
+    }
+    return this.cachedMaxVersions;
+  }
+
+  /**
+   * @param maxVersions maximum number of versions
+   */
+  public void setMaxVersions(int maxVersions) {
+    if (maxVersions <= 0) {
+      // TODO: Allow maxVersion of 0 to be the way you say "Keep all versions".
+      // Until there is support, consider 0 or < 0 -- a configuration error.
+      throw new IllegalArgumentException("Maximum versions must be positive");
+    }
+    this.delegatee.put(HConstants.VERSIONS, maxVersions);
+    this.cachedMaxVersions = -1;
+  }
+
+  /**
+   * @return Blocksize.
+   */
+  public int getBlocksize() {
+    if (this.cachedBlocksize == -1) {
+      this.cachedBlocksize =
+        Attributes.getInt(this.delegatee, BLOCKSIZE, DEFAULT_BLOCKSIZE);
+    }
+    return this.cachedBlocksize;
+  }
+
+  /**
+   * @param s
+   */
+  public void setBlocksize(int s) {
+    this.delegatee.put(BLOCKSIZE, s);
+    this.cachedBlocksize = -1;
+  }
+
+  /**
+   * @return Compression type setting.
+   */
+  public Compression.Algorithm getCompressionType() {
+    return getCompression();
+  }
+
+  /**
+   * Compression types supported in hbase.
+   * LZO is not bundled as part of the hbase distribution.
+   * See <a href="http://wiki.apache.org/hadoop/UsingLzoCompression">LZO Compression</a>
+   * for how to enable it.
+   * @param type Compression type setting.
+   */
+  public void setCompressionType(Compression.Algorithm type) {
+    String compressionType;
+    switch (type) {
+      case LZO: compressionType = "LZO"; break;
+      case GZ: compressionType = "GZ"; break;
+      default: compressionType = "NONE"; break;
+    }
+    this.delegatee.put(COMPRESSION, compressionType);
+  }
+
+  /**
+   * @return True if we are to keep all in use HRegionServer cache.
+   */
+  public boolean isInMemory() {
+    return Attributes.getBoolean(this.delegatee,
+      HConstants.IN_MEMORY, DEFAULT_IN_MEMORY);
+  }
+  
+  /**
+   * @param inMemory True if we are to keep all values in the HRegionServer
+   * cache
+   */
+  public void setInMemory(boolean inMemory) {
+    this.delegatee.put(HConstants.IN_MEMORY,
+      inMemory? Boolean.TRUE: Boolean.FALSE);
+  }
+
+  /**
+   * @return Time-to-live of cell contents, in seconds.
+   */
+  public int getTimeToLive() {
+    return Attributes.getInt(this.delegatee, TTL, DEFAULT_TTL);
+  }
+
+  /**
+   * @param timeToLive Time-to-live of cell contents, in seconds.
+   */
+  public void setTimeToLive(int timeToLive) {
+    this.delegatee.put(TTL, timeToLive);
+  }
+
+  /**
+   * @return True if MapFile blocks should be cached.
+   */
+  public boolean isBlockCacheEnabled() {
+    return Attributes.getBoolean(this.delegatee, BLOCKCACHE,
+      DEFAULT_BLOCKCACHE);
+  }
+
+  /**
+   * @param blockCacheEnabled True if MapFile blocks should be cached.
+   */
+  public void setBlockCacheEnabled(boolean blockCacheEnabled) {
+    this.delegatee.put(BLOCKCACHE, blockCacheEnabled);
+  }
+
+  /**
+   * @see java.lang.Object#toString()
+   */
+  @Override
+  public String toString() {
+    return JSONing.toString(this);
+  }
+
+  /**
+   * @see java.lang.Object#equals(java.lang.Object)
+   */
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) {
+      return true;
+    }
+    if (obj == null) {
+      return false;
+    }
+    if (!(obj instanceof ColumnFamilyDefinition)) {
+      return false;
+    }
+    return compareTo((ColumnFamilyDefinition)obj) == 0;
+  }
+
+  /**
+   * @see java.lang.Object#hashCode()
+   */
+  @Override
+  public int hashCode() {
+    return getName().hashCode();
+  }
+
+  public int compareTo(ColumnFamilyDefinition o) {
+    String myname = getName();
+    String hername = o.getName();
+    int result = myname.compareTo(hername);
+    if (result == 0) {
+      // Punt on comparison for ordering, just calculate difference
+      result = this.delegatee.hashCode() - o.delegatee.hashCode();
+      if (result < 0)
+        result = -1;
+      else if (result > 0)
+        result = 1;
+    }
+    return result;
+  }
+
+  @Override
+  public void clear() {
+    this.delegatee.clear();
+  }
+
+  @Override
+  public boolean containsKey(Object key) {
+    return this.delegatee.containsKey(key);
+  }
+
+  @Override
+  public boolean containsValue(Object value) {
+    return this.delegatee.containsValue(value);
+  }
+
+  @Override
+  public Set<java.util.Map.Entry<String, Object>> entrySet() {
+    return this.delegatee.entrySet();
+  }
+
+  @Override
+  public Object get(Object key) {
+    return this.delegatee.get(key);
+  }
+
+  @Override
+  public boolean isEmpty() {
+    return this.delegatee.isEmpty();
+  }
+
+  @Override
+  public Set<String> keySet() {
+    return this.delegatee.keySet();
+  }
+
+  @Override
+  public Object put(String key, Object value) {
+    return this.delegatee.put(key, value);
+  }
+
+  @Override
+  public void putAll(Map<? extends String, ? extends Object> m) {
+    this.delegatee.putAll(m);
+  }
+
+  @Override
+  public Object remove(Object key) {
+    return this.delegatee.remove(key);
+  }
+
+  @Override
+  public int size() {
+    return this.delegatee.size();
+  }
+
+  @Override
+  public Collection<Object> values() {
+    return this.delegatee.values();
+  }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/HConstants.java b/src/java/org/apache/hadoop/hbase/HConstants.java
index 04340f5..2e47532 100644
--- a/src/java/org/apache/hadoop/hbase/HConstants.java
+++ b/src/java/org/apache/hadoop/hbase/HConstants.java
@@ -165,10 +165,12 @@ public interface HConstants {
   //
   
   /** The root table's name.*/
-  static final byte [] ROOT_TABLE_NAME = Bytes.toBytes("-ROOT-");
+  static final String ROOT_TABLE_NAME_STR = "-ROOT-";
+  static final byte [] ROOT_TABLE_NAME = Bytes.toBytes(ROOT_TABLE_NAME_STR);
 
   /** The META table's name. */
-  static final byte [] META_TABLE_NAME = Bytes.toBytes(".META.");  
+  static final String META_TABLE_NAME_STR = ".META.";
+  static final byte [] META_TABLE_NAME = Bytes.toBytes(META_TABLE_NAME_STR);  
 
   /** delimiter used between portions of a region name */
   public static final int META_ROW_DELIMITER = ',';
@@ -261,14 +263,10 @@ public interface HConstants {
   public static final String HBASE_CLIENT_RETRIES_NUMBER_KEY =
     "hbase.client.retries.number";
 
-  //TODO: although the following are referenced widely to format strings for
-  //      the shell. They really aren't a part of the public API. It would be
-  //      nice if we could put them somewhere where they did not need to be
-  //      public. They could have package visibility
-  static final String NAME = "NAME";
-  static final String VERSIONS = "VERSIONS";
-  static final String IN_MEMORY = "IN_MEMORY";
-  
+  public static final String NAME = "NAME";
+  public static final String VERSIONS = "VERSIONS";
+  public static final String IN_MEMORY = "IN_MEMORY";
+
   /**
    * This is a retry backoff multiplier table similar to the BSD TCP syn
    * backoff table, a bit more aggressive than simple exponential backoff.
diff --git a/src/java/org/apache/hadoop/hbase/HMerge.java b/src/java/org/apache/hadoop/hbase/HMerge.java
index 6453345..268c854 100644
--- a/src/java/org/apache/hadoop/hbase/HMerge.java
+++ b/src/java/org/apache/hadoop/hbase/HMerge.java
@@ -319,7 +319,7 @@ class HMerge implements HConstants {
       
       super(conf, fs, META_TABLE_NAME);
 
-      Path rootTableDir = HTableDescriptor.getTableDir(
+      Path rootTableDir = TableDefinition.getTableDir(
           fs.makeQualified(new Path(conf.get(HBASE_DIR))),
           ROOT_TABLE_NAME);
 
diff --git a/src/java/org/apache/hadoop/hbase/HRegionInfo.java b/src/java/org/apache/hadoop/hbase/HRegionInfo.java
index 0aa0580..e76d122 100644
--- a/src/java/org/apache/hadoop/hbase/HRegionInfo.java
+++ b/src/java/org/apache/hadoop/hbase/HRegionInfo.java
@@ -51,11 +51,11 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
 
   /** HRegionInfo for root region */
   public static final HRegionInfo ROOT_REGIONINFO =
-    new HRegionInfo(0L, HTableDescriptor.ROOT_TABLEDESC);
+    new HRegionInfo(0L, TableSchema.TableDefinition);
 
   /** HRegionInfo for first meta region */
   public static final HRegionInfo FIRST_META_REGIONINFO =
-    new HRegionInfo(1L, HTableDescriptor.META_TABLEDESC);
+    new HRegionInfo(1L, TableSchema.TableDefinition);
 
   private byte [] endKey = HConstants.EMPTY_BYTE_ARRAY;
   private boolean offLine = false;
@@ -64,7 +64,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
   private String regionNameStr = "";
   private boolean split = false;
   private byte [] startKey = HConstants.EMPTY_BYTE_ARRAY;
-  protected HTableDescriptor tableDesc = null;
+  protected TableDefinition tableDesc = null;
   private int hashCode = -1;
   //TODO: Move NO_HASH to HStoreFile which is really the only place it is used.
   public static final int NO_HASH = -1;
@@ -84,7 +84,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
    * Private constructor used constructing HRegionInfo for the catalog root and
    * first meta regions
    */
-  private HRegionInfo(long regionId, HTableDescriptor tableDesc) {
+  private HRegionInfo(long regionId, TableDefinition tableDesc) {
     super();
     this.regionId = regionId;
     this.tableDesc = tableDesc;
@@ -96,7 +96,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
   /** Default constructor - creates empty object */
   public HRegionInfo() {
     super();
-    this.tableDesc = new HTableDescriptor();
+    this.tableDesc = new TableDefinition();
   }
   
   /**
@@ -107,7 +107,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
    * @param endKey end of key range
    * @throws IllegalArgumentException
    */
-  public HRegionInfo(final HTableDescriptor tableDesc, final byte [] startKey,
+  public HRegionInfo(final TableDefinition tableDesc, final byte [] startKey,
       final byte [] endKey)
   throws IllegalArgumentException {
     this(tableDesc, startKey, endKey, false);
@@ -123,7 +123,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
    * regions that may or may not hold references to this region.
    * @throws IllegalArgumentException
    */
-  public HRegionInfo(HTableDescriptor tableDesc, final byte [] startKey,
+  public HRegionInfo(TableDefinition tableDesc, final byte [] startKey,
       final byte [] endKey, final boolean split)
   throws IllegalArgumentException {
     this(tableDesc, startKey, endKey, split, System.currentTimeMillis());
@@ -140,7 +140,7 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
    * @param regionid Region id to use.
    * @throws IllegalArgumentException
    */
-  public HRegionInfo(HTableDescriptor tableDesc, final byte [] startKey,
+  public HRegionInfo(TableDefinition tableDesc, final byte [] startKey,
     final byte [] endKey, final boolean split, final long regionid)
   throws IllegalArgumentException {
     super();
@@ -298,14 +298,14 @@ public class HRegionInfo extends VersionedWritable implements WritableComparable
   }
 
   /** @return the tableDesc */
-  public HTableDescriptor getTableDesc(){
+  public TableDefinition getTableDesc(){
     return tableDesc;
   }
 
   /**
    * @param newDesc new table descriptor to use
    */
-  public void setTableDesc(HTableDescriptor newDesc) {
+  public void setTableDesc(TableDefinition newDesc) {
     this.tableDesc = newDesc;
   }
 
diff --git a/src/java/org/apache/hadoop/hbase/KeyValue.java b/src/java/org/apache/hadoop/hbase/KeyValue.java
index ed6997c..f85645e 100644
--- a/src/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/src/java/org/apache/hadoop/hbase/KeyValue.java
@@ -118,10 +118,10 @@ public class KeyValue implements Writable, HeapSize {
    * @return The comparator.
    */
   public static KeyComparator getRowComparator(byte [] tableName) {
-    if(Bytes.equals(HTableDescriptor.ROOT_TABLEDESC.getName(),tableName)) {
+    if(Bytes.equals(TableSchema.TableDefinition.getName(),tableName)) {
       return ROOT_COMPARATOR.getRawComparator();
     }
-    if(Bytes.equals(HTableDescriptor.META_TABLEDESC.getName(), tableName)) {
+    if(Bytes.equals(TableSchema.TableDefinition.getName(), tableName)) {
       return META_COMPARATOR.getRawComparator();
     }
     return COMPARATOR.getRawComparator();
diff --git a/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java b/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
index 7b18d1b..331328a 100644
--- a/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
+++ b/src/java/org/apache/hadoop/hbase/LocalHBaseCluster.java
@@ -328,8 +328,8 @@ public class LocalHBaseCluster implements HConstants {
     LocalHBaseCluster cluster = new LocalHBaseCluster(conf);
     cluster.startup();
     HBaseAdmin admin = new HBaseAdmin(conf);
-    HTableDescriptor htd =
-      new HTableDescriptor(Bytes.toBytes(cluster.getClass().getName()));
+    TableDefinition htd =
+      new TableDefinition(Bytes.toBytes(cluster.getClass().getName()));
     admin.createTable(htd);
     cluster.shutdown();
   }
diff --git a/src/java/org/apache/hadoop/hbase/RegionException.java b/src/java/org/apache/hadoop/hbase/RegionException.java
index 63063a5..81f557e 100644
--- a/src/java/org/apache/hadoop/hbase/RegionException.java
+++ b/src/java/org/apache/hadoop/hbase/RegionException.java
@@ -39,5 +39,4 @@ public class RegionException extends IOException {
   public RegionException(String s) {
     super(s);
   }
-
 }
diff --git a/src/java/org/apache/hadoop/hbase/TableDefinition.java b/src/java/org/apache/hadoop/hbase/TableDefinition.java
new file mode 100644
index 0000000..bc04f12
--- /dev/null
+++ b/src/java/org/apache/hadoop/hbase/TableDefinition.java
@@ -0,0 +1,383 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.util.Attributes;
+import org.apache.hadoop.hbase.util.JSONing;
+
+/**
+ * TableSchema contains table name, attributes, and column families.
+ * <p>Not thread-safe.
+ * <p>TODO: public static final TableSchema xMETA_TABLEDESC = new TableSchema(
+      HConstants.META_TABLE_NAME_STR, new ColumnFamilySchema[] {
+          new ColumnFamilySchema(HConstants.CATALOG_FAMILY_STR,
+            10, // Ten is arbitrary number.  Keep versions to help debuggging.
+            Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,
+            HConstants.FOREVER)});
+ *  <p>TODO: Don't use a Map; use explicit serializer with version
+ */
+public class TableDefinition implements Map<String, Object> {
+  // Backing Map.
+  private Map<String, Object> delegatee =
+    new TreeMap<String, Object>();
+
+  private static final String FAMILIES = "FAMILIES";
+  public static final String MAX_FILESIZE = "MAX_FILESIZE";
+  public static final String MEMSTORE_FLUSHSIZE = "MEMSTORE_FLUSHSIZE";
+  public static final String DEFERRED_LOG_FLUSH = "DEFERRED_LOG_FLUSH";
+
+  public static final long DEFAULT_MEMSTORE_FLUSH_SIZE = 1024*1024*64L;
+  public static final long DEFAULT_MAX_FILESIZE = 1024*1024*256L;
+  public static final boolean DEFAULT_DEFERRED_LOG_FLUSH = true;
+
+  private volatile Boolean cachedDeferredLog = null;
+
+  /**
+   * Constructs an empty object.
+   * For deserializing an TableDescriptor instance only.
+   */
+  public TableDefinition() {
+    super();
+  }
+
+  /**
+   * Private constructor used internally creating table descriptors for 
+   * catalog tables: e.g. .META. and -ROOT-.
+   * @param name
+   * @param families
+   * @throws IllegalArgumentException if passed a table name
+   * that is made of other than 'word' characters, underscore or period: i.e.
+   * <code>[a-zA-Z_0-9.].
+   */
+  protected TableDefinition(final String name, List<ColumnFamilyDefinition> families) {
+    this(name, families, null);
+  }
+
+  /**
+   * Private constructor used internally creating table descriptors for 
+   * catalog tables: e.g. .META. and -ROOT-.
+   * @param name
+   * @param columnFamilies Families
+   * @param attributes
+   * @throws IllegalArgumentException if passed a table name
+   * that is made of other than 'word' characters, underscore or period: i.e.
+   * <code>[a-zA-Z_0-9.].
+   */
+  protected TableDefinition(final String name,
+      final List<ColumnFamilyDefinition> columnFamilies,
+      final Map<String, Object> attributes) {
+    isLegalTableName(name);
+    this.delegatee.put(HConstants.NAME, name);
+    if (attributes != null) {
+      for (Map.Entry<String, Object> entry : attributes.entrySet()) {
+        this.delegatee.put(entry.getKey(), entry.getValue());
+      }
+    }
+    if (columnFamilies != null) {
+      for (ColumnFamilyDefinition f: columnFamilies) {
+        addFamily(f);
+      }
+    }
+  }
+
+  /**
+   * Constructor.
+   * @param name Table name.
+   * @throws IllegalArgumentException if passed a table name
+   * that is made of other than 'word' characters, underscore or period: i.e.
+   * <code>[a-zA-Z_0-9.].
+   * @see <a href="HADOOP-1581">HADOOP-1581 HBASE: Un-openable tablename bug</a>
+   */
+  public TableDefinition(final String name) {
+    this(name, null, null);
+  }
+
+  /**
+   * Constructor.
+   * <p>
+   * Makes a deep copy of the supplied descriptor.
+   * @param desc The descriptor.
+   */
+  public TableDefinition(final TableDefinition desc) {
+    this(desc.getName(), null, desc);
+  }
+
+  /** @return true if this is the root region */
+  public boolean isRoot() {
+    return this.delegatee.get(HConstants.NAME).equals(HConstants.ROOT_TABLE_NAME_STR);
+  }
+
+  /** @return true if this is a meta region (part of the root or meta tables) */
+  public boolean isMeta() {
+    return isRoot() ||
+      this.delegatee.get(HConstants.NAME).equals(HConstants.META_TABLE_NAME_STR);
+  }
+
+  /**
+   * Check passed buffer is legal user-space table name.
+   * @param b Table name.
+   * @return Returns passed <code>b</code> param
+   * @throws NullPointerException If passed <code>b</code> is null
+   * @throws IllegalArgumentException if passed a table name
+   * that is made of other than 'word' characters or underscores: i.e.
+   * <code>[a-zA-Z_0-9].
+   */
+  public static String isLegalTableName(final String b) {
+    if (b == null || b.length() <= 0) {
+      throw new IllegalArgumentException("Name is null or empty");
+    }
+    // If its root or meta, then its legal name.
+    if (b.equals(HConstants.ROOT_TABLE_NAME_STR) ||
+        b.equals(HConstants.META_TABLE_NAME_STR)) {
+      return b;
+    }
+    char firstChar = b.charAt(0);
+    if (firstChar == '.' || firstChar == '-') {
+      throw new IllegalArgumentException("Illegal first character <" + firstChar +
+        ">. " + "User-space table names can only start with 'word " +
+        "characters': i.e. [a-zA-Z_0-9]: " + b);
+    }
+    for (int i = 0; i < b.length(); i++) {
+      char c = b.charAt(i);
+      if (Character.isLetterOrDigit(c) || c == '_' || c == '-' || c == '.') {
+        continue;
+      }
+      throw new IllegalArgumentException("Illegal character <" + c + ">. " +
+        "User-space table names can only contain 'word characters':" +
+        "i.e. [a-zA-Z_0-9-.]: " + b);
+    }
+    return b;
+  }
+
+  /**
+   * @return true if that table's log is hflush by other means
+   */
+  public boolean isDeferredLogFlush() {
+    if (this.cachedDeferredLog == null) {
+      this.cachedDeferredLog = Attributes.getBoolean(this.delegatee,
+        DEFERRED_LOG_FLUSH, DEFAULT_DEFERRED_LOG_FLUSH);
+    }
+    return this.cachedDeferredLog;
+  }
+
+  /**
+   * @param deferredLogFlush true if that table's log is hlfush by oter means
+   * only.
+   */
+  public void setDeferredLogFlush(final boolean deferredLogFlush) {
+    this.delegatee.put(DEFERRED_LOG_FLUSH,
+      deferredLogFlush? Boolean.TRUE: Boolean.FALSE);
+    // Clear cached value.
+    this.cachedDeferredLog = null;
+  }
+
+  /** @return name of table */
+  public String getName() {
+    return (String)this.delegatee.get(HConstants.NAME);
+  }
+
+  /** @return max hregion size for table */
+  public long getMaxFileSize() {
+    return Attributes.getLong(this.delegatee, MAX_FILESIZE,
+      DEFAULT_MAX_FILESIZE);
+  }
+
+  /**
+   * @param maxFileSize The maximum file size that a store file can grow to
+   * before a split is triggered.
+   */
+  public void setMaxFileSize(final long maxFileSize) {
+    this.delegatee.put(MAX_FILESIZE, maxFileSize);
+  }
+
+  /**
+   * @return memory cache flush size for each hregion
+   */
+  public long getMemStoreFlushSize() {
+    return Attributes.getLong(this.delegatee, MEMSTORE_FLUSHSIZE,
+      DEFAULT_MEMSTORE_FLUSH_SIZE);
+  }
+
+  /**
+   * @param memstoreFlushSize memory cache flush size for each hregion
+   */
+  public void setMemStoreFlushSize(long memstoreFlushSize) {
+    this.delegatee.put(MEMSTORE_FLUSHSIZE, memstoreFlushSize);
+  }
+
+  /**
+   * Adds a column family.
+   * @param family HColumnDescriptor of familyto add.
+   */
+  public void addFamily(final ColumnFamilyDefinition family) {
+    if (family.getName() == null || family.getName().length() <= 0) {
+      throw new NullPointerException("Family name cannot be null or empty");
+    }
+    Map<String, ColumnFamilyDefinition> families = getFamilies();
+    if (families == null) {
+      families =  new TreeMap<String, ColumnFamilyDefinition>();
+      this.delegatee.put(FAMILIES, families);
+    }
+    families.put(family.getName(), family);
+  }
+
+  /**
+   * Checks to see if this table contains the given column family
+   * @param family Family name.
+   * @return true if the table contains the specified family name
+   */
+  public boolean hasFamily(final String family) {
+    Map<String, ColumnFamilyDefinition> families = getFamilies();
+    if (families == null) return false;
+    return families.containsKey(family);
+  }
+
+  /**
+   * @return Name of this table and then a map of all of the column family
+   * descriptors.
+   * @see #getNameAsString()
+   */
+  @Override
+  public String toString() {
+    return JSONing.toString(this);
+  }
+
+  /**
+   * @see java.lang.Object#equals(java.lang.Object)
+   */
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) {
+      return true;
+    }
+    if (obj == null) {
+      return false;
+    }
+    if (!(obj instanceof TableDefinition)) {
+      return false;
+    }
+    return compareTo((TableDefinition)obj) == 0;
+  }
+
+  /**
+   * @see java.lang.Object#hashCode()
+   */
+  @Override
+  public int hashCode() {
+    // If table name is same, consider them equal.
+    return getName().hashCode();
+  }
+
+  // Comparable
+
+  public int compareTo(final TableDefinition other) {
+    int dis = hashCode();
+    int dother = other.hashCode();
+    return dis - dother;
+  }
+
+  /**
+   * @param family
+   * @return Column family descriptor for the passed family name.
+   */
+  public ColumnFamilyDefinition getFamily(final String family) {
+    Map<String, ColumnFamilyDefinition> families = getFamilies();
+    if (families == null) return null;
+    return families.get(family);
+  }
+
+  public Map<String, ColumnFamilyDefinition> getFamilies() {
+    return (Map<String, ColumnFamilyDefinition>)this.delegatee.get(FAMILIES);
+  }
+
+  /**
+   * @param family
+   * @return Column family descriptor for the passed family name.
+   */
+  public Map<String, Object> removeFamily(final String family) {
+    Map<String, ColumnFamilyDefinition> families = getFamilies();
+    if (families == null) return null;
+    return families.remove(family);
+  }
+
+  /**
+   * @param rootdir qualified path of HBase root directory
+   * @param tableName name of table
+   * @return path for table
+   */
+  public static Path getTableDir(Path rootdir, final String tableName) {
+    return new Path(rootdir, tableName);
+  }
+
+  public void clear() {
+    delegatee.clear();
+  }
+
+  public boolean containsKey(Object key) {
+    return delegatee.containsKey(key);
+  }
+
+  public boolean containsValue(Object value) {
+    return delegatee.containsValue(value);
+  }
+
+  public Set<java.util.Map.Entry<String, Object>> entrySet() {
+    return delegatee.entrySet();
+  }
+
+  public Object get(Object key) {
+    return delegatee.get(key);
+  }
+
+  public boolean isEmpty() {
+    return delegatee.isEmpty();
+  }
+
+  public Set<String> keySet() {
+    return delegatee.keySet();
+  }
+
+  public Object put(String key, Object value) {
+    return delegatee.put(key, value);
+  }
+
+  public void putAll(Map<? extends String, ? extends Object> m) {
+    delegatee.putAll(m);
+  }
+
+  public Object remove(Object key) {
+    return delegatee.remove(key);
+  }
+
+  public int size() {
+    return delegatee.size();
+  }
+
+  public Collection<Object> values() {
+    return delegatee.values();
+  }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/TableState.java b/src/java/org/apache/hadoop/hbase/TableState.java
new file mode 100644
index 0000000..b483486
--- /dev/null
+++ b/src/java/org/apache/hadoop/hbase/TableState.java
@@ -0,0 +1,74 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import org.apache.hadoop.hbase.util.JSONing;
+
+
+/**
+ * Encapsulates table state: e.g. read-only, offline, freeze compactions or
+ * flushes, etc.
+ * <p>TODO: Change so this object is immutable; make serialization work with
+ * immutable?
+ */
+public class TableState {
+  private volatile boolean readonly = false;
+  private volatile boolean offline = false;
+
+  public TableState() {
+    super();
+  }
+
+  public void setOffline(final boolean b) {
+    this.offline = b;
+  }
+
+  public synchronized boolean isOffline() {
+    return this.offline;
+  }
+
+  public void setReadonly(final boolean b) {
+    this.readonly = b;
+  }
+
+  public synchronized boolean isReadonly() {
+    return this.readonly;
+  }
+
+  @Override
+  public String toString() {
+    return JSONing.toString(this);
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (!(other instanceof TableState)) return false;
+    TableState otherts = (TableState)other;
+    return isReadonly() == otherts.isReadonly() &&
+      isOffline() == otherts.isOffline();
+  }
+  
+  @Override
+  public int hashCode() {
+    int hash = 31 + (this.readonly? 1: 0);
+    hash = (31 * hash) + (this.offline? 1: 0);
+    return hash;
+  }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java b/src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
index e520844..77d5835 100644
--- a/src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
+++ b/src/java/org/apache/hadoop/hbase/client/HBaseAdmin.java
@@ -20,75 +20,65 @@
 package org.apache.hadoop.hbase.client;
 
 import java.io.IOException;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.NavigableMap;
+import java.util.Set;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.RegionException;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
-import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.MetaUtils;
 import org.apache.hadoop.hbase.util.Writables;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.ipc.RemoteException;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
 
 /**
- * Provides administrative functions for HBase
+ * Administrative functions for HBase such as table creation, deletion,
+ * onlining, offlining, etc.
  */
-public class HBaseAdmin {
+public class HBaseAdmin implements Watcher {
+  // TODO: We used to implement the HMasterInterface but now that we go to
+  // zk ensemble rather than to master, we don't bother with that anymore.
+  // The thought is that whats in zk should be able to migrate itself if
+  // radical change (its persisted as JSON).
   private final Log LOG = LogFactory.getLog(this.getClass().getName());
 //  private final HConnection connection;
-  final HConnection connection;
-  private volatile Configuration conf;
-  private final long pause;
-  private final int numRetries;
-  private volatile HMasterInterface master;
+  final ZooKeeperWrapper wrapper;
+  private final Configuration conf;
 
   /**
    * Constructor
    * 
    * @param conf Configuration object
-   * @throws MasterNotRunningException
+   * @throws IOException
    */
-  public HBaseAdmin(Configuration conf) throws MasterNotRunningException {
-    this.connection = HConnectionManager.getConnection(conf);
+  public HBaseAdmin(Configuration conf) throws IOException {
+    this.wrapper = new ZooKeeperWrapper(conf, this);
     this.conf = conf;
-    this.pause = conf.getLong("hbase.client.pause", 30 * 1000);
-    this.numRetries = conf.getInt("hbase.client.retries.number", 5);
-    this.master = connection.getMaster();
-  }
-
-  /** @return HConnection used by this object. */
-  public HConnection getConnection() {
-    return connection;
   }
 
-  /**
-   * @return proxy connection to master server for this instance
-   * @throws MasterNotRunningException
-   */
-  public HMasterInterface getMaster() throws MasterNotRunningException{
-    return this.connection.getMaster();
-  }
-  
-  /** @return - true if the master server is running */
+  /** @return True if a master server is running */
   public boolean isMasterRunning() {
-    return this.connection.isMasterRunning();
+    return this.wrapper.readMasterAddress(null) == null;
   }
 
   /**
@@ -98,7 +88,7 @@ public class HBaseAdmin {
    */
   public boolean tableExists(final String tableName)
   throws MasterNotRunningException {
-    return tableExists(Bytes.toBytes(tableName));
+    return this.wrapper.isTable(tableName);
   }
 
   /**
@@ -108,24 +98,30 @@ public class HBaseAdmin {
    */
   public boolean tableExists(final byte [] tableName)
   throws MasterNotRunningException {
-    if (this.master == null) {
-      throw new MasterNotRunningException("master has been shut down");
-    }
-    return connection.tableExists(tableName);
+    return tableExists(Bytes.toString(tableName));
   }
 
   /**
-   * List all the userspace tables.  In other words, scan the META table.
-   *
-   * If we wanted this to be really fast, we could implement a special
-   * catalog table that just contains table names and their descriptors.
-   * Right now, it only exists as part of the META table's region info.
+   * Set of all userspace tables.
    *
-   * @return - returns an array of HTableDescriptors 
+   * @return Set of TableDefinitions.
    * @throws IOException
    */
-  public HTableDescriptor[] listTables() throws IOException {
-    return this.connection.listTables();
+  public Set<TableDefinition> listTables() throws IOException {
+    Set<ZooKeeperWrapper.TableData> tables = tables();
+    Set<TableDefinition> result = new HashSet<TableDefinition>(tables.size());
+    for (ZooKeeperWrapper.TableData d: tables) {
+      result.add(d.getDefinition());
+    }
+    return result;
+  }
+
+  /*
+   * @return All tables.
+   * @throws IOException 
+   */
+  private Set<ZooKeeperWrapper.TableData> tables() throws IOException {
+    return this.wrapper.tables();
   }
 
   
@@ -135,11 +131,11 @@ public class HBaseAdmin {
    * @return the tableDescriptor
    * @throws IOException
    */
-  public HTableDescriptor getTableDescriptor(final byte [] tableName)
+  public TableDefinition getTableDefinition(final String tableName)
   throws IOException {
     return this.connection.getHTableDescriptor(tableName);
   }
-  
+
   private long getPauseTime(int tries) {
 	int triesCount = tries;
     if (triesCount >= HConstants.RETRY_BACKOFF.length)
@@ -160,9 +156,9 @@ public class HBaseAdmin {
    * and attempt-at-creation).
    * @throws IOException
    */
-  public void createTable(HTableDescriptor desc)
+  public void createTable(TableDefinition desc)
   throws IOException {
-    HTableDescriptor.isLegalTableName(desc.getName());
+    TableDefinition.isLegalTableName(desc.getName());
     createTableAsync(desc);
     for (int tries = 0; tries < numRetries; tries++) {
       try {
@@ -197,12 +193,12 @@ public class HBaseAdmin {
    * and attempt-at-creation).
    * @throws IOException
    */
-  public void createTableAsync(HTableDescriptor desc)
+  public void createTableAsync(TableDefinition desc)
   throws IOException {
     if (this.master == null) {
       throw new MasterNotRunningException("master has been shut down");
     }
-    HTableDescriptor.isLegalTableName(desc.getName());
+    TableDefinition.isLegalTableName(desc.getName());
     try {
       this.master.createTable(desc);
     } catch (RemoteException e) {
@@ -232,7 +228,7 @@ public class HBaseAdmin {
     if (this.master == null) {
       throw new MasterNotRunningException("master has been shut down");
     }
-    HTableDescriptor.isLegalTableName(tableName);
+    TableDefinition.isLegalTableName(tableName);
     HRegionLocation firstMetaServer = getFirstMetaServerForTable(tableName);
     try {
       this.master.deleteTable(tableName);
@@ -468,7 +464,7 @@ public class HBaseAdmin {
    * @param column column descriptor of column to be added
    * @throws IOException
    */
-  public void addColumn(final String tableName, HColumnDescriptor column)
+  public void addColumn(final String tableName, ColumnFamilyDefinition column)
   throws IOException {
     addColumn(Bytes.toBytes(tableName), column);
   }
@@ -481,12 +477,12 @@ public class HBaseAdmin {
    * @param column column descriptor of column to be added
    * @throws IOException
    */
-  public void addColumn(final byte [] tableName, HColumnDescriptor column)
+  public void addColumn(final byte [] tableName, ColumnFamilyDefinition column)
   throws IOException {
     if (this.master == null) {
       throw new MasterNotRunningException("master has been shut down");
     }
-    HTableDescriptor.isLegalTableName(tableName);
+    TableDefinition.isLegalTableName(tableName);
     try {
       this.master.addColumn(tableName, column);
     } catch (RemoteException e) {
@@ -520,7 +516,7 @@ public class HBaseAdmin {
     if (this.master == null) {
       throw new MasterNotRunningException("master has been shut down");
     }
-    HTableDescriptor.isLegalTableName(tableName);
+    TableDefinition.isLegalTableName(tableName);
     try {
       this.master.deleteColumn(tableName, columnName);
     } catch (RemoteException e) {
@@ -538,7 +534,7 @@ public class HBaseAdmin {
    * @throws IOException
    */
   public void modifyColumn(final String tableName, final String columnName, 
-      HColumnDescriptor descriptor)
+      ColumnFamilyDefinition descriptor)
   throws IOException {
     modifyColumn(Bytes.toBytes(tableName), Bytes.toBytes(columnName),
       descriptor);
@@ -554,12 +550,12 @@ public class HBaseAdmin {
    * @throws IOException
    */
   public void modifyColumn(final byte [] tableName, final byte [] columnName, 
-    HColumnDescriptor descriptor)
+    ColumnFamilyDefinition descriptor)
   throws IOException {
     if (this.master == null) {
       throw new MasterNotRunningException("master has been shut down");
     }
-    HTableDescriptor.isLegalTableName(tableName);
+    TableDefinition.isLegalTableName(tableName);
     try {
       this.master.modifyColumn(tableName, columnName, descriptor);
     } catch (RemoteException e) {
@@ -724,7 +720,7 @@ public class HBaseAdmin {
    * @param htd modified description of the table
    * @throws IOException
    */
-  public void modifyTable(final byte [] tableName, HTableDescriptor htd) 
+  public void modifyTable(final byte [] tableName, TableDefinition htd) 
   throws IOException {
     modifyTable(tableName, HConstants.Modify.TABLE_SET_HTD, htd);
   }
@@ -748,18 +744,18 @@ public class HBaseAdmin {
     // Let pass if its a catalog table.  Used by admins.
     if (tableName != null && !MetaUtils.isMetaTableName(tableName)) {
       // This will throw exception
-      HTableDescriptor.isLegalTableName(tableName);
+      TableDefinition.isLegalTableName(tableName);
     }
     Writable[] arr = null;
     try {
       switch (op) {
       case TABLE_SET_HTD:
         if (args == null || args.length < 1 || 
-            !(args[0] instanceof HTableDescriptor)) {
+            !(args[0] instanceof TableDefinition)) {
           throw new IllegalArgumentException("SET_HTD requires a HTableDescriptor");
         }
         arr = new Writable[1];
-        arr[0] = (HTableDescriptor)args[0];
+        arr[0] = (TableDefinition)args[0];
         this.master.modifyTable(tableName, op, arr);
         break;
 
@@ -859,4 +855,10 @@ public class HBaseAdmin {
     copyOfConf.setInt("hbase.client.retries.number", 1);
     new HBaseAdmin(copyOfConf);
   }
+
+  @Override
+  public void process(WatchedEvent event) {
+    // TODO Auto-generated method stub
+    
+  }
 }
diff --git a/src/java/org/apache/hadoop/hbase/client/HConnection.java b/src/java/org/apache/hadoop/hbase/client/HConnection.java
index ecd568d..9d10945 100644
--- a/src/java/org/apache/hadoop/hbase/client/HConnection.java
+++ b/src/java/org/apache/hadoop/hbase/client/HConnection.java
@@ -25,7 +25,7 @@ import java.util.List;
 
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.ipc.HMasterInterface;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
@@ -95,14 +95,14 @@ public interface HConnection {
    * @return - returns an array of HTableDescriptors 
    * @throws IOException
    */
-  public HTableDescriptor[] listTables() throws IOException;
+  public TableDefinition[] listTables() throws IOException;
   
   /**
    * @param tableName
    * @return table metadata 
    * @throws IOException
    */
-  public HTableDescriptor getHTableDescriptor(byte[] tableName)
+  public TableDefinition getHTableDescriptor(byte[] tableName)
   throws IOException;
   
   /**
diff --git a/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java b/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
index 901ac67..1af07dc 100644
--- a/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
+++ b/src/java/org/apache/hadoop/hbase/client/HConnectionManager.java
@@ -40,7 +40,7 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
@@ -403,7 +403,7 @@ public class HConnectionManager implements HConstants {
       }
       boolean exists = false;
       try {
-        HTableDescriptor[] tables = listTables();
+        TableDefinition[] tables = listTables();
         for (int i = 0; i < tables.length; i++) {
           if (Bytes.equals(tables[i].getName(), tableName)) {
             exists = true;
@@ -430,10 +430,10 @@ public class HConnectionManager implements HConstants {
       return reload? relocateRegion(name, row): locateRegion(name, row);
     }
 
-    public HTableDescriptor[] listTables() throws IOException {
+    public TableDefinition[] listTables() throws IOException {
       getMaster();
-      final TreeSet<HTableDescriptor> uniqueTables =
-        new TreeSet<HTableDescriptor>();
+      final TreeSet<TableDefinition> uniqueTables =
+        new TreeSet<TableDefinition>();
       MetaScannerVisitor visitor = new MetaScannerVisitor() {
         public boolean processRow(Result result) throws IOException {
           try {
@@ -455,7 +455,7 @@ public class HConnectionManager implements HConstants {
       };
       MetaScanner.metaScan(conf, visitor);
 
-      return uniqueTables.toArray(new HTableDescriptor[uniqueTables.size()]);
+      return uniqueTables.toArray(new TableDefinition[uniqueTables.size()]);
     }
     
     public boolean isTableEnabled(byte[] tableName) throws IOException {
@@ -561,36 +561,36 @@ public class HConnectionManager implements HConstants {
     private static class HTableDescriptorFinder 
     implements MetaScanner.MetaScannerVisitor {
         byte[] tableName;
-        HTableDescriptor result;
+        TableDefinition result;
         protected HTableDescriptorFinder(byte[] tableName) {
           this.tableName = tableName;
         }
         public boolean processRow(Result rowResult) throws IOException {
           HRegionInfo info = Writables.getHRegionInfo(
               rowResult.getValue(CATALOG_FAMILY, REGIONINFO_QUALIFIER));
-          HTableDescriptor desc = info.getTableDesc();
+          TableDefinition desc = info.getTableDesc();
           if (Bytes.compareTo(desc.getName(), tableName) == 0) {
             result = desc;
             return false;
           }
           return true;
         }
-        HTableDescriptor getResult() {
+        TableDefinition getResult() {
           return result;
         }
     }
 
-    public HTableDescriptor getHTableDescriptor(final byte[] tableName)
+    public TableDefinition getHTableDescriptor(final byte[] tableName)
     throws IOException {
       if (Bytes.equals(tableName, HConstants.ROOT_TABLE_NAME)) {
-        return new UnmodifyableHTableDescriptor(HTableDescriptor.ROOT_TABLEDESC);
+        return new UnmodifyableHTableDescriptor(TableSchema.TableDefinition);
       }
       if (Bytes.equals(tableName, HConstants.META_TABLE_NAME)) {
-        return HTableDescriptor.META_TABLEDESC;
+        return TableSchema.TableDefinition;
       }
       HTableDescriptorFinder finder = new HTableDescriptorFinder(tableName);
       MetaScanner.metaScan(conf, finder);
-      HTableDescriptor result = finder.getResult();
+      TableDefinition result = finder.getResult();
       if (result == null) {
         throw new TableNotFoundException(Bytes.toString(tableName));
       }
diff --git a/src/java/org/apache/hadoop/hbase/client/HTable.java b/src/java/org/apache/hadoop/hbase/client/HTable.java
index c5361d4..3e7ffed 100644
--- a/src/java/org/apache/hadoop/hbase/client/HTable.java
+++ b/src/java/org/apache/hadoop/hbase/client/HTable.java
@@ -36,7 +36,7 @@ import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.UnknownScannerException;
@@ -226,7 +226,7 @@ public class HTable implements HTableInterface {
    * @return table metadata
    * @throws IOException
    */
-  public HTableDescriptor getTableDescriptor() throws IOException {
+  public TableDefinition getTableDescriptor() throws IOException {
     return new UnmodifyableHTableDescriptor(
       this.connection.getHTableDescriptor(this.tableName));
   }
@@ -925,8 +925,12 @@ public class HTable implements HTableInterface {
     }
 
     /**
+     * Get <param>nbRows</param> rows.
+     * How many RPCs are made is determined by the {@link Scan#setCaching(int)}
+     * setting (or hbase.client.scanner.caching in hbase-site.xml).
      * @param nbRows number of rows to return
-     * @return Between zero and <param>nbRows</param> RowResults
+     * @return Between zero and <param>nbRows</param> RowResults.  Scan is done
+     * if returned array is of zero-length (We never return null).
      * @throws IOException
      */
     public Result [] next(int nbRows) throws IOException {
diff --git a/src/java/org/apache/hadoop/hbase/client/HTableInterface.java b/src/java/org/apache/hadoop/hbase/client/HTableInterface.java
index e550257..041b84e 100644
--- a/src/java/org/apache/hadoop/hbase/client/HTableInterface.java
+++ b/src/java/org/apache/hadoop/hbase/client/HTableInterface.java
@@ -22,7 +22,7 @@ package org.apache.hadoop.hbase.client;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 
 /**
  * Used to communicate with a single HBase table.
@@ -43,7 +43,7 @@ public interface HTableInterface {
    * @return table metadata
    * @throws IOException
    */
-  HTableDescriptor getTableDescriptor() throws IOException;
+  TableDefinition getTableDescriptor() throws IOException;
 
   /**
    * Test for the existence of columns in the table, as specified in the Get.
diff --git a/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java b/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java
deleted file mode 100644
index 9bc569a..0000000
--- a/src/java/org/apache/hadoop/hbase/client/UnmodifyableHColumnDescriptor.java
+++ /dev/null
@@ -1,81 +0,0 @@
-package org.apache.hadoop.hbase.client;
-
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.io.hfile.Compression;
-
-/**
- * Immutable HColumnDescriptor
- */
-public class UnmodifyableHColumnDescriptor extends HColumnDescriptor {
-
-  /**
-   * @param desc
-   */
-  public UnmodifyableHColumnDescriptor (final HColumnDescriptor desc) {
-    super(desc);
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setValue(byte[], byte[])
-   */
-  @Override
-  public void setValue(byte[] key, byte[] value) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setValue(java.lang.String, java.lang.String)
-   */
-  @Override
-  public void setValue(String key, String value) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setMaxVersions(int)
-   */
-  @Override
-  public void setMaxVersions(int maxVersions) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setInMemory(boolean)
-   */
-  @Override
-  public void setInMemory(boolean inMemory) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setBlockCacheEnabled(boolean)
-   */
-  @Override
-  public void setBlockCacheEnabled(boolean blockCacheEnabled) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setTimeToLive(int)
-   */
-  @Override
-  public void setTimeToLive(int timeToLive) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setCompressionType(org.apache.hadoop.hbase.io.hfile.Compression.Algorithm)
-   */
-  @Override
-  public void setCompressionType(Compression.Algorithm type) {
-    throw new UnsupportedOperationException("HColumnDescriptor is read-only");
-  }
-
-  /**
-   * @see org.apache.hadoop.hbase.HColumnDescriptor#setMapFileIndexInterval(int)
-   */
-  @Override
-  public void setMapFileIndexInterval(int interval) {
-    throw new UnsupportedOperationException("HTableDescriptor is read-only");
-  }
-}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java b/src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
index 6ed3769..5e07f29 100644
--- a/src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
+++ b/src/java/org/apache/hadoop/hbase/client/UnmodifyableHTableDescriptor.java
@@ -20,14 +20,14 @@
 
 package org.apache.hadoop.hbase.client;
 
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 //import org.apache.hadoop.hbase.client.tableindexed.IndexSpecification;
 
 /**
  * Read-only table descriptor.
  */
-public class UnmodifyableHTableDescriptor extends HTableDescriptor {
+public class UnmodifyableHTableDescriptor extends TableDefinition {
   /** Default constructor */
   public UnmodifyableHTableDescriptor() {
 	  super();
@@ -37,7 +37,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
    * Create an unmodifyable copy of an HTableDescriptor
    * @param desc
    */
-  UnmodifyableHTableDescriptor(final HTableDescriptor desc) {
+  UnmodifyableHTableDescriptor(final TableDefinition desc) {
     super(desc.getName(), getUnmodifyableFamilies(desc), desc.getValues());
   }
   
@@ -46,11 +46,11 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
    * @param desc
    * @return Families as unmodifiable array.
    */
-  private static HColumnDescriptor[] getUnmodifyableFamilies(
-      final HTableDescriptor desc) {
-    HColumnDescriptor [] f = new HColumnDescriptor[desc.getFamilies().size()];
+  private static ColumnFamilyDefinition[] getUnmodifyableFamilies(
+      final TableDefinition desc) {
+    ColumnFamilyDefinition [] f = new ColumnFamilyDefinition[desc.getFamilies().size()];
     int i = 0;
-    for (HColumnDescriptor c: desc.getFamilies()) {
+    for (ColumnFamilyDefinition c: desc.getFamilies()) {
       f[i++] = c;
     }
     return f;
@@ -61,7 +61,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
    * @param family HColumnDescriptor of familyto add.
    */
   @Override
-  public void addFamily(final HColumnDescriptor family) {
+  public void addFamily(final ColumnFamilyDefinition family) {
     throw new UnsupportedOperationException("HTableDescriptor is read-only");
   }
 
@@ -71,12 +71,12 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
    * passed in column.
    */
   @Override
-  public HColumnDescriptor removeFamily(final byte [] column) {
+  public ColumnFamilyDefinition removeFamily(final byte [] column) {
     throw new UnsupportedOperationException("HTableDescriptor is read-only");
   }
   
   /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setReadOnly(boolean)
+   * @see org.apache.hadoop.hbase.TableDefinition#setReadOnly(boolean)
    */
   @Override
   public void setReadOnly(boolean readOnly) {
@@ -84,7 +84,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
   }
 
   /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setValue(byte[], byte[])
+   * @see org.apache.hadoop.hbase.TableDefinition#setValue(byte[], byte[])
    */
   @Override
   public void setValue(byte[] key, byte[] value) {
@@ -92,7 +92,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
   }
 
   /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setValue(java.lang.String, java.lang.String)
+   * @see org.apache.hadoop.hbase.TableDefinition#setValue(java.lang.String, java.lang.String)
    */
   @Override
   public void setValue(String key, String value) {
@@ -100,7 +100,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
   }
 
   /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setMaxFileSize(long)
+   * @see org.apache.hadoop.hbase.TableDefinition#setMaxFileSize(long)
    */
   @Override
   public void setMaxFileSize(long maxFileSize) {
@@ -108,7 +108,7 @@ public class UnmodifyableHTableDescriptor extends HTableDescriptor {
   }
 
   /**
-   * @see org.apache.hadoop.hbase.HTableDescriptor#setMemStoreFlushSize(long)
+   * @see org.apache.hadoop.hbase.TableDefinition#setMemStoreFlushSize(long)
    */
   @Override
   public void setMemStoreFlushSize(long memstoreFlushSize) {
diff --git a/src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java b/src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
index 4c2998a..5065902 100644
--- a/src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
+++ b/src/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java
@@ -31,13 +31,13 @@ import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.hbase.ClusterStatus;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HMsg;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
@@ -108,7 +108,7 @@ public class HbaseObjectWritable implements Writable, Configurable {
     addToMap(NullInstance.class, code++);
 
     // Hbase types
-    addToMap(HColumnDescriptor.class, code++);
+    addToMap(ColumnFamilyDefinition.class, code++);
     addToMap(HConstants.Modify.class, code++);
     addToMap(HMsg.class, code++);
     addToMap(HMsg[].class, code++);
@@ -118,7 +118,7 @@ public class HbaseObjectWritable implements Writable, Configurable {
     addToMap(HRegionInfo[].class, code++);
     addToMap(HServerAddress.class, code++);
     addToMap(HServerInfo.class, code++);
-    addToMap(HTableDescriptor.class, code++);
+    addToMap(TableDefinition.class, code++);
     addToMap(MapWritable.class, code++);
     
     //
diff --git a/src/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java b/src/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
index a514d6d..a877a4f 100644
--- a/src/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
+++ b/src/java/org/apache/hadoop/hbase/ipc/HMasterInterface.java
@@ -22,9 +22,9 @@ package org.apache.hadoop.hbase.ipc;
 import java.io.IOException;
 
 import org.apache.hadoop.hbase.ClusterStatus;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.io.Writable;
 
 /**
@@ -35,6 +35,7 @@ import org.apache.hadoop.io.Writable;
  * <p>NOTE: if you change the interface, you must change the RPC version
  * number in HBaseRPCProtocolVersion
  * 
+ * @deprecated Not used any more.  Now we go to zookeeper to do this stuff.
  */
 public interface HMasterInterface extends HBaseRPCProtocolVersion {
 
@@ -48,7 +49,7 @@ public interface HMasterInterface extends HBaseRPCProtocolVersion {
    * @param desc table descriptor
    * @throws IOException
    */
-  public void createTable(HTableDescriptor desc) throws IOException;
+  public void createTable(TableDefinition desc) throws IOException;
 
   /**
    * Deletes a table
@@ -63,7 +64,7 @@ public interface HMasterInterface extends HBaseRPCProtocolVersion {
    * @param column column descriptor
    * @throws IOException
    */
-  public void addColumn(final byte [] tableName, HColumnDescriptor column)
+  public void addColumn(final byte [] tableName, ColumnFamilyDefinition column)
   throws IOException;
 
   /**
@@ -74,7 +75,7 @@ public interface HMasterInterface extends HBaseRPCProtocolVersion {
    * @throws IOException
    */
   public void modifyColumn(final byte [] tableName, final byte [] columnName, 
-    HColumnDescriptor descriptor) 
+    ColumnFamilyDefinition descriptor) 
   throws IOException;
 
 
@@ -123,4 +124,4 @@ public interface HMasterInterface extends HBaseRPCProtocolVersion {
    * Return cluster status.
    */
   public ClusterStatus getClusterStatus();
-}
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/master/AddColumn.java b/src/java/org/apache/hadoop/hbase/master/AddColumn.java
index c46aa41..5122f17 100644
--- a/src/java/org/apache/hadoop/hbase/master/AddColumn.java
+++ b/src/java/org/apache/hadoop/hbase/master/AddColumn.java
@@ -21,16 +21,16 @@ package org.apache.hadoop.hbase.master;
 
 import java.io.IOException;
 
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 
 /** Instantiated to add a column family to a table */
 class AddColumn extends ColumnOperation {
-  private final HColumnDescriptor newColumn;
+  private final ColumnFamilyDefinition newColumn;
 
   AddColumn(final HMaster master, final byte [] tableName, 
-    final HColumnDescriptor newColumn) 
+    final ColumnFamilyDefinition newColumn) 
   throws IOException {
     super(master, tableName);
     this.newColumn = newColumn;
diff --git a/src/java/org/apache/hadoop/hbase/master/BaseScanner.java b/src/java/org/apache/hadoop/hbase/master/BaseScanner.java
index 74524e0..8a3fa98 100644
--- a/src/java/org/apache/hadoop/hbase/master/BaseScanner.java
+++ b/src/java/org/apache/hadoop/hbase/master/BaseScanner.java
@@ -32,7 +32,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hbase.Chore;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerInfo;
@@ -317,7 +317,7 @@ abstract class BaseScanner extends Chore implements HConstants {
     }
     Path tabledir =
       new Path(this.master.getRootDir(), split.getTableDesc().getNameAsString());
-    for (HColumnDescriptor family: split.getTableDesc().getFamilies()) {
+    for (ColumnFamilyDefinition family: split.getTableDesc().getFamilies()) {
       Path p = Store.getStoreHomedir(tabledir, split.getEncodedName(),
         family.getName());
       if (!this.master.getFileSystem().exists(p)) continue;
diff --git a/src/java/org/apache/hadoop/hbase/master/HMaster.java b/src/java/org/apache/hadoop/hbase/master/HMaster.java
index 0ca14fe..c71c540 100644
--- a/src/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/src/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -45,7 +45,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.ClusterStatus;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HMsg;
 import org.apache.hadoop.hbase.HRegionInfo;
@@ -53,7 +53,7 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
 import org.apache.hadoop.hbase.HServerLoad;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.LocalHBaseCluster;
 import org.apache.hadoop.hbase.MasterNotRunningException;
@@ -264,7 +264,7 @@ public class HMaster extends Thread implements HConstants, HMasterInterface,
    * @param b
    */
   private static void setBlockCaching(final HRegionInfo hri, final boolean b) {
-    for (HColumnDescriptor hcd: hri.getTableDesc().families.values()) {
+    for (ColumnFamilyDefinition hcd: hri.getTableDesc().families.values()) {
       hcd.setBlockCacheEnabled(b);
     }
   }
@@ -743,7 +743,7 @@ public class HMaster extends Thread implements HConstants, HMasterInterface,
     this.zooKeeperWrapper.setClusterState(false);
   }
 
-  public void createTable(HTableDescriptor desc)
+  public void createTable(TableDefinition desc)
   throws IOException {    
     if (!isMasterRunning()) {
       throw new MasterNotRunningException();
@@ -811,13 +811,13 @@ public class HMaster extends Thread implements HConstants, HMasterInterface,
     LOG.info("deleted table: " + Bytes.toString(tableName));
   }
 
-  public void addColumn(byte [] tableName, HColumnDescriptor column)
+  public void addColumn(byte [] tableName, ColumnFamilyDefinition column)
   throws IOException {    
     new AddColumn(this, tableName, column).process();
   }
 
   public void modifyColumn(byte [] tableName, byte [] columnName, 
-    HColumnDescriptor descriptor)
+    ColumnFamilyDefinition descriptor)
   throws IOException {
     new ModifyColumn(this, tableName, columnName, descriptor).process();
   }
@@ -981,9 +981,9 @@ public class HMaster extends Thread implements HConstants, HMasterInterface,
     switch (op) {
     case TABLE_SET_HTD:
       if (args == null || args.length < 1 || 
-          !(args[0] instanceof HTableDescriptor))
+          !(args[0] instanceof TableDefinition))
         throw new IOException("SET_HTD request requires an HTableDescriptor");
-      HTableDescriptor htd = (HTableDescriptor) args[0];
+      TableDefinition htd = (TableDefinition) args[0];
       LOG.info("modifyTable(SET_HTD): " + htd);
       new ModifyTableMeta(this, tableName, htd).process();
       break;
diff --git a/src/java/org/apache/hadoop/hbase/master/ModifyColumn.java b/src/java/org/apache/hadoop/hbase/master/ModifyColumn.java
index c50ca5d..426b23b 100644
--- a/src/java/org/apache/hadoop/hbase/master/ModifyColumn.java
+++ b/src/java/org/apache/hadoop/hbase/master/ModifyColumn.java
@@ -20,18 +20,18 @@
 package org.apache.hadoop.hbase.master;
 
 import java.io.IOException;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.HRegionInfo;
 
 /** Instantiated to modify an existing column family on a table */
 class ModifyColumn extends ColumnOperation {
-  private final HColumnDescriptor descriptor;
+  private final ColumnFamilyDefinition descriptor;
   private final byte [] columnName;
   
   ModifyColumn(final HMaster master, final byte [] tableName, 
-    final byte [] columnName, HColumnDescriptor descriptor) 
+    final byte [] columnName, ColumnFamilyDefinition descriptor) 
   throws IOException {
     super(master, tableName);
     this.descriptor = descriptor;
diff --git a/src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java b/src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java
index 0cf369b..6dd83c4 100644
--- a/src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java
+++ b/src/java/org/apache/hadoop/hbase/master/ModifyTableMeta.java
@@ -24,7 +24,7 @@ import java.io.IOException;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.ipc.HRegionInterface;
@@ -36,10 +36,10 @@ class ModifyTableMeta extends TableOperation {
 
   private static Log LOG = LogFactory.getLog(ModifyTableMeta.class);
 
-  private HTableDescriptor desc;
+  private TableDefinition desc;
 
   ModifyTableMeta(final HMaster master, final byte [] tableName, 
-    HTableDescriptor desc) 
+    TableDefinition desc) 
   throws IOException {
     super(master, tableName);
     this.desc = desc;
diff --git a/src/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java b/src/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java
index d7e8518..5b53757 100644
--- a/src/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java
+++ b/src/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java
@@ -20,7 +20,6 @@
 package org.apache.hadoop.hbase.metrics;
 
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -59,12 +58,27 @@ public class MetricsMBeanBase extends MetricsDynamicMBeanBase {
   protected MBeanInfo extendedInfo;
   
   protected MetricsMBeanBase( MetricsRegistry mr, String description ) {
-    super(mr, description);
+    super(copyMinusHBaseMetrics(mr), description);
     this.registry = mr;
     this.description = description;
     this.init();
   }
 
+  /**
+   * @param mr MetricsRegistry.
+   * @return A copy of the passed MetricsRegistry minus the hbase metrics
+   */
+  private static MetricsRegistry copyMinusHBaseMetrics(final MetricsRegistry mr) {
+    MetricsRegistry copy = new MetricsRegistry();
+    for (MetricsBase metric : mr.getMetricsList()) {
+      if (metric instanceof org.apache.hadoop.hbase.metrics.MetricsRate) {
+        continue;
+      }
+      copy.add(metric.getName(), metric);
+    }
+    return copy;
+  }
+
   protected void init() {
     List<MBeanAttributeInfo> attributes = new ArrayList<MBeanAttributeInfo>();
     MBeanInfo parentInfo = super.getMBeanInfo();
@@ -81,13 +95,12 @@ public class MetricsMBeanBase extends MetricsDynamicMBeanBase {
         continue;
       
       // add on custom HBase metric types
-      if (metric instanceof MetricsRate) {
+      if (metric instanceof org.apache.hadoop.hbase.metrics.MetricsRate) {
         attributes.add( new MBeanAttributeInfo(metric.getName(), 
             "java.lang.Float", metric.getDescription(), true, false, false) );
         extendedAttributes.put(metric.getName(), metric);
-      }  else {
-        LOG.error("unknown metrics instance: "+metric.getClass().getName());
-      }      
+      }
+      // Else skip the hadoop metrics.
     }
 
     this.extendedInfo = new MBeanInfo( this.getClass().getName(), 
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 68670c1..6dd4d38 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -43,10 +43,10 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.DroppedSnapshotException;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.client.Delete;
@@ -269,9 +269,9 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
     this.regionCompactionDir =
       new Path(getCompactionDir(basedir), encodedNameStr);
     long flushSize = regionInfo.getTableDesc().getMemStoreFlushSize();
-    if (flushSize == HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE) {
+    if (flushSize == TableDefinition.DEFAULT_MEMSTORE_FLUSH_SIZE) {
       flushSize = conf.getLong("hbase.hregion.memstore.flush.size",
-                      HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE);
+                      TableDefinition.DEFAULT_MEMSTORE_FLUSH_SIZE);
     }
     this.memstoreFlushSize = flushSize;
     this.blockingMemStoreSize = this.memstoreFlushSize *
@@ -303,7 +303,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
     long maxSeqId = -1;
     long minSeqIdToRecover = Integer.MAX_VALUE;
     
-    for (HColumnDescriptor c : this.regionInfo.getTableDesc().getFamilies()) {
+    for (ColumnFamilyDefinition c : this.regionInfo.getTableDesc().getFamilies()) {
       Store store = instantiateHStore(this.basedir, c, oldLogFile, reporter);
       this.stores.put(c.getName(), store);
       long storeSeqId = store.getMaxSequenceId();
@@ -529,7 +529,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
   }
 
   /** @return HTableDescriptor for this region */
-  public HTableDescriptor getTableDesc() {
+  public TableDefinition getTableDesc() {
     return this.regionInfo.getTableDesc();
   }
 
@@ -1502,7 +1502,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
   }
 
   protected Store instantiateHStore(Path baseDir, 
-    HColumnDescriptor c, Path oldLogFile, Progressable reporter)
+    ColumnFamilyDefinition c, Path oldLogFile, Progressable reporter)
   throws IOException {
     return new Store(baseDir, this, c, this.fs, oldLogFile,
       this.conf, reporter);
@@ -1855,7 +1855,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
     final Configuration conf)
   throws IOException {
     Path tableDir =
-      HTableDescriptor.getTableDir(rootDir, info.getTableDesc().getName());
+      TableDefinition.getTableDir(rootDir, info.getTableDesc().getName());
     Path regionDir = HRegion.getRegionDir(tableDir, info.getEncodedName());
     FileSystem fs = FileSystem.get(conf);
     fs.mkdirs(regionDir);
@@ -1889,7 +1889,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
       throw new NullPointerException("Passed region info is null");
     }
     HRegion r = new HRegion(
-        HTableDescriptor.getTableDir(rootDir, info.getTableDesc().getName()),
+        TableDefinition.getTableDir(rootDir, info.getTableDesc().getName()),
         log, FileSystem.get(conf), conf, info, null);
     r.initialize(null, null);
     if (log != null) {
@@ -2022,7 +2022,7 @@ public class HRegion implements HConstants, HeapSize { // , Writable{
    */
   public static Path getRegionDir(final Path rootdir, final HRegionInfo info) {
     return new Path(
-      HTableDescriptor.getTableDir(rootdir, info.getTableDesc().getName()),
+      TableDefinition.getTableDir(rootdir, info.getTableDesc().getName()),
       Integer.toString(info.getEncodedName()));
   }
 
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java b/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index cd31af0..111aad6 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -64,7 +64,7 @@ import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
 import org.apache.hadoop.hbase.HServerLoad;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.LeaseListener;
 import org.apache.hadoop.hbase.Leases;
@@ -1469,7 +1469,7 @@ public class HRegionServer implements HConstants, HRegionInterface,
   
   protected HRegion instantiateRegion(final HRegionInfo regionInfo)
       throws IOException {
-    HRegion r = new HRegion(HTableDescriptor.getTableDir(rootDir, regionInfo
+    HRegion r = new HRegion(TableDefinition.getTableDir(rootDir, regionInfo
         .getTableDesc().getName()), this.hlog, this.fs, conf, regionInfo,
         this.cacheFlusher);
     r.initialize(null,  new Progressable() {
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/Store.java b/src/java/org/apache/hadoop/hbase/regionserver/Store.java
index 96f2d1d..278ce3d 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/Store.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/Store.java
@@ -97,7 +97,7 @@ public class Store implements HConstants, HeapSize {
   // This stores directory in the filesystem.
   private final Path homedir;
   private final HRegion region;
-  private final HColumnDescriptor family;
+  private final ColumnFamilyDefinition family;
   final FileSystem fs;
   private final Configuration conf;
   // ttl in milliseconds.
@@ -233,7 +233,7 @@ public class Store implements HConstants, HeapSize {
     }
   }
     
-  HColumnDescriptor getFamily() {
+  ColumnFamilyDefinition getFamily() {
     return this.family;
   }
 
diff --git a/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 99cf373..ceffe73 100644
--- a/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -53,7 +53,7 @@ import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.RemoteExceptionHandler;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -1158,7 +1158,7 @@ public class HLog implements HConstants, Syncable {
                   HLog.Entry logEntry = i.previous();
                   WriterAndPath wap = logWriters.get(key);
                   if (wap == null) {
-                    Path logfile = new Path(HRegion.getRegionDir(HTableDescriptor
+                    Path logfile = new Path(HRegion.getRegionDir(TableDefinition
                         .getTableDir(rootDir, logEntry.getKey().getTablename()),
                         HRegionInfo.encodeRegionName(key)),
                         HREGION_OLDLOGFILE_NAME);
diff --git a/src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java b/src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
index 9b943c1..82cace5 100644
--- a/src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
+++ b/src/java/org/apache/hadoop/hbase/thrift/ThriftServer.java
@@ -29,11 +29,11 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MasterNotRunningException;
 import org.apache.hadoop.hbase.client.Delete;
@@ -93,7 +93,7 @@ public class ThriftServer {
      * @throws IOException
      */
     byte[][] getAllColumns(HTable table) throws IOException {
-      HColumnDescriptor[] cds = table.getTableDescriptor().getColumnFamilies();
+      ColumnFamilyDefinition[] cds = table.getTableDescriptor().getColumnFamilies();
       byte[][] columns = new byte[cds.length][];
       for (int i = 0; i < cds.length; i++) {
         columns[i] = Bytes.add(cds[i].getName(), 
@@ -203,7 +203,7 @@ public class ThriftServer {
     
     public List<byte[]> getTableNames() throws IOError {
       try {
-        HTableDescriptor[] tables = this.admin.listTables();
+        TableDefinition[] tables = this.admin.listTables();
         ArrayList<byte[]> list = new ArrayList<byte[]>(tables.length);
         for (int i = 0; i < tables.length; i++) {
           list.add(tables[i].getName());
@@ -405,9 +405,9 @@ public class ThriftServer {
         if (admin.tableExists(tableName)) {
           throw new AlreadyExists("table name already in use");
         }
-        HTableDescriptor desc = new HTableDescriptor(tableName);
+        TableDefinition desc = new TableDefinition(tableName);
         for (ColumnDescriptor col : columnFamilies) {
-          HColumnDescriptor colDesc = ThriftUtilities.colDescFromThrift(col);
+          ColumnFamilyDefinition colDesc = ThriftUtilities.colDescFromThrift(col);
           desc.addFamily(colDesc);
         }
         admin.createTable(desc);
@@ -700,9 +700,9 @@ public class ThriftServer {
           new TreeMap<byte[], ColumnDescriptor>(Bytes.BYTES_COMPARATOR);
         
         HTable table = getTable(tableName);
-        HTableDescriptor desc = table.getTableDescriptor();
+        TableDefinition desc = table.getTableDescriptor();
         
-        for (HColumnDescriptor e : desc.getFamilies()) {
+        for (ColumnFamilyDefinition e : desc.getFamilies()) {
           ColumnDescriptor col = ThriftUtilities.colDescFromHbase(e);
           columns.put(col.name, col);
         }
diff --git a/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java b/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
index 176996a..a52aa60 100644
--- a/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
+++ b/src/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
@@ -22,7 +22,7 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.TreeMap;
 
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.io.hfile.Compression;
@@ -43,7 +43,7 @@ public class ThriftUtilities {
    * @return HColumnDescriptor
    * @throws IllegalArgument
    */
-  static public HColumnDescriptor colDescFromThrift(ColumnDescriptor in)
+  static public ColumnFamilyDefinition colDescFromThrift(ColumnDescriptor in)
       throws IllegalArgument {
     Compression.Algorithm comp =
       Compression.getCompressionAlgorithmByName(in.compression.toLowerCase());
@@ -56,7 +56,7 @@ public class ThriftUtilities {
       throw new IllegalArgument("column name is empty");
     }
     byte [] parsedName = KeyValue.parseColumn(in.name)[0];
-    HColumnDescriptor col = new HColumnDescriptor(parsedName,
+    ColumnFamilyDefinition col = new ColumnFamilyDefinition(parsedName,
         in.maxVersions, comp.getName(), in.inMemory, in.blockCacheEnabled,
         in.timeToLive, bloom);
     return col;
@@ -70,7 +70,7 @@ public class ThriftUtilities {
    *          Hbase HColumnDescriptor object
    * @return Thrift ColumnDescriptor
    */
-  static public ColumnDescriptor colDescFromHbase(HColumnDescriptor in) {
+  static public ColumnDescriptor colDescFromHbase(ColumnFamilyDefinition in) {
     ColumnDescriptor col = new ColumnDescriptor();
     col.name = Bytes.add(in.getName(), KeyValue.COLUMN_FAMILY_DELIM_ARRAY);
     col.maxVersions = in.getMaxVersions();
diff --git a/src/java/org/apache/hadoop/hbase/util/Attributes.java b/src/java/org/apache/hadoop/hbase/util/Attributes.java
new file mode 100644
index 0000000..413788e
--- /dev/null
+++ b/src/java/org/apache/hadoop/hbase/util/Attributes.java
@@ -0,0 +1,48 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.util.Map;
+
+/**
+ * Utility to help table and column family attributes maintenance.
+ * Attributes are like java.util.Properties only the value is an Object rather
+ * than a String.  Used by {@link org.apache.hadoop.hbase.ColumnFamilyDefinition}
+ * and {@link TableDefinition}.
+ */
+public class Attributes {
+  public static int getInt(final Map<String, Object> attributes,
+      final String key, final int defaultValue) {
+    Integer v = (Integer)attributes.get(key);
+    return v == null? defaultValue: v.intValue();
+  }
+
+  public static long getLong(final Map<String, Object> attributes,
+      final String key, final long defaultValue) {
+    Long v = (Long)attributes.get(key);
+    return v == null? defaultValue: v.longValue();
+  }
+
+  public static boolean getBoolean(final Map<String, Object> attributes,
+      final String key, final boolean defaultValue) {
+    Boolean v = (Boolean)attributes.get(key);
+    return v == null? defaultValue: v.booleanValue();
+  }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/util/JSONing.java b/src/java/org/apache/hadoop/hbase/util/JSONing.java
new file mode 100644
index 0000000..6bd0667
--- /dev/null
+++ b/src/java/org/apache/hadoop/hbase/util/JSONing.java
@@ -0,0 +1,73 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.io.StringWriter;
+
+import org.codehaus.jackson.JsonGenerationException;
+import org.codehaus.jackson.map.JsonMappingException;
+import org.codehaus.jackson.map.ObjectMapper;
+
+/**
+ * Utility JSONing.
+ */
+public class JSONing {
+  /**
+   * @param o
+   * @return <param>o</param> serialized as a String of JSON.
+   */
+  public static String toString(final Object o) {
+    // JSON Serializer
+    // TODO: OK to have this in every class?  Move to the toString method.
+    ObjectMapper mapper = new ObjectMapper();
+    StringWriter sw = new StringWriter();
+    try {
+      mapper.writeValue(sw, o);
+    } catch (JsonGenerationException e) {
+      e.printStackTrace();
+    } catch (JsonMappingException e) {
+      e.printStackTrace();
+    } catch (IOException e) {
+      e.printStackTrace();
+    }
+    try {
+      sw.close();
+    } catch (IOException e) {
+      e.printStackTrace();
+    }
+    return sw.toString();
+  }
+
+  /**
+   * Deserialize String of JSON into an instance of the passed class.
+   * @param c Class to use making Object.
+   * @param serialization Object serialization as a String.
+   * @return Instance of deserialized object.
+   * @throws IOException
+   */
+  public static Object deserialize(final Class<?> c, final String serialization)
+  throws IOException {
+    StringReader reader = new StringReader(serialization);
+    ObjectMapper mapper = new ObjectMapper();
+    return mapper.readValue(reader, c);
+  }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/hadoop/hbase/util/MetaUtils.java b/src/java/org/apache/hadoop/hbase/util/MetaUtils.java
index 831f82c..1193687 100644
--- a/src/java/org/apache/hadoop/hbase/util/MetaUtils.java
+++ b/src/java/org/apache/hadoop/hbase/util/MetaUtils.java
@@ -33,7 +33,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
@@ -323,7 +323,7 @@ public class MetaUtils {
    * @throws IOException 
    */
   public void addColumn(final byte [] tableName,
-      final HColumnDescriptor hcd)
+      final ColumnFamilyDefinition hcd)
   throws IOException {
     List<HRegionInfo> metas = getMETARows(tableName);
     for (HRegionInfo hri: metas) {
diff --git a/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java b/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
index cccd9d3..51a4f74 100644
--- a/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
+++ b/src/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWrapper.java
@@ -29,6 +29,8 @@ import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Properties;
+import java.util.Set;
+import java.util.TreeSet;
 import java.util.Map.Entry;
 
 import org.apache.commons.logging.Log;
@@ -37,12 +39,17 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HServerAddress;
 import org.apache.hadoop.hbase.HServerInfo;
+import org.apache.hadoop.hbase.TableDefinition;
+import org.apache.hadoop.hbase.TableState;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.JSONing;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.zookeeper.AsyncCallback;
 import org.apache.zookeeper.CreateMode;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.Watcher;
 import org.apache.zookeeper.ZooKeeper;
+import org.apache.zookeeper.AsyncCallback.StatCallback;
 import org.apache.zookeeper.ZooDefs.Ids;
 import org.apache.zookeeper.ZooKeeper.States;
 import org.apache.zookeeper.data.Stat;
@@ -69,6 +76,7 @@ public class ZooKeeperWrapper implements HConstants {
   private final String rsZNode;
   private final String masterElectionZNode;
   public final String clusterStateZNode;
+  private final String tablesZNode;
 
   /**
    * Create a ZooKeeperWrapper.
@@ -102,11 +110,13 @@ public class ZooKeeperWrapper implements HConstants {
       "master");
     String stateZNodeName = conf.get("zookeeper.znode.state",
     "shutdown");
+    String tablesZNodeName = conf.get("zookeeper.znode.tables", "tables");
     
     rootRegionZNode = getZNode(parentZNode, rootServerZNodeName);
     rsZNode = getZNode(parentZNode, rsZNodeName);
     masterElectionZNode = getZNode(parentZNode, masterAddressZNodeName);
     clusterStateZNode = getZNode(parentZNode, stateZNodeName);
+    this.tablesZNode = getZNode(this.parentZNode, tablesZNodeName);
   }
 
   private void setQuorumServers(Properties properties) {
@@ -306,17 +316,30 @@ public class ZooKeeperWrapper implements HConstants {
   /**
    * Watch the state of the cluster, up or down
    * @param watcher Watcher to set on cluster state node
+   * @param callback Called on successful set of watch?
    */
-  public void setClusterStateWatch(Watcher watcher) {
+  public void setClusterStateWatch(final Watcher watcher,
+      final StatCallback completion) {
+    setWatcher(this.clusterStateZNode, watcher, null);
+  }
+
+  public void setWatcher(final String parent, final String child,
+      final Watcher watcher, final StatCallback completion) {
+    setWatcher(joinPath(parent, child), watcher, completion);
+  }
+
+  public void setWatcher(final String znode, final Watcher watcher,
+      final StatCallback completion) {
     try {
-      zooKeeper.exists(clusterStateZNode, watcher);
+      if (completion == null) this.zooKeeper.exists(znode, watcher);
+      else this.zooKeeper.exists(znode, watcher, completion, null);
     } catch (InterruptedException e) {
-      LOG.warn("Failed to check on ZNode " + clusterStateZNode, e);
+      LOG.warn("Failed to check on ZNode " + znode, e);
     } catch (KeeperException e) {
-      LOG.warn("Failed to check on ZNode " + clusterStateZNode, e);
+      LOG.warn("Failed to check on ZNode " + znode, e);
     }
   }
-  
+
   /**
    * Set the cluster state, up or down
    * @param up True to write the node, false to delete it
@@ -378,7 +401,8 @@ public class ZooKeeperWrapper implements HConstants {
     }
   }
 
-  private HServerAddress readAddressOrThrow(String znode, Watcher watcher) throws IOException {
+  private HServerAddress readAddressOrThrow(String znode, Watcher watcher)
+  throws IOException {
     byte[] data;
     try {
       data = zooKeeper.getData(znode, watcher, null);
@@ -609,6 +633,17 @@ public class ZooKeeperWrapper implements HConstants {
       LOG.warn("Failed to delete " + rsZNode + " znodes in ZooKeeper: " + e);
     }
   }
+
+  /*
+   * @param znode
+   * @param watcher
+   * @param callback
+   * @param context
+   */
+  private void getChildren(final String znode, final Watcher watcher,
+      final AsyncCallback.ChildrenCallback callback, final Object context) {
+    this.zooKeeper.getChildren(znode, watcher, callback, context);
+  }
   
   private boolean checkExistenceOf(String path) {
     Stat stat = null;
@@ -624,6 +659,243 @@ public class ZooKeeperWrapper implements HConstants {
   }
 
   /**
+   * Data structure used serializing and deserializing content of a table znode.
+   * Holds table state and definition.
+   * Passed TableState and TableDefinition cannot be null.
+   */
+  public static class TableData implements Comparable<TableData> {
+    private TableState state;
+    private TableDefinition definition;
+
+    /**
+     * Do not use.  Needed deserializing.
+     */
+    TableData() {
+      super();
+    }
+
+    TableData(final TableState s, final TableDefinition d) {
+      setState(s);
+      setDefinition(d);
+    }
+
+    public TableState getState() {
+      return state;
+    }
+
+    public TableDefinition getDefinition() {
+      return definition;
+    }
+
+    public void setState(TableState state) {
+      if (state == null) throw new NullPointerException();
+      this.state = state;
+    }
+
+    public void setDefinition(TableDefinition definition) {
+      this.definition = definition;
+    }
+
+    @Override
+    public String toString() {
+      return JSONing.toString(this);
+    }
+
+    @Override
+    public int compareTo(TableData o) {
+      return this.definition.getName().compareTo(o.getDefinition().getName());
+    }
+
+    /**
+     * @param s
+     * @return A TableData instance made from passed JSON String.
+     * @throws IOException
+     */
+    public static TableData deserialize(final String s) throws IOException {
+      return (ZooKeeperWrapper.TableData)JSONing. 
+        deserialize(ZooKeeperWrapper.TableData.class, s);
+    }
+  }
+
+  /**
+   * Update table definition and/or status.  If no table znode, one is added.
+   * The contents of a table znode is state and definition.
+   * @param definition Table definition.  Do not pass null. Includes table name.
+   * @param state Table state.  Do not pass null.
+   * @return true if operation succeeded, false otherwise.
+   */
+  public boolean updateTable(final TableState state,
+      final TableDefinition definition) {
+    TableData td = new TableData(state, definition);
+    String tablePath = joinPath(this.tablesZNode, definition.getName());
+    return createOrUpdate(tablePath, td.toString());
+  }
+
+  /**
+   * Delete znode.
+   * @param znode
+   */
+  public void delete(final String znode) {
+    try {
+      this.zooKeeper.delete(znode, -1);
+    } catch (InterruptedException e) {
+      e.printStackTrace();
+    } catch (KeeperException e) {
+      e.printStackTrace();
+    }
+  }
+
+  /**
+   * @return True if tables exists.
+   */
+  public boolean ensureTablesExists() {
+    return ensureExists(this.tablesZNode);
+  }
+
+  /**
+   * Sets watcher on tables directory.
+   * Ensures tables directory exists.
+   * @param watcher
+   * @param callback Callback when children are changed.
+   * @param context
+   */
+  public void setWatcherOnTables(final Watcher watcher,
+      final AsyncCallback.ChildrenCallback callback,
+      final Object context) {
+    getChildren(this.tablesZNode, watcher, callback, context);
+  }
+
+  /**
+   * @return Tables up in zookeeper.
+   * @throws IOException 
+   */
+  public Set<TableData> tables() throws IOException {
+    Set<TableData> tables = new TreeSet<TableData>();
+    List<String> children;
+    try {
+      children = this.zooKeeper.getChildren(this.tablesZNode, false);
+      for (String child: children) {
+        tables.add(getTableData(child));
+      }
+    } catch (KeeperException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    } catch (InterruptedException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    }
+    return tables;
+  }
+
+  /**
+   * @param tableName
+   * @return Content of the table znode.
+   * @throws IOException
+   */
+  public TableData table(final String tableName) throws IOException {
+    TableData result = null;
+    try {
+      result = getTableData(joinPath(this.tablesZNode, tableName));
+    } catch (KeeperException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    } catch (InterruptedException e) {
+      // TODO Auto-generated catch block
+      e.printStackTrace();
+    }
+    return result;
+  }
+
+  /*
+   * @param znode
+   * @return Content of the passed table znode.
+   * @throws KeeperException
+   * @throws InterruptedException
+   * @throws IOException
+   */
+  private TableData getTableData(final String znode)
+  throws KeeperException, InterruptedException, IOException {
+    byte [] bytes = this.zooKeeper.getData(znode, false, null);
+    return TableData.deserialize(Bytes.toString(bytes));
+  }
+
+  /**
+   * @param tableName
+   * @return True if table exists.
+   */
+  public boolean isTable(final String tableName) {
+    return this.checkExistenceOf(joinPath(this.tablesZNode, tableName));
+  }
+
+  /**
+   * @param znode
+   * @return True if passed <code>znode</code> matches the tables path.
+   */
+  public boolean isTablesZNode(final String znode) {
+    return this.tablesZNode.equals(znode);
+  }
+
+  /**
+   * Does not check for existence, just that passed znode path could be a table
+   * znode.
+   * @param znode
+   * @return True if passed <code>znode</code> is a subdir of tables path; does
+   * not check for existence, just that passed znode path could be a table
+   * znode.
+   */
+  public boolean isTableZNode(final String znode) {
+    return hasParent(this.tablesZNode, znode);
+  }
+
+  /**
+   * @param parentZNode
+   * @param znode
+   * @return True if znode is a child of <code>parentZNode</code>.
+   */
+  public static boolean hasParent(final String parentZNode, final String znode) {
+    if (!znode.startsWith(parentZNode)) return false;
+    int index = znode.lastIndexOf(ZNODE_PATH_SEPARATOR);
+    if (index <= 0) {   // Parent is root, which always exists.
+      return false;
+    }
+    return index == parentZNode.length() && znode.length() > index + 1;
+  }
+
+  /*
+   * Check table exists and if not, add it, else update it.
+   * @param znode Path to table
+   * @param data
+   * @return
+   */
+  private boolean createOrUpdate(final String znode, final String data) {
+    byte [] dataBytes = Bytes.toBytes(data);
+    if (!checkExistenceOf(znode)) {
+      try {
+        this.zooKeeper.create(znode, dataBytes, Ids.OPEN_ACL_UNSAFE,
+          CreateMode.EPHEMERAL);
+        LOG.debug("Created ZNode " + znode + " with data: " + data);
+        return true;
+      } catch (KeeperException e) {
+        LOG.warn("Failed to create " + znode + " znode in ZooKeeper: " + e);
+      } catch (InterruptedException e) {
+        LOG.warn("Failed to create " + znode + " znode in ZooKeeper: " + e);
+        return false;
+      }
+    }
+    // Update existing znode.
+    try {
+      this.zooKeeper.setData(znode, dataBytes, -1);
+      LOG.debug("Updated ZNode " + znode + " with data: " + data);
+      return true;
+    } catch (KeeperException e) {
+      LOG.warn("Failed to update " + znode + " znode in ZooKeeper: " + e);
+    } catch (InterruptedException e) {
+      LOG.warn("Failed to update " + znode + " znode in ZooKeeper: " + e);
+    }
+    return false;
+  }
+
+  /**
    * Close this ZooKeeper session.
    */
   public void close() {
@@ -651,6 +923,4 @@ public class ZooKeeperWrapper implements HConstants {
   public String getMasterElectionZNode() {
     return masterElectionZNode;
   }
-  
-  
-}
+}
\ No newline at end of file
diff --git a/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java b/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java
index 8f677ca..a850d06 100644
--- a/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java
+++ b/src/test/org/apache/hadoop/hbase/AbstractMergeTestBase.java
@@ -36,7 +36,7 @@ public abstract class AbstractMergeTestBase extends HBaseClusterTestCase {
     LogFactory.getLog(AbstractMergeTestBase.class.getName());
   static final byte [] COLUMN_NAME = Bytes.toBytes("contents");
   protected final Random rand = new Random();
-  protected HTableDescriptor desc;
+  protected TableDefinition desc;
   protected ImmutableBytesWritable value;
   protected boolean startMiniHBase;
   
@@ -66,8 +66,8 @@ public abstract class AbstractMergeTestBase extends HBaseClusterTestCase {
     } catch (UnsupportedEncodingException e) {
       fail();
     }
-    desc = new HTableDescriptor(Bytes.toBytes("test"));
-    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));
+    desc = new TableDefinition(Bytes.toBytes("test"));
+    desc.addFamily(new ColumnFamilyDefinition(COLUMN_NAME));
   }
 
   @Override
diff --git a/src/test/org/apache/hadoop/hbase/HBaseTestCase.java b/src/test/org/apache/hadoop/hbase/HBaseTestCase.java
index ad509cb..6bdbe1a 100644
--- a/src/test/org/apache/hadoop/hbase/HBaseTestCase.java
+++ b/src/test/org/apache/hadoop/hbase/HBaseTestCase.java
@@ -151,7 +151,7 @@ public abstract class HBaseTestCase extends TestCase {
         conf.get(TEST_DIRECTORY_KEY, "test/build/data"), testName);
   }
 
-  protected HRegion createNewHRegion(HTableDescriptor desc, byte [] startKey,
+  protected HRegion createNewHRegion(TableDefinition desc, byte [] startKey,
       byte [] endKey)
   throws IOException {
     FileSystem filesystem = FileSystem.get(conf);
@@ -178,7 +178,7 @@ public abstract class HBaseTestCase extends TestCase {
    * @param name Name to give table.
    * @return Column descriptor.
    */
-  protected HTableDescriptor createTableDescriptor(final String name) {
+  protected TableDefinition createTableDescriptor(final String name) {
     return createTableDescriptor(name, MAXVERSIONS);
   }
   
@@ -189,17 +189,17 @@ public abstract class HBaseTestCase extends TestCase {
    * @param versions How many versions to allow per column.
    * @return Column descriptor.
    */
-  protected HTableDescriptor createTableDescriptor(final String name,
+  protected TableDefinition createTableDescriptor(final String name,
       final int versions) {
-    HTableDescriptor htd = new HTableDescriptor(name);
-    htd.addFamily(new HColumnDescriptor(fam1, versions,
-      HColumnDescriptor.DEFAULT_COMPRESSION, false, false,
+    TableDefinition htd = new TableDefinition(name);
+    htd.addFamily(new ColumnFamilyDefinition(fam1, versions,
+      ColumnFamilyDefinition.DEFAULT_COMPRESSION, false, false,
       Integer.MAX_VALUE, HConstants.FOREVER, false));
-    htd.addFamily(new HColumnDescriptor(fam2, versions,
-        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,
+    htd.addFamily(new ColumnFamilyDefinition(fam2, versions,
+        ColumnFamilyDefinition.DEFAULT_COMPRESSION, false, false,
         Integer.MAX_VALUE, HConstants.FOREVER, false));
-    htd.addFamily(new HColumnDescriptor(fam3, versions,
-        HColumnDescriptor.DEFAULT_COMPRESSION, false, false,
+    htd.addFamily(new ColumnFamilyDefinition(fam3, versions,
+        ColumnFamilyDefinition.DEFAULT_COMPRESSION, false, false,
         Integer.MAX_VALUE,  HConstants.FOREVER, false));
     return htd;
   }
diff --git a/src/test/org/apache/hadoop/hbase/HBaseTestingUtility.java b/src/test/org/apache/hadoop/hbase/HBaseTestingUtility.java
index d3107eb..7175092 100644
--- a/src/test/org/apache/hadoop/hbase/HBaseTestingUtility.java
+++ b/src/test/org/apache/hadoop/hbase/HBaseTestingUtility.java
@@ -60,7 +60,6 @@ import org.apache.zookeeper.ZooKeeper;
  * make changes to configuration parameters.
  */
 public class HBaseTestingUtility {
-  
   private final Log LOG = LogFactory.getLog(getClass());
   
   private final Configuration conf = HBaseConfiguration.create();
@@ -68,7 +67,7 @@ public class HBaseTestingUtility {
   private MiniDFSCluster dfsCluster = null;
   private MiniHBaseCluster hbaseCluster = null;
   private MiniMRCluster mrCluster = null;
-  private File clusterTestBuildDir = null;
+  private File clusterDir = null;
   private HBaseAdmin hbaseAdmin = null;
   
   /** System property key to get test directory value.
@@ -85,8 +84,15 @@ public class HBaseTestingUtility {
   /**
    * @return Where to write test data on local filesystem; usually build/test/data
    */
-  public Path getTestDir() {
-    return new Path(System.getProperty(TEST_DIRECTORY_KEY, "build/test/data"));
+  public static Path getTestDir() {
+    return new Path(getTestDirAsStr());
+  }
+
+  /**
+   * @return Where to write test data on local filesystem; usually build/test/data
+   */
+  public static String getTestDirAsStr() {
+    return System.getProperty(TEST_DIRECTORY_KEY, "build/test/data");
   }
 
   /**
@@ -94,11 +100,29 @@ public class HBaseTestingUtility {
    * @return Path to a subdirectory named <code>subdirName</code> under
    * {@link #getTestDir()}.
    */
-  public Path getTestDir(final String subdirName) {
+  public static Path getTestDir(final String subdirName) {
     return new Path(getTestDir(), subdirName);
   }
 
   /**
+   * Home for our cluster in a dir test.  Gives it a random name
+   * so can have many concurrent clusters running if we need to.  Need to
+   * amend the test.build.data System property.  Its what minidfscluster bases
+   * its data dir on.  Moding a System property is not the way to do concurrent
+   * instances -- another instance could grab the temporary
+   * value unintentionally -- but not anything can do about it at moment; its
+   * how the minidfscluster works.
+   * @return directory for a cluster to write its files too.
+   * @throws IOException 
+   */
+  public static File getClusterDir() {
+    String oldTestBuildDir = getTestDirAsStr();
+    String randomStr = UUID.randomUUID().toString();
+    String clusterTestBuildDirStr = oldTestBuildDir + "." + randomStr;
+    return new File(clusterTestBuildDirStr).getAbsoluteFile();
+  }
+
+  /**
    * Start up a minicluster of hbase, dfs, and zookeeper.
    * @throws Exception 
    */
@@ -107,7 +131,7 @@ public class HBaseTestingUtility {
   }
 
   /**
-   * Start up a minicluster of hbase, optinally dfs, and zookeeper.
+   * Start up a minicluster of hbase, optionally dfs, and zookeeper.
    * Modifies Configuration.  Homes the cluster data directory under a random
    * subdirectory in a directory under System property test.build.data.
    * @param servers Number of servers to start up.  We'll start this many
@@ -117,31 +141,17 @@ public class HBaseTestingUtility {
    * @throws Exception
    * @see {@link #shutdownMiniCluster()}
    */
-  public void startMiniCluster(final int servers)
-  throws Exception {
-    LOG.info("Starting up minicluster");
+  public void startMiniCluster(final int servers) throws Exception {
     // If we already put up a cluster, fail.
-    if (this.clusterTestBuildDir != null) {
+    if (this.clusterDir != null) {
       throw new IOException("Cluster already running at " +
-        this.clusterTestBuildDir);
+        this.clusterDir);
     }
-    // Now, home our cluster in a dir under build/test.  Give it a random name
-    // so can have many concurrent clusters running if we need to.  Need to
-    // amend the test.build.data System property.  Its what minidfscluster bases
-    // it data dir on.  Moding a System property is not the way to do concurrent
-    // instances -- another instance could grab the temporary
-    // value unintentionally -- but not anything can do about it at moment; its
-    // how the minidfscluster works.
-    String oldTestBuildDir =
-      System.getProperty(TEST_DIRECTORY_KEY, "build/test/data");
-    String randomStr = UUID.randomUUID().toString();
-    String clusterTestBuildDirStr = oldTestBuildDir + "." + randomStr;
-    this.clusterTestBuildDir =
-      new File(clusterTestBuildDirStr).getAbsoluteFile();
+    this.clusterDir = getClusterDir();
     // Have it cleaned up on exit
-    this.clusterTestBuildDir.deleteOnExit();
+    this.clusterDir.deleteOnExit();
     // Set our random dir while minidfscluster is being constructed.
-    System.setProperty(TEST_DIRECTORY_KEY, clusterTestBuildDirStr);
+    System.setProperty(TEST_DIRECTORY_KEY, clusterDir.getPath());
     // Bring up mini dfs cluster. This spews a bunch of warnings about missing
     // scheme. TODO: fix.
     // Complaints are 'Scheme is undefined for build/test/data/dfs/name1'.
@@ -149,8 +159,7 @@ public class HBaseTestingUtility {
       true, true, null, null, null, null);
     // Restore System property. minidfscluster accesses content of
     // the TEST_DIRECTORY_KEY to make bad blocks, a feature we are not using,
-    // but otherwise, just in constructor.
-    System.setProperty(TEST_DIRECTORY_KEY, oldTestBuildDir);
+    // but otherwise, just in constructoroldTestBuildDirroperty(TEST_DIRECTORY_KEY, getTestDirAsStr());
     // Mangle conf so fs parameter points to minidfs we just started up
     FileSystem fs = this.dfsCluster.getFileSystem();
     this.conf.set("fs.defaultFS", fs.getUri().toString());
@@ -158,10 +167,8 @@ public class HBaseTestingUtility {
 
     // Note that this is done before we create the MiniHBaseCluster because we
     // need to edit the config to add the ZooKeeper servers.
-    this.zkCluster = new MiniZooKeeperCluster();
-    int clientPort = this.zkCluster.startup(this.clusterTestBuildDir);
-    this.conf.set("hbase.zookeeper.property.clientPort",
-      Integer.toString(clientPort));
+    this.zkCluster = startMiniZooKeeperCluster(this.conf,
+      this.clusterDir);
 
     // Now do the mini hbase cluster.  Set the hbase.rootdir in config.
     Path hbaseRootdir = fs.makeQualified(fs.getHomeDirectory());
@@ -187,23 +194,51 @@ public class HBaseTestingUtility {
       // Wait till hbase is down before going on to shutdown zk.
       this.hbaseCluster.join();
     }
-    if (this.zkCluster != null) this.zkCluster.shutdown();
+    shutdownMiniZooKeeperCluster(this.zkCluster);
     if (this.dfsCluster != null) {
       // The below throws an exception per dn, AsynchronousCloseException.
       this.dfsCluster.shutdown();
     }
     // Clean up our directory.
-    if (this.clusterTestBuildDir != null && this.clusterTestBuildDir.exists()) {
+    if (this.clusterDir != null && this.clusterDir.exists()) {
       // Need to use deleteDirectory because File.delete required dir is empty.
       if (!FSUtils.deleteDirectory(FileSystem.getLocal(this.conf),
-          new Path(this.clusterTestBuildDir.toString()))) {
-        LOG.warn("Failed delete of " + this.clusterTestBuildDir.toString());
+          new Path(this.clusterDir.toString()))) {
+        LOG.warn("Failed delete of " + this.clusterDir.toString());
       }
     }
     LOG.info("Minicluster is down");
   }
 
   /**
+   * @param c Configuration.  Will have "hbase.zookeeper.property.clientPort"
+   * set into it when done.
+   * @param dir Where to have zk write its logs.
+   * @return Instance of MiniZooKeeperCluster that we started up; it also
+   * populates the passed Configuration with "hbase.zookeeper.property.clientPort".
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  public static MiniZooKeeperCluster startMiniZooKeeperCluster(final Configuration c,
+      final File dir)
+  throws IOException, InterruptedException {
+    MiniZooKeeperCluster zkCluster = new MiniZooKeeperCluster();
+    int clientPort = zkCluster.startup(dir);
+    c.set("hbase.zookeeper.property.clientPort", Integer.toString(clientPort));
+    return zkCluster;
+  }
+
+  /**
+   * 
+   * @param zkc
+   * @throws IOException
+   */
+  public static void shutdownMiniZooKeeperCluster(final MiniZooKeeperCluster zkc)
+  throws IOException {
+    if (zkc != null) zkc.shutdown();
+  }
+
+  /**
    * Flushes all caches in the mini hbase cluster
    * @throws IOException
    */
@@ -233,9 +268,9 @@ public class HBaseTestingUtility {
    */
   public HTable createTable(byte[] tableName, byte[][] families) 
   throws IOException {
-    HTableDescriptor desc = new HTableDescriptor(tableName);
+    TableDefinition desc = new TableDefinition(Bytes.toString(tableName));
     for(byte[] family : families) {
-      desc.addFamily(new HColumnDescriptor(family));
+      desc.addFamily(new ColumnFamilyDefinition(Bytes.toString(family)));
     }
     (new HBaseAdmin(getConfiguration())).createTable(desc);
     return new HTable(getConfiguration(), tableName);
@@ -265,13 +300,14 @@ public class HBaseTestingUtility {
   public HTable createTable(byte[] tableName, byte[][] families,
       int numVersions)
   throws IOException {
-    HTableDescriptor desc = new HTableDescriptor(tableName);
+    TableDefinition desc = new TableDefinition(Bytes.toString(tableName));
     for (byte[] family : families) {
-      HColumnDescriptor hcd = new HColumnDescriptor(family, numVersions,
-          HColumnDescriptor.DEFAULT_COMPRESSION,
-          HColumnDescriptor.DEFAULT_IN_MEMORY,
-          HColumnDescriptor.DEFAULT_BLOCKCACHE,
-          Integer.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL, false);
+      ColumnFamilyDefinition hcd = new ColumnFamilyDefinition(Bytes.toString(family),
+          numVersions,
+          ColumnFamilyDefinition.DEFAULT_COMPRESSION,
+          ColumnFamilyDefinition.DEFAULT_IN_MEMORY,
+          ColumnFamilyDefinition.DEFAULT_BLOCKCACHE,
+          Integer.MAX_VALUE, ColumnFamilyDefinition.DEFAULT_TTL);
       desc.addFamily(hcd);
     }
     (new HBaseAdmin(getConfiguration())).createTable(desc);
@@ -289,14 +325,15 @@ public class HBaseTestingUtility {
   public HTable createTable(byte[] tableName, byte[][] families,
       int[] numVersions)
   throws IOException {
-    HTableDescriptor desc = new HTableDescriptor(tableName);
+    TableDefinition desc = new TableDefinition(Bytes.toString(tableName));
     int i = 0;
     for (byte[] family : families) {
-      HColumnDescriptor hcd = new HColumnDescriptor(family, numVersions[i],
-          HColumnDescriptor.DEFAULT_COMPRESSION,
-          HColumnDescriptor.DEFAULT_IN_MEMORY,
-          HColumnDescriptor.DEFAULT_BLOCKCACHE,
-          Integer.MAX_VALUE, HColumnDescriptor.DEFAULT_TTL, false);
+      ColumnFamilyDefinition hcd = new ColumnFamilyDefinition(Bytes.toString(family),
+          numVersions[i],
+          ColumnFamilyDefinition.DEFAULT_COMPRESSION,
+          ColumnFamilyDefinition.DEFAULT_IN_MEMORY,
+          ColumnFamilyDefinition.DEFAULT_BLOCKCACHE,
+          Integer.MAX_VALUE, ColumnFamilyDefinition.DEFAULT_TTL);
       desc.addFamily(hcd);
       i++;
     }
@@ -353,9 +390,10 @@ public class HBaseTestingUtility {
 
     Configuration c = getConfiguration();
     HTable meta = new HTable(c, HConstants.META_TABLE_NAME);
-    HTableDescriptor htd = table.getTableDescriptor();
-    if(!htd.hasFamily(columnFamily)) {
-      HColumnDescriptor hcd = new HColumnDescriptor(columnFamily);
+    TableDefinition htd = table.getTableDescriptor();
+    String columnFamilyStr = Bytes.toString(columnFamily);
+    if(!htd.hasFamily(columnFamilyStr)) {
+      ColumnFamilyDefinition hcd = new ColumnFamilyDefinition(columnFamilyStr);
       htd.addFamily(hcd);
     }
     // remove empty region - this is tricky as the mini cluster during the test
@@ -404,25 +442,6 @@ public class HBaseTestingUtility {
   }
 
   /**
-   * Removes all rows from the .META. in preparation to add custom ones.
-   *
-   * @throws IOException When removing the rows fails.
-   */
-  private void emptyMetaTable() throws IOException {
-    HTable t = new HTable(this.conf, HConstants.META_TABLE_NAME);
-    ArrayList<Delete> deletes = new ArrayList<Delete>();
-    ResultScanner s = t.getScanner(new Scan());
-    for (Result result : s) {
-      LOG.info("emptyMetaTable: remove row -> " + 
-        Bytes.toStringBinary(result.getRow()));
-      Delete del = new Delete(result.getRow());
-      deletes.add(del);
-    }
-    s.close();
-    t.delete(deletes);
-  }
-  
-  /**
    * Starts a <code>MiniMRCluster</code> with a default number of 
    * <code>TaskTracker</code>'s.
    *
diff --git a/src/test/org/apache/hadoop/hbase/MultiRegionTable.java b/src/test/org/apache/hadoop/hbase/MultiRegionTable.java
index f7d2ba4..a94e2dc 100644
--- a/src/test/org/apache/hadoop/hbase/MultiRegionTable.java
+++ b/src/test/org/apache/hadoop/hbase/MultiRegionTable.java
@@ -57,7 +57,7 @@ public class MultiRegionTable extends HBaseClusterTestCase {
   };
   
   protected final byte [] columnFamily;
-  protected HTableDescriptor desc;
+  protected TableDefinition desc;
 
   /**
    * @param familyName the family to populate.
diff --git a/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java b/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
index 0400b1a..7ff4e51 100644
--- a/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
+++ b/src/test/org/apache/hadoop/hbase/PerformanceEvaluation.java
@@ -96,9 +96,9 @@ public class PerformanceEvaluation implements HConstants {
   public static final byte [] FAMILY_NAME = Bytes.toBytes("info");
   public static final byte [] QUALIFIER_NAME = Bytes.toBytes("data");
   
-  protected HTableDescriptor TABLE_DESCRIPTOR; {
-    TABLE_DESCRIPTOR = new HTableDescriptor("TestTable");
-    TABLE_DESCRIPTOR.addFamily(new HColumnDescriptor(FAMILY_NAME));
+  protected TableDefinition TABLE_DESCRIPTOR; {
+    TABLE_DESCRIPTOR = new TableDefinition("TestTable");
+    TABLE_DESCRIPTOR.addFamily(new ColumnFamilyDefinition(FAMILY_NAME));
   }
   
   private static final String RANDOM_READ = "randomRead";
@@ -168,10 +168,10 @@ public class PerformanceEvaluation implements HConstants {
    * Implementations can have their status set.
    */
   protected void setTableDesc() {
-    TABLE_DESCRIPTOR = new HTableDescriptor(conf.get("hbase.pe.tablename", "TestTable"));
-    TABLE_DESCRIPTOR.addFamily(new HColumnDescriptor(FAMILY_NAME, 
-        3, conf.get("hbase.pe.compress", HColumnDescriptor.DEFAULT_COMPRESSION), 
-        false, true, HColumnDescriptor.DEFAULT_TTL, false));
+    TABLE_DESCRIPTOR = new TableDefinition(conf.get("hbase.pe.tablename", "TestTable"));
+    TABLE_DESCRIPTOR.addFamily(new ColumnFamilyDefinition(FAMILY_NAME, 
+        3, conf.get("hbase.pe.compress", ColumnFamilyDefinition.DEFAULT_COMPRESSION), 
+        false, true, ColumnFamilyDefinition.DEFAULT_TTL, false));
   }
   
   /**
diff --git a/src/test/org/apache/hadoop/hbase/TestColumnFamilyDefinition.java b/src/test/org/apache/hadoop/hbase/TestColumnFamilyDefinition.java
new file mode 100644
index 0000000..c8642a7
--- /dev/null
+++ b/src/test/org/apache/hadoop/hbase/TestColumnFamilyDefinition.java
@@ -0,0 +1,166 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotSame;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.io.hfile.Compression;
+import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
+import org.codehaus.jackson.JsonParseException;
+import org.codehaus.jackson.map.JsonMappingException;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.junit.Before;
+import org.junit.Test;
+
+public class TestColumnFamilyDefinition {
+  private final Log LOG = LogFactory.getLog(this.getClass().getName());
+  private ColumnFamilyDefinition cfd;
+
+  @Before
+  public void setUp() throws Exception {
+    this.cfd = new ColumnFamilyDefinition("a");
+  }
+
+  @Test public void testSerialization()
+  throws JsonParseException, JsonMappingException, IOException {
+    String serialized = this.cfd.toString();
+    LOG.info("before=" + serialized);
+    ObjectMapper mapper = new ObjectMapper();
+    Reader reader = new StringReader(serialized);
+    ColumnFamilyDefinition deserhcd =
+      mapper.readValue(reader, ColumnFamilyDefinition.class);
+    LOG.info("after=" + deserhcd.toString());
+    assertEquals(this.cfd.hashCode(), deserhcd.hashCode());
+    // Do a more complicated one
+    this.cfd.setBlocksize(123);
+    this.cfd.setCompressionType(Algorithm.LZO);
+    this.cfd.setInMemory(true);
+    this.cfd.put("arbitrary", "some value");
+    serialized = this.cfd.toString();
+    LOG.info("before2=" + serialized);
+    reader = new StringReader(serialized);
+    deserhcd = mapper.readValue(reader, ColumnFamilyDefinition.class);
+    LOG.info("after2=" + deserhcd.toString());
+    assertEquals(this.cfd.hashCode(), deserhcd.hashCode());
+  }
+
+  @Test
+  public void testHashCode() {
+    ColumnFamilyDefinition hcd2 = new ColumnFamilyDefinition("a");
+    assertEquals(this.cfd.hashCode(), hcd2.hashCode());
+    ColumnFamilyDefinition hcd3 = new ColumnFamilyDefinition("b");
+    assertNotSame(this.cfd.hashCode(), hcd3.hashCode());
+    ColumnFamilyDefinition hcd4 = new ColumnFamilyDefinition(this.cfd);
+    assertEquals(this.cfd.hashCode(), hcd4.hashCode());
+  }
+
+  @Test
+  public void testIsLegalFamilyName() {
+    ColumnFamilyDefinition.isLegalFamilyName("a");
+  }
+
+  @Test (expected=IllegalArgumentException.class)
+  public void testIllegalFamilyName1() {
+    ColumnFamilyDefinition.isLegalFamilyName("a:");
+  }
+
+  @Test (expected=IllegalArgumentException.class)
+  public void testIllegalFamilyName2() {
+    ColumnFamilyDefinition.isLegalFamilyName(".META.");
+  }
+
+  @Test
+  public void testGetCompression() {
+    assertEquals(Compression.Algorithm.NONE, this.cfd.getCompression());
+    this.cfd.setCompressionType(Compression.Algorithm.LZO);
+    assertEquals(Compression.Algorithm.LZO, this.cfd.getCompression());
+  }
+
+  @Test
+  public void testGetMaxVersions() {
+    assertEquals(ColumnFamilyDefinition.DEFAULT_VERSIONS, this.cfd.getMaxVersions());
+    int max = 5;
+    this.cfd.setMaxVersions(max);
+    assertEquals(max, this.cfd.getMaxVersions());
+  }
+
+  @Test
+  public void testGetBlocksize() {
+    assertEquals(ColumnFamilyDefinition.DEFAULT_BLOCKSIZE, this.cfd.getBlocksize());
+    int max = 12;
+    this.cfd.setBlocksize(max);
+    assertEquals(max, this.cfd.getBlocksize());
+  }
+
+  @Test
+  public void testIsInMemory() {
+    assertEquals(ColumnFamilyDefinition.DEFAULT_IN_MEMORY, this.cfd.isInMemory());
+    boolean b = true;
+    this.cfd.setInMemory(b);
+    assertEquals(b, this.cfd.isInMemory());
+  }
+
+  @Test
+  public void testGetTimeToLive() {
+    assertEquals(ColumnFamilyDefinition.DEFAULT_TTL, this.cfd.getTimeToLive());
+    int max = 12;
+    this.cfd.setTimeToLive(max);
+    assertEquals(max, this.cfd.getTimeToLive());
+  }
+
+  @Test
+  public void testIsBlockCacheEnabled() {
+    assertEquals(ColumnFamilyDefinition.DEFAULT_BLOCKCACHE,
+      this.cfd.isBlockCacheEnabled());
+    boolean b = true;
+    this.cfd.isBlockCacheEnabled();
+    assertEquals(b, this.cfd.isBlockCacheEnabled());
+  }
+
+  @Test
+  public void testToString() {
+    this.cfd.setBlockCacheEnabled(true);
+    this.cfd.setBlocksize(123);
+    this.cfd.setCompressionType(Algorithm.GZ);
+    this.cfd.setInMemory(true);
+    this.cfd.setMaxVersions(47);
+    this.cfd.setTimeToLive(21);
+    System.out.println(this.cfd.toString());
+  }
+
+  @Test
+  public void testEqualsObject() {
+    ColumnFamilyDefinition hcd2 = new ColumnFamilyDefinition("a");
+    assertEquals(this.cfd, hcd2);
+  }
+
+  @Test
+  public void testNotEqualsObject() {
+    ColumnFamilyDefinition hcd2 = new ColumnFamilyDefinition("b");
+    assertNotSame(this.cfd, hcd2);
+  }
+}
diff --git a/src/test/org/apache/hadoop/hbase/TestCompare.java b/src/test/org/apache/hadoop/hbase/TestCompare.java
index a807b12..51ddec2 100644
--- a/src/test/org/apache/hadoop/hbase/TestCompare.java
+++ b/src/test/org/apache/hadoop/hbase/TestCompare.java
@@ -31,10 +31,10 @@ public class TestCompare extends TestCase {
    * Sort of HRegionInfo.
    */
   public void testHRegionInfo() {
-    HRegionInfo a = new HRegionInfo(new HTableDescriptor("a"), null, null);
-    HRegionInfo b = new HRegionInfo(new HTableDescriptor("b"), null, null);
+    HRegionInfo a = new HRegionInfo(new TableDefinition("a"), null, null);
+    HRegionInfo b = new HRegionInfo(new TableDefinition("b"), null, null);
     assertTrue(a.compareTo(b) != 0);
-    HTableDescriptor t = new HTableDescriptor("t");
+    TableDefinition t = new TableDefinition("t");
     byte [] midway = Bytes.toBytes("midway");
     a = new HRegionInfo(t, null, midway);
     b = new HRegionInfo(t, midway, null);
diff --git a/src/test/org/apache/hadoop/hbase/TestHMsg.java b/src/test/org/apache/hadoop/hbase/TestHMsg.java
index ec68801..e6f90be 100644
--- a/src/test/org/apache/hadoop/hbase/TestHMsg.java
+++ b/src/test/org/apache/hadoop/hbase/TestHMsg.java
@@ -34,7 +34,7 @@ public class TestHMsg extends TestCase {
     for (int i = 0; i < size; i++) {
       byte [] b = Bytes.toBytes(i);
       hmsg = new HMsg(HMsg.Type.MSG_REGION_OPEN,
-        new HRegionInfo(new HTableDescriptor(Bytes.toBytes("test")), b, b));
+        new HRegionInfo(new TableDefinition(Bytes.toBytes("test")), b, b));
       msgs.add(hmsg);
     }
     assertEquals(size, msgs.size());
@@ -44,12 +44,12 @@ public class TestHMsg extends TestCase {
     assertEquals(size - 1, msgs.size());
     byte [] other = Bytes.toBytes("other");
     hmsg = new HMsg(HMsg.Type.MSG_REGION_OPEN,
-      new HRegionInfo(new HTableDescriptor(Bytes.toBytes("test")), other, other));
+      new HRegionInfo(new TableDefinition(Bytes.toBytes("test")), other, other));
     assertEquals(-1, msgs.indexOf(hmsg));
     // Assert that two HMsgs are same if same content.
     byte [] b = Bytes.toBytes(1);
     hmsg = new HMsg(HMsg.Type.MSG_REGION_OPEN,
-     new HRegionInfo(new HTableDescriptor(Bytes.toBytes("test")), b, b));
+     new HRegionInfo(new TableDefinition(Bytes.toBytes("test")), b, b));
     assertNotSame(-1, msgs.indexOf(hmsg));
   }
 }
diff --git a/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java b/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java
index 83ad2e1..1d0680d 100644
--- a/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java
+++ b/src/test/org/apache/hadoop/hbase/TestRegionRebalancing.java
@@ -40,7 +40,7 @@ public class TestRegionRebalancing extends HBaseClusterTestCase {
   final Log LOG = LogFactory.getLog(this.getClass().getName());
   HTable table;
 
-  HTableDescriptor desc;
+  TableDefinition desc;
   
   final byte[] FIVE_HUNDRED_KBYTES;
   
@@ -54,8 +54,8 @@ public class TestRegionRebalancing extends HBaseClusterTestCase {
       FIVE_HUNDRED_KBYTES[i] = 'x';
     }
     
-    desc = new HTableDescriptor("test");
-    desc.addFamily(new HColumnDescriptor(FAMILY_NAME));
+    desc = new TableDefinition("test");
+    desc.addFamily(new ColumnFamilyDefinition(FAMILY_NAME));
   }
   
   /**
diff --git a/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java b/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java
index 2976477..c9bbe6f 100644
--- a/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java
+++ b/src/test/org/apache/hadoop/hbase/TestScanMultipleVersions.java
@@ -45,7 +45,7 @@ public class TestScanMultipleVersions extends HBaseClusterTestCase {
       100L,
       1000L
   };
-  private HTableDescriptor desc = null;
+  private TableDefinition desc = null;
 
   @Override
   protected void preHBaseClusterSetup() throws Exception {
@@ -53,8 +53,8 @@ public class TestScanMultipleVersions extends HBaseClusterTestCase {
     
     // Create table description
     
-    this.desc = new HTableDescriptor(TABLE_NAME);
-    this.desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
+    this.desc = new TableDefinition(TABLE_NAME);
+    this.desc.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY));
 
     // Region 0 will contain the key range [,row_0500)
     INFOS[0] = new HRegionInfo(this.desc, HConstants.EMPTY_START_ROW,
diff --git a/src/test/org/apache/hadoop/hbase/TestSerialization.java b/src/test/org/apache/hadoop/hbase/TestSerialization.java
index 1c9f30e..152f155 100644
--- a/src/test/org/apache/hadoop/hbase/TestSerialization.java
+++ b/src/test/org/apache/hadoop/hbase/TestSerialization.java
@@ -96,7 +96,7 @@ public class TestSerialization extends HBaseTestCase {
     HMsg deserializedHMsg = (HMsg)Writables.getWritable(mb, new HMsg());
     assertTrue(m.equals(deserializedHMsg));
     m = new HMsg(HMsg.Type.MSG_REGIONSERVER_QUIESCE,
-      new HRegionInfo(new HTableDescriptor(getName()),
+      new HRegionInfo(new TableDefinition(getName()),
         HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY),
         "Some message".getBytes());
     mb = Writables.getBytes(m);
@@ -105,10 +105,10 @@ public class TestSerialization extends HBaseTestCase {
   }
   
   public void testTableDescriptor() throws Exception {
-    HTableDescriptor htd = createTableDescriptor(getName());
+    TableDefinition htd = createTableDescriptor(getName());
     byte [] mb = Writables.getBytes(htd);
-    HTableDescriptor deserializedHtd =
-      (HTableDescriptor)Writables.getWritable(mb, new HTableDescriptor());
+    TableDefinition deserializedHtd =
+      (TableDefinition)Writables.getWritable(mb, new TableDefinition());
     assertEquals(htd.getNameAsString(), deserializedHtd.getNameAsString());
   }
 
@@ -117,10 +117,10 @@ public class TestSerialization extends HBaseTestCase {
    * @throws Exception
    */
   public void testRegionInfo() throws Exception {
-    HTableDescriptor htd = new HTableDescriptor(getName());
+    TableDefinition htd = new TableDefinition(getName());
     String [] families = new String [] {"info", "anchor"};
     for (int i = 0; i < families.length; i++) {
-      htd.addFamily(new HColumnDescriptor(families[i]));
+      htd.addFamily(new ColumnFamilyDefinition(families[i]));
     }
     HRegionInfo hri = new HRegionInfo(htd,
       HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
diff --git a/src/test/org/apache/hadoop/hbase/TestTableDefinition.java b/src/test/org/apache/hadoop/hbase/TestTableDefinition.java
new file mode 100644
index 0000000..9324a60
--- /dev/null
+++ b/src/test/org/apache/hadoop/hbase/TestTableDefinition.java
@@ -0,0 +1,76 @@
+/**
+ * Copyright 2009 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import static org.junit.Assert.assertEquals;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.codehaus.jackson.JsonParseException;
+import org.codehaus.jackson.map.JsonMappingException;
+import org.codehaus.jackson.map.ObjectMapper;
+import org.junit.Test;
+
+
+public class TestTableDefinition {
+  private final Log LOG = LogFactory.getLog(this.getClass().getName());
+
+  @Test public void testEquals() {
+    TableDefinition td = new TableDefinition("test");
+    td.addFamily(new ColumnFamilyDefinition("a"));
+    td.addFamily(new ColumnFamilyDefinition("b"));
+    td.addFamily(new ColumnFamilyDefinition("c"));
+    TableDefinition td2 = new TableDefinition("test");
+    td2.addFamily(new ColumnFamilyDefinition("a"));
+    td2.addFamily(new ColumnFamilyDefinition("b"));
+    td2.addFamily(new ColumnFamilyDefinition("c"));
+    assertEquals(td, td2);
+  }
+
+  @Test public void testSerialization()
+  throws JsonParseException, JsonMappingException, IOException {
+    TableDefinition td = new TableDefinition("test");
+    td.addFamily(new ColumnFamilyDefinition("a"));
+    td.addFamily(new ColumnFamilyDefinition("b"));
+    td.addFamily(new ColumnFamilyDefinition("c"));
+    String serialized = td.toString();
+    LOG.info("before=" + serialized);
+    ObjectMapper mapper = new ObjectMapper();
+    Reader reader = new StringReader(serialized);
+    TableDefinition desertd =
+      mapper.readValue(reader, TableDefinition.class);
+    LOG.info("after=" + desertd.toString());
+    assertEquals(td.hashCode(), desertd.hashCode());
+    // Do a more complicated one
+    td.setDeferredLogFlush(true);
+    td.setMaxFileSize(123);
+    td.setMemStoreFlushSize(456);
+    serialized = td.toString();
+    LOG.info("before2=" + serialized);
+    reader = new StringReader(serialized);
+    desertd = mapper.readValue(reader, TableDefinition.class);
+    LOG.info("after2=" + desertd.toString());
+    assertEquals(td.hashCode(), desertd.hashCode());
+  }
+}
\ No newline at end of file
diff --git a/src/test/org/apache/hadoop/hbase/TestTableState.java b/src/test/org/apache/hadoop/hbase/TestTableState.java
new file mode 100644
index 0000000..77a1725
--- /dev/null
+++ b/src/test/org/apache/hadoop/hbase/TestTableState.java
@@ -0,0 +1,66 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase;
+
+import java.io.IOException;
+
+import junit.framework.Assert;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.util.JSONing;
+import org.junit.Test;
+
+/**
+ * Test table state
+ */
+public class TestTableState {
+  private final Log LOG = LogFactory.getLog(this.getClass().getName());
+
+  @Test public void testBasicTableState() throws IOException {
+    TableState state = new TableState();
+    Assert.assertTrue(!state.isReadonly());
+    Assert.assertTrue(!state.isOffline());
+    state.setOffline(true);
+    state.setReadonly(true);
+    Assert.assertTrue(state.isReadonly());
+    Assert.assertTrue(state.isOffline());
+    String serialization = state.toString();
+    LOG.info(serialization);
+    TableState newState =
+      (TableState)JSONing.deserialize(TableState.class, serialization);
+    Assert.assertTrue(newState.isReadonly());
+    Assert.assertTrue(newState.isOffline());
+  }
+  
+  @Test public void testEquals() {
+    TableState one = new TableState();
+    TableState two = new TableState();
+    Assert.assertEquals(one, two);
+    one.setReadonly(true);
+    Assert.assertNotSame(one, two);
+    two.setReadonly(true);
+    Assert.assertEquals(one, two);
+    one.setOffline(true);
+    Assert.assertNotSame(one, two);
+    two.setOffline(true);
+    Assert.assertEquals(one, two);
+  }
+}
\ No newline at end of file
diff --git a/src/test/org/apache/hadoop/hbase/TestZooKeeper.java b/src/test/org/apache/hadoop/hbase/TestZooKeeper.java
index 935c42d..29c9408 100644
--- a/src/test/org/apache/hadoop/hbase/TestZooKeeper.java
+++ b/src/test/org/apache/hadoop/hbase/TestZooKeeper.java
@@ -22,6 +22,7 @@ package org.apache.hadoop.hbase;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.client.*;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.JSONing;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper;
 import org.apache.zookeeper.ZooKeeper;
 import org.junit.*;
@@ -59,8 +60,8 @@ public class TestZooKeeper {
    */
   @Before
   public void setUp() throws Exception {
-    conf = TEST_UTIL.getConfiguration();
-    cluster = TEST_UTIL.getHBaseCluster();
+    this.conf = TEST_UTIL.getConfiguration();
+    TEST_UTIL.getHBaseCluster();
   }
 
   /**
@@ -118,9 +119,9 @@ public class TestZooKeeper {
 
     HBaseAdmin admin = new HBaseAdmin(conf);
     String tableName = "test"+System.currentTimeMillis();
-    HTableDescriptor desc =
-        new HTableDescriptor(tableName);
-    HColumnDescriptor family = new HColumnDescriptor("fam");
+    TableDefinition desc =
+        new TableDefinition(tableName);
+    ColumnFamilyDefinition family = new ColumnFamilyDefinition("fam");
     desc.addFamily(family);
     admin.createTable(desc);
 
diff --git a/src/test/org/apache/hadoop/hbase/client/TestAdmin.java b/src/test/org/apache/hadoop/hbase/client/TestAdmin.java
index 982fe20..0410db9 100644
--- a/src/test/org/apache/hadoop/hbase/client/TestAdmin.java
+++ b/src/test/org/apache/hadoop/hbase/client/TestAdmin.java
@@ -31,11 +31,11 @@ import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.TableExistsException;
 import org.apache.hadoop.hbase.TableNotDisabledException;
 import org.apache.hadoop.hbase.TableNotFoundException;
@@ -76,7 +76,7 @@ public class TestAdmin {
 
   @Test
   public void testCreateTable() throws IOException {
-    HTableDescriptor [] tables = admin.listTables();
+    TableDefinition [] tables = admin.listTables();
     int numTables = tables.length;
     TEST_UTIL.createTable(Bytes.toBytes("testCreateTable"),
       HConstants.CATALOG_FAMILY);
@@ -223,7 +223,7 @@ public class TestAdmin {
    */
   @Test (expected=IllegalArgumentException.class)
   public void testEmptyHHTableDescriptor() throws IOException {
-    this.admin.createTable(new HTableDescriptor());
+    this.admin.createTable(new TableDefinition());
   }
 
   @Test
@@ -236,7 +236,7 @@ public class TestAdmin {
     } catch (org.apache.hadoop.hbase.client.RegionOfflineException e) {
       // Expected
     }
-    this.admin.addColumn(tableName, new HColumnDescriptor("col2"));
+    this.admin.addColumn(tableName, new ColumnFamilyDefinition("col2"));
     this.admin.enableTable(tableName);
     try {
       this.admin.deleteColumn(tableName, Bytes.toBytes("col2"));
@@ -252,27 +252,27 @@ public class TestAdmin {
   public void testCreateBadTables() throws IOException {
     String msg = null;
     try {
-      this.admin.createTable(HTableDescriptor.ROOT_TABLEDESC);
+      this.admin.createTable(TableSchema.TableDefinition);
     } catch (IllegalArgumentException e) {
       msg = e.toString();
     }
     assertTrue("Unexcepted exception message " + msg, msg != null &&
       msg.startsWith(IllegalArgumentException.class.getName()) &&
-      msg.contains(HTableDescriptor.ROOT_TABLEDESC.getNameAsString()));
+      msg.contains(TableSchema.TableDefinition.getNameAsString()));
     msg = null;
     try {
-      this.admin.createTable(HTableDescriptor.META_TABLEDESC);
+      this.admin.createTable(TableSchema.TableDefinition);
     } catch(IllegalArgumentException e) {
       msg = e.toString();
     }
     assertTrue("Unexcepted exception message " + msg, msg != null &&
       msg.startsWith(IllegalArgumentException.class.getName()) &&
-      msg.contains(HTableDescriptor.META_TABLEDESC.getNameAsString()));
+      msg.contains(TableSchema.TableDefinition.getNameAsString()));
 
     // Now try and do concurrent creation with a bunch of threads.
-    final HTableDescriptor threadDesc =
-      new HTableDescriptor("threaded_testCreateBadTables");
-    threadDesc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
+    final TableDefinition threadDesc =
+      new TableDefinition("threaded_testCreateBadTables");
+    threadDesc.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY));
     int count = 10;
     Thread [] threads = new Thread [count];
     final AtomicInteger successes = new AtomicInteger(0);
@@ -318,8 +318,8 @@ public class TestAdmin {
   @Test
   public void testTableNameClash() throws Exception {
     String name = "testTableNameClash";
-    admin.createTable(new HTableDescriptor(name + "SOMEUPPERCASE"));
-    admin.createTable(new HTableDescriptor(name));
+    admin.createTable(new TableDefinition(name + "SOMEUPPERCASE"));
+    admin.createTable(new TableDefinition(name));
     // Before fix, below would fail throwing a NoServerForRegionException.
     new HTable(TEST_UTIL.getConfiguration(), name);
   }
@@ -354,7 +354,7 @@ public class TestAdmin {
     };
     for (int i = 0; i < illegalNames.length; i++) {
       try {
-        new HTableDescriptor(illegalNames[i]);
+        new TableDefinition(illegalNames[i]);
         throw new IOException("Did not detect '" +
           Bytes.toString(illegalNames[i]) + "' as an illegal user table name");
       } catch (IllegalArgumentException e) {
@@ -363,7 +363,7 @@ public class TestAdmin {
     }
     byte[] legalName = Bytes.toBytes("g-oo.d");
     try {
-      new HTableDescriptor(legalName);
+      new TableDefinition(legalName);
     } catch (IllegalArgumentException e) {
       throw new IOException("Legal user table name: '" +
         Bytes.toString(legalName) + "' caused IllegalArgumentException: " +
diff --git a/src/test/org/apache/hadoop/hbase/client/TestFromClientSide.java b/src/test/org/apache/hadoop/hbase/client/TestFromClientSide.java
index d5b97ec..66d8f3f 100644
--- a/src/test/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ b/src/test/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -36,11 +36,11 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter;
@@ -3332,8 +3332,8 @@ public class TestFromClientSide {
       TEST_UTIL.createTable(tables[i], FAMILY);
     }
     HBaseAdmin admin = new HBaseAdmin(TEST_UTIL.getConfiguration());
-    HTableDescriptor[] ts = admin.listTables();
-    HashSet<HTableDescriptor> result = new HashSet<HTableDescriptor>(ts.length);
+    TableDefinition[] ts = admin.listTables();
+    HashSet<TableDefinition> result = new HashSet<TableDefinition>(ts.length);
     for (int i = 0; i < ts.length; i++) {
       result.add(ts[i]);
     }
@@ -3397,13 +3397,13 @@ public class TestFromClientSide {
     // Test user metadata
     HBaseAdmin admin = new HBaseAdmin(TEST_UTIL.getConfiguration());
     // make a modifiable descriptor
-    HTableDescriptor desc = new HTableDescriptor(a.getTableDescriptor());
+    TableDefinition desc = new TableDefinition(a.getTableDescriptor());
     // offline the table
     admin.disableTable(tableAname);
     // add a user attribute to HTD
     desc.setValue(attrName, attrValue);
     // add a user attribute to HCD
-    for (HColumnDescriptor c : desc.getFamilies())
+    for (ColumnFamilyDefinition c : desc.getFamilies())
       c.setValue(attrName, attrValue);
     // update metadata for all regions of this table
     admin.modifyTable(tableAname, HConstants.Modify.TABLE_SET_HTD, desc);
@@ -3420,7 +3420,7 @@ public class TestFromClientSide {
     assertFalse("HTD attribute value is incorrect",
       Bytes.compareTo(value, attrValue) != 0);
     // check HCD attribute
-    for (HColumnDescriptor c : desc.getFamilies()) {
+    for (ColumnFamilyDefinition c : desc.getFamilies()) {
       value = c.getValue(attrName);
       assertFalse("missing HCD attribute value", value == null);
       assertFalse("HCD attribute value is incorrect",
diff --git a/src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java b/src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java
index 46da51e..63fd3e4 100644
--- a/src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java
+++ b/src/test/org/apache/hadoop/hbase/client/TestGetRowVersions.java
@@ -25,8 +25,8 @@ import java.util.NavigableMap;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.util.Bytes;
 
 /**
@@ -49,8 +49,8 @@ public class TestGetRowVersions extends HBaseClusterTestCase {
   @Override
   public void setUp() throws Exception {
     super.setUp();
-    HTableDescriptor desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(CONTENTS));
+    TableDefinition desc = new TableDefinition(TABLE_NAME);
+    desc.addFamily(new ColumnFamilyDefinition(CONTENTS));
     this.admin = new HBaseAdmin(conf);
     this.admin.createTable(desc);
     this.table = new HTable(conf, TABLE_NAME);
diff --git a/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java b/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java
index bcefa7c..b15bf2f 100644
--- a/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java
+++ b/src/test/org/apache/hadoop/hbase/client/TestTimestamp.java
@@ -21,8 +21,8 @@ package org.apache.hadoop.hbase.client;
 import java.io.IOException;
 
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.TimestampTestBase;
 
 /**
@@ -67,8 +67,8 @@ public class TestTimestamp extends HBaseClusterTestCase {
    * @throws IOException
    */
   private HTable createTable() throws IOException {
-    HTableDescriptor desc = new HTableDescriptor(getName());
-    desc.addFamily(new HColumnDescriptor(COLUMN_NAME));
+    TableDefinition desc = new TableDefinition(getName());
+    desc.addFamily(new ColumnFamilyDefinition(COLUMN_NAME));
     HBaseAdmin admin = new HBaseAdmin(conf);
     admin.createTable(desc);
     return new HTable(conf, getName());
diff --git a/src/test/org/apache/hadoop/hbase/filter/TestFilter.java b/src/test/org/apache/hadoop/hbase/filter/TestFilter.java
index ebd9202..5e0c1f1 100644
--- a/src/test/org/apache/hadoop/hbase/filter/TestFilter.java
+++ b/src/test/org/apache/hadoop/hbase/filter/TestFilter.java
@@ -8,10 +8,10 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Put;
@@ -67,9 +67,9 @@ public class TestFilter extends HBaseTestCase {
   
   protected void setUp() throws Exception {
     super.setUp();
-    HTableDescriptor htd = new HTableDescriptor(getName());
-    htd.addFamily(new HColumnDescriptor(FAMILIES[0]));
-    htd.addFamily(new HColumnDescriptor(FAMILIES[1]));
+    TableDefinition htd = new TableDefinition(getName());
+    htd.addFamily(new ColumnFamilyDefinition(FAMILIES[0]));
+    htd.addFamily(new ColumnFamilyDefinition(FAMILIES[1]));
     HRegionInfo info = new HRegionInfo(htd, null, null, false);
     this.region = HRegion.createHRegion(info, this.testDir, this.conf);
     
diff --git a/src/test/org/apache/hadoop/hbase/mapreduce/DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex.java b/src/test/org/apache/hadoop/hbase/mapreduce/DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex.java
index d61b19d..fb92d86 100644
--- a/src/test/org/apache/hadoop/hbase/mapreduce/DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex.java
+++ b/src/test/org/apache/hadoop/hbase/mapreduce/DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex.java
@@ -32,8 +32,8 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MultiRegionTable;
 import org.apache.hadoop.hbase.client.HTable;
@@ -69,9 +69,9 @@ public class DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex extends
   /** default constructor */
   public DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex() {
     super(Bytes.toString(INPUT_FAMILY));
-    desc = new HTableDescriptor(TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(INPUT_FAMILY));
-    desc.addFamily(new HColumnDescriptor(OUTPUT_FAMILY));
+    desc = new TableDefinition(TABLE_NAME);
+    desc.addFamily(new ColumnFamilyDefinition(INPUT_FAMILY));
+    desc.addFamily(new ColumnFamilyDefinition(OUTPUT_FAMILY));
   }
 
     @Override
diff --git a/src/test/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java b/src/test/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
index 4a7d62c..1d895f0 100644
--- a/src/test/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
+++ b/src/test/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
@@ -28,9 +28,9 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MultiRegionTable;
 import org.apache.hadoop.hbase.client.HTable;
@@ -60,9 +60,9 @@ public class TestTableMapReduce extends MultiRegionTable {
   /** constructor */ 
   public TestTableMapReduce() {
     super(Bytes.toString(INPUT_FAMILY));
-    desc = new HTableDescriptor(MULTI_REGION_TABLE_NAME);
-    desc.addFamily(new HColumnDescriptor(INPUT_FAMILY));
-    desc.addFamily(new HColumnDescriptor(OUTPUT_FAMILY));
+    desc = new TableDefinition(MULTI_REGION_TABLE_NAME);
+    desc.addFamily(new ColumnFamilyDefinition(INPUT_FAMILY));
+    desc.addFamily(new ColumnFamilyDefinition(OUTPUT_FAMILY));
   }
 
   /**
diff --git a/src/test/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java b/src/test/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
index 667d43b..618ade8 100644
--- a/src/test/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
+++ b/src/test/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
@@ -34,8 +34,8 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
@@ -74,7 +74,7 @@ public class TestTimeRangeMapRed extends HBaseClusterTestCase {
   static final byte[] FAMILY_NAME = Bytes.toBytes("text");
   static final byte[] COLUMN_NAME = Bytes.toBytes("input");
  
-  protected HTableDescriptor desc;
+  protected TableDefinition desc;
   protected HTable table;
  
   public TestTimeRangeMapRed() {
@@ -87,8 +87,8 @@ public class TestTimeRangeMapRed extends HBaseClusterTestCase {
   @Override 
   public void setUp() throws Exception {
     super.setUp();
-    desc = new HTableDescriptor(TABLE_NAME);
-    HColumnDescriptor col = new HColumnDescriptor(FAMILY_NAME);
+    desc = new TableDefinition(TABLE_NAME);
+    ColumnFamilyDefinition col = new ColumnFamilyDefinition(FAMILY_NAME);
     col.setMaxVersions(Integer.MAX_VALUE);
     desc.addFamily(col);
     HBaseAdmin admin = new HBaseAdmin(conf);
diff --git a/src/test/org/apache/hadoop/hbase/master/TestRegionManager.java b/src/test/org/apache/hadoop/hbase/master/TestRegionManager.java
index 308a756..58bb270 100644
--- a/src/test/org/apache/hadoop/hbase/master/TestRegionManager.java
+++ b/src/test/org/apache/hadoop/hbase/master/TestRegionManager.java
@@ -4,7 +4,7 @@ import org.apache.hadoop.hbase.HBaseClusterTestCase;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.util.Bytes;
 
@@ -14,8 +14,8 @@ public class TestRegionManager extends HBaseClusterTestCase {
      HTable meta = new HTable(HConstants.META_TABLE_NAME);
      HMaster master = this.cluster.getMaster();
      HServerAddress address = master.getMasterAddress();
-     HTableDescriptor tableDesc = new HTableDescriptor(Bytes.toBytes("_MY_TABLE_"));
-     HTableDescriptor metaTableDesc = meta.getTableDescriptor();
+     TableDefinition tableDesc = new TableDefinition(Bytes.toBytes("_MY_TABLE_"));
+     TableDefinition metaTableDesc = meta.getTableDescriptor();
      // master.regionManager.onlineMetaRegions already contains first .META. region at key Bytes.toBytes("")
      byte[] startKey0 = Bytes.toBytes("f");
      byte[] endKey0 = Bytes.toBytes("h");
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java b/src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
index 246c360..e259bee 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/DisabledTestRegionServerExit.java
@@ -26,9 +26,9 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.LocalHBaseCluster;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
@@ -102,8 +102,8 @@ public class DisabledTestRegionServerExit extends HBaseClusterTestCase {
   
   private byte [] createTableAndAddRow(final String tableName)
   throws IOException {
-    HTableDescriptor desc = new HTableDescriptor(tableName);
-    desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
+    TableDefinition desc = new TableDefinition(tableName);
+    desc.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY));
     HBaseAdmin admin = new HBaseAdmin(conf);
     admin.createTable(desc);
     // put some values in the table
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java b/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index 0be6bec..5e97460 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -28,7 +28,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseTestCase;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.client.Delete;
@@ -72,7 +72,7 @@ public class TestCompaction extends HBaseTestCase {
     this.conf.set(HConstants.HBASE_DIR,
       this.cluster.getFileSystem().getHomeDirectory().toString());
     super.setUp();
-    HTableDescriptor htd = createTableDescriptor(getName());
+    TableDefinition htd = createTableDescriptor(getName());
     this.r = createNewHRegion(htd, null, null);
     this.compactionDir = HRegion.getCompactionDir(this.r.getBaseDir());
     this.regionCompactionDir = new Path(this.compactionDir, 
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java b/src/test/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
index 95dded5..21ccd06 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
@@ -31,7 +31,7 @@ import org.apache.hadoop.hbase.HBaseTestCase;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
@@ -78,7 +78,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase implements HConstant
       rootdir, this.conf);
     // Write rows for three tables 'A', 'B', and 'C'.
     for (char c = 'A'; c < 'D'; c++) {
-      HTableDescriptor htd = new HTableDescriptor("" + c);
+      TableDefinition htd = new TableDefinition("" + c);
       final int last = 128;
       final int interval = 2;
       for (int i = 0; i <= last; i += interval) {
@@ -182,7 +182,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase implements HConstant
     byte [] c0 = COLUMNS[0];
     byte [] c1 = COLUMNS[1];
     try {
-      HTableDescriptor htd = createTableDescriptor(getName());
+      TableDefinition htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
       
       Put p = new Put(T00);
@@ -289,7 +289,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase implements HConstant
     HRegion region = null;
     byte [] c0 = COLUMNS[0];
     try {
-      HTableDescriptor htd = createTableDescriptor(getName());
+      TableDefinition htd = createTableDescriptor(getName());
       region = createNewHRegion(htd, null, null);
       
       Put p = new Put(T10);
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java b/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 721755d..b323adb 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -29,10 +29,10 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
@@ -1907,9 +1907,9 @@ public class TestHRegion extends HBaseTestCase {
   private void initHRegion (byte [] tableName, String callingMethod,
     HBaseConfiguration conf, byte [] ... families)
   throws IOException{
-    HTableDescriptor htd = new HTableDescriptor(tableName);
+    TableDefinition htd = new TableDefinition(tableName);
     for(byte [] family : families) {
-      htd.addFamily(new HColumnDescriptor(family));
+      htd.addFamily(new ColumnFamilyDefinition(family));
     }
     HRegionInfo info = new HRegionInfo(htd, null, null, false);
     Path path = new Path(DIR + callingMethod); 
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java b/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java
index 55f8fff..05dfad2 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestScanner.java
@@ -28,11 +28,11 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HServerAddress;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Put;
@@ -61,10 +61,10 @@ public class TestScanner extends HBaseTestCase {
       //HConstants.STARTCODE_QUALIFIER
   };
   
-  static final HTableDescriptor TESTTABLEDESC =
-    new HTableDescriptor("testscanner");
+  static final TableDefinition TESTTABLEDESC =
+    new TableDefinition("testscanner");
   static {
-    TESTTABLEDESC.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY,
+    TESTTABLEDESC.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY,
       10,  // Ten is arbitrary number.  Keep versions to help debuggging.
       Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,
       HConstants.FOREVER, false));
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestStore.java b/src/test/org/apache/hadoop/hbase/regionserver/TestStore.java
index e59c2a2..f80c2f8 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestStore.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestStore.java
@@ -4,9 +4,9 @@ import junit.framework.TestCase;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.client.Get;
@@ -72,7 +72,7 @@ public class TestStore extends TestCase {
     //Setting up a Store
     Path basedir = new Path(DIR+methodName);
     Path logdir = new Path(DIR+methodName+"/logs");
-    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    ColumnFamilyDefinition hcd = new ColumnFamilyDefinition(family);
     HBaseConfiguration conf = new HBaseConfiguration();
     FileSystem fs = FileSystem.get(conf);
     Path reconstructionLog = null; 
@@ -80,7 +80,7 @@ public class TestStore extends TestCase {
 
     fs.delete(logdir, true);
 
-    HTableDescriptor htd = new HTableDescriptor(table);
+    TableDefinition htd = new TableDefinition(table);
     htd.addFamily(hcd);
     HRegionInfo info = new HRegionInfo(htd, null, null, false);
     HLog hlog = new HLog(fs, logdir, conf, null);
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestStoreReconstruction.java b/src/test/org/apache/hadoop/hbase/regionserver/TestStoreReconstruction.java
index 7803f1e..1051d8e 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestStoreReconstruction.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestStoreReconstruction.java
@@ -91,8 +91,8 @@ public class TestStoreReconstruction {
   public void runReconstructionLog() throws Exception {
 
     byte[] family = Bytes.toBytes("column");
-    HColumnDescriptor hcd = new HColumnDescriptor(family);
-    HTableDescriptor htd = new HTableDescriptor(TABLE);
+    ColumnFamilyDefinition hcd = new ColumnFamilyDefinition(family);
+    TableDefinition htd = new TableDefinition(TABLE);
     htd.addFamily(hcd);
     HRegionInfo info = new HRegionInfo(htd, null, null, false);
     HLog log = new HLog(cluster.getFileSystem(), this.dir,conf, null);
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/TestWideScanner.java b/src/test/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
index d2fdded..0e5a657 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/TestWideScanner.java
@@ -7,10 +7,10 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Scan;
@@ -26,10 +26,10 @@ public class TestWideScanner extends HBaseTestCase {
   private MiniDFSCluster cluster = null;
   private HRegion r;
 
-  static final HTableDescriptor TESTTABLEDESC =
-    new HTableDescriptor("testwidescan");
+  static final TableDefinition TESTTABLEDESC =
+    new TableDefinition("testwidescan");
   static {
-    TESTTABLEDESC.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY,
+    TESTTABLEDESC.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY,
       10,  // Ten is arbitrary number.  Keep versions to help debuggging.
       Compression.Algorithm.NONE.getName(), false, true, 8 * 1024,
       HConstants.FOREVER, false));
diff --git a/src/test/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java b/src/test/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
index 880391a..54f73eb 100644
--- a/src/test/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
+++ b/src/test/org/apache/hadoop/hbase/regionserver/wal/TestLogRolling.java
@@ -25,9 +25,9 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.regionserver.HRegionServer;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -103,8 +103,8 @@ public class TestLogRolling extends HBaseClusterTestCase {
     this.log = server.getLog();
     
     // Create the test table and open it
-    HTableDescriptor desc = new HTableDescriptor(tableName);
-    desc.addFamily(new HColumnDescriptor(HConstants.CATALOG_FAMILY));
+    TableDefinition desc = new TableDefinition(tableName);
+    desc.addFamily(new ColumnFamilyDefinition(HConstants.CATALOG_FAMILY));
     HBaseAdmin admin = new HBaseAdmin(conf);
     admin.createTable(desc);
     HTable table = new HTable(conf, tableName);
diff --git a/src/test/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java b/src/test/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java
index 7bf8efb..d6aaff5 100644
--- a/src/test/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java
+++ b/src/test/org/apache/hadoop/hbase/util/DisabledTestMetaUtils.java
@@ -20,8 +20,8 @@
 package org.apache.hadoop.hbase.util;
 
 import org.apache.hadoop.hbase.HBaseClusterTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
@@ -36,8 +36,8 @@ public class DisabledTestMetaUtils extends HBaseClusterTestCase {
     final String oldColumn = "oldcolumn:";
     // Add three tables
     for (int i = 0; i < 5; i++) {
-      HTableDescriptor htd = new HTableDescriptor(getName() + i);
-      htd.addFamily(new HColumnDescriptor(oldColumn));
+      TableDefinition htd = new TableDefinition(getName() + i);
+      htd.addFamily(new ColumnFamilyDefinition(oldColumn));
       admin.createTable(htd);
     }
     this.cluster.shutdown();
@@ -46,7 +46,7 @@ public class DisabledTestMetaUtils extends HBaseClusterTestCase {
     // Add a new column to the third table, getName() + '2', and remove the old.
     final byte [] editTable = Bytes.toBytes(getName() + 2);
     final byte [] newColumn = Bytes.toBytes("newcolumn:");
-    utils.addColumn(editTable, new HColumnDescriptor(newColumn));
+    utils.addColumn(editTable, new ColumnFamilyDefinition(newColumn));
     utils.deleteColumn(editTable, Bytes.toBytes(oldColumn));
     utils.shutdown();
     // Delete again so we go get it all fresh.
@@ -55,8 +55,8 @@ public class DisabledTestMetaUtils extends HBaseClusterTestCase {
     this.cluster = new MiniHBaseCluster(this.conf, 1);
     // Now assert columns were added and deleted.
     HTable t = new HTable(conf, editTable);
-    HTableDescriptor htd = t.getTableDescriptor();
-    HColumnDescriptor hcd = htd.getFamily(newColumn);
+    TableDefinition htd = t.getTableDescriptor();
+    ColumnFamilyDefinition hcd = htd.getFamily(newColumn);
     assertTrue(hcd != null);
     assertNull(htd.getFamily(Bytes.toBytes(oldColumn)));
   }
diff --git a/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java b/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java
index f08c116..50a6102 100644
--- a/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java
+++ b/src/test/org/apache/hadoop/hbase/util/TestMergeTool.java
@@ -28,10 +28,10 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseTestCase;
-import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.ColumnFamilyDefinition;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDefinition;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Put;
@@ -52,7 +52,7 @@ public class TestMergeTool extends HBaseTestCase {
   
   private final HRegionInfo[] sourceRegions = new HRegionInfo[5];
   private final HRegion[] regions = new HRegion[5];
-  private HTableDescriptor desc;
+  private TableDefinition desc;
   private byte [][][] rows;
   private MiniDFSCluster dfsCluster = null;
   
@@ -61,8 +61,8 @@ public class TestMergeTool extends HBaseTestCase {
     this.conf.set("hbase.hstore.compactionThreshold", "2");
 
     // Create table description
-    this.desc = new HTableDescriptor("TestMergeTool");
-    this.desc.addFamily(new HColumnDescriptor(FAMILY));
+    this.desc = new TableDefinition("TestMergeTool");
+    this.desc.addFamily(new ColumnFamilyDefinition(FAMILY));
 
     /*
      * Create the HRegionInfos for the regions.
diff --git a/src/test/org/apache/hadoop/hbase/zookeeper/TestZooKeeperWrapper.java b/src/test/org/apache/hadoop/hbase/zookeeper/TestZooKeeperWrapper.java
new file mode 100644
index 0000000..4a9003c
--- /dev/null
+++ b/src/test/org/apache/hadoop/hbase/zookeeper/TestZooKeeperWrapper.java
@@ -0,0 +1,286 @@
+/**
+ * Copyright 2010 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.zookeeper;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.MiniZooKeeperCluster;
+import org.apache.hadoop.hbase.TableDefinition;
+import org.apache.hadoop.hbase.TableState;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.JSONing;
+import org.apache.zookeeper.AsyncCallback;
+import org.apache.zookeeper.KeeperException;
+import org.apache.zookeeper.WatchedEvent;
+import org.apache.zookeeper.Watcher;
+import org.apache.zookeeper.AsyncCallback.StatCallback;
+import org.apache.zookeeper.KeeperException.Code;
+import org.apache.zookeeper.KeeperException.SessionExpiredException;
+import org.apache.zookeeper.Watcher.Event.EventType;
+import org.apache.zookeeper.data.Stat;
+import org.junit.Assert;
+import org.junit.Test;
+
+
+/**
+ * Test methods and classes of {@link ZooKeeperWrapper}
+ */
+public class TestZooKeeperWrapper
+implements Watcher, StatCallback, AsyncCallback.ChildrenCallback {
+  private final Log LOG = LogFactory.getLog(this.getClass().getName());
+  // Used by a few tests.  Keep the reference out here so can get to it from 
+  // inside a Watcher or a StatCompletion.
+  private ZooKeeperWrapper wrapper = null;
+
+  /* Map of table name to current state and schema. Used to figure changes
+   * when zk indicates change.
+   */
+  private Map<String, byte[]> tableStateAndSchema =
+    new ConcurrentHashMap<String, byte []>();
+
+  @Test public void testTableDataStructure() throws IOException {
+    TableState ts = new TableState();
+    ts.setReadonly(true);
+    TableDefinition tdef = new TableDefinition("test");
+    ZooKeeperWrapper.TableData td = new ZooKeeperWrapper.TableData(ts, tdef);
+    String s = td.toString();
+    ZooKeeperWrapper.TableData desertd = (ZooKeeperWrapper.TableData)JSONing.
+      deserialize(ZooKeeperWrapper.TableData.class, s);
+    LOG.info(desertd.toString());
+    desertd.getState().equals(ts);
+    desertd.getDefinition().equals(tdef);
+  }
+
+  @Test public void testHasParent() throws IOException {
+    Assert.assertTrue(ZooKeeperWrapper.hasParent("/hbase/tables",
+      "/hbase/tables/xyz"));
+    Assert.assertFalse(ZooKeeperWrapper.hasParent("/hbase/tables",
+      "/hbase/tables/"));
+    Assert.assertFalse(ZooKeeperWrapper.hasParent("/hbase/tables",
+      "/hbase/tables"));
+    Assert.assertFalse(ZooKeeperWrapper.hasParent("/hbase/tables",
+      "/hbase/tables/xyz/abc"));
+  }
+
+  /**
+   * Start up a zk cluster and write definitions and state there.
+   * Add some watchers.
+   * @throws IOException
+   * @throws InterruptedException
+   */
+  @Test public void testTableDataInZookeeper()
+  throws IOException, InterruptedException {
+    Configuration conf = HBaseConfiguration.create();
+    // Get dir for use by cluster.
+    File dir = HBaseTestingUtility.getClusterDir();
+    dir.mkdirs();  // Make all intermediate directories if necessary.
+    MiniZooKeeperCluster cluster =
+      HBaseTestingUtility.startMiniZooKeeperCluster(conf, dir);
+    try {
+      // Construction of wrapper connects to zk ensemble.
+      this.wrapper = new ZooKeeperWrapper(conf, this);
+      Thread t = new Thread("client actor thread") {
+        @Override
+        public void run() {
+          if (!wrapper.ensureTablesExists()) throw new RuntimeException();
+          // Now set a watch on the 'tables' directory.
+          wrapper.setWatcherOnTables(TestZooKeeperWrapper.this,
+            TestZooKeeperWrapper.this, null);
+          TableState ts = new TableState();
+          TableDefinition td = new TableDefinition("test");
+          // A table znode holds state and definition.  Add one.
+          wrapper.updateTable(ts, td);
+          // Update 'test' table.  First make no change.
+          td = new TableDefinition("test");
+          wrapper.updateTable(ts, td);
+          //  Now make a change to the table.  Is change noticed?
+          td.setMaxFileSize(21);
+          wrapper.updateTable(ts, td);
+          // Are we flagged that the thing was rewritten but didn't change?
+          wrapper.updateTable(ts, td);
+          // Add a new table.
+          td = new TableDefinition("test2");
+          wrapper.updateTable(ts, td);
+        }
+      };
+      t.start();
+      t.join();
+      Thread.sleep(1 * 1000);
+      // Make some dumb assertions about what just happened.
+      // Assert I have two datastructures, one for 'test' and one for 'test2'.
+      Assert.assertTrue(this.tableStateAndSchema.size() == 2);
+      for (Map.Entry<String, byte []> e: this.tableStateAndSchema.entrySet()) {
+        this.wrapper.delete(e.getKey());
+      }
+      Thread.sleep(1 * 1000);
+      Assert.assertTrue(this.tableStateAndSchema.isEmpty());
+    } finally {
+      if (wrapper != null) wrapper.close();
+      HBaseTestingUtility.shutdownMiniZooKeeperCluster(cluster);
+    }
+  }
+
+  @Override
+  public void process(WatchedEvent event) {
+    String path = event.getPath();
+    LOG.info("Process Watcher " + event);
+    if (event.getType() == Event.EventType.None) {
+      // We are are being told that the state of the connection has changed
+      switch (event.getState()) {
+        case SyncConnected:
+          // In this particular example we don't need to do anything
+          // here - watches are automatically re-registered with 
+          // server and any watches triggered while the client was 
+          // disconnected will be delivered (in order of course)
+          break;
+        case Expired:
+          // It's all over
+          // TODO: Set a dead flag.
+          break;
+      }
+      return;
+    }
+    if (event.getType() == Event.EventType.NodeChildrenChanged) {
+      if (this.wrapper.isTablesZNode(path)) {
+        // Reset the watcher on tables.  We get triggered here when node 
+        // added under us or if deleted.
+        this.wrapper.setWatcherOnTables(this, this, null);
+      } else {
+      // Reset watcher on parent tables directory.
+      // this.wrapper.setWatcher(path, this, this);
+      // LOG.info("NodeChildrenChanged=" + path);
+      // this.wrapper.getChildren(path, this, this, null);
+        LOG.info("DO WHAT?");
+      }
+    } else if (event.getType() == Event.EventType.NodeCreated) {
+    } else if (event.getType() == EventType.NodeDataChanged) {
+      try {
+        byte [] data = this.wrapper.getZooKeeper().getData(path, true, null);
+        byte [] previousData = tableStateAndSchema.get(path);
+        if (previousData == null) this.tableStateAndSchema.put(path, data);
+        if (Bytes.equals(data, previousData)) LOG.info("No change " + path);
+        else LOG.info("Changed " + path);
+      } catch (SessionExpiredException e) {
+        LOG.info("Session expired " + e.toString());
+      } catch (KeeperException e) {
+        // TODO Auto-generated catch block
+        e.printStackTrace();
+      } catch (InterruptedException e) {
+        // TODO Auto-generated catch block
+        e.printStackTrace();
+      }
+    } else if (event.getType() == EventType.NodeDeleted) {
+      // Don't set a new watcher.
+      this.tableStateAndSchema.remove(path);
+    } else {
+      if (path != null) {
+        // Something has changed on the node, let's find out
+        this.wrapper.getZooKeeper().exists(path, true, this, null);
+        LOG.info("Process Setting watcher inside watcher on " + path);
+      }
+    }
+  }
+
+  @Override
+  public void processResult(int rc, String znode, Object ctx, Stat stat) {
+    LOG.info("Completion path=" + znode + ", context=" + ctx + ", stat=" +
+      stat.toString().trim() + " " + Code.get(rc));
+    boolean dead = false;
+    boolean exists;
+    switch (Code.get(rc)) {
+    case OK:
+        exists = true;
+        break;
+    case NONODE:
+        exists = false;
+        break;
+    case SESSIONEXPIRED:
+    case NOAUTH:
+        dead = true;
+        return;
+    default:
+        // Retry errors
+        this.wrapper.getZooKeeper().exists(znode, true, this, null);
+        return;
+    }
+ 
+    byte b[] = null;
+    if (exists && this.wrapper.isTableZNode(znode)) {
+        try {
+          b = this.wrapper.getZooKeeper().getData(znode, false, null);
+          LOG.info("Completion Stat getData=" + Bytes.toString(b) + ", " +
+              "WRAPPER=" + this.wrapper + ", " +
+              this.wrapper.getZooKeeper() + ", znode=" + znode);
+          this.tableStateAndSchema.put(znode, b);
+        } catch (SessionExpiredException e) {
+          LOG.info("Session expired " + e);
+        } catch (KeeperException e) {
+            // We don't need to worry about recovering now. The watch
+            // callbacks will kick off any exception handling
+            e.printStackTrace();
+        } catch (InterruptedException e) {
+            return;
+        }
+    }
+    LOG.info("Completion Change done on " + znode);
+  }
+
+  // Called on completion setting children watchers.
+  @Override
+  public void processResult(int rc, String path, Object ctx,
+      List<String> children) {
+    LOG.info("ChildChange under " + (path == null? "null": path) + " " +
+      Code.get(rc) + " children=" + (children == null? "null": children.size()));
+    // Unused.
+    boolean dead = false;
+    switch (Code.get(rc)) {
+    case OK:
+      if (children != null && children.size() > 0) {
+        for(String child: children) {
+          LOG.info("ChildChange: Setting watcher on: " + child);
+          // I could be setting new watchers on stuff that I already have
+          // a watch on, given the way I do the below.  Then again, I might
+          // not see creation of all children so this is 'safe'.
+          this.wrapper.setWatcher(path, child, this, this);
+        }
+      }
+      break;
+    case NONODE:
+        break;
+    case SESSIONEXPIRED:
+    case NOAUTH:
+         dead = true;
+        return;
+    default:
+        // Retry errors?
+        // In tutorial resets watcher by doing an exists.
+        return;
+    }
+  }
+}
\ No newline at end of file
