### Eclipse Workspace Patch 1.0
#P hbase-trunk
Index: src/java/org/apache/hadoop/hbase/thrift/ThriftServer2.java
===================================================================
--- src/java/org/apache/hadoop/hbase/thrift/ThriftServer2.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/thrift/ThriftServer2.java	(revision 0)
@@ -0,0 +1,296 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.thrift;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Get;
+import org.apache.hadoop.hbase.client.HTableInterface;
+import org.apache.hadoop.hbase.client.HTablePool;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.thrift.generated2.HBase;
+import org.apache.hadoop.hbase.thrift.generated2.IOError;
+import org.apache.hadoop.hbase.thrift.generated2.TColumn;
+import org.apache.hadoop.hbase.thrift.generated2.TDelete;
+import org.apache.hadoop.hbase.thrift.generated2.TGet;
+import org.apache.hadoop.hbase.thrift.generated2.TPut;
+import org.apache.hadoop.hbase.thrift.generated2.TResult;
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocolFactory;
+import org.apache.thrift.server.TServer;
+import org.apache.thrift.server.TThreadPoolServer;
+import org.apache.thrift.transport.TServerSocket;
+import org.apache.thrift.transport.TServerTransport;
+
+/**
+ * ThriftServer - this class starts up a Thrift server which implements the Hbase API specified in the Hbase.thrift IDL
+ * file.
+ */
+public class ThriftServer2 {
+
+  /**
+   * The HBaseHandler is a glue object that connects Thrift RPC calls to the HBase client API primarily defined in the
+   * HBaseAdmin and HTable objects.
+   */
+  public static class HBaseHandler implements HBase.Iface {
+    protected HTablePool htablePool = new HTablePool();
+    protected final Log LOG = LogFactory.getLog(this.getClass().getName());
+
+    protected Delete tDeleteToDelete(TDelete tdelete) {
+      if (!tdelete.isSetColumns())
+        return null;
+
+      Delete delete = new Delete(tdelete.row);
+      for (TColumn column : tdelete.columns) {
+        if (column.isSetTimestamp()) {
+          if (column.isSetQualifier()) {
+            delete.deleteColumns(column.family, column.qualifier, column.timestamp);
+          } else {
+            delete.deleteFamily(column.family, column.timestamp);
+          }
+        } else {
+          if (column.isSetQualifier()) {
+            delete.deleteColumns(column.family, column.qualifier);
+          } else {
+            delete.deleteFamily(column.family);
+          }
+        }
+      }
+      return delete;
+    }
+
+    protected Get tGetToGet(TGet tget) {
+      if (!tget.isSetColumns())
+        return null;
+
+      Get get = new Get(tget.row);
+      if (tget.isSetTimestamp()) {
+        get.setTimeStamp(tget.timestamp);
+      }
+      for (TColumn column : tget.columns) {
+        if (column.isSetQualifier()) {
+          get.addColumn(column.family, column.qualifier);
+        } else {
+          get.addFamily(column.family);
+        }
+      }
+      return get;
+    }
+
+    protected TResult resultToTResult(Result result) {
+      TResult tresult = new TResult();
+      tresult.setRow(result.getRow());
+      Map<TColumn, byte[]> columns = new HashMap<TColumn, byte[]>();
+      for (KeyValue keyValue : result.list()) {
+        TColumn tcolumn = new TColumn(keyValue.getFamily(), keyValue.getQualifier(), keyValue.getTimestamp());
+        columns.put(tcolumn, keyValue.getValue());
+      }
+      return tresult;
+    }
+
+    protected Put tPutToPut(TPut tput) {
+      if (!tput.isSetValues())
+        return null;
+
+      Put put = new Put(tput.row);
+      for (Entry<TColumn, byte[]> entry : tput.values.entrySet()) {
+        TColumn column = entry.getKey();
+        if (column.isSetTimestamp()) {
+          put.add(column.family, column.qualifier, column.timestamp, entry.getValue());
+        } else {
+          put.add(column.family, column.qualifier, entry.getValue());
+        }
+      }
+      return put;
+    }
+
+    @Override
+    public void delet(byte[] table, TDelete tdelete) throws IOError, TException {
+      HTableInterface htable = htablePool.getTable(table);
+      try {
+        Delete delete = tDeleteToDelete(tdelete);
+        if (tdelete != null) {
+          htable.delete(delete);
+        }
+      } catch (IOException e) {
+        throw new IOError(e.getMessage());
+      } finally {
+        htablePool.putTable(htable);
+      }
+    }
+
+    @Override
+    public void deleteN(byte[] table, List<TDelete> tdeletes) throws IOError, TException {
+      for (TDelete tdelete : tdeletes) {
+        delet(table, tdelete);
+      }
+    }
+
+    @Override
+    public TResult get(byte[] table, TGet tget) throws IOError, TException {
+      HTableInterface htable = htablePool.getTable(table);
+      TResult tresult = null;
+      try {
+        Get get = tGetToGet(tget);
+        if (get != null) {
+          Result result = htable.get(get);
+          tresult = resultToTResult(result);
+        }
+      } catch (IOException e) {
+        throw new IOError(e.getMessage());
+      } finally {
+        htablePool.putTable(htable);
+      }
+      return tresult;
+    }
+
+    @Override
+    public List<TResult> getN(byte[] table, List<TGet> tgets) throws IOError, TException {
+      List<TResult> tresults = new ArrayList<TResult>();
+
+      for (TGet tget : tgets) {
+        tresults.add(get(table, tget));
+      }
+      return tresults;
+    }
+
+    @Override
+    public void put(byte[] table, TPut tput) throws IOError, TException {
+      HTableInterface htable = htablePool.getTable(table);
+      try {
+        Put put = tPutToPut(tput);
+        if (put != null) {
+          htable.put(tPutToPut(tput));
+        }
+      } catch (IOException e) {
+        throw new IOError(e.getMessage());
+      } finally {
+        htablePool.putTable(htable);
+      }
+    }
+
+    @Override
+    public void putN(byte[] table, List<TPut> tputs) throws IOError, TException {
+      HTableInterface htable = htablePool.getTable(table);
+      try {
+        List<Put> puts = new ArrayList<Put>();
+        for (TPut tput : tputs) {
+          Put put = tPutToPut(tput);
+          if (put != null) {
+            puts.add(put);
+          }
+        }
+        htable.put(puts);
+      } catch (IOException e) {
+        throw new IOError(e.getMessage());
+      } finally {
+        htablePool.putTable(htable);
+      }
+    }
+  }
+
+  //
+  // Main program and support routines
+  //
+
+  private static void printUsageAndExit() {
+    printUsageAndExit(null);
+  }
+
+  private static void printUsageAndExit(final String message) {
+    if (message != null) {
+      System.err.println(message);
+    }
+    System.out.println("Usage: java org.apache.hadoop.hbase.thrift.ThriftServer " + "--help | [--port=PORT] start");
+    System.out.println("Arguments:");
+    System.out.println(" start Start thrift server");
+    System.out.println(" stop  Stop thrift server");
+    System.out.println("Options:");
+    System.out.println(" port  Port to listen on. Default: 9090");
+    // System.out.println(" bind  Address to bind on. Default: 0.0.0.0.");
+    System.out.println(" help  Print this message and exit");
+    System.exit(0);
+  }
+
+  /*
+   * Start up the Thrift server.
+   * @param args
+   */
+  protected static void doMain(final String[] args) throws Exception {
+    if (args.length < 1) {
+      printUsageAndExit();
+    }
+
+    int port = 9090;
+    // String bindAddress = "0.0.0.0";
+
+    // Process command-line args. TODO: Better cmd-line processing
+    // (but hopefully something not as painful as cli options).
+    // final String addressArgKey = "--bind=";
+    final String portArgKey = "--port=";
+    for (String cmd : args) {
+      // if (cmd.startsWith(addressArgKey)) {
+      // bindAddress = cmd.substring(addressArgKey.length());
+      // continue;
+      // } else
+      if (cmd.startsWith(portArgKey)) {
+        port = Integer.parseInt(cmd.substring(portArgKey.length()));
+        continue;
+      } else if (cmd.equals("--help") || cmd.equals("-h")) {
+        printUsageAndExit();
+      } else if (cmd.equals("start")) {
+        continue;
+      } else if (cmd.equals("stop")) {
+        printUsageAndExit("To shutdown the thrift server run "
+            + "bin/hbase-daemon.sh stop thrift or send a kill signal to " + "the thrift server pid");
+      }
+
+      // Print out usage if we get to here.
+      printUsageAndExit();
+    }
+    Log LOG = LogFactory.getLog("ThriftServer");
+    LOG.info("starting HBase Thrift server on port " + Integer.toString(port));
+    HBaseHandler handler = new HBaseHandler();
+    HBase.Processor processor = new HBase.Processor(handler);
+    TServerTransport serverTransport = new TServerSocket(port);
+    TProtocolFactory protFactory = new TBinaryProtocol.Factory(true, true);
+    TServer server = new TThreadPoolServer(processor, serverTransport, protFactory);
+    server.serve();
+  }
+
+  /**
+   * @param args
+   * @throws Exception
+   */
+  public static void main(String[] args) throws Exception {
+    doMain(args);
+  }
+
+}
Index: src/java/org/apache/hadoop/hbase/thrift/HBase2.thrift
===================================================================
--- src/java/org/apache/hadoop/hbase/thrift/HBase2.thrift	(revision 0)
+++ src/java/org/apache/hadoop/hbase/thrift/HBase2.thrift	(revision 0)
@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// ----------------------------------------------------------------
+// HBase.thrift -
+//
+// This is a Thrift interface definition file for the Hbase service.
+// Target language libraries for C++, Java, Ruby, PHP, (and more) are
+// generated by running this file through the Thrift compiler with the
+// appropriate flags.  The Thrift compiler binary and runtime
+// libraries for various languages is currently available from
+// Facebook (http://developers.facebook.com/thrift/).  The intent is
+// for the Thrift project to migrate to Apache Incubator.
+//
+// See the package.html file for information on the version of Thrift
+// used to generate the *.java files checked into the Hbase project.
+// ----------------------------------------------------------------
+
+namespace java org.apache.hadoop.hbase.thrift.generated2
+namespace cpp  apache.hadoop.hbase.thrift
+namespace rb Apache.Hadoop.Hbase.Thrift
+namespace py hbase
+namespace perl Hbase
+
+// note: other language namespaces tbd...
+
+//
+// Types
+//
+
+typedef binary Bytes
+
+
+struct TColumn {
+  1:Bytes family,
+  2:Bytes qualifier,
+  3:i64 timestamp
+}
+
+struct TPut {
+  1:Bytes row,
+  2:map<TColumn, Bytes> values
+}
+
+struct TDelete {
+  1:Bytes row,
+  2:list<TColumn> columns
+}
+
+/**
+ * TColumn timestamps are ignored for gets.
+ */  
+struct TGet {
+  1:Bytes row,
+  2:list<TColumn> columns,
+  
+  // Set timestamp to get a specific timetamp (this overrides using a range), 
+  3:i64 timestamp,
+  
+  // Set timerangeStart and timerangeEnd to get a range of timestamps. 
+  4:i64 timerangeStart,
+  5:i64 timerangeEnd,
+}
+
+struct TResult {
+  1:Bytes row,
+  2:map<TColumn, Bytes> values
+}
+
+//
+// Exceptions
+//
+/**
+ * An IOError exception signals that an error occurred communicating
+ * to the Hbase master or an Hbase region server.  Also used to return
+ * more general Hbase error conditions.
+ */
+exception IOError {
+  1:string message
+}
+
+//
+// Service 
+//
+
+service HBase {
+
+  TResult get(1:Bytes table, 2:TGet get) throws (1:IOError io)
+  list<TResult> getN(1:Bytes table, 2:list<TGet> gets) throws (1:IOError io)
+  
+  void put(1:Bytes table, 2:TPut put) throws (1:IOError io)
+  void putN(1:Bytes table, 2:list<TPut> puts) throws (1:IOError io)
+
+  void delet(1:Bytes table, 2:TDelete delet) throws (1:IOError io)
+  void deleteN(1:Bytes table, 2:list<TDelete> deletes) throws (1:IOError io)
+  
+}

