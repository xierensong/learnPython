diff --git hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
index 2bcdd58..a0c7dc0 100644
--- hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
+++ hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
@@ -200,7 +200,7 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
     }
     // We just need the rowkey. Get it from 1st KV.
     byte[] row = deleteRow.get(0).getRow();
-    Delete delete = new Delete(row, ts, null);
+    Delete delete = new Delete(row, ts);
     if (deleteType == DeleteType.FAMILY) {
       Set<byte[]> families = new TreeSet<byte[]>(Bytes.BYTES_COMPARATOR);
       for (KeyValue kv : deleteRow) {
diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
index 87569e2..d0bc5c0 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
@@ -528,33 +528,29 @@ public final class ClientProtos {
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameBytesPairOrBuilder getAttributeOrBuilder(
         int index);
     
-    // optional uint64 lockId = 4;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional .Filter filter = 5;
+    // optional .Filter filter = 4;
     boolean hasFilter();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.FilterOrBuilder getFilterOrBuilder();
     
-    // optional .TimeRange timeRange = 6;
+    // optional .TimeRange timeRange = 5;
     boolean hasTimeRange();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder getTimeRangeOrBuilder();
     
-    // optional uint32 maxVersions = 7 [default = 1];
+    // optional uint32 maxVersions = 6 [default = 1];
     boolean hasMaxVersions();
     int getMaxVersions();
     
-    // optional bool cacheBlocks = 8 [default = true];
+    // optional bool cacheBlocks = 7 [default = true];
     boolean hasCacheBlocks();
     boolean getCacheBlocks();
     
-    // optional uint32 storeLimit = 9;
+    // optional uint32 storeLimit = 8;
     boolean hasStoreLimit();
     int getStoreLimit();
     
-    // optional uint32 storeOffset = 10;
+    // optional uint32 storeOffset = 9;
     boolean hasStoreOffset();
     int getStoreOffset();
   }
@@ -639,21 +635,11 @@ public final class ClientProtos {
       return attribute_.get(index);
     }
     
-    // optional uint64 lockId = 4;
-    public static final int LOCKID_FIELD_NUMBER = 4;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional .Filter filter = 5;
-    public static final int FILTER_FIELD_NUMBER = 5;
+    // optional .Filter filter = 4;
+    public static final int FILTER_FIELD_NUMBER = 4;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter filter_;
     public boolean hasFilter() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000002) == 0x00000002);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter() {
       return filter_;
@@ -662,11 +648,11 @@ public final class ClientProtos {
       return filter_;
     }
     
-    // optional .TimeRange timeRange = 6;
-    public static final int TIMERANGE_FIELD_NUMBER = 6;
+    // optional .TimeRange timeRange = 5;
+    public static final int TIMERANGE_FIELD_NUMBER = 5;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_;
     public boolean hasTimeRange() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000004) == 0x00000004);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
       return timeRange_;
@@ -675,41 +661,41 @@ public final class ClientProtos {
       return timeRange_;
     }
     
-    // optional uint32 maxVersions = 7 [default = 1];
-    public static final int MAXVERSIONS_FIELD_NUMBER = 7;
+    // optional uint32 maxVersions = 6 [default = 1];
+    public static final int MAXVERSIONS_FIELD_NUMBER = 6;
     private int maxVersions_;
     public boolean hasMaxVersions() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000008) == 0x00000008);
     }
     public int getMaxVersions() {
       return maxVersions_;
     }
     
-    // optional bool cacheBlocks = 8 [default = true];
-    public static final int CACHEBLOCKS_FIELD_NUMBER = 8;
+    // optional bool cacheBlocks = 7 [default = true];
+    public static final int CACHEBLOCKS_FIELD_NUMBER = 7;
     private boolean cacheBlocks_;
     public boolean hasCacheBlocks() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000010) == 0x00000010);
     }
     public boolean getCacheBlocks() {
       return cacheBlocks_;
     }
     
-    // optional uint32 storeLimit = 9;
-    public static final int STORELIMIT_FIELD_NUMBER = 9;
+    // optional uint32 storeLimit = 8;
+    public static final int STORELIMIT_FIELD_NUMBER = 8;
     private int storeLimit_;
     public boolean hasStoreLimit() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000020) == 0x00000020);
     }
     public int getStoreLimit() {
       return storeLimit_;
     }
     
-    // optional uint32 storeOffset = 10;
-    public static final int STOREOFFSET_FIELD_NUMBER = 10;
+    // optional uint32 storeOffset = 9;
+    public static final int STOREOFFSET_FIELD_NUMBER = 9;
     private int storeOffset_;
     public boolean hasStoreOffset() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000040) == 0x00000040);
     }
     public int getStoreOffset() {
       return storeOffset_;
@@ -719,7 +705,6 @@ public final class ClientProtos {
       row_ = com.google.protobuf.ByteString.EMPTY;
       column_ = java.util.Collections.emptyList();
       attribute_ = java.util.Collections.emptyList();
-      lockId_ = 0L;
       filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
       timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
       maxVersions_ = 1;
@@ -771,25 +756,22 @@ public final class ClientProtos {
         output.writeMessage(3, attribute_.get(i));
       }
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(4, lockId_);
+        output.writeMessage(4, filter_);
       }
       if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeMessage(5, filter_);
+        output.writeMessage(5, timeRange_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeMessage(6, timeRange_);
+        output.writeUInt32(6, maxVersions_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeUInt32(7, maxVersions_);
+        output.writeBool(7, cacheBlocks_);
       }
       if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBool(8, cacheBlocks_);
+        output.writeUInt32(8, storeLimit_);
       }
       if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        output.writeUInt32(9, storeLimit_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        output.writeUInt32(10, storeOffset_);
+        output.writeUInt32(9, storeOffset_);
       }
       getUnknownFields().writeTo(output);
     }
@@ -814,31 +796,27 @@ public final class ClientProtos {
       }
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(4, lockId_);
+          .computeMessageSize(4, filter_);
       }
       if (((bitField0_ & 0x00000004) == 0x00000004)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(5, filter_);
+          .computeMessageSize(5, timeRange_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(6, timeRange_);
+          .computeUInt32Size(6, maxVersions_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(7, maxVersions_);
+          .computeBoolSize(7, cacheBlocks_);
       }
       if (((bitField0_ & 0x00000020) == 0x00000020)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeBoolSize(8, cacheBlocks_);
+          .computeUInt32Size(8, storeLimit_);
       }
       if (((bitField0_ & 0x00000040) == 0x00000040)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(9, storeLimit_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(10, storeOffset_);
+          .computeUInt32Size(9, storeOffset_);
       }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
@@ -872,11 +850,6 @@ public final class ClientProtos {
           .equals(other.getColumnList());
       result = result && getAttributeList()
           .equals(other.getAttributeList());
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
       result = result && (hasFilter() == other.hasFilter());
       if (hasFilter()) {
         result = result && getFilter()
@@ -928,10 +901,6 @@ public final class ClientProtos {
         hash = (37 * hash) + ATTRIBUTE_FIELD_NUMBER;
         hash = (53 * hash) + getAttributeList().hashCode();
       }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
       if (hasFilter()) {
         hash = (37 * hash) + FILTER_FIELD_NUMBER;
         hash = (53 * hash) + getFilter().hashCode();
@@ -1090,28 +1059,26 @@ public final class ClientProtos {
         } else {
           attributeBuilder_.clear();
         }
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
         if (filterBuilder_ == null) {
           filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
         } else {
           filterBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000010);
+        bitField0_ = (bitField0_ & ~0x00000008);
         if (timeRangeBuilder_ == null) {
           timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000020);
+        bitField0_ = (bitField0_ & ~0x00000010);
         maxVersions_ = 1;
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         cacheBlocks_ = true;
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         storeLimit_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000100);
+        bitField0_ = (bitField0_ & ~0x00000080);
         storeOffset_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000200);
+        bitField0_ = (bitField0_ & ~0x00000100);
         return this;
       }
       
@@ -1175,38 +1142,34 @@ public final class ClientProtos {
         if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
           to_bitField0_ |= 0x00000002;
         }
-        result.lockId_ = lockId_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000004;
-        }
         if (filterBuilder_ == null) {
           result.filter_ = filter_;
         } else {
           result.filter_ = filterBuilder_.build();
         }
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000008;
+        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+          to_bitField0_ |= 0x00000004;
         }
         if (timeRangeBuilder_ == null) {
           result.timeRange_ = timeRange_;
         } else {
           result.timeRange_ = timeRangeBuilder_.build();
         }
+        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+          to_bitField0_ |= 0x00000008;
+        }
+        result.maxVersions_ = maxVersions_;
         if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
           to_bitField0_ |= 0x00000010;
         }
-        result.maxVersions_ = maxVersions_;
+        result.cacheBlocks_ = cacheBlocks_;
         if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
           to_bitField0_ |= 0x00000020;
         }
-        result.cacheBlocks_ = cacheBlocks_;
+        result.storeLimit_ = storeLimit_;
         if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
           to_bitField0_ |= 0x00000040;
         }
-        result.storeLimit_ = storeLimit_;
-        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
-          to_bitField0_ |= 0x00000080;
-        }
         result.storeOffset_ = storeOffset_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
@@ -1279,9 +1242,6 @@ public final class ClientProtos {
             }
           }
         }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
         if (other.hasFilter()) {
           mergeFilter(other.getFilter());
         }
@@ -1370,12 +1330,7 @@ public final class ClientProtos {
               addAttribute(subBuilder.buildPartial());
               break;
             }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 42: {
+            case 34: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.newBuilder();
               if (hasFilter()) {
                 subBuilder.mergeFrom(getFilter());
@@ -1384,7 +1339,7 @@ public final class ClientProtos {
               setFilter(subBuilder.buildPartial());
               break;
             }
-            case 50: {
+            case 42: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder();
               if (hasTimeRange()) {
                 subBuilder.mergeFrom(getTimeRange());
@@ -1393,23 +1348,23 @@ public final class ClientProtos {
               setTimeRange(subBuilder.buildPartial());
               break;
             }
+            case 48: {
+              bitField0_ |= 0x00000020;
+              maxVersions_ = input.readUInt32();
+              break;
+            }
             case 56: {
               bitField0_ |= 0x00000040;
-              maxVersions_ = input.readUInt32();
+              cacheBlocks_ = input.readBool();
               break;
             }
             case 64: {
               bitField0_ |= 0x00000080;
-              cacheBlocks_ = input.readBool();
+              storeLimit_ = input.readUInt32();
               break;
             }
             case 72: {
               bitField0_ |= 0x00000100;
-              storeLimit_ = input.readUInt32();
-              break;
-            }
-            case 80: {
-              bitField0_ |= 0x00000200;
               storeOffset_ = input.readUInt32();
               break;
             }
@@ -1815,33 +1770,12 @@ public final class ClientProtos {
         return attributeBuilder_;
       }
       
-      // optional uint64 lockId = 4;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000008;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional .Filter filter = 5;
+      // optional .Filter filter = 4;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.FilterOrBuilder> filterBuilder_;
       public boolean hasFilter() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000008) == 0x00000008);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter() {
         if (filterBuilder_ == null) {
@@ -1860,7 +1794,7 @@ public final class ClientProtos {
         } else {
           filterBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder setFilter(
@@ -1871,12 +1805,12 @@ public final class ClientProtos {
         } else {
           filterBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder mergeFilter(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter value) {
         if (filterBuilder_ == null) {
-          if (((bitField0_ & 0x00000010) == 0x00000010) &&
+          if (((bitField0_ & 0x00000008) == 0x00000008) &&
               filter_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance()) {
             filter_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.newBuilder(filter_).mergeFrom(value).buildPartial();
@@ -1887,7 +1821,7 @@ public final class ClientProtos {
         } else {
           filterBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder clearFilter() {
@@ -1897,11 +1831,11 @@ public final class ClientProtos {
         } else {
           filterBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000010);
+        bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder getFilterBuilder() {
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         onChanged();
         return getFilterFieldBuilder().getBuilder();
       }
@@ -1926,12 +1860,12 @@ public final class ClientProtos {
         return filterBuilder_;
       }
       
-      // optional .TimeRange timeRange = 6;
+      // optional .TimeRange timeRange = 5;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder> timeRangeBuilder_;
       public boolean hasTimeRange() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000010) == 0x00000010);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
         if (timeRangeBuilder_ == null) {
@@ -1950,7 +1884,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder setTimeRange(
@@ -1961,12 +1895,12 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder mergeTimeRange(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange value) {
         if (timeRangeBuilder_ == null) {
-          if (((bitField0_ & 0x00000020) == 0x00000020) &&
+          if (((bitField0_ & 0x00000010) == 0x00000010) &&
               timeRange_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance()) {
             timeRange_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder(timeRange_).mergeFrom(value).buildPartial();
@@ -1977,7 +1911,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder clearTimeRange() {
@@ -1987,11 +1921,11 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000020);
+        bitField0_ = (bitField0_ & ~0x00000010);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder getTimeRangeBuilder() {
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         onChanged();
         return getTimeRangeFieldBuilder().getBuilder();
       }
@@ -2016,85 +1950,85 @@ public final class ClientProtos {
         return timeRangeBuilder_;
       }
       
-      // optional uint32 maxVersions = 7 [default = 1];
+      // optional uint32 maxVersions = 6 [default = 1];
       private int maxVersions_ = 1;
       public boolean hasMaxVersions() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000020) == 0x00000020);
       }
       public int getMaxVersions() {
         return maxVersions_;
       }
       public Builder setMaxVersions(int value) {
-        bitField0_ |= 0x00000040;
+        bitField0_ |= 0x00000020;
         maxVersions_ = value;
         onChanged();
         return this;
       }
       public Builder clearMaxVersions() {
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         maxVersions_ = 1;
         onChanged();
         return this;
       }
       
-      // optional bool cacheBlocks = 8 [default = true];
+      // optional bool cacheBlocks = 7 [default = true];
       private boolean cacheBlocks_ = true;
       public boolean hasCacheBlocks() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000040) == 0x00000040);
       }
       public boolean getCacheBlocks() {
         return cacheBlocks_;
       }
       public Builder setCacheBlocks(boolean value) {
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         cacheBlocks_ = value;
         onChanged();
         return this;
       }
       public Builder clearCacheBlocks() {
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         cacheBlocks_ = true;
         onChanged();
         return this;
       }
       
-      // optional uint32 storeLimit = 9;
+      // optional uint32 storeLimit = 8;
       private int storeLimit_ ;
       public boolean hasStoreLimit() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
+        return ((bitField0_ & 0x00000080) == 0x00000080);
       }
       public int getStoreLimit() {
         return storeLimit_;
       }
       public Builder setStoreLimit(int value) {
-        bitField0_ |= 0x00000100;
+        bitField0_ |= 0x00000080;
         storeLimit_ = value;
         onChanged();
         return this;
       }
       public Builder clearStoreLimit() {
-        bitField0_ = (bitField0_ & ~0x00000100);
+        bitField0_ = (bitField0_ & ~0x00000080);
         storeLimit_ = 0;
         onChanged();
         return this;
       }
       
-      // optional uint32 storeOffset = 10;
+      // optional uint32 storeOffset = 9;
       private int storeOffset_ ;
       public boolean hasStoreOffset() {
-        return ((bitField0_ & 0x00000200) == 0x00000200);
+        return ((bitField0_ & 0x00000100) == 0x00000100);
       }
       public int getStoreOffset() {
         return storeOffset_;
       }
       public Builder setStoreOffset(int value) {
-        bitField0_ |= 0x00000200;
+        bitField0_ |= 0x00000100;
         storeOffset_ = value;
         onChanged();
         return this;
       }
       public Builder clearStoreOffset() {
-        bitField0_ = (bitField0_ & ~0x00000200);
+        bitField0_ = (bitField0_ & ~0x00000100);
         storeOffset_ = 0;
         onChanged();
         return this;
@@ -4834,11 +4768,7 @@ public final class ClientProtos {
     boolean hasTimestamp();
     long getTimestamp();
     
-    // optional uint64 lockId = 6;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional bool writeToWAL = 7 [default = true];
+    // optional bool writeToWAL = 6 [default = true];
     boolean hasWriteToWAL();
     boolean getWriteToWAL();
     
@@ -6340,21 +6270,11 @@ public final class ClientProtos {
       return timestamp_;
     }
     
-    // optional uint64 lockId = 6;
-    public static final int LOCKID_FIELD_NUMBER = 6;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional bool writeToWAL = 7 [default = true];
-    public static final int WRITETOWAL_FIELD_NUMBER = 7;
+    // optional bool writeToWAL = 6 [default = true];
+    public static final int WRITETOWAL_FIELD_NUMBER = 6;
     private boolean writeToWAL_;
     public boolean hasWriteToWAL() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000008) == 0x00000008);
     }
     public boolean getWriteToWAL() {
       return writeToWAL_;
@@ -6364,7 +6284,7 @@ public final class ClientProtos {
     public static final int TIMERANGE_FIELD_NUMBER = 10;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_;
     public boolean hasTimeRange() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000010) == 0x00000010);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
       return timeRange_;
@@ -6379,7 +6299,6 @@ public final class ClientProtos {
       columnValue_ = java.util.Collections.emptyList();
       attribute_ = java.util.Collections.emptyList();
       timestamp_ = 0L;
-      lockId_ = 0L;
       writeToWAL_ = true;
       timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
     }
@@ -6431,12 +6350,9 @@ public final class ClientProtos {
         output.writeUInt64(5, timestamp_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt64(6, lockId_);
+        output.writeBool(6, writeToWAL_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeBool(7, writeToWAL_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
         output.writeMessage(10, timeRange_);
       }
       getUnknownFields().writeTo(output);
@@ -6470,14 +6386,10 @@ public final class ClientProtos {
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(6, lockId_);
+          .computeBoolSize(6, writeToWAL_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeBoolSize(7, writeToWAL_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(10, timeRange_);
       }
       size += getUnknownFields().getSerializedSize();
@@ -6522,11 +6434,6 @@ public final class ClientProtos {
         result = result && (getTimestamp()
             == other.getTimestamp());
       }
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
       result = result && (hasWriteToWAL() == other.hasWriteToWAL());
       if (hasWriteToWAL()) {
         result = result && (getWriteToWAL()
@@ -6566,10 +6473,6 @@ public final class ClientProtos {
         hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
         hash = (53 * hash) + hashLong(getTimestamp());
       }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
       if (hasWriteToWAL()) {
         hash = (37 * hash) + WRITETOWAL_FIELD_NUMBER;
         hash = (53 * hash) + hashBoolean(getWriteToWAL());
@@ -6715,16 +6618,14 @@ public final class ClientProtos {
         }
         timestamp_ = 0L;
         bitField0_ = (bitField0_ & ~0x00000010);
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000020);
         writeToWAL_ = true;
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         if (timeRangeBuilder_ == null) {
           timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
       
@@ -6796,14 +6697,10 @@ public final class ClientProtos {
         if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
           to_bitField0_ |= 0x00000008;
         }
-        result.lockId_ = lockId_;
+        result.writeToWAL_ = writeToWAL_;
         if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
           to_bitField0_ |= 0x00000010;
         }
-        result.writeToWAL_ = writeToWAL_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000020;
-        }
         if (timeRangeBuilder_ == null) {
           result.timeRange_ = timeRange_;
         } else {
@@ -6886,9 +6783,6 @@ public final class ClientProtos {
         if (other.hasTimestamp()) {
           setTimestamp(other.getTimestamp());
         }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
         if (other.hasWriteToWAL()) {
           setWriteToWAL(other.getWriteToWAL());
         }
@@ -6981,11 +6875,6 @@ public final class ClientProtos {
             }
             case 48: {
               bitField0_ |= 0x00000020;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000040;
               writeToWAL_ = input.readBool();
               break;
             }
@@ -7445,43 +7334,22 @@ public final class ClientProtos {
         return this;
       }
       
-      // optional uint64 lockId = 6;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000020;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000020);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional bool writeToWAL = 7 [default = true];
+      // optional bool writeToWAL = 6 [default = true];
       private boolean writeToWAL_ = true;
       public boolean hasWriteToWAL() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000020) == 0x00000020);
       }
       public boolean getWriteToWAL() {
         return writeToWAL_;
       }
       public Builder setWriteToWAL(boolean value) {
-        bitField0_ |= 0x00000040;
+        bitField0_ |= 0x00000020;
         writeToWAL_ = value;
         onChanged();
         return this;
       }
       public Builder clearWriteToWAL() {
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         writeToWAL_ = true;
         onChanged();
         return this;
@@ -7492,7 +7360,7 @@ public final class ClientProtos {
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder> timeRangeBuilder_;
       public boolean hasTimeRange() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000040) == 0x00000040);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
         if (timeRangeBuilder_ == null) {
@@ -7511,7 +7379,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder setTimeRange(
@@ -7522,12 +7390,12 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder mergeTimeRange(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange value) {
         if (timeRangeBuilder_ == null) {
-          if (((bitField0_ & 0x00000080) == 0x00000080) &&
+          if (((bitField0_ & 0x00000040) == 0x00000040) &&
               timeRange_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance()) {
             timeRange_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder(timeRange_).mergeFrom(value).buildPartial();
@@ -7538,7 +7406,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder clearTimeRange() {
@@ -7548,11 +7416,11 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder getTimeRangeBuilder() {
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         onChanged();
         return getTimeRangeFieldBuilder().getBuilder();
       }
@@ -12356,1882 +12224,6 @@ public final class ClientProtos {
     // @@protoc_insertion_point(class_scope:ScanResponse)
   }
   
-  public interface LockRowRequestOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required .RegionSpecifier region = 1;
-    boolean hasRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
-    
-    // repeated bytes row = 2;
-    java.util.List<com.google.protobuf.ByteString> getRowList();
-    int getRowCount();
-    com.google.protobuf.ByteString getRow(int index);
-  }
-  public static final class LockRowRequest extends
-      com.google.protobuf.GeneratedMessage
-      implements LockRowRequestOrBuilder {
-    // Use LockRowRequest.newBuilder() to construct.
-    private LockRowRequest(Builder builder) {
-      super(builder);
-    }
-    private LockRowRequest(boolean noInit) {}
-    
-    private static final LockRowRequest defaultInstance;
-    public static LockRowRequest getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public LockRowRequest getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required .RegionSpecifier region = 1;
-    public static final int REGION_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_;
-    public boolean hasRegion() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-      return region_;
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-      return region_;
-    }
-    
-    // repeated bytes row = 2;
-    public static final int ROW_FIELD_NUMBER = 2;
-    private java.util.List<com.google.protobuf.ByteString> row_;
-    public java.util.List<com.google.protobuf.ByteString>
-        getRowList() {
-      return row_;
-    }
-    public int getRowCount() {
-      return row_.size();
-    }
-    public com.google.protobuf.ByteString getRow(int index) {
-      return row_.get(index);
-    }
-    
-    private void initFields() {
-      region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      row_ = java.util.Collections.emptyList();;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasRegion()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!getRegion().isInitialized()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, region_);
-      }
-      for (int i = 0; i < row_.size(); i++) {
-        output.writeBytes(2, row_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, region_);
-      }
-      {
-        int dataSize = 0;
-        for (int i = 0; i < row_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(row_.get(i));
-        }
-        size += dataSize;
-        size += 1 * getRowList().size();
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest) obj;
-      
-      boolean result = true;
-      result = result && (hasRegion() == other.hasRegion());
-      if (hasRegion()) {
-        result = result && getRegion()
-            .equals(other.getRegion());
-      }
-      result = result && getRowList()
-          .equals(other.getRowList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasRegion()) {
-        hash = (37 * hash) + REGION_FIELD_NUMBER;
-        hash = (53 * hash) + getRegion().hashCode();
-      }
-      if (getRowCount() > 0) {
-        hash = (37 * hash) + ROW_FIELD_NUMBER;
-        hash = (53 * hash) + getRowList().hashCode();
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequestOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getRegionFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        row_ = java.util.Collections.emptyList();;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (regionBuilder_ == null) {
-          result.region_ = region_;
-        } else {
-          result.region_ = regionBuilder_.build();
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          row_ = java.util.Collections.unmodifiableList(row_);
-          bitField0_ = (bitField0_ & ~0x00000002);
-        }
-        result.row_ = row_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance()) return this;
-        if (other.hasRegion()) {
-          mergeRegion(other.getRegion());
-        }
-        if (!other.row_.isEmpty()) {
-          if (row_.isEmpty()) {
-            row_ = other.row_;
-            bitField0_ = (bitField0_ & ~0x00000002);
-          } else {
-            ensureRowIsMutable();
-            row_.addAll(other.row_);
-          }
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasRegion()) {
-          
-          return false;
-        }
-        if (!getRegion().isInitialized()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder();
-              if (hasRegion()) {
-                subBuilder.mergeFrom(getRegion());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setRegion(subBuilder.buildPartial());
-              break;
-            }
-            case 18: {
-              ensureRowIsMutable();
-              row_.add(input.readBytes());
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required .RegionSpecifier region = 1;
-      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> regionBuilder_;
-      public boolean hasRegion() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-        if (regionBuilder_ == null) {
-          return region_;
-        } else {
-          return regionBuilder_.getMessage();
-        }
-      }
-      public Builder setRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          region_ = value;
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder setRegion(
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
-        if (regionBuilder_ == null) {
-          region_ = builderForValue.build();
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder mergeRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              region_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
-            region_ =
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(region_).mergeFrom(value).buildPartial();
-          } else {
-            region_ = value;
-          }
-          onChanged();
-        } else {
-          regionBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder clearRegion() {
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-          onChanged();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getRegionBuilder() {
-        bitField0_ |= 0x00000001;
-        onChanged();
-        return getRegionFieldBuilder().getBuilder();
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-        if (regionBuilder_ != null) {
-          return regionBuilder_.getMessageOrBuilder();
-        } else {
-          return region_;
-        }
-      }
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
-          getRegionFieldBuilder() {
-        if (regionBuilder_ == null) {
-          regionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
-                  region_,
-                  getParentForChildren(),
-                  isClean());
-          region_ = null;
-        }
-        return regionBuilder_;
-      }
-      
-      // repeated bytes row = 2;
-      private java.util.List<com.google.protobuf.ByteString> row_ = java.util.Collections.emptyList();;
-      private void ensureRowIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          row_ = new java.util.ArrayList<com.google.protobuf.ByteString>(row_);
-          bitField0_ |= 0x00000002;
-         }
-      }
-      public java.util.List<com.google.protobuf.ByteString>
-          getRowList() {
-        return java.util.Collections.unmodifiableList(row_);
-      }
-      public int getRowCount() {
-        return row_.size();
-      }
-      public com.google.protobuf.ByteString getRow(int index) {
-        return row_.get(index);
-      }
-      public Builder setRow(
-          int index, com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureRowIsMutable();
-        row_.set(index, value);
-        onChanged();
-        return this;
-      }
-      public Builder addRow(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureRowIsMutable();
-        row_.add(value);
-        onChanged();
-        return this;
-      }
-      public Builder addAllRow(
-          java.lang.Iterable<? extends com.google.protobuf.ByteString> values) {
-        ensureRowIsMutable();
-        super.addAll(values, row_);
-        onChanged();
-        return this;
-      }
-      public Builder clearRow() {
-        row_ = java.util.Collections.emptyList();;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:LockRowRequest)
-    }
-    
-    static {
-      defaultInstance = new LockRowRequest(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:LockRowRequest)
-  }
-  
-  public interface LockRowResponseOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required uint64 lockId = 1;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional uint32 ttl = 2;
-    boolean hasTtl();
-    int getTtl();
-  }
-  public static final class LockRowResponse extends
-      com.google.protobuf.GeneratedMessage
-      implements LockRowResponseOrBuilder {
-    // Use LockRowResponse.newBuilder() to construct.
-    private LockRowResponse(Builder builder) {
-      super(builder);
-    }
-    private LockRowResponse(boolean noInit) {}
-    
-    private static final LockRowResponse defaultInstance;
-    public static LockRowResponse getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public LockRowResponse getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required uint64 lockId = 1;
-    public static final int LOCKID_FIELD_NUMBER = 1;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional uint32 ttl = 2;
-    public static final int TTL_FIELD_NUMBER = 2;
-    private int ttl_;
-    public boolean hasTtl() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public int getTtl() {
-      return ttl_;
-    }
-    
-    private void initFields() {
-      lockId_ = 0L;
-      ttl_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasLockId()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, lockId_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(2, ttl_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, lockId_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(2, ttl_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) obj;
-      
-      boolean result = true;
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
-      result = result && (hasTtl() == other.hasTtl());
-      if (hasTtl()) {
-        result = result && (getTtl()
-            == other.getTtl());
-      }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
-      if (hasTtl()) {
-        hash = (37 * hash) + TTL_FIELD_NUMBER;
-        hash = (53 * hash) + getTtl();
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponseOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        ttl_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.lockId_ = lockId_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.ttl_ = ttl_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance()) return this;
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
-        if (other.hasTtl()) {
-          setTtl(other.getTtl());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasLockId()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              ttl_ = input.readUInt32();
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required uint64 lockId = 1;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000001;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional uint32 ttl = 2;
-      private int ttl_ ;
-      public boolean hasTtl() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public int getTtl() {
-        return ttl_;
-      }
-      public Builder setTtl(int value) {
-        bitField0_ |= 0x00000002;
-        ttl_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearTtl() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        ttl_ = 0;
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:LockRowResponse)
-    }
-    
-    static {
-      defaultInstance = new LockRowResponse(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:LockRowResponse)
-  }
-  
-  public interface UnlockRowRequestOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required .RegionSpecifier region = 1;
-    boolean hasRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
-    
-    // required uint64 lockId = 2;
-    boolean hasLockId();
-    long getLockId();
-  }
-  public static final class UnlockRowRequest extends
-      com.google.protobuf.GeneratedMessage
-      implements UnlockRowRequestOrBuilder {
-    // Use UnlockRowRequest.newBuilder() to construct.
-    private UnlockRowRequest(Builder builder) {
-      super(builder);
-    }
-    private UnlockRowRequest(boolean noInit) {}
-    
-    private static final UnlockRowRequest defaultInstance;
-    public static UnlockRowRequest getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public UnlockRowRequest getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required .RegionSpecifier region = 1;
-    public static final int REGION_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_;
-    public boolean hasRegion() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-      return region_;
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-      return region_;
-    }
-    
-    // required uint64 lockId = 2;
-    public static final int LOCKID_FIELD_NUMBER = 2;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    private void initFields() {
-      region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      lockId_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasRegion()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!hasLockId()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!getRegion().isInitialized()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, region_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(2, lockId_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, region_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(2, lockId_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest) obj;
-      
-      boolean result = true;
-      result = result && (hasRegion() == other.hasRegion());
-      if (hasRegion()) {
-        result = result && getRegion()
-            .equals(other.getRegion());
-      }
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasRegion()) {
-        hash = (37 * hash) + REGION_FIELD_NUMBER;
-        hash = (53 * hash) + getRegion().hashCode();
-      }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequestOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getRegionFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (regionBuilder_ == null) {
-          result.region_ = region_;
-        } else {
-          result.region_ = regionBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.lockId_ = lockId_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance()) return this;
-        if (other.hasRegion()) {
-          mergeRegion(other.getRegion());
-        }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasRegion()) {
-          
-          return false;
-        }
-        if (!hasLockId()) {
-          
-          return false;
-        }
-        if (!getRegion().isInitialized()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder();
-              if (hasRegion()) {
-                subBuilder.mergeFrom(getRegion());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setRegion(subBuilder.buildPartial());
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              lockId_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required .RegionSpecifier region = 1;
-      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> regionBuilder_;
-      public boolean hasRegion() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-        if (regionBuilder_ == null) {
-          return region_;
-        } else {
-          return regionBuilder_.getMessage();
-        }
-      }
-      public Builder setRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          region_ = value;
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder setRegion(
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
-        if (regionBuilder_ == null) {
-          region_ = builderForValue.build();
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder mergeRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              region_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
-            region_ =
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(region_).mergeFrom(value).buildPartial();
-          } else {
-            region_ = value;
-          }
-          onChanged();
-        } else {
-          regionBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder clearRegion() {
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-          onChanged();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getRegionBuilder() {
-        bitField0_ |= 0x00000001;
-        onChanged();
-        return getRegionFieldBuilder().getBuilder();
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-        if (regionBuilder_ != null) {
-          return regionBuilder_.getMessageOrBuilder();
-        } else {
-          return region_;
-        }
-      }
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
-          getRegionFieldBuilder() {
-        if (regionBuilder_ == null) {
-          regionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
-                  region_,
-                  getParentForChildren(),
-                  isClean());
-          region_ = null;
-        }
-        return regionBuilder_;
-      }
-      
-      // required uint64 lockId = 2;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000002;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:UnlockRowRequest)
-    }
-    
-    static {
-      defaultInstance = new UnlockRowRequest(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:UnlockRowRequest)
-  }
-  
-  public interface UnlockRowResponseOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-  }
-  public static final class UnlockRowResponse extends
-      com.google.protobuf.GeneratedMessage
-      implements UnlockRowResponseOrBuilder {
-    // Use UnlockRowResponse.newBuilder() to construct.
-    private UnlockRowResponse(Builder builder) {
-      super(builder);
-    }
-    private UnlockRowResponse(boolean noInit) {}
-    
-    private static final UnlockRowResponse defaultInstance;
-    public static UnlockRowResponse getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public UnlockRowResponse getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_fieldAccessorTable;
-    }
-    
-    private void initFields() {
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) obj;
-      
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponseOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse(this);
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance()) return this;
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-          }
-        }
-      }
-      
-      
-      // @@protoc_insertion_point(builder_scope:UnlockRowResponse)
-    }
-    
-    static {
-      defaultInstance = new UnlockRowResponse(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:UnlockRowResponse)
-  }
-  
   public interface BulkLoadHFileRequestOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
     
@@ -23070,16 +21062,6 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse> done);
       
-      public abstract void lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done);
-      
-      public abstract void unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done);
-      
       public abstract void bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -23130,22 +21112,6 @@ public final class ClientProtos {
         }
         
         @java.lang.Override
-        public  void lockRow(
-            com.google.protobuf.RpcController controller,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done) {
-          impl.lockRow(controller, request, done);
-        }
-        
-        @java.lang.Override
-        public  void unlockRow(
-            com.google.protobuf.RpcController controller,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done) {
-          impl.unlockRow(controller, request, done);
-        }
-        
-        @java.lang.Override
         public  void bulkLoadHFile(
             com.google.protobuf.RpcController controller,
             org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -23206,16 +21172,12 @@ public final class ClientProtos {
             case 2:
               return impl.scan(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest)request);
             case 3:
-              return impl.lockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)request);
-            case 4:
-              return impl.unlockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)request);
-            case 5:
               return impl.bulkLoadHFile(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest)request);
-            case 6:
+            case 4:
               return impl.execCoprocessor(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest)request);
-            case 7:
+            case 5:
               return impl.execService(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest)request);
-            case 8:
+            case 6:
               return impl.multi(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest)request);
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23238,16 +21200,12 @@ public final class ClientProtos {
             case 2:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest.getDefaultInstance();
             case 3:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-            case 4:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.getDefaultInstance();
-            case 6:
+            case 4:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.getDefaultInstance();
-            case 7:
+            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.getDefaultInstance();
-            case 8:
+            case 6:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.getDefaultInstance();
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23270,16 +21228,12 @@ public final class ClientProtos {
             case 2:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance();
             case 3:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-            case 4:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance();
-            case 6:
+            case 4:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance();
-            case 7:
+            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance();
-            case 8:
+            case 6:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance();
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23304,16 +21258,6 @@ public final class ClientProtos {
         org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request,
         com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse> done);
     
-    public abstract void lockRow(
-        com.google.protobuf.RpcController controller,
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done);
-    
-    public abstract void unlockRow(
-        com.google.protobuf.RpcController controller,
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done);
-    
     public abstract void bulkLoadHFile(
         com.google.protobuf.RpcController controller,
         org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -23372,31 +21316,21 @@ public final class ClientProtos {
               done));
           return;
         case 3:
-          this.lockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)request,
-            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse>specializeCallback(
-              done));
-          return;
-        case 4:
-          this.unlockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)request,
-            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse>specializeCallback(
-              done));
-          return;
-        case 5:
           this.bulkLoadHFile(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse>specializeCallback(
               done));
           return;
-        case 6:
+        case 4:
           this.execCoprocessor(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse>specializeCallback(
               done));
           return;
-        case 7:
+        case 5:
           this.execService(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse>specializeCallback(
               done));
           return;
-        case 8:
+        case 6:
           this.multi(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse>specializeCallback(
               done));
@@ -23422,16 +21356,12 @@ public final class ClientProtos {
         case 2:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest.getDefaultInstance();
         case 3:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-        case 4:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.getDefaultInstance();
-        case 6:
+        case 4:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.getDefaultInstance();
-        case 7:
+        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.getDefaultInstance();
-        case 8:
+        case 6:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.getDefaultInstance();
         default:
           throw new java.lang.AssertionError("Can't get here.");
@@ -23454,16 +21384,12 @@ public final class ClientProtos {
         case 2:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance();
         case 3:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-        case 4:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance();
-        case 6:
+        case 4:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance();
-        case 7:
+        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance();
-        case 8:
+        case 6:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance();
         default:
           throw new java.lang.AssertionError("Can't get here.");
@@ -23531,42 +21457,12 @@ public final class ClientProtos {
             org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance()));
       }
       
-      public  void lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done) {
-        channel.callMethod(
-          getDescriptor().getMethods().get(3),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance(),
-          com.google.protobuf.RpcUtil.generalizeCallback(
-            done,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.class,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance()));
-      }
-      
-      public  void unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done) {
-        channel.callMethod(
-          getDescriptor().getMethods().get(4),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance(),
-          com.google.protobuf.RpcUtil.generalizeCallback(
-            done,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.class,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance()));
-      }
-      
       public  void bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(3),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance(),
@@ -23581,7 +21477,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(4),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance(),
@@ -23596,7 +21492,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(5),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance(),
@@ -23611,7 +21507,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(8),
+          getDescriptor().getMethods().get(6),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance(),
@@ -23643,16 +21539,6 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request)
           throws com.google.protobuf.ServiceException;
       
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request)
-          throws com.google.protobuf.ServiceException;
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request)
-          throws com.google.protobuf.ServiceException;
-      
       public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request)
@@ -23717,36 +21603,12 @@ public final class ClientProtos {
       }
       
       
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request)
-          throws com.google.protobuf.ServiceException {
-        return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(3),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance());
-      }
-      
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request)
-          throws com.google.protobuf.ServiceException {
-        return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(4),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance());
-      }
-      
-      
       public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(3),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance());
@@ -23758,7 +21620,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(4),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance());
@@ -23770,7 +21632,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(5),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance());
@@ -23782,7 +21644,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(8),
+          getDescriptor().getMethods().get(6),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance());
@@ -23862,26 +21724,6 @@ public final class ClientProtos {
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_ScanResponse_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_LockRowRequest_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_LockRowRequest_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_LockRowResponse_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_LockRowResponse_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_UnlockRowRequest_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_UnlockRowRequest_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_UnlockRowResponse_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_UnlockRowResponse_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_BulkLoadHFileRequest_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
@@ -23957,100 +21799,91 @@ public final class ClientProtos {
     java.lang.String[] descriptorData = {
       "\n\014Client.proto\032\013hbase.proto\032\020Comparator." +
       "proto\"+\n\006Column\022\016\n\006family\030\001 \002(\014\022\021\n\tquali" +
-      "fier\030\002 \003(\014\"\362\001\n\003Get\022\013\n\003row\030\001 \002(\014\022\027\n\006colum" +
+      "fier\030\002 \003(\014\"\342\001\n\003Get\022\013\n\003row\030\001 \002(\014\022\027\n\006colum" +
       "n\030\002 \003(\0132\007.Column\022!\n\tattribute\030\003 \003(\0132\016.Na" +
-      "meBytesPair\022\016\n\006lockId\030\004 \001(\004\022\027\n\006filter\030\005 " +
-      "\001(\0132\007.Filter\022\035\n\ttimeRange\030\006 \001(\0132\n.TimeRa" +
-      "nge\022\026\n\013maxVersions\030\007 \001(\r:\0011\022\031\n\013cacheBloc" +
-      "ks\030\010 \001(\010:\004true\022\022\n\nstoreLimit\030\t \001(\r\022\023\n\013st" +
-      "oreOffset\030\n \001(\r\"%\n\006Result\022\033\n\010keyValue\030\001 " +
-      "\003(\0132\t.KeyValue\"r\n\nGetRequest\022 \n\006region\030\001",
-      " \002(\0132\020.RegionSpecifier\022\021\n\003get\030\002 \002(\0132\004.Ge" +
-      "t\022\030\n\020closestRowBefore\030\003 \001(\010\022\025\n\rexistence" +
-      "Only\030\004 \001(\010\"6\n\013GetResponse\022\027\n\006result\030\001 \001(" +
-      "\0132\007.Result\022\016\n\006exists\030\002 \001(\010\"\177\n\tCondition\022" +
-      "\013\n\003row\030\001 \002(\014\022\016\n\006family\030\002 \002(\014\022\021\n\tqualifie" +
-      "r\030\003 \002(\014\022!\n\013compareType\030\004 \002(\0162\014.CompareTy" +
-      "pe\022\037\n\ncomparator\030\005 \002(\0132\013.Comparator\"\306\004\n\006" +
-      "Mutate\022\013\n\003row\030\001 \002(\014\022&\n\nmutateType\030\002 \002(\0162" +
-      "\022.Mutate.MutateType\022(\n\013columnValue\030\003 \003(\013" +
-      "2\023.Mutate.ColumnValue\022!\n\tattribute\030\004 \003(\013",
-      "2\016.NameBytesPair\022\021\n\ttimestamp\030\005 \001(\004\022\016\n\006l" +
-      "ockId\030\006 \001(\004\022\030\n\nwriteToWAL\030\007 \001(\010:\004true\022\035\n" +
-      "\ttimeRange\030\n \001(\0132\n.TimeRange\032\310\001\n\013ColumnV" +
-      "alue\022\016\n\006family\030\001 \002(\014\022:\n\016qualifierValue\030\002" +
-      " \003(\0132\".Mutate.ColumnValue.QualifierValue" +
-      "\032m\n\016QualifierValue\022\021\n\tqualifier\030\001 \001(\014\022\r\n" +
-      "\005value\030\002 \001(\014\022\021\n\ttimestamp\030\003 \001(\004\022&\n\ndelet" +
-      "eType\030\004 \001(\0162\022.Mutate.DeleteType\"<\n\nMutat" +
-      "eType\022\n\n\006APPEND\020\000\022\r\n\tINCREMENT\020\001\022\007\n\003PUT\020" +
-      "\002\022\n\n\006DELETE\020\003\"U\n\nDeleteType\022\026\n\022DELETE_ON",
-      "E_VERSION\020\000\022\034\n\030DELETE_MULTIPLE_VERSIONS\020" +
-      "\001\022\021\n\rDELETE_FAMILY\020\002\"i\n\rMutateRequest\022 \n" +
-      "\006region\030\001 \002(\0132\020.RegionSpecifier\022\027\n\006mutat" +
-      "e\030\002 \002(\0132\007.Mutate\022\035\n\tcondition\030\003 \001(\0132\n.Co" +
-      "ndition\"<\n\016MutateResponse\022\027\n\006result\030\001 \001(" +
-      "\0132\007.Result\022\021\n\tprocessed\030\002 \001(\010\"\243\002\n\004Scan\022\027" +
-      "\n\006column\030\001 \003(\0132\007.Column\022!\n\tattribute\030\002 \003" +
-      "(\0132\016.NameBytesPair\022\020\n\010startRow\030\003 \001(\014\022\017\n\007" +
-      "stopRow\030\004 \001(\014\022\027\n\006filter\030\005 \001(\0132\007.Filter\022\035" +
-      "\n\ttimeRange\030\006 \001(\0132\n.TimeRange\022\026\n\013maxVers",
-      "ions\030\007 \001(\r:\0011\022\031\n\013cacheBlocks\030\010 \001(\010:\004true" +
-      "\022\021\n\tbatchSize\030\t \001(\r\022\025\n\rmaxResultSize\030\n \001" +
-      "(\004\022\022\n\nstoreLimit\030\013 \001(\r\022\023\n\013storeOffset\030\014 " +
-      "\001(\r\"\230\001\n\013ScanRequest\022 \n\006region\030\001 \001(\0132\020.Re" +
-      "gionSpecifier\022\023\n\004scan\030\002 \001(\0132\005.Scan\022\021\n\tsc" +
-      "annerId\030\003 \001(\004\022\024\n\014numberOfRows\030\004 \001(\r\022\024\n\014c" +
-      "loseScanner\030\005 \001(\010\022\023\n\013nextCallSeq\030\006 \001(\004\"\\" +
-      "\n\014ScanResponse\022\027\n\006result\030\001 \003(\0132\007.Result\022" +
-      "\021\n\tscannerId\030\002 \001(\004\022\023\n\013moreResults\030\003 \001(\010\022" +
-      "\013\n\003ttl\030\004 \001(\r\"?\n\016LockRowRequest\022 \n\006region",
-      "\030\001 \002(\0132\020.RegionSpecifier\022\013\n\003row\030\002 \003(\014\".\n" +
-      "\017LockRowResponse\022\016\n\006lockId\030\001 \002(\004\022\013\n\003ttl\030" +
-      "\002 \001(\r\"D\n\020UnlockRowRequest\022 \n\006region\030\001 \002(" +
-      "\0132\020.RegionSpecifier\022\016\n\006lockId\030\002 \002(\004\"\023\n\021U" +
-      "nlockRowResponse\"\260\001\n\024BulkLoadHFileReques" +
-      "t\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\0224\n\nf" +
-      "amilyPath\030\002 \003(\0132 .BulkLoadHFileRequest.F" +
-      "amilyPath\022\024\n\014assignSeqNum\030\003 \001(\010\032*\n\nFamil" +
-      "yPath\022\016\n\006family\030\001 \002(\014\022\014\n\004path\030\002 \002(\t\"\'\n\025B" +
-      "ulkLoadHFileResponse\022\016\n\006loaded\030\001 \002(\010\"\203\001\n",
-      "\004Exec\022\013\n\003row\030\001 \002(\014\022\024\n\014protocolName\030\002 \002(\t" +
-      "\022\022\n\nmethodName\030\003 \002(\t\022!\n\010property\030\004 \003(\0132\017" +
-      ".NameStringPair\022!\n\tparameter\030\005 \003(\0132\016.Nam" +
-      "eBytesPair\"O\n\026ExecCoprocessorRequest\022 \n\006" +
-      "region\030\001 \002(\0132\020.RegionSpecifier\022\023\n\004call\030\002" +
-      " \002(\0132\005.Exec\"8\n\027ExecCoprocessorResponse\022\035" +
-      "\n\005value\030\001 \002(\0132\016.NameBytesPair\"_\n\026Coproce" +
-      "ssorServiceCall\022\013\n\003row\030\001 \002(\014\022\023\n\013serviceN" +
-      "ame\030\002 \002(\t\022\022\n\nmethodName\030\003 \002(\t\022\017\n\007request" +
-      "\030\004 \002(\014\"d\n\031CoprocessorServiceRequest\022 \n\006r",
-      "egion\030\001 \002(\0132\020.RegionSpecifier\022%\n\004call\030\002 " +
-      "\002(\0132\027.CoprocessorServiceCall\"]\n\032Coproces" +
-      "sorServiceResponse\022 \n\006region\030\001 \002(\0132\020.Reg" +
-      "ionSpecifier\022\035\n\005value\030\002 \002(\0132\016.NameBytesP" +
-      "air\"N\n\013MultiAction\022\027\n\006mutate\030\001 \001(\0132\007.Mut" +
-      "ate\022\021\n\003get\030\002 \001(\0132\004.Get\022\023\n\004exec\030\003 \001(\0132\005.E" +
-      "xec\"P\n\014ActionResult\022\035\n\005value\030\001 \001(\0132\016.Nam" +
-      "eBytesPair\022!\n\texception\030\002 \001(\0132\016.NameByte" +
-      "sPair\"^\n\014MultiRequest\022 \n\006region\030\001 \002(\0132\020." +
-      "RegionSpecifier\022\034\n\006action\030\002 \003(\0132\014.MultiA",
-      "ction\022\016\n\006atomic\030\003 \001(\010\".\n\rMultiResponse\022\035" +
-      "\n\006result\030\001 \003(\0132\r.ActionResult2\331\003\n\rClient" +
-      "Service\022 \n\003get\022\013.GetRequest\032\014.GetRespons" +
-      "e\022)\n\006mutate\022\016.MutateRequest\032\017.MutateResp" +
-      "onse\022#\n\004scan\022\014.ScanRequest\032\r.ScanRespons" +
-      "e\022,\n\007lockRow\022\017.LockRowRequest\032\020.LockRowR" +
-      "esponse\0222\n\tunlockRow\022\021.UnlockRowRequest\032" +
-      "\022.UnlockRowResponse\022>\n\rbulkLoadHFile\022\025.B" +
-      "ulkLoadHFileRequest\032\026.BulkLoadHFileRespo" +
-      "nse\022D\n\017execCoprocessor\022\027.ExecCoprocessor",
-      "Request\032\030.ExecCoprocessorResponse\022F\n\013exe" +
-      "cService\022\032.CoprocessorServiceRequest\032\033.C" +
-      "oprocessorServiceResponse\022&\n\005multi\022\r.Mul" +
-      "tiRequest\032\016.MultiResponseBB\n*org.apache." +
-      "hadoop.hbase.protobuf.generatedB\014ClientP" +
-      "rotosH\001\210\001\001\240\001\001"
+      "meBytesPair\022\027\n\006filter\030\004 \001(\0132\007.Filter\022\035\n\t" +
+      "timeRange\030\005 \001(\0132\n.TimeRange\022\026\n\013maxVersio" +
+      "ns\030\006 \001(\r:\0011\022\031\n\013cacheBlocks\030\007 \001(\010:\004true\022\022" +
+      "\n\nstoreLimit\030\010 \001(\r\022\023\n\013storeOffset\030\t \001(\r\"" +
+      "%\n\006Result\022\033\n\010keyValue\030\001 \003(\0132\t.KeyValue\"r" +
+      "\n\nGetRequest\022 \n\006region\030\001 \002(\0132\020.RegionSpe",
+      "cifier\022\021\n\003get\030\002 \002(\0132\004.Get\022\030\n\020closestRowB" +
+      "efore\030\003 \001(\010\022\025\n\rexistenceOnly\030\004 \001(\010\"6\n\013Ge" +
+      "tResponse\022\027\n\006result\030\001 \001(\0132\007.Result\022\016\n\006ex" +
+      "ists\030\002 \001(\010\"\177\n\tCondition\022\013\n\003row\030\001 \002(\014\022\016\n\006" +
+      "family\030\002 \002(\014\022\021\n\tqualifier\030\003 \002(\014\022!\n\013compa" +
+      "reType\030\004 \002(\0162\014.CompareType\022\037\n\ncomparator" +
+      "\030\005 \002(\0132\013.Comparator\"\266\004\n\006Mutate\022\013\n\003row\030\001 " +
+      "\002(\014\022&\n\nmutateType\030\002 \002(\0162\022.Mutate.MutateT" +
+      "ype\022(\n\013columnValue\030\003 \003(\0132\023.Mutate.Column" +
+      "Value\022!\n\tattribute\030\004 \003(\0132\016.NameBytesPair",
+      "\022\021\n\ttimestamp\030\005 \001(\004\022\030\n\nwriteToWAL\030\006 \001(\010:" +
+      "\004true\022\035\n\ttimeRange\030\n \001(\0132\n.TimeRange\032\310\001\n" +
+      "\013ColumnValue\022\016\n\006family\030\001 \002(\014\022:\n\016qualifie" +
+      "rValue\030\002 \003(\0132\".Mutate.ColumnValue.Qualif" +
+      "ierValue\032m\n\016QualifierValue\022\021\n\tqualifier\030" +
+      "\001 \001(\014\022\r\n\005value\030\002 \001(\014\022\021\n\ttimestamp\030\003 \001(\004\022" +
+      "&\n\ndeleteType\030\004 \001(\0162\022.Mutate.DeleteType\"" +
+      "<\n\nMutateType\022\n\n\006APPEND\020\000\022\r\n\tINCREMENT\020\001" +
+      "\022\007\n\003PUT\020\002\022\n\n\006DELETE\020\003\"U\n\nDeleteType\022\026\n\022D" +
+      "ELETE_ONE_VERSION\020\000\022\034\n\030DELETE_MULTIPLE_V",
+      "ERSIONS\020\001\022\021\n\rDELETE_FAMILY\020\002\"i\n\rMutateRe" +
+      "quest\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\022" +
+      "\027\n\006mutate\030\002 \002(\0132\007.Mutate\022\035\n\tcondition\030\003 " +
+      "\001(\0132\n.Condition\"<\n\016MutateResponse\022\027\n\006res" +
+      "ult\030\001 \001(\0132\007.Result\022\021\n\tprocessed\030\002 \001(\010\"\243\002" +
+      "\n\004Scan\022\027\n\006column\030\001 \003(\0132\007.Column\022!\n\tattri" +
+      "bute\030\002 \003(\0132\016.NameBytesPair\022\020\n\010startRow\030\003" +
+      " \001(\014\022\017\n\007stopRow\030\004 \001(\014\022\027\n\006filter\030\005 \001(\0132\007." +
+      "Filter\022\035\n\ttimeRange\030\006 \001(\0132\n.TimeRange\022\026\n" +
+      "\013maxVersions\030\007 \001(\r:\0011\022\031\n\013cacheBlocks\030\010 \001",
+      "(\010:\004true\022\021\n\tbatchSize\030\t \001(\r\022\025\n\rmaxResult" +
+      "Size\030\n \001(\004\022\022\n\nstoreLimit\030\013 \001(\r\022\023\n\013storeO" +
+      "ffset\030\014 \001(\r\"\230\001\n\013ScanRequest\022 \n\006region\030\001 " +
+      "\001(\0132\020.RegionSpecifier\022\023\n\004scan\030\002 \001(\0132\005.Sc" +
+      "an\022\021\n\tscannerId\030\003 \001(\004\022\024\n\014numberOfRows\030\004 " +
+      "\001(\r\022\024\n\014closeScanner\030\005 \001(\010\022\023\n\013nextCallSeq" +
+      "\030\006 \001(\004\"\\\n\014ScanResponse\022\027\n\006result\030\001 \003(\0132\007" +
+      ".Result\022\021\n\tscannerId\030\002 \001(\004\022\023\n\013moreResult" +
+      "s\030\003 \001(\010\022\013\n\003ttl\030\004 \001(\r\"\260\001\n\024BulkLoadHFileRe" +
+      "quest\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\022",
+      "4\n\nfamilyPath\030\002 \003(\0132 .BulkLoadHFileReque" +
+      "st.FamilyPath\022\024\n\014assignSeqNum\030\003 \001(\010\032*\n\nF" +
+      "amilyPath\022\016\n\006family\030\001 \002(\014\022\014\n\004path\030\002 \002(\t\"" +
+      "\'\n\025BulkLoadHFileResponse\022\016\n\006loaded\030\001 \002(\010" +
+      "\"\203\001\n\004Exec\022\013\n\003row\030\001 \002(\014\022\024\n\014protocolName\030\002" +
+      " \002(\t\022\022\n\nmethodName\030\003 \002(\t\022!\n\010property\030\004 \003" +
+      "(\0132\017.NameStringPair\022!\n\tparameter\030\005 \003(\0132\016" +
+      ".NameBytesPair\"O\n\026ExecCoprocessorRequest" +
+      "\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\023\n\004ca" +
+      "ll\030\002 \002(\0132\005.Exec\"8\n\027ExecCoprocessorRespon",
+      "se\022\035\n\005value\030\001 \002(\0132\016.NameBytesPair\"_\n\026Cop" +
+      "rocessorServiceCall\022\013\n\003row\030\001 \002(\014\022\023\n\013serv" +
+      "iceName\030\002 \002(\t\022\022\n\nmethodName\030\003 \002(\t\022\017\n\007req" +
+      "uest\030\004 \002(\014\"d\n\031CoprocessorServiceRequest\022" +
+      " \n\006region\030\001 \002(\0132\020.RegionSpecifier\022%\n\004cal" +
+      "l\030\002 \002(\0132\027.CoprocessorServiceCall\"]\n\032Copr" +
+      "ocessorServiceResponse\022 \n\006region\030\001 \002(\0132\020" +
+      ".RegionSpecifier\022\035\n\005value\030\002 \002(\0132\016.NameBy" +
+      "tesPair\"N\n\013MultiAction\022\027\n\006mutate\030\001 \001(\0132\007" +
+      ".Mutate\022\021\n\003get\030\002 \001(\0132\004.Get\022\023\n\004exec\030\003 \001(\013",
+      "2\005.Exec\"P\n\014ActionResult\022\035\n\005value\030\001 \001(\0132\016" +
+      ".NameBytesPair\022!\n\texception\030\002 \001(\0132\016.Name" +
+      "BytesPair\"^\n\014MultiRequest\022 \n\006region\030\001 \002(" +
+      "\0132\020.RegionSpecifier\022\034\n\006action\030\002 \003(\0132\014.Mu" +
+      "ltiAction\022\016\n\006atomic\030\003 \001(\010\".\n\rMultiRespon" +
+      "se\022\035\n\006result\030\001 \003(\0132\r.ActionResult2\367\002\n\rCl" +
+      "ientService\022 \n\003get\022\013.GetRequest\032\014.GetRes" +
+      "ponse\022)\n\006mutate\022\016.MutateRequest\032\017.Mutate" +
+      "Response\022#\n\004scan\022\014.ScanRequest\032\r.ScanRes" +
+      "ponse\022>\n\rbulkLoadHFile\022\025.BulkLoadHFileRe",
+      "quest\032\026.BulkLoadHFileResponse\022D\n\017execCop" +
+      "rocessor\022\027.ExecCoprocessorRequest\032\030.Exec" +
+      "CoprocessorResponse\022F\n\013execService\022\032.Cop" +
+      "rocessorServiceRequest\032\033.CoprocessorServ" +
+      "iceResponse\022&\n\005multi\022\r.MultiRequest\032\016.Mu" +
+      "ltiResponseBB\n*org.apache.hadoop.hbase.p" +
+      "rotobuf.generatedB\014ClientProtosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -24070,7 +21903,7 @@ public final class ClientProtos {
           internal_static_Get_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Get_descriptor,
-              new java.lang.String[] { "Row", "Column", "Attribute", "LockId", "Filter", "TimeRange", "MaxVersions", "CacheBlocks", "StoreLimit", "StoreOffset", },
+              new java.lang.String[] { "Row", "Column", "Attribute", "Filter", "TimeRange", "MaxVersions", "CacheBlocks", "StoreLimit", "StoreOffset", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Get.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Get.Builder.class);
           internal_static_Result_descriptor =
@@ -24110,7 +21943,7 @@ public final class ClientProtos {
           internal_static_Mutate_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Mutate_descriptor,
-              new java.lang.String[] { "Row", "MutateType", "ColumnValue", "Attribute", "Timestamp", "LockId", "WriteToWAL", "TimeRange", },
+              new java.lang.String[] { "Row", "MutateType", "ColumnValue", "Attribute", "Timestamp", "WriteToWAL", "TimeRange", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.Builder.class);
           internal_static_Mutate_ColumnValue_descriptor =
@@ -24169,40 +22002,8 @@ public final class ClientProtos {
               new java.lang.String[] { "Result", "ScannerId", "MoreResults", "Ttl", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.Builder.class);
-          internal_static_LockRowRequest_descriptor =
-            getDescriptor().getMessageTypes().get(12);
-          internal_static_LockRowRequest_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_LockRowRequest_descriptor,
-              new java.lang.String[] { "Region", "Row", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.Builder.class);
-          internal_static_LockRowResponse_descriptor =
-            getDescriptor().getMessageTypes().get(13);
-          internal_static_LockRowResponse_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_LockRowResponse_descriptor,
-              new java.lang.String[] { "LockId", "Ttl", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.Builder.class);
-          internal_static_UnlockRowRequest_descriptor =
-            getDescriptor().getMessageTypes().get(14);
-          internal_static_UnlockRowRequest_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UnlockRowRequest_descriptor,
-              new java.lang.String[] { "Region", "LockId", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.Builder.class);
-          internal_static_UnlockRowResponse_descriptor =
-            getDescriptor().getMessageTypes().get(15);
-          internal_static_UnlockRowResponse_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UnlockRowResponse_descriptor,
-              new java.lang.String[] { },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.Builder.class);
           internal_static_BulkLoadHFileRequest_descriptor =
-            getDescriptor().getMessageTypes().get(16);
+            getDescriptor().getMessageTypes().get(12);
           internal_static_BulkLoadHFileRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_BulkLoadHFileRequest_descriptor,
@@ -24218,7 +22019,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder.class);
           internal_static_BulkLoadHFileResponse_descriptor =
-            getDescriptor().getMessageTypes().get(17);
+            getDescriptor().getMessageTypes().get(13);
           internal_static_BulkLoadHFileResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_BulkLoadHFileResponse_descriptor,
@@ -24226,7 +22027,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.Builder.class);
           internal_static_Exec_descriptor =
-            getDescriptor().getMessageTypes().get(18);
+            getDescriptor().getMessageTypes().get(14);
           internal_static_Exec_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Exec_descriptor,
@@ -24234,7 +22035,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Exec.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Exec.Builder.class);
           internal_static_ExecCoprocessorRequest_descriptor =
-            getDescriptor().getMessageTypes().get(19);
+            getDescriptor().getMessageTypes().get(15);
           internal_static_ExecCoprocessorRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ExecCoprocessorRequest_descriptor,
@@ -24242,7 +22043,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.Builder.class);
           internal_static_ExecCoprocessorResponse_descriptor =
-            getDescriptor().getMessageTypes().get(20);
+            getDescriptor().getMessageTypes().get(16);
           internal_static_ExecCoprocessorResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ExecCoprocessorResponse_descriptor,
@@ -24250,7 +22051,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.Builder.class);
           internal_static_CoprocessorServiceCall_descriptor =
-            getDescriptor().getMessageTypes().get(21);
+            getDescriptor().getMessageTypes().get(17);
           internal_static_CoprocessorServiceCall_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceCall_descriptor,
@@ -24258,7 +22059,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall.Builder.class);
           internal_static_CoprocessorServiceRequest_descriptor =
-            getDescriptor().getMessageTypes().get(22);
+            getDescriptor().getMessageTypes().get(18);
           internal_static_CoprocessorServiceRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceRequest_descriptor,
@@ -24266,7 +22067,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.Builder.class);
           internal_static_CoprocessorServiceResponse_descriptor =
-            getDescriptor().getMessageTypes().get(23);
+            getDescriptor().getMessageTypes().get(19);
           internal_static_CoprocessorServiceResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceResponse_descriptor,
@@ -24274,7 +22075,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.Builder.class);
           internal_static_MultiAction_descriptor =
-            getDescriptor().getMessageTypes().get(24);
+            getDescriptor().getMessageTypes().get(20);
           internal_static_MultiAction_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiAction_descriptor,
@@ -24282,7 +22083,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction.Builder.class);
           internal_static_ActionResult_descriptor =
-            getDescriptor().getMessageTypes().get(25);
+            getDescriptor().getMessageTypes().get(21);
           internal_static_ActionResult_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ActionResult_descriptor,
@@ -24290,7 +22091,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ActionResult.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ActionResult.Builder.class);
           internal_static_MultiRequest_descriptor =
-            getDescriptor().getMessageTypes().get(26);
+            getDescriptor().getMessageTypes().get(22);
           internal_static_MultiRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiRequest_descriptor,
@@ -24298,7 +22099,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.Builder.class);
           internal_static_MultiResponse_descriptor =
-            getDescriptor().getMessageTypes().get(27);
+            getDescriptor().getMessageTypes().get(23);
           internal_static_MultiResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiResponse_descriptor,
diff --git hbase-protocol/src/main/protobuf/Client.proto hbase-protocol/src/main/protobuf/Client.proto
index 50aac7d..8ed67e7 100644
--- hbase-protocol/src/main/protobuf/Client.proto
+++ hbase-protocol/src/main/protobuf/Client.proto
@@ -42,13 +42,12 @@ message Get {
   required bytes row = 1;
   repeated Column column = 2;
   repeated NameBytesPair attribute = 3;
-  optional uint64 lockId = 4;
-  optional Filter filter = 5;
-  optional TimeRange timeRange = 6;
-  optional uint32 maxVersions = 7 [default = 1];
-  optional bool cacheBlocks = 8 [default = true];
-  optional uint32 storeLimit = 9;
-  optional uint32 storeOffset = 10;
+  optional Filter filter = 4;
+  optional TimeRange timeRange = 5;
+  optional uint32 maxVersions = 6 [default = 1];
+  optional bool cacheBlocks = 7 [default = true];
+  optional uint32 storeLimit = 8;
+  optional uint32 storeOffset = 9;
 }
 
 message Result {
@@ -109,8 +108,7 @@ message Mutate {
   repeated ColumnValue columnValue = 3;
   repeated NameBytesPair attribute = 4;
   optional uint64 timestamp = 5;
-  optional uint64 lockId = 6;
-  optional bool writeToWAL = 7 [default = true];
+  optional bool writeToWAL = 6 [default = true];
 
   // For some mutate, result may be returned, in which case,
   // time range can be specified for potential performance gain
@@ -219,24 +217,6 @@ message ScanResponse {
   optional uint32 ttl = 4;
 }
 
-message LockRowRequest {
-  required RegionSpecifier region = 1;
-  repeated bytes row = 2;
-}
-
-message LockRowResponse {
-  required uint64 lockId = 1;
-  optional uint32 ttl = 2;
-}
-
-message UnlockRowRequest {
-  required RegionSpecifier region = 1;
-  required uint64 lockId = 2;
-}
-
-message UnlockRowResponse {
-}
-
 /**
  * Atomically bulk load multiple HFiles (say from different column families)
  * into an open region.
@@ -361,12 +341,6 @@ service ClientService {
   rpc scan(ScanRequest)
     returns(ScanResponse);
 
-  rpc lockRow(LockRowRequest)
-    returns(LockRowResponse);
-
-  rpc unlockRow(UnlockRowRequest)
-    returns(UnlockRowResponse);
-
   rpc bulkLoadHFile(BulkLoadHFileRequest)
     returns(BulkLoadHFileResponse);
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
deleted file mode 100644
index e42f3a9..0000000
--- hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-
-/**
- * Thrown if a region server is passed an unknown row lock id
- */
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class UnknownRowLockException extends DoNotRetryIOException {
-  private static final long serialVersionUID = 993179627856392526L;
-
-  /** constructor */
-  public UnknownRowLockException() {
-    super();
-  }
-
-  /**
-   * Constructor
-   * @param s message
-   */
-  public UnknownRowLockException(String s) {
-    super(s);
-  }
-}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
index 9a75546..4103b77 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
@@ -74,12 +74,11 @@ public class Delete extends Mutation implements Comparable<Row> {
    * @param row row key
    */
   public Delete(byte [] row) {
-    this(row, HConstants.LATEST_TIMESTAMP, null);
+    this(row, HConstants.LATEST_TIMESTAMP);
   }
 
   /**
-   * Create a Delete operation for the specified row and timestamp, using
-   * an optional row lock.<p>
+   * Create a Delete operation for the specified row and timestamp.<p>
    *
    * If no further operations are done, this will delete all columns in all
    * families of the specified row with a timestamp less than or equal to the
@@ -89,14 +88,10 @@ public class Delete extends Mutation implements Comparable<Row> {
    * families or columns, you must specify each timestamp individually.
    * @param row row key
    * @param timestamp maximum version timestamp (only for delete row)
-   * @param rowLock previously acquired row lock, or null
    */
-  public Delete(byte [] row, long timestamp, RowLock rowLock) {
+  public Delete(byte [] row, long timestamp) {
     this.row = row;
     this.ts = timestamp;
-    if (rowLock != null) {
-    	this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -105,7 +100,6 @@ public class Delete extends Mutation implements Comparable<Row> {
   public Delete(final Delete d) {
     this.row = d.getRow();
     this.ts = d.getTimeStamp();
-    this.lockId = d.getLockId();
     this.familyMap.putAll(d.getFamilyMap());
     this.writeToWAL = d.writeToWAL;
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
index 0ade4e9..a5a9d37 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
@@ -66,7 +66,6 @@ public class Get extends OperationWithAttributes
   implements Row, Comparable<Row> {
 
   private byte [] row = null;
-  private long lockId = -1L;
   private int maxVersions = 1;
   private boolean cacheBlocks = true;
   private int storeLimit = -1;
@@ -84,22 +83,7 @@ public class Get extends OperationWithAttributes
    * @param row row key
    */
   public Get(byte [] row) {
-    this(row, null);
-  }
-
-  /**
-   * Create a Get operation for the specified row, using an existing row lock.
-   * <p>
-   * If no further operations are done, this will get the latest version of
-   * all columns in all families of the specified row.
-   * @param row row key
-   * @param rowLock previously acquired row lock, or null
-   */
-  public Get(byte [] row, RowLock rowLock) {
     this.row = row;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -261,22 +245,6 @@ public class Get extends OperationWithAttributes
   }
 
   /**
-   * Method for retrieving the get's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the get's lockId
-   * @return lockId
-   */
-  public long getLockId() {
-    return this.lockId;
-  }
-
-  /**
    * Method for retrieving the get's maximum number of version
    * @return the maximum number of version to fetch for this get
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
index 373a766..368b6d4 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
@@ -61,12 +61,9 @@ import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CompareType;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -980,46 +977,6 @@ public class HTable implements HTableInterface {
    * {@inheritDoc}
    */
   @Override
-  public RowLock lockRow(final byte [] row)
-  throws IOException {
-    return new ServerCallable<RowLock>(connection, tableName, row, operationTimeout) {
-        public RowLock call() throws IOException {
-          try {
-            LockRowRequest request = RequestConverter.buildLockRowRequest(
-              location.getRegionInfo().getRegionName(), row);
-            LockRowResponse response = server.lockRow(null, request);
-            return new RowLock(row, response.getLockId());
-          } catch (ServiceException se) {
-            throw ProtobufUtil.getRemoteException(se);
-          }
-        }
-      }.withRetries();
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public void unlockRow(final RowLock rl)
-  throws IOException {
-    new ServerCallable<Boolean>(connection, tableName, rl.getRow(), operationTimeout) {
-        public Boolean call() throws IOException {
-          try {
-            UnlockRowRequest request = RequestConverter.buildUnlockRowRequest(
-              location.getRegionInfo().getRegionName(), rl.getLockId());
-            server.unlockRow(null, request);
-            return Boolean.TRUE;
-          } catch (ServiceException se) {
-            throw ProtobufUtil.getRemoteException(se);
-          }
-        }
-      }.withRetries();
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
   public boolean isAutoFlush() {
     return autoFlush;
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
index a988b1d..05c5840 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
@@ -396,27 +396,6 @@ public interface HTableInterface extends Closeable {
   void close() throws IOException;
 
   /**
-   * Obtains a lock on a row.
-   *
-   * @param row The row to lock.
-   * @return A {@link RowLock} containing the row and lock id.
-   * @throws IOException if a remote or network exception occurs.
-   * @see RowLock
-   * @see #unlockRow
-   */
-  RowLock lockRow(byte[] row) throws IOException;
-
-  /**
-   * Releases a row lock.
-   *
-   * @param rl The row lock to release.
-   * @throws IOException if a remote or network exception occurs.
-   * @see RowLock
-   * @see #unlockRow
-   */
-  void unlockRow(RowLock rl) throws IOException;
-
-  /**
    * Creates and returns a proxy to the CoprocessorProtocol instance running in the
    * region containing the specified row.  The row given does not actually have
    * to exist.  Whichever region would contain the row based on start and end keys will
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
index 3dc5b49..93dcb96 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
@@ -465,16 +465,6 @@ public class HTablePool implements Closeable {
     }
 
     @Override
-    public RowLock lockRow(byte[] row) throws IOException {
-      return table.lockRow(row);
-    }
-
-    @Override
-    public void unlockRow(RowLock rl) throws IOException {
-      table.unlockRow(rl);
-    }
-
-    @Override
     public <T extends CoprocessorProtocol> T coprocessorProxy(
         Class<T> protocol, byte[] row) {
       return table.coprocessorProxy(protocol, row);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
index 7df6e4b..d4123a1 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
@@ -45,7 +45,6 @@ import org.apache.hadoop.hbase.util.Bytes;
 @InterfaceStability.Stable
 public class Increment implements Row {
   private byte [] row = null;
-  private long lockId = -1L;
   private boolean writeToWAL = true;
   private TimeRange tr = new TimeRange();
   private Map<byte [], NavigableMap<byte [], Long>> familyMap =
@@ -55,31 +54,17 @@ public class Increment implements Row {
   public Increment() {}
 
   /**
-   * Create a Increment operation for the specified row.
-   * <p>
-   * At least one column must be incremented.
-   * @param row row key
-   */
-  public Increment(byte [] row) {
-    this(row, null);
-  }
-
-  /**
    * Create a Increment operation for the specified row, using an existing row
    * lock.
    * <p>
    * At least one column must be incremented.
    * @param row row key
-   * @param rowLock previously acquired row lock, or null
    */
-  public Increment(byte [] row, RowLock rowLock) {
+  public Increment(byte [] row) {
     if (row == null) {
       throw new IllegalArgumentException("Cannot increment a null row");
     }
     this.row = row;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -119,22 +104,6 @@ public class Increment implements Row {
   }
 
   /**
-   * Method for retrieving the increment's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the increment's lockId
-   * @return lockId
-   */
-  public long getLockId() {
-    return this.lockId;
-  }
-
-  /**
    * Method for retrieving whether WAL will be written to or not
    * @return true if WAL should be used, false if not
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
index 3fd78bd..2c6b494 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
@@ -40,7 +40,6 @@ public abstract class Mutation extends OperationWithAttributes implements Row {
 
   protected byte [] row = null;
   protected long ts = HConstants.LATEST_TIMESTAMP;
-  protected long lockId = -1L;
   protected boolean writeToWAL = true;
   protected Map<byte [], List<KeyValue>> familyMap =
       new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
@@ -165,23 +164,6 @@ public abstract class Mutation extends OperationWithAttributes implements Row {
   }
 
   /**
-   * Method for retrieving the delete's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the delete's lock ID.
-   *
-   * @return The lock ID.
-   */
-  public long getLockId() {
-  return this.lockId;
-  }
-
-  /**
    * Method for retrieving the timestamp
    * @return timestamp
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
index 31b5573..805f53c 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
@@ -46,7 +46,7 @@ import java.util.TreeMap;
 public class Put extends Mutation implements HeapSize, Comparable<Row> {
   private static final long OVERHEAD = ClassSize.align(
       ClassSize.OBJECT + 2 * ClassSize.REFERENCE +
-      2 * Bytes.SIZEOF_LONG + Bytes.SIZEOF_BOOLEAN +
+      1 * Bytes.SIZEOF_LONG + Bytes.SIZEOF_BOOLEAN +
       ClassSize.REFERENCE + ClassSize.TREEMAP);
 
   /**
@@ -54,16 +54,7 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * @param row row key
    */
   public Put(byte [] row) {
-    this(row, null);
-  }
-
-  /**
-   * Create a Put operation for the specified row, using an existing row lock.
-   * @param row row key
-   * @param rowLock previously acquired row lock, or null
-   */
-  public Put(byte [] row, RowLock rowLock) {
-      this(row, HConstants.LATEST_TIMESTAMP, rowLock);
+    this(row, HConstants.LATEST_TIMESTAMP);
   }
 
   /**
@@ -73,24 +64,11 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * @param ts timestamp
    */
   public Put(byte[] row, long ts) {
-    this(row, ts, null);
-  }
-
-  /**
-   * Create a Put operation for the specified row, using a given timestamp, and an existing row lock.
-   * @param row row key
-   * @param ts timestamp
-   * @param rowLock previously acquired row lock, or null
-   */
-  public Put(byte [] row, long ts, RowLock rowLock) {
     if(row == null || row.length > HConstants.MAX_ROW_LENGTH) {
       throw new IllegalArgumentException("Row key is invalid");
     }
     this.row = Arrays.copyOf(row, row.length);
     this.ts = ts;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -98,7 +76,7 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * @param putToCopy put to copy
    */
   public Put(Put putToCopy) {
-    this(putToCopy.getRow(), putToCopy.ts, putToCopy.getRowLock());
+    this(putToCopy.getRow(), putToCopy.ts);
     this.familyMap =
       new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
     for(Map.Entry<byte [], List<KeyValue>> entry :
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java
deleted file mode 100644
index 6736877..0000000
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Holds row name and lock id.
- */
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class RowLock {
-  private byte [] row = null;
-  private long lockId = -1L;
-
-  /**
-   * Creates a RowLock from a row and lock id
-   * @param row row to lock on
-   * @param lockId the lock id
-   */
-  public RowLock(final byte [] row, final long lockId) {
-    this.row = row;
-    this.lockId = lockId;
-  }
-
-  /**
-   * Creates a RowLock with only a lock id
-   * @param lockId lock id
-   */
-  public RowLock(final long lockId) {
-    this.lockId = lockId;
-  }
-
-  /**
-   * Get the row for this RowLock
-   * @return the row
-   */
-  public byte [] getRow() {
-    return row;
-  }
-
-  /**
-   * Get the lock id from this RowLock
-   * @return the lock id
-   */
-  public long getLockId() {
-    return lockId;
-  }
-}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
index e0e90ba..33476aa 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
@@ -547,16 +547,6 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
         return tableName;
       }
 
-      public RowLock lockRow(byte[] row) throws IOException {
-        throw new RuntimeException(
-          "row locking is not allowed within the coprocessor environment");
-      }
-
-      public void unlockRow(RowLock rl) throws IOException {
-        throw new RuntimeException(
-          "row locking is not allowed within the coprocessor environment");
-      }
-
       @Override
       public void batch(List<? extends Row> actions, Object[] results)
           throws IOException, InterruptedException {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index f4786a1..885bc5d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -62,7 +62,6 @@ import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
@@ -354,11 +353,7 @@ public final class ProtobufUtil {
       final ClientProtos.Get proto) throws IOException {
     if (proto == null) return null;
     byte[] row = proto.getRow().toByteArray();
-    RowLock rowLock = null;
-    if (proto.hasLockId()) {
-      rowLock = new RowLock(proto.getLockId());
-    }
-    Get get = new Get(row, rowLock);
+    Get get = new Get(row);
     if (proto.hasCacheBlocks()) {
       get.setCacheBlocks(proto.getCacheBlocks());
     }
@@ -421,11 +416,7 @@ public final class ProtobufUtil {
     if (proto.hasTimestamp()) {
       timestamp = proto.getTimestamp();
     }
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
-    Put put = new Put(row, timestamp, lock);
+    Put put = new Put(row, timestamp);
     put.setWriteToWAL(proto.getWriteToWAL());
     for (NameBytesPair attribute: proto.getAttributeList()) {
       put.setAttribute(attribute.getName(),
@@ -464,11 +455,7 @@ public final class ProtobufUtil {
     if (proto.hasTimestamp()) {
       timestamp = proto.getTimestamp();
     }
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
-    Delete delete = new Delete(row, timestamp, lock);
+    Delete delete = new Delete(row, timestamp);
     delete.setWriteToWAL(proto.getWriteToWAL());
     for (NameBytesPair attribute: proto.getAttributeList()) {
       delete.setAttribute(attribute.getName(),
@@ -563,12 +550,8 @@ public final class ProtobufUtil {
       final Mutate proto) throws IOException {
     MutateType type = proto.getMutateType();
     assert type == MutateType.INCREMENT : type.name();
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
     byte[] row = proto.getRow().toByteArray();
-    Increment increment = new Increment(row, lock);
+    Increment increment = new Increment(row);
     increment.setWriteToWAL(proto.getWriteToWAL());
     if (proto.hasTimeRange()) {
       HBaseProtos.TimeRange timeRange = proto.getTimeRange();
@@ -789,9 +772,6 @@ public final class ProtobufUtil {
     builder.setRow(ByteString.copyFrom(get.getRow()));
     builder.setCacheBlocks(get.getCacheBlocks());
     builder.setMaxVersions(get.getMaxVersions());
-    if (get.getLockId() >= 0) {
-      builder.setLockId(get.getLockId());
-    }
     if (get.getFilter() != null) {
       builder.setFilter(ProtobufUtil.toFilter(get.getFilter()));
     }
@@ -847,9 +827,6 @@ public final class ProtobufUtil {
     builder.setRow(ByteString.copyFrom(increment.getRow()));
     builder.setMutateType(MutateType.INCREMENT);
     builder.setWriteToWAL(increment.getWriteToWAL());
-    if (increment.getLockId() >= 0) {
-      builder.setLockId(increment.getLockId());
-    }
     TimeRange timeRange = increment.getTimeRange();
     if (!timeRange.isAllTime()) {
       HBaseProtos.TimeRange.Builder timeRangeBuilder =
@@ -892,9 +869,6 @@ public final class ProtobufUtil {
     mutateBuilder.setRow(ByteString.copyFrom(mutation.getRow()));
     mutateBuilder.setMutateType(mutateType);
     mutateBuilder.setWriteToWAL(mutation.getWriteToWAL());
-    if (mutation.getLockId() >= 0) {
-      mutateBuilder.setLockId(mutation.getLockId());
-    }
     mutateBuilder.setTimestamp(mutation.getTimeStamp());
     Map<String, byte[]> attributes = mutation.getAttributesMap();
     if (!attributes.isEmpty()) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
index 0218f2e..e0ae6a5 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
@@ -68,7 +68,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Column;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Condition;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate;
@@ -77,7 +76,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.ColumnValu
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.MutateType;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CompareType;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
@@ -437,40 +435,6 @@ public final class RequestConverter {
   }
 
   /**
-   * Create a protocol buffer LockRowRequest
-   *
-   * @param regionName
-   * @param row
-   * @return a lock row request
-   */
-  public static LockRowRequest buildLockRowRequest(
-      final byte[] regionName, final byte[] row) {
-    LockRowRequest.Builder builder = LockRowRequest.newBuilder();
-    RegionSpecifier region = buildRegionSpecifier(
-      RegionSpecifierType.REGION_NAME, regionName);
-    builder.setRegion(region);
-    builder.addRow(ByteString.copyFrom(row));
-    return builder.build();
-  }
-
-  /**
-   * Create a protocol buffer UnlockRowRequest
-   *
-   * @param regionName
-   * @param lockId
-   * @return a unlock row request
-   */
-  public static UnlockRowRequest buildUnlockRowRequest(
-      final byte[] regionName, final long lockId) {
-    UnlockRowRequest.Builder builder = UnlockRowRequest.newBuilder();
-    RegionSpecifier region = buildRegionSpecifier(
-      RegionSpecifierType.REGION_NAME, regionName);
-    builder.setRegion(region);
-    builder.setLockId(lockId);
-    return builder.build();
-  }
-
-  /**
    * Create a protocol buffer bulk load request
    *
    * @param familyPaths
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 0651ef5..0e89caa 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -97,7 +97,6 @@ import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
@@ -1721,7 +1720,7 @@ public class HRegion implements HeapSize { // , Writable{
       if (key != null) {
         Get get = new Get(key.getRow());
         get.addFamily(family);
-        result = get(get, null);
+        result = get(get);
       }
       if (coprocessorHost != null) {
         coprocessorHost.postGetClosestRowBefore(row, family, result);
@@ -1802,28 +1801,19 @@ public class HRegion implements HeapSize { // , Writable{
   //////////////////////////////////////////////////////////////////////////////
   /**
    * @param delete delete object
-   * @param lockid existing lock id, or null for grab a lock
    * @param writeToWAL append to the write ahead lock or not
    * @throws IOException read exceptions
    */
-  public void delete(Delete delete, Integer lockid, boolean writeToWAL)
+  public void delete(Delete delete, boolean writeToWAL)
   throws IOException {
     checkReadOnly();
     checkResources();
-    Integer lid = null;
     startRegionOperation();
     this.writeRequestsCount.increment();
     try {
       byte [] row = delete.getRow();
-      // If we did not pass an existing row lock, obtain a new one
-      lid = getLock(lockid, row, true);
-
-      try {
-        // All edits for the given row (across all column families) must happen atomically.
-        doBatchMutate(delete, lid);
-      } finally {
-        if(lockid == null) releaseRowLock(lid);
-      }
+      // All edits for the given row (across all column families) must happen atomically.
+      doBatchMutate(delete, null);
     } finally {
       closeRegionOperation();
     }
@@ -1903,7 +1893,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @throws IOException
    */
   public void put(Put put) throws IOException {
-    this.put(put, null, put.getWriteToWAL());
+    this.put(put, put.getWriteToWAL());
   }
 
   /**
@@ -1911,28 +1901,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @param writeToWAL
    * @throws IOException
    */
-  public void put(Put put, boolean writeToWAL) throws IOException {
-    this.put(put, null, writeToWAL);
-  }
-
-  /**
-   * @param put
-   * @param lockid
-   * @throws IOException
-   */
-  public void put(Put put, Integer lockid) throws IOException {
-    this.put(put, lockid, put.getWriteToWAL());
-  }
-
-
-
-  /**
-   * @param put
-   * @param lockid
-   * @param writeToWAL
-   * @throws IOException
-   */
-  public void put(Put put, Integer lockid, boolean writeToWAL)
+  public void put(Put put, boolean writeToWAL)
   throws IOException {
     checkReadOnly();
 
@@ -1950,15 +1919,9 @@ public class HRegion implements HeapSize { // , Writable{
       // See HRegionServer#RegionListener for how the expire on HRegionServer
       // invokes a HRegion#abort.
       byte [] row = put.getRow();
-      // If we did not pass an existing row lock, obtain a new one
-      Integer lid = getLock(lockid, row, true);
 
-      try {
-        // All edits for the given row (across all column families) must happen atomically.
-        doBatchMutate(put, lid);
-      } finally {
-        if(lockid == null) releaseRowLock(lid);
-      }
+      // All edits for the given row (across all column families) must happen atomically.
+      doBatchMutate(put, null);
     } finally {
       closeRegionOperation();
     }
@@ -2397,14 +2360,14 @@ public class HRegion implements HeapSize { // , Writable{
    * @param qualifier
    * @param compareOp
    * @param comparator
-   * @param lockId
+   * @param w
    * @param writeToWAL
    * @throws IOException
    * @return true if the new put was executed, false otherwise
    */
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
       CompareOp compareOp, ByteArrayComparable comparator, Mutation w,
-      Integer lockId, boolean writeToWAL)
+      boolean writeToWAL)
   throws IOException{
     checkReadOnly();
     //TODO, add check for value length or maybe even better move this to the
@@ -2420,13 +2383,12 @@ public class HRegion implements HeapSize { // , Writable{
 
     startRegionOperation();
     try {
-      RowLock lock = isPut ? ((Put)w).getRowLock() : ((Delete)w).getRowLock();
-      Get get = new Get(row, lock);
+      Get get = new Get(row);
       checkFamily(family);
       get.addColumn(family, qualifier);
 
       // Lock row
-      Integer lid = getLock(lockId, get.getRow(), true);
+      Integer lid = getLock(null, get.getRow(), true);
       // wait for all previous transactions to complete (with lock held)
       mvcc.completeMemstoreInsert(mvcc.beginMemstoreInsert());
       List<KeyValue> result = null;
@@ -2479,7 +2441,7 @@ public class HRegion implements HeapSize { // , Writable{
         this.checkAndMutateChecksFailed.increment();
         return false;
       } finally {
-        if(lockId == null) releaseRowLock(lid);
+        releaseRowLock(lid);
       }
     } finally {
       closeRegionOperation();
@@ -2595,7 +2557,7 @@ public class HRegion implements HeapSize { // , Writable{
    * @praram now
    * @throws IOException
    */
-  private void put(final byte [] row, byte [] family, List<KeyValue> edits, Integer lid)
+  private void put(final byte [] row, byte [] family, List<KeyValue> edits)
   throws IOException {
     Map<byte[], List<KeyValue>> familyMap;
     familyMap = new HashMap<byte[], List<KeyValue>>();
@@ -2605,7 +2567,7 @@ public class HRegion implements HeapSize { // , Writable{
     p.setFamilyMap(familyMap);
     p.setClusterId(HConstants.DEFAULT_CLUSTER_ID);
     p.setWriteToWAL(true);
-    doBatchMutate(p, lid);
+    doBatchMutate(p, null);
   }
  
   /**
@@ -3124,13 +3086,8 @@ public class HRegion implements HeapSize { // , Writable{
    * <pre>
    *   LOCKS ==> ROWS
    * </pre>
-   *
-   * But it acts as a guard on the client; a miswritten client just can't
-   * submit the name of a row and start writing to it; it must know the correct
-   * lockid, which matches the lock list in memory.
-   *
-   * <p>It would be more memory-efficient to assume a correctly-written client,
-   * which maybe we'll do in the future.
+   * <p>It would be more memory-efficient to just have one mapping;
+   * maybe we'll do that in the future.
    *
    * @param row Name of row to lock.
    * @throws IOException
@@ -3153,7 +3110,7 @@ public class HRegion implements HeapSize { // , Writable{
    *        null if unavailable.
    */
   private Integer internalObtainRowLock(final byte[] row, boolean waitForLock)
-      throws IOException {
+  throws IOException {
     checkRow(row, "row lock");
     startRegionOperation();
     try {
@@ -3199,16 +3156,6 @@ public class HRegion implements HeapSize { // , Writable{
   }
 
   /**
-   * Used by unit tests.
-   * @param lockid
-   * @return Row that goes with <code>lockid</code>
-   */
-  byte[] getRowFromLock(final Integer lockid) {
-    HashedBytes rowKey = lockIds.get(lockid);
-    return rowKey == null ? null : rowKey.getBytes();
-  }
-
-  /**
    * Release the row lock!
    * @param lockId  The lock ID to release.
    */
@@ -3968,21 +3915,16 @@ public class HRegion implements HeapSize { // , Writable{
     meta.checkResources();
     // The row key is the region name
     byte[] row = r.getRegionName();
-    Integer lid = meta.obtainRowLock(row);
-    try {
-      final long now = EnvironmentEdgeManager.currentTimeMillis();
-      final List<KeyValue> edits = new ArrayList<KeyValue>(2);
-      edits.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
-        HConstants.REGIONINFO_QUALIFIER, now,
-        r.getRegionInfo().toByteArray()));
-      // Set into the root table the version of the meta table.
-      edits.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
-        HConstants.META_VERSION_QUALIFIER, now,
-        Bytes.toBytes(HConstants.META_VERSION)));
-      meta.put(row, HConstants.CATALOG_FAMILY, edits, lid);
-    } finally {
-      meta.releaseRowLock(lid);
-    }
+    final long now = EnvironmentEdgeManager.currentTimeMillis();
+    final List<KeyValue> edits = new ArrayList<KeyValue>(2);
+    edits.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
+      HConstants.REGIONINFO_QUALIFIER, now,
+      r.getRegionInfo().toByteArray()));
+    // Set into the root table the version of the meta table.
+    edits.add(new KeyValue(row, HConstants.CATALOG_FAMILY,
+      HConstants.META_VERSION_QUALIFIER, now,
+      Bytes.toBytes(HConstants.META_VERSION)));
+    meta.put(row, HConstants.CATALOG_FAMILY, edits);
   }
 
   /**
@@ -4289,11 +4231,10 @@ public class HRegion implements HeapSize { // , Writable{
   //
   /**
    * @param get get object
-   * @param lockid existing lock id, or null for no previous lock
    * @return result
    * @throws IOException read exceptions
    */
-  public Result get(final Get get, final Integer lockid) throws IOException {
+  public Result get(final Get get) throws IOException {
     checkRow(get.getRow(), "Get");
     // Verify families are all valid
     if (get.hasFamilies()) {
@@ -4584,12 +4525,11 @@ public class HRegion implements HeapSize { // , Writable{
    * Perform one or more append operations on a row.
    *
    * @param append
-   * @param lockid
    * @param writeToWAL
    * @return new keyvalues after increment
    * @throws IOException
    */
-  public Result append(Append append, Integer lockid, boolean writeToWAL)
+  public Result append(Append append, boolean writeToWAL)
       throws IOException {
     byte[] row = append.getRow();
     checkRow(row, "append");
@@ -4606,7 +4546,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(lockid, row, true);
+      Integer lid = getLock(null, row, true);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
@@ -4748,13 +4688,11 @@ public class HRegion implements HeapSize { // , Writable{
   /**
    * Perform one or more increment operations on a row.
    * @param increment
-   * @param lockid
    * @param writeToWAL
    * @return new keyvalues after increment
    * @throws IOException
    */
-  public Result increment(Increment increment, Integer lockid,
-      boolean writeToWAL)
+  public Result increment(Increment increment, boolean writeToWAL)
   throws IOException {
     byte [] row = increment.getRow();
     checkRow(row, "increment");
@@ -4772,7 +4710,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(lockid, row, true);
+      Integer lid = getLock(null, row, true);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index 98c1c36..7341294 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -77,7 +77,6 @@ import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.Stoppable;
 import org.apache.hadoop.hbase.TableDescriptors;
-import org.apache.hadoop.hbase.UnknownRowLockException;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.YouAreDeadException;
 import org.apache.hadoop.hbase.ZNodeClearer;
@@ -152,8 +151,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRe
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate;
@@ -162,8 +159,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameBytesPair;
@@ -597,8 +592,6 @@ public class  HRegionServer implements ClientProtocol,
         GetRequest.class,
         MutateRequest.class,
         ScanRequest.class,
-        LockRowRequest.class,
-        UnlockRowRequest.class,
         MultiRequest.class
     };
 
@@ -2276,28 +2269,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Instantiated as a row lock lease. If the lease times out, the row lock is
-   * released
-   */
-  private class RowLockListener implements LeaseListener {
-    private final String lockName;
-    private final HRegion region;
-
-    RowLockListener(final String lockName, final HRegion region) {
-      this.lockName = lockName;
-      this.region = region;
-    }
-
-    public void leaseExpired() {
-      LOG.info("Row Lock " + this.lockName + " lease expired");
-      Integer r = rowlocks.remove(this.lockName);
-      if (r != null) {
-        region.releaseRowLock(r);
-      }
-    }
-  }
-
-  /**
    * Instantiated as a scanner lease. If the lease times out, the scanner is
    * closed
    */
@@ -2335,29 +2306,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Method to get the Integer lock identifier used internally from the long
-   * lock identifier used by the client.
-   *
-   * @param lockId
-   *          long row lock identifier from client
-   * @return intId Integer row lock used internally in HRegion
-   * @throws IOException
-   *           Thrown if this is not a valid client lock id.
-   */
-  Integer getLockFromId(long lockId) throws IOException {
-    if (lockId == -1L) {
-      return null;
-    }
-    String lockName = String.valueOf(lockId);
-    Integer rl = rowlocks.get(lockName);
-    if (rl == null) {
-      throw new UnknownRowLockException("Invalid row lock");
-    }
-    this.leases.renewLease(lockName);
-    return rl;
-  }
-
-  /**
    * Called to verify that this server is up and running.
    *
    * @throws IOException
@@ -2601,18 +2549,6 @@ public class  HRegionServer implements ClientProtocol,
     return this.fsOk;
   }
 
-  protected long addRowLock(Integer r, HRegion region) throws LeaseStillHeldException {
-    String lockName = null;
-    long lockId;
-    do {
-      lockId = nextLong();
-      lockName = String.valueOf(lockId);
-    } while (rowlocks.putIfAbsent(lockName, r) != null);
-    this.leases.createLease(lockName, this.rowLockLeaseTimeoutPeriod, new RowLockListener(lockName,
-        region));
-    return lockId;
-  }
-
   protected long addScanner(RegionScanner s) throws LeaseStillHeldException {
     long scannerId = -1;
     while (true) {
@@ -2680,8 +2616,7 @@ public class  HRegionServer implements ClientProtocol,
           existence = region.getCoprocessorHost().preExists(clientGet);
         }
         if (existence == null) {
-          Integer lock = getLockFromId(clientGet.getLockId());
-          r = region.get(clientGet, lock);
+          r = region.get(clientGet);
           if (request.getExistenceOnly()) {
             boolean exists = r != null && !r.isEmpty();
             if (region.getCoprocessorHost() != null) {
@@ -2722,7 +2657,6 @@ public class  HRegionServer implements ClientProtocol,
       if (!region.getRegionInfo().isMetaTable()) {
         cacheFlusher.reclaimMemStoreMemory();
       }
-      Integer lock = null;
       Result r = null;
       Boolean processed = null;
       MutateType type = mutate.getMutateType();
@@ -2735,7 +2669,6 @@ public class  HRegionServer implements ClientProtocol,
         break;
       case PUT:
         Put put = ProtobufUtil.toPut(mutate);
-        lock = getLockFromId(put.getLockId());
         if (request.hasCondition()) {
           Condition condition = request.getCondition();
           byte[] row = condition.getRow().toByteArray();
@@ -2750,7 +2683,7 @@ public class  HRegionServer implements ClientProtocol,
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, put, lock, true);
+              qualifier, compareOp, comparator, put, true);
             if (region.getCoprocessorHost() != null) {
               result = region.getCoprocessorHost().postCheckAndPut(row, family,
                 qualifier, compareOp, comparator, put, result);
@@ -2758,13 +2691,12 @@ public class  HRegionServer implements ClientProtocol,
             processed = Boolean.valueOf(result);
           }
         } else {
-          region.put(put, lock);
+          region.put(put);
           processed = Boolean.TRUE;
         }
         break;
       case DELETE:
         Delete delete = ProtobufUtil.toDelete(mutate);
-        lock = getLockFromId(delete.getLockId());
         if (request.hasCondition()) {
           Condition condition = request.getCondition();
           byte[] row = condition.getRow().toByteArray();
@@ -2779,7 +2711,7 @@ public class  HRegionServer implements ClientProtocol,
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, delete, lock, true);
+              qualifier, compareOp, comparator, delete, true);
             if (region.getCoprocessorHost() != null) {
               result = region.getCoprocessorHost().postCheckAndDelete(row, family,
                 qualifier, compareOp, comparator, delete, result);
@@ -2787,7 +2719,7 @@ public class  HRegionServer implements ClientProtocol,
             processed = Boolean.valueOf(result);
           }
         } else {
-          region.delete(delete, lock, delete.getWriteToWAL());
+          region.delete(delete, delete.getWriteToWAL());
           processed = Boolean.TRUE;
         }
         break;
@@ -3032,78 +2964,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Lock a row in a table.
-   *
-   * @param controller the RPC controller
-   * @param request the lock row request
-   * @throws ServiceException
-   */
-  @Override
-  public LockRowResponse lockRow(final RpcController controller,
-      final LockRowRequest request) throws ServiceException {
-    try {
-      if (request.getRowCount() != 1) {
-        throw new DoNotRetryIOException(
-          "lockRow supports only one row now, not " + request.getRowCount() + " rows");
-      }
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      byte[] row = request.getRow(0).toByteArray();
-      try {
-        Integer r = region.obtainRowLock(row);
-        long lockId = addRowLock(r, region);
-        LOG.debug("Row lock " + lockId + " explicitly acquired by client");
-        LockRowResponse.Builder builder = LockRowResponse.newBuilder();
-        builder.setLockId(lockId);
-        return builder.build();
-      } catch (Throwable t) {
-        throw convertThrowableToIOE(cleanup(t,
-          "Error obtaining row lock (fsOk: " + this.fsOk + ")"));
-      }
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Unlock a locked row in a table.
-   *
-   * @param controller the RPC controller
-   * @param request the unlock row request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public UnlockRowResponse unlockRow(final RpcController controller,
-      final UnlockRowRequest request) throws ServiceException {
-    try {
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      if (!request.hasLockId()) {
-        throw new DoNotRetryIOException(
-          "Invalid unlock rowrequest, missing lock id");
-      }
-      long lockId = request.getLockId();
-      String lockName = String.valueOf(lockId);
-      try {
-        Integer r = rowlocks.remove(lockName);
-        if (r == null) {
-          throw new UnknownRowLockException(lockName);
-        }
-        region.releaseRowLock(r);
-        this.leases.cancelLease(lockName);
-        LOG.debug("Row lock " + lockId
-          + " has been explicitly released by client");
-        return UnlockRowResponse.newBuilder().build();
-      } catch (Throwable t) {
-        throw convertThrowableToIOE(cleanup(t));
-      }
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
    * Atomically bulk load several HFiles into an open region
    * @return true if successful, false is failed but recoverably (no action)
    * @throws IOException if failed unrecoverably
@@ -3231,8 +3091,7 @@ public class  HRegionServer implements ClientProtocol,
             Object result = null;
             if (actionUnion.hasGet()) {
               Get get = ProtobufUtil.toGet(actionUnion.getGet());
-              Integer lock = getLockFromId(get.getLockId());
-              Result r = region.get(get, lock);
+              Result r = region.get(get);
               if (r != null) {
                 result = ProtobufUtil.toResult(r);
               }
@@ -3745,8 +3604,7 @@ public class  HRegionServer implements ClientProtocol,
       r = region.getCoprocessorHost().preAppend(append);
     }
     if (r == null) {
-      Integer lock = getLockFromId(append.getLockId());
-      r = region.append(append, lock, append.getWriteToWAL());
+      r = region.append(append, append.getWriteToWAL());
       if (region.getCoprocessorHost() != null) {
         region.getCoprocessorHost().postAppend(append, r);
       }
@@ -3772,8 +3630,7 @@ public class  HRegionServer implements ClientProtocol,
       r = region.getCoprocessorHost().preIncrement(increment);
     }
     if (r == null) {
-      Integer lock = getLockFromId(increment.getLockId());
-      r = region.increment(increment, lock, increment.getWriteToWAL());
+      r = region.increment(increment, increment.getWriteToWAL());
       if (region.getCoprocessorHost() != null) {
         r = region.getCoprocessorHost().postIncrement(increment, r);
       }
@@ -3811,8 +3668,7 @@ public class  HRegionServer implements ClientProtocol,
           mutation = ProtobufUtil.toDelete(m);
           batchContainsDelete = true;
         }
-        Integer lock = getLockFromId(mutation.getLockId());
-        mutationsWithLocks[i++] = new Pair<Mutation, Integer>(mutation, lock);
+        mutationsWithLocks[i++] = new Pair<Mutation, Integer>(mutation, null);
         builder.addResult(result);
       }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
index 4fce0f5..8db0502 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResource.java
@@ -328,7 +328,7 @@ public class RowResource extends ResourceBase {
     }
     Delete delete = null;
     if (rowspec.hasTimestamp())
-      delete = new Delete(rowspec.getRow(), rowspec.getTimestamp(), null);
+      delete = new Delete(rowspec.getRow(), rowspec.getTimestamp());
     else
       delete = new Delete(rowspec.getRow());
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
index aef10c7..8960399 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
@@ -55,7 +55,6 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Row;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.rest.Constants;
@@ -607,14 +606,6 @@ public class RemoteHTable implements HTableInterface {
     throw new IOException("getRowOrBefore not supported");
   }
 
-  public RowLock lockRow(byte[] row) throws IOException {
-    throw new IOException("lockRow not implemented");
-  }
-
-  public void unlockRow(RowLock rl) throws IOException {
-    throw new IOException("unlockRow not implemented");
-  }
-
   public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
       byte[] value, Put put) throws IOException {
     // column to check-the-value
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
index 3441c14..dcd8a09 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
@@ -903,7 +903,7 @@ public class ThriftServerRunner implements Runnable {
         Map<ByteBuffer, ByteBuffer> attributes) throws IOError {
       try {
         HTable table = getTable(tableName);
-        Delete delete  = new Delete(getBytes(row), timestamp, null);
+        Delete delete  = new Delete(getBytes(row), timestamp);
         addAttributes(delete, attributes);
         table.delete(delete);
       } catch (IOException e) {
@@ -969,7 +969,7 @@ public class ThriftServerRunner implements Runnable {
       HTable table = null;
       try {
         table = getTable(tableName);
-        Put put = new Put(getBytes(row), timestamp, null);
+        Put put = new Put(getBytes(row), timestamp);
         addAttributes(put, attributes);
 
         Delete delete = new Delete(getBytes(row));
@@ -1034,7 +1034,7 @@ public class ThriftServerRunner implements Runnable {
         List<Mutation> mutations = batch.mutations;
         Delete delete = new Delete(row);
         addAttributes(delete, attributes);
-        Put put = new Put(row, timestamp, null);
+        Put put = new Put(row, timestamp);
         addAttributes(put, attributes);
         for (Mutation m : mutations) {
           byte[][] famAndQf = KeyValue.parseColumn(getBytes(m.column));
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
index 0edfcd2..0188dab 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
@@ -148,7 +148,7 @@ public class ThriftUtilities {
     Put out;
 
     if (in.isSetTimestamp()) {
-      out = new Put(in.getRow(), in.getTimestamp(), null);
+      out = new Put(in.getRow(), in.getTimestamp());
     } else {
       out = new Put(in.getRow());
     }
@@ -222,7 +222,7 @@ public class ThriftUtilities {
       }
     } else {
       if (in.isSetTimestamp()) {
-        out = new Delete(in.getRow(), in.getTimestamp(), null);
+        out = new Delete(in.getRow(), in.getTimestamp());
       } else {
         out = new Delete(in.getRow());
       }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
index 777f5e2..0d1f2b4 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/util/HMerge.java
@@ -415,7 +415,7 @@ class HMerge {
             HConstants.SPLITA_QUALIFIER);
         delete.deleteColumns(HConstants.CATALOG_FAMILY,
             HConstants.SPLITB_QUALIFIER);
-        root.delete(delete, null, true);
+        root.delete(delete, true);
 
         if(LOG.isDebugEnabled()) {
           LOG.debug("updated columns in row: " + Bytes.toStringBinary(regionsToDelete[r]));
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
index 85d365f..3037aae 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/util/Merge.java
@@ -151,13 +151,13 @@ public class Merge extends Configured implements Tool {
     HRegion rootRegion = utils.getRootRegion();
     Get get = new Get(region1);
     get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-    Result result1 =  rootRegion.get(get, null);
+    Result result1 =  rootRegion.get(get);
     Preconditions.checkState(!result1.isEmpty(), "First region cells can not be null");
     HRegionInfo info1 = HRegionInfo.getHRegionInfo(result1);
 
     get = new Get(region2);
     get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-    Result result2 =  rootRegion.get(get, null);
+    Result result2 =  rootRegion.get(get);
     Preconditions.checkState(!result2.isEmpty(), "Second region cells can not be null");
     HRegionInfo info2 = HRegionInfo.getHRegionInfo(result2);
     HRegion merged = merge(HTableDescriptor.META_TABLEDESC, info1, rootRegion, info2, rootRegion);
@@ -222,7 +222,7 @@ public class Merge extends Configured implements Tool {
     HRegion metaRegion1 = this.utils.getMetaRegion(meta1);
     Get get = new Get(region1);
     get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-    Result result1 =  metaRegion1.get(get, null);
+    Result result1 =  metaRegion1.get(get);
     Preconditions.checkState(!result1.isEmpty(),
         "First region cells can not be null");
     HRegionInfo info1 = HRegionInfo.getHRegionInfo(result1);
@@ -239,7 +239,7 @@ public class Merge extends Configured implements Tool {
     }
     get = new Get(region2);
     get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-    Result result2 =  metaRegion2.get(get, null);
+    Result result2 =  metaRegion2.get(get);
     Preconditions.checkState(!result2.isEmpty(),
         "Second region cells can not be null");
     HRegionInfo info2 = HRegionInfo.getHRegionInfo(result2);
@@ -335,8 +335,8 @@ public class Merge extends Configured implements Tool {
     }
 
     Delete delete  = new Delete(regioninfo.getRegionName(),
-        System.currentTimeMillis(), null);
-    meta.delete(delete, null, true);
+        System.currentTimeMillis());
+    meta.delete(delete, true);
   }
 
   /*
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
index b6fb9b0..7b3c98c 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/util/MetaUtils.java
@@ -330,7 +330,7 @@ public class MetaUtils {
     if (LOG.isDebugEnabled()) {
       Get get = new Get(hri.getRegionName());
       get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-      Result res = r.get(get, null);
+      Result res = r.get(get);
       KeyValue [] kvs = res.raw();
       if(kvs.length <= 0) {
         return;
@@ -351,7 +351,7 @@ public class MetaUtils {
     if (LOG.isDebugEnabled()) {
       Get get = new Get(hri.getRegionName());
       get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-      Result res = r.get(get, null);
+      Result res = r.get(get);
       KeyValue [] kvs = res.raw();
       if(kvs.length <= 0) {
         return;
diff --git hbase-server/src/main/ruby/hbase/table.rb hbase-server/src/main/ruby/hbase/table.rb
index 64c0e10..1f8bca8 100644
--- hbase-server/src/main/ruby/hbase/table.rb
+++ hbase-server/src/main/ruby/hbase/table.rb
@@ -141,7 +141,7 @@ EOF
     # Delete a row
     def _deleteall_internal(row, column = nil, timestamp = org.apache.hadoop.hbase.HConstants::LATEST_TIMESTAMP)
       raise ArgumentError, "Row Not Found" if _get_internal(row).nil?
-      d = org.apache.hadoop.hbase.client.Delete.new(row.to_s.to_java_bytes, timestamp, nil)
+      d = org.apache.hadoop.hbase.client.Delete.new(row.to_s.to_java_bytes, timestamp)
       if column
         family, qualifier = parse_column_name(column)
         d.deleteColumns(family, qualifier, timestamp)
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
index 322e198..d287c28 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
@@ -332,7 +332,7 @@ public abstract class HBaseTestCase extends TestCase {
           try {
             Put put;
             if(ts != -1) {
-              put = new Put(t, ts, null);
+              put = new Put(t, ts);
             } else {
               put = new Put(t);
             }
@@ -403,11 +403,10 @@ public abstract class HBaseTestCase extends TestCase {
     /**
      *
      * @param delete
-     * @param lockid
      * @param writeToWAL
      * @throws IOException
      */
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete,  boolean writeToWAL)
     throws IOException;
 
     /**
@@ -448,13 +447,13 @@ public abstract class HBaseTestCase extends TestCase {
       region.put(put);
     }
 
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete,  boolean writeToWAL)
     throws IOException {
-      this.region.delete(delete, lockid, writeToWAL);
+      this.region.delete(delete, writeToWAL);
     }
 
     public Result get(Get get) throws IOException {
-      return region.get(get, null);
+      return region.get(get);
     }
 
     public ScannerIncommon getScanner(byte [] family, byte [][] qualifiers,
@@ -473,11 +472,6 @@ public abstract class HBaseTestCase extends TestCase {
           InternalScannerIncommon(region.getScanner(scan));
       }
 
-    public Result get(Get get, Integer lockid) throws IOException{
-      return this.region.get(get, lockid);
-    }
-
-
     public void flushcache() throws IOException {
       this.region.flushcache();
     }
@@ -502,7 +496,7 @@ public abstract class HBaseTestCase extends TestCase {
     }
 
 
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete, boolean writeToWAL)
     throws IOException {
       this.table.delete(delete);
     }
@@ -610,7 +604,7 @@ public abstract class HBaseTestCase extends TestCase {
     throws IOException {
       Get get = new Get(row);
       get.setTimeStamp(timestamp);
-      Result res = region.get(get, null);
+      Result res = region.get(get);
       NavigableMap<byte[], NavigableMap<byte[], NavigableMap<Long, byte[]>>> map =
         res.getMap();
       byte [] res_value = map.get(family).get(qualifier).get(timestamp);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
index 09fa97f..09e7b72 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
@@ -134,7 +134,7 @@ public class TestMultiVersions {
     final HTableDescriptor desc = new HTableDescriptor(tableName);
     desc.addFamily(new HColumnDescriptor(contents));
     this.admin.createTable(desc);
-    Put put = new Put(row, timestamp1, null);
+    Put put = new Put(row, timestamp1);
     put.add(contents, contents, value1);
     HTable table = new HTable(UTIL.getConfiguration(), tableName);
     table.put(put);
@@ -147,7 +147,7 @@ public class TestMultiVersions {
     // is tied to an HConnection that has since gone stale.
     table = new HTable(new Configuration(UTIL.getConfiguration()), tableName);
     // Overwrite previous value
-    put = new Put(row, timestamp2, null);
+    put = new Put(row, timestamp2);
     put.add(contents, contents, value2);
     table.put(put);
     // Now verify that getRow(row, column, latest) works
@@ -218,7 +218,7 @@ public class TestMultiVersions {
     // Insert data
     for (int i = 0; i < locations.size(); i++) {
       for (int j = 0; j < timestamp.length; j++) {
-        Put put = new Put(rows[i], timestamp[j], null);
+        Put put = new Put(rows[i], timestamp[j]);
         put.add(HConstants.CATALOG_FAMILY, null, timestamp[j],
             Bytes.toBytes(timestamp[j]));
         table.put(put);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
index 3f7edd0..bbc78d1 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
@@ -33,7 +33,6 @@ import java.util.NavigableSet;
 import java.util.Set;
 
 import org.apache.hadoop.hbase.client.Get;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
@@ -259,10 +258,8 @@ public class TestSerialization {
 
     long ts = System.currentTimeMillis();
     int maxVersions = 2;
-    long lockid = 5;
-    RowLock rowLock = new RowLock(lockid);
 
-    Get get = new Get(row, rowLock);
+    Get get = new Get(row);
     get.addColumn(fam, qf1);
     get.setTimeRange(ts, ts+1);
     get.setMaxVersions(maxVersions);
@@ -284,7 +281,6 @@ public class TestSerialization {
       }
     }
 
-    assertEquals(get.getLockId(), desGet.getLockId());
     assertEquals(get.getMaxVersions(), desGet.getMaxVersions());
     TimeRange tr = get.getTimeRange();
     TimeRange desTr = desGet.getTimeRange();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
index 633d7a9..7d24b6f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
@@ -87,7 +87,7 @@ public class TimestampTestBase extends HBaseTestCase {
 
     Delete delete = new Delete(ROW);
     delete.deleteColumns(FAMILY_NAME, QUALIFIER_NAME, T2);
-    incommon.delete(delete, null, true);
+    incommon.delete(delete, true);
 
     // Should only be current value in set.  Assert this is so
     assertOnlyLatest(incommon, HConstants.LATEST_TIMESTAMP);
@@ -241,7 +241,7 @@ public class TimestampTestBase extends HBaseTestCase {
   public static void put(final Incommon loader, final byte [] bytes,
     final long ts)
   throws IOException {
-    Put put = new Put(ROW, ts, null);
+    Put put = new Put(ROW, ts);
     put.setWriteToWAL(false);
     put.add(FAMILY_NAME, QUALIFIER_NAME, bytes);
     loader.put(put);
@@ -265,9 +265,9 @@ public class TimestampTestBase extends HBaseTestCase {
       final long ts)
   throws IOException {
     Delete delete = ts == HConstants.LATEST_TIMESTAMP?
-      new Delete(ROW): new Delete(ROW, ts, null);
+      new Delete(ROW): new Delete(ROW, ts);
     delete.deleteColumn(FAMILY_NAME, QUALIFIER_NAME, ts);
-    loader.delete(delete, null, true);
+    loader.delete(delete, true);
   }
 
   public static Result get(final Incommon loader) throws IOException {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index 8e48cb5..1a8f031 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -184,10 +184,10 @@ public class TestFromClientSide {
      p.add(FAMILY, C0, T3);
      h.put(p);
 
-     Delete d = new Delete(T1, ts+3, null);
+     Delete d = new Delete(T1, ts+3);
      h.delete(d);
 
-     d = new Delete(T1, ts+3, null);
+     d = new Delete(T1, ts+3);
      d.deleteColumns(FAMILY, C0, ts+3);
      h.delete(d);
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
index 02cd31d..44e1cfc 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestCoprocessorInterface.java
@@ -323,7 +323,7 @@ public class TestCoprocessorInterface extends HBaseTestCase {
     for (int i = 0; i < regions.length; i++) {
       try {
         Get g = new Get(regions[i].getStartKey());
-        regions[i].get(g, null);
+        regions[i].get(g);
         fail();
       } catch (DoNotRetryIOException xc) {
       }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
index e3075d2..2445fd5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
@@ -125,9 +125,7 @@ public class TestRegionObserverStacking extends TestCase {
 
     Put put = new Put(ROW);
     put.add(A, A, A);
-    int lockid = region.obtainRowLock(ROW);
-    region.put(put, lockid);
-    region.releaseRowLock(lockid);
+    region.put(put);
 
     Coprocessor c = h.findCoprocessor(ObserverA.class.getName());
     long idA = ((ObserverA)c).id;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
index eb43a1c..201b7a2 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilter.java
@@ -179,13 +179,13 @@ public class TestFilter {
       Delete d = new Delete(ROW);
       d.deleteColumns(FAMILIES[0], QUALIFIERS_ONE[1]);
       d.deleteColumns(FAMILIES[1], QUALIFIERS_ONE[1]);
-      this.region.delete(d, null, false);
+      this.region.delete(d, false);
     }
     for(byte [] ROW : ROWS_TWO) {
       Delete d = new Delete(ROW);
       d.deleteColumns(FAMILIES[0], QUALIFIERS_TWO[1]);
       d.deleteColumns(FAMILIES[1], QUALIFIERS_TWO[1]);
-      this.region.delete(d, null, false);
+      this.region.delete(d, false);
     }
     colsPerRow -= 2;
 
@@ -194,13 +194,13 @@ public class TestFilter {
       Delete d = new Delete(ROWS_ONE[1]);
       d.deleteColumns(FAMILIES[0], QUALIFIER);
       d.deleteColumns(FAMILIES[1], QUALIFIER);
-      this.region.delete(d, null, false);
+      this.region.delete(d, false);
     }
     for(byte [] QUALIFIER : QUALIFIERS_TWO) {
       Delete d = new Delete(ROWS_TWO[1]);
       d.deleteColumns(FAMILIES[0], QUALIFIER);
       d.deleteColumns(FAMILIES[1], QUALIFIER);
-      this.region.delete(d, null, false);
+      this.region.delete(d, false);
     }
     numRows -= 2;
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
index 7d81a60..3177a33 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
@@ -161,7 +161,7 @@ public class TestEncodedSeekers {
         final byte[] qualBytes = Bytes.toBytes(qualStr);
         Get get = new Get(rowKey);
         get.addColumn(CF_BYTES, qualBytes);
-        Result result = region.get(get, null);
+        Result result = region.get(get);
         assertEquals(1, result.size());
         assertTrue(LoadTestKVGenerator.verify(Bytes.toString(rowKey), qualStr,
             result.getValue(CF_BYTES, qualBytes)));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestForceCacheImportantBlocks.java hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestForceCacheImportantBlocks.java
index 59c4845..0f10416 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestForceCacheImportantBlocks.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestForceCacheImportantBlocks.java
@@ -114,7 +114,7 @@ public class TestForceCacheImportantBlocks {
 
     for (int i = 0; i < NUM_ROWS; ++i) {
       Get get = new Get(Bytes.toBytes("row" + i));
-      region.get(get, null);
+      region.get(get);
     }
 
     List<BlockCategory> importantBlockCategories =
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
index 390cc75..c4fc69c 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
@@ -215,7 +215,7 @@ public class TestImportExport {
     p.add(FAMILYA, QUAL, now+4, QUAL);
     t.put(p);
 
-    Delete d = new Delete(ROW1, now+3, null);
+    Delete d = new Delete(ROW1, now+3);
     t.delete(d);
     d = new Delete(ROW1);
     d.deleteColumns(FAMILYA, QUAL, now+2);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
index 7def888..2822a41 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
@@ -69,15 +69,11 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRe
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -386,20 +382,6 @@ class MockRegionServer implements AdminProtocol, ClientProtocol, RegionServerSer
   }
 
   @Override
-  public LockRowResponse lockRow(RpcController controller,
-      LockRowRequest request) throws ServiceException {
-    // TODO Auto-generated method stub
-    return null;
-  }
-
-  @Override
-  public UnlockRowResponse unlockRow(RpcController controller,
-      UnlockRowRequest request) throws ServiceException {
-    // TODO Auto-generated method stub
-    return null;
-  }
-
-  @Override
   public BulkLoadHFileResponse bulkLoadHFile(RpcController controller,
       BulkLoadHFileRequest request) throws ServiceException {
     // TODO Auto-generated method stub
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
index 92b0c45..e5bba9b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
@@ -93,11 +93,11 @@ public class TestAtomicOperation extends HBaseTestCase {
     a.setReturnResults(false);
     a.add(fam1, qual1, Bytes.toBytes(v1));
     a.add(fam1, qual2, Bytes.toBytes(v2));
-    assertNull(region.append(a, null, true));
+    assertNull(region.append(a, true));
     a = new Append(row);
     a.add(fam1, qual1, Bytes.toBytes(v2));
     a.add(fam1, qual2, Bytes.toBytes(v1));
-    Result result = region.append(a, null, true);
+    Result result = region.append(a, true);
     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v1+v2), result.getValue(fam1, qual1)));
     assertEquals(0, Bytes.compareTo(Bytes.toBytes(v2+v1), result.getValue(fam1, qual2)));
   }
@@ -150,7 +150,7 @@ public class TestAtomicOperation extends HBaseTestCase {
     // run a get and see?
     Get get = new Get(row);
     get.addColumn(familiy, qualifier);
-    Result result = region.get(get, null);
+    Result result = region.get(get);
     assertEquals(1, result.size());
 
     KeyValue kv = result.raw()[0];
@@ -210,11 +210,11 @@ public class TestAtomicOperation extends HBaseTestCase {
           inc.addColumn(fam1, qual1, amount);
           inc.addColumn(fam1, qual2, amount*2);
           inc.addColumn(fam2, qual3, amount*3);
-          region.increment(inc, null, true);
+          region.increment(inc, true);
 
           // verify: Make sure we only see completed increments
           Get g = new Get(row);
-          Result result = region.get(g, null);
+          Result result = region.get(g);
           assertEquals(Bytes.toLong(result.getValue(fam1, qual1))*2, Bytes.toLong(result.getValue(fam1, qual2))); 
           assertEquals(Bytes.toLong(result.getValue(fam1, qual1))*3, Bytes.toLong(result.getValue(fam2, qual3)));
         } catch (IOException e) {
@@ -246,10 +246,10 @@ public class TestAtomicOperation extends HBaseTestCase {
               a.add(fam1, qual1, val);
               a.add(fam1, qual2, val);
               a.add(fam2, qual3, val);
-              region.append(a, null, true);
+              region.append(a, true);
 
               Get g = new Get(row);
-              Result result = region.get(g, null);
+              Result result = region.get(g);
               assertEquals(result.getValue(fam1, qual1).length, result.getValue(fam1, qual2).length); 
               assertEquals(result.getValue(fam1, qual1).length, result.getValue(fam2, qual3).length); 
             } catch (IOException e) {
@@ -276,7 +276,7 @@ public class TestAtomicOperation extends HBaseTestCase {
     }
     assertEquals(0, failures.get());
     Get g = new Get(row);
-    Result result = region.get(g, null);
+    Result result = region.get(g);
     assertEquals(result.getValue(fam1, qual1).length, 10000);
     assertEquals(result.getValue(fam1, qual2).length, 10000);
     assertEquals(result.getValue(fam2, qual3).length, 10000);
@@ -336,7 +336,7 @@ public class TestAtomicOperation extends HBaseTestCase {
               op ^= true;
               // check: should always see exactly one column
               Get g = new Get(row);
-              Result r = region.get(g, null);
+              Result r = region.get(g);
               if (r.size() != 1) {
                 LOG.debug(r);
                 failures.incrementAndGet();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
index e7402c9..35671fa 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
@@ -158,7 +158,7 @@ public class TestBlocksRead extends HBaseTestCase {
         get.addColumn(cf, Bytes.toBytes(column));
       }
 
-      kvs = region.get(get, null).raw();
+      kvs = region.get(get).raw();
       long blocksEnd = getBlkAccessCount(cf);
       if (expBlocks[i] != -1) {
         assertEquals("Blocks Read Check for Bloom: " + bloomType, expBlocks[i],
@@ -189,7 +189,7 @@ public class TestBlocksRead extends HBaseTestCase {
     del.deleteFamily(Bytes.toBytes(family + "_ROWCOL"), version);
     del.deleteFamily(Bytes.toBytes(family + "_ROW"), version);
     del.deleteFamily(Bytes.toBytes(family + "_NONE"), version);
-    region.delete(del, null, true);
+    region.delete(del, true);
   }
 
   private static void verifyData(KeyValue kv, String expectedRow,
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
index 72c50aa..1b2b74b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestCompaction.java
@@ -127,7 +127,7 @@ public class TestCompaction extends HBaseTestCase {
     do {
       List<KeyValue> results = new ArrayList<KeyValue>();
       boolean result = s.next(results);
-      r.delete(new Delete(results.get(0).getRow()), null, false);
+      r.delete(new Delete(results.get(0).getRow()), false);
       if (!result) break;
     } while(true);
     s.close();
@@ -199,7 +199,7 @@ public class TestCompaction extends HBaseTestCase {
     // Default is that there only 3 (MAXVERSIONS) versions allowed per column.
     //
     // Assert == 3 when we ask for versions.
-    Result result = r.get(new Get(STARTROW).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100), null);
+    Result result = r.get(new Get(STARTROW).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
 
     // see if CompactionProgress is in place but null
@@ -229,7 +229,7 @@ public class TestCompaction extends HBaseTestCase {
 
     // Always 3 versions if that is what max versions is.
     result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).
-        setMaxVersions(100), null);
+        setMaxVersions(100));
     LOG.debug("Row " + Bytes.toStringBinary(secondRowBytes) + " after " +
         "initial compaction: " + result);
     assertEquals("Invalid number of versions of row "
@@ -242,32 +242,32 @@ public class TestCompaction extends HBaseTestCase {
     // should result in a compacted store file that has no references to the
     // deleted row.
     LOG.debug("Adding deletes to memstore and flushing");
-    Delete delete = new Delete(secondRowBytes, System.currentTimeMillis(), null);
+    Delete delete = new Delete(secondRowBytes, System.currentTimeMillis());
     byte [][] famAndQf = {COLUMN_FAMILY, null};
     delete.deleteFamily(famAndQf[0]);
-    r.delete(delete, null, true);
+    r.delete(delete, true);
 
     // Assert deleted.
-    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100), null );
+    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100));
     assertTrue("Second row should have been deleted", result.isEmpty());
 
     r.flushcache();
 
-    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100), null );
+    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100));
     assertTrue("Second row should have been deleted", result.isEmpty());
 
     // Add a bit of data and flush.  Start adding at 'bbb'.
     createSmallerStoreFile(this.r);
     r.flushcache();
     // Assert that the second row is still deleted.
-    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100), null );
+    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100));
     assertTrue("Second row should still be deleted", result.isEmpty());
 
     // Force major compaction.
     r.compactStores(true);
     assertEquals(r.getStore(COLUMN_FAMILY_TEXT).getStorefiles().size(), 1);
 
-    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100), null );
+    result = r.get(new Get(secondRowBytes).addFamily(COLUMN_FAMILY_TEXT).setMaxVersions(100));
     assertTrue("Second row should still be deleted", result.isEmpty());
 
     // Make sure the store files do have some 'aaa' keys in them -- exactly 3.
@@ -406,22 +406,22 @@ public class TestCompaction extends HBaseTestCase {
       r.flushcache();
     }
 
-    Result result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100), null);
+    Result result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
-    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100), null);
+    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
 
     // Now add deletes to memstore and then flush it.  That will put us over
     // the compaction threshold of 3 store files.  Compacting these store files
     // should result in a compacted store file that has no references to the
     // deleted row.
-    r.delete(delete, null, true);
+    r.delete(delete, true);
 
     // Make sure that we have only deleted family2 from secondRowBytes
-    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100), null);
+    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100));
     assertEquals(expectedResultsAfterDelete, result.size());
     // but we still have firstrow
-    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100), null);
+    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
 
     r.flushcache();
@@ -429,10 +429,10 @@ public class TestCompaction extends HBaseTestCase {
     // Let us check again
 
     // Make sure that we have only deleted family2 from secondRowBytes
-    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100), null);
+    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100));
     assertEquals(expectedResultsAfterDelete, result.size());
     // but we still have firstrow
-    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100), null);
+    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
 
     // do a compaction
@@ -447,10 +447,10 @@ public class TestCompaction extends HBaseTestCase {
     assertTrue("Was not supposed to be a major compaction", numFiles2 > 1);
 
     // Make sure that we have only deleted family2 from secondRowBytes
-    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100), null);
+    result = r.get(new Get(secondRowBytes).addColumn(fam2, col2).setMaxVersions(100));
     assertEquals(expectedResultsAfterDelete, result.size());
     // but we still have firstrow
-    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100), null);
+    result = r.get(new Get(firstRowBytes).addColumn(fam1, col1).setMaxVersions(100));
     assertEquals(compactionThreshold, result.size());
   }
 
@@ -530,7 +530,7 @@ public class TestCompaction extends HBaseTestCase {
         Delete delete = new Delete(Bytes.add(STARTROW, Bytes.toBytes(i)));
         byte [][] famAndQf = {COLUMN_FAMILY, null};
         delete.deleteFamily(famAndQf[0]);
-        r.delete(delete, null, true);
+        r.delete(delete, true);
       }
       r.flushcache();
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
index 8643431..ff94902 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestGetClosestAtOrBefore.java
@@ -112,7 +112,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
     try {
       List<KeyValue> keys = new ArrayList<KeyValue>();
       while (s.next(keys)) {
-        mr.delete(new Delete(keys.get(0).getRow()), null, false);
+        mr.delete(new Delete(keys.get(0).getRow()), false);
         keys.clear();
       }
     } finally {
@@ -207,7 +207,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
 
       Delete d = new Delete(T20);
       d.deleteColumn(c0, c0);
-      region.delete(d, null, false);
+      region.delete(d, false);
 
       r = region.getClosestRowBefore(T20, c0);
       assertTrue(Bytes.equals(T10, r.getRow()));
@@ -221,7 +221,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
 
       d = new Delete(T30);
       d.deleteColumn(c0, c0);
-      region.delete(d, null, false);
+      region.delete(d, false);
 
       r = region.getClosestRowBefore(T30, c0);
       assertTrue(Bytes.equals(T10, r.getRow()));
@@ -257,7 +257,7 @@ public class TestGetClosestAtOrBefore extends HBaseTestCase {
       // in memory; make sure we get back t10 again.
       d = new Delete(T20);
       d.deleteColumn(c1, c1);
-      region.delete(d, null, false);
+      region.delete(d, false);
       r = region.getClosestRowBefore(T30, c0);
       assertTrue(Bytes.equals(T10, r.getRow()));
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHBase7051.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHBase7051.java
index df6c293..c88d758 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHBase7051.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHBase7051.java
@@ -115,7 +115,7 @@ public class TestHBase7051 {
           }
           count++;
           region.checkAndMutate(Bytes.toBytes("r1"), Bytes.toBytes(family), Bytes.toBytes("q1"),
-              CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("10")), put, null, true);
+              CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("10")), put, true);
           checkAndPutCompleted = true;
         } catch (IOException e) {
           // TODO Auto-generated catch block
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 2ba5490..4e24742 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -168,7 +168,7 @@ public class TestHRegion extends HBaseTestCase {
     RegionScanner scanner1 = region.getScanner(scan);
 
     Delete delete = new Delete(Bytes.toBytes("r1"));
-    region.delete(delete, null, false);
+    region.delete(delete, false);
     region.flushcache();
 
     // open the second scanner
@@ -273,7 +273,7 @@ public class TestHRegion extends HBaseTestCase {
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, status);
       assertEquals(maxSeqId, seqId);
       Get get = new Get(row);
-      Result result = region.get(get, null);
+      Result result = region.get(get);
       for (long i = minSeqId; i <= maxSeqId; i += 10) {
         List<KeyValue> kvs = result.getColumn(family, Bytes.toBytes(i));
         assertEquals(1, kvs.size());
@@ -326,7 +326,7 @@ public class TestHRegion extends HBaseTestCase {
       long seqId = region.replayRecoveredEditsIfAny(regiondir, maxSeqIdInStores, null, status);
       assertEquals(maxSeqId, seqId);
       Get get = new Get(row);
-      Result result = region.get(get, null);
+      Result result = region.get(get);
       for (long i = minSeqId; i <= maxSeqId; i += 10) {
         List<KeyValue> kvs = result.getColumn(family, Bytes.toBytes(i));
         if (i < recoverSeqId) {
@@ -460,7 +460,7 @@ public class TestHRegion extends HBaseTestCase {
     public void run() {
       while (!this.done.get()) {
         try {
-          assertTrue(region.get(g, null).size() > 0);
+          assertTrue(region.get(g).size() > 0);
           this.count.incrementAndGet();
         } catch (Exception e) {
           this.e = e;
@@ -531,7 +531,7 @@ public class TestHRegion extends HBaseTestCase {
         break;
       Delete delete = new Delete(results.get(0).getRow());
       delete.deleteColumn(Bytes.toBytes("trans-tags"), Bytes.toBytes("qual2"));
-      r.delete(delete, null, false);
+      r.delete(delete, false);
       results.clear();
     } while (more);
     assertEquals("Did not perform correct number of deletes", 3, count);
@@ -770,7 +770,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] emptyVal  = new byte[] {};
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -782,7 +781,7 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with empty value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, true);
       assertTrue(res);
 
       //Putting data in key
@@ -791,25 +790,25 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, true);
       assertTrue(res);
 
       // not empty anymore
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, true);
       assertFalse(res);
 
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, true);
       assertFalse(res);
 
       put = new Put(row1);
       put.add(fam1, qf1, val2);
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, true);
       assertTrue(res);
 
       //checkAndDelete with correct value
@@ -817,12 +816,12 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam1, qf1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, true);
       assertTrue(res);
 
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, true);
       assertTrue(res);
 
       //checkAndPut looking for a null value
@@ -830,7 +829,7 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam1, qf1, val1);
 
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new NullComparator(), put, lockId, true);
+          new NullComparator(), put, true);
       assertTrue(res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -845,7 +844,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -858,14 +856,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with wrong value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), put, lockId, true);
+          new BinaryComparator(val2), put, true);
       assertEquals(false, res);
 
       //checkAndDelete with wrong value
       Delete delete = new Delete(row1);
       delete.deleteFamily(fam1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, true);
       assertEquals(false, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -879,7 +877,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] fam1 = Bytes.toBytes("fam1");
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -892,14 +889,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, true);
       assertEquals(true, res);
 
       //checkAndDelete with correct value
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, true);
       assertEquals(true, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -915,7 +912,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     byte [][] families = {fam1, fam2};
 
@@ -939,13 +935,13 @@ public class TestHRegion extends HBaseTestCase {
       store.memstore.kvset.size();
 
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, true);
       assertEquals(true, res);
       store.memstore.kvset.size();
 
       Get get = new Get(row1);
       get.addColumn(fam2, qf1);
-      KeyValue [] actual = region.get(get, null).raw();
+      KeyValue [] actual = region.get(get).raw();
 
       KeyValue [] expected = {kv};
 
@@ -966,7 +962,7 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam1, qual1, value1);
       try {
         boolean res = region.checkAndMutate(row, fam1, qual1, CompareOp.EQUAL,
-            new BinaryComparator(value2), put, null, false);
+            new BinaryComparator(value2), put, false);
         fail();
       } catch (DoNotRetryIOException expected) {
         // expected exception.
@@ -989,7 +985,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] val2  = Bytes.toBytes("value2");
     byte [] val3  = Bytes.toBytes("value3");
     byte[] emptyVal = new byte[] { };
-    Integer lockId = null;
 
     byte [][] families = {fam1, fam2};
 
@@ -1017,14 +1012,14 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam2, qf1);
       delete.deleteColumn(fam1, qf3);
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, true);
       assertEquals(true, res);
 
       Get get = new Get(row1);
       get.addColumn(fam1, qf1);
       get.addColumn(fam1, qf3);
       get.addColumn(fam2, qf2);
-      Result r = region.get(get, null);
+      Result r = region.get(get);
       assertEquals(2, r.size());
       assertEquals(val1, r.getValue(fam1, qf1));
       assertEquals(val2, r.getValue(fam2, qf2));
@@ -1033,21 +1028,21 @@ public class TestHRegion extends HBaseTestCase {
       delete = new Delete(row1);
       delete.deleteFamily(fam2);
       res = region.checkAndMutate(row1, fam2, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, true);
       assertEquals(true, res);
 
       get = new Get(row1);
-      r = region.get(get, null);
+      r = region.get(get);
       assertEquals(1, r.size());
       assertEquals(val1, r.getValue(fam1, qf1));
 
       //Row delete
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), delete, lockId, true);
+          new BinaryComparator(val1), delete, true);
       assertEquals(true, res);
       get = new Get(row1);
-      r = region.get(get, null);
+      r = region.get(get);
       assertEquals(0, r.size());
     } finally {
       HRegion.closeHRegion(this.region);
@@ -1078,11 +1073,11 @@ public class TestHRegion extends HBaseTestCase {
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qual);
       delete.deleteColumn(fam1, qual);
-      region.delete(delete, null, false);
+      region.delete(delete, false);
 
       Get get = new Get(row1);
       get.addFamily(fam1);
-      Result r = region.get(get, null);
+      Result r = region.get(get);
       assertEquals(0, r.size());
     } finally {
       HRegion.closeHRegion(this.region);
@@ -1164,19 +1159,19 @@ public class TestHRegion extends HBaseTestCase {
       // ok now delete a split:
       Delete delete = new Delete(row);
       delete.deleteColumns(fam, splitA);
-      region.delete(delete, null, true);
+      region.delete(delete, true);
 
       // assert some things:
       Get get = new Get(row).addColumn(fam, serverinfo);
-      Result result = region.get(get, null);
+      Result result = region.get(get);
       assertEquals(1, result.size());
 
       get = new Get(row).addColumn(fam, splitA);
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(0, result.size());
 
       get = new Get(row).addColumn(fam, splitB);
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(1, result.size());
 
       // Assert that after a delete, I can put.
@@ -1184,16 +1179,16 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam, splitA, Bytes.toBytes("reference_A"));
       region.put(put);
       get = new Get(row);
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(3, result.size());
 
       // Now delete all... then test I can add stuff back
       delete = new Delete(row);
-      region.delete(delete, null, false);
-      assertEquals(0, region.get(get, null).size());
+      region.delete(delete, false);
+      assertEquals(0, region.get(get).size());
 
       region.put(new Put(row).add(fam, splitA, Bytes.toBytes("reference_A")));
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(1, result.size());
     } finally {
       HRegion.closeHRegion(this.region);
@@ -1219,20 +1214,20 @@ public class TestHRegion extends HBaseTestCase {
 
       // now delete something in the present
       Delete delete = new Delete(row);
-      region.delete(delete, null, true);
+      region.delete(delete, true);
 
       // make sure we still see our data
       Get get = new Get(row).addColumn(fam, serverinfo);
-      Result result = region.get(get, null);
+      Result result = region.get(get);
       assertEquals(1, result.size());
 
       // delete the future row
-      delete = new Delete(row,HConstants.LATEST_TIMESTAMP-3,null);
-      region.delete(delete, null, true);
+      delete = new Delete(row,HConstants.LATEST_TIMESTAMP-3);
+      region.delete(delete, true);
 
       // make sure it is gone
       get = new Get(row).addColumn(fam, serverinfo);
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(0, result.size());
     } finally {
       HRegion.closeHRegion(this.region);
@@ -1262,7 +1257,7 @@ public class TestHRegion extends HBaseTestCase {
 
       // Make sure it shows up with an actual timestamp
       Get get = new Get(row).addColumn(fam, qual);
-      Result result = region.get(get, null);
+      Result result = region.get(get);
       assertEquals(1, result.size());
       KeyValue kv = result.raw()[0];
       LOG.info("Got: " + kv);
@@ -1278,7 +1273,7 @@ public class TestHRegion extends HBaseTestCase {
 
       // Make sure it shows up with an actual timestamp
       get = new Get(row).addColumn(fam, qual);
-      result = region.get(get, null);
+      result = region.get(get);
       assertEquals(1, result.size());
       kv = result.raw()[0];
       LOG.info("Got: " + kv);
@@ -1343,7 +1338,7 @@ public class TestHRegion extends HBaseTestCase {
       Delete delete = new Delete(rowA);
       delete.deleteFamily(fam1);
 
-      region.delete(delete, null, true);
+      region.delete(delete, true);
 
       // now create data.
       Put put = new Put(rowA);
@@ -1394,7 +1389,7 @@ public class TestHRegion extends HBaseTestCase {
       region.put(put);
 
       // now delete the value:
-      region.delete(delete, null, true);
+      region.delete(delete, true);
 
 
       // ok put data:
@@ -1406,7 +1401,7 @@ public class TestHRegion extends HBaseTestCase {
       Get get = new Get(row);
       get.addColumn(fam1, qual1);
 
-      Result r = region.get(get, null);
+      Result r = region.get(get);
       assertEquals(1, r.size());
       assertByteEquals(value2, r.getValue(fam1, qual1));
 
@@ -1486,7 +1481,7 @@ public class TestHRegion extends HBaseTestCase {
 
       //Test
       try {
-        region.get(get, null);
+        region.get(get);
       } catch (DoNotRetryIOException e) {
         assertFalse(false);
         return;
@@ -1530,7 +1525,7 @@ public class TestHRegion extends HBaseTestCase {
       KeyValue [] expected = {kv1, kv2};
 
       //Test
-      Result res = region.get(get, null);
+      Result res = region.get(get);
       assertEquals(expected.length, res.size());
       for(int i=0; i<res.size(); i++){
         assertEquals(0,
@@ -1546,7 +1541,7 @@ public class TestHRegion extends HBaseTestCase {
       Get g = new Get(row1);
       final int count = 2;
       g.setFilter(new ColumnCountGetFilter(count));
-      res = region.get(g, null);
+      res = region.get(g);
       assertEquals(count, res.size());
     } finally {
       HRegion.closeHRegion(this.region);
@@ -1564,7 +1559,7 @@ public class TestHRegion extends HBaseTestCase {
     try {
       Get get = new Get(row);
       get.addFamily(fam);
-      Result r = region.get(get, null);
+      Result r = region.get(get);
 
       assertTrue(r.isEmpty());
     } finally {
@@ -1597,7 +1592,7 @@ public class TestHRegion extends HBaseTestCase {
       KeyValue [] expected = {kv1};
 
       //Test from memstore
-      Result res = region.get(get, null);
+      Result res = region.get(get);
 
       assertEquals(expected.length, res.size());
       for(int i=0; i<res.size(); i++){
@@ -1614,7 +1609,7 @@ public class TestHRegion extends HBaseTestCase {
       region.flushcache();
 
       //test2
-      res = region.get(get, null);
+      res = region.get(get);
 
       assertEquals(expected.length, res.size());
       for(int i=0; i<res.size(); i++){
@@ -1651,74 +1646,6 @@ public class TestHRegion extends HBaseTestCase {
   }
 
   //////////////////////////////////////////////////////////////////////////////
-  // Lock test
-  //////////////////////////////////////////////////////////////////////////////
-  public void testLocks() throws IOException{
-    byte [] tableName = Bytes.toBytes("testtable");
-    byte [][] families = {fam1, fam2, fam3};
-
-    Configuration hc = initSplit();
-    //Setting up region
-    String method = this.getName();
-    this.region = initHRegion(tableName, method, hc, families);
-    try {
-      final int threadCount = 10;
-      final int lockCount = 10;
-
-      List<Thread>threads = new ArrayList<Thread>(threadCount);
-      for (int i = 0; i < threadCount; i++) {
-        threads.add(new Thread(Integer.toString(i)) {
-          @Override
-          public void run() {
-            Integer [] lockids = new Integer[lockCount];
-            // Get locks.
-            for (int i = 0; i < lockCount; i++) {
-              try {
-                byte [] rowid = Bytes.toBytes(Integer.toString(i));
-                lockids[i] = region.obtainRowLock(rowid);
-                assertEquals(rowid, region.getRowFromLock(lockids[i]));
-                LOG.debug(getName() + " locked " + Bytes.toString(rowid));
-              } catch (IOException e) {
-                e.printStackTrace();
-              }
-            }
-            LOG.debug(getName() + " set " +
-                Integer.toString(lockCount) + " locks");
-
-            // Abort outstanding locks.
-            for (int i = lockCount - 1; i >= 0; i--) {
-              region.releaseRowLock(lockids[i]);
-              LOG.debug(getName() + " unlocked " + i);
-            }
-            LOG.debug(getName() + " released " +
-                Integer.toString(lockCount) + " locks");
-          }
-        });
-      }
-
-      // Startup all our threads.
-      for (Thread t : threads) {
-        t.start();
-      }
-
-      // Now wait around till all are done.
-      for (Thread t: threads) {
-        while (t.isAlive()) {
-          try {
-            Thread.sleep(1);
-          } catch (InterruptedException e) {
-            // Go around again.
-          }
-        }
-      }
-      LOG.info("locks completed.");
-    } finally {
-      HRegion.closeHRegion(this.region);
-      this.region = null;
-    }
-  }
-
-  //////////////////////////////////////////////////////////////////////////////
   // Merge test
   //////////////////////////////////////////////////////////////////////////////
   public void testMerge() throws IOException {
@@ -2389,7 +2316,7 @@ public class TestHRegion extends HBaseTestCase {
     // run a get and see?
     Get get = new Get(row);
     get.addColumn(familiy, qualifier);
-    Result result = region.get(get, null);
+    Result result = region.get(get);
     assertEquals(1, result.size());
 
     KeyValue kv = result.raw()[0];
@@ -2404,7 +2331,7 @@ public class TestHRegion extends HBaseTestCase {
     // run a get and see?
     Get get = new Get(row);
     get.addColumn(familiy, qualifier);
-    Result result = region.get(get, null);
+    Result result = region.get(get);
     assertEquals(1, result.size());
 
     KeyValue kv = result.raw()[0];
@@ -2890,8 +2817,8 @@ public class TestHRegion extends HBaseTestCase {
             numPutsFinished++;
             if (numPutsFinished > 0 && numPutsFinished % 47 == 0) {
               System.out.println("put iteration = " + numPutsFinished);
-              Delete delete = new Delete(row, (long)numPutsFinished-30, null);
-              region.delete(delete, null, true);
+              Delete delete = new Delete(row, (long)numPutsFinished-30);
+              region.delete(delete, true);
             }
             numPutsFinished++;
           }
@@ -2976,7 +2903,7 @@ public class TestHRegion extends HBaseTestCase {
       for (int i = 0; i < testCount; i++) {
 
         boolean previousEmpty = result == null || result.isEmpty();
-        result = region.get(get, null);
+        result = region.get(get);
         if (!result.isEmpty() || !previousEmpty || i > compactInterval) {
           assertEquals("i=" + i, expectedCount, result.size());
           // TODO this was removed, now what dangit?!
@@ -3035,14 +2962,14 @@ public class TestHRegion extends HBaseTestCase {
       byte[] rowNotServed = Bytes.toBytes("a");
       Get g = new Get(rowNotServed);
       try {
-        region.get(g, null);
+        region.get(g);
         fail();
       } catch (WrongRegionException x) {
         // OK
       }
       byte[] row = Bytes.toBytes("y");
       g = new Get(row);
-      region.get(g, null);
+      region.get(g);
     } finally {
       HRegion.closeHRegion(this.region);
       this.region = null;
@@ -3063,9 +2990,9 @@ public class TestHRegion extends HBaseTestCase {
 
       region.flushcache();
 
-      Delete delete = new Delete(Bytes.toBytes(1L), 1L, null);
+      Delete delete = new Delete(Bytes.toBytes(1L), 1L);
       //delete.deleteColumn(family, qual1);
-      region.delete(delete, null, true);
+      region.delete(delete, true);
 
       put = new Put(Bytes.toBytes(2L));
       put.add(family, qual1, 2L, Bytes.toBytes(2L));
@@ -3189,7 +3116,7 @@ public class TestHRegion extends HBaseTestCase {
       //Get rows
       Get get = new Get(row);
       get.setMaxVersions();
-      KeyValue[] kvs = region.get(get, null).raw();
+      KeyValue[] kvs = region.get(get).raw();
 
       //Check if rows are correct
       assertEquals(4, kvs.length);
@@ -3233,14 +3160,14 @@ public class TestHRegion extends HBaseTestCase {
       region.flushcache();
 
       Delete del = new Delete(row);
-      region.delete(del, null, true);
+      region.delete(del, true);
       region.flushcache();
 
       // Get remaining rows (should have none)
       Get get = new Get(row);
       get.addColumn(familyName, col);
 
-      KeyValue[] keyValues = region.get(get, null).raw();
+      KeyValue[] keyValues = region.get(get).raw();
       assertTrue(keyValues.length == 0);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -3371,7 +3298,7 @@ public class TestHRegion extends HBaseTestCase {
         inc.addColumn(family, qualifier, ONE);
         count++;
         try {
-          region.increment(inc, null, true);
+          region.increment(inc, true);
         } catch (IOException e) {
           e.printStackTrace();
           break;
@@ -3426,7 +3353,7 @@ public class TestHRegion extends HBaseTestCase {
     Get get = new Get(Incrementer.incRow);
     get.addColumn(Incrementer.family, Incrementer.qualifier);
     get.setMaxVersions(1);
-    Result res = this.region.get(get, null);
+    Result res = this.region.get(get);
     List<KeyValue> kvs = res.getColumn(Incrementer.family,
         Incrementer.qualifier);
     
@@ -3462,7 +3389,7 @@ public class TestHRegion extends HBaseTestCase {
         app.add(family, qualifier, CHAR);
         count++;
         try {
-          region.append(app, null, true);
+          region.append(app, true);
         } catch (IOException e) {
           e.printStackTrace();
           break;
@@ -3520,7 +3447,7 @@ public class TestHRegion extends HBaseTestCase {
     Get get = new Get(Appender.appendRow);
     get.addColumn(Appender.family, Appender.qualifier);
     get.setMaxVersions(1);
-    Result res = this.region.get(get, null);
+    Result res = this.region.get(get);
     List<KeyValue> kvs = res.getColumn(Appender.family,
         Appender.qualifier);
     
@@ -3557,7 +3484,7 @@ public class TestHRegion extends HBaseTestCase {
     get = new Get(row);
     get.addColumn(family, qualifier);
     get.setMaxVersions();
-    res = this.region.get(get, null);
+    res = this.region.get(get);
     kvs = res.getColumn(family, qualifier);
     assertEquals(1, kvs.size());
     assertEquals(Bytes.toBytes("value0"), kvs.get(0).getValue());
@@ -3566,7 +3493,7 @@ public class TestHRegion extends HBaseTestCase {
     get = new Get(row);
     get.addColumn(family, qualifier);
     get.setMaxVersions();
-    res = this.region.get(get, null);
+    res = this.region.get(get);
     kvs = res.getColumn(family, qualifier);
     assertEquals(1, kvs.size());
     assertEquals(Bytes.toBytes("value0"), kvs.get(0).getValue());
@@ -3578,7 +3505,7 @@ public class TestHRegion extends HBaseTestCase {
     get = new Get(row);
     get.addColumn(family, qualifier);
     get.setMaxVersions();
-    res = this.region.get(get, null);
+    res = this.region.get(get);
     kvs = res.getColumn(family, qualifier);
     assertEquals(1, kvs.size());
     assertEquals(Bytes.toBytes("value1"), kvs.get(0).getValue());
@@ -3587,7 +3514,7 @@ public class TestHRegion extends HBaseTestCase {
     get = new Get(row);
     get.addColumn(family, qualifier);
     get.setMaxVersions();
-    res = this.region.get(get, null);
+    res = this.region.get(get);
     kvs = res.getColumn(family, qualifier);
     assertEquals(1, kvs.size());
     assertEquals(Bytes.toBytes("value1"), kvs.get(0).getValue());
@@ -3615,7 +3542,7 @@ public class TestHRegion extends HBaseTestCase {
       for(byte [] family : families) {
         get.addColumn(family, qf);
       }
-      Result result = newReg.get(get, null);
+      Result result = newReg.get(get);
       KeyValue [] raw = result.raw();
       assertEquals(families.length, result.size());
       for(int j=0; j<families.length; j++) {
@@ -3630,7 +3557,7 @@ public class TestHRegion extends HBaseTestCase {
   throws IOException {
     // Now I have k, get values out and assert they are as expected.
     Get get = new Get(k).addFamily(family).setMaxVersions();
-    KeyValue [] results = r.get(get, null).raw();
+    KeyValue [] results = r.get(get).raw();
     for (int j = 0; j < results.length; j++) {
       byte [] tmp = results[j].getValue();
       // Row should be equal to value every time.
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionBusyWait.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionBusyWait.java
index 10a9370..966eca3 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionBusyWait.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegionBusyWait.java
@@ -70,7 +70,7 @@ public class TestHRegionBusyWait extends TestHRegion {
       while (stopped.get()) {
         Thread.sleep(100);
       }
-      region.get(get, null);
+      region.get(get);
       fail("Should throw RegionTooBusyException");
     } catch (InterruptedException ie) {
       fail("test interrupted");
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
index 6f2c498..260431f 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestKeepDeletes.java
@@ -70,8 +70,8 @@ public class TestKeepDeletes extends HBaseTestCase {
     region.put(p);
 
     // now place a delete marker at ts+2
-    Delete d = new Delete(T1, ts+2, null);
-    region.delete(d, null, true);
+    Delete d = new Delete(T1, ts+2);
+    region.delete(d, true);
 
     // a raw scan can see the delete markers
     // (one for each column family)
@@ -81,14 +81,14 @@ public class TestKeepDeletes extends HBaseTestCase {
     Get g = new Get(T1);
     g.setMaxVersions();
     g.setTimeRange(0L, ts+2);
-    Result r = region.get(g, null);
+    Result r = region.get(g);
     checkResult(r, c0, c0, T2,T1);
 
     // flush
     region.flushcache();
 
     // yep, T2 still there, T1 gone
-    r = region.get(g, null);
+    r = region.get(g);
     checkResult(r, c0, c0, T2);
 
     // major compact
@@ -100,12 +100,12 @@ public class TestKeepDeletes extends HBaseTestCase {
     assertEquals(1, countDeleteMarkers(region));
 
     // still there (even after multiple compactions)
-    r = region.get(g, null);
+    r = region.get(g);
     checkResult(r, c0, c0, T2);
 
     // a timerange that includes the delete marker won't see past rows
     g.setTimeRange(0L, ts+4);
-    r = region.get(g, null);
+    r = region.get(g);
     assertTrue(r.isEmpty());
 
     // two more puts, this will expire the older puts.
@@ -121,7 +121,7 @@ public class TestKeepDeletes extends HBaseTestCase {
     p = new Put(T1, ts);
     p.add(c0, c0, T1);
     region.put(p);
-    r = region.get(g, null);
+    r = region.get(g);
     assertTrue(r.isEmpty());
 
     region.flushcache();
@@ -130,7 +130,7 @@ public class TestKeepDeletes extends HBaseTestCase {
 
     // verify that the delete marker itself was collected
     region.put(p);
-    r = region.get(g, null);
+    r = region.get(g);
     checkResult(r, c0, c0, T1);
     assertEquals(0, countDeleteMarkers(region));
 
@@ -156,9 +156,9 @@ public class TestKeepDeletes extends HBaseTestCase {
     p.add(c0, c0, T1);
     region.put(p);
 
-    Delete d = new Delete(T1, ts, null);
+    Delete d = new Delete(T1, ts);
     d.deleteColumn(c0, c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // scan still returns delete markers and deletes rows
     Scan s = new Scan();
@@ -199,15 +199,15 @@ public class TestKeepDeletes extends HBaseTestCase {
     Put p = new Put(T1, ts);
     p.add(c0, c0, T1);
     region.put(p);
-    Delete d = new Delete(T1, ts+2, null);
+    Delete d = new Delete(T1, ts+2);
     d.deleteColumn(c0, c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // "past" get does not see rows behind delete marker
     Get g = new Get(T1);
     g.setMaxVersions();
     g.setTimeRange(0L, ts+1);
-    Result r = region.get(g, null);
+    Result r = region.get(g);
     assertTrue(r.isEmpty());
 
     // "past" scan does not see rows behind delete marker
@@ -272,16 +272,16 @@ public class TestKeepDeletes extends HBaseTestCase {
     p.add(c0, c0, T3);
     region.put(p);
 
-    Delete d = new Delete(T1, ts+1, null);
-    region.delete(d, null, true);
+    Delete d = new Delete(T1, ts+1);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts+2, null);
+    d = new Delete(T1, ts+2);
     d.deleteColumn(c0, c0, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts+3, null);
+    d = new Delete(T1, ts+3);
     d.deleteColumns(c0, c0, ts+3);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     Scan s = new Scan();
     s.setRaw(true);
@@ -309,21 +309,21 @@ public class TestKeepDeletes extends HBaseTestCase {
 
     long ts = System.currentTimeMillis();
 
-    Delete d = new Delete(T1, ts, null);
+    Delete d = new Delete(T1, ts);
     d.deleteColumns(c0, c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteFamily(c0);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteColumn(c0, c0, ts+1);
-    region.delete(d, null, true);
+    region.delete(d, true);
     
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteColumn(c0, c0, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // 1 family marker, 1 column marker, 2 version markers
     assertEquals(4, countDeleteMarkers(region));
@@ -361,21 +361,21 @@ public class TestKeepDeletes extends HBaseTestCase {
     region.put(p);
 
     // all the following deletes affect the put
-    Delete d = new Delete(T1, ts, null);
+    Delete d = new Delete(T1, ts);
     d.deleteColumns(c0, c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteFamily(c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteColumn(c0, c0, ts+1);
-    region.delete(d, null, true);
+    region.delete(d, true);
     
-    d = new Delete(T1, ts, null);
+    d = new Delete(T1, ts);
     d.deleteColumn(c0, c0, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // 1 family marker, 1 column marker, 2 version markers
     assertEquals(4, countDeleteMarkers(region));
@@ -440,22 +440,22 @@ public class TestKeepDeletes extends HBaseTestCase {
     p.add(c1, c1, T2);
     region.put(p);
 
-    Delete d = new Delete(T1, ts+2, null);
+    Delete d = new Delete(T1, ts+2);
     d.deleteColumns(c0, c0, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts+2, null);
+    d = new Delete(T1, ts+2);
     d.deleteFamily(c1, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T2, ts+2, null);
+    d = new Delete(T2, ts+2);
     d.deleteFamily(c0, ts+2);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // add an older delete, to make sure it is filtered
-    d = new Delete(T1, ts-10, null);
+    d = new Delete(T1, ts-10);
     d.deleteFamily(c1, ts-10);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     // ts + 2 does NOT include the delete at ts+2
     checkGet(region, T1, c0, c0, ts+2, T2, T1);
@@ -503,18 +503,18 @@ public class TestKeepDeletes extends HBaseTestCase {
     p.add(c0, c1, T1);
     region.put(p);
     
-    Delete d = new Delete(T1, ts, null);
+    Delete d = new Delete(T1, ts);
     // test corner case (Put and Delete have same TS)
     d.deleteColumns(c0, c0, ts);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
-    d = new Delete(T1, ts+1, null);
+    d = new Delete(T1, ts+1);
     d.deleteColumn(c0, c0, ts+1);
-    region.delete(d, null, true);
+    region.delete(d, true);
     
-    d = new Delete(T1, ts+3, null);
+    d = new Delete(T1, ts+3);
     d.deleteColumn(c0, c0, ts+3);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     region.flushcache();
     region.compactStores(true);
@@ -601,11 +601,11 @@ public class TestKeepDeletes extends HBaseTestCase {
     region.put(p);
 
     // family markers are each family
-    Delete d = new Delete(T1, ts+1, null);
-    region.delete(d, null, true);
+    Delete d = new Delete(T1, ts+1);
+    region.delete(d, true);
 
-    d = new Delete(T2, ts+2, null);
-    region.delete(d, null, true);
+    d = new Delete(T2, ts+2);
+    region.delete(d, true);
 
     Scan s = new Scan(T1);
     s.setTimeRange(0, ts+1);
@@ -652,17 +652,17 @@ public class TestKeepDeletes extends HBaseTestCase {
     // all puts now are just retained because of min versions = 3
 
     // place a family delete marker
-    Delete d = new Delete(T1, ts-1, null);
-    region.delete(d, null, true);
+    Delete d = new Delete(T1, ts-1);
+    region.delete(d, true);
     // and a column delete marker
-    d = new Delete(T1, ts-2, null);
+    d = new Delete(T1, ts-2);
     d.deleteColumns(c0, c0, ts-1);
-    region.delete(d, null, true);
+    region.delete(d, true);
 
     Get g = new Get(T1);
     g.setMaxVersions();
     g.setTimeRange(0L, ts-2);
-    Result r = region.get(g, null);
+    Result r = region.get(g);
     checkResult(r, c0, c0, T1,T0);
 
     // 3 families, one column delete marker
@@ -672,7 +672,7 @@ public class TestKeepDeletes extends HBaseTestCase {
     // no delete marker removes by the flush
     assertEquals(4, countDeleteMarkers(region));
 
-    r = region.get(g, null);
+    r = region.get(g);
     checkResult(r, c0, c0, T1);
     p = new Put(T1, ts+1);
     p.add(c0, c0, T4);
@@ -681,7 +681,7 @@ public class TestKeepDeletes extends HBaseTestCase {
 
     assertEquals(4, countDeleteMarkers(region));
 
-    r = region.get(g, null);
+    r = region.get(g);
     checkResult(r, c0, c0, T1);
 
     // this will push out the last put before
@@ -709,7 +709,7 @@ public class TestKeepDeletes extends HBaseTestCase {
     g.addColumn(fam, col);
     g.setMaxVersions();
     g.setTimeRange(0L, time);
-    Result r = region.get(g, null);
+    Result r = region.get(g);
     checkResult(r, fam, col, vals);
 
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
index de655ff..788e0de 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMinVersions.java
@@ -133,13 +133,13 @@ public class TestMinVersions extends HBaseTestCase {
 
       Get g = new Get(T1);
       g.setMaxVersions();
-      Result r = region.get(g, null); // this'll use ScanWildcardColumnTracker
+      Result r = region.get(g); // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T3,T2,T1);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T3,T2,T1);
     } finally {
       HRegion.closeHRegion(region);
@@ -169,18 +169,18 @@ public class TestMinVersions extends HBaseTestCase {
       p.add(c0, c0, T3);
       region.put(p);
 
-      Delete d = new Delete(T1, ts-1, null);
-      region.delete(d, null, true);
+      Delete d = new Delete(T1, ts-1);
+      region.delete(d, true);
 
       Get g = new Get(T1);
       g.setMaxVersions();
-      Result r = region.get(g, null);  // this'll use ScanWildcardColumnTracker
+      Result r = region.get(g);  // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T3);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T3);
 
       // now flush/compact
@@ -190,13 +190,13 @@ public class TestMinVersions extends HBaseTestCase {
       // try again
       g = new Get(T1);
       g.setMaxVersions();
-      r = region.get(g, null);  // this'll use ScanWildcardColumnTracker
+      r = region.get(g);  // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T3);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T3);
     } finally {
       HRegion.closeHRegion(region);
@@ -241,18 +241,18 @@ public class TestMinVersions extends HBaseTestCase {
       // now the latest change is in the memstore,
       // but it is not the latest version
 
-      Result r = region.get(new Get(T1), null);
+      Result r = region.get(new Get(T1));
       checkResult(r, c0, T4);
 
       Get g = new Get(T1);
       g.setMaxVersions();
-      r = region.get(g, null); // this'll use ScanWildcardColumnTracker
+      r = region.get(g); // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T4,T3);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T4,T3);
 
       p = new Put(T1, ts+1);
@@ -263,13 +263,13 @@ public class TestMinVersions extends HBaseTestCase {
 
       g = new Get(T1);
       g.setMaxVersions();
-      r = region.get(g, null);  // this'll use ScanWildcardColumnTracker
+      r = region.get(g);  // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T5,T4);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T5,T4);
     } finally {
       HRegion.closeHRegion(region);
@@ -308,30 +308,30 @@ public class TestMinVersions extends HBaseTestCase {
       p.add(c0, c0, T4);
       region.put(p);
 
-      Result r = region.get(new Get(T1), null);
+      Result r = region.get(new Get(T1));
       checkResult(r, c0, T4);
 
       Get g = new Get(T1);
       g.setTimeRange(0L, ts+1);
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T4);
 
   // oldest version still exists
       g.setTimeRange(0L, ts-2);
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T1);
 
       // gets see only available versions
       // even before compactions
       g = new Get(T1);
       g.setMaxVersions();
-      r = region.get(g, null); // this'll use ScanWildcardColumnTracker
+      r = region.get(g); // this'll use ScanWildcardColumnTracker
       checkResult(r, c0, T4,T3);
 
       g = new Get(T1);
       g.setMaxVersions();
       g.addColumn(c0, c0);
-      r = region.get(g, null);  // this'll use ExplicitColumnTracker
+      r = region.get(g);  // this'll use ExplicitColumnTracker
       checkResult(r, c0, T4,T3);
 
       // now flush
@@ -340,7 +340,7 @@ public class TestMinVersions extends HBaseTestCase {
       // with HBASE-4241 a flush will eliminate the expired rows
       g = new Get(T1);
       g.setTimeRange(0L, ts-2);
-      r = region.get(g, null);
+      r = region.get(g);
       assertTrue(r.isEmpty());
 
       // major compaction
@@ -349,17 +349,17 @@ public class TestMinVersions extends HBaseTestCase {
       // after compaction the 4th version is still available
       g = new Get(T1);
       g.setTimeRange(0L, ts+1);
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T4);
 
       // so is the 3rd
       g.setTimeRange(0L, ts);
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T3);
 
       // but the 2nd and earlier versions are gone
       g.setTimeRange(0L, ts-1);
-      r = region.get(g, null);
+      r = region.get(g);
       assertTrue(r.isEmpty());
     } finally {
       HRegion.closeHRegion(region);
@@ -407,14 +407,14 @@ public class TestMinVersions extends HBaseTestCase {
       g.addColumn(c1,c1);
       g.setFilter(new TimestampsFilter(tss));
       g.setMaxVersions();
-      Result r = region.get(g, null);
+      Result r = region.get(g);
       checkResult(r, c1, T2,T1);
 
       g = new Get(T1);
       g.addColumn(c0,c0);
       g.setFilter(new TimestampsFilter(tss));
       g.setMaxVersions();
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T2,T1);
 
       // now flush/compact
@@ -425,14 +425,14 @@ public class TestMinVersions extends HBaseTestCase {
       g.addColumn(c1,c1);
       g.setFilter(new TimestampsFilter(tss));
       g.setMaxVersions();
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c1, T2);
 
       g = new Get(T1);
       g.addColumn(c0,c0);
       g.setFilter(new TimestampsFilter(tss));
       g.setMaxVersions();
-      r = region.get(g, null);
+      r = region.get(g);
       checkResult(r, c0, T2);
     } finally {
       HRegion.closeHRegion(region);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
index 12b86f2..850581c 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestMultiColumnScanner.java
@@ -214,7 +214,7 @@ public class TestMultiColumnScanner {
               deletedSomething = true;
             }
           if (deletedSomething)
-            region.delete(d, null, true);
+            region.delete(d, true);
         }
       }
       region.flushcache();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
index 2ad1053..f4e7c30 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
@@ -167,7 +167,7 @@ public class TestParallelPut extends HBaseTestCase {
     // run a get and see if the value matches
     Get get = new Get(row);
     get.addColumn(familiy, qualifier);
-    Result result = region.get(get, null);
+    Result result = region.get(get);
     assertEquals(1, result.size());
 
     KeyValue kv = result.raw()[0];
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
index 6888a01..89b4a87 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
@@ -78,14 +78,14 @@ public class TestResettingCounters {
       }
 
       // increment odd qualifiers 5 times and flush
-      for (int i=0;i<5;i++) region.increment(odd, null, false);
+      for (int i=0;i<5;i++) region.increment(odd, false);
       region.flushcache();
 
       // increment even qualifiers 5 times
-      for (int i=0;i<5;i++) region.increment(even, null, false);
+      for (int i=0;i<5;i++) region.increment(even, false);
 
       // increment all qualifiers, should have value=6 for all
-      Result result = region.increment(all, null, false);
+      Result result = region.increment(all, false);
       assertEquals(numQualifiers, result.size());
       KeyValue [] kvs = result.raw();
       for (int i=0;i<kvs.length;i++) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
index 322b747..c50cecc 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestScanner.java
@@ -234,7 +234,7 @@ public class TestScanner extends HBaseTestCase {
 
       // Write information to the meta table
 
-      Put put = new Put(ROW_KEY, System.currentTimeMillis(), null);
+      Put put = new Put(ROW_KEY, System.currentTimeMillis());
 
       put.add(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER,
           REGION_INFO.toByteArray());
@@ -261,7 +261,7 @@ public class TestScanner extends HBaseTestCase {
 
       String address = HConstants.LOCALHOST_IP + ":" + HBaseTestingUtility.randomFreePort();
 
-      put = new Put(ROW_KEY, System.currentTimeMillis(), null);
+      put = new Put(ROW_KEY, System.currentTimeMillis());
       put.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
           Bytes.toBytes(address));
 
@@ -299,7 +299,7 @@ public class TestScanner extends HBaseTestCase {
 
       address = "bar.foo.com:4321";
 
-      put = new Put(ROW_KEY, System.currentTimeMillis(), null);
+      put = new Put(ROW_KEY, System.currentTimeMillis());
 
       put.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
           Bytes.toBytes(address));
@@ -428,7 +428,7 @@ public class TestScanner extends HBaseTestCase {
   private void getRegionInfo() throws IOException {
     Get get = new Get(ROW_KEY);
     get.addColumn(HConstants.CATALOG_FAMILY, HConstants.REGIONINFO_QUALIFIER);
-    Result result = region.get(get, null);
+    Result result = region.get(get);
     byte [] bytes = result.value();
     validateRegionInfo(bytes);
   }
@@ -498,7 +498,7 @@ public class TestScanner extends HBaseTestCase {
       Delete dc = new Delete(firstRowBytes);
       /* delete column1 of firstRow */
       dc.deleteColumns(fam1, col1);
-      r.delete(dc, null, true);
+      r.delete(dc, true);
       r.flushcache();
 
       addContent(hri, Bytes.toString(fam1), Bytes.toString(col1),
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
index d9b8835..bc085dc 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestSeekOptimizations.java
@@ -421,7 +421,7 @@ public class TestSeekOptimizations {
 
         region.put(put);
         if (!del.isEmpty()) {
-          region.delete(del, null, true);
+          region.delete(del, true);
         }
 
         // Add remaining timestamps (those we have not deleted) to expected
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
index 2adbf04..8ff9e0d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
@@ -380,7 +380,7 @@ public class TestWALReplay {
     }
     // Now assert edits made it in.
     final Get g = new Get(rowName);
-    Result result = region.get(g, null);
+    Result result = region.get(g);
     assertEquals(countPerFamily * htd.getFamilies().size(),
       result.size());
     // Now close the region (without flush), split the log, reopen the region and assert that
@@ -395,7 +395,7 @@ public class TestWALReplay {
     // HRegionServer usually does this. It knows the largest seqid across all regions.
     wal2.setSequenceNumber(seqid2);
     assertTrue(seqid + result.size() < seqid2);
-    final Result result1b = region2.get(g, null);
+    final Result result1b = region2.get(g);
     assertEquals(result.size(), result1b.size());
 
     // Next test.  Add more edits, then 'crash' this region by stealing its wal
@@ -405,7 +405,7 @@ public class TestWALReplay {
       addRegionEdits(rowName, hcd.getName(), countPerFamily, this.ee, region2, "y");
     }
     // Get count of edits.
-    final Result result2 = region2.get(g, null);
+    final Result result2 = region2.get(g);
     assertEquals(2 * result.size(), result2.size());
     wal2.sync();
     // Set down maximum recovery so we dfsclient doesn't linger retrying something
@@ -432,7 +432,7 @@ public class TestWALReplay {
         long seqid3 = region3.initialize();
         // HRegionServer usually does this. It knows the largest seqid across all regions.
         wal3.setSequenceNumber(seqid3);
-        Result result3 = region3.get(g, null);
+        Result result3 = region3.get(g);
         // Assert that count of cells is same as before crash.
         assertEquals(result2.size(), result3.size());
         assertEquals(htd.getFamilies().size() * countPerFamily,
@@ -492,7 +492,7 @@ public class TestWALReplay {
 
     // Now assert edits made it in.
     final Get g = new Get(rowName);
-    Result result = region.get(g, null);
+    Result result = region.get(g);
     assertEquals(countPerFamily * htd.getFamilies().size(),
       result.size());
 
@@ -524,7 +524,7 @@ public class TestWALReplay {
     wal2.setSequenceNumber(seqid2);
     assertTrue(seqid + result.size() < seqid2);
 
-    final Result result1b = region2.get(g, null);
+    final Result result1b = region2.get(g);
     assertEquals(result.size(), result1b.size());
   }
 
@@ -612,7 +612,7 @@ public class TestWALReplay {
           assertTrue(seqid > wal.getSequenceNumber());
 
           Get get = new Get(rowName);
-          Result result = region.get(get, -1);
+          Result result = region.get(get);
           // Make sure we only see the good edits
           assertEquals(countPerFamily * (htd.getFamilies().size() - 1),
             result.size());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
index c92e371..21eb94e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/security/access/TestAccessController.java
@@ -35,7 +35,6 @@ import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.LargeTests;
 import org.apache.hadoop.hbase.ServerName;
-import org.apache.hadoop.hbase.UnknownRowLockException;
 import org.apache.hadoop.hbase.client.Append;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Get;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
index 6dc80be..8a44771 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
@@ -233,7 +233,7 @@ public class TestMergeTool extends HBaseTestCase {
       for (int j = 0; j < rows[i].length; j++) {
         Get get = new Get(rows[i][j]);
         get.addFamily(FAMILY);
-        Result result = merged.get(get, null);
+        Result result = merged.get(get);
         assertEquals(1, result.size());
         byte [] bytes = result.raw()[0].getValue();
         assertNotNull(Bytes.toStringBinary(rows[i][j]), bytes);
@@ -253,7 +253,7 @@ public class TestMergeTool extends HBaseTestCase {
       for (int j = 0; j < rows[i].length; j++) {
         Get get = new Get(rows[i][j]);
         get.addFamily(FAMILY);
-        Result result = regions[i].get(get, null);
+        Result result = regions[i].get(get);
         byte [] bytes = result.raw()[0].getValue();
         assertNotNull(bytes);
         assertTrue(Bytes.equals(bytes, rows[i][j]));
