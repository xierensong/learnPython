diff --git hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
index 05a43f6..ee3a153 100644
--- hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
+++ hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/ClientProtos.java
@@ -528,33 +528,29 @@ public final class ClientProtos {
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameBytesPairOrBuilder getAttributeOrBuilder(
         int index);
     
-    // optional uint64 lockId = 4;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional .Filter filter = 5;
+    // optional .Filter filter = 4;
     boolean hasFilter();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.FilterOrBuilder getFilterOrBuilder();
     
-    // optional .TimeRange timeRange = 6;
+    // optional .TimeRange timeRange = 5;
     boolean hasTimeRange();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange();
     org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder getTimeRangeOrBuilder();
     
-    // optional uint32 maxVersions = 7 [default = 1];
+    // optional uint32 maxVersions = 6 [default = 1];
     boolean hasMaxVersions();
     int getMaxVersions();
     
-    // optional bool cacheBlocks = 8 [default = true];
+    // optional bool cacheBlocks = 7 [default = true];
     boolean hasCacheBlocks();
     boolean getCacheBlocks();
     
-    // optional uint32 storeLimit = 9;
+    // optional uint32 storeLimit = 8;
     boolean hasStoreLimit();
     int getStoreLimit();
     
-    // optional uint32 storeOffset = 10;
+    // optional uint32 storeOffset = 9;
     boolean hasStoreOffset();
     int getStoreOffset();
   }
@@ -639,21 +635,11 @@ public final class ClientProtos {
       return attribute_.get(index);
     }
     
-    // optional uint64 lockId = 4;
-    public static final int LOCKID_FIELD_NUMBER = 4;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional .Filter filter = 5;
-    public static final int FILTER_FIELD_NUMBER = 5;
+    // optional .Filter filter = 4;
+    public static final int FILTER_FIELD_NUMBER = 4;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter filter_;
     public boolean hasFilter() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
+      return ((bitField0_ & 0x00000002) == 0x00000002);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter() {
       return filter_;
@@ -662,11 +648,11 @@ public final class ClientProtos {
       return filter_;
     }
     
-    // optional .TimeRange timeRange = 6;
-    public static final int TIMERANGE_FIELD_NUMBER = 6;
+    // optional .TimeRange timeRange = 5;
+    public static final int TIMERANGE_FIELD_NUMBER = 5;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_;
     public boolean hasTimeRange() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
+      return ((bitField0_ & 0x00000004) == 0x00000004);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
       return timeRange_;
@@ -675,41 +661,41 @@ public final class ClientProtos {
       return timeRange_;
     }
     
-    // optional uint32 maxVersions = 7 [default = 1];
-    public static final int MAXVERSIONS_FIELD_NUMBER = 7;
+    // optional uint32 maxVersions = 6 [default = 1];
+    public static final int MAXVERSIONS_FIELD_NUMBER = 6;
     private int maxVersions_;
     public boolean hasMaxVersions() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000008) == 0x00000008);
     }
     public int getMaxVersions() {
       return maxVersions_;
     }
     
-    // optional bool cacheBlocks = 8 [default = true];
-    public static final int CACHEBLOCKS_FIELD_NUMBER = 8;
+    // optional bool cacheBlocks = 7 [default = true];
+    public static final int CACHEBLOCKS_FIELD_NUMBER = 7;
     private boolean cacheBlocks_;
     public boolean hasCacheBlocks() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000010) == 0x00000010);
     }
     public boolean getCacheBlocks() {
       return cacheBlocks_;
     }
     
-    // optional uint32 storeLimit = 9;
-    public static final int STORELIMIT_FIELD_NUMBER = 9;
+    // optional uint32 storeLimit = 8;
+    public static final int STORELIMIT_FIELD_NUMBER = 8;
     private int storeLimit_;
     public boolean hasStoreLimit() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
+      return ((bitField0_ & 0x00000020) == 0x00000020);
     }
     public int getStoreLimit() {
       return storeLimit_;
     }
     
-    // optional uint32 storeOffset = 10;
-    public static final int STOREOFFSET_FIELD_NUMBER = 10;
+    // optional uint32 storeOffset = 9;
+    public static final int STOREOFFSET_FIELD_NUMBER = 9;
     private int storeOffset_;
     public boolean hasStoreOffset() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
+      return ((bitField0_ & 0x00000040) == 0x00000040);
     }
     public int getStoreOffset() {
       return storeOffset_;
@@ -719,7 +705,6 @@ public final class ClientProtos {
       row_ = com.google.protobuf.ByteString.EMPTY;
       column_ = java.util.Collections.emptyList();
       attribute_ = java.util.Collections.emptyList();
-      lockId_ = 0L;
       filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
       timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
       maxVersions_ = 1;
@@ -771,25 +756,22 @@ public final class ClientProtos {
         output.writeMessage(3, attribute_.get(i));
       }
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(4, lockId_);
+        output.writeMessage(4, filter_);
       }
       if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeMessage(5, filter_);
+        output.writeMessage(5, timeRange_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeMessage(6, timeRange_);
+        output.writeUInt32(6, maxVersions_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeUInt32(7, maxVersions_);
+        output.writeBool(7, cacheBlocks_);
       }
       if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBool(8, cacheBlocks_);
+        output.writeUInt32(8, storeLimit_);
       }
       if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        output.writeUInt32(9, storeLimit_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        output.writeUInt32(10, storeOffset_);
+        output.writeUInt32(9, storeOffset_);
       }
       getUnknownFields().writeTo(output);
     }
@@ -814,31 +796,27 @@ public final class ClientProtos {
       }
       if (((bitField0_ & 0x00000002) == 0x00000002)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(4, lockId_);
+          .computeMessageSize(4, filter_);
       }
       if (((bitField0_ & 0x00000004) == 0x00000004)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(5, filter_);
+          .computeMessageSize(5, timeRange_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(6, timeRange_);
+          .computeUInt32Size(6, maxVersions_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(7, maxVersions_);
+          .computeBoolSize(7, cacheBlocks_);
       }
       if (((bitField0_ & 0x00000020) == 0x00000020)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeBoolSize(8, cacheBlocks_);
+          .computeUInt32Size(8, storeLimit_);
       }
       if (((bitField0_ & 0x00000040) == 0x00000040)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(9, storeLimit_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(10, storeOffset_);
+          .computeUInt32Size(9, storeOffset_);
       }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
@@ -872,11 +850,6 @@ public final class ClientProtos {
           .equals(other.getColumnList());
       result = result && getAttributeList()
           .equals(other.getAttributeList());
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
       result = result && (hasFilter() == other.hasFilter());
       if (hasFilter()) {
         result = result && getFilter()
@@ -928,10 +901,6 @@ public final class ClientProtos {
         hash = (37 * hash) + ATTRIBUTE_FIELD_NUMBER;
         hash = (53 * hash) + getAttributeList().hashCode();
       }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
       if (hasFilter()) {
         hash = (37 * hash) + FILTER_FIELD_NUMBER;
         hash = (53 * hash) + getFilter().hashCode();
@@ -1090,28 +1059,26 @@ public final class ClientProtos {
         } else {
           attributeBuilder_.clear();
         }
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
         if (filterBuilder_ == null) {
           filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
         } else {
           filterBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000010);
+        bitField0_ = (bitField0_ & ~0x00000008);
         if (timeRangeBuilder_ == null) {
           timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000020);
+        bitField0_ = (bitField0_ & ~0x00000010);
         maxVersions_ = 1;
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         cacheBlocks_ = true;
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         storeLimit_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000100);
+        bitField0_ = (bitField0_ & ~0x00000080);
         storeOffset_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000200);
+        bitField0_ = (bitField0_ & ~0x00000100);
         return this;
       }
       
@@ -1175,38 +1142,34 @@ public final class ClientProtos {
         if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
           to_bitField0_ |= 0x00000002;
         }
-        result.lockId_ = lockId_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000004;
-        }
         if (filterBuilder_ == null) {
           result.filter_ = filter_;
         } else {
           result.filter_ = filterBuilder_.build();
         }
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000008;
+        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
+          to_bitField0_ |= 0x00000004;
         }
         if (timeRangeBuilder_ == null) {
           result.timeRange_ = timeRange_;
         } else {
           result.timeRange_ = timeRangeBuilder_.build();
         }
+        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
+          to_bitField0_ |= 0x00000008;
+        }
+        result.maxVersions_ = maxVersions_;
         if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
           to_bitField0_ |= 0x00000010;
         }
-        result.maxVersions_ = maxVersions_;
+        result.cacheBlocks_ = cacheBlocks_;
         if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
           to_bitField0_ |= 0x00000020;
         }
-        result.cacheBlocks_ = cacheBlocks_;
+        result.storeLimit_ = storeLimit_;
         if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
           to_bitField0_ |= 0x00000040;
         }
-        result.storeLimit_ = storeLimit_;
-        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
-          to_bitField0_ |= 0x00000080;
-        }
         result.storeOffset_ = storeOffset_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
@@ -1279,9 +1242,6 @@ public final class ClientProtos {
             }
           }
         }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
         if (other.hasFilter()) {
           mergeFilter(other.getFilter());
         }
@@ -1370,12 +1330,7 @@ public final class ClientProtos {
               addAttribute(subBuilder.buildPartial());
               break;
             }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 42: {
+            case 34: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.newBuilder();
               if (hasFilter()) {
                 subBuilder.mergeFrom(getFilter());
@@ -1384,7 +1339,7 @@ public final class ClientProtos {
               setFilter(subBuilder.buildPartial());
               break;
             }
-            case 50: {
+            case 42: {
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder();
               if (hasTimeRange()) {
                 subBuilder.mergeFrom(getTimeRange());
@@ -1393,23 +1348,23 @@ public final class ClientProtos {
               setTimeRange(subBuilder.buildPartial());
               break;
             }
+            case 48: {
+              bitField0_ |= 0x00000020;
+              maxVersions_ = input.readUInt32();
+              break;
+            }
             case 56: {
               bitField0_ |= 0x00000040;
-              maxVersions_ = input.readUInt32();
+              cacheBlocks_ = input.readBool();
               break;
             }
             case 64: {
               bitField0_ |= 0x00000080;
-              cacheBlocks_ = input.readBool();
+              storeLimit_ = input.readUInt32();
               break;
             }
             case 72: {
               bitField0_ |= 0x00000100;
-              storeLimit_ = input.readUInt32();
-              break;
-            }
-            case 80: {
-              bitField0_ |= 0x00000200;
               storeOffset_ = input.readUInt32();
               break;
             }
@@ -1815,33 +1770,12 @@ public final class ClientProtos {
         return attributeBuilder_;
       }
       
-      // optional uint64 lockId = 4;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000008;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional .Filter filter = 5;
+      // optional .Filter filter = 4;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter filter_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.FilterOrBuilder> filterBuilder_;
       public boolean hasFilter() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
+        return ((bitField0_ & 0x00000008) == 0x00000008);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter getFilter() {
         if (filterBuilder_ == null) {
@@ -1860,7 +1794,7 @@ public final class ClientProtos {
         } else {
           filterBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder setFilter(
@@ -1871,12 +1805,12 @@ public final class ClientProtos {
         } else {
           filterBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder mergeFilter(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter value) {
         if (filterBuilder_ == null) {
-          if (((bitField0_ & 0x00000010) == 0x00000010) &&
+          if (((bitField0_ & 0x00000008) == 0x00000008) &&
               filter_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.getDefaultInstance()) {
             filter_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.newBuilder(filter_).mergeFrom(value).buildPartial();
@@ -1887,7 +1821,7 @@ public final class ClientProtos {
         } else {
           filterBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         return this;
       }
       public Builder clearFilter() {
@@ -1897,11 +1831,11 @@ public final class ClientProtos {
         } else {
           filterBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000010);
+        bitField0_ = (bitField0_ & ~0x00000008);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Filter.Builder getFilterBuilder() {
-        bitField0_ |= 0x00000010;
+        bitField0_ |= 0x00000008;
         onChanged();
         return getFilterFieldBuilder().getBuilder();
       }
@@ -1926,12 +1860,12 @@ public final class ClientProtos {
         return filterBuilder_;
       }
       
-      // optional .TimeRange timeRange = 6;
+      // optional .TimeRange timeRange = 5;
       private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder> timeRangeBuilder_;
       public boolean hasTimeRange() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
+        return ((bitField0_ & 0x00000010) == 0x00000010);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
         if (timeRangeBuilder_ == null) {
@@ -1950,7 +1884,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder setTimeRange(
@@ -1961,12 +1895,12 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder mergeTimeRange(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange value) {
         if (timeRangeBuilder_ == null) {
-          if (((bitField0_ & 0x00000020) == 0x00000020) &&
+          if (((bitField0_ & 0x00000010) == 0x00000010) &&
               timeRange_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance()) {
             timeRange_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder(timeRange_).mergeFrom(value).buildPartial();
@@ -1977,7 +1911,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         return this;
       }
       public Builder clearTimeRange() {
@@ -1987,11 +1921,11 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000020);
+        bitField0_ = (bitField0_ & ~0x00000010);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder getTimeRangeBuilder() {
-        bitField0_ |= 0x00000020;
+        bitField0_ |= 0x00000010;
         onChanged();
         return getTimeRangeFieldBuilder().getBuilder();
       }
@@ -2016,85 +1950,85 @@ public final class ClientProtos {
         return timeRangeBuilder_;
       }
       
-      // optional uint32 maxVersions = 7 [default = 1];
+      // optional uint32 maxVersions = 6 [default = 1];
       private int maxVersions_ = 1;
       public boolean hasMaxVersions() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000020) == 0x00000020);
       }
       public int getMaxVersions() {
         return maxVersions_;
       }
       public Builder setMaxVersions(int value) {
-        bitField0_ |= 0x00000040;
+        bitField0_ |= 0x00000020;
         maxVersions_ = value;
         onChanged();
         return this;
       }
       public Builder clearMaxVersions() {
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         maxVersions_ = 1;
         onChanged();
         return this;
       }
       
-      // optional bool cacheBlocks = 8 [default = true];
+      // optional bool cacheBlocks = 7 [default = true];
       private boolean cacheBlocks_ = true;
       public boolean hasCacheBlocks() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000040) == 0x00000040);
       }
       public boolean getCacheBlocks() {
         return cacheBlocks_;
       }
       public Builder setCacheBlocks(boolean value) {
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         cacheBlocks_ = value;
         onChanged();
         return this;
       }
       public Builder clearCacheBlocks() {
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         cacheBlocks_ = true;
         onChanged();
         return this;
       }
       
-      // optional uint32 storeLimit = 9;
+      // optional uint32 storeLimit = 8;
       private int storeLimit_ ;
       public boolean hasStoreLimit() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
+        return ((bitField0_ & 0x00000080) == 0x00000080);
       }
       public int getStoreLimit() {
         return storeLimit_;
       }
       public Builder setStoreLimit(int value) {
-        bitField0_ |= 0x00000100;
+        bitField0_ |= 0x00000080;
         storeLimit_ = value;
         onChanged();
         return this;
       }
       public Builder clearStoreLimit() {
-        bitField0_ = (bitField0_ & ~0x00000100);
+        bitField0_ = (bitField0_ & ~0x00000080);
         storeLimit_ = 0;
         onChanged();
         return this;
       }
       
-      // optional uint32 storeOffset = 10;
+      // optional uint32 storeOffset = 9;
       private int storeOffset_ ;
       public boolean hasStoreOffset() {
-        return ((bitField0_ & 0x00000200) == 0x00000200);
+        return ((bitField0_ & 0x00000100) == 0x00000100);
       }
       public int getStoreOffset() {
         return storeOffset_;
       }
       public Builder setStoreOffset(int value) {
-        bitField0_ |= 0x00000200;
+        bitField0_ |= 0x00000100;
         storeOffset_ = value;
         onChanged();
         return this;
       }
       public Builder clearStoreOffset() {
-        bitField0_ = (bitField0_ & ~0x00000200);
+        bitField0_ = (bitField0_ & ~0x00000100);
         storeOffset_ = 0;
         onChanged();
         return this;
@@ -4642,11 +4576,7 @@ public final class ClientProtos {
     boolean hasTimestamp();
     long getTimestamp();
     
-    // optional uint64 lockId = 6;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional bool writeToWAL = 7 [default = true];
+    // optional bool writeToWAL = 6 [default = true];
     boolean hasWriteToWAL();
     boolean getWriteToWAL();
     
@@ -6148,21 +6078,11 @@ public final class ClientProtos {
       return timestamp_;
     }
     
-    // optional uint64 lockId = 6;
-    public static final int LOCKID_FIELD_NUMBER = 6;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional bool writeToWAL = 7 [default = true];
-    public static final int WRITETOWAL_FIELD_NUMBER = 7;
+    // optional bool writeToWAL = 6 [default = true];
+    public static final int WRITETOWAL_FIELD_NUMBER = 6;
     private boolean writeToWAL_;
     public boolean hasWriteToWAL() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
+      return ((bitField0_ & 0x00000008) == 0x00000008);
     }
     public boolean getWriteToWAL() {
       return writeToWAL_;
@@ -6172,7 +6092,7 @@ public final class ClientProtos {
     public static final int TIMERANGE_FIELD_NUMBER = 10;
     private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange timeRange_;
     public boolean hasTimeRange() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
+      return ((bitField0_ & 0x00000010) == 0x00000010);
     }
     public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
       return timeRange_;
@@ -6187,7 +6107,6 @@ public final class ClientProtos {
       columnValue_ = java.util.Collections.emptyList();
       attribute_ = java.util.Collections.emptyList();
       timestamp_ = 0L;
-      lockId_ = 0L;
       writeToWAL_ = true;
       timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
     }
@@ -6239,12 +6158,9 @@ public final class ClientProtos {
         output.writeUInt64(5, timestamp_);
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt64(6, lockId_);
+        output.writeBool(6, writeToWAL_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeBool(7, writeToWAL_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
         output.writeMessage(10, timeRange_);
       }
       getUnknownFields().writeTo(output);
@@ -6278,14 +6194,10 @@ public final class ClientProtos {
       }
       if (((bitField0_ & 0x00000008) == 0x00000008)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(6, lockId_);
+          .computeBoolSize(6, writeToWAL_);
       }
       if (((bitField0_ & 0x00000010) == 0x00000010)) {
         size += com.google.protobuf.CodedOutputStream
-          .computeBoolSize(7, writeToWAL_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
           .computeMessageSize(10, timeRange_);
       }
       size += getUnknownFields().getSerializedSize();
@@ -6330,11 +6242,6 @@ public final class ClientProtos {
         result = result && (getTimestamp()
             == other.getTimestamp());
       }
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
       result = result && (hasWriteToWAL() == other.hasWriteToWAL());
       if (hasWriteToWAL()) {
         result = result && (getWriteToWAL()
@@ -6374,10 +6281,6 @@ public final class ClientProtos {
         hash = (37 * hash) + TIMESTAMP_FIELD_NUMBER;
         hash = (53 * hash) + hashLong(getTimestamp());
       }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
       if (hasWriteToWAL()) {
         hash = (37 * hash) + WRITETOWAL_FIELD_NUMBER;
         hash = (53 * hash) + hashBoolean(getWriteToWAL());
@@ -6523,16 +6426,14 @@ public final class ClientProtos {
         }
         timestamp_ = 0L;
         bitField0_ = (bitField0_ & ~0x00000010);
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000020);
         writeToWAL_ = true;
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         if (timeRangeBuilder_ == null) {
           timeRange_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance();
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
       
@@ -6604,14 +6505,10 @@ public final class ClientProtos {
         if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
           to_bitField0_ |= 0x00000008;
         }
-        result.lockId_ = lockId_;
+        result.writeToWAL_ = writeToWAL_;
         if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
           to_bitField0_ |= 0x00000010;
         }
-        result.writeToWAL_ = writeToWAL_;
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000020;
-        }
         if (timeRangeBuilder_ == null) {
           result.timeRange_ = timeRange_;
         } else {
@@ -6694,9 +6591,6 @@ public final class ClientProtos {
         if (other.hasTimestamp()) {
           setTimestamp(other.getTimestamp());
         }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
         if (other.hasWriteToWAL()) {
           setWriteToWAL(other.getWriteToWAL());
         }
@@ -6789,11 +6683,6 @@ public final class ClientProtos {
             }
             case 48: {
               bitField0_ |= 0x00000020;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 56: {
-              bitField0_ |= 0x00000040;
               writeToWAL_ = input.readBool();
               break;
             }
@@ -7253,43 +7142,22 @@ public final class ClientProtos {
         return this;
       }
       
-      // optional uint64 lockId = 6;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000020;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000020);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional bool writeToWAL = 7 [default = true];
+      // optional bool writeToWAL = 6 [default = true];
       private boolean writeToWAL_ = true;
       public boolean hasWriteToWAL() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
+        return ((bitField0_ & 0x00000020) == 0x00000020);
       }
       public boolean getWriteToWAL() {
         return writeToWAL_;
       }
       public Builder setWriteToWAL(boolean value) {
-        bitField0_ |= 0x00000040;
+        bitField0_ |= 0x00000020;
         writeToWAL_ = value;
         onChanged();
         return this;
       }
       public Builder clearWriteToWAL() {
-        bitField0_ = (bitField0_ & ~0x00000040);
+        bitField0_ = (bitField0_ & ~0x00000020);
         writeToWAL_ = true;
         onChanged();
         return this;
@@ -7300,7 +7168,7 @@ public final class ClientProtos {
       private com.google.protobuf.SingleFieldBuilder<
           org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRangeOrBuilder> timeRangeBuilder_;
       public boolean hasTimeRange() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
+        return ((bitField0_ & 0x00000040) == 0x00000040);
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange getTimeRange() {
         if (timeRangeBuilder_ == null) {
@@ -7319,7 +7187,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(value);
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder setTimeRange(
@@ -7330,12 +7198,12 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.setMessage(builderForValue.build());
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder mergeTimeRange(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange value) {
         if (timeRangeBuilder_ == null) {
-          if (((bitField0_ & 0x00000080) == 0x00000080) &&
+          if (((bitField0_ & 0x00000040) == 0x00000040) &&
               timeRange_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.getDefaultInstance()) {
             timeRange_ =
               org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.newBuilder(timeRange_).mergeFrom(value).buildPartial();
@@ -7346,7 +7214,7 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.mergeFrom(value);
         }
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         return this;
       }
       public Builder clearTimeRange() {
@@ -7356,11 +7224,11 @@ public final class ClientProtos {
         } else {
           timeRangeBuilder_.clear();
         }
-        bitField0_ = (bitField0_ & ~0x00000080);
+        bitField0_ = (bitField0_ & ~0x00000040);
         return this;
       }
       public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.TimeRange.Builder getTimeRangeBuilder() {
-        bitField0_ |= 0x00000080;
+        bitField0_ |= 0x00000040;
         onChanged();
         return getTimeRangeFieldBuilder().getBuilder();
       }
@@ -12140,1882 +12008,6 @@ public final class ClientProtos {
     // @@protoc_insertion_point(class_scope:ScanResponse)
   }
   
-  public interface LockRowRequestOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required .RegionSpecifier region = 1;
-    boolean hasRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
-    
-    // repeated bytes row = 2;
-    java.util.List<com.google.protobuf.ByteString> getRowList();
-    int getRowCount();
-    com.google.protobuf.ByteString getRow(int index);
-  }
-  public static final class LockRowRequest extends
-      com.google.protobuf.GeneratedMessage
-      implements LockRowRequestOrBuilder {
-    // Use LockRowRequest.newBuilder() to construct.
-    private LockRowRequest(Builder builder) {
-      super(builder);
-    }
-    private LockRowRequest(boolean noInit) {}
-    
-    private static final LockRowRequest defaultInstance;
-    public static LockRowRequest getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public LockRowRequest getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required .RegionSpecifier region = 1;
-    public static final int REGION_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_;
-    public boolean hasRegion() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-      return region_;
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-      return region_;
-    }
-    
-    // repeated bytes row = 2;
-    public static final int ROW_FIELD_NUMBER = 2;
-    private java.util.List<com.google.protobuf.ByteString> row_;
-    public java.util.List<com.google.protobuf.ByteString>
-        getRowList() {
-      return row_;
-    }
-    public int getRowCount() {
-      return row_.size();
-    }
-    public com.google.protobuf.ByteString getRow(int index) {
-      return row_.get(index);
-    }
-    
-    private void initFields() {
-      region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      row_ = java.util.Collections.emptyList();;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasRegion()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!getRegion().isInitialized()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, region_);
-      }
-      for (int i = 0; i < row_.size(); i++) {
-        output.writeBytes(2, row_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, region_);
-      }
-      {
-        int dataSize = 0;
-        for (int i = 0; i < row_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(row_.get(i));
-        }
-        size += dataSize;
-        size += 1 * getRowList().size();
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest) obj;
-      
-      boolean result = true;
-      result = result && (hasRegion() == other.hasRegion());
-      if (hasRegion()) {
-        result = result && getRegion()
-            .equals(other.getRegion());
-      }
-      result = result && getRowList()
-          .equals(other.getRowList());
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasRegion()) {
-        hash = (37 * hash) + REGION_FIELD_NUMBER;
-        hash = (53 * hash) + getRegion().hashCode();
-      }
-      if (getRowCount() > 0) {
-        hash = (37 * hash) + ROW_FIELD_NUMBER;
-        hash = (53 * hash) + getRowList().hashCode();
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequestOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowRequest_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getRegionFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        row_ = java.util.Collections.emptyList();;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (regionBuilder_ == null) {
-          result.region_ = region_;
-        } else {
-          result.region_ = regionBuilder_.build();
-        }
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          row_ = java.util.Collections.unmodifiableList(row_);
-          bitField0_ = (bitField0_ & ~0x00000002);
-        }
-        result.row_ = row_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance()) return this;
-        if (other.hasRegion()) {
-          mergeRegion(other.getRegion());
-        }
-        if (!other.row_.isEmpty()) {
-          if (row_.isEmpty()) {
-            row_ = other.row_;
-            bitField0_ = (bitField0_ & ~0x00000002);
-          } else {
-            ensureRowIsMutable();
-            row_.addAll(other.row_);
-          }
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasRegion()) {
-          
-          return false;
-        }
-        if (!getRegion().isInitialized()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder();
-              if (hasRegion()) {
-                subBuilder.mergeFrom(getRegion());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setRegion(subBuilder.buildPartial());
-              break;
-            }
-            case 18: {
-              ensureRowIsMutable();
-              row_.add(input.readBytes());
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required .RegionSpecifier region = 1;
-      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> regionBuilder_;
-      public boolean hasRegion() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-        if (regionBuilder_ == null) {
-          return region_;
-        } else {
-          return regionBuilder_.getMessage();
-        }
-      }
-      public Builder setRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          region_ = value;
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder setRegion(
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
-        if (regionBuilder_ == null) {
-          region_ = builderForValue.build();
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder mergeRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              region_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
-            region_ =
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(region_).mergeFrom(value).buildPartial();
-          } else {
-            region_ = value;
-          }
-          onChanged();
-        } else {
-          regionBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder clearRegion() {
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-          onChanged();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getRegionBuilder() {
-        bitField0_ |= 0x00000001;
-        onChanged();
-        return getRegionFieldBuilder().getBuilder();
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-        if (regionBuilder_ != null) {
-          return regionBuilder_.getMessageOrBuilder();
-        } else {
-          return region_;
-        }
-      }
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
-          getRegionFieldBuilder() {
-        if (regionBuilder_ == null) {
-          regionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
-                  region_,
-                  getParentForChildren(),
-                  isClean());
-          region_ = null;
-        }
-        return regionBuilder_;
-      }
-      
-      // repeated bytes row = 2;
-      private java.util.List<com.google.protobuf.ByteString> row_ = java.util.Collections.emptyList();;
-      private void ensureRowIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          row_ = new java.util.ArrayList<com.google.protobuf.ByteString>(row_);
-          bitField0_ |= 0x00000002;
-         }
-      }
-      public java.util.List<com.google.protobuf.ByteString>
-          getRowList() {
-        return java.util.Collections.unmodifiableList(row_);
-      }
-      public int getRowCount() {
-        return row_.size();
-      }
-      public com.google.protobuf.ByteString getRow(int index) {
-        return row_.get(index);
-      }
-      public Builder setRow(
-          int index, com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureRowIsMutable();
-        row_.set(index, value);
-        onChanged();
-        return this;
-      }
-      public Builder addRow(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureRowIsMutable();
-        row_.add(value);
-        onChanged();
-        return this;
-      }
-      public Builder addAllRow(
-          java.lang.Iterable<? extends com.google.protobuf.ByteString> values) {
-        ensureRowIsMutable();
-        super.addAll(values, row_);
-        onChanged();
-        return this;
-      }
-      public Builder clearRow() {
-        row_ = java.util.Collections.emptyList();;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:LockRowRequest)
-    }
-    
-    static {
-      defaultInstance = new LockRowRequest(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:LockRowRequest)
-  }
-  
-  public interface LockRowResponseOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required uint64 lockId = 1;
-    boolean hasLockId();
-    long getLockId();
-    
-    // optional uint32 ttl = 2;
-    boolean hasTtl();
-    int getTtl();
-  }
-  public static final class LockRowResponse extends
-      com.google.protobuf.GeneratedMessage
-      implements LockRowResponseOrBuilder {
-    // Use LockRowResponse.newBuilder() to construct.
-    private LockRowResponse(Builder builder) {
-      super(builder);
-    }
-    private LockRowResponse(boolean noInit) {}
-    
-    private static final LockRowResponse defaultInstance;
-    public static LockRowResponse getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public LockRowResponse getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required uint64 lockId = 1;
-    public static final int LOCKID_FIELD_NUMBER = 1;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    // optional uint32 ttl = 2;
-    public static final int TTL_FIELD_NUMBER = 2;
-    private int ttl_;
-    public boolean hasTtl() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public int getTtl() {
-      return ttl_;
-    }
-    
-    private void initFields() {
-      lockId_ = 0L;
-      ttl_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasLockId()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, lockId_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(2, ttl_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, lockId_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(2, ttl_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) obj;
-      
-      boolean result = true;
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
-      result = result && (hasTtl() == other.hasTtl());
-      if (hasTtl()) {
-        result = result && (getTtl()
-            == other.getTtl());
-      }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
-      if (hasTtl()) {
-        hash = (37 * hash) + TTL_FIELD_NUMBER;
-        hash = (53 * hash) + getTtl();
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponseOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_LockRowResponse_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        ttl_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.lockId_ = lockId_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.ttl_ = ttl_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance()) return this;
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
-        if (other.hasTtl()) {
-          setTtl(other.getTtl());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasLockId()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              lockId_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              ttl_ = input.readUInt32();
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required uint64 lockId = 1;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000001;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // optional uint32 ttl = 2;
-      private int ttl_ ;
-      public boolean hasTtl() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public int getTtl() {
-        return ttl_;
-      }
-      public Builder setTtl(int value) {
-        bitField0_ |= 0x00000002;
-        ttl_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearTtl() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        ttl_ = 0;
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:LockRowResponse)
-    }
-    
-    static {
-      defaultInstance = new LockRowResponse(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:LockRowResponse)
-  }
-  
-  public interface UnlockRowRequestOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-    
-    // required .RegionSpecifier region = 1;
-    boolean hasRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion();
-    org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder();
-    
-    // required uint64 lockId = 2;
-    boolean hasLockId();
-    long getLockId();
-  }
-  public static final class UnlockRowRequest extends
-      com.google.protobuf.GeneratedMessage
-      implements UnlockRowRequestOrBuilder {
-    // Use UnlockRowRequest.newBuilder() to construct.
-    private UnlockRowRequest(Builder builder) {
-      super(builder);
-    }
-    private UnlockRowRequest(boolean noInit) {}
-    
-    private static final UnlockRowRequest defaultInstance;
-    public static UnlockRowRequest getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public UnlockRowRequest getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_fieldAccessorTable;
-    }
-    
-    private int bitField0_;
-    // required .RegionSpecifier region = 1;
-    public static final int REGION_FIELD_NUMBER = 1;
-    private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_;
-    public boolean hasRegion() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-      return region_;
-    }
-    public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-      return region_;
-    }
-    
-    // required uint64 lockId = 2;
-    public static final int LOCKID_FIELD_NUMBER = 2;
-    private long lockId_;
-    public boolean hasLockId() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    public long getLockId() {
-      return lockId_;
-    }
-    
-    private void initFields() {
-      region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      lockId_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      if (!hasRegion()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!hasLockId()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      if (!getRegion().isInitialized()) {
-        memoizedIsInitialized = 0;
-        return false;
-      }
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, region_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(2, lockId_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, region_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(2, lockId_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest) obj;
-      
-      boolean result = true;
-      result = result && (hasRegion() == other.hasRegion());
-      if (hasRegion()) {
-        result = result && getRegion()
-            .equals(other.getRegion());
-      }
-      result = result && (hasLockId() == other.hasLockId());
-      if (hasLockId()) {
-        result = result && (getLockId()
-            == other.getLockId());
-      }
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      if (hasRegion()) {
-        hash = (37 * hash) + REGION_FIELD_NUMBER;
-        hash = (53 * hash) + getRegion().hashCode();
-      }
-      if (hasLockId()) {
-        hash = (37 * hash) + LOCKID_FIELD_NUMBER;
-        hash = (53 * hash) + hashLong(getLockId());
-      }
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequestOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowRequest_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getRegionFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        lockId_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (regionBuilder_ == null) {
-          result.region_ = region_;
-        } else {
-          result.region_ = regionBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.lockId_ = lockId_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance()) return this;
-        if (other.hasRegion()) {
-          mergeRegion(other.getRegion());
-        }
-        if (other.hasLockId()) {
-          setLockId(other.getLockId());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        if (!hasRegion()) {
-          
-          return false;
-        }
-        if (!hasLockId()) {
-          
-          return false;
-        }
-        if (!getRegion().isInitialized()) {
-          
-          return false;
-        }
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder subBuilder = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder();
-              if (hasRegion()) {
-                subBuilder.mergeFrom(getRegion());
-              }
-              input.readMessage(subBuilder, extensionRegistry);
-              setRegion(subBuilder.buildPartial());
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              lockId_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      }
-      
-      private int bitField0_;
-      
-      // required .RegionSpecifier region = 1;
-      private org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> regionBuilder_;
-      public boolean hasRegion() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier getRegion() {
-        if (regionBuilder_ == null) {
-          return region_;
-        } else {
-          return regionBuilder_.getMessage();
-        }
-      }
-      public Builder setRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          region_ = value;
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder setRegion(
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder builderForValue) {
-        if (regionBuilder_ == null) {
-          region_ = builderForValue.build();
-          onChanged();
-        } else {
-          regionBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder mergeRegion(org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier value) {
-        if (regionBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              region_ != org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance()) {
-            region_ =
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.newBuilder(region_).mergeFrom(value).buildPartial();
-          } else {
-            region_ = value;
-          }
-          onChanged();
-        } else {
-          regionBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      public Builder clearRegion() {
-        if (regionBuilder_ == null) {
-          region_ = org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.getDefaultInstance();
-          onChanged();
-        } else {
-          regionBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder getRegionBuilder() {
-        bitField0_ |= 0x00000001;
-        onChanged();
-        return getRegionFieldBuilder().getBuilder();
-      }
-      public org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder getRegionOrBuilder() {
-        if (regionBuilder_ != null) {
-          return regionBuilder_.getMessageOrBuilder();
-        } else {
-          return region_;
-        }
-      }
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder> 
-          getRegionFieldBuilder() {
-        if (regionBuilder_ == null) {
-          regionBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.Builder, org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifierOrBuilder>(
-                  region_,
-                  getParentForChildren(),
-                  isClean());
-          region_ = null;
-        }
-        return regionBuilder_;
-      }
-      
-      // required uint64 lockId = 2;
-      private long lockId_ ;
-      public boolean hasLockId() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      public long getLockId() {
-        return lockId_;
-      }
-      public Builder setLockId(long value) {
-        bitField0_ |= 0x00000002;
-        lockId_ = value;
-        onChanged();
-        return this;
-      }
-      public Builder clearLockId() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        lockId_ = 0L;
-        onChanged();
-        return this;
-      }
-      
-      // @@protoc_insertion_point(builder_scope:UnlockRowRequest)
-    }
-    
-    static {
-      defaultInstance = new UnlockRowRequest(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:UnlockRowRequest)
-  }
-  
-  public interface UnlockRowResponseOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-  }
-  public static final class UnlockRowResponse extends
-      com.google.protobuf.GeneratedMessage
-      implements UnlockRowResponseOrBuilder {
-    // Use UnlockRowResponse.newBuilder() to construct.
-    private UnlockRowResponse(Builder builder) {
-      super(builder);
-    }
-    private UnlockRowResponse(boolean noInit) {}
-    
-    private static final UnlockRowResponse defaultInstance;
-    public static UnlockRowResponse getDefaultInstance() {
-      return defaultInstance;
-    }
-    
-    public UnlockRowResponse getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-    
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_descriptor;
-    }
-    
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_fieldAccessorTable;
-    }
-    
-    private void initFields() {
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-      
-      memoizedIsInitialized = 1;
-      return true;
-    }
-    
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      getUnknownFields().writeTo(output);
-    }
-    
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-    
-      size = 0;
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-    
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-    
-    @java.lang.Override
-    public boolean equals(final java.lang.Object obj) {
-      if (obj == this) {
-       return true;
-      }
-      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse)) {
-        return super.equals(obj);
-      }
-      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse other = (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) obj;
-      
-      boolean result = true;
-      result = result &&
-          getUnknownFields().equals(other.getUnknownFields());
-      return result;
-    }
-    
-    @java.lang.Override
-    public int hashCode() {
-      int hash = 41;
-      hash = (19 * hash) + getDescriptorForType().hashCode();
-      hash = (29 * hash) + getUnknownFields().hashCode();
-      return hash;
-    }
-    
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return newBuilder().mergeFrom(data, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      Builder builder = newBuilder();
-      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
-        return builder.buildParsed();
-      } else {
-        return null;
-      }
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input).buildParsed();
-    }
-    public static org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return newBuilder().mergeFrom(input, extensionRegistry)
-               .buildParsed();
-    }
-    
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-    
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponseOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_descriptor;
-      }
-      
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.internal_static_UnlockRowResponse_fieldAccessorTable;
-      }
-      
-      // Construct using org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-      
-      private Builder(BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-      
-      public Builder clear() {
-        super.clear();
-        return this;
-      }
-      
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-      
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDescriptor();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse getDefaultInstanceForType() {
-        return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse build() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-      
-      private org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse buildParsed()
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(
-            result).asInvalidProtocolBufferException();
-        }
-        return result;
-      }
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse buildPartial() {
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse result = new org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse(this);
-        onBuilt();
-        return result;
-      }
-      
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) {
-          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-      
-      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse other) {
-        if (other == org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance()) return this;
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-      
-      public final boolean isInitialized() {
-        return true;
-      }
-      
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder(
-            this.getUnknownFields());
-        while (true) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              this.setUnknownFields(unknownFields.build());
-              onChanged();
-              return this;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                this.setUnknownFields(unknownFields.build());
-                onChanged();
-                return this;
-              }
-              break;
-            }
-          }
-        }
-      }
-      
-      
-      // @@protoc_insertion_point(builder_scope:UnlockRowResponse)
-    }
-    
-    static {
-      defaultInstance = new UnlockRowResponse(true);
-      defaultInstance.initFields();
-    }
-    
-    // @@protoc_insertion_point(class_scope:UnlockRowResponse)
-  }
-  
   public interface BulkLoadHFileRequestOrBuilder
       extends com.google.protobuf.MessageOrBuilder {
     
@@ -22854,16 +20846,6 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse> done);
       
-      public abstract void lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done);
-      
-      public abstract void unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done);
-      
       public abstract void bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -22914,22 +20896,6 @@ public final class ClientProtos {
         }
         
         @java.lang.Override
-        public  void lockRow(
-            com.google.protobuf.RpcController controller,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done) {
-          impl.lockRow(controller, request, done);
-        }
-        
-        @java.lang.Override
-        public  void unlockRow(
-            com.google.protobuf.RpcController controller,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-            com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done) {
-          impl.unlockRow(controller, request, done);
-        }
-        
-        @java.lang.Override
         public  void bulkLoadHFile(
             com.google.protobuf.RpcController controller,
             org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -22990,16 +20956,12 @@ public final class ClientProtos {
             case 2:
               return impl.scan(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest)request);
             case 3:
-              return impl.lockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)request);
-            case 4:
-              return impl.unlockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)request);
-            case 5:
               return impl.bulkLoadHFile(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest)request);
-            case 6:
+            case 4:
               return impl.execCoprocessor(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest)request);
-            case 7:
+            case 5:
               return impl.execService(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest)request);
-            case 8:
+            case 6:
               return impl.multi(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest)request);
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23022,16 +20984,12 @@ public final class ClientProtos {
             case 2:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest.getDefaultInstance();
             case 3:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-            case 4:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.getDefaultInstance();
-            case 6:
+            case 4:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.getDefaultInstance();
-            case 7:
+            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.getDefaultInstance();
-            case 8:
+            case 6:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.getDefaultInstance();
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23054,16 +21012,12 @@ public final class ClientProtos {
             case 2:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance();
             case 3:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-            case 4:
-              return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance();
-            case 6:
+            case 4:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance();
-            case 7:
+            case 5:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance();
-            case 8:
+            case 6:
               return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance();
             default:
               throw new java.lang.AssertionError("Can't get here.");
@@ -23088,16 +21042,6 @@ public final class ClientProtos {
         org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request,
         com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse> done);
     
-    public abstract void lockRow(
-        com.google.protobuf.RpcController controller,
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done);
-    
-    public abstract void unlockRow(
-        com.google.protobuf.RpcController controller,
-        org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-        com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done);
-    
     public abstract void bulkLoadHFile(
         com.google.protobuf.RpcController controller,
         org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
@@ -23156,31 +21100,21 @@ public final class ClientProtos {
               done));
           return;
         case 3:
-          this.lockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest)request,
-            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse>specializeCallback(
-              done));
-          return;
-        case 4:
-          this.unlockRow(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest)request,
-            com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse>specializeCallback(
-              done));
-          return;
-        case 5:
           this.bulkLoadHFile(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse>specializeCallback(
               done));
           return;
-        case 6:
+        case 4:
           this.execCoprocessor(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse>specializeCallback(
               done));
           return;
-        case 7:
+        case 5:
           this.execService(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse>specializeCallback(
               done));
           return;
-        case 8:
+        case 6:
           this.multi(controller, (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest)request,
             com.google.protobuf.RpcUtil.<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse>specializeCallback(
               done));
@@ -23206,16 +21140,12 @@ public final class ClientProtos {
         case 2:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest.getDefaultInstance();
         case 3:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.getDefaultInstance();
-        case 4:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.getDefaultInstance();
-        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.getDefaultInstance();
-        case 6:
+        case 4:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.getDefaultInstance();
-        case 7:
+        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.getDefaultInstance();
-        case 8:
+        case 6:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.getDefaultInstance();
         default:
           throw new java.lang.AssertionError("Can't get here.");
@@ -23238,16 +21168,12 @@ public final class ClientProtos {
         case 2:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance();
         case 3:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance();
-        case 4:
-          return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance();
-        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance();
-        case 6:
+        case 4:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance();
-        case 7:
+        case 5:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance();
-        case 8:
+        case 6:
           return org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance();
         default:
           throw new java.lang.AssertionError("Can't get here.");
@@ -23315,42 +21241,12 @@ public final class ClientProtos {
             org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.getDefaultInstance()));
       }
       
-      public  void lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse> done) {
-        channel.callMethod(
-          getDescriptor().getMethods().get(3),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance(),
-          com.google.protobuf.RpcUtil.generalizeCallback(
-            done,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.class,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance()));
-      }
-      
-      public  void unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request,
-          com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse> done) {
-        channel.callMethod(
-          getDescriptor().getMethods().get(4),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance(),
-          com.google.protobuf.RpcUtil.generalizeCallback(
-            done,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.class,
-            org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance()));
-      }
-      
       public  void bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(3),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance(),
@@ -23365,7 +21261,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(4),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance(),
@@ -23380,7 +21276,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(5),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance(),
@@ -23395,7 +21291,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest request,
           com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse> done) {
         channel.callMethod(
-          getDescriptor().getMethods().get(8),
+          getDescriptor().getMethods().get(6),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance(),
@@ -23427,16 +21323,6 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest request)
           throws com.google.protobuf.ServiceException;
       
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request)
-          throws com.google.protobuf.ServiceException;
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request)
-          throws com.google.protobuf.ServiceException;
-      
       public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request)
@@ -23501,36 +21387,12 @@ public final class ClientProtos {
       }
       
       
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse lockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest request)
-          throws com.google.protobuf.ServiceException {
-        return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(3),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.getDefaultInstance());
-      }
-      
-      
-      public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse unlockRow(
-          com.google.protobuf.RpcController controller,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest request)
-          throws com.google.protobuf.ServiceException {
-        return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(4),
-          controller,
-          request,
-          org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.getDefaultInstance());
-      }
-      
-      
       public org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse bulkLoadHFile(
           com.google.protobuf.RpcController controller,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(5),
+          getDescriptor().getMethods().get(3),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.getDefaultInstance());
@@ -23542,7 +21404,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(6),
+          getDescriptor().getMethods().get(4),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.getDefaultInstance());
@@ -23554,7 +21416,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(7),
+          getDescriptor().getMethods().get(5),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.getDefaultInstance());
@@ -23566,7 +21428,7 @@ public final class ClientProtos {
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest request)
           throws com.google.protobuf.ServiceException {
         return (org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse) channel.callBlockingMethod(
-          getDescriptor().getMethods().get(8),
+          getDescriptor().getMethods().get(6),
           controller,
           request,
           org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse.getDefaultInstance());
@@ -23646,26 +21508,6 @@ public final class ClientProtos {
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_ScanResponse_fieldAccessorTable;
   private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_LockRowRequest_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_LockRowRequest_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_LockRowResponse_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_LockRowResponse_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_UnlockRowRequest_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_UnlockRowRequest_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_UnlockRowResponse_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_UnlockRowResponse_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
     internal_static_BulkLoadHFileRequest_descriptor;
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
@@ -23741,100 +21583,91 @@ public final class ClientProtos {
     java.lang.String[] descriptorData = {
       "\n\014Client.proto\032\013hbase.proto\032\020Comparator." +
       "proto\"+\n\006Column\022\016\n\006family\030\001 \002(\014\022\021\n\tquali" +
-      "fier\030\002 \003(\014\"\362\001\n\003Get\022\013\n\003row\030\001 \002(\014\022\027\n\006colum" +
+      "fier\030\002 \003(\014\"\342\001\n\003Get\022\013\n\003row\030\001 \002(\014\022\027\n\006colum" +
       "n\030\002 \003(\0132\007.Column\022!\n\tattribute\030\003 \003(\0132\016.Na" +
-      "meBytesPair\022\016\n\006lockId\030\004 \001(\004\022\027\n\006filter\030\005 " +
-      "\001(\0132\007.Filter\022\035\n\ttimeRange\030\006 \001(\0132\n.TimeRa" +
-      "nge\022\026\n\013maxVersions\030\007 \001(\r:\0011\022\031\n\013cacheBloc" +
-      "ks\030\010 \001(\010:\004true\022\022\n\nstoreLimit\030\t \001(\r\022\023\n\013st" +
-      "oreOffset\030\n \001(\r\"\037\n\006Result\022\025\n\rkeyValueByt" +
-      "es\030\001 \003(\014\"r\n\nGetRequest\022 \n\006region\030\001 \002(\0132\020",
-      ".RegionSpecifier\022\021\n\003get\030\002 \002(\0132\004.Get\022\030\n\020c" +
-      "losestRowBefore\030\003 \001(\010\022\025\n\rexistenceOnly\030\004" +
-      " \001(\010\"6\n\013GetResponse\022\027\n\006result\030\001 \001(\0132\007.Re" +
-      "sult\022\016\n\006exists\030\002 \001(\010\"\177\n\tCondition\022\013\n\003row" +
-      "\030\001 \002(\014\022\016\n\006family\030\002 \002(\014\022\021\n\tqualifier\030\003 \002(" +
-      "\014\022!\n\013compareType\030\004 \002(\0162\014.CompareType\022\037\n\n" +
-      "comparator\030\005 \002(\0132\013.Comparator\"\306\004\n\006Mutate" +
-      "\022\013\n\003row\030\001 \002(\014\022&\n\nmutateType\030\002 \002(\0162\022.Muta" +
-      "te.MutateType\022(\n\013columnValue\030\003 \003(\0132\023.Mut" +
-      "ate.ColumnValue\022!\n\tattribute\030\004 \003(\0132\016.Nam",
-      "eBytesPair\022\021\n\ttimestamp\030\005 \001(\004\022\016\n\006lockId\030" +
-      "\006 \001(\004\022\030\n\nwriteToWAL\030\007 \001(\010:\004true\022\035\n\ttimeR" +
-      "ange\030\n \001(\0132\n.TimeRange\032\310\001\n\013ColumnValue\022\016" +
-      "\n\006family\030\001 \002(\014\022:\n\016qualifierValue\030\002 \003(\0132\"" +
-      ".Mutate.ColumnValue.QualifierValue\032m\n\016Qu" +
-      "alifierValue\022\021\n\tqualifier\030\001 \001(\014\022\r\n\005value" +
-      "\030\002 \001(\014\022\021\n\ttimestamp\030\003 \001(\004\022&\n\ndeleteType\030" +
-      "\004 \001(\0162\022.Mutate.DeleteType\"<\n\nMutateType\022" +
-      "\n\n\006APPEND\020\000\022\r\n\tINCREMENT\020\001\022\007\n\003PUT\020\002\022\n\n\006D" +
-      "ELETE\020\003\"U\n\nDeleteType\022\026\n\022DELETE_ONE_VERS",
-      "ION\020\000\022\034\n\030DELETE_MULTIPLE_VERSIONS\020\001\022\021\n\rD" +
-      "ELETE_FAMILY\020\002\"i\n\rMutateRequest\022 \n\006regio" +
-      "n\030\001 \002(\0132\020.RegionSpecifier\022\027\n\006mutate\030\002 \002(" +
-      "\0132\007.Mutate\022\035\n\tcondition\030\003 \001(\0132\n.Conditio" +
-      "n\"<\n\016MutateResponse\022\027\n\006result\030\001 \001(\0132\007.Re" +
-      "sult\022\021\n\tprocessed\030\002 \001(\010\"\243\002\n\004Scan\022\027\n\006colu" +
-      "mn\030\001 \003(\0132\007.Column\022!\n\tattribute\030\002 \003(\0132\016.N" +
-      "ameBytesPair\022\020\n\010startRow\030\003 \001(\014\022\017\n\007stopRo" +
-      "w\030\004 \001(\014\022\027\n\006filter\030\005 \001(\0132\007.Filter\022\035\n\ttime" +
-      "Range\030\006 \001(\0132\n.TimeRange\022\026\n\013maxVersions\030\007",
-      " \001(\r:\0011\022\031\n\013cacheBlocks\030\010 \001(\010:\004true\022\021\n\tba" +
-      "tchSize\030\t \001(\r\022\025\n\rmaxResultSize\030\n \001(\004\022\022\n\n" +
-      "storeLimit\030\013 \001(\r\022\023\n\013storeOffset\030\014 \001(\r\"\230\001" +
-      "\n\013ScanRequest\022 \n\006region\030\001 \001(\0132\020.RegionSp" +
-      "ecifier\022\023\n\004scan\030\002 \001(\0132\005.Scan\022\021\n\tscannerI" +
-      "d\030\003 \001(\004\022\024\n\014numberOfRows\030\004 \001(\r\022\024\n\014closeSc" +
-      "anner\030\005 \001(\010\022\023\n\013nextCallSeq\030\006 \001(\004\"\\\n\014Scan" +
-      "Response\022\027\n\006result\030\001 \003(\0132\007.Result\022\021\n\tsca" +
-      "nnerId\030\002 \001(\004\022\023\n\013moreResults\030\003 \001(\010\022\013\n\003ttl" +
-      "\030\004 \001(\r\"?\n\016LockRowRequest\022 \n\006region\030\001 \002(\013",
-      "2\020.RegionSpecifier\022\013\n\003row\030\002 \003(\014\".\n\017LockR" +
-      "owResponse\022\016\n\006lockId\030\001 \002(\004\022\013\n\003ttl\030\002 \001(\r\"" +
-      "D\n\020UnlockRowRequest\022 \n\006region\030\001 \002(\0132\020.Re" +
-      "gionSpecifier\022\016\n\006lockId\030\002 \002(\004\"\023\n\021UnlockR" +
-      "owResponse\"\260\001\n\024BulkLoadHFileRequest\022 \n\006r" +
-      "egion\030\001 \002(\0132\020.RegionSpecifier\0224\n\nfamilyP" +
-      "ath\030\002 \003(\0132 .BulkLoadHFileRequest.FamilyP" +
-      "ath\022\024\n\014assignSeqNum\030\003 \001(\010\032*\n\nFamilyPath\022" +
-      "\016\n\006family\030\001 \002(\014\022\014\n\004path\030\002 \002(\t\"\'\n\025BulkLoa" +
-      "dHFileResponse\022\016\n\006loaded\030\001 \002(\010\"\203\001\n\004Exec\022",
-      "\013\n\003row\030\001 \002(\014\022\024\n\014protocolName\030\002 \002(\t\022\022\n\nme" +
-      "thodName\030\003 \002(\t\022!\n\010property\030\004 \003(\0132\017.NameS" +
-      "tringPair\022!\n\tparameter\030\005 \003(\0132\016.NameBytes" +
-      "Pair\"O\n\026ExecCoprocessorRequest\022 \n\006region" +
-      "\030\001 \002(\0132\020.RegionSpecifier\022\023\n\004call\030\002 \002(\0132\005" +
-      ".Exec\"8\n\027ExecCoprocessorResponse\022\035\n\005valu" +
-      "e\030\001 \002(\0132\016.NameBytesPair\"_\n\026CoprocessorSe" +
-      "rviceCall\022\013\n\003row\030\001 \002(\014\022\023\n\013serviceName\030\002 " +
-      "\002(\t\022\022\n\nmethodName\030\003 \002(\t\022\017\n\007request\030\004 \002(\014" +
-      "\"d\n\031CoprocessorServiceRequest\022 \n\006region\030",
-      "\001 \002(\0132\020.RegionSpecifier\022%\n\004call\030\002 \002(\0132\027." +
-      "CoprocessorServiceCall\"]\n\032CoprocessorSer" +
-      "viceResponse\022 \n\006region\030\001 \002(\0132\020.RegionSpe" +
-      "cifier\022\035\n\005value\030\002 \002(\0132\016.NameBytesPair\"N\n" +
-      "\013MultiAction\022\027\n\006mutate\030\001 \001(\0132\007.Mutate\022\021\n" +
-      "\003get\030\002 \001(\0132\004.Get\022\023\n\004exec\030\003 \001(\0132\005.Exec\"P\n" +
-      "\014ActionResult\022\035\n\005value\030\001 \001(\0132\016.NameBytes" +
-      "Pair\022!\n\texception\030\002 \001(\0132\016.NameBytesPair\"" +
-      "^\n\014MultiRequest\022 \n\006region\030\001 \002(\0132\020.Region" +
-      "Specifier\022\034\n\006action\030\002 \003(\0132\014.MultiAction\022",
-      "\016\n\006atomic\030\003 \001(\010\".\n\rMultiResponse\022\035\n\006resu" +
-      "lt\030\001 \003(\0132\r.ActionResult2\331\003\n\rClientServic" +
-      "e\022 \n\003get\022\013.GetRequest\032\014.GetResponse\022)\n\006m" +
-      "utate\022\016.MutateRequest\032\017.MutateResponse\022#" +
-      "\n\004scan\022\014.ScanRequest\032\r.ScanResponse\022,\n\007l" +
-      "ockRow\022\017.LockRowRequest\032\020.LockRowRespons" +
-      "e\0222\n\tunlockRow\022\021.UnlockRowRequest\032\022.Unlo" +
-      "ckRowResponse\022>\n\rbulkLoadHFile\022\025.BulkLoa" +
-      "dHFileRequest\032\026.BulkLoadHFileResponse\022D\n" +
-      "\017execCoprocessor\022\027.ExecCoprocessorReques",
-      "t\032\030.ExecCoprocessorResponse\022F\n\013execServi" +
-      "ce\022\032.CoprocessorServiceRequest\032\033.Coproce" +
-      "ssorServiceResponse\022&\n\005multi\022\r.MultiRequ" +
-      "est\032\016.MultiResponseBB\n*org.apache.hadoop" +
-      ".hbase.protobuf.generatedB\014ClientProtosH" +
-      "\001\210\001\001\240\001\001"
+      "meBytesPair\022\027\n\006filter\030\004 \001(\0132\007.Filter\022\035\n\t" +
+      "timeRange\030\005 \001(\0132\n.TimeRange\022\026\n\013maxVersio" +
+      "ns\030\006 \001(\r:\0011\022\031\n\013cacheBlocks\030\007 \001(\010:\004true\022\022" +
+      "\n\nstoreLimit\030\010 \001(\r\022\023\n\013storeOffset\030\t \001(\r\"" +
+      "\037\n\006Result\022\025\n\rkeyValueBytes\030\001 \003(\014\"r\n\nGetR" +
+      "equest\022 \n\006region\030\001 \002(\0132\020.RegionSpecifier",
+      "\022\021\n\003get\030\002 \002(\0132\004.Get\022\030\n\020closestRowBefore\030" +
+      "\003 \001(\010\022\025\n\rexistenceOnly\030\004 \001(\010\"6\n\013GetRespo" +
+      "nse\022\027\n\006result\030\001 \001(\0132\007.Result\022\016\n\006exists\030\002" +
+      " \001(\010\"\177\n\tCondition\022\013\n\003row\030\001 \002(\014\022\016\n\006family" +
+      "\030\002 \002(\014\022\021\n\tqualifier\030\003 \002(\014\022!\n\013compareType" +
+      "\030\004 \002(\0162\014.CompareType\022\037\n\ncomparator\030\005 \002(\013" +
+      "2\013.Comparator\"\266\004\n\006Mutate\022\013\n\003row\030\001 \002(\014\022&\n" +
+      "\nmutateType\030\002 \002(\0162\022.Mutate.MutateType\022(\n" +
+      "\013columnValue\030\003 \003(\0132\023.Mutate.ColumnValue\022" +
+      "!\n\tattribute\030\004 \003(\0132\016.NameBytesPair\022\021\n\tti",
+      "mestamp\030\005 \001(\004\022\030\n\nwriteToWAL\030\006 \001(\010:\004true\022" +
+      "\035\n\ttimeRange\030\n \001(\0132\n.TimeRange\032\310\001\n\013Colum" +
+      "nValue\022\016\n\006family\030\001 \002(\014\022:\n\016qualifierValue" +
+      "\030\002 \003(\0132\".Mutate.ColumnValue.QualifierVal" +
+      "ue\032m\n\016QualifierValue\022\021\n\tqualifier\030\001 \001(\014\022" +
+      "\r\n\005value\030\002 \001(\014\022\021\n\ttimestamp\030\003 \001(\004\022&\n\ndel" +
+      "eteType\030\004 \001(\0162\022.Mutate.DeleteType\"<\n\nMut" +
+      "ateType\022\n\n\006APPEND\020\000\022\r\n\tINCREMENT\020\001\022\007\n\003PU" +
+      "T\020\002\022\n\n\006DELETE\020\003\"U\n\nDeleteType\022\026\n\022DELETE_" +
+      "ONE_VERSION\020\000\022\034\n\030DELETE_MULTIPLE_VERSION",
+      "S\020\001\022\021\n\rDELETE_FAMILY\020\002\"i\n\rMutateRequest\022" +
+      " \n\006region\030\001 \002(\0132\020.RegionSpecifier\022\027\n\006mut" +
+      "ate\030\002 \002(\0132\007.Mutate\022\035\n\tcondition\030\003 \001(\0132\n." +
+      "Condition\"<\n\016MutateResponse\022\027\n\006result\030\001 " +
+      "\001(\0132\007.Result\022\021\n\tprocessed\030\002 \001(\010\"\243\002\n\004Scan" +
+      "\022\027\n\006column\030\001 \003(\0132\007.Column\022!\n\tattribute\030\002" +
+      " \003(\0132\016.NameBytesPair\022\020\n\010startRow\030\003 \001(\014\022\017" +
+      "\n\007stopRow\030\004 \001(\014\022\027\n\006filter\030\005 \001(\0132\007.Filter" +
+      "\022\035\n\ttimeRange\030\006 \001(\0132\n.TimeRange\022\026\n\013maxVe" +
+      "rsions\030\007 \001(\r:\0011\022\031\n\013cacheBlocks\030\010 \001(\010:\004tr",
+      "ue\022\021\n\tbatchSize\030\t \001(\r\022\025\n\rmaxResultSize\030\n" +
+      " \001(\004\022\022\n\nstoreLimit\030\013 \001(\r\022\023\n\013storeOffset\030" +
+      "\014 \001(\r\"\230\001\n\013ScanRequest\022 \n\006region\030\001 \001(\0132\020." +
+      "RegionSpecifier\022\023\n\004scan\030\002 \001(\0132\005.Scan\022\021\n\t" +
+      "scannerId\030\003 \001(\004\022\024\n\014numberOfRows\030\004 \001(\r\022\024\n" +
+      "\014closeScanner\030\005 \001(\010\022\023\n\013nextCallSeq\030\006 \001(\004" +
+      "\"\\\n\014ScanResponse\022\027\n\006result\030\001 \003(\0132\007.Resul" +
+      "t\022\021\n\tscannerId\030\002 \001(\004\022\023\n\013moreResults\030\003 \001(" +
+      "\010\022\013\n\003ttl\030\004 \001(\r\"\260\001\n\024BulkLoadHFileRequest\022" +
+      " \n\006region\030\001 \002(\0132\020.RegionSpecifier\0224\n\nfam",
+      "ilyPath\030\002 \003(\0132 .BulkLoadHFileRequest.Fam" +
+      "ilyPath\022\024\n\014assignSeqNum\030\003 \001(\010\032*\n\nFamilyP" +
+      "ath\022\016\n\006family\030\001 \002(\014\022\014\n\004path\030\002 \002(\t\"\'\n\025Bul" +
+      "kLoadHFileResponse\022\016\n\006loaded\030\001 \002(\010\"\203\001\n\004E" +
+      "xec\022\013\n\003row\030\001 \002(\014\022\024\n\014protocolName\030\002 \002(\t\022\022" +
+      "\n\nmethodName\030\003 \002(\t\022!\n\010property\030\004 \003(\0132\017.N" +
+      "ameStringPair\022!\n\tparameter\030\005 \003(\0132\016.NameB" +
+      "ytesPair\"O\n\026ExecCoprocessorRequest\022 \n\006re" +
+      "gion\030\001 \002(\0132\020.RegionSpecifier\022\023\n\004call\030\002 \002" +
+      "(\0132\005.Exec\"8\n\027ExecCoprocessorResponse\022\035\n\005",
+      "value\030\001 \002(\0132\016.NameBytesPair\"_\n\026Coprocess" +
+      "orServiceCall\022\013\n\003row\030\001 \002(\014\022\023\n\013serviceNam" +
+      "e\030\002 \002(\t\022\022\n\nmethodName\030\003 \002(\t\022\017\n\007request\030\004" +
+      " \002(\014\"d\n\031CoprocessorServiceRequest\022 \n\006reg" +
+      "ion\030\001 \002(\0132\020.RegionSpecifier\022%\n\004call\030\002 \002(" +
+      "\0132\027.CoprocessorServiceCall\"]\n\032Coprocesso" +
+      "rServiceResponse\022 \n\006region\030\001 \002(\0132\020.Regio" +
+      "nSpecifier\022\035\n\005value\030\002 \002(\0132\016.NameBytesPai" +
+      "r\"N\n\013MultiAction\022\027\n\006mutate\030\001 \001(\0132\007.Mutat" +
+      "e\022\021\n\003get\030\002 \001(\0132\004.Get\022\023\n\004exec\030\003 \001(\0132\005.Exe",
+      "c\"P\n\014ActionResult\022\035\n\005value\030\001 \001(\0132\016.NameB" +
+      "ytesPair\022!\n\texception\030\002 \001(\0132\016.NameBytesP" +
+      "air\"^\n\014MultiRequest\022 \n\006region\030\001 \002(\0132\020.Re" +
+      "gionSpecifier\022\034\n\006action\030\002 \003(\0132\014.MultiAct" +
+      "ion\022\016\n\006atomic\030\003 \001(\010\".\n\rMultiResponse\022\035\n\006" +
+      "result\030\001 \003(\0132\r.ActionResult2\367\002\n\rClientSe" +
+      "rvice\022 \n\003get\022\013.GetRequest\032\014.GetResponse\022" +
+      ")\n\006mutate\022\016.MutateRequest\032\017.MutateRespon" +
+      "se\022#\n\004scan\022\014.ScanRequest\032\r.ScanResponse\022" +
+      ">\n\rbulkLoadHFile\022\025.BulkLoadHFileRequest\032",
+      "\026.BulkLoadHFileResponse\022D\n\017execCoprocess" +
+      "or\022\027.ExecCoprocessorRequest\032\030.ExecCoproc" +
+      "essorResponse\022F\n\013execService\022\032.Coprocess" +
+      "orServiceRequest\032\033.CoprocessorServiceRes" +
+      "ponse\022&\n\005multi\022\r.MultiRequest\032\016.MultiRes" +
+      "ponseBB\n*org.apache.hadoop.hbase.protobu" +
+      "f.generatedB\014ClientProtosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -23854,7 +21687,7 @@ public final class ClientProtos {
           internal_static_Get_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Get_descriptor,
-              new java.lang.String[] { "Row", "Column", "Attribute", "LockId", "Filter", "TimeRange", "MaxVersions", "CacheBlocks", "StoreLimit", "StoreOffset", },
+              new java.lang.String[] { "Row", "Column", "Attribute", "Filter", "TimeRange", "MaxVersions", "CacheBlocks", "StoreLimit", "StoreOffset", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Get.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Get.Builder.class);
           internal_static_Result_descriptor =
@@ -23894,7 +21727,7 @@ public final class ClientProtos {
           internal_static_Mutate_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Mutate_descriptor,
-              new java.lang.String[] { "Row", "MutateType", "ColumnValue", "Attribute", "Timestamp", "LockId", "WriteToWAL", "TimeRange", },
+              new java.lang.String[] { "Row", "MutateType", "ColumnValue", "Attribute", "Timestamp", "WriteToWAL", "TimeRange", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.Builder.class);
           internal_static_Mutate_ColumnValue_descriptor =
@@ -23953,40 +21786,8 @@ public final class ClientProtos {
               new java.lang.String[] { "Result", "ScannerId", "MoreResults", "Ttl", },
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse.Builder.class);
-          internal_static_LockRowRequest_descriptor =
-            getDescriptor().getMessageTypes().get(12);
-          internal_static_LockRowRequest_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_LockRowRequest_descriptor,
-              new java.lang.String[] { "Region", "Row", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest.Builder.class);
-          internal_static_LockRowResponse_descriptor =
-            getDescriptor().getMessageTypes().get(13);
-          internal_static_LockRowResponse_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_LockRowResponse_descriptor,
-              new java.lang.String[] { "LockId", "Ttl", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse.Builder.class);
-          internal_static_UnlockRowRequest_descriptor =
-            getDescriptor().getMessageTypes().get(14);
-          internal_static_UnlockRowRequest_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UnlockRowRequest_descriptor,
-              new java.lang.String[] { "Region", "LockId", },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest.Builder.class);
-          internal_static_UnlockRowResponse_descriptor =
-            getDescriptor().getMessageTypes().get(15);
-          internal_static_UnlockRowResponse_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_UnlockRowResponse_descriptor,
-              new java.lang.String[] { },
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.class,
-              org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse.Builder.class);
           internal_static_BulkLoadHFileRequest_descriptor =
-            getDescriptor().getMessageTypes().get(16);
+            getDescriptor().getMessageTypes().get(12);
           internal_static_BulkLoadHFileRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_BulkLoadHFileRequest_descriptor,
@@ -24002,7 +21803,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileRequest.FamilyPath.Builder.class);
           internal_static_BulkLoadHFileResponse_descriptor =
-            getDescriptor().getMessageTypes().get(17);
+            getDescriptor().getMessageTypes().get(13);
           internal_static_BulkLoadHFileResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_BulkLoadHFileResponse_descriptor,
@@ -24010,7 +21811,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.BulkLoadHFileResponse.Builder.class);
           internal_static_Exec_descriptor =
-            getDescriptor().getMessageTypes().get(18);
+            getDescriptor().getMessageTypes().get(14);
           internal_static_Exec_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_Exec_descriptor,
@@ -24018,7 +21819,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Exec.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Exec.Builder.class);
           internal_static_ExecCoprocessorRequest_descriptor =
-            getDescriptor().getMessageTypes().get(19);
+            getDescriptor().getMessageTypes().get(15);
           internal_static_ExecCoprocessorRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ExecCoprocessorRequest_descriptor,
@@ -24026,7 +21827,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest.Builder.class);
           internal_static_ExecCoprocessorResponse_descriptor =
-            getDescriptor().getMessageTypes().get(20);
+            getDescriptor().getMessageTypes().get(16);
           internal_static_ExecCoprocessorResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ExecCoprocessorResponse_descriptor,
@@ -24034,7 +21835,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse.Builder.class);
           internal_static_CoprocessorServiceCall_descriptor =
-            getDescriptor().getMessageTypes().get(21);
+            getDescriptor().getMessageTypes().get(17);
           internal_static_CoprocessorServiceCall_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceCall_descriptor,
@@ -24042,7 +21843,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceCall.Builder.class);
           internal_static_CoprocessorServiceRequest_descriptor =
-            getDescriptor().getMessageTypes().get(22);
+            getDescriptor().getMessageTypes().get(18);
           internal_static_CoprocessorServiceRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceRequest_descriptor,
@@ -24050,7 +21851,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceRequest.Builder.class);
           internal_static_CoprocessorServiceResponse_descriptor =
-            getDescriptor().getMessageTypes().get(23);
+            getDescriptor().getMessageTypes().get(19);
           internal_static_CoprocessorServiceResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_CoprocessorServiceResponse_descriptor,
@@ -24058,7 +21859,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.CoprocessorServiceResponse.Builder.class);
           internal_static_MultiAction_descriptor =
-            getDescriptor().getMessageTypes().get(24);
+            getDescriptor().getMessageTypes().get(20);
           internal_static_MultiAction_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiAction_descriptor,
@@ -24066,7 +21867,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction.Builder.class);
           internal_static_ActionResult_descriptor =
-            getDescriptor().getMessageTypes().get(25);
+            getDescriptor().getMessageTypes().get(21);
           internal_static_ActionResult_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_ActionResult_descriptor,
@@ -24074,7 +21875,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ActionResult.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ActionResult.Builder.class);
           internal_static_MultiRequest_descriptor =
-            getDescriptor().getMessageTypes().get(26);
+            getDescriptor().getMessageTypes().get(22);
           internal_static_MultiRequest_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiRequest_descriptor,
@@ -24082,7 +21883,7 @@ public final class ClientProtos {
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.class,
               org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest.Builder.class);
           internal_static_MultiResponse_descriptor =
-            getDescriptor().getMessageTypes().get(27);
+            getDescriptor().getMessageTypes().get(23);
           internal_static_MultiResponse_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_MultiResponse_descriptor,
diff --git hbase-protocol/src/main/protobuf/Client.proto hbase-protocol/src/main/protobuf/Client.proto
index dd34998..44856f7 100644
--- hbase-protocol/src/main/protobuf/Client.proto
+++ hbase-protocol/src/main/protobuf/Client.proto
@@ -42,13 +42,12 @@ message Get {
   required bytes row = 1;
   repeated Column column = 2;
   repeated NameBytesPair attribute = 3;
-  optional uint64 lockId = 4;
-  optional Filter filter = 5;
-  optional TimeRange timeRange = 6;
-  optional uint32 maxVersions = 7 [default = 1];
-  optional bool cacheBlocks = 8 [default = true];
-  optional uint32 storeLimit = 9;
-  optional uint32 storeOffset = 10;
+  optional Filter filter = 4;
+  optional TimeRange timeRange = 5;
+  optional uint32 maxVersions = 6 [default = 1];
+  optional bool cacheBlocks = 7 [default = true];
+  optional uint32 storeLimit = 8;
+  optional uint32 storeOffset = 9;
 }
 
 /**
@@ -113,8 +112,7 @@ message Mutate {
   repeated ColumnValue columnValue = 3;
   repeated NameBytesPair attribute = 4;
   optional uint64 timestamp = 5;
-  optional uint64 lockId = 6;
-  optional bool writeToWAL = 7 [default = true];
+  optional bool writeToWAL = 6 [default = true];
 
   // For some mutate, result may be returned, in which case,
   // time range can be specified for potential performance gain
@@ -223,24 +221,6 @@ message ScanResponse {
   optional uint32 ttl = 4;
 }
 
-message LockRowRequest {
-  required RegionSpecifier region = 1;
-  repeated bytes row = 2;
-}
-
-message LockRowResponse {
-  required uint64 lockId = 1;
-  optional uint32 ttl = 2;
-}
-
-message UnlockRowRequest {
-  required RegionSpecifier region = 1;
-  required uint64 lockId = 2;
-}
-
-message UnlockRowResponse {
-}
-
 /**
  * Atomically bulk load multiple HFiles (say from different column families)
  * into an open region.
@@ -365,12 +345,6 @@ service ClientService {
   rpc scan(ScanRequest)
     returns(ScanResponse);
 
-  rpc lockRow(LockRowRequest)
-    returns(LockRowResponse);
-
-  rpc unlockRow(UnlockRowRequest)
-    returns(UnlockRowResponse);
-
   rpc bulkLoadHFile(BulkLoadHFileRequest)
     returns(BulkLoadHFileResponse);
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
deleted file mode 100644
index e42f3a9..0000000
--- hbase-server/src/main/java/org/apache/hadoop/hbase/UnknownRowLockException.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-
-/**
- * Thrown if a region server is passed an unknown row lock id
- */
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class UnknownRowLockException extends DoNotRetryIOException {
-  private static final long serialVersionUID = 993179627856392526L;
-
-  /** constructor */
-  public UnknownRowLockException() {
-    super();
-  }
-
-  /**
-   * Constructor
-   * @param s message
-   */
-  public UnknownRowLockException(String s) {
-    super(s);
-  }
-}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
index 9a75546..d2503b2 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Delete.java
@@ -30,6 +30,8 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import javax.lang.model.type.NullType;
+
 /**
  * Used to perform Delete operations on a single row.
  * <p>
@@ -78,8 +80,7 @@ public class Delete extends Mutation implements Comparable<Row> {
   }
 
   /**
-   * Create a Delete operation for the specified row and timestamp, using
-   * an optional row lock.<p>
+   * Create a Delete operation for the specified row and timestamp.<p>
    *
    * If no further operations are done, this will delete all columns in all
    * families of the specified row with a timestamp less than or equal to the
@@ -89,14 +90,12 @@ public class Delete extends Mutation implements Comparable<Row> {
    * families or columns, you must specify each timestamp individually.
    * @param row row key
    * @param timestamp maximum version timestamp (only for delete row)
-   * @param rowLock previously acquired row lock, or null
+   * @param rowLock no longer supported.  NullType here so client code that just
+   * passed in null will still work.
    */
-  public Delete(byte [] row, long timestamp, RowLock rowLock) {
+  public Delete(byte [] row, long timestamp, NullType rowLock) {
     this.row = row;
     this.ts = timestamp;
-    if (rowLock != null) {
-    	this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -105,7 +104,6 @@ public class Delete extends Mutation implements Comparable<Row> {
   public Delete(final Delete d) {
     this.row = d.getRow();
     this.ts = d.getTimeStamp();
-    this.lockId = d.getLockId();
     this.familyMap.putAll(d.getFamilyMap());
     this.writeToWAL = d.writeToWAL;
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
index 0ade4e9..267f16f 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Get.java
@@ -36,6 +36,8 @@ import java.util.Set;
 import java.util.TreeMap;
 import java.util.TreeSet;
 
+import javax.lang.model.type.NullType;
+
 /**
  * Used to perform Get operations on a single row.
  * <p>
@@ -66,7 +68,6 @@ public class Get extends OperationWithAttributes
   implements Row, Comparable<Row> {
 
   private byte [] row = null;
-  private long lockId = -1L;
   private int maxVersions = 1;
   private boolean cacheBlocks = true;
   private int storeLimit = -1;
@@ -93,13 +94,11 @@ public class Get extends OperationWithAttributes
    * If no further operations are done, this will get the latest version of
    * all columns in all families of the specified row.
    * @param row row key
-   * @param rowLock previously acquired row lock, or null
+   * @param rowLock no longer supported.  NullType here so client code that just
+   * passed in null will still work.
    */
-  public Get(byte [] row, RowLock rowLock) {
+  public Get(byte [] row, NullType rowLock) {
     this.row = row;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -261,22 +260,6 @@ public class Get extends OperationWithAttributes
   }
 
   /**
-   * Method for retrieving the get's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the get's lockId
-   * @return lockId
-   */
-  public long getLockId() {
-    return this.lockId;
-  }
-
-  /**
    * Method for retrieving the get's maximum number of version
    * @return the maximum number of version to fetch for this get
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
index 373a766..368b6d4 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTable.java
@@ -61,12 +61,9 @@ import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CompareType;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
@@ -980,46 +977,6 @@ public class HTable implements HTableInterface {
    * {@inheritDoc}
    */
   @Override
-  public RowLock lockRow(final byte [] row)
-  throws IOException {
-    return new ServerCallable<RowLock>(connection, tableName, row, operationTimeout) {
-        public RowLock call() throws IOException {
-          try {
-            LockRowRequest request = RequestConverter.buildLockRowRequest(
-              location.getRegionInfo().getRegionName(), row);
-            LockRowResponse response = server.lockRow(null, request);
-            return new RowLock(row, response.getLockId());
-          } catch (ServiceException se) {
-            throw ProtobufUtil.getRemoteException(se);
-          }
-        }
-      }.withRetries();
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public void unlockRow(final RowLock rl)
-  throws IOException {
-    new ServerCallable<Boolean>(connection, tableName, rl.getRow(), operationTimeout) {
-        public Boolean call() throws IOException {
-          try {
-            UnlockRowRequest request = RequestConverter.buildUnlockRowRequest(
-              location.getRegionInfo().getRegionName(), rl.getLockId());
-            server.unlockRow(null, request);
-            return Boolean.TRUE;
-          } catch (ServiceException se) {
-            throw ProtobufUtil.getRemoteException(se);
-          }
-        }
-      }.withRetries();
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
   public boolean isAutoFlush() {
     return autoFlush;
   }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
index a988b1d..05c5840 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTableInterface.java
@@ -396,27 +396,6 @@ public interface HTableInterface extends Closeable {
   void close() throws IOException;
 
   /**
-   * Obtains a lock on a row.
-   *
-   * @param row The row to lock.
-   * @return A {@link RowLock} containing the row and lock id.
-   * @throws IOException if a remote or network exception occurs.
-   * @see RowLock
-   * @see #unlockRow
-   */
-  RowLock lockRow(byte[] row) throws IOException;
-
-  /**
-   * Releases a row lock.
-   *
-   * @param rl The row lock to release.
-   * @throws IOException if a remote or network exception occurs.
-   * @see RowLock
-   * @see #unlockRow
-   */
-  void unlockRow(RowLock rl) throws IOException;
-
-  /**
    * Creates and returns a proxy to the CoprocessorProtocol instance running in the
    * region containing the specified row.  The row given does not actually have
    * to exist.  Whichever region would contain the row based on start and end keys will
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
index 3dc5b49..93dcb96 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
@@ -465,16 +465,6 @@ public class HTablePool implements Closeable {
     }
 
     @Override
-    public RowLock lockRow(byte[] row) throws IOException {
-      return table.lockRow(row);
-    }
-
-    @Override
-    public void unlockRow(RowLock rl) throws IOException {
-      table.unlockRow(rl);
-    }
-
-    @Override
     public <T extends CoprocessorProtocol> T coprocessorProxy(
         Class<T> protocol, byte[] row) {
       return table.coprocessorProxy(protocol, row);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
index 7df6e4b..1bc4806 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Increment.java
@@ -24,6 +24,8 @@ import java.util.NavigableMap;
 import java.util.Set;
 import java.util.TreeMap;
 
+import javax.lang.model.type.NullType;
+
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.hbase.io.TimeRange;
@@ -45,7 +47,6 @@ import org.apache.hadoop.hbase.util.Bytes;
 @InterfaceStability.Stable
 public class Increment implements Row {
   private byte [] row = null;
-  private long lockId = -1L;
   private boolean writeToWAL = true;
   private TimeRange tr = new TimeRange();
   private Map<byte [], NavigableMap<byte [], Long>> familyMap =
@@ -70,16 +71,14 @@ public class Increment implements Row {
    * <p>
    * At least one column must be incremented.
    * @param row row key
-   * @param rowLock previously acquired row lock, or null
+   * @param rowLock no longer supported.  NullType here so client code that just
+   * passed in null will still work.
    */
-  public Increment(byte [] row, RowLock rowLock) {
+  public Increment(byte [] row, NullType rowLock) {
     if (row == null) {
       throw new IllegalArgumentException("Cannot increment a null row");
     }
     this.row = row;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -119,22 +118,6 @@ public class Increment implements Row {
   }
 
   /**
-   * Method for retrieving the increment's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the increment's lockId
-   * @return lockId
-   */
-  public long getLockId() {
-    return this.lockId;
-  }
-
-  /**
    * Method for retrieving whether WAL will be written to or not
    * @return true if WAL should be used, false if not
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
index 3fd78bd..2c6b494 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
@@ -40,7 +40,6 @@ public abstract class Mutation extends OperationWithAttributes implements Row {
 
   protected byte [] row = null;
   protected long ts = HConstants.LATEST_TIMESTAMP;
-  protected long lockId = -1L;
   protected boolean writeToWAL = true;
   protected Map<byte [], List<KeyValue>> familyMap =
       new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
@@ -165,23 +164,6 @@ public abstract class Mutation extends OperationWithAttributes implements Row {
   }
 
   /**
-   * Method for retrieving the delete's RowLock
-   * @return RowLock
-   */
-  public RowLock getRowLock() {
-    return new RowLock(this.row, this.lockId);
-  }
-
-  /**
-   * Method for retrieving the delete's lock ID.
-   *
-   * @return The lock ID.
-   */
-  public long getLockId() {
-  return this.lockId;
-  }
-
-  /**
    * Method for retrieving the timestamp
    * @return timestamp
    */
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
index 31b5573..1655f93 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/client/Put.java
@@ -34,6 +34,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
 
+import javax.lang.model.type.NullType;
+
 /**
  * Used to perform Put operations for a single row.
  * <p>
@@ -46,7 +48,7 @@ import java.util.TreeMap;
 public class Put extends Mutation implements HeapSize, Comparable<Row> {
   private static final long OVERHEAD = ClassSize.align(
       ClassSize.OBJECT + 2 * ClassSize.REFERENCE +
-      2 * Bytes.SIZEOF_LONG + Bytes.SIZEOF_BOOLEAN +
+      1 * Bytes.SIZEOF_LONG + Bytes.SIZEOF_BOOLEAN +
       ClassSize.REFERENCE + ClassSize.TREEMAP);
 
   /**
@@ -60,9 +62,10 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
   /**
    * Create a Put operation for the specified row, using an existing row lock.
    * @param row row key
-   * @param rowLock previously acquired row lock, or null
+   * @param rowLock no longer supported.  NullType here so client code that just
+   * passed in null will still work.
    */
-  public Put(byte [] row, RowLock rowLock) {
+  public Put(byte [] row, NullType rowLock) {
       this(row, HConstants.LATEST_TIMESTAMP, rowLock);
   }
 
@@ -80,17 +83,15 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * Create a Put operation for the specified row, using a given timestamp, and an existing row lock.
    * @param row row key
    * @param ts timestamp
-   * @param rowLock previously acquired row lock, or null
+   * @param rowLock no longer supported.  NullType here so client code that just
+   * passed in null will still work.
    */
-  public Put(byte [] row, long ts, RowLock rowLock) {
+  public Put(byte [] row, long ts, NullType rowLock) {
     if(row == null || row.length > HConstants.MAX_ROW_LENGTH) {
       throw new IllegalArgumentException("Row key is invalid");
     }
     this.row = Arrays.copyOf(row, row.length);
     this.ts = ts;
-    if(rowLock != null) {
-      this.lockId = rowLock.getLockId();
-    }
   }
 
   /**
@@ -98,7 +99,7 @@ public class Put extends Mutation implements HeapSize, Comparable<Row> {
    * @param putToCopy put to copy
    */
   public Put(Put putToCopy) {
-    this(putToCopy.getRow(), putToCopy.ts, putToCopy.getRowLock());
+    this(putToCopy.getRow(), putToCopy.ts);
     this.familyMap =
       new TreeMap<byte [], List<KeyValue>>(Bytes.BYTES_COMPARATOR);
     for(Map.Entry<byte [], List<KeyValue>> entry :
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java
deleted file mode 100644
index 6736877..0000000
--- hbase-server/src/main/java/org/apache/hadoop/hbase/client/RowLock.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hbase.client;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Holds row name and lock id.
- */
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class RowLock {
-  private byte [] row = null;
-  private long lockId = -1L;
-
-  /**
-   * Creates a RowLock from a row and lock id
-   * @param row row to lock on
-   * @param lockId the lock id
-   */
-  public RowLock(final byte [] row, final long lockId) {
-    this.row = row;
-    this.lockId = lockId;
-  }
-
-  /**
-   * Creates a RowLock with only a lock id
-   * @param lockId lock id
-   */
-  public RowLock(final long lockId) {
-    this.lockId = lockId;
-  }
-
-  /**
-   * Get the row for this RowLock
-   * @return the row
-   */
-  public byte [] getRow() {
-    return row;
-  }
-
-  /**
-   * Get the lock id from this RowLock
-   * @return the lock id
-   */
-  public long getLockId() {
-    return lockId;
-  }
-}
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
index 49d4c0f..9fb3a8d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java
@@ -486,16 +486,6 @@ public abstract class CoprocessorHost<E extends CoprocessorEnvironment> {
         return tableName;
       }
 
-      public RowLock lockRow(byte[] row) throws IOException {
-        throw new RuntimeException(
-          "row locking is not allowed within the coprocessor environment");
-      }
-
-      public void unlockRow(RowLock rl) throws IOException {
-        throw new RuntimeException(
-          "row locking is not allowed within the coprocessor environment");
-      }
-
       @Override
       public void batch(List<? extends Row> actions, Object[] results)
           throws IOException, InterruptedException {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index 8dfa59d..5b9dae0 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -62,7 +62,6 @@ import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
@@ -353,11 +352,7 @@ public final class ProtobufUtil {
       final ClientProtos.Get proto) throws IOException {
     if (proto == null) return null;
     byte[] row = proto.getRow().toByteArray();
-    RowLock rowLock = null;
-    if (proto.hasLockId()) {
-      rowLock = new RowLock(proto.getLockId());
-    }
-    Get get = new Get(row, rowLock);
+    Get get = new Get(row);
     if (proto.hasCacheBlocks()) {
       get.setCacheBlocks(proto.getCacheBlocks());
     }
@@ -420,11 +415,7 @@ public final class ProtobufUtil {
     if (proto.hasTimestamp()) {
       timestamp = proto.getTimestamp();
     }
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
-    Put put = new Put(row, timestamp, lock);
+    Put put = new Put(row, timestamp);
     put.setWriteToWAL(proto.getWriteToWAL());
     for (NameBytesPair attribute: proto.getAttributeList()) {
       put.setAttribute(attribute.getName(),
@@ -463,11 +454,7 @@ public final class ProtobufUtil {
     if (proto.hasTimestamp()) {
       timestamp = proto.getTimestamp();
     }
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
-    Delete delete = new Delete(row, timestamp, lock);
+    Delete delete = new Delete(row, timestamp, null);
     delete.setWriteToWAL(proto.getWriteToWAL());
     for (NameBytesPair attribute: proto.getAttributeList()) {
       delete.setAttribute(attribute.getName(),
@@ -562,12 +549,8 @@ public final class ProtobufUtil {
       final Mutate proto) throws IOException {
     MutateType type = proto.getMutateType();
     assert type == MutateType.INCREMENT : type.name();
-    RowLock lock = null;
-    if (proto.hasLockId()) {
-      lock = new RowLock(proto.getLockId());
-    }
     byte[] row = proto.getRow().toByteArray();
-    Increment increment = new Increment(row, lock);
+    Increment increment = new Increment(row);
     increment.setWriteToWAL(proto.getWriteToWAL());
     if (proto.hasTimeRange()) {
       HBaseProtos.TimeRange timeRange = proto.getTimeRange();
@@ -788,9 +771,6 @@ public final class ProtobufUtil {
     builder.setRow(ByteString.copyFrom(get.getRow()));
     builder.setCacheBlocks(get.getCacheBlocks());
     builder.setMaxVersions(get.getMaxVersions());
-    if (get.getLockId() >= 0) {
-      builder.setLockId(get.getLockId());
-    }
     if (get.getFilter() != null) {
       builder.setFilter(ProtobufUtil.toFilter(get.getFilter()));
     }
@@ -846,9 +826,6 @@ public final class ProtobufUtil {
     builder.setRow(ByteString.copyFrom(increment.getRow()));
     builder.setMutateType(MutateType.INCREMENT);
     builder.setWriteToWAL(increment.getWriteToWAL());
-    if (increment.getLockId() >= 0) {
-      builder.setLockId(increment.getLockId());
-    }
     TimeRange timeRange = increment.getTimeRange();
     if (!timeRange.isAllTime()) {
       HBaseProtos.TimeRange.Builder timeRangeBuilder =
@@ -891,9 +868,6 @@ public final class ProtobufUtil {
     mutateBuilder.setRow(ByteString.copyFrom(mutation.getRow()));
     mutateBuilder.setMutateType(mutateType);
     mutateBuilder.setWriteToWAL(mutation.getWriteToWAL());
-    if (mutation.getLockId() >= 0) {
-      mutateBuilder.setLockId(mutation.getLockId());
-    }
     mutateBuilder.setTimestamp(mutation.getTimeStamp());
     Map<String, byte[]> attributes = mutation.getAttributesMap();
     if (!attributes.isEmpty()) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
index 0218f2e..e0ae6a5 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/protobuf/RequestConverter.java
@@ -68,7 +68,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Column;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Condition;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiAction;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate;
@@ -77,7 +76,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.ColumnValu
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate.MutateType;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.CompareType;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
@@ -437,40 +435,6 @@ public final class RequestConverter {
   }
 
   /**
-   * Create a protocol buffer LockRowRequest
-   *
-   * @param regionName
-   * @param row
-   * @return a lock row request
-   */
-  public static LockRowRequest buildLockRowRequest(
-      final byte[] regionName, final byte[] row) {
-    LockRowRequest.Builder builder = LockRowRequest.newBuilder();
-    RegionSpecifier region = buildRegionSpecifier(
-      RegionSpecifierType.REGION_NAME, regionName);
-    builder.setRegion(region);
-    builder.addRow(ByteString.copyFrom(row));
-    return builder.build();
-  }
-
-  /**
-   * Create a protocol buffer UnlockRowRequest
-   *
-   * @param regionName
-   * @param lockId
-   * @return a unlock row request
-   */
-  public static UnlockRowRequest buildUnlockRowRequest(
-      final byte[] regionName, final long lockId) {
-    UnlockRowRequest.Builder builder = UnlockRowRequest.newBuilder();
-    RegionSpecifier region = buildRegionSpecifier(
-      RegionSpecifierType.REGION_NAME, regionName);
-    builder.setRegion(region);
-    builder.setLockId(lockId);
-    return builder.build();
-  }
-
-  /**
    * Create a protocol buffer bulk load request
    *
    * @param familyPaths
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 93d1943..8bb7105 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -62,6 +62,8 @@ import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 
+import javax.lang.model.type.NullType;
+
 import com.google.protobuf.*;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -97,7 +99,6 @@ import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Row;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.RowMutations;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.client.coprocessor.Exec;
@@ -1802,11 +1803,12 @@ public class HRegion implements HeapSize { // , Writable{
   //////////////////////////////////////////////////////////////////////////////
   /**
    * @param delete delete object
-   * @param lockid existing lock id, or null for grab a lock
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @param writeToWAL append to the write ahead lock or not
    * @throws IOException read exceptions
    */
-  public void delete(Delete delete, Integer lockid, boolean writeToWAL)
+  public void delete(Delete delete, NullType lockid, boolean writeToWAL)
   throws IOException {
     checkReadOnly();
     checkResources();
@@ -1816,13 +1818,13 @@ public class HRegion implements HeapSize { // , Writable{
     try {
       byte [] row = delete.getRow();
       // If we did not pass an existing row lock, obtain a new one
-      lid = getLock(lockid, row, true);
+      lid = getLock(null, row, true);
 
       try {
         // All edits for the given row (across all column families) must happen atomically.
         doBatchMutate(delete, lid);
       } finally {
-        if(lockid == null) releaseRowLock(lid);
+        releaseRowLock(lid);
       }
     } finally {
       closeRegionOperation();
@@ -1914,25 +1916,25 @@ public class HRegion implements HeapSize { // , Writable{
   public void put(Put put, boolean writeToWAL) throws IOException {
     this.put(put, null, writeToWAL);
   }
-
+ 
   /**
    * @param put
-   * @param lockid
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @throws IOException
    */
-  public void put(Put put, Integer lockid) throws IOException {
+  public void put(Put put, NullType lockid) throws IOException {
     this.put(put, lockid, put.getWriteToWAL());
   }
 
-
-
   /**
    * @param put
-   * @param lockid
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @param writeToWAL
    * @throws IOException
    */
-  public void put(Put put, Integer lockid, boolean writeToWAL)
+  public void put(Put put, NullType lockid, boolean writeToWAL)
   throws IOException {
     checkReadOnly();
 
@@ -1951,13 +1953,13 @@ public class HRegion implements HeapSize { // , Writable{
       // invokes a HRegion#abort.
       byte [] row = put.getRow();
       // If we did not pass an existing row lock, obtain a new one
-      Integer lid = getLock(lockid, row, true);
+      Integer lid = getLock(null, row, true);
 
       try {
         // All edits for the given row (across all column families) must happen atomically.
         doBatchMutate(put, lid);
       } finally {
-        if(lockid == null) releaseRowLock(lid);
+        releaseRowLock(lid);
       }
     } finally {
       closeRegionOperation();
@@ -2397,14 +2399,16 @@ public class HRegion implements HeapSize { // , Writable{
    * @param qualifier
    * @param compareOp
    * @param comparator
-   * @param lockId
+   * @param w
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @param writeToWAL
    * @throws IOException
    * @return true if the new put was executed, false otherwise
    */
   public boolean checkAndMutate(byte [] row, byte [] family, byte [] qualifier,
       CompareOp compareOp, ByteArrayComparable comparator, Mutation w,
-      Integer lockId, boolean writeToWAL)
+      NullType lockId, boolean writeToWAL)
   throws IOException{
     checkReadOnly();
     //TODO, add check for value length or maybe even better move this to the
@@ -2420,13 +2424,12 @@ public class HRegion implements HeapSize { // , Writable{
 
     startRegionOperation();
     try {
-      RowLock lock = isPut ? ((Put)w).getRowLock() : ((Delete)w).getRowLock();
-      Get get = new Get(row, lock);
+      Get get = new Get(row, null);
       checkFamily(family);
       get.addColumn(family, qualifier);
 
       // Lock row
-      Integer lid = getLock(lockId, get.getRow(), true);
+      Integer lid = getLock(null, get.getRow(), true);
       // wait for all previous transactions to complete (with lock held)
       mvcc.completeMemstoreInsert(mvcc.beginMemstoreInsert());
       List<KeyValue> result = null;
@@ -2479,7 +2482,7 @@ public class HRegion implements HeapSize { // , Writable{
         this.checkAndMutateChecksFailed.increment();
         return false;
       } finally {
-        if(lockId == null) releaseRowLock(lid);
+        releaseRowLock(lid);
       }
     } finally {
       closeRegionOperation();
@@ -3124,13 +3127,8 @@ public class HRegion implements HeapSize { // , Writable{
    * <pre>
    *   LOCKS ==> ROWS
    * </pre>
-   *
-   * But it acts as a guard on the client; a miswritten client just can't
-   * submit the name of a row and start writing to it; it must know the correct
-   * lockid, which matches the lock list in memory.
-   *
-   * <p>It would be more memory-efficient to assume a correctly-written client,
-   * which maybe we'll do in the future.
+   * <p>It would be more memory-efficient to just have one mapping;
+   * maybe we'll do that in the future.
    *
    * @param row Name of row to lock.
    * @throws IOException
@@ -3153,7 +3151,7 @@ public class HRegion implements HeapSize { // , Writable{
    *        null if unavailable.
    */
   private Integer internalObtainRowLock(final byte[] row, boolean waitForLock)
-      throws IOException {
+  throws IOException {
     checkRow(row, "row lock");
     startRegionOperation();
     try {
@@ -3199,16 +3197,6 @@ public class HRegion implements HeapSize { // , Writable{
   }
 
   /**
-   * Used by unit tests.
-   * @param lockid
-   * @return Row that goes with <code>lockid</code>
-   */
-  byte[] getRowFromLock(final Integer lockid) {
-    HashedBytes rowKey = lockIds.get(lockid);
-    return rowKey == null ? null : rowKey.getBytes();
-  }
-
-  /**
    * Release the row lock!
    * @param lockId  The lock ID to release.
    */
@@ -4273,11 +4261,12 @@ public class HRegion implements HeapSize { // , Writable{
   //
   /**
    * @param get get object
-   * @param lockid existing lock id, or null for no previous lock
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @return result
    * @throws IOException read exceptions
    */
-  public Result get(final Get get, final Integer lockid) throws IOException {
+  public Result get(final Get get, final NullType lockid) throws IOException {
     checkRow(get.getRow(), "Get");
     // Verify families are all valid
     if (get.hasFamilies()) {
@@ -4568,12 +4557,13 @@ public class HRegion implements HeapSize { // , Writable{
    * Perform one or more append operations on a row.
    *
    * @param append
-   * @param lockid
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @param writeToWAL
    * @return new keyvalues after increment
    * @throws IOException
    */
-  public Result append(Append append, Integer lockid, boolean writeToWAL)
+  public Result append(Append append, NullType lockid, boolean writeToWAL)
       throws IOException {
     byte[] row = append.getRow();
     checkRow(row, "append");
@@ -4590,7 +4580,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(lockid, row, true);
+      Integer lid = getLock(null, row, true);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
@@ -4732,13 +4722,13 @@ public class HRegion implements HeapSize { // , Writable{
   /**
    * Perform one or more increment operations on a row.
    * @param increment
-   * @param lockid
+   * @param lockid no longer supported.  NullType here so coprocessor code that just
+   * passed in null will still work.
    * @param writeToWAL
    * @return new keyvalues after increment
    * @throws IOException
    */
-  public Result increment(Increment increment, Integer lockid,
-      boolean writeToWAL)
+  public Result increment(Increment increment, NullType lockid, boolean writeToWAL)
   throws IOException {
     byte [] row = increment.getRow();
     checkRow(row, "increment");
@@ -4756,7 +4746,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(lockid, row, true);
+      Integer lid = getLock(null, row, true);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index c480ccd..12b5426 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -77,7 +77,6 @@ import org.apache.hadoop.hbase.ServerLoad;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.Stoppable;
 import org.apache.hadoop.hbase.TableDescriptors;
-import org.apache.hadoop.hbase.UnknownRowLockException;
 import org.apache.hadoop.hbase.UnknownScannerException;
 import org.apache.hadoop.hbase.YouAreDeadException;
 import org.apache.hadoop.hbase.ZNodeClearer;
@@ -152,8 +151,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRe
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.Mutate;
@@ -162,8 +159,6 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameBytesPair;
@@ -595,8 +590,6 @@ public class  HRegionServer implements ClientProtocol,
         GetRequest.class,
         MutateRequest.class,
         ScanRequest.class,
-        LockRowRequest.class,
-        UnlockRowRequest.class,
         MultiRequest.class
     };
 
@@ -2270,28 +2263,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Instantiated as a row lock lease. If the lease times out, the row lock is
-   * released
-   */
-  private class RowLockListener implements LeaseListener {
-    private final String lockName;
-    private final HRegion region;
-
-    RowLockListener(final String lockName, final HRegion region) {
-      this.lockName = lockName;
-      this.region = region;
-    }
-
-    public void leaseExpired() {
-      LOG.info("Row Lock " + this.lockName + " lease expired");
-      Integer r = rowlocks.remove(this.lockName);
-      if (r != null) {
-        region.releaseRowLock(r);
-      }
-    }
-  }
-
-  /**
    * Instantiated as a scanner lease. If the lease times out, the scanner is
    * closed
    */
@@ -2329,29 +2300,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Method to get the Integer lock identifier used internally from the long
-   * lock identifier used by the client.
-   *
-   * @param lockId
-   *          long row lock identifier from client
-   * @return intId Integer row lock used internally in HRegion
-   * @throws IOException
-   *           Thrown if this is not a valid client lock id.
-   */
-  Integer getLockFromId(long lockId) throws IOException {
-    if (lockId == -1L) {
-      return null;
-    }
-    String lockName = String.valueOf(lockId);
-    Integer rl = rowlocks.get(lockName);
-    if (rl == null) {
-      throw new UnknownRowLockException("Invalid row lock");
-    }
-    this.leases.renewLease(lockName);
-    return rl;
-  }
-
-  /**
    * Called to verify that this server is up and running.
    *
    * @throws IOException
@@ -2584,18 +2532,6 @@ public class  HRegionServer implements ClientProtocol,
     return this.fsOk;
   }
 
-  protected long addRowLock(Integer r, HRegion region) throws LeaseStillHeldException {
-    String lockName = null;
-    long lockId;
-    do {
-      lockId = nextLong();
-      lockName = String.valueOf(lockId);
-    } while (rowlocks.putIfAbsent(lockName, r) != null);
-    this.leases.createLease(lockName, this.rowLockLeaseTimeoutPeriod, new RowLockListener(lockName,
-        region));
-    return lockId;
-  }
-
   protected long addScanner(RegionScanner s) throws LeaseStillHeldException {
     long scannerId = -1;
     while (true) {
@@ -2663,8 +2599,7 @@ public class  HRegionServer implements ClientProtocol,
           existence = region.getCoprocessorHost().preExists(clientGet);
         }
         if (existence == null) {
-          Integer lock = getLockFromId(clientGet.getLockId());
-          r = region.get(clientGet, lock);
+          r = region.get(clientGet, null);
           if (request.getExistenceOnly()) {
             boolean exists = r != null && !r.isEmpty();
             if (region.getCoprocessorHost() != null) {
@@ -2705,7 +2640,6 @@ public class  HRegionServer implements ClientProtocol,
       if (!region.getRegionInfo().isMetaTable()) {
         cacheFlusher.reclaimMemStoreMemory();
       }
-      Integer lock = null;
       Result r = null;
       Boolean processed = null;
       MutateType type = mutate.getMutateType();
@@ -2718,7 +2652,6 @@ public class  HRegionServer implements ClientProtocol,
         break;
       case PUT:
         Put put = ProtobufUtil.toPut(mutate);
-        lock = getLockFromId(put.getLockId());
         if (request.hasCondition()) {
           Condition condition = request.getCondition();
           byte[] row = condition.getRow().toByteArray();
@@ -2733,7 +2666,7 @@ public class  HRegionServer implements ClientProtocol,
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, put, lock, true);
+              qualifier, compareOp, comparator, put, null, true);
             if (region.getCoprocessorHost() != null) {
               result = region.getCoprocessorHost().postCheckAndPut(row, family,
                 qualifier, compareOp, comparator, put, result);
@@ -2741,13 +2674,12 @@ public class  HRegionServer implements ClientProtocol,
             processed = Boolean.valueOf(result);
           }
         } else {
-          region.put(put, lock);
+          region.put(put);
           processed = Boolean.TRUE;
         }
         break;
       case DELETE:
         Delete delete = ProtobufUtil.toDelete(mutate);
-        lock = getLockFromId(delete.getLockId());
         if (request.hasCondition()) {
           Condition condition = request.getCondition();
           byte[] row = condition.getRow().toByteArray();
@@ -2762,7 +2694,7 @@ public class  HRegionServer implements ClientProtocol,
           }
           if (processed == null) {
             boolean result = region.checkAndMutate(row, family,
-              qualifier, compareOp, comparator, delete, lock, true);
+              qualifier, compareOp, comparator, delete, null, true);
             if (region.getCoprocessorHost() != null) {
               result = region.getCoprocessorHost().postCheckAndDelete(row, family,
                 qualifier, compareOp, comparator, delete, result);
@@ -2770,7 +2702,7 @@ public class  HRegionServer implements ClientProtocol,
             processed = Boolean.valueOf(result);
           }
         } else {
-          region.delete(delete, lock, delete.getWriteToWAL());
+          region.delete(delete, null, delete.getWriteToWAL());
           processed = Boolean.TRUE;
         }
         break;
@@ -3005,78 +2937,6 @@ public class  HRegionServer implements ClientProtocol,
   }
 
   /**
-   * Lock a row in a table.
-   *
-   * @param controller the RPC controller
-   * @param request the lock row request
-   * @throws ServiceException
-   */
-  @Override
-  public LockRowResponse lockRow(final RpcController controller,
-      final LockRowRequest request) throws ServiceException {
-    try {
-      if (request.getRowCount() != 1) {
-        throw new DoNotRetryIOException(
-          "lockRow supports only one row now, not " + request.getRowCount() + " rows");
-      }
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      byte[] row = request.getRow(0).toByteArray();
-      try {
-        Integer r = region.obtainRowLock(row);
-        long lockId = addRowLock(r, region);
-        LOG.debug("Row lock " + lockId + " explicitly acquired by client");
-        LockRowResponse.Builder builder = LockRowResponse.newBuilder();
-        builder.setLockId(lockId);
-        return builder.build();
-      } catch (Throwable t) {
-        throw convertThrowableToIOE(cleanup(t,
-          "Error obtaining row lock (fsOk: " + this.fsOk + ")"));
-      }
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
-   * Unlock a locked row in a table.
-   *
-   * @param controller the RPC controller
-   * @param request the unlock row request
-   * @throws ServiceException
-   */
-  @Override
-  @QosPriority(priority=HConstants.HIGH_QOS)
-  public UnlockRowResponse unlockRow(final RpcController controller,
-      final UnlockRowRequest request) throws ServiceException {
-    try {
-      requestCount.increment();
-      HRegion region = getRegion(request.getRegion());
-      if (!request.hasLockId()) {
-        throw new DoNotRetryIOException(
-          "Invalid unlock rowrequest, missing lock id");
-      }
-      long lockId = request.getLockId();
-      String lockName = String.valueOf(lockId);
-      try {
-        Integer r = rowlocks.remove(lockName);
-        if (r == null) {
-          throw new UnknownRowLockException(lockName);
-        }
-        region.releaseRowLock(r);
-        this.leases.cancelLease(lockName);
-        LOG.debug("Row lock " + lockId
-          + " has been explicitly released by client");
-        return UnlockRowResponse.newBuilder().build();
-      } catch (Throwable t) {
-        throw convertThrowableToIOE(cleanup(t));
-      }
-    } catch (IOException ie) {
-      throw new ServiceException(ie);
-    }
-  }
-
-  /**
    * Atomically bulk load several HFiles into an open region
    * @return true if successful, false is failed but recoverably (no action)
    * @throws IOException if failed unrecoverably
@@ -3204,8 +3064,7 @@ public class  HRegionServer implements ClientProtocol,
             Object result = null;
             if (actionUnion.hasGet()) {
               Get get = ProtobufUtil.toGet(actionUnion.getGet());
-              Integer lock = getLockFromId(get.getLockId());
-              Result r = region.get(get, lock);
+              Result r = region.get(get, null);
               if (r != null) {
                 result = ProtobufUtil.toResult(r);
               }
@@ -3710,8 +3569,7 @@ public class  HRegionServer implements ClientProtocol,
       r = region.getCoprocessorHost().preAppend(append);
     }
     if (r == null) {
-      Integer lock = getLockFromId(append.getLockId());
-      r = region.append(append, lock, append.getWriteToWAL());
+      r = region.append(append, null, append.getWriteToWAL());
       if (region.getCoprocessorHost() != null) {
         region.getCoprocessorHost().postAppend(append, r);
       }
@@ -3737,8 +3595,7 @@ public class  HRegionServer implements ClientProtocol,
       r = region.getCoprocessorHost().preIncrement(increment);
     }
     if (r == null) {
-      Integer lock = getLockFromId(increment.getLockId());
-      r = region.increment(increment, lock, increment.getWriteToWAL());
+      r = region.increment(increment, null, increment.getWriteToWAL());
       if (region.getCoprocessorHost() != null) {
         r = region.getCoprocessorHost().postIncrement(increment, r);
       }
@@ -3776,8 +3633,7 @@ public class  HRegionServer implements ClientProtocol,
           mutation = ProtobufUtil.toDelete(m);
           batchContainsDelete = true;
         }
-        Integer lock = getLockFromId(mutation.getLockId());
-        mutationsWithLocks[i++] = new Pair<Mutation, Integer>(mutation, lock);
+        mutationsWithLocks[i++] = new Pair<Mutation, Integer>(mutation, null);
         builder.addResult(result);
       }
 
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
index beebe96..211c54c 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java
@@ -55,7 +55,6 @@ import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Row;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.rest.Constants;
@@ -582,14 +581,6 @@ public class RemoteHTable implements HTableInterface {
     throw new IOException("getRowOrBefore not supported");
   }
 
-  public RowLock lockRow(byte[] row) throws IOException {
-    throw new IOException("lockRow not implemented");
-  }
-
-  public void unlockRow(RowLock rl) throws IOException {
-    throw new IOException("unlockRow not implemented");
-  }
-
   public boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier,
       byte[] value, Put put) throws IOException {
     // column to check-the-value
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
index 322e198..2486050 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
@@ -23,6 +23,8 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.NavigableMap;
 
+import javax.lang.model.type.NullType;
+
 import junit.framework.AssertionFailedError;
 import junit.framework.TestCase;
 
@@ -407,7 +409,7 @@ public abstract class HBaseTestCase extends TestCase {
      * @param writeToWAL
      * @throws IOException
      */
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete,  NullType lockid, boolean writeToWAL)
     throws IOException;
 
     /**
@@ -448,7 +450,7 @@ public abstract class HBaseTestCase extends TestCase {
       region.put(put);
     }
 
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete,  NullType lockid, boolean writeToWAL)
     throws IOException {
       this.region.delete(delete, lockid, writeToWAL);
     }
@@ -473,7 +475,7 @@ public abstract class HBaseTestCase extends TestCase {
           InternalScannerIncommon(region.getScanner(scan));
       }
 
-    public Result get(Get get, Integer lockid) throws IOException{
+    public Result get(Get get, NullType lockid) throws IOException{
       return this.region.get(get, lockid);
     }
 
@@ -502,7 +504,7 @@ public abstract class HBaseTestCase extends TestCase {
     }
 
 
-    public void delete(Delete delete,  Integer lockid, boolean writeToWAL)
+    public void delete(Delete delete,  NullType lockid, boolean writeToWAL)
     throws IOException {
       this.table.delete(delete);
     }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
index 5494240..b318c20 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestSerialization.java
@@ -29,7 +29,6 @@ import java.util.NavigableSet;
 import java.util.Set;
 
 import org.apache.hadoop.hbase.client.Get;
-import org.apache.hadoop.hbase.client.RowLock;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
@@ -245,10 +244,8 @@ public class TestSerialization {
 
     long ts = System.currentTimeMillis();
     int maxVersions = 2;
-    long lockid = 5;
-    RowLock rowLock = new RowLock(lockid);
 
-    Get get = new Get(row, rowLock);
+    Get get = new Get(row, null);
     get.addColumn(fam, qf1);
     get.setTimeRange(ts, ts+1);
     get.setMaxVersions(maxVersions);
@@ -270,7 +267,6 @@ public class TestSerialization {
       }
     }
 
-    assertEquals(get.getLockId(), desGet.getLockId());
     assertEquals(get.getMaxVersions(), desGet.getMaxVersions());
     TimeRange tr = get.getTimeRange();
     TimeRange desTr = desGet.getTimeRange();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
index e3075d2..7ab9ce5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverStacking.java
@@ -125,9 +125,7 @@ public class TestRegionObserverStacking extends TestCase {
 
     Put put = new Put(ROW);
     put.add(A, A, A);
-    int lockid = region.obtainRowLock(ROW);
-    region.put(put, lockid);
-    region.releaseRowLock(lockid);
+    region.put(put, null);
 
     Coprocessor c = h.findCoprocessor(ObserverA.class.getName());
     long idA = ((ObserverA)c).id;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
index 7def888..2822a41 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/master/MockRegionServer.java
@@ -69,15 +69,11 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorRe
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ExecCoprocessorResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.GetResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.LockRowResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MultiRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.MutateResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowRequest;
-import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.UnlockRowResponse;
 import org.apache.hadoop.hbase.regionserver.CompactionRequestor;
 import org.apache.hadoop.hbase.regionserver.FlushRequester;
 import org.apache.hadoop.hbase.regionserver.HRegion;
@@ -386,20 +382,6 @@ class MockRegionServer implements AdminProtocol, ClientProtocol, RegionServerSer
   }
 
   @Override
-  public LockRowResponse lockRow(RpcController controller,
-      LockRowRequest request) throws ServiceException {
-    // TODO Auto-generated method stub
-    return null;
-  }
-
-  @Override
-  public UnlockRowResponse unlockRow(RpcController controller,
-      UnlockRowRequest request) throws ServiceException {
-    // TODO Auto-generated method stub
-    return null;
-  }
-
-  @Override
   public BulkLoadHFileResponse bulkLoadHFile(RpcController controller,
       BulkLoadHFileRequest request) throws ServiceException {
     // TODO Auto-generated method stub
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 2ba5490..98d1346 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -770,7 +770,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] emptyVal  = new byte[] {};
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -782,7 +781,7 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with empty value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, null, true);
       assertTrue(res);
 
       //Putting data in key
@@ -791,25 +790,25 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, null, true);
       assertTrue(res);
 
       // not empty anymore
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), put, lockId, true);
+          new BinaryComparator(emptyVal), put, null, true);
       assertFalse(res);
 
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, null, true);
       assertFalse(res);
 
       put = new Put(row1);
       put.add(fam1, qf1, val2);
       //checkAndPut with correct value
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, null, true);
       assertTrue(res);
 
       //checkAndDelete with correct value
@@ -817,12 +816,12 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam1, qf1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, null, true);
       assertTrue(res);
 
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, null, true);
       assertTrue(res);
 
       //checkAndPut looking for a null value
@@ -830,7 +829,7 @@ public class TestHRegion extends HBaseTestCase {
       put.add(fam1, qf1, val1);
 
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new NullComparator(), put, lockId, true);
+          new NullComparator(), put, null, true);
       assertTrue(res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -845,7 +844,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -858,14 +856,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with wrong value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), put, lockId, true);
+          new BinaryComparator(val2), put, null, true);
       assertEquals(false, res);
 
       //checkAndDelete with wrong value
       Delete delete = new Delete(row1);
       delete.deleteFamily(fam1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, null, true);
       assertEquals(false, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -879,7 +877,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] fam1 = Bytes.toBytes("fam1");
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
-    Integer lockId = null;
 
     //Setting up region
     String method = this.getName();
@@ -892,14 +889,14 @@ public class TestHRegion extends HBaseTestCase {
 
       //checkAndPut with correct value
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, null, true);
       assertEquals(true, res);
 
       //checkAndDelete with correct value
       Delete delete = new Delete(row1);
       delete.deleteColumn(fam1, qf1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, null, true);
       assertEquals(true, res);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -915,7 +912,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] qf1  = Bytes.toBytes("qualifier");
     byte [] val1  = Bytes.toBytes("value1");
     byte [] val2  = Bytes.toBytes("value2");
-    Integer lockId = null;
 
     byte [][] families = {fam1, fam2};
 
@@ -939,7 +935,7 @@ public class TestHRegion extends HBaseTestCase {
       store.memstore.kvset.size();
 
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), put, lockId, true);
+          new BinaryComparator(val1), put, null, true);
       assertEquals(true, res);
       store.memstore.kvset.size();
 
@@ -989,7 +985,6 @@ public class TestHRegion extends HBaseTestCase {
     byte [] val2  = Bytes.toBytes("value2");
     byte [] val3  = Bytes.toBytes("value3");
     byte[] emptyVal = new byte[] { };
-    Integer lockId = null;
 
     byte [][] families = {fam1, fam2};
 
@@ -1017,7 +1012,7 @@ public class TestHRegion extends HBaseTestCase {
       delete.deleteColumn(fam2, qf1);
       delete.deleteColumn(fam1, qf3);
       boolean res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val2), delete, lockId, true);
+          new BinaryComparator(val2), delete, null, true);
       assertEquals(true, res);
 
       Get get = new Get(row1);
@@ -1033,7 +1028,7 @@ public class TestHRegion extends HBaseTestCase {
       delete = new Delete(row1);
       delete.deleteFamily(fam2);
       res = region.checkAndMutate(row1, fam2, qf1, CompareOp.EQUAL,
-          new BinaryComparator(emptyVal), delete, lockId, true);
+          new BinaryComparator(emptyVal), delete, null, true);
       assertEquals(true, res);
 
       get = new Get(row1);
@@ -1044,7 +1039,7 @@ public class TestHRegion extends HBaseTestCase {
       //Row delete
       delete = new Delete(row1);
       res = region.checkAndMutate(row1, fam1, qf1, CompareOp.EQUAL,
-          new BinaryComparator(val1), delete, lockId, true);
+          new BinaryComparator(val1), delete, null, true);
       assertEquals(true, res);
       get = new Get(row1);
       r = region.get(get, null);
@@ -1651,74 +1646,6 @@ public class TestHRegion extends HBaseTestCase {
   }
 
   //////////////////////////////////////////////////////////////////////////////
-  // Lock test
-  //////////////////////////////////////////////////////////////////////////////
-  public void testLocks() throws IOException{
-    byte [] tableName = Bytes.toBytes("testtable");
-    byte [][] families = {fam1, fam2, fam3};
-
-    Configuration hc = initSplit();
-    //Setting up region
-    String method = this.getName();
-    this.region = initHRegion(tableName, method, hc, families);
-    try {
-      final int threadCount = 10;
-      final int lockCount = 10;
-
-      List<Thread>threads = new ArrayList<Thread>(threadCount);
-      for (int i = 0; i < threadCount; i++) {
-        threads.add(new Thread(Integer.toString(i)) {
-          @Override
-          public void run() {
-            Integer [] lockids = new Integer[lockCount];
-            // Get locks.
-            for (int i = 0; i < lockCount; i++) {
-              try {
-                byte [] rowid = Bytes.toBytes(Integer.toString(i));
-                lockids[i] = region.obtainRowLock(rowid);
-                assertEquals(rowid, region.getRowFromLock(lockids[i]));
-                LOG.debug(getName() + " locked " + Bytes.toString(rowid));
-              } catch (IOException e) {
-                e.printStackTrace();
-              }
-            }
-            LOG.debug(getName() + " set " +
-                Integer.toString(lockCount) + " locks");
-
-            // Abort outstanding locks.
-            for (int i = lockCount - 1; i >= 0; i--) {
-              region.releaseRowLock(lockids[i]);
-              LOG.debug(getName() + " unlocked " + i);
-            }
-            LOG.debug(getName() + " released " +
-                Integer.toString(lockCount) + " locks");
-          }
-        });
-      }
-
-      // Startup all our threads.
-      for (Thread t : threads) {
-        t.start();
-      }
-
-      // Now wait around till all are done.
-      for (Thread t: threads) {
-        while (t.isAlive()) {
-          try {
-            Thread.sleep(1);
-          } catch (InterruptedException e) {
-            // Go around again.
-          }
-        }
-      }
-      LOG.info("locks completed.");
-    } finally {
-      HRegion.closeHRegion(this.region);
-      this.region = null;
-    }
-  }
-
-  //////////////////////////////////////////////////////////////////////////////
   // Merge test
   //////////////////////////////////////////////////////////////////////////////
   public void testMerge() throws IOException {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
index 2adbf04..24db267 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/wal/TestWALReplay.java
@@ -612,7 +612,7 @@ public class TestWALReplay {
           assertTrue(seqid > wal.getSequenceNumber());
 
           Get get = new Get(rowName);
-          Result result = region.get(get, -1);
+          Result result = region.get(get, null);
           // Make sure we only see the good edits
           assertEquals(countPerFamily * (htd.getFamilies().size() - 1),
             result.size());
