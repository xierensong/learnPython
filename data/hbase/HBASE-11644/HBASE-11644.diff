diff --git a/hbase-common/src/main/resources/hbase-default.xml b/hbase-common/src/main/resources/hbase-default.xml
index c9d1563..6973bb4 100644
--- a/hbase-common/src/main/resources/hbase-default.xml
+++ b/hbase-common/src/main/resources/hbase-default.xml
@@ -1372,4 +1372,30 @@ possible configurations would overwhelm and obscure the important.
     <name>hbase.mob.file.cache.size</name>
     <value>1000</value>
   </property>
+  <property>
+    <description>
+      If there're too many cells deleted in a mob file, it's regarded
+      as a invalid file and needs to be re-written/merged.
+      If (mobFileSize-existingCellsSize)/mobFileSize>=ratio, it's regarded
+      as a invalid file. The default value is 0.3f.
+    </description>
+    <name>hbase.mob.compaction.invalid.file.ratio</name>
+    <value>0.3f</value>
+  </property>
+  <property>
+    <description>
+      If the size of a mob is less than the threshold, it's regarded as a small
+      file and needs to be merged. The default value is 64MB.
+    </description>
+    <name>hbase.mob.compaction.small.file.threshold</name>
+    <value>67108864</value>
+  </property>
+  <property>
+    <description>
+      The flush size for the memstore used by sweep job. Each sweep reducer owns such a memstore.
+      The default value is 128MB.
+    </description>
+    <name>hbase.mob.compaction.memstore.flush.size</name>
+    <value>134217728</value>
+  </property>
 </configuration>
diff --git a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java
index 9b2ce5e..dcd850c 100644
--- a/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java
+++ b/hbase-protocol/src/main/java/org/apache/hadoop/hbase/protobuf/generated/FilterProtos.java
@@ -15881,6 +15881,344 @@ public Builder mergeFrom(
     // @@protoc_insertion_point(class_scope:FilterAllFilter)
   }
 
+  public interface ReferenceOnlyFilterOrBuilder
+      extends com.google.protobuf.MessageOrBuilder {
+  }
+  /**
+   * Protobuf type {@code ReferenceOnlyFilter}
+   */
+  public static final class ReferenceOnlyFilter extends
+      com.google.protobuf.GeneratedMessage
+      implements ReferenceOnlyFilterOrBuilder {
+    // Use ReferenceOnlyFilter.newBuilder() to construct.
+    private ReferenceOnlyFilter(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
+      super(builder);
+      this.unknownFields = builder.getUnknownFields();
+    }
+    private ReferenceOnlyFilter(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
+
+    private static final ReferenceOnlyFilter defaultInstance;
+    public static ReferenceOnlyFilter getDefaultInstance() {
+      return defaultInstance;
+    }
+
+    public ReferenceOnlyFilter getDefaultInstanceForType() {
+      return defaultInstance;
+    }
+
+    private final com.google.protobuf.UnknownFieldSet unknownFields;
+    @java.lang.Override
+    public final com.google.protobuf.UnknownFieldSet
+        getUnknownFields() {
+      return this.unknownFields;
+    }
+    private ReferenceOnlyFilter(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      initFields();
+      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
+          com.google.protobuf.UnknownFieldSet.newBuilder();
+      try {
+        boolean done = false;
+        while (!done) {
+          int tag = input.readTag();
+          switch (tag) {
+            case 0:
+              done = true;
+              break;
+            default: {
+              if (!parseUnknownField(input, unknownFields,
+                                     extensionRegistry, tag)) {
+                done = true;
+              }
+              break;
+            }
+          }
+        }
+      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+        throw e.setUnfinishedMessage(this);
+      } catch (java.io.IOException e) {
+        throw new com.google.protobuf.InvalidProtocolBufferException(
+            e.getMessage()).setUnfinishedMessage(this);
+      } finally {
+        this.unknownFields = unknownFields.build();
+        makeExtensionsImmutable();
+      }
+    }
+    public static final com.google.protobuf.Descriptors.Descriptor
+        getDescriptor() {
+      return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.internal_static_ReferenceOnlyFilter_descriptor;
+    }
+
+    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+        internalGetFieldAccessorTable() {
+      return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.internal_static_ReferenceOnlyFilter_fieldAccessorTable
+          .ensureFieldAccessorsInitialized(
+              org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.class, org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.Builder.class);
+    }
+
+    public static com.google.protobuf.Parser<ReferenceOnlyFilter> PARSER =
+        new com.google.protobuf.AbstractParser<ReferenceOnlyFilter>() {
+      public ReferenceOnlyFilter parsePartialFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws com.google.protobuf.InvalidProtocolBufferException {
+        return new ReferenceOnlyFilter(input, extensionRegistry);
+      }
+    };
+
+    @java.lang.Override
+    public com.google.protobuf.Parser<ReferenceOnlyFilter> getParserForType() {
+      return PARSER;
+    }
+
+    private void initFields() {
+    }
+    private byte memoizedIsInitialized = -1;
+    public final boolean isInitialized() {
+      byte isInitialized = memoizedIsInitialized;
+      if (isInitialized != -1) return isInitialized == 1;
+
+      memoizedIsInitialized = 1;
+      return true;
+    }
+
+    public void writeTo(com.google.protobuf.CodedOutputStream output)
+                        throws java.io.IOException {
+      getSerializedSize();
+      getUnknownFields().writeTo(output);
+    }
+
+    private int memoizedSerializedSize = -1;
+    public int getSerializedSize() {
+      int size = memoizedSerializedSize;
+      if (size != -1) return size;
+
+      size = 0;
+      size += getUnknownFields().getSerializedSize();
+      memoizedSerializedSize = size;
+      return size;
+    }
+
+    private static final long serialVersionUID = 0L;
+    @java.lang.Override
+    protected java.lang.Object writeReplace()
+        throws java.io.ObjectStreamException {
+      return super.writeReplace();
+    }
+
+    @java.lang.Override
+    public boolean equals(final java.lang.Object obj) {
+      if (obj == this) {
+       return true;
+      }
+      if (!(obj instanceof org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter)) {
+        return super.equals(obj);
+      }
+      org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter other = (org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter) obj;
+
+      boolean result = true;
+      result = result &&
+          getUnknownFields().equals(other.getUnknownFields());
+      return result;
+    }
+
+    private int memoizedHashCode = 0;
+    @java.lang.Override
+    public int hashCode() {
+      if (memoizedHashCode != 0) {
+        return memoizedHashCode;
+      }
+      int hash = 41;
+      hash = (19 * hash) + getDescriptorForType().hashCode();
+      hash = (29 * hash) + getUnknownFields().hashCode();
+      memoizedHashCode = hash;
+      return hash;
+    }
+
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        com.google.protobuf.ByteString data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        com.google.protobuf.ByteString data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(byte[] data)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        byte[] data,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws com.google.protobuf.InvalidProtocolBufferException {
+      return PARSER.parseFrom(data, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseDelimitedFrom(java.io.InputStream input)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseDelimitedFrom(
+        java.io.InputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseDelimitedFrom(input, extensionRegistry);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        com.google.protobuf.CodedInputStream input)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input);
+    }
+    public static org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parseFrom(
+        com.google.protobuf.CodedInputStream input,
+        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+        throws java.io.IOException {
+      return PARSER.parseFrom(input, extensionRegistry);
+    }
+
+    public static Builder newBuilder() { return Builder.create(); }
+    public Builder newBuilderForType() { return newBuilder(); }
+    public static Builder newBuilder(org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter prototype) {
+      return newBuilder().mergeFrom(prototype);
+    }
+    public Builder toBuilder() { return newBuilder(this); }
+
+    @java.lang.Override
+    protected Builder newBuilderForType(
+        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+      Builder builder = new Builder(parent);
+      return builder;
+    }
+    /**
+     * Protobuf type {@code ReferenceOnlyFilter}
+     */
+    public static final class Builder extends
+        com.google.protobuf.GeneratedMessage.Builder<Builder>
+       implements org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilterOrBuilder {
+      public static final com.google.protobuf.Descriptors.Descriptor
+          getDescriptor() {
+        return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.internal_static_ReferenceOnlyFilter_descriptor;
+      }
+
+      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
+          internalGetFieldAccessorTable() {
+        return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.internal_static_ReferenceOnlyFilter_fieldAccessorTable
+            .ensureFieldAccessorsInitialized(
+                org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.class, org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.Builder.class);
+      }
+
+      // Construct using org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.newBuilder()
+      private Builder() {
+        maybeForceBuilderInitialization();
+      }
+
+      private Builder(
+          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
+        super(parent);
+        maybeForceBuilderInitialization();
+      }
+      private void maybeForceBuilderInitialization() {
+        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
+        }
+      }
+      private static Builder create() {
+        return new Builder();
+      }
+
+      public Builder clear() {
+        super.clear();
+        return this;
+      }
+
+      public Builder clone() {
+        return create().mergeFrom(buildPartial());
+      }
+
+      public com.google.protobuf.Descriptors.Descriptor
+          getDescriptorForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.internal_static_ReferenceOnlyFilter_descriptor;
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter getDefaultInstanceForType() {
+        return org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.getDefaultInstance();
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter build() {
+        org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter result = buildPartial();
+        if (!result.isInitialized()) {
+          throw newUninitializedMessageException(result);
+        }
+        return result;
+      }
+
+      public org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter buildPartial() {
+        org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter result = new org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter(this);
+        onBuilt();
+        return result;
+      }
+
+      public Builder mergeFrom(com.google.protobuf.Message other) {
+        if (other instanceof org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter) {
+          return mergeFrom((org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter)other);
+        } else {
+          super.mergeFrom(other);
+          return this;
+        }
+      }
+
+      public Builder mergeFrom(org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter other) {
+        if (other == org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter.getDefaultInstance()) return this;
+        this.mergeUnknownFields(other.getUnknownFields());
+        return this;
+      }
+
+      public final boolean isInitialized() {
+        return true;
+      }
+
+      public Builder mergeFrom(
+          com.google.protobuf.CodedInputStream input,
+          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
+          throws java.io.IOException {
+        org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter parsedMessage = null;
+        try {
+          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
+        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
+          parsedMessage = (org.apache.hadoop.hbase.protobuf.generated.FilterProtos.ReferenceOnlyFilter) e.getUnfinishedMessage();
+          throw e;
+        } finally {
+          if (parsedMessage != null) {
+            mergeFrom(parsedMessage);
+          }
+        }
+        return this;
+      }
+
+      // @@protoc_insertion_point(builder_scope:ReferenceOnlyFilter)
+    }
+
+    static {
+      defaultInstance = new ReferenceOnlyFilter(true);
+      defaultInstance.initFields();
+    }
+
+    // @@protoc_insertion_point(class_scope:ReferenceOnlyFilter)
+  }
+
   private static com.google.protobuf.Descriptors.Descriptor
     internal_static_Filter_descriptor;
   private static
@@ -16021,6 +16359,11 @@ public Builder mergeFrom(
   private static
     com.google.protobuf.GeneratedMessage.FieldAccessorTable
       internal_static_FilterAllFilter_fieldAccessorTable;
+  private static com.google.protobuf.Descriptors.Descriptor
+    internal_static_ReferenceOnlyFilter_descriptor;
+  private static
+    com.google.protobuf.GeneratedMessage.FieldAccessorTable
+      internal_static_ReferenceOnlyFilter_fieldAccessorTable;
 
   public static com.google.protobuf.Descriptors.FileDescriptor
       getDescriptor() {
@@ -16075,9 +16418,9 @@ public Builder mergeFrom(
       "\022\026\n\ntimestamps\030\001 \003(\003B\002\020\001\"5\n\013ValueFilter\022" +
       "&\n\016compare_filter\030\001 \002(\0132\016.CompareFilter\"" +
       "+\n\020WhileMatchFilter\022\027\n\006filter\030\001 \002(\0132\007.Fi" +
-      "lter\"\021\n\017FilterAllFilterBB\n*org.apache.ha" +
-      "doop.hbase.protobuf.generatedB\014FilterPro" +
-      "tosH\001\210\001\001\240\001\001"
+      "lter\"\021\n\017FilterAllFilter\"\025\n\023ReferenceOnly" +
+      "FilterBB\n*org.apache.hadoop.hbase.protob" +
+      "uf.generatedB\014FilterProtosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -16252,6 +16595,12 @@ public Builder mergeFrom(
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_FilterAllFilter_descriptor,
               new java.lang.String[] { });
+          internal_static_ReferenceOnlyFilter_descriptor =
+            getDescriptor().getMessageTypes().get(28);
+          internal_static_ReferenceOnlyFilter_fieldAccessorTable = new
+            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
+              internal_static_ReferenceOnlyFilter_descriptor,
+              new java.lang.String[] { });
           return null;
         }
       };
diff --git a/hbase-protocol/src/main/protobuf/Filter.proto b/hbase-protocol/src/main/protobuf/Filter.proto
index d5c51a4..317de88 100644
--- a/hbase-protocol/src/main/protobuf/Filter.proto
+++ b/hbase-protocol/src/main/protobuf/Filter.proto
@@ -155,4 +155,7 @@ message WhileMatchFilter {
   required Filter filter = 1;
 }
 message FilterAllFilter {
+}
+
+message ReferenceOnlyFilter {
 }
\ No newline at end of file
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java
new file mode 100644
index 0000000..dfbcd21
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/ExpiredMobFileCleanerChore.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.master;
+
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.Chore;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableDescriptors;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.mob.compactions.ExpiredMobFileCleaner;
+
+/**
+ * The Class ExpiredMobFileCleanerChore for running cleaner regularly to remove the expired
+ * mob files.
+ */
+public class ExpiredMobFileCleanerChore extends Chore {
+
+  private static final Log LOG = LogFactory.getLog(ExpiredMobFileCleanerChore.class);
+  private final HMaster master;
+  private ExpiredMobFileCleaner cleaner;
+  private static final int DEFAULT_INTERVAL = 24 * 60 * 60 * 1000;
+
+  public ExpiredMobFileCleanerChore(HMaster master) {
+    super(master.getServerName() + "-ExpiredMobFileChore", master.getConfiguration().getInt(
+        "hbase.expired.mobfile.cleaner.interval", DEFAULT_INTERVAL), master);
+    this.master = master;
+    cleaner = new ExpiredMobFileCleaner();
+  }
+
+  @Override
+  protected void chore() {
+    try {
+      TableDescriptors htds = master.getTableDescriptors();
+      Map<String, HTableDescriptor> map = htds.getAll();
+      for (HTableDescriptor htd : map.values()) {
+        for (HColumnDescriptor hcd : htd.getColumnFamilies()) {
+          if (MobUtils.isMobFamily(hcd)) {
+            cleaner
+                .run(new String[] { htd.getTableName().getNameAsString(), hcd.getNameAsString() });
+          }
+        }
+      }
+    } catch (Exception e) {
+      LOG.error("Fail to clean the expired mob files", e);
+    }
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
index 0c87e63..cbfe3de 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/HMaster.java
@@ -209,6 +209,7 @@
   CatalogJanitor catalogJanitorChore;
   private LogCleaner logCleaner;
   private HFileCleaner hfileCleaner;
+  private ExpiredMobFileCleanerChore expiredMobFileCleanerChore;
 
   MasterCoprocessorHost cpHost;
 
@@ -607,6 +608,9 @@ private void finishActiveMasterInitialization(MonitoredTask status)
     // master initialization. See HBASE-5916.
     this.serverManager.clearDeadServersWithSameHostNameAndPortOfOnlineServer();
 
+    this.expiredMobFileCleanerChore = new ExpiredMobFileCleanerChore(this);
+    Threads.setDaemonThreadRunning(expiredMobFileCleanerChore.getThread());
+
     if (this.cpHost != null) {
       // don't let cp initialization errors kill the master
       try {
@@ -880,6 +884,9 @@ protected void stopServiceThreads() {
   }
 
   private void stopChores() {
+    if (this.expiredMobFileCleanerChore != null) {
+      this.expiredMobFileCleanerChore.interrupt();
+    }
     if (this.balancerChore != null) {
       this.balancerChore.interrupt();
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java
index 0659ac2..0a587e6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobConstants.java
@@ -40,6 +40,24 @@
   public static final String MOB_REGION_NAME = ".mob";
   public static final byte[] MOB_REGION_NAME_BYTES = Bytes.toBytes(MOB_REGION_NAME);
 
+  public static final String MOB_CLEAN_DELAY = "hbase.mob.cleaner.delay";
+  public static final String MOB_COMPACTION_START_DATE = "hbase.mob.compaction.start.date";
+
+  public static final String MOB_COMPACTION_INVALID_FILE_RATIO = 
+      "hbase.mob.compaction.invalid.file.ratio";
+  public static final String MOB_COMPACTION_SMALL_FILE_THRESHOLD = 
+      "hbase.mob.compaction.small.file.threshold";
+
+  public static final float DEFAULT_MOB_COMPACTION_INVALID_FILE_RATIO = 0.3f;
+  public static final long DEFAULT_MOB_COMPACTION_SMALL_FILE_THRESHOLD = 67108864; // 64M
+
+  public static final String DEFAULT_MOB_COMPACTION_TEMP_DIR_NAME = "mobcompaction";
+
+  public static final String MOB_COMPACTION_MEMSTORE_FLUSH_SIZE = 
+      "hbase.mob.compaction.memstore.flush.size";
+
+  public static final String MOB_COMPACTION_JOB_WORKING_BULK_LOAD_DIR_NAME = "bulkLoad";
+
   private MobConstants() {
 
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
index f905020..ef0e4a6 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobUtils.java
@@ -43,6 +43,8 @@
 import org.apache.hadoop.hbase.TagType;
 import org.apache.hadoop.hbase.backup.HFileArchiver;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.io.HFileLink;
+import org.apache.hadoop.hbase.regionserver.BloomType;
 import org.apache.hadoop.hbase.regionserver.MobFileStore;
 import org.apache.hadoop.hbase.regionserver.StoreFile;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -57,6 +59,7 @@
 public class MobUtils {
 
   private static final Log LOG = LogFactory.getLog(MobUtils.class);
+  private static final long ONE_HOUR = 60 * 60 * 1000; // 1 hour
 
   private static final ThreadLocal<SimpleDateFormat> LOCAL_FORMAT =
       new ThreadLocal<SimpleDateFormat>() {
@@ -88,6 +91,17 @@ public static long getMobThreshold(HColumnDescriptor hcd) {
   }
 
   /**
+   * Gets the clean delay for the expired mob files. The default is 1 hour, and unit
+   * is milliseconds.
+   * When the creationTime(of a mob file) < current - TTL - delay, it's expired,
+   * @param conf The current configuration.
+   * @return The clean delay. The default is 1 hour.
+   */
+  public static long getCleanDelayOfExpiredMobFiles(Configuration conf) {
+    return conf.getLong(MobConstants.MOB_CLEAN_DELAY, ONE_HOUR);
+  }
+
+  /**
    * Formats a date to a string.
    * @param date The date.
    * @return The string format of the date, it's yyyymmdd.
@@ -181,6 +195,92 @@ public static void setCacheMobBlocks(Scan scan, boolean cacheBlocks) {
   }
 
   /**
+   * Cleans the expired mob files.
+   * Cleans the files whose creation date is older than (current - columnFamily.ttl), and
+   * the minVersions of that column family is 0.
+   * @param fs The current file system.
+   * @param store The current MobFileStore.
+   * @param current The current time.
+   * @throws IOException
+   */
+  public static void cleanExpiredMobFiles(FileSystem fs, MobFileStore store, long current)
+      throws IOException {
+    HColumnDescriptor columnDescriptor = store.getColumnDescriptor();
+    long timeToLive = columnDescriptor.getTimeToLive();
+    if (Integer.MAX_VALUE == timeToLive) {
+      // no need to clean, because the TTL is not set.
+      return;
+    }
+
+    long cleanDelay = getCleanDelayOfExpiredMobFiles(store.getConfiguration());
+    Date expireDate = new Date(current - timeToLive * 1000 - cleanDelay);
+    expireDate = new Date(expireDate.getYear(), expireDate.getMonth(), expireDate.getDate());
+    LOG.info("MOB HFiles older than " + expireDate.toGMTString() + " will be deleted!");
+
+    FileStatus[] stats = null;
+    if (fs.exists(store.getPath())) {
+      try {
+        stats = fs.listStatus(store.getPath());
+      } catch (FileNotFoundException e) {
+      }
+    }
+    if (null == stats) {
+      // no file found
+      return;
+    }
+    List<StoreFile> storeFiles = new ArrayList<StoreFile>();
+    for (FileStatus file : stats) {
+      String fileName = file.getPath().getName();
+      try {
+        MobFileName mobFileName = null;
+        if (!HFileLink.isHFileLink(file.getPath())) {
+          mobFileName = MobFileName.create(fileName);
+        } else {
+          HFileLink hfileLink = new HFileLink(store.getConfiguration(), file.getPath());
+          mobFileName = MobFileName.create(hfileLink.getOriginPath().getName());
+        }
+        Date fileDate = parseDate(mobFileName.getDate());
+        LOG.info("[MOB] Checking file " + fileName);
+        if (fileDate.getTime() < expireDate.getTime()) {
+          LOG.info("[MOB] Delete expired file " + fileName);
+          storeFiles.add(new StoreFile(fs, file.getPath(), store.getConfiguration(), store
+              .getCacheConfig(), BloomType.NONE));
+        }
+      } catch (Exception e) {
+        LOG.error("Cannot parse the fileName " + fileName, e);
+      }
+      if (!storeFiles.isEmpty() && storeFiles.size() % 10 == 0) {
+        try {
+          MobUtils.removeMobFiles(store.getConfiguration(), fs, store.getTableName(), store
+              .getColumnDescriptor().getName(), storeFiles);
+        } catch (IOException e) {
+          LOG.error("Fail to delete the mob files " + storeFiles, e);
+        }
+        storeFiles.clear();
+      }
+    }
+    if (!storeFiles.isEmpty()) {
+      try {
+        MobUtils.removeMobFiles(store.getConfiguration(), fs, store.getTableName(), store
+            .getColumnDescriptor().getName(), storeFiles);
+      } catch (IOException e) {
+        LOG.error("Fail to remove the mob files " + storeFiles, e);
+      }
+      storeFiles.clear();
+    }
+  }
+
+  /**
+   * Gets the znode name of column family.
+   * @param tableName The current table name.
+   * @param columnName The name of the current column family.
+   * @return The znode name of column family.
+   */
+  public static String getStoreZNodeName(String tableName, String columnName) {
+    return tableName + ":" + columnName;
+  }
+
+  /**
    * Gets the root dir of the mob files.
    * It's {HBASE_DIR}/mobdir.
    * @param conf The current configuration.
@@ -254,6 +354,27 @@ public static HRegionInfo getMobRegionInfo(TableName tableName) {
   }
 
   /**
+   * Gets whether the current HRegionInfo is a mob one.
+   * @param regionInfo The current HRegionInfo.
+   * @return If true, the current HRegionInfo is a mob one.
+   */
+  public static boolean isMobRegionInfo(HRegionInfo regionInfo) {
+    return regionInfo == null ? false : getMobRegionInfo(regionInfo.getTable()).getEncodedName()
+        .equals(regionInfo.getEncodedName());
+  }
+
+  /**
+   * Gets the working directory of the mob compaction.
+   * @param root The root directory of the mob compaction.
+   * @param jobName The current job name.
+   * @return The directory of the mob compaction for the current job.
+   */
+  public static Path getCompactionWorkingPath(Path root, String jobName) {
+    Path parent = new Path(root, jobName);
+    return new Path(parent, "working");
+  }
+
+  /**
    * Archives the mob files.
    * @param conf The current configuration.
    * @param fs The current file system.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobZookeeper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobZookeeper.java
new file mode 100644
index 0000000..7dbbb8b
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/MobZookeeper.java
@@ -0,0 +1,231 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Abortable;
+import org.apache.hadoop.hbase.zookeeper.ZKUtil;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * The zookeeper used for MOB.
+ * This zookeeper is used to synchronize the HBase major compaction and sweep tool, 
+ */
+@InterfaceAudience.Private
+public class MobZookeeper {
+
+  private static final Log LOG = LogFactory.getLog(MobZookeeper.class);
+
+  private ZooKeeperWatcher zkw;
+  private String mobZnode;
+  private static final String LOCK_EPHEMERAL = "-lock";
+  private static final String SWEEPER_EPHEMERAL = "-sweeper";
+  private static final String MAJOR_COMPACTION_EPHEMERAL = "-mc-ephemeral";
+
+  private MobZookeeper(Configuration conf) throws IOException, KeeperException {
+    zkw = new ZooKeeperWatcher(conf, "MobZookeeper", new DummyMobAbortable());
+    mobZnode = ZKUtil.joinZNode(zkw.baseZNode, "MOB");
+    if (ZKUtil.checkExists(zkw, mobZnode) == -1) {
+      ZKUtil.createWithParents(zkw, mobZnode);
+    }
+  }
+
+  /**
+   * Creates an new instance of MobZookeeper.
+   * @param conf The current configuration.
+   * @return The new instance of MobZookeeper.
+   * @throws IOException
+   * @throws KeeperException
+   */
+  public static MobZookeeper newInstance(Configuration conf) throws IOException, KeeperException {
+    return new MobZookeeper(conf);
+  }
+
+  /**
+   * Acquire a lock on the current store.
+   * All the threads try to access the store acquire a lock which is actually create an ephemeral
+   * node in the zookeeper.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @return True if the lock is obtained successfully. Otherwise false is returned.
+   */
+  public boolean lockStore(String tableName, String storeName) {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    boolean locked = false;
+    try {
+      locked = ZKUtil.createEphemeralNodeAndWatch(zkw,
+          ZKUtil.joinZNode(mobZnode, znodeName + LOCK_EPHEMERAL), null);
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(locked ? "Locked the store " + znodeName : "Can not lock the store " + znodeName);
+      }
+    } catch (KeeperException e) {
+      LOG.error("Fail to lock the store " + znodeName, e);
+    }
+    return locked;
+  }
+
+  /**
+   * Release the lock on the current store.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   */
+  public void unlockStore(String tableName, String storeName) {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Unlocking the store " + znodeName);
+    }
+    try {
+      ZKUtil.deleteNode(zkw, ZKUtil.joinZNode(mobZnode, znodeName + LOCK_EPHEMERAL));
+    } catch (KeeperException e) {
+      LOG.warn("Fail to unlock the store " + znodeName, e);
+    }
+  }
+
+  /**
+   * Adds a node to zookeeper which indicates that a sweep tool is running.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @return True if the node is created successfully. Otherwise false is returned.
+   */
+  public boolean addSweeperZNode(String tableName, String storeName) {
+    boolean add = false;
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    try {
+      add = ZKUtil.createEphemeralNodeAndWatch(zkw,
+          ZKUtil.joinZNode(mobZnode, znodeName + SWEEPER_EPHEMERAL), null);
+      if (LOG.isDebugEnabled()) {
+        LOG.debug(add ? "Added a znode for sweeper " + znodeName
+            : "Cannot add a znode for sweeper " + znodeName);
+      }
+    } catch (KeeperException e) {
+      LOG.error("Fail to add a znode for sweeper " + znodeName, e);
+    }
+    return add;
+  }
+
+  /**
+   * Deletes the node from zookeeper which indicates that a sweep tool is finished.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   */
+  public void deleteSweeperZNode(String tableName, String storeName) {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    try {
+      ZKUtil.deleteNode(zkw, ZKUtil.joinZNode(mobZnode, znodeName + SWEEPER_EPHEMERAL));
+    } catch (KeeperException e) {
+      LOG.error("Fail to delete a znode for sweeper " + znodeName, e);
+    }
+  }
+
+  /**
+   * Checks whether the znode exists in the Zookeeper.
+   * If the node exists, it means a sweep tool is running.
+   * Otherwise, the sweep tool is not.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @return True if this node doesn't exist. Otherwise false is returned.
+   * @throws KeeperException
+   */
+  public boolean isSweeperZNodeExist(String tableName, String storeName) throws KeeperException {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    return ZKUtil.checkExists(zkw, ZKUtil.joinZNode(mobZnode, znodeName + SWEEPER_EPHEMERAL)) >= 0;
+  }
+
+  /**
+   * Checks whether there're major compactions nodes in the zookeeper.
+   * If there're such nodes, it means there're major compactions in progress now.
+   * Otherwise there're not.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @return True if there're major compactions in progress. Otherwise false is returned.
+   * @throws KeeperException
+   */
+  public boolean hasMajorCompactionChildren(String tableName, String storeName)
+      throws KeeperException {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    String mcPath = ZKUtil.joinZNode(mobZnode, znodeName + MAJOR_COMPACTION_EPHEMERAL);
+    List<String> children = ZKUtil.listChildrenNoWatch(zkw, mcPath);
+    return children != null && !children.isEmpty();
+  }
+
+  /**
+   * Creates a node of a major compaction to the Zookeeper.
+   * Before a HBase major compaction, such a node is created to the Zookeeper. It tells others that
+   * there're major compaction in progress, the sweep tool could not be run at this time.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @param compactionName The current compaction name.
+   * @return True if the node is created successfully. Otherwise false is returned.
+   * @throws KeeperException
+   */
+  public boolean addMajorCompactionZNode(String tableName, String storeName, String compactionName)
+      throws KeeperException {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    String mcPath = ZKUtil.joinZNode(mobZnode, znodeName + MAJOR_COMPACTION_EPHEMERAL);
+    ZKUtil.createEphemeralNodeAndWatch(zkw, mcPath, null);
+    String eachMcPath = ZKUtil.joinZNode(mcPath, compactionName);
+    return ZKUtil.createEphemeralNodeAndWatch(zkw, eachMcPath, null);
+  }
+
+  /**
+   * Deletes a major compaction node from the Zookeeper.
+   * @param tableName The current table name.
+   * @param storeName The current store name.
+   * @param compactionName The current compaction name.
+   * @throws KeeperException
+   */
+  public void deleteMajorCompactionZNode(String tableName, String storeName, String compactionName)
+      throws KeeperException {
+    String znodeName = MobUtils.getStoreZNodeName(tableName, storeName);
+    String mcPath = ZKUtil.joinZNode(mobZnode, znodeName + MAJOR_COMPACTION_EPHEMERAL);
+    String eachMcPath = ZKUtil.joinZNode(mcPath, compactionName);
+    ZKUtil.deleteNode(zkw, eachMcPath);
+  }
+
+  /**
+   * Closes the MobZookeeper.
+   */
+  public void close() {
+    this.zkw.close();
+  }
+
+  /**
+   * An dummy abortable. It's used for the MobZookeeper.
+   */
+  private static class DummyMobAbortable implements Abortable {
+
+    private boolean abort = false;
+
+    public void abort(String why, Throwable e) {
+      abort = true;
+    }
+
+    public boolean isAborted() {
+      return abort;
+    }
+
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ExpiredMobFileCleaner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ExpiredMobFileCleaner.java
new file mode 100644
index 0000000..83cf371
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ExpiredMobFileCleaner.java
@@ -0,0 +1,108 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.regionserver.MobFileStore;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+
+import com.google.protobuf.ServiceException;
+
+/**
+ * The cleaner to delete the expired MOB files.
+ */
+@InterfaceAudience.Private
+public class ExpiredMobFileCleaner extends Configured implements Tool {
+
+  /**
+   * Cleans the MOB files when they're expired and their min versions are 0.
+   * If the latest timestamp of Cells in a MOB file is older than the TTL in the column family,
+   * it's regarded as expired. This cleaner deletes them.
+   * Users are allowed to configure a clean delay (hbase.mob.cleaner.delay) in the configuration
+   * which allows the file is deleted when (latestTimeStamp < Now - TTL - delay).
+   * @param tableName The current table name.
+   * @param familyName The current family name.
+   * @throws ServiceException
+   * @throws IOException
+   */
+  void cleanExpiredMobFiles(String tableName, String familyName) throws ServiceException,
+      IOException {
+    Configuration conf = getConf();
+    HBaseAdmin.checkHBaseAvailable(conf);
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    try {
+      FileSystem fs = FileSystem.get(conf);
+      if (!admin.tableExists(tableName)) {
+        throw new IOException("Table " + tableName + " not exist");
+      }
+      HTableDescriptor htd = admin.getTableDescriptor(Bytes.toBytes(tableName));
+      HColumnDescriptor family = htd.getFamily(Bytes.toBytes(familyName));
+      if (!MobUtils.isMobFamily(family)) {
+        throw new IOException("It's not a MOB column family");
+      }
+      if(family.getMinVersions() > 0) {
+        throw new IOException(
+            "The minVersions of the column family is not 0, could not be handled by this cleaner");
+      }
+      System.out.println("Cleaning the expired MOB files...");
+      MobFileStore mobFileStore = MobFileStore.create(conf, fs, TableName.valueOf(tableName),
+          family);
+      MobUtils.cleanExpiredMobFiles(fs, mobFileStore, EnvironmentEdgeManager.currentTimeMillis());
+    } finally {
+      try {
+        admin.close();
+      } catch (IOException e) {
+        System.out.println("Fail to close the HBaseAdmin: " + e.getMessage());
+      }
+    }
+  }
+
+  public static void main(String[] args) throws Exception {
+
+    System.out.print("Usage:\n" + "--------------------------\n"
+        + ExpiredMobFileCleaner.class.getName() + "[tableName] [familyName]");
+
+    Configuration conf = HBaseConfiguration.create();
+    ToolRunner.run(conf, new ExpiredMobFileCleaner(), args);
+  }
+
+  public int run(String[] args) throws Exception {
+    if (args.length >= 2) {
+      String tableName = args[0];
+      String familyName = args[1];
+      cleanExpiredMobFiles(tableName, familyName);
+    }
+    return 0;
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MemStoreWrapper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MemStoreWrapper.java
new file mode 100644
index 0000000..2727207
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MemStoreWrapper.java
@@ -0,0 +1,183 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.Tag;
+import org.apache.hadoop.hbase.TagType;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.compactions.SweepReducer.SweepPartitionId;
+import org.apache.hadoop.hbase.regionserver.DefaultMemStore;
+import org.apache.hadoop.hbase.regionserver.KeyValueScanner;
+import org.apache.hadoop.hbase.regionserver.MemStoreSnapshot;
+import org.apache.hadoop.hbase.regionserver.MobFileStore;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.mapreduce.Reducer.Context;
+
+/**
+ * The wrapper of a DefaultMemStore.
+ * This wrapper is used in the sweep reducer to buffer and sort the cells written from
+ * the invalid and small mob files.
+ * It's flushed when it's full, the mob data are writren to the mob files, and their file names
+ * are written back to store files of HBase.
+ */
+public class MemStoreWrapper {
+
+  private static final Log LOG = LogFactory.getLog(MemStoreWrapper.class);
+
+  private DefaultMemStore memstore;
+  private long flushSize;
+  private SweepPartitionId partitionId;
+  private Context context;
+  private Configuration conf;
+  private MobFileStore mobFileStore;
+  private HTable table;
+
+  public MemStoreWrapper(Context context, HTable table, DefaultMemStore memstore,
+      MobFileStore mobFileStore) throws IOException {
+    this.memstore = memstore;
+    this.context = context;
+    this.mobFileStore = mobFileStore;
+    this.table = table;
+    this.conf = context.getConfiguration();
+    flushSize = this.conf.getLong(MobConstants.MOB_COMPACTION_MEMSTORE_FLUSH_SIZE,
+        HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE);
+  }
+
+  public void setPartitionId(SweepPartitionId partitionId) {
+    this.partitionId = partitionId;
+  }
+
+  /**
+   * Flushes the memstore if the size is large enough.
+   * @throws IOException
+   */
+  public void flushMemStoreIfNecessary() throws IOException {
+    if (memstore.heapSize() >= flushSize) {
+      flushMemStore();
+    }
+  }
+
+  /**
+   * Flushes the memstore anyway.
+   * @throws IOException
+   */
+  public void flushMemStore() throws IOException {
+    MemStoreSnapshot snapshot = memstore.snapshot();
+    internalFlushCache(snapshot);
+    memstore.clearSnapshot(snapshot.getId());
+  }
+
+  /**
+   * Flushes the snapshot of the memstore.
+   * Flushes the mob data to the mob files, and flushes the name of these mob files to HBase.
+   * @param snapshot The snapshot of the memstore.
+   * @throws IOException
+   */
+  private void internalFlushCache(final MemStoreSnapshot snapshot)
+      throws IOException {
+    if (snapshot.getSize() == 0) {
+      return;
+    }
+    // generate the files into a temp directory.
+    String tempPathString = context.getConfiguration().get(SweepJob.WORKING_FILES_DIR_KEY);
+    StoreFile.Writer mobFileWriter = mobFileStore.createWriterInTmp(partitionId.getDate(),
+        new Path(tempPathString), snapshot.getCellsCount(), mobFileStore.getColumnDescriptor()
+            .getCompactionCompression(), partitionId.getStartKey());
+
+    Path targetPath = mobFileStore.getPath();
+
+    String relativePath = mobFileWriter.getPath().getName();
+    LOG.info("Create files under a temp directory " + mobFileWriter.getPath().toString());
+
+    byte[] referenceValue = Bytes.toBytes(relativePath);
+    int keyValueCount = 0;
+    KeyValueScanner scanner = snapshot.getScanner();
+    scanner.seek(KeyValueUtil.createFirstOnRow(HConstants.EMPTY_START_ROW));
+    Cell cell = null;
+    while (null != (cell = scanner.next())) {
+      KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
+      mobFileWriter.append(kv);
+      keyValueCount++;
+    }
+    scanner.close();
+    // Write out the log sequence number that corresponds to this output
+    // hfile. The hfile is current up to and including logCacheFlushId.
+    mobFileWriter.appendMetadata(Long.MAX_VALUE, false);
+    mobFileWriter.close();
+
+    mobFileStore.commitFile(mobFileWriter.getPath(), targetPath);
+    context.getCounter(SweepCounter.FILE_AFTER_MERGE_OR_CLEAN).increment(1);
+    // write reference/fileName back to the store files of HBase.
+    scanner = snapshot.getScanner();
+    scanner.seek(KeyValueUtil.createFirstOnRow(HConstants.EMPTY_START_ROW));
+    cell = null;
+    while (null != (cell = scanner.next())) {
+      List<Tag> existingTags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(),
+          cell.getTagsLength());
+      if (existingTags.isEmpty()) {
+        existingTags = new ArrayList<Tag>();
+      }
+      // the cell whose value is the name of a mob file has such a tag.
+      Tag mobRefTag = new Tag(TagType.MOB_REFERENCE_TAG_TYPE, HConstants.EMPTY_BYTE_ARRAY);
+      existingTags.add(mobRefTag);
+      long valueLength = cell.getValueLength();
+      byte[] newValue = Bytes.add(Bytes.toBytes(valueLength), referenceValue);
+
+      KeyValue reference = new KeyValue(cell.getRowArray(), cell.getRowOffset(),
+          cell.getRowLength(), cell.getFamilyArray(), cell.getFamilyOffset(),
+          cell.getFamilyLength(), cell.getQualifierArray(), cell.getQualifierOffset(),
+          cell.getQualifierLength(), cell.getTimestamp(), KeyValue.Type.Put, newValue, 0,
+          newValue.length, existingTags);
+
+      Put put = new Put(reference.getRow());
+      put.add(reference);
+      table.put(put);
+      context.getCounter(SweepCounter.RECORDS_UPDATED).increment(1);
+    }
+    if (keyValueCount > 0) {
+      table.flushCommits();
+    }
+    scanner.close();
+  }
+
+  /**
+   * Adds a KeyValue into the memstore.
+   * @param kv The KeyValue to be added.
+   */
+  public void addToMemstore(KeyValue kv) {
+    memstore.add(kv);
+  }
+
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MobFilePathHashPartitioner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MobFilePathHashPartitioner.java
new file mode 100644
index 0000000..13ec989
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/MobFilePathHashPartitioner.java
@@ -0,0 +1,40 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.mob.MobFileName;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Partitioner;
+
+/**
+ * The partitioner for the sweep job.
+ */
+@InterfaceAudience.Private
+public class MobFilePathHashPartitioner extends Partitioner<Text, KeyValue> {
+
+  @Override
+  public int getPartition(Text fileName, KeyValue kv, int numPartitions) {
+    MobFileName mobFileName = MobFileName.create(fileName.toString());
+    String date = mobFileName.getDate();
+    int hash = date.hashCode();
+    return (hash & Integer.MAX_VALUE) % numPartitions;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ReferenceOnlyFilter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ReferenceOnlyFilter.java
new file mode 100644
index 0000000..1df65f8
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/ReferenceOnlyFilter.java
@@ -0,0 +1,84 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.util.ArrayList;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.exceptions.DeserializationException;
+import org.apache.hadoop.hbase.filter.Filter;
+import org.apache.hadoop.hbase.filter.FilterBase;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.protobuf.generated.FilterProtos;
+
+import com.google.common.base.Preconditions;
+import com.google.protobuf.InvalidProtocolBufferException;
+
+/**
+ * A filter that returns the cells which have mob reference tags.
+ */
+@InterfaceAudience.Public
+public class ReferenceOnlyFilter extends FilterBase {
+
+  @Override
+  public ReturnCode filterKeyValue(Cell cell) {
+    if (null != cell) {
+      // If a cell with a mob reference tag, it's included.
+      if (MobUtils.isMobReferenceCell(cell)) {
+        return ReturnCode.INCLUDE;
+      }
+    }
+    return ReturnCode.SKIP;
+  }
+
+  public static Filter createFilterFromArguments(ArrayList<byte[]> filterArguments) {
+    Preconditions.checkArgument(filterArguments.size() == 0, "Expected 0 but got: %s",
+        filterArguments.size());
+    return new ReferenceOnlyFilter();
+  }
+
+  /**
+   * @return The filter serialized using pb
+   */
+  public byte[] toByteArray() {
+    FilterProtos.ReferenceOnlyFilter.Builder builder = FilterProtos.ReferenceOnlyFilter
+        .newBuilder();
+    return builder.build().toByteArray();
+  }
+
+  /**
+   * @param pbBytes
+   *          A pb serialized {@link ReferenceOnlyFilter} instance
+   * @return An instance of {@link ReferenceOnlyFilter} made from <code>bytes</code>
+   * @throws org.apache.hadoop.hbase.exceptions.DeserializationException
+   * @see #toByteArray
+   */
+  public static ReferenceOnlyFilter parseFrom(final byte[] pbBytes)
+      throws DeserializationException {
+    // There is nothing to deserialize. Why do this at all?
+    try {
+      FilterProtos.ReferenceOnlyFilter.parseFrom(pbBytes);
+    } catch (InvalidProtocolBufferException e) {
+      throw new DeserializationException(e);
+    }
+    // Just return a new instance.
+    return new ReferenceOnlyFilter();
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepCounter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepCounter.java
new file mode 100644
index 0000000..8e44345
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepCounter.java
@@ -0,0 +1,48 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * The counter used in sweep job.
+ */
+@InterfaceAudience.Private
+public enum SweepCounter {
+
+  /**
+   * How many files are read. 
+   */
+  INPUT_FILE_COUNT,
+
+  /**
+   * How many files need to be merged or cleaned.
+   */
+  FILE_TO_BE_MERGE_OR_CLEAN,
+
+  /**
+   * How many files are left after merging.
+   */
+  FILE_AFTER_MERGE_OR_CLEAN,
+
+  /**
+   * How many records are updated.
+   */
+  RECORDS_UPDATED,
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepJob.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepJob.java
new file mode 100644
index 0000000..727982f
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepJob.java
@@ -0,0 +1,470 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.PriorityQueue;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
+import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.mob.MobZookeeper;
+import org.apache.hadoop.hbase.regionserver.BloomType;
+import org.apache.hadoop.hbase.regionserver.MobFileStore;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.serializer.JavaSerialization;
+import org.apache.hadoop.io.serializer.WritableSerialization;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
+import org.apache.hadoop.security.Credentials;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.zookeeper.KeeperException;
+
+/**
+ * The sweep job.
+ * Run map reduce to merges the smaller mob files into bigger ones and cleans the unused ones.
+ */
+@InterfaceAudience.Private
+public class SweepJob {
+
+  private final FileSystem fs;
+  private static final Log LOG = LogFactory.getLog(SweepJob.class);
+  static final String WORKING_DIR_KEY = "mob.compaction.dir";
+  static final String WORKING_ALLNAMES_FILE_KEY = "mob.compaction.all.file";
+  static final String WORKING_VISITED_DIR_KEY = "mob.compaction.visited.dir";
+  static final String WORKING_ALLNAMES_DIR = "all";
+  static final String WORKING_VISITED_DIR = "visited";
+  public static final String WORKING_FILES_DIR_KEY = "mob.compaction.files.dir";
+  //the MOB_COMPACTION_DELAY is ONE_DAY by default. Its value is only changed when testing.
+  public static final String MOB_COMPACTION_DELAY = "hbase.mob.compaction.delay";
+  protected static long ONE_DAY = 24 * 60 * 60 * 1000;
+  private long compactionStartTime = EnvironmentEdgeManager.currentTimeMillis();
+  public final static String CREDENTIALS_LOCATION = "credentials_location";
+
+  public SweepJob(FileSystem fs) {
+    this.fs = fs;
+  }
+
+  /**
+   * Runs map reduce to do the sweeping on the mob files.
+   * The running of the sweep tool on the same column family are mutually exclusive.
+   * The HBase major compaction and running of the sweep tool on the same column family
+   * are mutually exclusive.
+   * These synchronization is done by the Zookeeper.
+   * So in the beginning of the running, we need to make sure only this sweep tool is the only one
+   * that is currently running in this column family, and in this column family there're no major
+   * compaction in progress.   
+   * @param store The current MobFileStore.
+   * @throws IOException
+   * @throws ClassNotFoundException
+   * @throws InterruptedException
+   * @throws KeeperException
+   */
+  public void sweep(MobFileStore store) throws IOException, ClassNotFoundException,
+      InterruptedException, KeeperException {
+    Configuration conf = new Configuration(store.getConfiguration());
+    // check whether the current user is the same one with the owner of hbase root
+    String currentUserName = UserGroupInformation.getCurrentUser().getShortUserName();
+    FileStatus[] hbaseRootFileStat = fs.listStatus(new Path(conf.get(HConstants.HBASE_DIR)));
+    if (hbaseRootFileStat.length > 0) {
+      String owner = hbaseRootFileStat[0].getOwner();
+      if (!owner.equals(currentUserName)) {
+        String errorMsg = "The current user[" + currentUserName + "] doesn't have the privilege."
+            + " Please make sure the user is the root of the target HBase";
+        LOG.error(errorMsg);
+        throw new IOException(errorMsg);
+      }
+    } else {
+      LOG.error("The target HBase doesn't exist");
+      throw new IOException("The target HBase doesn't exist");
+    }
+    MobZookeeper zk = MobZookeeper.newInstance(conf);
+    try {
+      // Try to obtain the lock. Use this lock to synchronize all the query, creation/deletion
+      // in the Zookeeper.
+      if (!zk.lockStore(store.getTableName().getNameAsString(), store.getFamilyName())) {
+        LOG.warn("Can not lock the store " + store.getFamilyName()
+            + ". The major compaction in Apache HBase may be in-progress. Please re-run the job.");
+        return;
+      }
+      try {
+        // Checks whether there're HBase major compaction now.
+        boolean hasChildren = zk.hasMajorCompactionChildren(store.getTableName().getNameAsString(),
+            store.getFamilyName());
+        if (hasChildren) {
+          LOG.warn("The major compaction in Apache HBase may be in-progress. Please re-run the job.");
+          return;
+        } else {
+          // Checks whether there's sweep tool in progress.
+          boolean hasSweeper = zk.isSweeperZNodeExist(store.getTableName().getNameAsString(),
+              store.getFamilyName());
+          if (hasSweeper) {
+            LOG.warn("Another sweep job is running");
+            return;
+          } else {
+            // add the sweeper node, mark that there's one sweep tool in progress.
+            // All the HBase major compaction and sweep tool in this column family could not
+            // run until this sweep tool is finished.
+            zk.addSweeperZNode(store.getTableName().getNameAsString(), store.getFamilyName());
+          }
+        }
+      } finally {
+        zk.unlockStore(store.getTableName().getNameAsString(), store.getFamilyName());
+      }
+      Job job = null;
+      try {
+        Scan scan = new Scan();
+        // Do not retrieve the mob data when scanning
+        scan.setAttribute(MobConstants.MOB_SCAN_RAW, Bytes.toBytes(Boolean.TRUE));
+        scan.setFilter(new ReferenceOnlyFilter());
+        scan.setCaching(10000);
+        scan.setCacheBlocks(false);
+        scan.setMaxVersions(store.getColumnDescriptor().getMaxVersions());
+        conf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,
+            JavaSerialization.class.getName() + "," + WritableSerialization.class.getName());
+        job = prepareJob(store, scan, conf);
+        job.getConfiguration().set(TableInputFormat.SCAN_COLUMN_FAMILY, store.getFamilyName());
+        // Record the compaction start time.
+        // In the sweep tool, only the mob file whose modification time is older than
+        // (startTime - delay) could be handled by this tool.
+        // The delay is one day. It could be configured as well, but this is only used
+        // in the test.
+        job.getConfiguration().setLong(MobConstants.MOB_COMPACTION_START_DATE,
+            compactionStartTime);
+
+        job.setPartitionerClass(MobFilePathHashPartitioner.class);
+        submit(job, store);
+        if (job.waitForCompletion(true)) {
+          // Archive the unused mob files.
+          removeUnusedFiles(job, store);
+        }
+      } finally {
+        cleanup(job, store);
+        zk.deleteSweeperZNode(store.getTableName().getNameAsString(), store.getFamilyName());
+      }
+    } finally {
+      zk.close();
+    }
+  }
+
+  /**
+   * Prepares a map reduce job.
+   * @param store The current MobFileStore.
+   * @param scan The current scan.
+   * @param conf The current configuration.
+   * @return A map reduce job.
+   * @throws IOException
+   */
+  private Job prepareJob(MobFileStore store, Scan scan, Configuration conf) throws IOException {
+
+    Job job = Job.getInstance(conf);
+
+    job.setJarByClass(SweepMapper.class);
+    TableMapReduceUtil.initTableMapperJob(store.getTableName().getNameAsString(), scan,
+        SweepMapper.class, Text.class, Writable.class, job);
+
+    job.setInputFormatClass(TableInputFormat.class);
+    job.setMapOutputKeyClass(Text.class);
+    job.setMapOutputValueClass(KeyValue.class);
+    job.setReducerClass(SweepReducer.class);
+    job.setOutputFormatClass(NullOutputFormat.class);
+    String jobName = getCustomJobName(this.getClass().getSimpleName(), store.getTableName()
+        .getNameAsString(), store.getFamilyName());
+    job.setJobName(jobName);
+    if (StringUtils.isNotEmpty(conf.get(CREDENTIALS_LOCATION))) {
+      String fileLoc = conf.get(CREDENTIALS_LOCATION);
+      Credentials cred = Credentials.readTokenStorageFile(new File(fileLoc), conf);
+      job.getCredentials().addAll(cred);
+    }
+    return job;
+  }
+
+  /**
+   * Gets a customized job name.
+   * It's className-mapperClassName-reducerClassName-tableName-familyName.
+   * @param className The current class name.
+   * @param tableName The current table name.
+   * @param familyName The current family name.
+   * @return The customized job name.
+   */
+  private static String getCustomJobName(String className, String tableName, String familyName) {
+    StringBuilder name = new StringBuilder();
+    name.append(className);
+    name.append('-').append(SweepMapper.class.getSimpleName());
+    name.append('-').append(SweepReducer.class.getSimpleName());
+    name.append('-').append(tableName);
+    name.append('-').append(familyName);
+    return name.toString();
+  }
+
+  /**
+   * Submits a job.
+   * @param job The current job.
+   * @param store The current MobFileStore.
+   * @throws IOException
+   */
+  private void submit(Job job, MobFileStore store) throws IOException {
+    // delete the temp directory of the mob files in case the failure in the previous
+    // execution.
+    Path mobCompactionTempDir = new Path(store.getTmpDir(),
+        MobConstants.DEFAULT_MOB_COMPACTION_TEMP_DIR_NAME);
+    Path workingPath = MobUtils.getCompactionWorkingPath(mobCompactionTempDir, job.getJobName());
+    job.getConfiguration().set(WORKING_DIR_KEY, workingPath.toString());
+    // delete the working directory in case it'not deleted by the last running.
+    fs.delete(workingPath, true);
+    // create the working directory.
+    fs.mkdirs(workingPath);
+    // create a sequence file which contains the names of all the existing files.
+    Path workingPathOfFiles = new Path(workingPath, "files");
+    Path workingPathOfNames = new Path(workingPath, "names");
+    job.getConfiguration().set(WORKING_FILES_DIR_KEY, workingPathOfFiles.toString());
+    Path allFileNamesPath = new Path(workingPathOfNames, WORKING_ALLNAMES_DIR);
+    job.getConfiguration().set(WORKING_ALLNAMES_FILE_KEY, allFileNamesPath.toString());
+    Path vistiedFileNamesPath = new Path(workingPathOfNames, WORKING_VISITED_DIR);
+    job.getConfiguration().set(WORKING_VISITED_DIR_KEY, vistiedFileNamesPath.toString());
+    // create a file includes all the existing mob files whose creation time is older than
+    // (now - oneDay)
+    fs.create(allFileNamesPath, true);
+    // create a directory where the files contain names of visited mob files are saved. 
+    fs.mkdirs(vistiedFileNamesPath);
+    Path mobStorePath = MobUtils.getMobFamilyPath(job.getConfiguration(), store.getTableName(),
+        store.getFamilyName());
+    // Find all the files whose creation time are older than one day.
+    // Write those file names to a file.
+    // In each reducer there's a writer, it write the visited file names to a file which is saved
+    // in WORKING_VISITED_DIR.
+    // After the job is finished, compare those files, then find out the unused mob files and
+    // archive them.
+    FileStatus[] files = fs.listStatus(mobStorePath);
+    Set<String> fileNames = new TreeSet<String>();
+    long mobCompactionDelay = job.getConfiguration().getLong(MOB_COMPACTION_DELAY, ONE_DAY);
+    for (FileStatus fileStatus : files) {
+      if (fileStatus.isFile()) {
+        if (compactionStartTime - fileStatus.getModificationTime() > mobCompactionDelay) {
+          // only record the potentially unused files older than one day.
+          fileNames.add(fileStatus.getPath().getName());
+        }
+      }
+    }
+    // write the names to a sequence file
+    SequenceFile.Writer writer = SequenceFile.createWriter(fs, job.getConfiguration(),
+        allFileNamesPath, String.class, String.class);
+    try {
+      for (String fileName : fileNames) {
+        writer.append(fileName, "");
+      }
+    } finally {
+      IOUtils.closeStream(writer);
+    }
+  }
+
+  /**
+   * Archives unused mob files.
+   * Compare the file which contains all the existing mob files and the visited files,
+   * find out the unused mob file and archive them.
+   * @param job The current job.
+   * @param store The current MobFileStore.
+   * @throws IOException
+   */
+  private void removeUnusedFiles(Job job, MobFileStore store) throws IOException {
+    // find out the unused files and archive them
+    Path allFileNamesPath = new Path(job.getConfiguration().get(WORKING_ALLNAMES_FILE_KEY));
+    SequenceFile.Reader allNamesReader = null;
+    MergeSortReader visitedNamesReader = null;
+    List<StoreFile> storeFiles = new ArrayList<StoreFile>();
+    try {
+      allNamesReader = new SequenceFile.Reader(fs, allFileNamesPath, job.getConfiguration());
+      visitedNamesReader = new MergeSortReader(fs, job.getConfiguration(), new Path(job
+          .getConfiguration().get(WORKING_VISITED_DIR_KEY)));
+      List<String> toBeArchived = new ArrayList<String>();
+      String nextAll = (String) allNamesReader.next((String) null);
+      String nextVisited = visitedNamesReader.next();
+      do {
+        if (nextAll != null) {
+          if (nextVisited != null) {
+            int compare = nextAll.compareTo(nextVisited);
+            if (compare < 0) {
+              toBeArchived.add(nextAll);
+              nextAll = (String) allNamesReader.next((String) null);
+            } else if (compare > 0) {
+              nextVisited = visitedNamesReader.next();
+            } else {
+              nextAll = (String) allNamesReader.next((String) null);
+              nextVisited = visitedNamesReader.next();
+            }
+          } else {
+            toBeArchived.add(nextAll);
+            nextAll = (String) allNamesReader.next((String) null);
+          }
+        } else {
+          break;
+        }
+      } while (nextAll != null || nextVisited != null);
+      // archive them
+      Path mobStorePath = MobUtils.getMobFamilyPath(job.getConfiguration(), store.getTableName(),
+          store.getFamilyName());
+      for (String archiveFileName : toBeArchived) {
+        Path path = new Path(mobStorePath, archiveFileName);
+        storeFiles.add(new StoreFile(fs, path, job.getConfiguration(), store.getCacheConfig(),
+            BloomType.NONE));
+      }
+    } finally {
+      if (allNamesReader != null) {
+        allNamesReader.close();
+      }
+      if (visitedNamesReader != null) {
+        visitedNamesReader.close();
+      }
+    }
+    if (!storeFiles.isEmpty()) {
+      try {
+        MobUtils.removeMobFiles(job.getConfiguration(), fs, store.getTableName(), store
+            .getColumnDescriptor().getName(), storeFiles);
+        LOG.info(storeFiles.size() + " unused MOB files are removed");
+      } catch (Exception e) {
+        LOG.error("Fail to archive the store files " + storeFiles, e);
+      }
+    }
+  }
+
+  /**
+   * Deletes the working directory.
+   * @param job The current job.
+   * @param store The current MobFileStore.
+   * @throws IOException
+   */
+  private void cleanup(Job job, MobFileStore store) throws IOException {
+    if (job != null) {
+      // delete the working directory
+      Path workingPath = new Path(job.getConfiguration().get(WORKING_DIR_KEY));
+      try {
+        fs.delete(workingPath, true);
+      } catch (IOException e) {
+        LOG.warn(
+            "Fail to delete the working directory after sweeping store " + store.getFamilyName()
+                + " in the table " + store.getTableName().getNameAsString(), e);
+      }
+    }
+  }
+
+  /**
+   * A result with index.
+   */
+  private class IndexedResult implements Comparable<IndexedResult> {
+    private int index;
+    private String value;
+
+    public IndexedResult(int index, String value) {
+      this.index = index;
+      this.value = value;
+    }
+
+    public int getIndex() {
+      return this.index;
+    }
+
+    public String getValue() {
+      return this.value;
+    }
+
+    @Override
+    public int compareTo(IndexedResult o) {
+      if (this.value == null) {
+        return 0;
+      } else if (o.value == null) {
+        return 1;
+      } else {
+        return this.value.compareTo(o.value);
+      }
+    }
+
+  }
+
+  /**
+   * Merge sort reader.
+   * It merges and sort the readers in different sequence files as one where
+   * the results are read in order.
+   */
+  private class MergeSortReader {
+
+    private List<SequenceFile.Reader> readers = new ArrayList<SequenceFile.Reader>();
+    private PriorityQueue<IndexedResult> results = new PriorityQueue<IndexedResult>();
+
+    public MergeSortReader(FileSystem fs, Configuration conf, Path path) throws IOException {
+      if (fs.exists(path)) {
+        FileStatus[] files = fs.listStatus(path);
+        int index = 0;
+        for (FileStatus file : files) {
+          if (file.isFile()) {
+            SequenceFile.Reader reader = new SequenceFile.Reader(fs, file.getPath(), conf);
+            String key = (String) reader.next((String) null);
+            if (key != null) {
+              results.add(new IndexedResult(index, key));
+              readers.add(reader);
+              index++;
+            }
+          }
+        }
+      }
+    }
+
+    public String next() throws IOException {
+      IndexedResult result = results.poll();
+      if (result != null) {
+        SequenceFile.Reader reader = readers.get(result.getIndex());
+        String key = (String) reader.next((String) null);
+        if (key != null) {
+          results.add(new IndexedResult(result.getIndex(), key));
+        }
+        return result.getValue();
+      }
+      return null;
+    }
+
+    public void close() {
+      for (SequenceFile.Reader reader : readers) {
+        IOUtils.closeStream(reader);
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepMapper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepMapper.java
new file mode 100644
index 0000000..d909a1c
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepMapper.java
@@ -0,0 +1,55 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.mapreduce.TableMapper;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Text;
+
+/**
+ * The mapper of a sweep job.
+ * The key of the output is the value of a KeyValue which is actually a mob file name,
+ * and the value is this KeyValue.
+ */
+@InterfaceAudience.Private
+public class SweepMapper extends TableMapper<Text, KeyValue> {
+
+  @Override
+  public void map(ImmutableBytesWritable r, Result columns, Context context) throws IOException,
+      InterruptedException {
+    if (columns != null) {
+      KeyValue[] kvList = columns.raw();
+      if (kvList != null && kvList.length > 0) {
+        for (KeyValue kv : kvList) {
+          if (kv.getValueLength() > Bytes.SIZEOF_LONG) {
+            String fileName = Bytes.toString(kv.getValueArray(), kv.getValueOffset()
+                + Bytes.SIZEOF_LONG, kv.getValueLength() - Bytes.SIZEOF_LONG);
+            context.write(new Text(fileName), kv);
+          }
+        }
+      }
+    }
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepReducer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepReducer.java
new file mode 100644
index 0000000..4ea5ad5
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/SweepReducer.java
@@ -0,0 +1,523 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.UUID;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.io.HFileLink;
+import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.MobFile;
+import org.apache.hadoop.hbase.mob.MobFileName;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.regionserver.BloomType;
+import org.apache.hadoop.hbase.regionserver.DefaultMemStore;
+import org.apache.hadoop.hbase.regionserver.MobFileStore;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreFileScanner;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapreduce.Reducer;
+
+/**
+ * The reducer of a sweep job.
+ * This reducer merges the small mob files into bigger ones, and write visited
+ * names of mob files to a sequence file which is used by the sweep job to delete
+ * the unused mob files.
+ * The key of the input is a file name, the value is a collection of KeyValue.
+ * In this reducer, we could know how many cells exist in HBase for a mob file.
+ * If the (mobFileSize - existCellSize)/mobFileSize >= invalidRatio, this mob
+ * file needs to be merged. 
+ */
+@InterfaceAudience.Private
+public class SweepReducer extends Reducer<Text, KeyValue, Writable, Writable> {
+
+  private static final Log LOG = LogFactory.getLog(SweepReducer.class);
+
+  private SequenceFile.Writer writer = null;
+  private MemStoreWrapper memstore;
+  private Configuration conf;
+  private FileSystem fs;
+
+  private Path familyDir;
+  private CacheConfig cacheConf;
+  private long compactionBegin;
+  private HTable table;
+  private MobFileStore mobFileStore;
+  private long mobCompactionDelay;
+
+  @Override
+  protected void setup(Context context) throws IOException, InterruptedException {
+    this.conf = context.getConfiguration();
+    this.fs = FileSystem.get(conf);
+    // the MOB_COMPACTION_DELAY is ONE_DAY by default. Its value is only changed when testing.
+    mobCompactionDelay = conf.getLong(SweepJob.MOB_COMPACTION_DELAY, SweepJob.ONE_DAY);
+    String tableName = conf.get(TableInputFormat.INPUT_TABLE);
+    String familyName = conf.get(TableInputFormat.SCAN_COLUMN_FAMILY);
+    this.familyDir = MobUtils.getMobFamilyPath(conf, TableName.valueOf(tableName), familyName);
+    HBaseAdmin admin = new HBaseAdmin(this.conf);
+    HColumnDescriptor family = null;
+    try {
+      family = admin.getTableDescriptor(Bytes.toBytes(tableName)).getFamily(
+          Bytes.toBytes(familyName));
+    } finally {
+      try {
+        admin.close();
+      } catch (IOException e) {
+        LOG.warn("Fail to close the HBaseAdmin", e);
+      }
+    }
+    mobFileStore = MobFileStore.create(conf, fs, TableName.valueOf(tableName), family);
+    this.cacheConf = new CacheConfig(conf, mobFileStore.getColumnDescriptor());
+
+    table = new HTable(this.conf, Bytes.toBytes(tableName));
+    table.setAutoFlush(false, false);
+
+    table.setWriteBufferSize(1 * 1024 * 1024); // 1MB
+    memstore = new MemStoreWrapper(context, table, new DefaultMemStore(), mobFileStore);
+
+    // The start time of the sweep tool.
+    // Only the mob files whose creation time is older than startTime-oneDay will be handled by the
+    // reducer since it brings inconsistency to handle the latest mob files.
+    this.compactionBegin = conf.getLong(MobConstants.MOB_COMPACTION_START_DATE, 0);
+  }
+
+  private SweepPartition createPartition(SweepPartitionId id, Context context) throws IOException {
+    return new SweepPartition(id, context);
+  }
+
+  @Override
+  public void run(Context context) throws IOException, InterruptedException {
+    try {
+      setup(context);
+      // create a sequence contains all the visited file names in this reducer.
+      String dir = this.conf.get(SweepJob.WORKING_VISITED_DIR_KEY);
+      Path nameFilePath = new Path(dir, UUID.randomUUID().toString().replace("-", ""));
+      if (!fs.exists(nameFilePath)) {
+        fs.create(nameFilePath, true);
+      }
+      writer = SequenceFile.createWriter(fs, context.getConfiguration(), nameFilePath,
+          String.class, String.class);
+      SweepPartitionId id;
+      SweepPartition partition = null;
+      // the mob files which have the same start key and date are in the same partition. 
+      while (context.nextKey()) {
+        Text key = context.getCurrentKey();
+        String keyString = key.toString();
+        id = SweepPartitionId.create(keyString);
+        if (null == partition || !id.equals(partition.getId())) {
+          // It's the first mob file in the current partition.
+          if (null != partition) {
+            // this mob file is in different partitions with the previous mob file.
+            // directly close.
+            partition.close();
+          }
+          // create a new one
+          partition = createPartition(id, context);
+        }
+        if (partition != null) {
+          // run the partition
+          partition.execute(key, context.getValues());
+        }
+      }
+      if (null != partition) {
+        partition.close();
+      }
+    } finally {
+      cleanup(context);
+      if (writer != null) {
+        IOUtils.closeStream(writer);
+      }
+      if (table != null) {
+        try {
+          table.close();
+        } catch (IOException e) {
+          LOG.warn(e);
+        }
+      }
+    }
+
+  }
+
+  /**
+   * The mob files which have the same start key and date are in the same partition.
+   * The files in the same partition are merged together into bigger ones.
+   */
+  public class SweepPartition {
+
+    private final SweepPartitionId id;
+    private final Context context;
+    private boolean memstoreUpdated = false;
+    private boolean mergeSmall = false;
+    private final Map<String, MobFileStatus> fileStatusMap = new HashMap<String, MobFileStatus>();
+    private final List<Path> toBeDeleted = new ArrayList<Path>();;
+
+    public SweepPartition(SweepPartitionId id, Context context) throws IOException {
+      this.id = id;
+      this.context = context;
+      memstore.setPartitionId(id);
+      init();
+    }
+
+    public SweepPartitionId getId() {
+      return this.id;
+    }
+
+    /**
+     * Prepares the map of files.
+     * 
+     * @throws IOException
+     */
+    private void init() throws IOException {
+      FileStatus[] fileStats = listStatus(familyDir, id.getStartKey());
+      if (null == fileStats) {
+        return;
+      }
+
+      int smallFileCount = 0;
+      float invalidFileRatio = conf.getFloat(MobConstants.MOB_COMPACTION_INVALID_FILE_RATIO,
+          MobConstants.DEFAULT_MOB_COMPACTION_INVALID_FILE_RATIO);
+      long smallFileThreshold = conf.getLong(MobConstants.MOB_COMPACTION_SMALL_FILE_THRESHOLD,
+          MobConstants.DEFAULT_MOB_COMPACTION_SMALL_FILE_THRESHOLD);
+      // list the files. Just merge the hfiles, don't merge the hfile links.
+      // prepare the map of mob files. The key is the file name, the value is the file status.
+      // if the mob file is a hfile link, use the name of its referenced file as the key.
+      for (FileStatus fileStat : fileStats) {
+        MobFileStatus mobFileStatus = null;
+        if (HFileLink.isHFileLink(fileStat.getPath())) {
+          // Leave the hfile link alone
+          HFileLink hfileLink = new HFileLink(conf, fileStat.getPath());
+          Path originalPath = hfileLink.getOriginPath();
+          mobFileStatus = new MobFileStatus(fileStat, fileStat.getPath().getName());
+          // key is file name (not hfile name), value is hfile link status.
+          fileStatusMap.put(originalPath.getName(), mobFileStatus);
+        } else {
+          mobFileStatus = new MobFileStatus(fileStat);
+          mobFileStatus.setInvalidFileRatio(invalidFileRatio).setSmallFileThreshold(
+              smallFileThreshold);
+          if (mobFileStatus.needMerge()) {
+            smallFileCount++;
+          }
+          // key is file name (not hfile name), value is hfile status.
+          fileStatusMap.put(fileStat.getPath().getName(), mobFileStatus);
+        }
+      }
+      if (smallFileCount >= 2) {
+        // merge the files only when there're more than 1 files in the same partition.
+        this.mergeSmall = true;
+      }
+    }
+
+    /**
+     * Flushes the data into mob files and store files, and archives the small
+     * files after they're merged. 
+     * @throws IOException
+     */
+    public void close() throws IOException {
+      if (null == id) {
+        return;
+      }
+      // flush remain key values into mob files
+      if (memstoreUpdated) {
+        memstore.flushMemStore();
+      }
+      List<StoreFile> storeFiles = new ArrayList<StoreFile>();
+      // delete samll files after compaction
+      for (Path path : toBeDeleted) {
+        LOG.info("[In Partition close] Delete the file " + path + " in partition close");
+        storeFiles.add(new StoreFile(fs, path, conf, cacheConf, BloomType.NONE));
+      }
+      if (!storeFiles.isEmpty()) {
+        try {
+          MobUtils.removeMobFiles(conf, fs, mobFileStore.getTableName(), mobFileStore
+              .getColumnDescriptor().getName(), storeFiles);
+          context.getCounter(SweepCounter.FILE_TO_BE_MERGE_OR_CLEAN).increment(storeFiles.size());
+        } catch (IOException e) {
+          LOG.error("Fail to archive the store files " + storeFiles, e);
+        }
+        storeFiles.clear();
+      }
+      fileStatusMap.clear();
+    }
+
+    /**
+     * Merges the small mob files into bigger ones.
+     * @param fileName The current mob file name.
+     * @param values The collection of KeyValues in this mob file.
+     * @throws IOException
+     */
+    public void execute(Text fileName, Iterable<KeyValue> values) throws IOException {
+      if (null == values) {
+        return;
+      }
+      MobFileName mobFileName = MobFileName.create(fileName.toString());
+      LOG.info("[In reducer] The file name: " + fileName.toString());
+      MobFileStatus mobFileStat = fileStatusMap.get(mobFileName.getFileName());
+      if (null == mobFileStat) {
+        LOG.info("[In reducer] Cannot find the file, probably this record is obsolete");
+        return;
+      }
+      // only handle the files that are older then one day.
+      if (compactionBegin - mobFileStat.getFileStatus().getModificationTime()
+          <= mobCompactionDelay) {
+        return;
+      }
+      if (mobFileStat.getHfileLinkName() == null) {
+        // write the hfile name
+        writer.append(mobFileName.getFileName(), "");
+      } else {
+        // write the hfile link name
+        writer.append(mobFileStat.getHfileLinkName(), "");
+      }
+      Set<KeyValue> kvs = new HashSet<KeyValue>();
+      for (KeyValue kv : values) {
+        if (kv.getValueLength() > Bytes.SIZEOF_LONG) {
+          mobFileStat.addValidSize(Bytes.toLong(kv.getValueArray(), kv.getValueOffset(),
+              Bytes.SIZEOF_LONG));
+        }
+        kvs.add(kv.createKeyOnly(false));
+      }
+      // If the mob file is a invalid one or a small one, merge it into new/bigger ones.
+      if (mobFileStat.needClean() || (mergeSmall && mobFileStat.needMerge())) {
+        context.getCounter(SweepCounter.INPUT_FILE_COUNT).increment(1);
+        MobFile file = MobFile.create(fs,
+            new Path(familyDir, mobFileName.getFileName()), conf, cacheConf);
+        StoreFileScanner scanner = null;
+        try {
+          scanner = file.getScanner();
+          scanner.seek(KeyValueUtil.createFirstOnRow(new byte[] {}));
+
+          Cell cell;
+          while (null != (cell = scanner.next())) {
+            KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
+            KeyValue keyOnly = kv.createKeyOnly(false);
+            if (kvs.contains(keyOnly)) {
+              // write the KeyValue existing in HBase to the memstore.
+              memstore.addToMemstore(kv);
+              memstoreUpdated = true;
+              // flush the memstore if it's full.
+              memstore.flushMemStoreIfNecessary();
+            }
+          }
+        } finally {
+          if (scanner != null) {
+            scanner.close();
+          }
+        }
+        toBeDeleted.add(mobFileStat.getFileStatus().getPath());
+      }
+    }
+
+    /**
+     * Lists the files with the same prefix.
+     * @param p The file path.
+     * @param prefix The prefix.
+     * @return The files with the same prefix.
+     * @throws IOException
+     */
+    private FileStatus[] listStatus(Path p, String prefix) throws IOException {
+      return fs.listStatus(p, new PathPrefixFilter(prefix));
+    }
+  }
+
+  static class PathPrefixFilter implements PathFilter {
+
+    private final String prefix;
+
+    public PathPrefixFilter(String prefix) {
+      this.prefix = prefix;
+    }
+
+    public boolean accept(Path path) {
+      return path.getName().startsWith(prefix, 0);
+    }
+
+  }
+
+  /**
+   * The sweep partition id.
+   * It consists of the start key and date.
+   * The start key is a hex string of the checksum of a region start key.
+   * The date is the latest timestamp of cells in a mob file.
+   */
+  public static class SweepPartitionId {
+    private String date;
+    private String startKey;
+
+    public SweepPartitionId(MobFileName fileName) {
+      this.date = fileName.getDate();
+      this.startKey = fileName.getStartKey();
+    }
+
+    public SweepPartitionId(String date, String startKey) {
+      this.date = date;
+      this.startKey = startKey;
+    }
+
+    public static SweepPartitionId create(String key) {
+      return new SweepPartitionId(MobFileName.create(key));
+    }
+
+    @Override
+    public boolean equals(Object anObject) {
+      if (this == anObject) {
+        return true;
+      }
+      if (anObject instanceof SweepPartitionId) {
+        SweepPartitionId another = (SweepPartitionId) anObject;
+        if (this.date.equals(another.getDate()) && this.startKey.equals(another.getStartKey())) {
+          return true;
+        }
+      }
+      return false;
+    }
+
+    public String getDate() {
+      return this.date;
+    }
+
+    public String getStartKey() {
+      return this.startKey;
+    }
+
+    public void setDate(String date) {
+      this.date = date;
+    }
+
+    public void setStartKey(String startKey) {
+      this.startKey = startKey;
+    }
+  }
+
+  /**
+   * The mob file status used in the sweep reduecer.
+   */
+  private static class MobFileStatus {
+    private FileStatus fileStatus;
+    private int validSize;
+    private long size;
+    private final String hfileLinkName;
+
+    private float invalidFileRatio = MobConstants.DEFAULT_MOB_COMPACTION_INVALID_FILE_RATIO;
+    private long smallFileThreshold = MobConstants.DEFAULT_MOB_COMPACTION_SMALL_FILE_THRESHOLD;
+
+    public MobFileStatus(FileStatus status) {
+      this(status, null);
+    }
+
+    public MobFileStatus(FileStatus fileStatus, String hfileLinkName) {
+      this.fileStatus = fileStatus;
+      this.size = fileStatus.getLen();
+      validSize = 0;
+      this.hfileLinkName = hfileLinkName;
+    }
+
+    /**
+     * Sets the ratio for a mob file.
+     * If there're too many cells deleted in a mob file, it's regarded as invalid,
+     * and needs to be written to a new one.
+     * @param invalidFileRatio the invalid ratio. 
+     *   If (fileSize-existingCellSize)/fileSize>invalidFileRatio, it's regarded as a invalid one.
+     * @return The current instance of MobFileStatus.
+     */
+    public MobFileStatus setInvalidFileRatio(float invalidFileRatio) {
+      this.invalidFileRatio = invalidFileRatio;
+      return this;
+    }
+
+    /**
+     * Sets the threshold of a small file.
+     * @param smallFileThreshold A threshold of a small file. If the size of a mob file is less
+     * than this threshold, it's regarded as a small one.
+     * @return The current instance of MobFileStatus.
+     */
+    public MobFileStatus setSmallFileThreshold(long smallFileThreshold) {
+      this.smallFileThreshold = smallFileThreshold;
+      return this;
+    }
+
+    /**
+     * Add size to this file.
+     * @param size The size to be added.
+     */
+    public void addValidSize(long size) {
+      this.validSize += size;
+    }
+
+    /**
+     * Whether the mob files need to be cleaned.
+     * If there're too many cells deleted in this mob file, it needs to be cleaned.
+     * @return True if it needs to be cleaned.
+     */
+    public boolean needClean() {
+      return hfileLinkName == null && size - validSize > invalidFileRatio * size;
+    }
+
+    /**
+     * Whether the mob files need to be merged.
+     * If this mob file is too small, it needs to be merged.
+     * @return True if it needs to be merged.
+     */
+    public boolean needMerge() {
+      return hfileLinkName == null && this.size < smallFileThreshold;
+    }
+
+    /**
+     * Gets the name of hfile link.
+     * @return the name of hfile link. If this mob file is not a hfile link null is returned.
+     */
+    public String getHfileLinkName() {
+      return hfileLinkName;
+    }
+
+    /**
+     * Gets the file status.
+     * @return The file status.
+     */
+    public FileStatus getFileStatus() {
+      return fileStatus;
+    }
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/Sweeper.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/Sweeper.java
new file mode 100644
index 0000000..5e0ba10
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mob/compactions/Sweeper.java
@@ -0,0 +1,108 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mob.compactions;
+
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.conf.Configured;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.regionserver.MobFileStore;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.util.Tool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.zookeeper.KeeperException;
+
+import com.google.protobuf.ServiceException;
+
+/**
+ * The sweep tool.
+ * It deletes the mob files that are not used and merges the small mob files to bigger ones.
+ * Each running of this sweep tool is only handle one column family.
+ * Each running on the same column family are mutually exclusive. And the major compaction
+ * and sweep tool on the same column family are mutually exclusive too.
+ */
+@InterfaceAudience.Public
+public class Sweeper extends Configured implements Tool {
+
+  /**
+   * Sweeps the mob files on one column family.
+   * It deletes the unused mob files and merges the small mob files into bigger ones.
+   * @param tableName The current table name in string format.
+   * @param familyName The column family name.
+   * @throws IOException
+   * @throws InterruptedException
+   * @throws ClassNotFoundException
+   * @throws KeeperException
+   * @throws ServiceException
+   */
+  void sweepFamily(String tableName, String familyName) throws IOException, InterruptedException,
+      ClassNotFoundException, KeeperException, ServiceException {
+    Configuration conf = getConf();
+    // make sure the target HBase exists.
+    HBaseAdmin.checkHBaseAvailable(conf);
+    HBaseAdmin admin = new HBaseAdmin(conf);
+    try {
+      FileSystem fs = FileSystem.get(conf);
+      if (!admin.tableExists(tableName)) {
+        throw new IOException("Table " + tableName + " not exist");
+      }
+      HTableDescriptor htd = admin.getTableDescriptor(Bytes.toBytes(tableName));
+      HColumnDescriptor family = htd.getFamily(Bytes.toBytes(familyName));
+      if (!MobUtils.isMobFamily(family)) {
+        throw new IOException("It's not a MOB column family");
+      }
+      MobFileStore store = MobFileStore.create(conf, fs, TableName.valueOf(tableName), family);
+      SweepJob job = new SweepJob(fs);
+      // Run the sweeping
+      job.sweep(store);
+    } finally {
+      try {
+        admin.close();
+      } catch (IOException e) {
+        System.out.println("Fail to close the HBaseAdmin: " + e.getMessage());
+      }
+    }
+  }
+
+  public static void main(String[] args) throws Exception {
+
+    System.out.print("Usage:\n" + "--------------------------\n" + Sweeper.class.getName()
+        + "[tableName] [familyName]");
+
+    Configuration conf = HBaseConfiguration.create();
+    ToolRunner.run(conf, new Sweeper(), args);
+  }
+
+  public int run(String[] args) throws Exception {
+    if (args.length >= 2) {
+      String table = args[0];
+      String family = args[1];
+      sweepFamily(table, family);
+    }
+    return 0;
+  }
+}
\ No newline at end of file
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
index bb79b99..542ffcd 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HMobStore.java
@@ -19,11 +19,16 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
+import java.util.List;
 import java.util.NavigableSet;
+import java.util.UUID;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.mob.MobZookeeper;
+import org.apache.hadoop.hbase.regionserver.compactions.CompactionContext;
+import org.apache.zookeeper.KeeperException;
 
 /**
  * The store implementation to save MOBs (medium objects), it extends the HStore.
@@ -69,4 +74,88 @@ public KeyValueScanner getScanner(Scan scan, NavigableSet<byte[]> targetCols, lo
       lock.readLock().unlock();
     }
   }
+
+  /**
+   * The compaction in the store of mob.
+   * The cells in this store contains the path of the mob files. There might be race
+   * condition between the major compaction and the sweeping in mob files.
+   * In order to avoid this, we need mutually exclude the running of the major compaction and
+   * sweeping in mob files.
+   * The minor compaction is not affected.
+   * The major compaction is converted to a minor one when a sweeping is in progress.
+   */
+  @Override
+  public List<StoreFile> compact(CompactionContext compaction) throws IOException {
+    // If it's major compaction, try to find whether there's a sweeper is running
+    // If yes, change the major compaction to a minor one.
+    if (compaction.getRequest().isMajor()) {
+      // Use the Zookeeper to coordinate.
+      // 1. Acquire a operation lock.
+      //   1.1. If no, convert the major compaction to a minor one and continue the compaction.
+      //   1.2. If the lock is obtained, search the node of sweeping.
+      //      1.2.1. If the node is there, the sweeping is in progress, convert the major
+      //             compaction to a minor one and continue the compaction.
+      //      1.2.2. If the node is not there, add a child to the major compaction node, and
+      //             run the compaction directly.
+      MobZookeeper zk = null;
+      try {
+        zk = MobZookeeper.newInstance(this.getHRegion().conf);
+      } catch (KeeperException e) {
+        LOG.error("Cannot connect to the zookeeper, ready to perform the minor compaction instead",
+            e);
+        // change the major compaction into a minor one
+        compaction.getRequest().setIsMajor(false, false);
+        return super.compact(compaction);
+      }
+      boolean major = false;
+      String compactionName = UUID.randomUUID().toString().replaceAll("-", "");
+      try {
+        // try to acquire the operation lock.
+        if (zk.lockStore(getTableName().getNameAsString(), getFamily().getNameAsString())) {
+          try {
+            LOG.info("Obtain the lock for the store[" + this
+                + "], ready to perform the major compaction");
+            // check the sweeping node to find out whether the sweeping is in progress.
+            boolean hasSweeper = zk.isSweeperZNodeExist(getTableName().getNameAsString(),
+                getFamily().getNameAsString());
+            if (!hasSweeper) {
+              // if not, add a child to the major compaction node of this store. 
+              major = zk.addMajorCompactionZNode(getTableName().getNameAsString(), getFamily()
+                  .getNameAsString(), compactionName);
+            }
+          } catch (Exception e) {
+            LOG.error("Fail to handle the Zookeeper", e);
+          } finally {
+            // release the operation lock
+            zk.unlockStore(getTableName().getNameAsString(), getFamily().getNameAsString());
+          }
+        }
+        try {
+          if (major) {
+            return super.compact(compaction);
+          } else {
+            LOG.info("Cannot obtain the lock or there's another major compaction for the store["
+                + this + "], ready to perform the minor compaction instead");
+            // change the major compaction into a minor one
+            compaction.getRequest().setIsMajor(false, false);
+            return super.compact(compaction);
+          }
+        } finally {
+          if (major) {
+            try {
+              zk.deleteMajorCompactionZNode(getTableName().getNameAsString(), getFamily()
+                  .getNameAsString(), compactionName);
+            } catch (KeeperException e) {
+              LOG.error("Fail to delete the compaction znode" + compactionName, e);
+            }
+          }
+        }
+      } finally {
+        zk.close();
+      }
+    } else {
+      // If it's not a major compaction, continue the compaction.
+      return super.compact(compaction);
+    }
+  }
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestExpiredMobFileCleaner.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestExpiredMobFileCleaner.java
new file mode 100644
index 0000000..895e650
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestExpiredMobFileCleaner.java
@@ -0,0 +1,209 @@
+package org.apache.hadoop.hbase.regionserver.mob.compactions;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.mob.compactions.ExpiredMobFileCleaner;
+import org.apache.hadoop.hbase.regionserver.DefaultMobStoreFlusher;
+import org.apache.hadoop.hbase.regionserver.DefaultStoreEngine;
+import org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.util.ToolRunner;
+import org.junit.*;
+import org.junit.experimental.categories.Category;
+
+import java.util.HashSet;
+import java.util.Random;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+
+@Category(MediumTests.class)
+public class TestExpiredMobFileCleaner {
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private final static String tableName = "TestExpiredMobFileCleaner";
+  private final static String family = "family";
+  private final static byte[] row1 = Bytes.toBytes("row1");
+  private final static byte[] row2 = Bytes.toBytes("row2");
+  private final static byte[] row3 = Bytes.toBytes("row3");
+  private final static byte[] qf = Bytes.toBytes("qf");
+  private final static byte[] value1 = Bytes.toBytes("value1");
+  private final static byte[] value2 = Bytes.toBytes("value2");
+  private final static byte[] value3 = Bytes.toBytes("value3");
+
+  private static HTable table;
+  private static HBaseAdmin admin;
+
+  private int mobThreshold = 300;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    TEST_UTIL.getConfiguration().setInt("hbase.master.info.port", 0);
+    TEST_UTIL.getConfiguration().setBoolean("hbase.regionserver.info.port.auto", true);
+    TEST_UTIL.getConfiguration().setClass(
+            DefaultStoreEngine.DEFAULT_STORE_FLUSHER_CLASS_KEY,
+            DefaultMobStoreFlusher.class, DefaultStoreFlusher.class);
+
+    TEST_UTIL.getConfiguration().setInt("hfile.format.version", 3);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+
+  }
+
+  @Before
+  public void setUp() throws Exception {
+    TEST_UTIL.startMiniCluster(1);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    admin.disableTable(tableName);
+    admin.deleteTable(tableName);
+    admin.close();
+    TEST_UTIL.shutdownMiniCluster();
+    TEST_UTIL.getTestFileSystem().delete(TEST_UTIL.getDataTestDir(), true);
+  }
+
+  private void init(int cleanDelayDays) throws Exception {
+    int cleanDelay = cleanDelayDays * secondsOfDay() * 1000;
+    TEST_UTIL.getConfiguration().setLong(MobConstants.MOB_CLEAN_DELAY, cleanDelay);
+
+    HTableDescriptor desc = new HTableDescriptor(tableName);
+    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    hcd.setValue(MobConstants.IS_MOB, "true");
+    hcd.setMaxVersions(4);
+    desc.addFamily(hcd);
+
+    admin = TEST_UTIL.getHBaseAdmin();
+    admin.createTable(desc);
+    table = new HTable(TEST_UTIL.getConfiguration(), tableName);
+    table.setAutoFlush(false);
+  }
+
+  private void modifyColumn(int expireDays) throws Exception {
+    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    hcd.setValue(MobConstants.IS_MOB, "true");
+    hcd.setMaxVersions(4);
+    // change ttl as expire days to make some row expired
+    int timeToLive = expireDays * secondsOfDay();
+    hcd.setTimeToLive(timeToLive);
+
+    admin.modifyColumn(tableName, hcd);
+  }
+
+  private void putKVAndFlush(HTable table, byte[] row, byte[] value, long ts)
+      throws Exception {
+
+    Put put = new Put(row, ts);
+    put.add(Bytes.toBytes(family), qf, value);
+    table.put(put);
+
+    table.flushCommits();
+    admin.flush(tableName);
+  }
+
+  @Test
+  public void testCleanerNoDelay() throws Exception {
+    init(0); // no clean delay
+
+    Path mobDirPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+
+    byte[] dummyData = makeDummyData(600);
+    long ts = System.currentTimeMillis() - 3 * secondsOfDay() * 1000; // 3 days before
+    putKVAndFlush(table, row1, dummyData, ts);
+    FileStatus[] firstFiles = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    //the first mob file
+    assertEquals("Before cleanup without delay 1", 1, firstFiles.length);
+    String firstFile = firstFiles[0].getPath().getName();
+
+    ts = System.currentTimeMillis() - 1 * secondsOfDay() * 1000; // 1 day before
+    putKVAndFlush(table, row2, dummyData, ts);
+    FileStatus[] secondFiles = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    //now there are 2 mob files
+    assertEquals("Before cleanup without delay 2", 2, secondFiles.length);
+    String f1 = secondFiles[0].getPath().getName();
+    String f2 = secondFiles[1].getPath().getName();
+    String secondFile = f1.equals(firstFile) ? f2 : f1;
+
+    modifyColumn(2); // ttl = 2, make the first row expired
+
+    //run the cleaner
+    String[] args = new String[2];
+    args[0] = tableName;
+    args[1] = family;
+    ToolRunner.run(TEST_UTIL.getConfiguration(), new ExpiredMobFileCleaner(), args);
+
+    FileStatus[] filesAfterClean = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    String lastFile = filesAfterClean[0].getPath().getName();
+    //the first mob fie is removed
+    assertEquals("After cleanup without delay 1", 1, filesAfterClean.length);
+    assertEquals("After cleanup without delay 2", secondFile, lastFile);
+  }
+
+  @Test
+  public void testCleanerWithDelay() throws Exception {
+
+    init(2); // 2 days clean delay
+
+    Path mobDirPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+
+    byte[] dummyData = makeDummyData(600);
+    long ts = System.currentTimeMillis() - 4 * secondsOfDay() * 1000; // 4 days before
+    putKVAndFlush(table, row1, dummyData, ts);
+    FileStatus[] firstFiles = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    assertEquals("Before cleanup with delay 1", 1, firstFiles.length);
+    String mob1 = firstFiles[0].getPath().getName();
+
+    ts = System.currentTimeMillis() - 2 * secondsOfDay() * 1000; // 2 days before
+    putKVAndFlush(table, row2, dummyData, ts);
+    putKVAndFlush(table, row3, dummyData, ts);
+    FileStatus[] secondFiles = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    assertEquals("Before cleanup with delay 2", 3, secondFiles.length);
+
+    //set ttl = 1 day, so row2 and row3 won't be cleaned, row1 will be.
+    modifyColumn(1);
+
+    //run the cleaner
+    String[] args = new String[2];
+    args[0] = tableName;
+    args[1] = family;
+    ToolRunner.run(TEST_UTIL.getConfiguration(), new ExpiredMobFileCleaner(), args);
+
+    FileStatus[] thirdFiles = TEST_UTIL.getTestFileSystem().listStatus(mobDirPath);
+    assertEquals("After cleanup with delay 1", 2, thirdFiles.length);
+
+    Set<String> files3 = new HashSet<String>();
+    for (int i = 0; i < thirdFiles.length; i++)
+      files3.add(thirdFiles[i].getPath().getName());
+
+    //only the first mob file is deleted due to the setting of  MobConstants.MOB_CLEAN_DELAY
+    assertEquals("After cleanup with delay 2", false, files3.contains(mob1));
+  }
+
+  private Path getMobFamilyPath(Configuration conf, String tableNameStr,
+                                String familyName) {
+    Path p = new Path(MobUtils.getMobRegionPath(conf, TableName.valueOf(tableNameStr)),
+        familyName);
+    return p;
+  }
+
+  private int secondsOfDay() {
+    return 24 * 3600;
+  }
+
+  private byte[] makeDummyData(int size) {
+    byte [] dummyData = new byte[size];
+    new Random().nextBytes(dummyData);
+    return dummyData;
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepMapper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepMapper.java
new file mode 100644
index 0000000..adfdc79
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepMapper.java
@@ -0,0 +1,60 @@
+package org.apache.hadoop.hbase.regionserver.mob.compactions;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.SmallTests;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.mob.compactions.SweepMapper;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+import static org.junit.Assert.assertEquals;
+import static org.mockito.Mockito.*;
+
+@Category(SmallTests.class)
+public class TestSweepMapper {
+
+  @Test
+  public void TestMap() throws Exception {
+    Configuration configuration = new Configuration();
+    String prefix = "00000000";
+    final String fileName = "19691231f2cd014ea28f42788214560a21a44cef";
+    final String mobFilePath = prefix + fileName;
+
+    ImmutableBytesWritable r = new ImmutableBytesWritable(Bytes.toBytes("r"));
+    final KeyValue[] kvList = new KeyValue[1];
+    kvList[0] = new KeyValue(Bytes.toBytes("row"), Bytes.toBytes("family"),
+            Bytes.toBytes("column"), Bytes.toBytes(mobFilePath));
+
+    Result columns = mock(Result.class);
+    when(columns.raw()).thenReturn(kvList);
+
+    Mapper<ImmutableBytesWritable, Result, Text, KeyValue>.Context ctx =
+            mock(Mapper.Context.class);
+    when(ctx.getConfiguration()).thenReturn(configuration);
+    SweepMapper map = new SweepMapper();
+    doAnswer(new Answer<Void>() {
+
+      @Override
+      public Void answer(InvocationOnMock invocation) throws Throwable {
+        Text text = (Text) invocation.getArguments()[0];
+        KeyValue kv = (KeyValue) invocation.getArguments()[1];
+
+        assertEquals(Bytes.toString(text.getBytes(), 0, text.getLength()), fileName);
+        assertEquals(0, Bytes.compareTo(kv.getKey(), kvList[0].getKey()));
+
+        return null;
+      }
+    }).when(ctx).write(any(Text.class), any(KeyValue.class));
+
+    map.map(r, columns, ctx);
+
+  }
+
+
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepReducer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepReducer.java
new file mode 100644
index 0000000..68d80ea
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweepReducer.java
@@ -0,0 +1,194 @@
+package org.apache.hadoop.hbase.regionserver.mob.compactions;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.CommonConfigurationKeys;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.mob.compactions.SweepCounter;
+import org.apache.hadoop.hbase.mob.compactions.SweepJob;
+import org.apache.hadoop.hbase.mob.compactions.SweepReducer;
+import org.apache.hadoop.hbase.regionserver.*;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.serializer.JavaSerialization;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.hadoop.mapreduce.counters.GenericCounter;
+import org.apache.hadoop.mapreduce.Counter;
+import org.junit.*;
+import org.junit.experimental.categories.Category;
+import org.mockito.Matchers;
+import java.util.*;
+import static org.junit.Assert.assertEquals;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.when;
+
+@Category(MediumTests.class)
+public class TestSweepReducer {
+
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private final static String tableName = "testSweepReducer";
+  private final static String row = "row";
+  private final static String family = "family";
+  private final static String qf = "qf";
+  private final static String value = "value";
+  private static HTable table;
+  private static HBaseAdmin admin;
+
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    TEST_UTIL.getConfiguration().setInt("hbase.master.info.port", 0);
+    TEST_UTIL.getConfiguration().setBoolean("hbase.regionserver.info.port.auto", true);
+    TEST_UTIL.getConfiguration().setClass(
+            DefaultStoreEngine.DEFAULT_STORE_FLUSHER_CLASS_KEY,
+            DefaultMobStoreFlusher.class, DefaultStoreFlusher.class);
+
+    TEST_UTIL.getConfiguration().setInt("hfile.format.version", 3);
+
+    TEST_UTIL.startMiniCluster(1);
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+  }
+
+  @SuppressWarnings("deprecation")
+  @Before
+  public void setUp() throws Exception {
+    HTableDescriptor desc = new HTableDescriptor(tableName);
+    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    hcd.setValue(MobConstants.IS_MOB, "true");
+    hcd.setMaxVersions(4);
+    desc.addFamily(hcd);
+
+    admin = TEST_UTIL.getHBaseAdmin();
+    admin.createTable(desc);
+    table = new HTable(TEST_UTIL.getConfiguration(), tableName);
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    admin.disableTable(tableName);
+    admin.deleteTable(tableName);
+    admin.close();
+  }
+
+  private List<String> getKeyFromSequenceFile(FileSystem fs, Path path,
+                                              Configuration conf) throws Exception {
+    List<String> list = new ArrayList<String>();
+    SequenceFile.Reader reader = new SequenceFile.Reader(fs, path, conf);
+
+    String next = (String) reader.next((String) null);
+    while (next != null) {
+      list.add(next);
+      next = (String) reader.next((String) null);
+    }
+
+    return list;
+  }
+
+
+  private Path getMobFamilyPath(Configuration conf, String tableNameStr, String familyName) {
+    Path p = new Path(MobUtils.getMobRegionPath(conf, TableName.valueOf(tableNameStr)),
+            familyName);
+    return p;
+  }
+
+  @Test
+  public void testRun() throws Exception {
+
+    byte[] mobValueBytes = new byte[100];
+
+    //get the path where mob files lie in
+    Path mobFamilyPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+
+    KeyValue kv = new KeyValue(Bytes.toBytes(row), Bytes.toBytes(family),
+            Bytes.toBytes(qf), 1, KeyValue.Type.Put, mobValueBytes);
+    KeyValue kv_ignore = new KeyValue(Bytes.toBytes(row + "ignore"), Bytes.toBytes(family),
+            Bytes.toBytes(qf), 1, KeyValue.Type.Put, mobValueBytes);
+
+    Put put = new Put(kv.getRow());
+    put.add(kv);
+    Put put2 = new Put(kv_ignore.getRow());
+    put2.add(kv_ignore);
+    table.put(put);
+    table.put(put2);
+    table.flushCommits();
+    admin.flush(tableName);
+
+
+    FileStatus[] fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    //check the generation of a mob file
+    assertEquals(1, fileStatuses.length);
+
+    String mobFile1 = fileStatuses[0].getPath().getName();
+
+    Configuration configuration = new Configuration(TEST_UTIL.getConfiguration());
+    configuration.setFloat(MobConstants.MOB_COMPACTION_INVALID_FILE_RATIO, 0.1f);
+    configuration.setStrings(TableInputFormat.INPUT_TABLE, tableName);
+    configuration.setStrings(TableInputFormat.SCAN_COLUMN_FAMILY, family);
+    configuration.setStrings("mob.compaction.visited.dir", "jobWorkingNamesDir");
+    configuration.setStrings(SweepJob.WORKING_FILES_DIR_KEY, "compactionFileDir");
+    configuration.setStrings(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,
+            JavaSerialization.class.getName());
+    configuration.set("mob.compaction.visited.dir", "compactionVisitedDir");
+    configuration.setLong(MobConstants.MOB_COMPACTION_START_DATE,
+        System.currentTimeMillis() + 24 * 3600 * 1000);
+
+
+    //use the same counter when mocking
+    Counter counter = new GenericCounter();
+    Reducer<Text, KeyValue, Writable, Writable>.Context ctx =
+            mock(Reducer.Context.class);
+    when(ctx.getConfiguration()).thenReturn(configuration);
+    when(ctx.getCounter(Matchers.any(SweepCounter.class))).thenReturn(counter);
+    when(ctx.nextKey()).thenReturn(true).thenReturn(false);
+    when(ctx.getCurrentKey()).thenReturn(new Text(mobFile1));
+
+    byte[] refBytes = Bytes.toBytes(mobFile1);
+    long valueLength = refBytes.length;
+    byte[] newValue = Bytes.add(Bytes.toBytes(valueLength), refBytes);
+    KeyValue kv2 = new KeyValue(Bytes.toBytes(row), Bytes.toBytes(family),
+            Bytes.toBytes(qf), 1, KeyValue.Type.Put, newValue);
+    List<KeyValue> list = new ArrayList<KeyValue>();
+    list.add(kv2);
+
+    when(ctx.getValues()).thenReturn(list);
+
+    SweepReducer reducer = new SweepReducer();
+    reducer.run(ctx);
+
+    FileStatus[] filsStatuses2 = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    String mobFile2 = filsStatuses2[0].getPath().getName();
+    //new mob file is generated, old one has been archived
+    assertEquals(1, filsStatuses2.length);
+    assertEquals(false, mobFile2.equalsIgnoreCase(mobFile1));
+
+
+    //test sequence file
+    String workingPath = configuration.get("mob.compaction.visited.dir");
+    FileStatus[] statuses = TEST_UTIL.getTestFileSystem().listStatus(new Path(workingPath));
+    Set<String> files = new TreeSet<String>();
+    for (FileStatus st : statuses) {
+      files.addAll(getKeyFromSequenceFile(TEST_UTIL.getTestFileSystem(),
+              st.getPath(), configuration));
+    }
+    assertEquals(1, files.size());
+    assertEquals(true, files.contains(mobFile1));
+
+
+  }
+
+
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweeper.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweeper.java
new file mode 100644
index 0000000..f926e9f
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/mob/compactions/TestSweeper.java
@@ -0,0 +1,274 @@
+package org.apache.hadoop.hbase.regionserver.mob.compactions;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.client.*;
+import org.apache.hadoop.hbase.mob.MobConstants;
+import org.apache.hadoop.hbase.mob.MobUtils;
+import org.apache.hadoop.hbase.mob.compactions.*;
+import org.apache.hadoop.hbase.regionserver.*;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.util.ToolRunner;
+import org.junit.*;
+import org.junit.experimental.categories.Category;
+import java.io.IOException;
+import java.util.*;
+import static org.junit.Assert.assertEquals;
+
+@Category(MediumTests.class)
+public class TestSweeper {
+
+
+  private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
+  private String tableName;
+  private final static String row = "row_";
+  private final static String family = "family";
+  private final static String column = "column";
+  private static HTable table;
+  private static HBaseAdmin admin;
+
+  private Random random = new Random();
+  @BeforeClass
+  public static void setUpBeforeClass() throws Exception {
+    TEST_UTIL.getConfiguration().setInt("hbase.master.info.port", 0);
+    TEST_UTIL.getConfiguration().setBoolean("hbase.regionserver.info.port.auto", true);
+    TEST_UTIL.getConfiguration().setClass(
+            DefaultStoreEngine.DEFAULT_STORE_FLUSHER_CLASS_KEY,
+            DefaultMobStoreFlusher.class, DefaultStoreFlusher.class);
+
+    TEST_UTIL.getConfiguration().setInt("hfile.format.version", 3);
+
+    TEST_UTIL.startMiniCluster();
+
+    TEST_UTIL.startMiniMapReduceCluster();
+  }
+
+  @AfterClass
+  public static void tearDownAfterClass() throws Exception {
+    TEST_UTIL.shutdownMiniCluster();
+    TEST_UTIL.shutdownMiniMapReduceCluster();
+  }
+
+  @SuppressWarnings("deprecation")
+  @Before
+  public void setUp() throws Exception {
+    long tid = System.currentTimeMillis();
+    tableName = "testSweeper" + tid;
+    HTableDescriptor desc = new HTableDescriptor(tableName);
+    HColumnDescriptor hcd = new HColumnDescriptor(family);
+    hcd.setValue(MobConstants.IS_MOB, "true");
+    hcd.setMaxVersions(4);
+    desc.addFamily(hcd);
+
+    admin = TEST_UTIL.getHBaseAdmin();
+    admin.createTable(desc);
+    table = new HTable(TEST_UTIL.getConfiguration(), tableName);
+    table.setAutoFlush(false);
+
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    admin.disableTable(tableName);
+    admin.deleteTable(tableName);
+    admin.close();
+  }
+
+  private Path getMobFamilyPath(Configuration conf, String tableNameStr,
+                                String familyName) {
+    Path p = new Path(MobUtils.getMobRegionPath(conf, TableName.valueOf(tableNameStr)),
+            familyName);
+    return p;
+  }
+
+
+  private String mergeString(Set<String> set) {
+    StringBuilder sb = new StringBuilder();
+    for (String s : set)
+      sb.append(s);
+    return sb.toString();
+  }
+
+
+  private void generateMobTable(int count, int flushStep)
+          throws IOException, InterruptedException {
+    if (count <= 0 || flushStep <= 0)
+      return;
+    int index = 0;
+    for (int i = 0; i < count; i++) {
+      byte[] mobVal = new byte[101*1024];
+      random.nextBytes(mobVal);
+
+      Put put = new Put(Bytes.toBytes(row + i));
+      put.add(Bytes.toBytes(family), Bytes.toBytes(column), mobVal);
+      table.put(put);
+      if (index++ % flushStep == 0) {
+        table.flushCommits();
+        admin.flush(tableName);
+      }
+
+
+    }
+    table.flushCommits();
+    admin.flush(tableName);
+  }
+
+  @Test
+  public void testSweeper() throws Exception {
+
+    int count = 10;
+    //create table and generate 10 mob files
+    generateMobTable(count, 1);
+
+    //get mob files
+    Path mobFamilyPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+    FileStatus[] fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    // mobFileSet0 stores the orignal mob files
+    TreeSet<String> mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    //scan the table, retreive the references
+    Scan scan = new Scan();
+    scan.setAttribute(MobConstants.MOB_SCAN_RAW, Bytes.toBytes(Boolean.TRUE));
+    scan.setFilter(new ReferenceOnlyFilter());
+    ResultScanner rs = table.getScanner(scan);
+    TreeSet<String> mobFilesScanned = new TreeSet<String>();
+    for (Result res : rs) {
+      byte[] valueBytes = res.getValue(Bytes.toBytes(family),
+          Bytes.toBytes(column));
+      mobFilesScanned.add(Bytes.toString(valueBytes, 8, valueBytes.length - 8));
+    }
+
+    //there should be 10 mob files
+    assertEquals(10, mobFilesScanned.size());
+    //check if we store the correct reference of mob files
+    assertEquals(mergeString(mobFilesSet), mergeString(mobFilesScanned));
+
+
+    Configuration conf = TEST_UTIL.getConfiguration();
+    conf.setLong(SweepJob.MOB_COMPACTION_DELAY, 24 * 60 * 60 * 1000);
+
+    String[] args = new String[2];
+    args[0] = tableName;
+    args[1] = family;
+    ToolRunner.run(conf, new Sweeper(), args);
+
+
+    mobFamilyPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+    fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    assertEquals(10, mobFilesSet.size());
+
+
+    scan = new Scan();
+    scan.setAttribute(MobConstants.MOB_SCAN_RAW, Bytes.toBytes(Boolean.TRUE));
+    scan.setFilter(new ReferenceOnlyFilter());
+    rs = table.getScanner(scan);
+    TreeSet<String> mobFilesScannedAfterJob = new TreeSet<String>();
+    for (Result res : rs) {
+      byte[] valueBytes = res.getValue(Bytes.toBytes(family), Bytes.toBytes(
+          column));
+      mobFilesScannedAfterJob.add(Bytes.toString(valueBytes, 8, valueBytes.length - 8));
+    }
+
+    assertEquals(10, mobFilesScannedAfterJob.size());
+
+    fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    assertEquals(10, mobFilesSet.size());
+    assertEquals(true, mobFilesScannedAfterJob.iterator().next()
+            .equalsIgnoreCase(mobFilesSet.iterator().next()));
+
+  }
+
+  @Test
+  public void testCompactionDelaySweeper() throws Exception {
+
+    int count = 10;
+    //create table and generate 10 mob files
+    generateMobTable(count, 1);
+
+    //get mob files
+    Path mobFamilyPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+    FileStatus[] fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    // mobFileSet0 stores the orignal mob files
+    TreeSet<String> mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    //scan the table, retreive the references
+    Scan scan = new Scan();
+    scan.setAttribute(MobConstants.MOB_SCAN_RAW, Bytes.toBytes(Boolean.TRUE));
+    scan.setFilter(new ReferenceOnlyFilter());
+    ResultScanner rs = table.getScanner(scan);
+    TreeSet<String> mobFilesScanned = new TreeSet<String>();
+    for (Result res : rs) {
+      byte[] valueBytes = res.getValue(Bytes.toBytes(family),
+              Bytes.toBytes(column));
+      mobFilesScanned.add(Bytes.toString(valueBytes, 8, valueBytes.length - 8));
+    }
+
+    //there should be 10 mob files
+    assertEquals(10, mobFilesScanned.size());
+    //check if we store the correct reference of mob files
+    assertEquals(mergeString(mobFilesSet), mergeString(mobFilesScanned));
+
+
+    Configuration conf = TEST_UTIL.getConfiguration();
+    conf.setLong(SweepJob.MOB_COMPACTION_DELAY, 0);
+
+    String[] args = new String[2];
+    args[0] = tableName;
+    args[1] = family;
+    ToolRunner.run(conf, new Sweeper(), args);
+
+
+    mobFamilyPath = getMobFamilyPath(TEST_UTIL.getConfiguration(), tableName, family);
+    fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    assertEquals(1, mobFilesSet.size());
+
+
+    scan = new Scan();
+    scan.setAttribute(MobConstants.MOB_SCAN_RAW, Bytes.toBytes(Boolean.TRUE));
+    scan.setFilter(new ReferenceOnlyFilter());
+    rs = table.getScanner(scan);
+    TreeSet<String> mobFilesScannedAfterJob = new TreeSet<String>();
+    for (Result res : rs) {
+      byte[] valueBytes = res.getValue(Bytes.toBytes(family), Bytes.toBytes(
+              column));
+      mobFilesScannedAfterJob.add(Bytes.toString(valueBytes, 8, valueBytes.length - 8));
+    }
+
+    assertEquals(1, mobFilesScannedAfterJob.size());
+
+    fileStatuses = TEST_UTIL.getTestFileSystem().listStatus(mobFamilyPath);
+    mobFilesSet = new TreeSet<String>();
+    for (FileStatus status : fileStatuses) {
+      mobFilesSet.add(status.getPath().getName());
+    }
+
+    assertEquals(1, mobFilesSet.size());
+    assertEquals(true, mobFilesScannedAfterJob.iterator().next()
+            .equalsIgnoreCase(mobFilesSet.iterator().next()));
+
+  }
+
+}
