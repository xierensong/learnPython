From e35232292bc3a233e1480011884e7adc957307c5 Mon Sep 17 00:00:00 2001
From: huzheng <openinx@gmail.com>
Date: Wed, 2 May 2018 10:44:42 +0800
Subject: [PATCH] HBASE-20481 Add a setSerial method to ReplicationEndpoint

---
 .../hbase/replication/ReplicationEndpoint.java     |   7 +
 .../HBaseInterClusterReplicationEndpoint.java      | 235 ++++++++++++---------
 .../RegionReplicaReplicationEndpoint.java          |   4 +
 .../regionserver/ReplicationSource.java            |   3 +
 .../visibility/VisibilityReplicationEndpoint.java  |   5 +
 .../TestReplicationAdminWithClusters.java          |   5 +
 .../replication/SerialReplicationTestBase.java     |   4 +
 .../hbase/replication/TestReplicationEndpoint.java |   4 +
 .../TestSerialReplicationEndpoint.java             | 201 ++++++++++++++++++
 9 files changed, 373 insertions(+), 95 deletions(-)
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestSerialReplicationEndpoint.java

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationEndpoint.java
index 543dc2f..ee8563e 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/ReplicationEndpoint.java
@@ -121,6 +121,13 @@ public interface ReplicationEndpoint extends ReplicationPeerConfigListener {
    */
   void init(Context context) throws IOException;
 
+  /**
+   * Set the replication endpoint to be serial, which means that all entries from source will be
+   * replicate to sink side serially by their sequence id order.
+   * @throws IOException
+   */
+  void setSerial() throws IOException;
+
   /** Whether or not, the replication endpoint can replicate to it's source cluster with the same
    * UUID */
   boolean canReplicateToSameCluster();
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
index fd3c671..f3753c3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java
@@ -24,9 +24,7 @@ import java.net.SocketTimeoutException;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 import java.util.concurrent.Callable;
 import java.util.concurrent.CompletionService;
 import java.util.concurrent.ExecutionException;
@@ -37,6 +35,9 @@ import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -107,6 +108,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
   private Path hfileArchiveDir;
   private boolean replicationBulkLoadDataEnabled;
   private Abortable abortable;
+  private boolean isSerial = false;
   private boolean dropOnDeletedTables;
 
   @Override
@@ -162,6 +164,11 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     hfileArchiveDir = new Path(rootDir, new Path(HConstants.HFILE_ARCHIVE_DIRECTORY, baseNSDir));
   }
 
+  @Override
+  public void setSerial() throws IOException {
+    this.isSerial = true;
+  }
+
   private void decorateConf() {
     String replicationCodec = this.conf.get(HConstants.REPLICATION_CODEC_CONF_KEY);
     if (StringUtils.isNotEmpty(replicationCodec)) {
@@ -203,40 +210,60 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     return sleepMultiplier < maxRetriesMultiplier;
   }
 
-  private List<List<Entry>> createBatches(final List<Entry> entries) {
+  private int getEstimatedEntrySize(Entry e) {
+    long size = e.getKey().estimatedSerializedSizeOf() + e.getEdit().estimatedSerializedSizeOf();
+    return (int) size;
+  }
+
+  private List<List<Entry>> createParallelBatches(final List<Entry> entries) {
     int numSinks = Math.max(replicationSinkMgr.getNumSinks(), 1);
-    int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);
-    // Maintains the current batch for a given partition index
-    Map<Integer, List<Entry>> entryMap = new HashMap<>(n);
-    List<List<Entry>> entryLists = new ArrayList<>();
+    int n = Math.min(Math.min(this.maxThreads, entries.size() / 100 + 1), numSinks);
+    List<List<Entry>> entryLists =
+        Stream.generate(ArrayList<Entry>::new).limit(n).collect(Collectors.toList());
     int[] sizes = new int[n];
-
-    for (int i = 0; i < n; i++) {
-      entryMap.put(i, new ArrayList<Entry>(entries.size()/n+1));
-    }
-
-    for (Entry e: entries) {
-      int index = Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n);
-      int entrySize = (int)e.getKey().estimatedSerializedSizeOf() +
-          (int)e.getEdit().estimatedSerializedSizeOf();
-      // If this batch is oversized, add it to final list and initialize a new empty batch
-      if (sizes[index] > 0 /* must include at least one entry */ &&
-          sizes[index] + entrySize > replicationRpcLimit) {
-        entryLists.add(entryMap.get(index));
-        entryMap.put(index, new ArrayList<Entry>());
+    for (Entry e : entries) {
+      int index = Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName()) % n);
+      int entrySize = getEstimatedEntrySize(e);
+      // If this batch has at least one entry and is over sized, move it to the tail of list and
+      // initialize the entryLists[index] to be a empty list.
+      if (sizes[index] > 0 && sizes[index] + entrySize > replicationRpcLimit) {
+        entryLists.add(entryLists.get(index));
+        entryLists.set(index, new ArrayList<>());
         sizes[index] = 0;
       }
-      entryMap.get(index).add(e);
+      entryLists.get(index).add(e);
       sizes[index] += entrySize;
     }
+    return entryLists;
+  }
 
-    entryLists.addAll(entryMap.values());
+  private List<List<Entry>> createSerialBatches(final List<Entry> entries) {
+    List<List<Entry>> entryLists = new ArrayList<>();
+    int sizeOfLastBatch = 0;
+    for (Entry e : entries) {
+      int entrySize = getEstimatedEntrySize(e);
+      if (entryLists.isEmpty()
+          || (sizeOfLastBatch > 0 && sizeOfLastBatch + entrySize > replicationRpcLimit)) {
+        entryLists.add(new ArrayList<>());
+        sizeOfLastBatch = 0;
+      }
+      entryLists.get(entryLists.size() - 1).add(e);
+      sizeOfLastBatch += entrySize;
+    }
     return entryLists;
   }
 
+  private List<List<Entry>> createBatches(final List<Entry> entries) {
+    if (isSerial) {
+      return createSerialBatches(entries);
+    } else {
+      return createParallelBatches(entries);
+    }
+  }
+
   private TableName parseTable(String msg) {
     // ... TableNotFoundException: '<table>'/n...
-    Pattern p = Pattern.compile("TableNotFoundException: \\'([\\S]*)\\'");
+    Pattern p = Pattern.compile("TableNotFoundException: '([\\S]*)'");
     Matcher m = p.matcher(msg);
     if (m.find()) {
       String table = m.group(1);
@@ -254,13 +281,8 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
   private List<List<Entry>> filterBatches(final List<List<Entry>> oldEntryList, TableName table) {
     List<List<Entry>> entryLists = new ArrayList<>();
     for (List<Entry> entries : oldEntryList) {
-      ArrayList<Entry> thisList = new ArrayList<Entry>(entries.size());
-      entryLists.add(thisList);
-      for (Entry e : entries) {
-        if (!e.getKey().getTableName().equals(table)) {
-          thisList.add(e);
-        }
-      }
+      entryLists.add(entries.stream().filter(e -> !e.getKey().getTableName().equals(table))
+          .collect(Collectors.toList()));
     }
     return entryLists;
   }
@@ -277,13 +299,71 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     }
   }
 
+  private long parallelReplicate(ReplicateContext replicateContext, List<List<Entry>> batches)
+      throws IOException {
+    CompletionService<Integer> pool = new ExecutorCompletionService<>(this.exec);
+    int futures = 0;
+    for (int i = 0; i < batches.size(); i++) {
+      List<Entry> entries = batches.get(i);
+      if (!entries.isEmpty()) {
+        LOG.trace("Submitting {} entries of total size {}", entries.size(),
+          replicateContext.getSize());
+        // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource
+        pool.submit(createReplicator(entries, i));
+        futures++;
+      }
+    }
+
+    IOException iox = null;
+    long lastWriteTime = 0;
+    for (int i = 0; i < futures; i++) {
+      try {
+        // wait for all futures, remove successful parts
+        // (only the remaining parts will be retried)
+        Future<Integer> f = pool.take();
+        int index = f.get();
+        List<Entry> batch = batches.get(index);
+        batches.set(index, Collections.emptyList()); // remove successful batch
+        // Find the most recent write time in the batch
+        long writeTime = batch.get(batch.size() - 1).getKey().getWriteTime();
+        if (writeTime > lastWriteTime) {
+          lastWriteTime = writeTime;
+        }
+      } catch (InterruptedException ie) {
+        iox = new IOException(ie);
+      } catch (ExecutionException ee) {
+        // cause must be an IOException
+        iox = (IOException) ee.getCause();
+      }
+    }
+    if (iox != null) {
+      // if we had any exceptions, try again
+      throw iox;
+    }
+    return lastWriteTime;
+  }
+
+  private long serialReplicate(ReplicateContext replicateContext, List<List<Entry>> batches)
+      throws IOException {
+    long lastWriteTime = 0L;
+    for (int i = 0; i < batches.size(); i++) {
+      Replicator replicator = createReplicator(batches.get(i), i);
+      LOG.trace("Submitting {} entries of total size {}", batches.get(i).size(),
+        replicateContext.getSize());
+      replicator.call();
+    }
+    if (batches.size() > 0) {
+      List<Entry> batch = batches.get(batches.size() - 1);
+      lastWriteTime = batch.get(batch.size() - 1).getKey().getWriteTime();
+    }
+    return lastWriteTime;
+  }
+
   /**
    * Do the shipping logic
    */
   @Override
   public boolean replicate(ReplicateContext replicateContext) {
-    CompletionService<Integer> pool = new ExecutorCompletionService<>(this.exec);
-    List<List<Entry>> batches;
     String walGroupId = replicateContext.getWalGroupId();
     int sleepMultiplier = 1;
 
@@ -299,8 +379,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
       return false;
     }
 
-    batches = createBatches(replicateContext.getEntries());
-
+    List<List<Entry>> batches = createBatches(replicateContext.getEntries());
     while (this.isRunning() && !exec.isShutdown()) {
       if (!isPeerEnabled()) {
         if (sleepForRetries("Replication is disabled", sleepMultiplier)) {
@@ -312,52 +391,20 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
         reconnectToPeerCluster();
       }
       try {
-        int futures = 0;
-        for (int i=0; i<batches.size(); i++) {
-          List<Entry> entries = batches.get(i);
-          if (!entries.isEmpty()) {
-            if (LOG.isTraceEnabled()) {
-              LOG.trace("Submitting " + entries.size() +
-                  " entries of total size " + replicateContext.getSize());
-            }
-            // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource
-            pool.submit(createReplicator(entries, i));
-            futures++;
-          }
-        }
-        IOException iox = null;
-
-        long lastWriteTime = 0;
-        for (int i=0; i<futures; i++) {
-          try {
-            // wait for all futures, remove successful parts
-            // (only the remaining parts will be retried)
-            Future<Integer> f = pool.take();
-            int index = f.get().intValue();
-            List<Entry> batch = batches.get(index);
-            batches.set(index, Collections.<Entry>emptyList()); // remove successful batch
-            // Find the most recent write time in the batch
-            long writeTime = batch.get(batch.size() - 1).getKey().getWriteTime();
-            if (writeTime > lastWriteTime) {
-              lastWriteTime = writeTime;
-            }
-          } catch (InterruptedException ie) {
-            iox =  new IOException(ie);
-          } catch (ExecutionException ee) {
-            // cause must be an IOException
-            iox = (IOException)ee.getCause();
-          }
-        }
-        if (iox != null) {
-          // if we had any exceptions, try again
-          throw iox;
+        long lastWriteTime;
+
+        // replicate the batches to sink side.
+        if (this.isSerial) {
+          lastWriteTime = serialReplicate(replicateContext, batches);
+        } else {
+          lastWriteTime = parallelReplicate(replicateContext, batches);
         }
+
         // update metrics
         if (lastWriteTime > 0) {
           this.metrics.setAgeOfLastShippedOp(lastWriteTime, walGroupId);
         }
         return true;
-
       } catch (IOException ioe) {
         // Didn't ship anything, but must still age the last time we did
         this.metrics.refreshAgeOfLastShippedOp(walGroupId);
@@ -376,7 +423,9 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
                     // Would potentially be better to retry in one of the outer loops
                     // and add a table filter there; but that would break the encapsulation,
                     // so we're doing the filtering here.
-                    LOG.info("Missing table detected at sink, local table also does not exist, filtering edits for '"+table+"'");
+                    LOG.info(
+                      "Missing table detected at sink, local table also does not exist, filtering edits for '"
+                          + table + "'");
                     batches = filterBatches(batches, table);
                     continue;
                   }
@@ -395,9 +444,10 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
             // This exception means we waited for more than 60s and nothing
             // happened, the cluster is alive and calling it right away
             // even for a test just makes things worse.
-            sleepForRetries("Encountered a SocketTimeoutException. Since the " +
-              "call to the remote cluster timed out, which is usually " +
-              "caused by a machine failure or a massive slowdown",
+            sleepForRetries(
+              "Encountered a SocketTimeoutException. Since the "
+                  + "call to the remote cluster timed out, which is usually "
+                  + "caused by a machine failure or a massive slowdown",
               this.socketTimeoutMultiplier);
           } else if (ioe instanceof ConnectException || ioe instanceof UnknownHostException) {
             LOG.warn("Peer is unavailable, rechecking all sinks: ", ioe);
@@ -420,7 +470,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
 
   @Override
   protected void doStop() {
-    disconnect(); //don't call super.doStop()
+    disconnect(); // don't call super.doStop()
     if (this.conn != null) {
       try {
         this.conn.close();
@@ -437,9 +487,9 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
     }
     // Abort if the tasks did not terminate in time
     if (!exec.isTerminated()) {
-      String errMsg = "HBaseInterClusterReplicationEndpoint termination failed. The " +
-          "ThreadPoolExecutor failed to finish all tasks within " + maxTerminationWait + "ms. " +
-          "Aborting to prevent Replication from deadlocking. See HBASE-16081.";
+      String errMsg = "HBaseInterClusterReplicationEndpoint termination failed. The "
+          + "ThreadPoolExecutor failed to finish all tasks within " + maxTerminationWait + "ms. "
+          + "Aborting to prevent Replication from deadlocking. See HBASE-16081.";
       abortable.abort(errMsg, new IOException(errMsg));
     }
     notifyStopped();
@@ -454,6 +504,7 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
   protected class Replicator implements Callable<Integer> {
     private List<Entry> entries;
     private int ordinal;
+
     public Replicator(List<Entry> entries, int ordinal) {
       this.entries = entries;
       this.ordinal = ordinal;
@@ -464,24 +515,18 @@ public class HBaseInterClusterReplicationEndpoint extends HBaseReplicationEndpoi
         throws IOException {
       if (LOG.isTraceEnabled()) {
         long size = 0;
-        for (Entry e: entries) {
-          size += e.getKey().estimatedSerializedSizeOf();
-          size += e.getEdit().estimatedSerializedSizeOf();
+        for (Entry e : entries) {
+          size += getEstimatedEntrySize(e);
         }
-        LOG.trace("Replicating batch " + System.identityHashCode(entries) + " of " +
-            entries.size() + " entries with total size " + size + " bytes to " +
-            replicationClusterId);
+        LOG.trace("Replicating batch {} of {} entries with total size {} bytes to {}",
+          System.identityHashCode(entries), entries.size(), size, replicationClusterId);
       }
       try {
         ReplicationProtbufUtil.replicateWALEntry(rrs, batch.toArray(new Entry[batch.size()]),
           replicationClusterId, baseNamespaceDir, hfileArchiveDir);
-        if (LOG.isTraceEnabled()) {
-          LOG.trace("Completed replicating batch " + System.identityHashCode(entries));
-        }
+        LOG.trace("Completed replicating batch {}", System.identityHashCode(entries));
       } catch (IOException e) {
-        if (LOG.isTraceEnabled()) {
-          LOG.trace("Failed replicating batch " + System.identityHashCode(entries), e);
-        }
+        LOG.trace("Failed replicating batch {}", System.identityHashCode(entries), e);
         throw e;
       }
     }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
index cb755fe..fda8394 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/RegionReplicaReplicationEndpoint.java
@@ -140,6 +140,10 @@ public class RegionReplicaReplicationEndpoint extends HBaseReplicationEndpoint {
   }
 
   @Override
+  public void setSerial() throws IOException {
+  }
+
+  @Override
   protected void doStart() {
     try {
       connection = (ClusterConnection) ConnectionFactory.createConnection(this.conf);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
index 236c575..48eea91 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java
@@ -278,6 +278,9 @@ public class ReplicationSource implements ReplicationSourceInterface {
     replicationEndpoint
         .init(new ReplicationEndpoint.Context(conf, replicationPeer.getConfiguration(), fs, peerId,
             clusterId, replicationPeer, metrics, tableDescriptors, server));
+    if (replicationPeer.getPeerConfig().isSerial()) {
+      replicationEndpoint.setSerial();
+    }
     replicationEndpoint.start();
     replicationEndpoint.awaitRunning(waitOnEndpointSeconds, TimeUnit.SECONDS);
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
index cd495ce..3c0d038 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.java
@@ -58,6 +58,11 @@ public class VisibilityReplicationEndpoint implements ReplicationEndpoint {
   }
 
   @Override
+  public void setSerial() throws IOException {
+    delegator.setSerial();
+  }
+
+  @Override
   public void peerConfigUpdated(ReplicationPeerConfig rpc){
     delegator.peerConfigUpdated(rpc);
   }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
index 7be8c16..48eb27e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/replication/TestReplicationAdminWithClusters.java
@@ -22,6 +22,7 @@ import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
+import java.io.IOException;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.Map;
@@ -299,6 +300,10 @@ public class TestReplicationAdminWithClusters extends TestReplicationBase {
 
 
     @Override
+    public void setSerial() throws IOException {
+    }
+
+    @Override
     public UUID getPeerUUID() {
       return UUID.randomUUID();
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SerialReplicationTestBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SerialReplicationTestBase.java
index 259914e..2cd1e5e 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SerialReplicationTestBase.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/SerialReplicationTestBase.java
@@ -75,6 +75,10 @@ public class SerialReplicationTestBase {
     private static final UUID PEER_UUID = UUID.randomUUID();
 
     @Override
+    public void setSerial() throws IOException {
+    }
+
+    @Override
     public UUID getPeerUUID() {
       return PEER_UUID;
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
index 3fca0ec..98ee9e6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationEndpoint.java
@@ -393,6 +393,10 @@ public class TestReplicationEndpoint extends TestReplicationBase {
     }
 
     @Override
+    public void setSerial() throws IOException {
+    }
+
+    @Override
     public UUID getPeerUUID() {
       return uuid;
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestSerialReplicationEndpoint.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestSerialReplicationEndpoint.java
new file mode 100644
index 0000000..c51f0c0
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/replication/regionserver/TestSerialReplicationEndpoint.java
@@ -0,0 +1,201 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.replication.regionserver;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.LinkedBlockingQueue;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HBaseClassTestRule;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.Waiter;
+import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder;
+import org.apache.hadoop.hbase.client.Connection;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.client.TableDescriptor;
+import org.apache.hadoop.hbase.client.TableDescriptorBuilder;
+import org.apache.hadoop.hbase.ipc.RpcServer;
+import org.apache.hadoop.hbase.replication.ReplicationPeerConfig;
+import org.apache.hadoop.hbase.testclassification.MediumTests;
+import org.apache.hadoop.hbase.testclassification.ReplicationTests;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.wal.WAL.Entry;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+import org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList;
+import org.apache.hbase.thirdparty.com.google.common.collect.ImmutableMap;
+
+@Category({ ReplicationTests.class, MediumTests.class })
+public class TestSerialReplicationEndpoint {
+
+  @ClassRule
+  public static final HBaseClassTestRule CLASS_RULE =
+      HBaseClassTestRule.forClass(TestSerialReplicationEndpoint.class);
+
+  private static HBaseTestingUtility UTIL = new HBaseTestingUtility();
+  private static Configuration CONF;
+  private static Connection CONN;
+
+  @BeforeClass
+  public static void setUp() throws Exception {
+    UTIL.startMiniCluster();
+    CONF = UTIL.getConfiguration();
+    CONF.setLong(RpcServer.MAX_REQUEST_SIZE, 102400);
+    CONN = UTIL.getConnection();
+  }
+
+  @AfterClass
+  public static void tearDown() throws Exception {
+    IOUtils.closeQuietly(CONN);
+    UTIL.shutdownMiniCluster();
+  }
+
+  private String getZKClusterKey() {
+    return String.format("127.0.0.1:%d:%s", UTIL.getZkCluster().getClientPort(),
+      CONF.get(HConstants.ZOOKEEPER_ZNODE_PARENT));
+  }
+
+  private void testHBaseReplicationEndpoint(String tableNameStr, String peerId, boolean isSerial)
+      throws IOException {
+    TestEndpoint.reset();
+    int cellNum = 10000;
+
+    TableName tableName = TableName.valueOf(tableNameStr);
+    byte[] family = Bytes.toBytes("f");
+    byte[] qualifier = Bytes.toBytes("q");
+    TableDescriptor td =
+        TableDescriptorBuilder.newBuilder(tableName).setColumnFamily(ColumnFamilyDescriptorBuilder
+            .newBuilder(family).setScope(HConstants.REPLICATION_SCOPE_GLOBAL).build()).build();
+    UTIL.createTable(td, null);
+
+    try (Admin admin = CONN.getAdmin()) {
+      ReplicationPeerConfig peerConfig = ReplicationPeerConfig.newBuilder()
+          .setClusterKey(getZKClusterKey()).setReplicationEndpointImpl(TestEndpoint.class.getName())
+          .setReplicateAllUserTables(false).setSerial(isSerial)
+          .setTableCFsMap(ImmutableMap.of(tableName, ImmutableList.of())).build();
+      admin.addReplicationPeer(peerId, peerConfig);
+    }
+
+    try (Table table = CONN.getTable(tableName)) {
+      for (int i = 0; i < cellNum; i++) {
+        Put put = new Put(Bytes.toBytes(i)).addColumn(family, qualifier, System.currentTimeMillis(),
+          Bytes.toBytes(i));
+        table.put(put);
+      }
+    }
+    Waiter.waitFor(CONF, 60000, () -> TestEndpoint.getEntries().size() >= cellNum);
+
+    int index = 0;
+    Assert.assertEquals(TestEndpoint.getEntries().size(), cellNum);
+    if (!isSerial) {
+      Collections.sort(TestEndpoint.getEntries(), (a, b) -> {
+        long seqA = a.getKey().getSequenceId();
+        long seqB = b.getKey().getSequenceId();
+        return seqA == seqB ? 0 : (seqA < seqB ? -1 : 1);
+      });
+    }
+    for (Entry entry : TestEndpoint.getEntries()) {
+      Assert.assertEquals(entry.getKey().getTableName(), tableName);
+      Assert.assertEquals(entry.getEdit().getCells().size(), 1);
+      Cell cell = entry.getEdit().getCells().get(0);
+      Assert.assertArrayEquals(
+        Bytes.copy(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()),
+        Bytes.toBytes(index));
+      index++;
+    }
+    Assert.assertEquals(index, cellNum);
+  }
+
+  @Test
+  public void testSerialReplicate() throws Exception {
+    testHBaseReplicationEndpoint("testSerialReplicate", "100", true);
+  }
+
+  @Test
+  public void testParallelReplicate() throws Exception {
+    testHBaseReplicationEndpoint("testParallelReplicate", "101", false);
+  }
+
+  public static class TestEndpoint extends HBaseInterClusterReplicationEndpoint {
+
+    private final static BlockingQueue<Entry> entryQueue = new LinkedBlockingQueue<>();
+
+    public static void reset() {
+      entryQueue.clear();
+    }
+
+    public static List<Entry> getEntries() {
+      return new ArrayList<>(entryQueue);
+    }
+
+    @Override
+    public boolean canReplicateToSameCluster() {
+      return true;
+    }
+
+    @Override
+    protected TestReplicator createReplicator(List<Entry> entries, int ordinal) {
+      return new TestReplicator(entries, ordinal);
+    }
+
+    @Override
+    public synchronized List<ServerName> getRegionServers() {
+      // Return multiple server names for endpoint parallel replication.
+      return new ArrayList<>(
+          ImmutableList.of(ServerName.valueOf("www.example.com", 12016, 1525245876026L),
+            ServerName.valueOf("www.example2.com", 12016, 1525245876026L),
+            ServerName.valueOf("www.example3.com", 12016, 1525245876026L),
+            ServerName.valueOf("www.example4.com", 12016, 1525245876026L),
+            ServerName.valueOf("www.example4.com", 12016, 1525245876026L)));
+    }
+
+    private class TestReplicator extends Replicator {
+      private List<Entry> entries;
+      private int ordinal;
+
+      public TestReplicator(List<Entry> entries, int ordinal) {
+        super(entries, ordinal);
+        this.entries = entries;
+        this.ordinal = ordinal;
+      }
+
+      @Override
+      public Integer call() throws IOException {
+        entryQueue.addAll(this.entries);
+        return ordinal;
+      }
+    }
+  }
+}
-- 
2.7.4

