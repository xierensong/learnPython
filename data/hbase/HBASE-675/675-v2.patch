Index: src/java/org/apache/hadoop/hbase/mapred/TableSplit.java
===================================================================
--- src/java/org/apache/hadoop/hbase/mapred/TableSplit.java	(revision 718421)
+++ src/java/org/apache/hadoop/hbase/mapred/TableSplit.java	(working copy)
@@ -34,11 +34,12 @@
   private byte [] m_tableName;
   private byte [] m_startRow;
   private byte [] m_endRow;
+  private String m_regionLocation;
 
   /** default constructor */
   public TableSplit() {
     this(HConstants.EMPTY_BYTE_ARRAY, HConstants.EMPTY_BYTE_ARRAY,
-      HConstants.EMPTY_BYTE_ARRAY);
+      HConstants.EMPTY_BYTE_ARRAY, "");
   }
 
   /**
@@ -47,10 +48,12 @@
    * @param startRow
    * @param endRow
    */
-  public TableSplit(byte [] tableName, byte [] startRow, byte [] endRow) {
+  public TableSplit(byte [] tableName, byte [] startRow, byte [] endRow,
+      final String location) {
     m_tableName = tableName;
     m_startRow = startRow;
     m_endRow = endRow;
+    this.m_regionLocation = location;
   }
 
   /** @return table name */
@@ -73,26 +76,38 @@
     return 0;
   }
 
+  /** @return the region's hostname */
+  public String getRegionLocation() {
+    return m_regionLocation;
+  }
+
   public String[] getLocations() {
     // Return a random node from the cluster for now
-    return new String[] { };
+    return new String[] {this.m_regionLocation};
   }
 
   public void readFields(DataInput in) throws IOException {
     this.m_tableName = Bytes.readByteArray(in);
     this.m_startRow = Bytes.readByteArray(in);
     this.m_endRow = Bytes.readByteArray(in);
+    this.m_regionLocation = Bytes.toString(Bytes.readByteArray(in));
   }
 
   public void write(DataOutput out) throws IOException {
     Bytes.writeByteArray(out, this.m_tableName);
     Bytes.writeByteArray(out, this.m_startRow);
     Bytes.writeByteArray(out, this.m_endRow);
+    Bytes.writeByteArray(out, Bytes.toBytes(this.m_regionLocation));
   }
 
   @Override
   public String toString() {
-    return Bytes.toString(m_tableName) +"," + Bytes.toString(m_startRow) +
-      "," + Bytes.toString(m_endRow);
+    return m_regionLocation + "," + Bytes.toString(m_tableName) +"," +
+      Bytes.toString(m_startRow) + "," + Bytes.toString(m_endRow);
   }
+
+  public int compareTo(Object other) {
+    TableSplit otherSplit = (TableSplit)other;
+    return Bytes.compareTo(getStartRow(), otherSplit.getStartRow());
+  }
 }
\ No newline at end of file
Index: src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java
===================================================================
--- src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java	(revision 718421)
+++ src/java/org/apache/hadoop/hbase/mapred/TableInputFormatBase.java	(working copy)
@@ -85,13 +85,13 @@
    */
   protected class TableRecordReader
   implements RecordReader<ImmutableBytesWritable, RowResult> {
-    private byte [] startRow;
-    private byte [] endRow;
     private byte [] lastRow;
+    private byte[][] regions;
     private RowFilterInterface trrRowFilter;
     private Scanner scanner;
     private HTable htable;
     private byte [][] trrInputColumns;
+    private int currentregion;
 
     /**
      * Restart from survivable exceptions by creating a new scanner.
@@ -99,7 +99,10 @@
      * @param firstRow
      * @throws IOException
      */
-    public void restart(byte[] firstRow) throws IOException {
+    public void restart(final byte[] firstRow) throws IOException {
+      currentregion = getRegion(row);
+      byte[] startRow = regions[currentregion * 2 + 0];
+      byte[] endRow = regions[currentregion * 2 + 1];
       if ((endRow != null) && (endRow.length > 0)) {
         if (trrRowFilter != null) {
           final Set<RowFilterInterface> rowFiltersSet =
@@ -125,9 +128,28 @@
      * @throws IOException
      */
     public void init() throws IOException {
-      restart(startRow);
+      restart(regions[0]);
     }
 
+    void setRegions(final byte[][] regions) {
+        this.regions = regions;
+    }
+
+    int getRegion(final byte[] row) {
+      for (int i = 0 ; i < regions.length ; i += 2) {
+        byte[] startRow = regions[i + 0];
+        byte[] endRow = regions[i + 1];
+        if (startRow.length > 0 && Bytes.compareTo(startRow, row) > 0) {
+          continue;
+        }
+        if (endRow.length > 0 && Bytes.compareTo(row, endRow) >= 0) {
+          continue;
+        }
+        return i/2;
+      }
+      return -1;
+    }
+
     /**
      * @param htable the {@link HTable} to scan.
      */
@@ -193,8 +215,8 @@
     }
 
     public float getProgress() {
-      // Depends on the total number of tuples and getPos
-      return 0;
+      int nregions = regions.length / 2;
+      return ((100 * (2 * currentregion + 1)) / (2 * nregions)) / 100f;
     }
 
     /**
@@ -203,9 +225,9 @@
      * @return true if there was more data
      * @throws IOException
      */
-    @SuppressWarnings("unchecked")
     public boolean next(ImmutableBytesWritable key, RowResult value)
     throws IOException {
+      while (true) {
       RowResult result;
       try {
         result = this.scanner.next();
@@ -216,12 +238,18 @@
         result = this.scanner.next();
       }
       boolean hasMore = result != null && result.size() > 0;
+      if (hasMore && currentRegion + 1 < regions.length / 2) {
+        // move to the next region
+        restart(regions[(currentregion+1)*2]);
+        continue;
+      }
       if (hasMore) {
         key.set(result.getRow());
         lastRow = key.get();
         Writables.copyWritable(result, value);
       }
       return hasMore;
+      }
     }
   }
 
@@ -248,6 +276,7 @@
     trr.setHTable(this.table);
     trr.setInputColumns(this.inputColumns);
     trr.setRowFilter(this.rowFilter);
+    trr.setRegions().regions;
     trr.init();
     return trr;
   }
@@ -270,6 +299,7 @@
    * @see org.apache.hadoop.mapred.InputFormat#getSplits(org.apache.hadoop.mapred.JobConf, int)
    */
   public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
+    InputSplit[] splits = null;
     byte [][] startKeys = this.table.getStartKeys();
     if (startKeys == null || startKeys.length == 0) {
       throw new IOException("Expecting at least one region");
@@ -280,6 +310,82 @@
     if (this.inputColumns == null || this.inputColumns.length == 0) {
       throw new IOException("Expecting at least one column");
     }
+
+        int nregions = 0;
+        int nhosts = 0;
+        HashMap<String, ArrayList<HRegionInfo>> hosts = new
+HashMap<String,
+ArrayList<HRegionInfo>>();
+        for (java.util.Map.Entry<HRegionInfo,HServerAddress> e :
+table.getRegionsInfo().entrySet()) {
+            String host = e.getValue().getHostname();
+            ArrayList<HRegionInfo> regions = hosts.get(host);
+            if (regions == null) {
+            regions = new ArrayList<HRegionInfo>();
+            hosts.put(host, regions);
+            nhosts++;
+            }
+            regions.add(e.getKey());
+            nregions++;
+        }
+        if (numSplits < nhosts) {
+            numSplits = nhosts;
+        }
+        if (numSplits > nregions) {
+            numSplits = nregions;
+        }
+        float sph = (float)numSplits/nhosts;
+        float sphremainder = 0f;
+        ArrayList<InputSplit> splitlist = new ArrayList<InputSplit>();
+        for (String host : hosts.keySet()) {
+            ArrayList<HRegionInfo> regions = hosts.get(host);
+            float rps = ((regions.size() - 1) + sphremainder) / (sph - 1);
+            sphremainder = sph;
+            float rpsremainder = 0f;
+            for (int i = 0 ; i < regions.size() ;) {
+            rpsremainder += rps;
+            int splitSize = Math.max(1, (int)rpsremainder);
+            if (i + splitSize > regions.size()) {
+                splitSize = regions.size() - i;
+            }
+            //System.out.println(host + ": " + numSplits + "/" + nregions
++
+"/" + splitSize + ":");
+            byte[][] splitregions = new byte[splitSize*2][];
+            for (int j = 0 ; j < splitSize ; j++) {
+                HRegionInfo region = regions.get(i + j);
+                splitregions[j*2 + 0] = region.getStartKey();
+                splitregions[j*2 + 1] = region.getEndKey();
+            }
+            splitlist.add(new MultiRegionTableSplit(table.getTableName(),
+host, splitregions));
+            i += splitSize;
+            rpsremainder -= splitSize;
+            sphremainder -= 1;
+            }
+        }
+        // copy into a real array (there must be a better way)
+        int n = splitlist.size();
+        //n = 1;
+        splits = new InputSplit[n];
+        for (int i = splits.length ; i-- > 0 ;) {
+            splits[i] = splitlist.get(i);
+        }
+        Arrays.sort(splits);
+        break;
+        }
+    }
+    // dump the splits
+    if (false) {
+        System.out.println("---- " + splits.length + " splits ----");
+        for (int i = 0 ; i < splits.length ; i++) {
+        System.out.println(i + ": " + splits[i].toString());
+        }
+        System.exit(1);
+    // return the splits
+    return splits;
+    }
+/* OLD STUFF
     int realNumSplits = numSplits > startKeys.length ? startKeys.length
         : numSplits;
     InputSplit[] splits = new InputSplit[realNumSplits];
@@ -288,16 +394,18 @@
     for (int i = 0; i < realNumSplits; i++) {
       int lastPos = startPos + middle;
       lastPos = startKeys.length % realNumSplits > i ? lastPos + 1 : lastPos;
+      String regionLocation = table.getRegionLocation(startKeys[startPos]).
+        getServerAddress().getHostname();
       splits[i] = new TableSplit(this.table.getTableName(),
-          startKeys[startPos], ((i + 1) < realNumSplits) ? startKeys[lastPos]
-              : HConstants.EMPTY_START_ROW);
+          startKeys[startPos], ((i + 1) < realNumSplits) ? startKeys[lastPos]:
+          HConstants.EMPTY_START_ROW, regionLocation);
       if (LOG.isDebugEnabled()) {
         LOG.debug("split: " + i + "->" + splits[i]);
       }
       startPos = lastPos;
     }
     return splits;
-
+*/
   }
 
   /**
Index: src/java/org/apache/hadoop/hbase/mapred/MultiRegionTableSplit.java
===================================================================
--- src/java/org/apache/hadoop/hbase/mapred/MultiRegionTableSplit.java	(revision 0)
+++ src/java/org/apache/hadoop/hbase/mapred/MultiRegionTableSplit.java	(revision 0)
@@ -0,0 +1,80 @@
+/**
+ * Copyright 2007 The Apache Software Foundation
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapred;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Split that crosses multiple regions.
+ */
+public class MultiRegionTableSplit extends TableSplit {
+  private byte [][] regions;
+
+  public MultiRegionTableSplit() {
+    super();
+  }
+
+  public MultiRegionTableSplit(byte [] tableName, String location,
+    final byte [][] regions) {
+    super(tableName, regions[0], regions[regions.length - 1], location);
+    this.regions = regions;
+  }
+
+  public void readFields(DataInput in) throws IOException {
+    super.readFields(in);
+    int n = in.readInt();
+    this.regions = new byte [n][];
+    for (int i = 0; i < n; i++) {
+      regions[i] = Bytes.readByteArray(in);
+    }
+  }
+
+  public void write(DataOutput out) throws IOException {
+    super.write(out);
+    out.writeInt(regions.length);
+    for (int i = 0; i < regions.length; i++) {
+      Bytes.writeByteArray(out, regions[i]);
+    }
+  }
+
+  public String toString() {
+    String str = super.toString();
+    for (int i = 0; i < regions.length; i += 2) {
+      if (i > 0) {
+        str += ", ";
+      }
+      str += Bytes.toString(regions[i]) + "-" + Bytes.toString(regions[i + 1]);
+    }
+    return str;
+  }
+
+  public int compareTo(Object other) {
+    MultiRegionTableSplit otherSplit = (MultiRegionTableSplit)other;
+    int result = otherSplit.regions.length - regions.length;
+    if (result == 0) {
+      result = Bytes.compareTo(getStartRow(), otherSplit.getStartRow());
+    }
+    return result;
+  }
+}
\ No newline at end of file
