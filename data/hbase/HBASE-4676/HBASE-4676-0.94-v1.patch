Index: src/main/java/org/apache/hadoop/hbase/KeyValue.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/KeyValue.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/KeyValue.java	(working copy)
@@ -211,12 +211,12 @@
   public static final KeyValue LOWESTKEY =
     new KeyValue(HConstants.EMPTY_BYTE_ARRAY, HConstants.LATEST_TIMESTAMP);
 
-  private byte [] bytes = null;
-  private int offset = 0;
-  private int length = 0;
+  protected byte [] bytes = null;
+  protected int offset = 0;
+  protected int length = 0;
 
   // the row cached
-  private volatile byte [] rowCache = null;
+  protected volatile byte [] rowCache = null;
 
   /**
    * @return True if a delete type, a {@link KeyValue.Type#Delete} or
@@ -239,7 +239,7 @@
   }
 
   // default value is 0, aka DNC
-  private long memstoreTS = 0;
+  protected long memstoreTS = 0;
 
   /** Dragon time over, return to normal business */
 
@@ -818,7 +818,7 @@
   /**
    * @return Length of key portion.
    */
-  private int keyLength = 0;
+  protected int keyLength = 0;
 
   public int getKeyLength() {
     if (keyLength == 0) {
@@ -1027,7 +1027,7 @@
    *
    * @return Timestamp
    */
-  private long timestampCache = -1;
+  protected long timestampCache = -1;
   public long getTimestamp() {
     if (timestampCache == -1) {
       timestampCache = getTimestamp(getKeyLength());
Index: src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java	(working copy)
@@ -38,7 +38,8 @@
   // id 1 is reserved for the BITSET algorithm to be added later
   PREFIX(2, new PrefixKeyDeltaEncoder()),
   DIFF(3, new DiffKeyDeltaEncoder()),
-  FAST_DIFF(4, new FastDiffDeltaEncoder());
+  FAST_DIFF(4, new FastDiffDeltaEncoder()),
+  TRIE(5, createTrieEncoder());
 
   private final short id;
   private final byte[] idInBytes;
@@ -176,5 +177,17 @@
   public static DataBlockEncoding getEncodingById(short dataBlockEncodingId) {
     return idToEncoding.get(dataBlockEncodingId);
   }
-
+  public static DataBlockEncoder createTrieEncoder(){
+	try {
+	  return (DataBlockEncoder)Class.forName(
+				  "org.apache.hadoop.hbase.io.encoding.PrefixTrieDataBlockEncoder")
+				  .newInstance();
+	} catch (InstantiationException e) {
+		throw new IllegalArgumentException("can't instantiate PrefixTrieDataBlockEncoder", e);
+	} catch (IllegalAccessException e) {
+		throw new IllegalArgumentException("can't access PrefixTrieDataBlockEncoder", e);
+	} catch (ClassNotFoundException e) {
+		throw new IllegalArgumentException("can't find PrefixTrieDataBlockEncoder", e);
+	}
+  }
 }
Index: src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java	(working copy)
@@ -1533,14 +1533,14 @@
    * We always prefetch the header of the next block, so that we know its
    * on-disk size in advance and can read it in one operation.
    */
-  private static class PrefetchedHeader {
+  public static class PrefetchedHeader {
     long offset = -1;
     byte[] header = new byte[HEADER_SIZE];
     ByteBuffer buf = ByteBuffer.wrap(header, 0, HEADER_SIZE);
   }
 
   /** Reads version 2 blocks from the filesystem. */
-  static class FSReaderV2 extends AbstractFSReader {
+  public static class FSReaderV2 extends AbstractFSReader {
 
     // The configuration states that we should validate hbase checksums
     private final boolean useHBaseChecksumConfigured;
@@ -1562,7 +1562,7 @@
     protected HFileDataBlockEncoder dataBlockEncoder =
         NoOpDataBlockEncoder.INSTANCE;
 
-    private ThreadLocal<PrefetchedHeader> prefetchedHeaderForThread =
+    public ThreadLocal<PrefetchedHeader> prefetchedHeaderForThread =
         new ThreadLocal<PrefetchedHeader>() {
           @Override
           public PrefetchedHeader initialValue() {
Index: src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java	(working copy)
@@ -35,6 +35,8 @@
  * options.
  */
 public class HFileDataBlockEncoderImpl implements HFileDataBlockEncoder {
+	private static final int INITIAL_ENCODING_BUFFER_BYTES = 1<<16;//64KB
+	
   private final DataBlockEncoding onDisk;
   private final DataBlockEncoding inCache;
 
@@ -175,7 +177,8 @@
   private ByteBuffer encodeBufferToHFileBlockBuffer(ByteBuffer in,
       DataBlockEncoding algo, boolean includesMemstoreTS,
       byte[] dummyHeader) {
-    ByteArrayOutputStream encodedStream = new ByteArrayOutputStream();
+    ByteArrayOutputStream encodedStream = new ByteArrayOutputStream(
+        INITIAL_ENCODING_BUFFER_BYTES);
     DataOutputStream dataOut = new DataOutputStream(encodedStream);
     DataBlockEncoder encoder = algo.getEncoder();
     try {
Index: src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java	(working copy)
@@ -235,7 +235,7 @@
    * @param dataBlockEncoder data block encoding algorithm.
    * @throws IOException When opening the reader fails.
    */
-  StoreFile(final FileSystem fs,
+  public StoreFile(final FileSystem fs,
             final Path p,
             final Configuration conf,
             final CacheConfig cacheConf,
Index: src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(revision 1308619)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java	(working copy)
@@ -38,7 +38,7 @@
  * KeyValueScanner adaptor over the Reader.  It also provides hooks into
  * bloom filter things.
  */
-class StoreFileScanner implements KeyValueScanner {
+public class StoreFileScanner implements KeyValueScanner {
   static final Log LOG = LogFactory.getLog(Store.class);
 
   // the reader it comes from:
Index: src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java	(revision 1308619)
+++ src/test/java/org/apache/hadoop/hbase/HBaseTestingUtility.java	(working copy)
@@ -1924,10 +1924,12 @@
     return hloc.getPort();
   }
 
-  public HRegion createTestRegion(String tableName, HColumnDescriptor hcd)
+  public HRegion createTestRegion(String tableName, HColumnDescriptor... hcds)
       throws IOException {
     HTableDescriptor htd = new HTableDescriptor(tableName);
-    htd.addFamily(hcd);
+    for(HColumnDescriptor hcd : hcds){
+	  htd.addFamily(hcd);
+  	}
     HRegionInfo info =
         new HRegionInfo(Bytes.toBytes(tableName), null, null, false);
     HRegion region =
Index: src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java	(revision 1308619)
+++ src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java	(working copy)
@@ -26,8 +26,11 @@
 
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
+import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.WritableUtils;
 
+import com.google.common.collect.Lists;
+
 /**
  * Generate list of key values which are very useful to test data block encoding
  * and compression.
@@ -48,6 +51,14 @@
   static int DEFAULT_QUALIFIER_LENGTH_VARIANCE = 3;
 
   static int DEFAULT_COLUMN_FAMILY_LENGTH = 9;
+  static List<String> COLUMN_FAMILY_CHOICES = Lists.newArrayList();
+  static{
+    COLUMN_FAMILY_CHOICES.add("abcdefghi");
+    COLUMN_FAMILY_CHOICES.add("123456789");
+    COLUMN_FAMILY_CHOICES.add("agoldfish");
+    COLUMN_FAMILY_CHOICES.add("THECOLFAM");
+    COLUMN_FAMILY_CHOICES.add("etcfametc");
+  }
   static int DEFAULT_VALUE_LENGTH = 8;
   static float DEFAULT_CHANCE_FOR_ZERO_VALUE = 0.5f;
 
@@ -191,8 +202,8 @@
     Map<Integer, List<byte[]>> rowsToQualifier =
         new HashMap<Integer, List<byte[]>>();
 
-    byte[] family = new byte[columnFamilyLength];
-    randomizer.nextBytes(family);
+    int familyIndex = randomizer.nextInt(COLUMN_FAMILY_CHOICES.size());
+    byte[] family = Bytes.toBytes(COLUMN_FAMILY_CHOICES.get(familyIndex));
 
     long baseTimestamp = Math.abs(randomizer.nextLong()) /
         baseTimestampDivide;
