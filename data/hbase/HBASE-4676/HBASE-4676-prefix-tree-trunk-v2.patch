diff --git a/bin/index-builder-setup.rb b/bin/index-builder-setup.rb
new file mode 100644
index 0000000..cda7fdd
--- /dev/null
+++ b/bin/index-builder-setup.rb
@@ -0,0 +1,31 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Set up sample data for IndexBuilder example
+create "people", "attributes"
+create "people-email", "INDEX"
+create "people-phone", "INDEX"
+create "people-name", "INDEX"
+
+[["1", "jenny", "jenny@example.com", "867-5309"],
+ ["2", "alice", "alice@example.com", "555-1234"],
+ ["3", "kevin", "kevinpet@example.com", "555-1212"]].each do |fields|
+  (id, name, email, phone) = *fields
+  put "people", id, "attributes:name", name
+  put "people", id, "attributes:email", email
+  put "people", id, "attributes:phone", phone
+end
+  
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
index 2a60eb7..573d682 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValue.java
@@ -32,6 +32,8 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
 import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ClassSize;
@@ -63,7 +65,7 @@ import com.google.common.primitives.Longs;
  */
 @InterfaceAudience.Public
 @InterfaceStability.Evolving
-public class KeyValue implements Writable, HeapSize {
+public class KeyValue implements Cell, Writable, HeapSize {
   static final Log LOG = LogFactory.getLog(KeyValue.class);
   // TODO: Group Key-only comparators and operations into a Key class, just
   // for neatness sake, if can figure what to call it.
@@ -261,6 +263,7 @@ public class KeyValue implements Writable, HeapSize {
   /** Here be dragons **/
 
   // used to achieve atomic operations in the memstore.
+  @Override
   public long getMemstoreTS() {
     return memstoreTS;
   }
@@ -831,19 +834,21 @@ public class KeyValue implements Writable, HeapSize {
         value, voffset, vlength);
   }
 
-  // Needed doing 'contains' on List.  Only compares the key portion, not the
-  // value.
+  /**
+   * Needed doing 'contains' on List.  Only compares the key portion, not the value.
+   * 
+   * For temporary backwards compatibility with the original KeyValue.equals method, we ignore the
+   * memstoreTS.
+   */
+  @Override
   public boolean equals(Object other) {
-    if (!(other instanceof KeyValue)) {
+    if (!(other instanceof Cell)) {
       return false;
     }
-    KeyValue kv = (KeyValue)other;
-    // Comparing bytes should be fine doing equals test.  Shouldn't have to
-    // worry about special .META. comparators doing straight equals.
-    return Bytes.equals(getBuffer(), getKeyOffset(), getKeyLength(),
-      kv.getBuffer(), kv.getKeyOffset(), kv.getKeyLength());
+    return CellComparator.equalsIgnoreMemstoreTS(this, (Cell)other);
   }
 
+  @Override
   public int hashCode() {
     byte[] b = getBuffer();
     int start = getOffset(), end = getOffset() + getLength();
@@ -864,6 +869,7 @@ public class KeyValue implements Writable, HeapSize {
    * Clones a KeyValue.  This creates a copy, re-allocating the buffer.
    * @return Fully copied clone of this KeyValue
    */
+  @Override
   public KeyValue clone() {
     byte [] b = new byte[this.length];
     System.arraycopy(this.bytes, this.offset, b, 0, this.length);
@@ -1042,8 +1048,17 @@ public class KeyValue implements Writable, HeapSize {
   }
 
   /**
+   * @return the backing array of the entire KeyValue
+   */
+  @Override
+  public byte[] getValueArray() {
+    return bytes;
+  }
+
+  /**
    * @return Value offset
    */
+  @Override
   public int getValueOffset() {
     return getKeyOffset() + getKeyLength();
   }
@@ -1051,13 +1066,23 @@ public class KeyValue implements Writable, HeapSize {
   /**
    * @return Value length
    */
+  @Override
   public int getValueLength() {
     return Bytes.toInt(this.bytes, this.offset + Bytes.SIZEOF_INT);
   }
 
   /**
+   * @return the backing array of the entire KeyValue
+   */
+  @Override
+  public byte[] getRowArray() {
+    return bytes;
+  }
+
+  /**
    * @return Row offset
    */
+  @Override
   public int getRowOffset() {
     return getKeyOffset() + Bytes.SIZEOF_SHORT;
   }
@@ -1065,13 +1090,23 @@ public class KeyValue implements Writable, HeapSize {
   /**
    * @return Row length
    */
+  @Override
   public short getRowLength() {
     return Bytes.toShort(this.bytes, getKeyOffset());
   }
 
   /**
+   * @return the backing array of the entire KeyValue
+   */
+  @Override
+  public byte[] getFamilyArray() {
+    return bytes;
+  }
+
+  /**
    * @return Family offset
    */
+  @Override
   public int getFamilyOffset() {
     return getFamilyOffset(getRowLength());
   }
@@ -1086,6 +1121,7 @@ public class KeyValue implements Writable, HeapSize {
   /**
    * @return Family length
    */
+  @Override
   public byte getFamilyLength() {
     return getFamilyLength(getFamilyOffset());
   }
@@ -1098,8 +1134,17 @@ public class KeyValue implements Writable, HeapSize {
   }
 
   /**
+   * @return the backing array of the entire KeyValue
+   */
+  @Override
+  public byte[] getQualifierArray() {
+    return bytes;
+  }
+
+  /**
    * @return Qualifier offset
    */
+  @Override
   public int getQualifierOffset() {
     return getQualifierOffset(getFamilyOffset());
   }
@@ -1114,6 +1159,7 @@ public class KeyValue implements Writable, HeapSize {
   /**
    * @return Qualifier length
    */
+  @Override
   public int getQualifierLength() {
     return getQualifierLength(getRowLength(),getFamilyLength());
   }
@@ -1273,6 +1319,7 @@ public class KeyValue implements Writable, HeapSize {
    * @return Timestamp
    */
   private long timestampCache = -1;
+  @Override
   public long getTimestamp() {
     if (timestampCache == -1) {
       timestampCache = getTimestamp(getKeyLength());
@@ -1297,6 +1344,14 @@ public class KeyValue implements Writable, HeapSize {
   }
 
   /**
+   * @return KeyValue.TYPE byte representation
+   */
+  @Override
+  public byte getTypeByte() {
+    return getType(getKeyLength());
+  }
+
+  /**
    * @param keylength Pass if you have it to save on a int creation.
    * @return Type of this KeyValue.
    */
@@ -2564,13 +2619,22 @@ public class KeyValue implements Writable, HeapSize {
     }
   }
 
-  // HeapSize
+  /**
+   * HeapSize implementation
+   *
+   * We do not count the bytes in the rowCache because it should be empty for a KeyValue in the
+   * MemStore.
+   */
+  @Override
   public long heapSize() {
-    return ClassSize.align(ClassSize.OBJECT + (2 * ClassSize.REFERENCE) +
-        ClassSize.align(ClassSize.ARRAY) + ClassSize.align(length) +
-        (3 * Bytes.SIZEOF_INT) +
-        ClassSize.align(ClassSize.ARRAY) +
-        (2 * Bytes.SIZEOF_LONG));
+    int sum = 0;
+    sum += ClassSize.OBJECT;// the KeyValue object itself
+    sum += 2 * ClassSize.REFERENCE;// 2 * pointers to byte[]
+    sum += 2 * ClassSize.align(ClassSize.ARRAY);// 2 * byte[]
+    sum += ClassSize.align(length);// number of bytes of data in the "bytes" array
+    sum += 3 * Bytes.SIZEOF_INT;// offset, length, keyLength
+    sum += 2 * Bytes.SIZEOF_LONG;// timestampCache, memstoreTS
+    return ClassSize.align(sum);
   }
 
   // this overload assumes that the length bytes have already been read,
@@ -2587,11 +2651,13 @@ public class KeyValue implements Writable, HeapSize {
   }
 
   // Writable
+  @Override
   public void readFields(final DataInput in) throws IOException {
     int length = in.readInt();
     readFields(length, in);
   }
 
+  @Override
   public void write(final DataOutput out) throws IOException {
     out.writeInt(this.length);
     out.write(this.bytes, this.offset, this.length);
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
new file mode 100644
index 0000000..eddfa72
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase;
+
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.StringUtils;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+
+import com.google.common.collect.Lists;
+
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class KeyValueTestUtil {
+
+  public static KeyValue create(
+      String row,
+      String family,
+      String qualifier,
+      long timestamp,
+      String value)
+  {
+    return create(row, family, qualifier, timestamp, KeyValue.Type.Put, value);
+  }
+
+  public static KeyValue create(
+      String row,
+      String family,
+      String qualifier,
+      long timestamp,
+      KeyValue.Type type,
+      String value)
+  {
+      return new KeyValue(
+          Bytes.toBytes(row),
+          Bytes.toBytes(family),
+          Bytes.toBytes(qualifier),
+          timestamp,
+          type,
+          Bytes.toBytes(value)
+      );
+  }
+
+
+  public static ByteBuffer toByteBufferAndRewind(final Iterable<? extends KeyValue> kvs,
+      boolean includeMemstoreTS) {
+    int totalBytes = KeyValueUtils.totalLengthWithMemstoreTSs(kvs, includeMemstoreTS);
+    ByteBuffer bb = ByteBuffer.allocate(totalBytes);
+    for (KeyValue kv : IterableUtils.nullSafe(kvs)) {
+      KeyValueUtils.appendToByteBuffer(bb, kv, includeMemstoreTS);
+    }
+    bb.rewind();
+    return bb;
+  }
+
+  public static List<KeyValue> rewindThenToList(final ByteBuffer bb,
+      final boolean includesMemstoreTS) {
+    bb.rewind();
+    List<KeyValue> kvs = Lists.newArrayList();
+    KeyValue kv = null;
+    while (true) {
+      kv = KeyValueUtils.nextShallowCopy(bb, includesMemstoreTS);
+      if (kv == null) {
+        break;
+      }
+      kvs.add(kv);
+    }
+    return kvs;
+  }
+
+
+  /********************* toString ************************************/
+
+  public static String toStringWithPadding(final Collection<? extends KeyValue> kvs,
+      final boolean includeMeta) {
+    int maxRowStringLength = 0;
+    int maxFamilyStringLength = 0;
+    int maxQualifierStringLength = 0;
+    int maxTimestampLength = 0;
+    for (KeyValue kv : kvs) {
+      maxRowStringLength = Math.max(maxRowStringLength, getRowString(kv).length());
+      maxFamilyStringLength = Math.max(maxFamilyStringLength, getFamilyString(kv).length());
+      maxQualifierStringLength = Math.max(maxQualifierStringLength, getQualifierString(kv)
+        .length());
+      maxTimestampLength = Math.max(maxTimestampLength, Long.valueOf(kv.getTimestamp()).toString()
+        .length());
+    }
+    StringBuilder sb = new StringBuilder();
+    for (KeyValue kv : kvs) {
+      if (sb.length() > 0) {
+        sb.append("\n");
+      }
+      String row = toStringWithPadding(kv, maxRowStringLength, maxFamilyStringLength,
+        maxQualifierStringLength, maxTimestampLength, includeMeta);
+      sb.append(row);
+    }
+    return sb.toString();
+  }
+
+  protected static String toStringWithPadding(final KeyValue kv, final int maxRowLength,
+      int maxFamilyLength, int maxQualifierLength, int maxTimestampLength, boolean includeMeta) {
+    String leadingLengths = "";
+    String familyLength = kv.getFamilyLength() + " ";
+    if (includeMeta) {
+      leadingLengths += StringUtils.padFront(kv.getKeyLength() + "", '0', 4);
+      leadingLengths += " ";
+      leadingLengths += StringUtils.padFront(kv.getValueLength() + "", '0', 4);
+      leadingLengths += " ";
+      leadingLengths += StringUtils.padFront(kv.getRowLength() + "", '0', 2);
+      leadingLengths += " ";
+    }
+    int spacesAfterRow = maxRowLength - getRowString(kv).length() + 2;
+    int spacesAfterFamily = maxFamilyLength - getFamilyString(kv).length() + 2;
+    int spacesAfterQualifier = maxQualifierLength - getQualifierString(kv).length() + 1;
+    int spacesAfterTimestamp = maxTimestampLength
+        - Long.valueOf(kv.getTimestamp()).toString().length() + 1;
+    return leadingLengths + getRowString(kv) + StringUtils.repeat(' ', spacesAfterRow)
+        + familyLength + getFamilyString(kv) + StringUtils.repeat(' ', spacesAfterFamily)
+        + getQualifierString(kv) + StringUtils.repeat(' ', spacesAfterQualifier)
+        + getTimestampString(kv) + StringUtils.repeat(' ', spacesAfterTimestamp)
+        + getTypeString(kv) + " " + getValueString(kv);
+  }
+
+  protected static String getRowString(final KeyValue kv) {
+    return Bytes.toStringBinary(kv.getRow());
+  }
+
+  protected static String getFamilyString(final KeyValue kv) {
+    return Bytes.toStringBinary(kv.getFamily());
+  }
+
+  protected static String getQualifierString(final KeyValue kv) {
+    return Bytes.toStringBinary(kv.getQualifier());
+  }
+
+  protected static String getTimestampString(final KeyValue kv) {
+    return kv.getTimestamp() + "";
+  }
+
+  protected static String getTypeString(final KeyValue kv) {
+    return KeyValue.Type.codeToType(kv.getType()).toString();
+  }
+
+  protected static String getValueString(final KeyValue kv) {
+    return Bytes.toStringBinary(kv.getValue());
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtils.java
new file mode 100644
index 0000000..d31650e
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/KeyValueUtils.java
@@ -0,0 +1,191 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase;
+
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.CellUtils;
+import org.apache.hadoop.hbase.util.ByteBufferUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+import org.apache.hadoop.io.WritableUtils;
+
+/**
+ * static convenience methods for dealing with KeyValues and collections of KeyValues
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class KeyValueUtils {
+
+  /**************** length *********************/
+
+  public static int length(final Cell cell) {
+    int keyLength = keyLength(cell);
+    return KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keyLength + cell.getValueLength();
+  }
+
+  protected static int keyLength(final Cell cell) {
+    int length = KeyValue.KEY_INFRASTRUCTURE_SIZE;
+    length += cell.getRowLength();
+    length += cell.getFamilyLength();
+    length += cell.getQualifierLength();
+    return length;
+  }
+
+  public static int lengthWithMemstoreTS(final KeyValue kv, final boolean includeMemstoreTS) {
+    int length = kv.getLength();
+    if (includeMemstoreTS) {
+      length += WritableUtils.getVIntSize(kv.getMemstoreTS());
+    }
+    return length;
+  }
+
+  public static int totalLengthWithMemstoreTSs(final Iterable<? extends KeyValue> kvs,
+      final boolean includeMemstoreTS) {
+    int length = 0;
+    for (KeyValue kv : IterableUtils.nullSafe(kvs)) {
+      length += lengthWithMemstoreTS(kv, includeMemstoreTS);
+    }
+    return length;
+  }
+
+
+  /**************** copy key only *********************/
+
+  public static KeyValue copyToNewKeyValue(final Cell cell) {
+    KeyValue kvCell = new KeyValue(copyToNewByteArray(cell));
+    kvCell.setMemstoreTS(cell.getMemstoreTS());
+    return kvCell;
+  }
+
+  public static ByteBuffer copyKeyToNewByteBuffer(final Cell cell) {
+    byte[] bytes = new byte[keyLength(cell)];
+    appendKeyToByteArrayWithoutValue(cell, bytes, 0);
+    ByteBuffer buffer = ByteBuffer.wrap(bytes);
+    buffer.position(buffer.limit());//make it look as if each field were appended
+    return buffer;
+  }
+
+  public static byte[] copyToNewByteArray(final Cell cell) {
+    int v1Length = length(cell);
+    byte[] backingBytes = new byte[v1Length];
+    appendToByteArray(cell, backingBytes, 0);
+    return backingBytes;
+  }
+
+  protected static int appendKeyToByteArrayWithoutValue(final Cell cell, final byte[] output,
+      final int offset) {
+    int nextOffset = offset;
+    nextOffset = Bytes.putShort(output, nextOffset, cell.getRowLength());
+    nextOffset = CellUtils.copyRowTo(cell, output, nextOffset);
+    nextOffset = Bytes.putByte(output, nextOffset, cell.getFamilyLength());
+    nextOffset = CellUtils.copyFamilyTo(cell, output, nextOffset);
+    nextOffset = CellUtils.copyQualifierTo(cell, output, nextOffset);
+    nextOffset = Bytes.putLong(output, nextOffset, cell.getTimestamp());
+    nextOffset = Bytes.putByte(output, nextOffset, cell.getTypeByte());
+    return nextOffset;
+  }
+
+
+  /**************** copy key and value *********************/
+
+  public static int appendToByteArray(final Cell cell, final byte[] output, final int offset) {
+    int pos = offset;
+    pos = Bytes.putInt(output, pos, keyLength(cell));
+    pos = Bytes.putInt(output, pos, cell.getValueLength());
+    pos = appendKeyToByteArrayWithoutValue(cell, output, pos);
+    CellUtils.copyValueTo(cell, output, pos);
+    return pos + cell.getValueLength();
+  }
+
+  public static ByteBuffer copyToNewByteBuffer(final Cell cell) {
+    byte[] bytes = new byte[length(cell)];
+    appendToByteArray(cell, bytes, 0);
+    ByteBuffer buffer = ByteBuffer.wrap(bytes);
+    buffer.position(buffer.limit());//make it look as if each field were appended
+    return buffer;
+  }
+
+  public static void appendToByteBuffer(final ByteBuffer bb, final KeyValue kv,
+      final boolean includeMemstoreTS) {
+    // keep pushing the limit out. assume enough capacity
+    bb.limit(bb.position() + kv.getLength());
+    bb.put(kv.getBuffer(), kv.getOffset(), kv.getLength());
+    if (includeMemstoreTS) {
+      int numMemstoreTSBytes = WritableUtils.getVIntSize(kv.getMemstoreTS());
+      ByteBufferUtils.extendLimit(bb, numMemstoreTSBytes);
+      ByteBufferUtils.writeVLong(bb, kv.getMemstoreTS());
+    }
+  }
+
+
+  /**************** iterating *******************************/
+
+  /**
+   * Creates a new KeyValue object positioned in the supplied ByteBuffer and sets the ByteBuffer's
+   * position to the start of the next KeyValue. Does not allocate a new array or copy data.
+   */
+  public static KeyValue nextShallowCopy(final ByteBuffer bb, final boolean includesMemstoreTS) {
+    if (bb.isDirect()) {
+      throw new IllegalArgumentException("only supports heap buffers");
+    }
+    if (bb.remaining() < 1) {
+      return null;
+    }
+    int underlyingArrayOffset = bb.arrayOffset() + bb.position();
+    int keyLength = bb.getInt();
+    int valueLength = bb.getInt();
+    int kvLength = KeyValue.KEYVALUE_INFRASTRUCTURE_SIZE + keyLength + valueLength;
+    KeyValue keyValue = new KeyValue(bb.array(), underlyingArrayOffset, kvLength);
+    ByteBufferUtils.skip(bb, keyLength + valueLength);
+    if (includesMemstoreTS) {
+      long memstoreTS = ByteBufferUtils.readVLong(bb);
+      keyValue.setMemstoreTS(memstoreTS);
+    }
+    return keyValue;
+  }
+
+
+  /*************** next/previous **********************************/
+
+  /**
+   * Append single byte 0x00 to the end of the input row key
+   */
+  public static KeyValue createFirstKeyInNextRow(final Cell in){
+    byte[] nextRow = new byte[in.getRowLength() + 1];
+    System.arraycopy(in.getRowArray(), in.getRowOffset(), nextRow, 0, in.getRowLength());
+    nextRow[nextRow.length - 1] = 0;//maybe not necessary
+    return KeyValue.createFirstOnRow(nextRow);
+  }
+
+  /**
+   * Decrement the timestamp.  For tests (currently wasteful)
+   *
+   * Remember timestamps are sorted reverse chronologically.
+   * @param in
+   * @return
+   */
+  public static KeyValue previousKey(final KeyValue in) {
+    return KeyValue.createFirstOnRow(CellUtils.getRowArray(in), CellUtils.getFamilyArray(in),
+      CellUtils.getQualifierArray(in), in.getTimestamp() - 1);
+  }
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/Cell.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/Cell.java
new file mode 100644
index 0000000..a67e523
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/Cell.java
@@ -0,0 +1,172 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+
+/**
+ * Note: the original Cell implementation (KeyValue) requires that all fields be encoded as
+ * consecutive bytes in the same byte[], whereas this interface allows fields to reside in separate
+ * byte[]'s.
+ * <p/>
+ * The unit of storage in HBase consisting of the following fields:<br/>
+ * <pre>
+ * 1) row
+ * 2) column family
+ * 3) column qualifier
+ * 4) timestamp
+ * 5) type
+ * 6) memstore timestamp
+ * 7) value
+ * </pre>
+ * <p/>
+ * Uniqueness is determined by the combination of row, column family, column qualifier, and
+ * timestamp.
+ * <p/>
+ * The natural comparator will perform a bitwise comparison on row, column family, and column
+ * qualifier. Less intuitively, it will then treat the greater timestamp as the lesser value with
+ * the goal of sorting newer cells first.
+ * <p/>
+ * This interface does not include methods that allocate new byte[]'s such as those used in client
+ * or debugging code. These should be placed in a sub-interface or the {@link CellUtils} class.
+ * <p/>
+ * Cell implements Comparable<Cell> which is only meaningful when comparing to other keys in the
+ * same table. It uses {@link #CellComparator} which does not work on the -ROOT- and .META. tables.
+ * <p/>
+ * In the future, we may consider adding a boolean isOnHeap() method and a getValueBuffer() method
+ * that can be used to pass a value directly from an off-heap ByteBuffer to the network without
+ * copying into an on-heap byte[].
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public interface Cell {
+
+  //1) Row
+
+  /**
+   * Contiguous raw bytes that may start at any index in the containing array. Max length is
+   * Short.MAX_VALUE which is 32,767 bytes.
+   * @return The array containing the row bytes.
+   */
+  byte[] getRowArray();
+
+  /**
+   * @return Array index of first row byte
+   */
+  int getRowOffset();
+
+  /**
+   * @return Number of row bytes. Must be < rowArray.length - offset.
+   */
+  short getRowLength();
+
+
+  //2) Family
+
+  /**
+   * Contiguous bytes composed of legal HDFS filename characters which may start at any index in the
+   * containing array. Max length is Byte.MAX_VALUE, which is 127 bytes.
+   * @return the array containing the family bytes.
+   */
+  byte[] getFamilyArray();
+
+  /**
+   * @return Array index of first row byte
+   */
+  int getFamilyOffset();
+
+  /**
+   * @return Number of family bytes.  Must be < familyArray.length - offset.
+   */
+  byte getFamilyLength();
+
+
+  //3) Qualifier
+
+  /**
+   * Contiguous raw bytes that may start at any index in the containing array. Max length is
+   * Short.MAX_VALUE which is 32,767 bytes.
+   * @return The array containing the qualifier bytes.
+   */
+  byte[] getQualifierArray();
+
+  /**
+   * @return Array index of first qualifier byte
+   */
+  int getQualifierOffset();
+
+  /**
+   * @return Number of qualifier bytes.  Must be < qualifierArray.length - offset.
+   */
+  int getQualifierLength();
+
+
+  //4) Timestamp
+
+  /**
+   * @return Long value representing time at which this cell was "Put" into the row.  Typically
+   * represents the time of insertion, but can be any value from Long.MIN_VALUE to Long.MAX_VALUE.
+   */
+  long getTimestamp();
+
+
+  //5) Type
+
+  /**
+   * see {@link #KeyValue.TYPE}
+   * @return The byte representation of the KeyValue.TYPE of this cell: one of Put, Delete, etc
+   */
+  byte getTypeByte();
+
+
+  //6) MemstoreTS
+
+  /**
+   * Internal use only. MemstoreTS does not relate to clock time, but is rather a memstore-specific
+   * sequence ID given to an operation on the specific memstore. It always exists for cells in the
+   * memstore but is not retained forever. It may survive several flushes, but generally becomes
+   * irrelevant after the cell's row is no longer involved in any operations that require strict
+   * consistency.
+   * @return memstoreTS (always >= 0 if exists), or 0 if it no longer exists
+   */
+  long getMemstoreTS();
+
+
+  //7) Value
+
+  /**
+   * Contiguous raw bytes that may start at any index in the containing array. Max length is
+   * Integer.MAX_VALUE which is 2,147,483,648 bytes.
+   * @return The array containing the value bytes.
+   */
+  byte[] getValueArray();
+
+  /**
+   * @return Array index of first value byte
+   */
+  int getValueOffset();
+
+  /**
+   * @return Number of value bytes.  Must be < valueArray.length - offset.
+   */
+  int getValueLength();
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/CellUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/CellUtils.java
new file mode 100644
index 0000000..c26b99e
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/CellUtils.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell;
+
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public final class CellUtils {
+
+  /******************* ByteRange *******************************/
+
+  public static ByteRange fillRowRange(Cell cell, ByteRange range) {
+    return range.set(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());
+  }
+
+  public static ByteRange fillFamilyRange(Cell cell, ByteRange range) {
+    return range.set(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength());
+  }
+
+  public static ByteRange fillQualifierRange(Cell cell, ByteRange range) {
+    return range.set(cell.getQualifierArray(), cell.getQualifierOffset(),
+      cell.getQualifierLength());
+  }
+
+
+  /***************** get individual arrays for tests ************/
+
+  public static byte[] getRowArray(Cell cell){
+    byte[] output = new byte[cell.getRowLength()];
+    copyRowTo(cell, output, 0);
+    return output;
+  }
+
+  public static byte[] getFamilyArray(Cell cell){
+    byte[] output = new byte[cell.getFamilyLength()];
+    copyFamilyTo(cell, output, 0);
+    return output;
+  }
+
+  public static byte[] getQualifierArray(Cell cell){
+    byte[] output = new byte[cell.getQualifierLength()];
+    copyQualifierTo(cell, output, 0);
+    return output;
+  }
+
+  public static byte[] getValueArray(Cell cell){
+    byte[] output = new byte[cell.getValueLength()];
+    copyValueTo(cell, output, 0);
+    return output;
+  }
+
+
+  /******************** copyTo **********************************/
+
+  public static int copyRowTo(Cell cell, byte[] destination, int destinationOffset) {
+    System.arraycopy(cell.getRowArray(), cell.getRowOffset(), destination, destinationOffset,
+      cell.getRowLength());
+    return destinationOffset + cell.getRowLength();
+  }
+
+  public static int copyFamilyTo(Cell cell, byte[] destination, int destinationOffset) {
+    System.arraycopy(cell.getFamilyArray(), cell.getFamilyOffset(), destination, destinationOffset,
+      cell.getFamilyLength());
+    return destinationOffset + cell.getFamilyLength();
+  }
+
+  public static int copyQualifierTo(Cell cell, byte[] destination, int destinationOffset) {
+    System.arraycopy(cell.getQualifierArray(), cell.getQualifierOffset(), destination,
+      destinationOffset, cell.getQualifierLength());
+    return destinationOffset + cell.getQualifierLength();
+  }
+
+  public static int copyValueTo(Cell cell, byte[] destination, int destinationOffset) {
+    System.arraycopy(cell.getValueArray(), cell.getValueOffset(), destination, destinationOffset,
+      cell.getValueLength());
+    return destinationOffset + cell.getValueLength();
+  }
+
+
+  /********************* misc *************************************/
+
+  public static byte getRowByte(Cell cell, int index) {
+    return cell.getRowArray()[cell.getRowOffset() + index];
+  }
+
+
+  /********************** KeyValue (move to KeyValueUtils) *********************/
+
+  public static ByteBuffer getValueBufferShallowCopy(Cell cell) {
+    ByteBuffer buffer = ByteBuffer.wrap(cell.getValueArray(), cell.getValueOffset(),
+      cell.getValueLength());
+//    buffer.position(buffer.limit());//make it look as if value was appended
+    return buffer;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/comparator/CellComparator.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/comparator/CellComparator.java
new file mode 100644
index 0000000..fe69dbe
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/comparator/CellComparator.java
@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.comparator;
+
+import java.util.Comparator;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.util.Bytes;
+
+import com.google.common.primitives.Longs;
+
+/**
+ * Compare two traditional HBase cells.
+ *
+ * Note: This comparator is not valid for -ROOT- and .META. tables.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public class CellComparator implements Comparator<Cell> {
+
+  @Override
+  public int compare(Cell a, Cell b) {
+    return compareStatic(a, b);
+  }
+
+
+  public static int compareStatic(Cell a, Cell b) {
+    //row
+    int c = Bytes.compareTo(
+        a.getRowArray(), a.getRowOffset(), a.getRowLength(),
+        b.getRowArray(), b.getRowOffset(), b.getRowLength());
+    if (c != 0) return c;
+
+    //family
+    c = Bytes.compareTo(
+      a.getFamilyArray(), a.getFamilyOffset(), a.getFamilyLength(),
+      b.getFamilyArray(), b.getFamilyOffset(), b.getFamilyLength());
+    if (c != 0) return c;
+
+    //qualifier
+    c = Bytes.compareTo(
+        a.getQualifierArray(), a.getQualifierOffset(), a.getQualifierLength(),
+        b.getQualifierArray(), b.getQualifierOffset(), b.getQualifierLength());
+    if (c != 0) return c;
+
+    //timestamp: later sorts first
+    c = -Longs.compare(a.getTimestamp(), b.getTimestamp());
+    if (c != 0) return c;
+
+    //type
+    c = (0xff & a.getTypeByte()) - (0xff & b.getTypeByte());
+    if (c != 0) return c;
+
+    //memstore timestamp: later sorts first
+    return -Longs.compare(a.getMemstoreTS(), b.getMemstoreTS());
+  }
+
+
+  /**************** equals ****************************/
+
+  public static boolean equals(Cell a, Cell b){
+    if (!areKeyLengthsEqual(a, b)) {
+      return false;
+    }
+    //TODO compare byte[]'s in reverse since later bytes more likely to differ
+    return 0 == compareStatic(a, b);
+  }
+
+  public static boolean equalsRow(Cell a, Cell b){
+    if(!areRowLengthsEqual(a, b)){
+      return false;
+    }
+    return 0 == Bytes.compareTo(
+      a.getRowArray(), a.getRowOffset(), a.getRowLength(),
+      b.getRowArray(), b.getRowOffset(), b.getRowLength());
+  }
+
+
+  /******************** lengths *************************/
+
+  public static boolean areKeyLengthsEqual(Cell a, Cell b) {
+    return a.getRowLength() == b.getRowLength()
+        && a.getFamilyLength() == b.getFamilyLength()
+        && a.getQualifierLength() == b.getQualifierLength();
+  }
+
+  public static boolean areRowLengthsEqual(Cell a, Cell b) {
+    return a.getRowLength() == b.getRowLength();
+  }
+
+
+  /***************** special cases ****************************/
+
+  /**
+   * special case for KeyValue.equals
+   */
+  private static int compareStaticIgnoreMemstoreTS(Cell a, Cell b) {
+    //row
+    int c = Bytes.compareTo(
+        a.getRowArray(), a.getRowOffset(), a.getRowLength(),
+        b.getRowArray(), b.getRowOffset(), b.getRowLength());
+    if (c != 0) return c;
+
+    //family
+    c = Bytes.compareTo(
+      a.getFamilyArray(), a.getFamilyOffset(), a.getFamilyLength(),
+      b.getFamilyArray(), b.getFamilyOffset(), b.getFamilyLength());
+    if (c != 0) return c;
+
+    //qualifier
+    c = Bytes.compareTo(
+        a.getQualifierArray(), a.getQualifierOffset(), a.getQualifierLength(),
+        b.getQualifierArray(), b.getQualifierOffset(), b.getQualifierLength());
+    if (c != 0) return c;
+
+    //timestamp: later sorts first
+    c = -Longs.compare(a.getTimestamp(), b.getTimestamp());
+    if (c != 0) return c;
+
+    //type
+    c = (0xff & a.getTypeByte()) - (0xff & b.getTypeByte());
+    return c;
+  }
+
+  /**
+   * special case for KeyValue.equals
+   */
+  public static boolean equalsIgnoreMemstoreTS(Cell a, Cell b){
+    return 0 == compareStaticIgnoreMemstoreTS(a, b);
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScanner.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScanner.java
new file mode 100644
index 0000000..734b583
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScanner.java
@@ -0,0 +1,74 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.scanner;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+
+/**
+ * Alternate name may be CellInputStream
+ * <p/>
+ * An interface for iterating through a sequence of cells. Similar to Java's Iterator, but without
+ * the hasNext() or remove() methods. The hasNext() method is problematic because it may require
+ * actually loading the next object, which in turn requires storing the previous object somewhere.
+ * The core data block decoder should be as fast as possible, so we push the complexity and
+ * performance expense of concurrently tracking multiple cells to layers above the CellScanner.
+ * <p/>
+ * The getCurrentCell() method will return a reference to a Cell implementation. This reference may
+ * or may not point to a reusable cell implementation, so users of the CellScanner should not, for
+ * example, accumulate a List of Cells. All of the references may point to the same object, which
+ * would be the latest state of the underlying Cell. In short, the Cell is mutable.
+ * <p/>
+ * At a minimum, an implementation will need to be able to advance from one cell to the next in a
+ * LinkedList fashion. The nextQualifier(), nextFamily(), and nextRow() methods can all be
+ * implemented by calling nextCell(), however, if the DataBlockEncoding supports random access into
+ * the block then it may provide smarter versions of these methods.
+ * <p/>
+ * Typical usage:
+ * 
+ * <pre>
+ * while (scanner.nextCell()) {
+ *   Cell cell = scanner.getCurrentCell();
+ *   // do something
+ * }
+ * </pre>
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public interface CellScanner{
+
+  /**
+   * Reset any state in the scanner so it appears it was freshly opened.
+   */
+  void resetToBeforeFirstEntry();
+
+  /**
+   * We cannot peek() because some implementations need to modify the current value.
+   * @return the current Cell which may be mutable
+   */
+  Cell getCurrentCell();
+
+  /**
+   * Advance the scanner 1 cell.
+   * @return true if the next cell is found and getCurrentCell() will return a valid Cell
+   */
+  boolean nextCell();
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScannerPosition.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScannerPosition.java
new file mode 100644
index 0000000..a64d5d7
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellScannerPosition.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.scanner;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * An indicator of the state of the scanner after an operation such as nextCell() or positionAt(..).
+ * For example:
+ * <ul>
+ * <li>In a DataBlockScanner, the AFTER_LAST position indicates to the parent StoreFileScanner that
+ * it should load the next block.</li>
+ * <li>In a StoreFileScanner, the AFTER_LAST position indicates that the file has been exhausted.</li>
+ * <li>In a RegionScanner, the AFTER_LAST position indicates that the scanner should move to the
+ * next region.</li>
+ * </ul>
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public enum CellScannerPosition {
+
+  /**
+   * getCurrentCell() will NOT return a valid cell. Calling nextCell() will advance to the first
+   * cell.
+   */
+  BEFORE_FIRST,
+
+  /**
+   * getCurrentCell() will return a valid cell, but it is not the cell requested by positionAt(..),
+   * rather it is the nearest cell before the requested cell.
+   */
+  BEFORE,
+
+  /**
+   * getCurrentCell() will return a valid cell, and it is exactly the cell that was requested by
+   * positionAt(..).
+   */
+  AT,
+
+  /**
+   * getCurrentCell() will return a valid cell, but it is not the cell requested by positionAt(..),
+   * rather it is the nearest cell after the requested cell.
+   */
+  AFTER,
+
+  /**
+   * getCurrentCell() will NOT return a valid cell. Calling nextCell() will have no effect.
+   */
+  AFTER_LAST
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellSearcher.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellSearcher.java
new file mode 100644
index 0000000..b38ccf7
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/CellSearcher.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.scanner;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+
+/**
+ * Methods for seeking to a random {@link Cell} inside a sorted collection of cells. Indicates that
+ * the implementation is able to navigate between cells without iterating forward through every
+ * cell.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public interface CellSearcher extends ReversibleCellScanner {
+
+  /**
+   * Do everything within this scanner's power to find the key. Look forward and backwards.
+   * <p/>
+   * Abort as soon as we know it can't be found, possibly leaving the Searcher in an invalid state.
+   * <p/>
+   * @param key position the CellScanner exactly on this key
+   * @return true if the cell existed and getCurrentCell() holds a valid cell
+   */
+  boolean positionAt(Cell key);
+
+  /**
+   * Same as positionAt(..), but go to the extra effort of finding the previous key if there's no
+   * exact match.
+   * <p/>
+   * @param key position the CellScanner on this key or the closest cell before
+   * @return AT if exact match<br/>
+   *         BEFORE if on last cell before key<br/>
+   *         BEFORE_FIRST if key was before the first cell in this scanner's scope
+   */
+  CellScannerPosition positionAtOrBefore(Cell key);
+
+  /**
+   * Same as positionAt(..), but go to the extra effort of finding the next key if there's no exact
+   * match.
+   * <p/>
+   * @param key position the CellScanner on this key or the closest cell after
+   * @return AT if exact match<br/>
+   *         AFTER if on first cell after key<br/>
+   *         AFTER_LAST if key was after the last cell in this scanner's scope
+   */
+  CellScannerPosition positionAtOrAfter(Cell key);
+
+  /**
+   * Note: Added for backwards compatibility with {@link #KeyValueScanner.reseek()}
+   * <p/>
+   * Look for the key, but only look after the current position. Probably not needed for an
+   * efficient tree implementation, but is important for implementations without random access such
+   * as unencoded KeyValue blocks.
+   * <p/>
+   * @param key position the CellScanner exactly on this key
+   * @return true if getCurrent() holds a valid cell
+   */
+  boolean seekForwardTo(Cell key);
+
+  /**
+   * Same as seekForwardTo(..), but go to the extra effort of finding the next key if there's no
+   * exact match.
+   * <p/>
+   * @param key
+   * @return AT if exact match<br/>
+   *         AFTER if on first cell after key<br/>
+   *         AFTER_LAST if key was after the last cell in this scanner's scope
+   */
+  CellScannerPosition seekForwardToOrBefore(Cell key);
+
+  /**
+   * Same as seekForwardTo(..), but go to the extra effort of finding the next key if there's no
+   * exact match.
+   * <p/>
+   * @param key
+   * @return AT if exact match<br/>
+   *         AFTER if on first cell after key<br/>
+   *         AFTER_LAST if key was after the last cell in this scanner's scope
+   */
+  CellScannerPosition seekForwardToOrAfter(Cell key);
+
+  /**
+   * Note: This may not be appropriate to have in the interface.  Need to investigate.
+   * <p/>
+   * Position the scanner in an invalid state after the last cell: CellScannerPosition.AFTER_LAST.
+   * This is used by tests and for handling certain edge cases.
+   */
+  void positionAfterLastCell();
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/ReversibleCellScanner.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/ReversibleCellScanner.java
new file mode 100644
index 0000000..6075b00
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/scanner/ReversibleCellScanner.java
@@ -0,0 +1,54 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.scanner;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+
+/**
+ * An extension of CellScanner indicating the scanner supports iterating backwards through cells.
+ * <p>
+ * Note: This was not added to suggest that HBase should support client facing reverse Scanners, but
+ * because some {@link CellSearcher} implementations, namely PrefixTree, need a method of backing up
+ * if the positionAt(..) method goes past the requested cell.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Unstable
+public interface ReversibleCellScanner extends CellScanner {
+
+  /**
+   * Try to position the scanner one Cell before the current position.
+   * @return true if the operation was successful, meaning getCurrentCell() will return a valid
+   *         Cell.<br/>
+   *         false if there were no previous cells, meaning getCurrentCell() will return null.
+   *         Scanner position will be {@link CellScannerPosition.BEFORE_FIRST}
+   */
+  boolean previousCell();
+
+  /**
+   * Try to position the scanner in the row before the current row.
+   * @param endOfRow true for the last cell in the previous row; false for the first cell
+   * @return true if the operation was successful, meaning getCurrentCell() will return a valid
+   *         Cell.<br/>
+   *         false if there were no previous cells, meaning getCurrentCell() will return null.
+   *         Scanner position will be {@link CellScannerPosition.BEFORE_FIRST}
+   */
+  boolean previousRow(boolean endOfRow);
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/stream/CellOutputStream.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/stream/CellOutputStream.java
new file mode 100644
index 0000000..0fd3434
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/cell/stream/CellOutputStream.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.cell.stream;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.hbase.cell.Cell;
+
+/**
+ * Accepts a stream of Cells and adds them to its internal data structure. This can be used to build
+ * a block of cells during compactions and flushes, or to build a byte[] to send to the client. This
+ * could be backed by a List<KeyValue>, but more efficient implementations will append results to a
+ * byte[] to eliminate overhead, and possibly encode the cells further.
+ */
+@InterfaceAudience.Private
+@InterfaceStability.Evolving
+public interface CellOutputStream {
+
+  /**
+   * Implementation must copy the entire state of the Cell. If the appended Cell is modified
+   * immediately after the append method returns, the modifications must have absolutely no effect
+   * on the copy of the Cell that was added to the appender. For example, calling someList.add(cell)
+   * is not correct.
+   */
+  void write(Cell cell);
+
+  /**
+   * Let the implementation decide what to do.  Usually means writing accumulated data into a byte[]
+   * that can then be read from the implementation to be sent to disk, put in the block cache, or
+   * sent over the network.
+   */
+  void flush();
+
+
+  /**
+   * The following 3 methods are optimized versions of write(Cell cell). The result should be
+   * identical, however the implementation may be able to execute them much more efficiently because
+   * it does not need to compare the unchanged fields with the previous cell's.
+   * <p/>
+   * Consider the benefits during compaction when paired with a CellScanner that is also aware of
+   * row boundaries. The CellScanner can easily use these methods instead of blindly passing Cells
+   * to the write(Cell cell) method.
+   * <p/>
+   * The savings of skipping duplicate row detection are significant with long row keys. A
+   * DataBlockEncoder may store a row key once in combination with a count of how many cells are in
+   * the row. With a 100 byte row key, we can replace 100 byte comparisons with a single increment
+   * of the counter, and that is for every cell in the row.
+   */
+
+  /**
+   * Add a Cell to the output stream but repeat the previous row. 
+   */
+  void writeWithRepeatRow(Cell cell);
+
+  /**
+   * Add a Cell to the output stream but repeat the previous row, and family.  Save the effort
+   * needed to detect the duplicate fields.
+   */
+  void writeWithRepeatRowFamily(Cell cell);
+
+  /**
+   * Add a Cell to the output stream but repeat the previous row, family, and qualifier.  Save the
+   * effort needed to detect the duplicate fields.
+   */
+  void writeWithRepeatRowFamilyQualifier(Cell cell);
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
index c659fb8..f4fb434 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoder.java
@@ -162,7 +162,7 @@ public interface DataBlockEncoder {
      */
     public ByteBuffer getValueShallowCopy();
 
-    /** @return key value at current position. */
+    /** @return key value at current position with position set to limit */
     public ByteBuffer getKeyValueBuffer();
 
     /**
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java
index cba3d36..ce6b4d6 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/DataBlockEncoding.java
@@ -38,7 +38,8 @@ public enum DataBlockEncoding {
   // id 1 is reserved for the BITSET algorithm to be added later
   PREFIX(2, createEncoder("org.apache.hadoop.hbase.io.encoding.PrefixKeyDeltaEncoder")),
   DIFF(3, createEncoder("org.apache.hadoop.hbase.io.encoding.DiffKeyDeltaEncoder")),
-  FAST_DIFF(4, createEncoder("org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder"));
+  FAST_DIFF(4, createEncoder("org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder")),
+  PREFIX_TREE(5, createEncoder("org.apache.hbase.codec.prefixtree.PrefixTreeCodec"));
 
   private final short id;
   private final byte[] idInBytes;
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java
new file mode 100644
index 0000000..a82318c
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.hadoop.hbase.io.encoding;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.io.hfile.Compression;
+import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
+
+/**
+ * A default implementation of {@link HFileBlockDecodingContext}. It assumes the
+ * block data section is compressed as a whole.
+ *
+ * @see HFileBlockDefaultEncodingContext for the default compression context
+ *
+ */
+public class HFileBlockDefaultDecodingContext implements
+    HFileBlockDecodingContext {
+
+  private final Compression.Algorithm compressAlgo;
+
+  public HFileBlockDefaultDecodingContext(
+      Compression.Algorithm compressAlgo) {
+    this.compressAlgo = compressAlgo;
+  }
+
+  @Override
+  public void prepareDecoding(int onDiskSizeWithoutHeader, int uncompressedSizeWithoutHeader,
+      ByteBuffer blockBufferWithoutHeader, byte[] onDiskBlock, int offset) throws IOException {
+    DataInputStream dis = new DataInputStream(new ByteArrayInputStream(onDiskBlock, offset,
+        onDiskSizeWithoutHeader));
+
+    Compression.decompress(blockBufferWithoutHeader.array(),
+      blockBufferWithoutHeader.arrayOffset(), (InputStream) dis, onDiskSizeWithoutHeader,
+      uncompressedSizeWithoutHeader, compressAlgo);
+  }
+
+  @Override
+  public Algorithm getCompression() {
+    return compressAlgo;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java
new file mode 100644
index 0000000..6e3f7cf
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java
@@ -0,0 +1,210 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.hadoop.hbase.io.encoding;
+
+import static org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.NONE;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.io.hfile.BlockType;
+import org.apache.hadoop.hbase.io.hfile.Compression;
+import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
+import org.apache.hadoop.hbase.io.hfile.HFileBlockConstants;
+import org.apache.hadoop.io.compress.CompressionOutputStream;
+import org.apache.hadoop.io.compress.Compressor;
+
+/**
+ * A default implementation of {@link HFileBlockEncodingContext}. It will
+ * compress the data section as one continuous buffer.
+ *
+ * @see HFileBlockDefaultDecodingContext for the decompression part
+ *
+ */
+@InterfaceAudience.Private
+public class HFileBlockDefaultEncodingContext implements
+    HFileBlockEncodingContext {
+
+  private byte[] onDiskBytesWithHeader;
+  private byte[] uncompressedBytesWithHeader;
+  private BlockType blockType;
+  private final DataBlockEncoding encodingAlgo;
+
+  /** Compressor, which is also reused between consecutive blocks. */
+  private Compressor compressor;
+
+  /** Compression output stream */
+  private CompressionOutputStream compressionStream;
+
+  /** Underlying stream to write compressed bytes to */
+  private ByteArrayOutputStream compressedByteStream;
+
+  /** Compression algorithm for all blocks this instance writes. */
+  private final Compression.Algorithm compressionAlgorithm;
+
+  private ByteArrayOutputStream encodedStream = new ByteArrayOutputStream();
+  private DataOutputStream dataOut = new DataOutputStream(encodedStream);
+
+  private final byte[] dummyHeader;
+
+  /**
+   * @param compressionAlgorithm compression algorithm used
+   * @param encoding encoding used
+   * @param headerBytes dummy header bytes
+   */
+  public HFileBlockDefaultEncodingContext(
+      Compression.Algorithm compressionAlgorithm,
+      DataBlockEncoding encoding, byte[] headerBytes) {
+    this.encodingAlgo = encoding;
+    this.compressionAlgorithm =
+        compressionAlgorithm == null ? NONE : compressionAlgorithm;
+    if (this.compressionAlgorithm != NONE) {
+      compressor = compressionAlgorithm.getCompressor();
+      compressedByteStream = new ByteArrayOutputStream();
+      try {
+        compressionStream =
+            compressionAlgorithm.createPlainCompressionStream(
+                compressedByteStream, compressor);
+      } catch (IOException e) {
+        throw new RuntimeException(
+            "Could not create compression stream for algorithm "
+                + compressionAlgorithm, e);
+      }
+    }
+    if (headerBytes == null) {
+      dummyHeader = HFileBlockConstants.DUMMY_HEADER;
+    } else {
+      dummyHeader = headerBytes;
+    }
+  }
+
+  /**
+   * @param compressionAlgorithm compression algorithm
+   * @param encoding encoding
+   */
+  public HFileBlockDefaultEncodingContext(
+      Compression.Algorithm compressionAlgorithm,
+      DataBlockEncoding encoding) {
+    this(compressionAlgorithm, encoding, null);
+  }
+
+  /**
+   * prepare to start a new encoding.
+   * @throws IOException
+   */
+  public void prepareEncoding() throws IOException {
+    encodedStream.reset();
+    dataOut.write(dummyHeader);
+    if (encodingAlgo != null
+        && encodingAlgo != DataBlockEncoding.NONE) {
+      encodingAlgo.writeIdInBytes(dataOut);
+    }
+  }
+
+  @Override
+  public void postEncoding(BlockType blockType)
+      throws IOException {
+    dataOut.flush();
+    compressAfterEncoding(encodedStream.toByteArray(), blockType);
+    this.blockType = blockType;
+  }
+
+  /**
+   * @param uncompressedBytesWithHeader
+   * @param blockType
+   * @throws IOException
+   */
+  public void compressAfterEncoding(byte[] uncompressedBytesWithHeader,
+      BlockType blockType) throws IOException {
+    compressAfterEncoding(uncompressedBytesWithHeader, blockType, dummyHeader);
+  }
+
+  /**
+   * @param uncompressedBytesWithHeader
+   * @param blockType
+   * @param headerBytes
+   * @throws IOException
+   */
+  protected void compressAfterEncoding(byte[] uncompressedBytesWithHeader,
+      BlockType blockType, byte[] headerBytes) throws IOException {
+    this.uncompressedBytesWithHeader = uncompressedBytesWithHeader;
+    if (compressionAlgorithm != NONE) {
+      compressedByteStream.reset();
+      compressedByteStream.write(headerBytes);
+      compressionStream.resetState();
+      compressionStream.write(uncompressedBytesWithHeader,
+          headerBytes.length, uncompressedBytesWithHeader.length
+              - headerBytes.length);
+
+      compressionStream.flush();
+      compressionStream.finish();
+      onDiskBytesWithHeader = compressedByteStream.toByteArray();
+    } else {
+      onDiskBytesWithHeader = uncompressedBytesWithHeader;
+    }
+    this.blockType = blockType;
+  }
+
+  @Override
+  public byte[] getOnDiskBytesWithHeader() {
+    return onDiskBytesWithHeader;
+  }
+
+  @Override
+  public byte[] getUncompressedBytesWithHeader() {
+    return uncompressedBytesWithHeader;
+  }
+
+  @Override
+  public BlockType getBlockType() {
+    return blockType;
+  }
+
+  /**
+   * Releases the compressor this writer uses to compress blocks into the
+   * compressor pool.
+   */
+  @Override
+  public void close() {
+    if (compressor != null) {
+      compressionAlgorithm.returnCompressor(compressor);
+      compressor = null;
+    }
+  }
+
+  @Override
+  public Algorithm getCompression() {
+    return this.compressionAlgorithm;
+  }
+
+  public DataOutputStream getOutputStreamForEncoder() {
+    return this.dataOut;
+  }
+
+  @Override
+  public DataBlockEncoding getDataBlockEncoding() {
+    return this.encodingAlgo;
+  }
+
+  @Override
+  public int getHeaderSize() {
+    return this.dummyHeader.length;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java
index 45f2749..59310ce 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockEncodingContext.java
@@ -17,6 +17,7 @@
 package org.apache.hadoop.hbase.io.encoding;
 
 import java.io.IOException;
+import java.io.OutputStream;
 
 import org.apache.hadoop.hbase.io.hfile.BlockType;
 import org.apache.hadoop.hbase.io.hfile.Compression;
@@ -31,6 +32,11 @@ import org.apache.hadoop.hbase.io.hfile.Compression;
 public interface HFileBlockEncodingContext {
 
   /**
+   * @return OutputStream to which encoded data is written
+   */
+  public OutputStream getOutputStreamForEncoder();
+
+  /**
    * @return encoded and compressed bytes with header which are ready to write
    *         out to disk
    */
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java
index 135d25c..821b274 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/BlockType.java
@@ -101,15 +101,13 @@ public enum BlockType {
     }
   }
 
-  public static final int MAGIC_LENGTH = 8;
-
   private final byte[] magic;
   private final BlockCategory metricCat;
 
   private BlockType(String magicStr, BlockCategory metricCat) {
     magic = Bytes.toBytes(magicStr);
     this.metricCat = metricCat;
-    assert magic.length == MAGIC_LENGTH;
+    assert magic.length == HFileBlockConstants.MAGIC_LENGTH;
   }
 
   /**
@@ -141,22 +139,22 @@ public enum BlockType {
 
   public static BlockType parse(byte[] buf, int offset, int length)
       throws IOException {
-    if (length != MAGIC_LENGTH) {
+    if (length != HFileBlockConstants.MAGIC_LENGTH) {
       throw new IOException("Magic record of invalid length: "
           + Bytes.toStringBinary(buf, offset, length));
     }
 
     for (BlockType blockType : values())
-      if (Bytes.compareTo(blockType.magic, 0, MAGIC_LENGTH, buf, offset,
-          MAGIC_LENGTH) == 0)
+      if (Bytes.compareTo(blockType.magic, 0, HFileBlockConstants.MAGIC_LENGTH, buf, offset,
+          HFileBlockConstants.MAGIC_LENGTH) == 0)
         return blockType;
 
     throw new IOException("Invalid HFile block magic: "
-        + Bytes.toStringBinary(buf, offset, MAGIC_LENGTH));
+        + Bytes.toStringBinary(buf, offset, HFileBlockConstants.MAGIC_LENGTH));
   }
 
   public static BlockType read(DataInputStream in) throws IOException {
-    byte[] buf = new byte[MAGIC_LENGTH];
+    byte[] buf = new byte[HFileBlockConstants.MAGIC_LENGTH];
     in.readFully(buf);
     return parse(buf, 0, buf.length);
   }
@@ -164,10 +162,10 @@ public enum BlockType {
   public static BlockType read(ByteBuffer buf) throws IOException {
     BlockType blockType = parse(buf.array(),
         buf.arrayOffset() + buf.position(),
-        Math.min(buf.limit() - buf.position(), MAGIC_LENGTH));
+        Math.min(buf.limit() - buf.position(), HFileBlockConstants.MAGIC_LENGTH));
 
     // If we got here, we have read exactly MAGIC_LENGTH bytes.
-    buf.position(buf.position() + MAGIC_LENGTH);
+    buf.position(buf.position() + HFileBlockConstants.MAGIC_LENGTH);
     return blockType;
   }
 
@@ -179,16 +177,16 @@ public enum BlockType {
    * @return incremented offset
    */
   public int put(byte[] bytes, int offset) {
-    System.arraycopy(magic, 0, bytes, offset, MAGIC_LENGTH);
-    return offset + MAGIC_LENGTH;
+    System.arraycopy(magic, 0, bytes, offset, HFileBlockConstants.MAGIC_LENGTH);
+    return offset + HFileBlockConstants.MAGIC_LENGTH;
   }
 
   /**
-   * Reads a magic record of the length {@link #MAGIC_LENGTH} from the given
+   * Reads a magic record of the length {@link HFileBlockConstants#MAGIC_LENGTH} from the given
    * stream and expects it to match this block type.
    */
   public void readAndCheck(DataInputStream in) throws IOException {
-    byte[] buf = new byte[MAGIC_LENGTH];
+    byte[] buf = new byte[HFileBlockConstants.MAGIC_LENGTH];
     in.readFully(buf);
     if (Bytes.compareTo(buf, magic) != 0) {
       throw new IOException("Invalid magic: expected "
@@ -197,11 +195,11 @@ public enum BlockType {
   }
 
   /**
-   * Reads a magic record of the length {@link #MAGIC_LENGTH} from the given
+   * Reads a magic record of the length {@link HFileBlockConstants#MAGIC_LENGTH} from the given
    * byte buffer and expects it to match this block type.
    */
   public void readAndCheck(ByteBuffer in) throws IOException {
-    byte[] buf = new byte[MAGIC_LENGTH];
+    byte[] buf = new byte[HFileBlockConstants.MAGIC_LENGTH];
     in.get(buf);
     if (Bytes.compareTo(buf, magic) != 0) {
       throw new IOException("Invalid magic: expected "
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockConstants.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockConstants.java
new file mode 100644
index 0000000..efaec45
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockConstants.java
@@ -0,0 +1,46 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.io.hfile;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.hbase.util.Bytes;
+
+/**
+ * Constants needed by HFileBlockDefaultEncodingContext
+ */
+@InterfaceAudience.Private
+public class HFileBlockConstants {
+
+  public static final int MAGIC_LENGTH = 8;
+
+  /** The size data structures with minor version is 0 */
+  public static final int HEADER_SIZE_NO_CHECKSUM = MAGIC_LENGTH + 2 * Bytes.SIZEOF_INT
+      + Bytes.SIZEOF_LONG;
+
+  /** The size of a version 2 {@link HFile} block header, minor version 1.
+   * There is a 1 byte checksum type, followed by a 4 byte bytesPerChecksum
+   * followed by another 4 byte value to store sizeofDataOnDisk.
+   */
+  public static final int HEADER_SIZE = HEADER_SIZE_NO_CHECKSUM +
+    Bytes.SIZEOF_BYTE + 2 * Bytes.SIZEOF_INT;
+
+  /** Just an array of bytes of the right size. */
+  public static final byte[] DUMMY_HEADER = new byte[HEADER_SIZE];
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java
new file mode 100644
index 0000000..ddebbd5
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ArrayUtils.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util;
+
+import java.util.Arrays;
+
+public class ArrayUtils {
+
+  public static int length(byte[] a) {
+    if (a == null) {
+      return 0;
+    }
+    return a.length;
+  }
+
+  public static int length(Object[] a) {
+    if (a == null) {
+      return 0;
+    }
+    return a.length;
+  }
+
+  public static boolean isEmpty(byte[] a) {
+    if (a == null) {
+      return true;
+    }
+    if (a.length == 0) {
+      return true;
+    }
+    return false;
+  }
+
+  public static boolean isEmpty(Object[] a) {
+    if (a == null) {
+      return true;
+    }
+    if (a.length == 0) {
+      return true;
+    }
+    return false;
+  }
+
+  public static long getFirst(long[] a) {
+    return a[0];
+  }
+
+  public static long getLast(long[] a) {
+    return a[a.length - 1];
+  }
+
+  public static int getTotalLengthOfArrays(Iterable<byte[]> arrays) {
+    if (arrays == null) {
+      return 0;
+    }
+    int length = 0;
+    for (byte[] bytes : arrays) {
+      length += length(bytes);
+    }
+    return length;
+  }
+
+  /*
+   * copied from com.google.common.primitives.Bytes to remove param validation. params were
+   * especially bad because they created strings on each call. what's the rule on copy/pasting open
+   * source code?
+   */
+  public static byte[] ensureCapacity(byte[] array, int minLength, int padding) {
+    return (array.length < minLength) ? Arrays.copyOf(array, minLength + padding) : array;
+  }
+
+  public static int[] ensureCapacity(int[] array, int minLength, int padding) {
+    return (array.length < minLength) ? Arrays.copyOf(array, minLength + padding) : array;
+  }
+
+  public static long[] ensureCapacity(long[] array, int minLength, int padding) {
+    return (array.length < minLength) ? Arrays.copyOf(array, minLength + padding) : array;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java
new file mode 100644
index 0000000..52198bd
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java
@@ -0,0 +1,450 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.hadoop.hbase.util;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInput;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.classification.InterfaceStability;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.WritableUtils;
+
+/**
+ * Utility functions for working with byte buffers, such as reading/writing
+ * variable-length long numbers.
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public final class ByteBufferUtils {
+
+  // "Compressed integer" serialization helper constants.
+  private final static int VALUE_MASK = 0x7f;
+  private final static int NEXT_BIT_SHIFT = 7;
+  private final static int NEXT_BIT_MASK = 1 << 7;
+
+  private ByteBufferUtils() {
+  }
+
+  /**
+   * Similar to {@link WritableUtils#writeVLong(java.io.DataOutput, long)},
+   * but writes to a {@link ByteBuffer}.
+   */
+  public static void writeVLong(ByteBuffer out, long i) {
+    if (i >= -112 && i <= 127) {
+      out.put((byte) i);
+      return;
+    }
+
+    int len = -112;
+    if (i < 0) {
+      i ^= -1L; // take one's complement
+      len = -120;
+    }
+
+    long tmp = i;
+    while (tmp != 0) {
+      tmp = tmp >> 8;
+      len--;
+    }
+
+    out.put((byte) len);
+
+    len = (len < -120) ? -(len + 120) : -(len + 112);
+
+    for (int idx = len; idx != 0; idx--) {
+      int shiftbits = (idx - 1) * 8;
+      long mask = 0xFFL << shiftbits;
+      out.put((byte) ((i & mask) >> shiftbits));
+    }
+  }
+
+  /**
+   * Similar to {@link WritableUtils#readVLong(DataInput)} but reads from a
+   * {@link ByteBuffer}.
+   */
+  public static long readVLong(ByteBuffer in) {
+    byte firstByte = in.get();
+    int len = WritableUtils.decodeVIntSize(firstByte);
+    if (len == 1) {
+      return firstByte;
+    }
+    long i = 0;
+    for (int idx = 0; idx < len-1; idx++) {
+      byte b = in.get();
+      i = i << 8;
+      i = i | (b & 0xFF);
+    }
+    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);
+  }
+
+
+  /**
+   * Put in buffer integer using 7 bit encoding. For each written byte:
+   * 7 bits are used to store value
+   * 1 bit is used to indicate whether there is next bit.
+   * @param value Int to be compressed.
+   * @param out Where to put compressed data
+   * @return Number of bytes written.
+   * @throws IOException on stream error
+   */
+   public static int putCompressedInt(OutputStream out, final int value)
+      throws IOException {
+    int i = 0;
+    int tmpvalue = value;
+    do {
+      byte b = (byte) (tmpvalue & VALUE_MASK);
+      tmpvalue >>>= NEXT_BIT_SHIFT;
+      if (tmpvalue != 0) {
+        b |= (byte) NEXT_BIT_MASK;
+      }
+      out.write(b);
+      i++;
+    } while (tmpvalue != 0);
+    return i;
+  }
+
+   /**
+    * Put in output stream 32 bit integer (Big Endian byte order).
+    * @param out Where to put integer.
+    * @param value Value of integer.
+    * @throws IOException On stream error.
+    */
+   public static void putInt(OutputStream out, final int value)
+       throws IOException {
+     for (int i = Bytes.SIZEOF_INT - 1; i >= 0; --i) {
+       out.write((byte) (value >>> (i * 8)));
+     }
+   }
+
+  /**
+   * Copy the data to the output stream and update position in buffer.
+   * @param out the stream to write bytes to
+   * @param in the buffer to read bytes from
+   * @param length the number of bytes to copy
+   */
+  public static void moveBufferToStream(OutputStream out, ByteBuffer in,
+      int length) throws IOException {
+    copyBufferToStream(out, in, in.position(), length);
+    skip(in, length);
+  }
+
+  /**
+   * Copy data from a buffer to an output stream. Does not update the position
+   * in the buffer.
+   * @param out the stream to write bytes to
+   * @param in the buffer to read bytes from
+   * @param offset the offset in the buffer (from the buffer's array offset)
+   *      to start copying bytes from
+   * @param length the number of bytes to copy
+   */
+  public static void copyBufferToStream(OutputStream out, ByteBuffer in,
+      int offset, int length) throws IOException {
+    if (in.hasArray()) {
+      out.write(in.array(), in.arrayOffset() + offset,
+          length);
+    } else {
+      for (int i = 0; i < length; ++i) {
+        out.write(in.get(offset + i));
+      }
+    }
+  }
+
+  public static int putLong(OutputStream out, final long value,
+      final int fitInBytes) throws IOException {
+    long tmpValue = value;
+    for (int i = 0; i < fitInBytes; ++i) {
+      out.write((byte) (tmpValue & 0xff));
+      tmpValue >>>= 8;
+    }
+    return fitInBytes;
+  }
+
+  /**
+   * Check how many bytes are required to store value.
+   * @param value Value which size will be tested.
+   * @return How many bytes are required to store value.
+   */
+  public static int longFitsIn(final long value) {
+    if (value < 0) {
+      return 8;
+    }
+
+    if (value < (1l << 4 * 8)) {
+      // no more than 4 bytes
+      if (value < (1l << 2 * 8)) {
+        if (value < (1l << 1 * 8)) {
+          return 1;
+        }
+        return 2;
+      }
+      if (value < (1l << 3 * 8)) {
+        return 3;
+      }
+      return 4;
+    }
+    // more than 4 bytes
+    if (value < (1l << 6 * 8)) {
+      if (value < (1l << 5 * 8)) {
+        return 5;
+      }
+      return 6;
+    }
+    if (value < (1l << 7 * 8)) {
+      return 7;
+    }
+    return 8;
+  }
+
+  /**
+   * Check how many bytes is required to store value.
+   * @param value Value which size will be tested.
+   * @return How many bytes are required to store value.
+   */
+  public static int intFitsIn(final int value) {
+    if (value < 0) {
+      return 4;
+    }
+
+    if (value < (1 << 2 * 8)) {
+      if (value < (1 << 1 * 8)) {
+        return 1;
+      }
+      return 2;
+    }
+    if (value <= (1 << 3 * 8)) {
+      return 3;
+    }
+    return 4;
+  }
+
+  /**
+   * Read integer from stream coded in 7 bits and increment position.
+   * @return the integer that has been read
+   * @throws IOException
+   */
+  public static int readCompressedInt(InputStream input)
+      throws IOException {
+    int result = 0;
+    int i = 0;
+    byte b;
+    do {
+      b = (byte) input.read();
+      result += (b & VALUE_MASK) << (NEXT_BIT_SHIFT * i);
+      i++;
+      if (i > Bytes.SIZEOF_INT + 1) {
+        throw new IllegalStateException(
+            "Corrupted compressed int (too long: " + (i + 1) + " bytes)");
+      }
+    } while (0 != (b & NEXT_BIT_MASK));
+    return result;
+  }
+
+  /**
+   * Read integer from buffer coded in 7 bits and increment position.
+   * @return Read integer.
+   */
+  public static int readCompressedInt(ByteBuffer buffer) {
+    byte b = buffer.get();
+    if ((b & NEXT_BIT_MASK) != 0) {
+      return (b & VALUE_MASK) + (readCompressedInt(buffer) << NEXT_BIT_SHIFT);
+    }
+    return b & VALUE_MASK;
+  }
+
+  /**
+   * Read long which was written to fitInBytes bytes and increment position.
+   * @param fitInBytes In how many bytes given long is stored.
+   * @return The value of parsed long.
+   * @throws IOException
+   */
+  public static long readLong(InputStream in, final int fitInBytes)
+      throws IOException {
+    long tmpLong = 0;
+    for (int i = 0; i < fitInBytes; ++i) {
+      tmpLong |= (in.read() & 0xffl) << (8 * i);
+    }
+    return tmpLong;
+  }
+
+  /**
+   * Read long which was written to fitInBytes bytes and increment position.
+   * @param fitInBytes In how many bytes given long is stored.
+   * @return The value of parsed long.
+   */
+  public static long readLong(ByteBuffer in, final int fitInBytes) {
+    long tmpLength = 0;
+    for (int i = 0; i < fitInBytes; ++i) {
+      tmpLength |= (in.get() & 0xffl) << (8l * i);
+    }
+    return tmpLength;
+  }
+
+  /**
+   * Copy the given number of bytes from the given stream and put it at the
+   * current position of the given buffer, updating the position in the buffer.
+   * @param out the buffer to write data to
+   * @param in the stream to read data from
+   * @param length the number of bytes to read/write
+   */
+  public static void copyFromStreamToBuffer(ByteBuffer out,
+      DataInputStream in, int length) throws IOException {
+    if (out.hasArray()) {
+      in.readFully(out.array(), out.position() + out.arrayOffset(),
+          length);
+      skip(out, length);
+    } else {
+      for (int i = 0; i < length; ++i) {
+        out.put(in.readByte());
+      }
+    }
+  }
+  
+  /**
+   * Copy from the InputStream to a new heap ByteBuffer until the InputStream is exhausted.
+   */
+  public static ByteBuffer drainInputStreamToBuffer(InputStream is) throws IOException {
+    ByteArrayOutputStream baos = new ByteArrayOutputStream(4096);
+    IOUtils.copyBytes(is, baos, 4096, true);
+    ByteBuffer buffer = ByteBuffer.wrap(baos.toByteArray());
+    buffer.rewind();
+    return buffer;
+  }
+
+  /**
+   * Copy from one buffer to another from given offset
+   * @param out destination buffer
+   * @param in source buffer
+   * @param sourceOffset offset in the source buffer
+   * @param length how many bytes to copy
+   */
+  public static void copyFromBufferToBuffer(ByteBuffer out,
+      ByteBuffer in, int sourceOffset, int length) {
+    if (in.hasArray() && out.hasArray()) {
+      System.arraycopy(in.array(), sourceOffset + in.arrayOffset(),
+          out.array(), out.position() +
+          out.arrayOffset(), length);
+      skip(out, length);
+    } else {
+      for (int i = 0; i < length; ++i) {
+        out.put(in.get(sourceOffset + i));
+      }
+    }
+  }
+
+  /**
+   * Find length of common prefix of two parts in the buffer
+   * @param buffer Where parts are located.
+   * @param offsetLeft Offset of the first part.
+   * @param offsetRight Offset of the second part.
+   * @param limit Maximal length of common prefix.
+   * @return Length of prefix.
+   */
+  public static int findCommonPrefix(ByteBuffer buffer, int offsetLeft,
+      int offsetRight, int limit) {
+    int prefix = 0;
+
+    for (; prefix < limit; ++prefix) {
+      if (buffer.get(offsetLeft + prefix) != buffer.get(offsetRight + prefix)) {
+        break;
+      }
+    }
+
+    return prefix;
+  }
+
+  /**
+   * Find length of common prefix in two arrays.
+   * @param left Array to be compared.
+   * @param leftOffset Offset in left array.
+   * @param leftLength Length of left array.
+   * @param right Array to be compared.
+   * @param rightArray Offset in right array.
+   * @param rightLength Length of right array.
+   */
+  public static int findCommonPrefix(
+      byte[] left, int leftOffset, int leftLength,
+      byte[] right, int rightOffset, int rightLength) {
+    int length = Math.min(leftLength, rightLength);
+    int result = 0;
+
+    while (result < length &&
+        left[leftOffset + result] == right[rightOffset + result]) {
+      result++;
+    }
+
+    return result;
+  }
+
+  /**
+   * Check whether two parts in the same buffer are equal.
+   * @param buffer In which buffer there are parts
+   * @param offsetLeft Beginning of first part.
+   * @param lengthLeft Length of the first part.
+   * @param offsetRight Beginning of the second part.
+   * @param lengthRight Length of the second part.
+   * @return
+   */
+  public static boolean arePartsEqual(ByteBuffer buffer,
+      int offsetLeft, int lengthLeft,
+      int offsetRight, int lengthRight) {
+    if (lengthLeft != lengthRight) {
+      return false;
+    }
+
+    if (buffer.hasArray()) {
+      return 0 == Bytes.compareTo(
+          buffer.array(), buffer.arrayOffset() + offsetLeft, lengthLeft,
+          buffer.array(), buffer.arrayOffset() + offsetRight, lengthRight);
+    }
+
+    for (int i = 0; i < lengthRight; ++i) {
+      if (buffer.get(offsetLeft + i) != buffer.get(offsetRight + i)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  /**
+   * Increment position in buffer.
+   * @param buffer In this buffer.
+   * @param length By that many bytes.
+   */
+  public static void skip(ByteBuffer buffer, int length) {
+    buffer.position(buffer.position() + length);
+  }
+
+  public static void extendLimit(ByteBuffer buffer, int numBytes) {
+    buffer.limit(buffer.limit() + numBytes);
+  }
+
+  public static byte[] toBytes(ByteBuffer buffer, int startPosition) {
+    byte[] output = new byte[buffer.limit() - startPosition];
+    buffer.mark();
+    buffer.position(startPosition);
+    buffer.get(output);
+    buffer.reset();
+    return output;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
index ea32d3e..c4f7946 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
@@ -1648,7 +1648,7 @@ public class Bytes {
 
     return toString(b, 0, n);
   }
-  
+
   /**
    * Copy the byte array given in parameter and return an instance 
    * of a new byte array with the same length and the same content.
@@ -1661,4 +1661,62 @@ public class Bytes {
     System.arraycopy(bytes, 0, result, 0, bytes.length);	  
     return result;
   }
+
+  /**
+   * Search sorted array "a" for byte "key". I can't remember if I wrote this or copied it from
+   * somewhere. (mcorgan)
+   * @param a Array to search. Entries must be sorted and unique.
+   * @param fromIndex First index inclusive of "a" to include in the search.
+   * @param toIndex Last index exclusive of "a" to include in the search.
+   * @param key The byte to search for.
+   * @return The index of key if found. If not found, return -(index + 1), where negative indicates
+   *         "not found" and the "index + 1" handles the "-0" case.
+   */
+  public static int unsignedBinarySearch(byte[] a, int fromIndex, int toIndex, byte key) {
+    int unsignedKey = key & 0xff;
+    int low = fromIndex;
+    int high = toIndex - 1;
+
+    while (low <= high) {
+      int mid = (low + high) >>> 1;
+      int midVal = a[mid] & 0xff;
+
+      if (midVal < unsignedKey) {
+        low = mid + 1;
+      } else if (midVal > unsignedKey) {
+        high = mid - 1;
+      } else {
+        return mid; // key found
+      }
+    }
+    return -(low + 1); // key not found.
+  }
+
+  /**
+   * Treat the byte[] as an unsigned series of bytes, most significant bits first.  Start by adding
+   * 1 to the rightmost bit/byte and carry over all overflows to the more significant bits/bytes.
+   *
+   * @param input The byte[] to increment.
+   * @return The incremented copy of "in".  May be same length or 1 byte longer.
+   */
+  public static byte[] unsignedIncrement(final byte[] input) {
+    byte[] copy = copy(input);
+    if (copy == null) {
+      throw new IllegalArgumentException("cannot increment null array");
+    }
+    for (int i = copy.length - 1; i >= 0; --i) {
+      if (copy[i] == -1) {// -1 is all 1-bits, which is the unsigned maximum
+        copy[i] = 0;
+      } else {
+        ++copy[i];
+        return copy;
+      }
+    }
+    // we maxed out the array
+    byte[] out = new byte[copy.length + 1];
+    out[0] = 1;
+    System.arraycopy(copy, 0, out, 1, copy.length);
+    return out;
+  }
+
 }
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/FileUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/FileUtils.java
new file mode 100644
index 0000000..80eb38f
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/FileUtils.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * Utility methods for dealing with Java's File API.
+ */
+@InterfaceAudience.Private
+public class FileUtils {
+
+  /**
+   * Wrap an OutputStream around an existing File
+   * @param fileLocation full path to the file
+   * @param create whether to create the file if it doesn't exist yet
+   * @param append flag passed to FileOutputStream constructor
+   * @return new FileOutputStream
+   */
+  public static FileOutputStream makeFileOutputStream(String fileLocation, boolean create,
+      boolean append) {
+    File f = new File(fileLocation);
+    if (create) {
+      createFileAndParents(f);
+    }
+    try {
+      return new FileOutputStream(f, append);
+    } catch (FileNotFoundException fnfe) {
+      throw new IllegalArgumentException("FileNotFoundException for :" + fileLocation, fnfe);
+    }
+  }
+
+  /**
+   * @param aFile Full directory path and filename
+   */
+  public static void createFileAndParents(File aFile) {
+    if (aFile.exists()) {
+      return;
+    }
+    File parent = new File(aFile.getParent());
+    if (parent.exists()) {
+      return;
+    }
+    parent.mkdirs();
+  }
+
+  /**
+   * Delete a file but with safety check for "" and "/".
+   * @param path full path to file
+   */
+  public static void delete(String path) {
+    if (StringUtils.isEmpty(path) || "/".equals(path)) {
+      throw new IllegalArgumentException("cannot delete empty or root path");
+    }
+    File file = new File(path);
+    file.delete();
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/StringUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/StringUtils.java
new file mode 100644
index 0000000..eb02c6d
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/StringUtils.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util;
+
+import org.apache.hadoop.classification.InterfaceAudience;
+
+/**
+ * Utility methods for {@link java.lang.String}
+ */
+@InterfaceAudience.Private
+public class StringUtils {
+
+  /**
+   * Null-safe length check.
+   * @param input
+   * @return true if null or length==0
+   */
+  public static boolean isEmpty(String input) {
+    return input == null || input.length() == 0;
+  }
+
+  /**
+   * Push the input string to the right by appending a character before it, usually a space.
+   * @param input the string to pad
+   * @param padding the character to repeat to the left of the input string
+   * @param length the desired total length including the padding
+   * @return padding characters + input
+   */
+  public static String padFront(String input, char padding, int length) {
+    if (input.length() > length) {
+      throw new IllegalArgumentException("input \"" + input + "\" longer than maxLength=" + length);
+    }
+    int numPaddingCharacters = length - input.length();
+    return repeat(padding, numPaddingCharacters) + input;
+  }
+
+  /**
+   * @param c repeat this character
+   * @param reapeatFor the length of the output String
+   * @return c, repeated repeatFor times
+   */
+  public static String repeat(char c, int reapeatFor) {
+    StringBuilder sb = new StringBuilder();
+    for (int i = 0; i < reapeatFor; ++i) {
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRange.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRange.java
new file mode 100644
index 0000000..0f367fa
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRange.java
@@ -0,0 +1,278 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util.bytes;
+
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+
+/**
+ * Lightweight, reusable class for specifying ranges of byte[]'s.
+ * <p/>
+ * Mutable, and always evaluates equals, hashCode, and compareTo based on the current contents.
+ * <p/>
+ * Can contain convenience methods for comparing, printing, cloning, spawning new arrays, copying to
+ * other arrays, etc.  Please place non-core methods into {@link ByteRangeUtils}.
+ * <p/>
+ * We may consider converting this to an interface and creating separate implementations for a
+ * single byte[], a paged byte[] (growable byte[][]), a ByteBuffer, etc
+ */
+public class ByteRange implements Comparable<ByteRange> {
+
+  private static final int UNSET_HASH_VALUE = -1;
+
+
+  /********************** fields *****************************/
+
+  // Do not make these final, as the intention is to reuse objects of this class
+
+  /**
+   * The array containing the bytes in this range.  It will be >= length.
+   */
+  private byte[] bytes;
+
+  /**
+   * The index of the first byte in this range.  ByteRange.get(0) will return bytes[offset].
+   */
+  private int offset;
+
+  /**
+   * The number of bytes in the range.  Offset + length must be <= bytes.length
+   */
+  private int length;
+
+  /**
+   * Variable for lazy-caching the hashCode of this range.  Useful for frequently used ranges,
+   * long-lived ranges, or long ranges.
+   */
+  private int hash = UNSET_HASH_VALUE;
+
+
+  /********************** construct ***********************/
+
+  public ByteRange() {
+    set(new byte[0]);//Could probably get away with a null array if the need arises.
+  }
+
+  public ByteRange(byte[] bytes) {
+    set(bytes);
+  }
+
+  public ByteRange(byte[] bytes, int offset, int length) {
+    set(bytes, offset, length);
+  }
+
+
+  /********************** write methods *************************/
+
+  public ByteRange clear() {
+    clearHashCache();
+    bytes = null;
+    offset = 0;
+    length = 0;
+    return this;
+  }
+
+  public ByteRange set(byte[] bytes) {
+    clearHashCache();
+    this.bytes = bytes;
+    this.offset = 0;
+    this.length = bytes.length;
+    return this;
+  }
+
+  public ByteRange set(byte[] bytes, int offset, int length) {
+    clearHashCache();
+    this.bytes = bytes;
+    this.offset = offset;
+    this.length = length;
+    return this;
+  }
+
+  public void setBytes(byte[] bytes) {
+    clearHashCache();
+    this.bytes = bytes;
+  }
+
+  public void setOffset(int offset) {
+    clearHashCache();
+    this.offset = offset;
+  }
+
+  public void setLength(int length) {
+    clearHashCache();
+    this.length = length;
+  }
+
+
+  /*********** read methods (add non-core methods to ByteRangeUtils) *************/
+
+  /**
+   * @param index zero-based index
+   * @return single byte at index
+   */
+  public byte get(int index) {
+    return bytes[offset + index];
+  }
+
+  /**
+   * Instantiate a new byte[] with exact length, which is at least 24 bytes + length.  Copy the
+   * contents of this range into it.
+   * @return The newly cloned byte[].
+   */
+  public byte[] deepCopyToNewArray() {
+    byte[] result = new byte[length];
+    System.arraycopy(bytes, offset, result, 0, length);
+    return result;
+  }
+
+  /**
+   * Create a new ByteRange with new backing byte[] and copy the state of this range into the new
+   * range.  Copy the hash over if it is already calculated.
+   * @return 
+   */
+  public ByteRange deepCopy() {
+    ByteRange clone = new ByteRange(deepCopyToNewArray());
+    if (isHashCached()) {
+      clone.hash = hash;
+    }
+    return clone;
+  }
+
+  /**
+   * Wrapper for System.arraycopy.  Copy the contents of this range into the provided array.
+   * @param destination Copy to this array
+   * @param destinationOffset First index in the destination array.
+   * @return void to avoid confusion between which ByteRange should be returned
+   */
+  public void deepCopyTo(byte[] destination, int destinationOffset) {
+    System.arraycopy(bytes, offset, destination, destinationOffset, length);
+  }
+
+  /**
+   * Wrapper for System.arraycopy. Copy the contents of this range into the provided array.
+   * @param innerOffset Start copying from this index in this source ByteRange. First byte copied is
+   *          bytes[offset + innerOffset]
+   * @param copyLength Copy this many bytes
+   * @param destination Copy to this array
+   * @param destinationOffset First index in the destination array.
+   * @return void to avoid confusion between which ByteRange should be returned
+   */
+  public void deepCopySubRangeTo(int innerOffset, int copyLength, byte[] destination,
+      int destinationOffset) {
+    System.arraycopy(bytes, offset + innerOffset, destination, destinationOffset, copyLength);
+  }
+
+  /**
+   * Create a new ByteRange that points at this range's byte[]. The new range can have different
+   * values for offset and length, but modifying the shallowCopy will modify the bytes in this
+   * range's array. Pass over the hash code if it is already cached.
+   * @param innerOffset First byte of clone will be this.offset + copyOffset.
+   * @param copyLength Number of bytes in the clone.
+   * @return new ByteRange object referencing this range's byte[].
+   */
+  public ByteRange shallowCopySubRange(int innerOffset, int copyLength) {
+    ByteRange clone = new ByteRange(bytes, offset + innerOffset, copyLength);
+    if (isHashCached()) {
+      clone.hash = hash;
+    }
+    return clone;
+  }
+
+  //TODO move to ByteRangeUtils because it is non-core method
+  public int numEqualPrefixBytes(ByteRange that, int thatInnerOffset) {
+    int maxCompares = Math.min(length, that.length - thatInnerOffset);
+    for (int i = 0; i < maxCompares; ++i) {
+      if (bytes[offset + i] != that.bytes[that.offset + thatInnerOffset + i]) {
+        return i;
+      }
+    }
+    return maxCompares;
+  }
+
+  public byte[] getBytes() {
+    return bytes;
+  }
+
+  public int getOffset() {
+    return offset;
+  }
+
+  public int getLength() {
+    return length;
+  }
+
+
+  /******************* standard methods *********************/
+
+  @Override
+  public boolean equals(Object thatObject) {
+    if (this == thatObject) {
+      return true;
+    }
+    if (hashCode() != thatObject.hashCode()) {
+      return false;
+    }
+    if (!(thatObject instanceof ByteRange)) {
+      return false;
+    }
+    ByteRange that = (ByteRange) thatObject;
+    return Bytes.equals(bytes, offset, length, that.bytes, that.offset, that.length);
+  }
+
+  @Override
+  public int hashCode() {
+    if (isHashCached()) {// hash is already calculated and cached
+      return hash;
+    }
+    if (bytes == null || length == 0) {// return 0 for empty ByteRange
+      hash = 0;
+      return hash;
+    }
+    int off = offset;
+    hash = 0;
+    for (int i = 0; i < length; i++) {
+      hash = 31 * hash + bytes[off++];
+    }
+    return hash;
+  }
+
+  private boolean isHashCached() {
+    return hash != UNSET_HASH_VALUE;
+  }
+
+  private void clearHashCache() {
+    hash = UNSET_HASH_VALUE;
+  }
+
+  /**
+   * Bitwise comparison of each byte in the array.  Unsigned comparison, not paying attention to
+   * java's signed bytes.
+   */
+  @Override
+  public int compareTo(ByteRange other) {
+    return Bytes.compareTo(bytes, offset, length, other.bytes, other.offset, other.length);
+  }
+
+  @Override
+  public String toString() {
+    return Bytes.toStringBinary(bytes, offset, length);
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRangeUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRangeUtils.java
new file mode 100644
index 0000000..d3cdc64
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/bytes/ByteRangeUtils.java
@@ -0,0 +1,35 @@
+package org.apache.hadoop.hbase.util.bytes;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import com.google.common.collect.Lists;
+
+/**
+ * Utility methods {@link ByteRange}.
+ */
+public class ByteRangeUtils {
+
+  public static ArrayList<byte[]> copyToNewArrays(Collection<ByteRange> ranges) {
+    if (ranges == null) {
+      return new ArrayList<byte[]>(0);
+    }
+    ArrayList<byte[]> arrays = Lists.newArrayListWithCapacity(ranges.size());
+    for (ByteRange range : ranges) {
+      arrays.add(range.deepCopyToNewArray());
+    }
+    return arrays;
+  }
+
+  public static ArrayList<ByteRange> fromArrays(Collection<byte[]> arrays) {
+    if (arrays == null) {
+      return new ArrayList<ByteRange>(0);
+    }
+    ArrayList<ByteRange> ranges = Lists.newArrayListWithCapacity(arrays.size());
+    for (byte[] array : arrays) {
+      ranges.add(new ByteRange(array));
+    }
+    return ranges;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/CollectionUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/CollectionUtils.java
new file mode 100644
index 0000000..bd1536a
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/CollectionUtils.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util.collections;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Utility methods for dealing with Collections, including treating null collections as empty.
+ */
+public class CollectionUtils {
+
+  private static final List<Object> EMPTY_LIST = Collections.unmodifiableList(
+    new ArrayList<Object>(0));
+
+  
+  @SuppressWarnings("unchecked")
+  public static <T> Collection<T> nullSafe(Collection<T> in) {
+    if (in == null) {
+      return (Collection<T>)EMPTY_LIST;
+    }
+    return in;
+  }
+
+	/************************ size ************************************/
+
+  public static <T> int size(Collection<T> collection) {
+    if (collection == null) {
+      return 0;
+    }
+    return collection.size();
+  }
+
+  public static <A, B> boolean sameSize(Collection<A> a, Collection<B> b) {
+    return size(a) == size(b);
+  }
+
+	/*************************** empty ****************************************/
+
+  public static <T> boolean isEmpty(Collection<T> collection) {
+    if (collection == null || collection.isEmpty()) {
+      return true;
+    } else {
+      return false;
+    }
+  }
+
+  public static <T> boolean notEmpty(Collection<T> collection) {
+    if (collection == null || collection.isEmpty()) {
+      return false;
+    } else {
+      return true;
+    }
+  }
+
+	/************************ first/last **************************/
+
+  public static <T> T getFirst(Collection<T> collection) {
+    if (CollectionUtils.isEmpty(collection)) {
+      return null;
+    }
+    for (T t : collection) {
+      return t;
+    }
+    return null;
+  }
+  
+  /**
+   * @param list any list
+   * @return -1 if list is empty, otherwise the max index
+   */
+  public static int getLastIndex(List<?> list){
+    if(isEmpty(list)){
+      return -1;
+    }
+    return list.size() - 1;
+  }
+  
+  /**
+   * @param list
+   * @param index the index in question
+   * @return true if it is the last index or if list is empty and -1 is passed for the index param
+   */
+  public static boolean isLastIndex(List<?> list, int index){
+    return index == getLastIndex(list);
+  }
+
+  public static <T> T getLast(List<T> list) {
+    if (isEmpty(list)) {
+      return null;
+    }
+    return list.get(list.size() - 1);
+  }
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/IterableUtils.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/IterableUtils.java
new file mode 100644
index 0000000..52d0c16
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/collections/IterableUtils.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.util.collections;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+/**
+ * Utility methods for Iterable including null-safe handlers.
+ */
+public class IterableUtils {
+
+  private static final List<Object> EMPTY_LIST = Collections
+      .unmodifiableList(new ArrayList<Object>(0));
+
+  @SuppressWarnings("unchecked")
+  public static <T> Iterable<T> nullSafe(Iterable<T> in) {
+    if (in == null) {
+      return (List<T>) EMPTY_LIST;
+    }
+    return in;
+  }
+
+}
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/LoadTestKVGenerator.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/LoadTestKVGenerator.java
index 260cbaa..63b1aa4 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/LoadTestKVGenerator.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/LoadTestKVGenerator.java
@@ -80,8 +80,10 @@ public class LoadTestKVGenerator {
    */
   public byte[] generateRandomSizeValue(long key, String qual) {
     String rowKey = md5PrefixedKey(key);
-    int dataSize = minValueSize + randomForValueSize.nextInt(
-        Math.abs(maxValueSize - minValueSize));
+    int dataSize = minValueSize;
+    if(minValueSize != maxValueSize){
+      dataSize = minValueSize + randomForValueSize.nextInt(Math.abs(maxValueSize - minValueSize));
+    }
     return getValueForRowColumn(rowKey, qual, dataSize);
   }
 
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java
new file mode 100644
index 0000000..51932e0
--- /dev/null
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/test/RedundantKVGenerator.java
@@ -0,0 +1,290 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.hadoop.hbase.util.test;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.ByteBufferUtils;
+import org.apache.hadoop.io.WritableUtils;
+
+/**
+ * Generate list of key values which are very useful to test data block encoding
+ * and compression.
+ */
+public class RedundantKVGenerator {
+  // row settings
+  static int DEFAULT_NUMBER_OF_ROW_PREFIXES = 10;
+  static int DEFAULT_AVERAGE_PREFIX_LENGTH = 6;
+  static int DEFAULT_PREFIX_LENGTH_VARIANCE = 3;
+  static int DEFAULT_AVERAGE_SUFFIX_LENGTH = 3;
+  static int DEFAULT_SUFFIX_LENGTH_VARIANCE = 3;
+  static int DEFAULT_NUMBER_OF_ROW = 500;
+
+  // qualifier
+  static float DEFAULT_CHANCE_FOR_SAME_QUALIFIER = 0.5f;
+  static float DEFAULT_CHANCE_FOR_SIMILIAR_QUALIFIER = 0.4f;
+  static int DEFAULT_AVERAGE_QUALIFIER_LENGTH = 9;
+  static int DEFAULT_QUALIFIER_LENGTH_VARIANCE = 3;
+
+  static int DEFAULT_COLUMN_FAMILY_LENGTH = 9;
+  static int DEFAULT_VALUE_LENGTH = 8;
+  static float DEFAULT_CHANCE_FOR_ZERO_VALUE = 0.5f;
+
+  static int DEFAULT_BASE_TIMESTAMP_DIVIDE = 1000000;
+  static int DEFAULT_TIMESTAMP_DIFF_SIZE = 100000000;
+
+  /**
+   * Default constructor, assumes all parameters from class constants.
+   */
+  public RedundantKVGenerator() {
+    this(new Random(42L),
+        DEFAULT_NUMBER_OF_ROW_PREFIXES,
+        DEFAULT_AVERAGE_PREFIX_LENGTH,
+        DEFAULT_PREFIX_LENGTH_VARIANCE,
+        DEFAULT_AVERAGE_SUFFIX_LENGTH,
+        DEFAULT_SUFFIX_LENGTH_VARIANCE,
+        DEFAULT_NUMBER_OF_ROW,
+
+        DEFAULT_CHANCE_FOR_SAME_QUALIFIER,
+        DEFAULT_CHANCE_FOR_SIMILIAR_QUALIFIER,
+        DEFAULT_AVERAGE_QUALIFIER_LENGTH,
+        DEFAULT_QUALIFIER_LENGTH_VARIANCE,
+
+        DEFAULT_COLUMN_FAMILY_LENGTH,
+        DEFAULT_VALUE_LENGTH,
+        DEFAULT_CHANCE_FOR_ZERO_VALUE,
+
+        DEFAULT_BASE_TIMESTAMP_DIVIDE,
+        DEFAULT_TIMESTAMP_DIFF_SIZE
+    );
+  }
+
+
+  /**
+   * Various configuration options for generating key values
+   * @param randomizer pick things by random
+   */
+  public RedundantKVGenerator(Random randomizer,
+      int numberOfRowPrefixes,
+      int averagePrefixLength,
+      int prefixLengthVariance,
+      int averageSuffixLength,
+      int suffixLengthVariance,
+      int numberOfRows,
+
+      float chanceForSameQualifier,
+      float chanceForSimiliarQualifier,
+      int averageQualifierLength,
+      int qualifierLengthVariance,
+
+      int columnFamilyLength,
+      int valueLength,
+      float chanceForZeroValue,
+
+      int baseTimestampDivide,
+      int timestampDiffSize
+      ) {
+    this.randomizer = randomizer;
+
+    this.numberOfRowPrefixes = numberOfRowPrefixes;
+    this.averagePrefixLength = averagePrefixLength;
+    this.prefixLengthVariance = prefixLengthVariance;
+    this.averageSuffixLength = averageSuffixLength;
+    this.suffixLengthVariance = suffixLengthVariance;
+    this.numberOfRows = numberOfRows;
+
+    this.chanceForSameQualifier = chanceForSameQualifier;
+    this.chanceForSimiliarQualifier = chanceForSimiliarQualifier;
+    this.averageQualifierLength = averageQualifierLength;
+    this.qualifierLengthVariance = qualifierLengthVariance;
+
+    this.columnFamilyLength = columnFamilyLength;
+    this.valueLength = valueLength;
+    this.chanceForZeroValue = chanceForZeroValue;
+
+    this.baseTimestampDivide = baseTimestampDivide;
+    this.timestampDiffSize = timestampDiffSize;
+  }
+
+  /** Used to generate dataset */
+  private Random randomizer;
+
+  // row settings
+  private int numberOfRowPrefixes;
+  private int averagePrefixLength = 6;
+  private int prefixLengthVariance = 3;
+  private int averageSuffixLength = 3;
+  private int suffixLengthVariance = 3;
+  private int numberOfRows = 500;
+
+  // qualifier
+  private float chanceForSameQualifier = 0.5f;
+  private float chanceForSimiliarQualifier = 0.4f;
+  private int averageQualifierLength = 9;
+  private int qualifierLengthVariance = 3;
+
+  private int columnFamilyLength = 9;
+  private int valueLength = 8;
+  private float chanceForZeroValue = 0.5f;
+
+  private int baseTimestampDivide = 1000000;
+  private int timestampDiffSize = 100000000;
+
+  private List<byte[]> generateRows() {
+    // generate prefixes
+    List<byte[]> prefixes = new ArrayList<byte[]>();
+    prefixes.add(new byte[0]);
+    for (int i = 1; i < numberOfRowPrefixes; ++i) {
+      int prefixLength = averagePrefixLength;
+      prefixLength += randomizer.nextInt(2 * prefixLengthVariance + 1) -
+          prefixLengthVariance;
+      byte[] newPrefix = new byte[prefixLength];
+      randomizer.nextBytes(newPrefix);
+      prefixes.add(newPrefix);
+    }
+
+    // generate rest of the row
+    List<byte[]> rows = new ArrayList<byte[]>();
+    for (int i = 0; i < numberOfRows; ++i) {
+      int suffixLength = averageSuffixLength;
+      suffixLength += randomizer.nextInt(2 * suffixLengthVariance + 1) -
+          suffixLengthVariance;
+      int randomPrefix = randomizer.nextInt(prefixes.size());
+      byte[] row = new byte[prefixes.get(randomPrefix).length +
+                            suffixLength];
+      rows.add(row);
+    }
+
+    return rows;
+  }
+
+  /**
+   * Generate test data useful to test encoders.
+   * @param howMany How many Key values should be generated.
+   * @return sorted list of key values
+   */
+  public List<KeyValue> generateTestKeyValues(int howMany) {
+    List<KeyValue> result = new ArrayList<KeyValue>();
+
+    List<byte[]> rows = generateRows();
+    Map<Integer, List<byte[]>> rowsToQualifier =
+        new HashMap<Integer, List<byte[]>>();
+
+    byte[] family = new byte[columnFamilyLength];
+    randomizer.nextBytes(family);
+
+    long baseTimestamp = Math.abs(randomizer.nextLong()) /
+        baseTimestampDivide;
+
+    byte[] value = new byte[valueLength];
+
+    for (int i = 0; i < howMany; ++i) {
+      long timestamp = baseTimestamp + randomizer.nextInt(
+          timestampDiffSize);
+      Integer rowId = randomizer.nextInt(rows.size());
+      byte[] row = rows.get(rowId);
+
+      // generate qualifier, sometimes it is same, sometimes similar,
+      // occasionally completely different
+      byte[] qualifier;
+      float qualifierChance = randomizer.nextFloat();
+      if (!rowsToQualifier.containsKey(rowId) ||
+          qualifierChance > chanceForSameQualifier +
+          chanceForSimiliarQualifier) {
+        int qualifierLength = averageQualifierLength;
+        qualifierLength +=
+            randomizer.nextInt(2 * qualifierLengthVariance + 1) -
+            qualifierLengthVariance;
+        qualifier = new byte[qualifierLength];
+        randomizer.nextBytes(qualifier);
+
+        // add it to map
+        if (!rowsToQualifier.containsKey(rowId)) {
+          rowsToQualifier.put(rowId, new ArrayList<byte[]>());
+        }
+        rowsToQualifier.get(rowId).add(qualifier);
+      } else if (qualifierChance > chanceForSameQualifier) {
+        // similar qualifier
+        List<byte[]> previousQualifiers = rowsToQualifier.get(rowId);
+        byte[] originalQualifier = previousQualifiers.get(
+            randomizer.nextInt(previousQualifiers.size()));
+
+        qualifier = new byte[originalQualifier.length];
+        int commonPrefix = randomizer.nextInt(qualifier.length);
+        System.arraycopy(originalQualifier, 0, qualifier, 0, commonPrefix);
+        for (int j = commonPrefix; j < qualifier.length; ++j) {
+          qualifier[j] = (byte) (randomizer.nextInt() & 0xff);
+        }
+
+        rowsToQualifier.get(rowId).add(qualifier);
+      } else {
+        // same qualifier
+        List<byte[]> previousQualifiers = rowsToQualifier.get(rowId);
+        qualifier = previousQualifiers.get(
+            randomizer.nextInt(previousQualifiers.size()));
+      }
+
+      if (randomizer.nextFloat() < chanceForZeroValue) {
+        for (int j = 0; j < value.length; ++j) {
+          value[j] = (byte) 0;
+        }
+      } else {
+        randomizer.nextBytes(value);
+      }
+
+      result.add(new KeyValue(row, family, qualifier, timestamp, value));
+    }
+
+    Collections.sort(result, KeyValue.COMPARATOR);
+
+    return result;
+  }
+
+  /**
+   * Convert list of KeyValues to byte buffer.
+   * @param keyValues list of KeyValues to be converted.
+   * @return buffer with content from key values
+   */
+  public static ByteBuffer convertKvToByteBuffer(List<KeyValue> keyValues,
+      boolean includesMemstoreTS) {
+    int totalSize = 0;
+    for (KeyValue kv : keyValues) {
+      totalSize += kv.getLength();
+      if (includesMemstoreTS) {
+        totalSize += WritableUtils.getVIntSize(kv.getMemstoreTS());
+      }
+    }
+
+    ByteBuffer result = ByteBuffer.allocate(totalSize);
+    for (KeyValue kv : keyValues) {
+      result.put(kv.getBuffer(), kv.getOffset(), kv.getLength());
+      if (includesMemstoreTS) {
+        ByteBufferUtils.writeVLong(result, kv.getMemstoreTS());
+      }
+    }
+
+    return result;
+  }
+
+}
diff --git a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
index f6831ce..78a0c02 100644
--- a/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
+++ b/hbase-common/src/test/java/org/apache/hadoop/hbase/util/TestBytes.java
@@ -26,8 +26,10 @@ import java.math.BigDecimal;
 import java.util.Arrays;
 
 import junit.framework.TestCase;
-import org.junit.experimental.categories.Category;
+
 import org.apache.hadoop.hbase.SmallTests;
+import org.junit.Assert;
+import org.junit.experimental.categories.Category;
 
 
 @Category(SmallTests.class)
@@ -287,7 +289,7 @@ public class TestBytes extends TestCase {
     assertEquals("World", Bytes.readStringFixedSize(dis, 18));
     assertEquals("", Bytes.readStringFixedSize(dis, 9));
   }
-  
+
   public void testCopy() throws Exception {
     byte [] bytes = Bytes.toBytes("ABCDEFGHIJKLMNOPQRSTUVWXYZ");
     byte [] copy =  Bytes.copy(bytes);
@@ -307,5 +309,31 @@ public class TestBytes extends TestCase {
     String bytes = Bytes.toStringBinary(Bytes.toBytes(2.17));
     assertEquals(2.17, Bytes.toDouble(Bytes.toBytesBinary(bytes)), 0);        
   }
+
+  public void testUnsignedBinarySearch(){
+    byte[] bytes = new byte[]{0,5,123,127,-128,-100,-1};
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)5), 1);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)127), 3);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)-128), 4);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)-100), 5);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)-1), 6);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)2), -1-1);
+    Assert.assertEquals(Bytes.unsignedBinarySearch(bytes, 0, bytes.length, (byte)-5), -6-1);
+  }
+
+  public void testUnsignedIncrement(){
+    byte[] a = Bytes.toBytes(0);
+    int a2 = Bytes.toInt(Bytes.unsignedIncrement(a), 0);
+    Assert.assertTrue(a2==1);
+
+    byte[] b = Bytes.toBytes(-1);
+    byte[] actuals = Bytes.unsignedIncrement(b);
+    byte[] expected = new byte[]{1,0,0,0,0};
+    Assert.assertArrayEquals(expected, actuals);
+
+    byte[] c = Bytes.toBytes(255);//should wrap to the next significant byte
+    int c2 = Bytes.toInt(Bytes.unsignedIncrement(c), 0);
+    Assert.assertTrue(c2==256);
+  }
 }
 
diff --git a/hbase-prefix-tree/pom.xml b/hbase-prefix-tree/pom.xml
new file mode 100644
index 0000000..76965f1
--- /dev/null
+++ b/hbase-prefix-tree/pom.xml
@@ -0,0 +1,203 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+<!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <artifactId>hbase</artifactId>
+    <groupId>org.apache.hbase</groupId>
+    <version>0.95-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+
+  <artifactId>hbase-prefix-tree</artifactId>
+  <name>HBase - Prefix Tree</name>
+  <description>Prefix Tree Data Block Encoder</description>
+
+  <build>
+    <plugins>
+      <plugin>
+        <artifactId>maven-surefire-plugin</artifactId>
+        <!-- Always skip the second part executions, since we only run
+        simple unit tests in this module. -->
+        <executions>
+          <execution>
+            <id>secondPartTestsExecution</id>
+            <phase>test</phase>
+            <goals>
+              <goal>test</goal>
+            </goals>
+            <configuration>
+              <skip>true</skip>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+    </plugins>
+  </build>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-common</artifactId>
+    </dependency>
+  </dependencies>
+
+  <profiles>
+    <!-- Skip the tests in this module -->
+    <profile>
+      <id>skip-common-tests</id>
+      <activation>
+        <property>
+          <name>skip-common-tests</name>
+        </property>
+      </activation>
+      <properties>
+        <surefire.skipFirstPart>true</surefire.skipFirstPart>
+      </properties>
+    </profile>
+
+    <!-- profile against Hadoop 1.0.x: This is the default. It has to have the same
+    activation property as the parent Hadoop 1.0.x profile to make sure it gets run at
+    the same time. -->
+    <profile>
+      <id>hadoop-1.0</id>
+      <activation>
+        <property>
+          <name>!hadoop.profile</name>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-core</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-test</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+
+    <!--
+      profile for building against Hadoop 2.0.0-alpha. Activate using:
+       mvn -Dhadoop.profile=2.0
+    -->
+    <profile>
+      <id>hadoop-2.0</id>
+      <activation>
+        <property>
+          <name>hadoop.profile</name>
+          <value>2.0</value>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-client</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-annotations</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-minicluster</artifactId>
+        </dependency>
+      </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <artifactId>maven-dependency-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>create-mrapp-generated-classpath</id>
+                <phase>generate-test-resources</phase>
+                <goals>
+                  <goal>build-classpath</goal>
+                </goals>
+                <configuration>
+                  <!-- needed to run the unit test for DS to generate
+                  the required classpath that is required in the env
+                  of the launch container in the mini mr/yarn cluster
+                  -->
+                  <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+
+    <!--
+      profile for building against Hadoop 3.0.x. Activate using:
+       mvn -Dhadoop.profile=3.0
+    -->
+    <profile>
+      <id>hadoop-3.0</id>
+      <activation>
+        <property>
+          <name>hadoop.profile</name>
+          <value>3.0</value>
+        </property>
+      </activation>
+      <properties>
+        <hadoop.version>3.0-SNAPSHOT</hadoop.version>
+      </properties>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-annotations</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-minicluster</artifactId>
+        </dependency>
+      </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <artifactId>maven-dependency-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>create-mrapp-generated-classpath</id>
+                <phase>generate-test-resources</phase>
+                <goals>
+                  <goal>build-classpath</goal>
+                </goals>
+                <configuration>
+                  <!-- needed to run the unit test for DS to generate
+                  the required classpath that is required in the env
+                  of the launch container in the mini mr/yarn cluster
+                  -->
+                  <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+  </profiles>
+</project>
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeBlockMeta.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeBlockMeta.java
new file mode 100644
index 0000000..4c2325a
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeBlockMeta.java
@@ -0,0 +1,844 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.util.ByteBufferUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.util.vint.UVIntTool;
+import org.apache.hbase.util.vint.UVLongTool;
+
+/**
+ * Information about the block.  Stored at the beginning of the byte[].  Contains things
+ * like minimum timestamp and width of FInts in the row tree.
+ *
+ * Most fields stored in VInts that get decoded on the first access of each new block.
+ *
+ */
+public class PrefixTreeBlockMeta {
+
+  /******************* static fields ********************/
+
+  public static final int VERSION = 0;
+
+  public static final int MAX_FAMILY_LENGTH = Byte.MAX_VALUE;// hard-coded in KeyValue
+
+  public static final int
+	  NUM_LONGS = 2,
+    NUM_INTS = 22,
+    NUM_SHORTS = 0,//keyValueTypeWidth not persisted
+    NUM_SINGLE_BYTES = 2,
+    MAX_BYTES = Bytes.SIZEOF_LONG * NUM_LONGS
+        + Bytes.SIZEOF_SHORT * NUM_SHORTS
+        + Bytes.SIZEOF_INT * NUM_INTS
+        + NUM_SINGLE_BYTES;
+
+
+  /**************** transient fields *********************/
+
+  protected int arrayOffset;
+  protected int bufferOffset;
+
+
+  /**************** persisted fields **********************/
+
+  // PrefixTree version to allow future format modifications
+  protected int version;
+  protected int numMetaBytes;
+  protected int numKeyValueBytes;
+  protected boolean includesMemstoreTimestamp;//probably don't need this explicitly, but only 1 byte
+
+  // split the byte[] into 6 sections for the different data types
+  protected int numRowBytes;
+  protected int numFamilyBytes;
+  protected int numQualifierBytes;
+  protected int numTimestampBytes;
+  protected int numMemstoreTimestampBytes;
+  protected int numDataBytes;
+
+  // number of bytes in each section of fixed width VInts
+  protected int nextNodeOffsetWidth;
+  protected int familyOffsetWidth;
+  protected int qualifierOffsetWidth;
+  protected int timestampIndexWidth;
+  protected int memstoreTimestampIndexWidth;
+  protected int dataOffsetWidth;
+  protected int dataLengthWidth;
+
+  // used to pre-allocate structures for reading
+  protected int rowTreeDepth;
+  protected int maxRowLength;
+  protected int maxQualifierLength;
+
+  // the timestamp from which the deltas are calculated
+  protected long minTimestamp;
+  protected int timestampDeltaWidth;
+  protected long minMemstoreTimestamp;
+  protected int memstoreTimestampDeltaWidth;
+
+  protected boolean allSameType;
+  protected byte allTypes;
+
+  protected int numUniqueRows;
+  protected int numUniqueFamilies;
+  protected int numUniqueQualifiers;
+
+
+  /***************** constructors ********************/
+
+  public PrefixTreeBlockMeta() {
+  }
+
+  public PrefixTreeBlockMeta(InputStream is) {
+    this.version = VERSION;
+    this.arrayOffset = 0;
+    this.bufferOffset = 0;
+    readVariableBytesFromInputStream(is);
+  }
+
+  public PrefixTreeBlockMeta(ByteBuffer buffer) {// pass in buffer positioned at start of PtBlockMeta
+    initOnBlock(buffer);
+  }
+
+  public void initOnBlock(ByteBuffer buffer) {
+    arrayOffset = buffer.arrayOffset();
+    bufferOffset = buffer.position();
+    readVariableBytesFromArray(buffer.array(), arrayOffset + bufferOffset);
+  }
+
+
+	/**************** operate on each field **********************/
+
+  public int calculateNumMetaBytes(){
+    int numBytes = 0;
+    numBytes += UVIntTool.numBytes(version);
+    numBytes += UVLongTool.numBytes(numMetaBytes);
+    numBytes += UVIntTool.numBytes(numKeyValueBytes);
+    ++numBytes;//os.write(getIncludesMemstoreTimestamp());
+
+    numBytes += UVIntTool.numBytes(numRowBytes);
+    numBytes += UVIntTool.numBytes(numFamilyBytes);
+    numBytes += UVIntTool.numBytes(numQualifierBytes);
+    numBytes += UVIntTool.numBytes(numTimestampBytes);
+    numBytes += UVIntTool.numBytes(numMemstoreTimestampBytes);
+    numBytes += UVIntTool.numBytes(numDataBytes);
+
+    numBytes += UVIntTool.numBytes(nextNodeOffsetWidth);
+    numBytes += UVIntTool.numBytes(familyOffsetWidth);
+    numBytes += UVIntTool.numBytes(qualifierOffsetWidth);
+    numBytes += UVIntTool.numBytes(timestampIndexWidth);
+    numBytes += UVIntTool.numBytes(memstoreTimestampIndexWidth);
+    numBytes += UVIntTool.numBytes(dataOffsetWidth);
+    numBytes += UVIntTool.numBytes(dataLengthWidth);
+
+    numBytes += UVIntTool.numBytes(rowTreeDepth);
+    numBytes += UVIntTool.numBytes(maxRowLength);
+    numBytes += UVIntTool.numBytes(maxQualifierLength);
+
+    numBytes += UVLongTool.numBytes(minTimestamp);
+    numBytes += UVIntTool.numBytes(timestampDeltaWidth);
+    numBytes += UVLongTool.numBytes(minMemstoreTimestamp);
+    numBytes += UVIntTool.numBytes(memstoreTimestampDeltaWidth);
+    ++numBytes;//os.write(getAllSameTypeByte());
+    ++numBytes;//os.write(allTypes);
+
+    numBytes += UVIntTool.numBytes(numUniqueRows);
+    numBytes += UVIntTool.numBytes(numUniqueFamilies);
+    numBytes += UVIntTool.numBytes(numUniqueQualifiers);
+    return numBytes;
+  }
+
+  public void writeVariableBytesToOutputStream(OutputStream os) {
+    try {
+      UVIntTool.writeBytes(version, os);
+      UVIntTool.writeBytes(numMetaBytes, os);
+      UVIntTool.writeBytes(numKeyValueBytes, os);
+      os.write(getIncludesMemstoreTimestampByte());
+
+      UVIntTool.writeBytes(numRowBytes, os);
+      UVIntTool.writeBytes(numFamilyBytes, os);
+      UVIntTool.writeBytes(numQualifierBytes, os);
+      UVIntTool.writeBytes(numTimestampBytes, os);
+      UVIntTool.writeBytes(numMemstoreTimestampBytes, os);
+      UVIntTool.writeBytes(numDataBytes, os);
+
+      UVIntTool.writeBytes(nextNodeOffsetWidth, os);
+      UVIntTool.writeBytes(familyOffsetWidth, os);
+      UVIntTool.writeBytes(qualifierOffsetWidth, os);
+      UVIntTool.writeBytes(timestampIndexWidth, os);
+      UVIntTool.writeBytes(memstoreTimestampIndexWidth, os);
+      UVIntTool.writeBytes(dataOffsetWidth, os);
+      UVIntTool.writeBytes(dataLengthWidth, os);
+
+      UVIntTool.writeBytes(rowTreeDepth, os);
+      UVIntTool.writeBytes(maxRowLength, os);
+      UVIntTool.writeBytes(maxQualifierLength, os);
+
+      UVLongTool.writeBytes(minTimestamp, os);
+      UVIntTool.writeBytes(timestampDeltaWidth, os);
+      UVLongTool.writeBytes(minMemstoreTimestamp, os);
+      UVIntTool.writeBytes(memstoreTimestampDeltaWidth, os);
+      os.write(getAllSameTypeByte());
+      os.write(allTypes);
+
+      UVIntTool.writeBytes(numUniqueRows, os);
+      UVIntTool.writeBytes(numUniqueFamilies, os);
+      UVIntTool.writeBytes(numUniqueQualifiers, os);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public void readVariableBytesFromInputStream(InputStream is) {
+    try {
+      version = UVIntTool.getInt(is);
+      numMetaBytes = UVIntTool.getInt(is);
+      numKeyValueBytes = UVIntTool.getInt(is);
+      setIncludesMemstoreTimestamp((byte) is.read());
+
+      numRowBytes = UVIntTool.getInt(is);
+      numFamilyBytes = UVIntTool.getInt(is);
+      numQualifierBytes = UVIntTool.getInt(is);
+      numTimestampBytes = UVIntTool.getInt(is);
+      numMemstoreTimestampBytes = UVIntTool.getInt(is);
+      numDataBytes = UVIntTool.getInt(is);
+
+      nextNodeOffsetWidth = UVIntTool.getInt(is);
+      familyOffsetWidth = UVIntTool.getInt(is);
+      qualifierOffsetWidth = UVIntTool.getInt(is);
+      timestampIndexWidth = UVIntTool.getInt(is);
+      memstoreTimestampIndexWidth = UVIntTool.getInt(is);
+      dataOffsetWidth = UVIntTool.getInt(is);
+      dataLengthWidth = UVIntTool.getInt(is);
+
+      rowTreeDepth = UVIntTool.getInt(is);
+      maxRowLength = UVIntTool.getInt(is);
+      maxQualifierLength = UVIntTool.getInt(is);
+
+      minTimestamp = UVLongTool.getLong(is);
+      timestampDeltaWidth = UVIntTool.getInt(is);
+      minMemstoreTimestamp = UVLongTool.getLong(is);
+      memstoreTimestampDeltaWidth = UVIntTool.getInt(is);
+
+      setAllSameType((byte) is.read());
+      allTypes = (byte) is.read();
+
+      numUniqueRows = UVIntTool.getInt(is);
+      numUniqueFamilies = UVIntTool.getInt(is);
+      numUniqueQualifiers = UVIntTool.getInt(is);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public void readVariableBytesFromArray(byte[] bytes, int offset) {
+    int position = offset;
+
+    version = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(version);
+    numMetaBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numMetaBytes);
+    numKeyValueBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numKeyValueBytes);
+    setIncludesMemstoreTimestamp(bytes[position]);
+    ++position;
+
+    numRowBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numRowBytes);
+    numFamilyBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numFamilyBytes);
+    numQualifierBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numQualifierBytes);
+    numTimestampBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numTimestampBytes);
+    numMemstoreTimestampBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numMemstoreTimestampBytes);
+    numDataBytes = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numDataBytes);
+
+    nextNodeOffsetWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(nextNodeOffsetWidth);
+    familyOffsetWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(familyOffsetWidth);
+    qualifierOffsetWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(qualifierOffsetWidth);
+    timestampIndexWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(timestampIndexWidth);
+    memstoreTimestampIndexWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(memstoreTimestampIndexWidth);
+    dataOffsetWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(dataOffsetWidth);
+    dataLengthWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(dataLengthWidth);
+
+    rowTreeDepth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(rowTreeDepth);
+    maxRowLength = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(maxRowLength);
+    maxQualifierLength = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(maxQualifierLength);
+
+    minTimestamp = UVLongTool.getLong(bytes, position);
+    position += UVLongTool.numBytes(minTimestamp);
+    timestampDeltaWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(timestampDeltaWidth);
+    minMemstoreTimestamp = UVLongTool.getLong(bytes, position);
+    position += UVLongTool.numBytes(minMemstoreTimestamp);
+    memstoreTimestampDeltaWidth = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(memstoreTimestampDeltaWidth);
+
+    setAllSameType(bytes[position]);
+    ++position;
+    allTypes = bytes[position];
+    ++position;
+
+    numUniqueRows = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numUniqueRows);
+    numUniqueFamilies = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numUniqueFamilies);
+    numUniqueQualifiers = UVIntTool.getInt(bytes, position);
+    position += UVIntTool.numBytes(numUniqueQualifiers);
+  }
+
+	//TODO method that can read directly from ByteBuffer instead of InputStream
+
+
+  /*************** methods *************************/
+
+  public int getKeyValueTypeWidth() {
+    return allSameType ? 0 : 1;
+  }
+
+  public byte getIncludesMemstoreTimestampByte() {
+    return includesMemstoreTimestamp ? (byte) 1 : (byte) 0;
+  }
+
+  public void setIncludesMemstoreTimestamp(byte includesMemstoreTimestampByte) {
+    includesMemstoreTimestamp = includesMemstoreTimestampByte != 0;
+  }
+
+  public byte getAllSameTypeByte() {
+    return allSameType ? (byte) 1 : (byte) 0;
+  }
+
+  public void setAllSameType(byte allSameTypeByte) {
+    allSameType = allSameTypeByte != 0;
+  }
+
+  public boolean isAllSameTimestamp() {
+    return timestampIndexWidth == 0;
+  }
+
+  public boolean isAllSameMemstoreTimestamp() {
+    return memstoreTimestampIndexWidth == 0;
+  }
+
+  // for debugging
+  public byte[] getMetaBytes() {
+    ByteArrayOutputStream baos = new ByteArrayOutputStream(1000);
+    writeVariableBytesToOutputStream(baos);
+    return baos.toByteArray();
+  }
+
+  public byte[] getNonMetaBytes(ByteBuffer block) {
+    return ByteBufferUtils.toBytes(block, getNumMetaBytes());
+  }
+
+	@Override
+  public boolean equals(Object obj) {
+    if (this == obj)
+      return true;
+    if (obj == null)
+      return false;
+    if (getClass() != obj.getClass())
+      return false;
+    PrefixTreeBlockMeta other = (PrefixTreeBlockMeta) obj;
+    if (allSameType != other.allSameType)
+      return false;
+    if (allTypes != other.allTypes)
+      return false;
+    if (arrayOffset != other.arrayOffset)
+      return false;
+    if (bufferOffset != other.bufferOffset)
+      return false;
+    if (dataLengthWidth != other.dataLengthWidth)
+      return false;
+    if (dataOffsetWidth != other.dataOffsetWidth)
+      return false;
+    if (familyOffsetWidth != other.familyOffsetWidth)
+      return false;
+    if (includesMemstoreTimestamp != other.includesMemstoreTimestamp)
+      return false;
+    if (maxQualifierLength != other.maxQualifierLength)
+      return false;
+    if (maxRowLength != other.maxRowLength)
+      return false;
+    if (memstoreTimestampDeltaWidth != other.memstoreTimestampDeltaWidth)
+      return false;
+    if (memstoreTimestampIndexWidth != other.memstoreTimestampIndexWidth)
+      return false;
+    if (minMemstoreTimestamp != other.minMemstoreTimestamp)
+      return false;
+    if (minTimestamp != other.minTimestamp)
+      return false;
+    if (nextNodeOffsetWidth != other.nextNodeOffsetWidth)
+      return false;
+    if (numDataBytes != other.numDataBytes)
+      return false;
+    if (numFamilyBytes != other.numFamilyBytes)
+      return false;
+    if (numMemstoreTimestampBytes != other.numMemstoreTimestampBytes)
+      return false;
+    if (numMetaBytes != other.numMetaBytes)
+      return false;
+    if (numQualifierBytes != other.numQualifierBytes)
+      return false;
+    if (numRowBytes != other.numRowBytes)
+      return false;
+    if (numTimestampBytes != other.numTimestampBytes)
+      return false;
+    if (numUniqueFamilies != other.numUniqueFamilies)
+      return false;
+    if (numUniqueQualifiers != other.numUniqueQualifiers)
+      return false;
+    if (numUniqueRows != other.numUniqueRows)
+      return false;
+    if (numKeyValueBytes != other.numKeyValueBytes)
+      return false;
+    if (qualifierOffsetWidth != other.qualifierOffsetWidth)
+      return false;
+    if (rowTreeDepth != other.rowTreeDepth)
+      return false;
+    if (timestampDeltaWidth != other.timestampDeltaWidth)
+      return false;
+    if (timestampIndexWidth != other.timestampIndexWidth)
+      return false;
+    if (version != other.version)
+      return false;
+    return true;
+  }
+
+
+  /************** absolute getters *******************/
+
+  public int getAbsoluteMetaOffset() {
+    return arrayOffset + bufferOffset;
+  }
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = 1;
+    result = prime * result + (allSameType ? 1231 : 1237);
+    result = prime * result + allTypes;
+    result = prime * result + arrayOffset;
+    result = prime * result + bufferOffset;
+    result = prime * result + dataLengthWidth;
+    result = prime * result + dataOffsetWidth;
+    result = prime * result + familyOffsetWidth;
+    result = prime * result + (includesMemstoreTimestamp ? 1231 : 1237);
+    result = prime * result + maxQualifierLength;
+    result = prime * result + maxRowLength;
+    result = prime * result + memstoreTimestampDeltaWidth;
+    result = prime * result + memstoreTimestampIndexWidth;
+    result = prime * result + (int) (minMemstoreTimestamp ^ (minMemstoreTimestamp >>> 32));
+    result = prime * result + (int) (minTimestamp ^ (minTimestamp >>> 32));
+    result = prime * result + nextNodeOffsetWidth;
+    result = prime * result + numDataBytes;
+    result = prime * result + numFamilyBytes;
+    result = prime * result + numMemstoreTimestampBytes;
+    result = prime * result + numMetaBytes;
+    result = prime * result + numQualifierBytes;
+    result = prime * result + numRowBytes;
+    result = prime * result + numTimestampBytes;
+    result = prime * result + numUniqueFamilies;
+    result = prime * result + numUniqueQualifiers;
+    result = prime * result + numUniqueRows;
+    result = prime * result + numKeyValueBytes;
+    result = prime * result + qualifierOffsetWidth;
+    result = prime * result + rowTreeDepth;
+    result = prime * result + timestampDeltaWidth;
+    result = prime * result + timestampIndexWidth;
+    result = prime * result + version;
+    return result;
+  }
+
+
+  @Override
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append("PtBlockMeta [arrayOffset=");
+    builder.append(arrayOffset);
+    builder.append(", bufferOffset=");
+    builder.append(bufferOffset);
+    builder.append(", version=");
+    builder.append(version);
+    builder.append(", numMetaBytes=");
+    builder.append(numMetaBytes);
+    builder.append(", numKeyValueBytes=");
+    builder.append(numKeyValueBytes);
+    builder.append(", includesMemstoreTimestamp=");
+    builder.append(includesMemstoreTimestamp);
+    builder.append(", numRowBytes=");
+    builder.append(numRowBytes);
+    builder.append(", numFamilyBytes=");
+    builder.append(numFamilyBytes);
+    builder.append(", numQualifierBytes=");
+    builder.append(numQualifierBytes);
+    builder.append(", numTimestampBytes=");
+    builder.append(numTimestampBytes);
+    builder.append(", numMemstoreTimestampBytes=");
+    builder.append(numMemstoreTimestampBytes);
+    builder.append(", numDataBytes=");
+    builder.append(numDataBytes);
+    builder.append(", nextNodeOffsetWidth=");
+    builder.append(nextNodeOffsetWidth);
+    builder.append(", familyOffsetWidth=");
+    builder.append(familyOffsetWidth);
+    builder.append(", qualifierOffsetWidth=");
+    builder.append(qualifierOffsetWidth);
+    builder.append(", timestampIndexWidth=");
+    builder.append(timestampIndexWidth);
+    builder.append(", memstoreTimestampIndexWidth=");
+    builder.append(memstoreTimestampIndexWidth);
+    builder.append(", dataOffsetWidth=");
+    builder.append(dataOffsetWidth);
+    builder.append(", dataLengthWidth=");
+    builder.append(dataLengthWidth);
+    builder.append(", rowTreeDepth=");
+    builder.append(rowTreeDepth);
+    builder.append(", maxRowLength=");
+    builder.append(maxRowLength);
+    builder.append(", maxQualifierLength=");
+    builder.append(maxQualifierLength);
+    builder.append(", minTimestamp=");
+    builder.append(minTimestamp);
+    builder.append(", timestampDeltaWidth=");
+    builder.append(timestampDeltaWidth);
+    builder.append(", minMemstoreTimestamp=");
+    builder.append(minMemstoreTimestamp);
+    builder.append(", memstoreTimestampDeltaWidth=");
+    builder.append(memstoreTimestampDeltaWidth);
+    builder.append(", allSameType=");
+    builder.append(allSameType);
+    builder.append(", allTypes=");
+    builder.append(allTypes);
+    builder.append(", numUniqueRows=");
+    builder.append(numUniqueRows);
+    builder.append(", numUniqueFamilies=");
+    builder.append(numUniqueFamilies);
+    builder.append(", numUniqueQualifiers=");
+    builder.append(numUniqueQualifiers);
+    builder.append("]");
+    return builder.toString();
+  }
+
+  public int getAbsoluteRowOffset() {
+    return getAbsoluteMetaOffset() + numMetaBytes;
+  }
+
+  public int getAbsoluteFamilyOffset() {
+    return getAbsoluteRowOffset() + numRowBytes;
+  }
+
+  public int getAbsoluteQualifierOffset() {
+    return getAbsoluteFamilyOffset() + numFamilyBytes;
+  }
+
+  public int getAbsoluteTimestampOffset() {
+    return getAbsoluteQualifierOffset() + numQualifierBytes;
+  }
+
+  public int getAbsoluteMemstoreTimestampOffset() {
+    return getAbsoluteTimestampOffset() + numTimestampBytes;
+  }
+
+  public int getAbsoluteDataOffset() {
+    return getAbsoluteMemstoreTimestampOffset() + numMemstoreTimestampBytes;
+  }
+
+
+  /************** methods *******************/
+
+  public int getNumDataRegionBytes(){
+    return numRowBytes
+        + numFamilyBytes
+        + numQualifierBytes
+        + numTimestampBytes
+        + numMemstoreTimestampBytes
+        + numDataBytes;
+	}
+
+
+  /*************** get/set ***************************/
+
+  public int getTimestampDeltaWidth() {
+    return timestampDeltaWidth;
+  }
+
+  public void setTimestampDeltaWidth(int timestampDeltaWidth) {
+    this.timestampDeltaWidth = timestampDeltaWidth;
+  }
+
+  public int getDataOffsetWidth() {
+    return dataOffsetWidth;
+  }
+
+  public void setDataOffsetWidth(int dataOffsetWidth) {
+    this.dataOffsetWidth = dataOffsetWidth;
+  }
+
+  public int getDataLengthWidth() {
+    return dataLengthWidth;
+  }
+
+  public void setDataLengthWidth(int dataLengthWidth) {
+    this.dataLengthWidth = dataLengthWidth;
+  }
+
+  public int getMaxRowLength() {
+    return maxRowLength;
+  }
+
+  public void setMaxRowLength(int maxRowLength) {
+    this.maxRowLength = maxRowLength;
+  }
+
+  public long getMinTimestamp() {
+    return minTimestamp;
+  }
+
+  public void setMinTimestamp(long minTimestamp) {
+    this.minTimestamp = minTimestamp;
+  }
+
+  public byte getAllTypes() {
+    return allTypes;
+  }
+
+  public void setAllTypes(byte allTypes) {
+    this.allTypes = allTypes;
+  }
+
+  public boolean isAllSameType() {
+    return allSameType;
+  }
+
+  public void setAllSameType(boolean allSameType) {
+    this.allSameType = allSameType;
+  }
+
+  public int getNextNodeOffsetWidth() {
+    return nextNodeOffsetWidth;
+  }
+
+  public void setNextNodeOffsetWidth(int nextNodeOffsetWidth) {
+    this.nextNodeOffsetWidth = nextNodeOffsetWidth;
+  }
+
+  public int getNumRowBytes() {
+    return numRowBytes;
+  }
+
+  public void setNumRowBytes(int numRowBytes) {
+    this.numRowBytes = numRowBytes;
+  }
+
+  public int getNumTimestampBytes() {
+    return numTimestampBytes;
+  }
+
+  public void setNumTimestampBytes(int numTimestampBytes) {
+    this.numTimestampBytes = numTimestampBytes;
+  }
+
+  public int getNumDataBytes() {
+    return numDataBytes;
+  }
+
+  public void setNumDataBytes(int numDataBytes) {
+    this.numDataBytes = numDataBytes;
+  }
+
+  public int getNumMetaBytes() {
+    return numMetaBytes;
+  }
+
+  public void setNumMetaBytes(int numMetaBytes) {
+    this.numMetaBytes = numMetaBytes;
+  }
+
+  public int getArrayOffset() {
+    return arrayOffset;
+  }
+
+  public void setArrayOffset(int arrayOffset) {
+    this.arrayOffset = arrayOffset;
+  }
+
+  public int getBufferOffset() {
+    return bufferOffset;
+  }
+
+  public void setBufferOffset(int bufferOffset) {
+    this.bufferOffset = bufferOffset;
+  }
+
+  public int getNumKeyValueBytes() {
+    return numKeyValueBytes;
+  }
+
+  public void setNumKeyValueBytes(int numKeyValueBytes) {
+    this.numKeyValueBytes = numKeyValueBytes;
+  }
+
+  public int getRowTreeDepth() {
+    return rowTreeDepth;
+  }
+
+  public void setRowTreeDepth(int rowTreeDepth) {
+    this.rowTreeDepth = rowTreeDepth;
+  }
+
+  public int getNumMemstoreTimestampBytes() {
+    return numMemstoreTimestampBytes;
+  }
+
+  public void setNumMemstoreTimestampBytes(int numMemstoreTimestampBytes) {
+    this.numMemstoreTimestampBytes = numMemstoreTimestampBytes;
+  }
+
+  public int getMemstoreTimestampDeltaWidth() {
+    return memstoreTimestampDeltaWidth;
+  }
+
+  public void setMemstoreTimestampDeltaWidth(int memstoreTimestampDeltaWidth) {
+    this.memstoreTimestampDeltaWidth = memstoreTimestampDeltaWidth;
+  }
+
+  public long getMinMemstoreTimestamp() {
+    return minMemstoreTimestamp;
+  }
+
+  public void setMinMemstoreTimestamp(long minMemstoreTimestamp) {
+    this.minMemstoreTimestamp = minMemstoreTimestamp;
+  }
+
+  public int getNumFamilyBytes() {
+    return numFamilyBytes;
+  }
+
+  public void setNumFamilyBytes(int numFamilyBytes) {
+    this.numFamilyBytes = numFamilyBytes;
+  }
+
+  public int getFamilyOffsetWidth() {
+    return familyOffsetWidth;
+  }
+
+  public void setFamilyOffsetWidth(int familyOffsetWidth) {
+    this.familyOffsetWidth = familyOffsetWidth;
+  }
+
+  public int getNumUniqueRows() {
+    return numUniqueRows;
+  }
+
+  public void setNumUniqueRows(int numUniqueRows) {
+    this.numUniqueRows = numUniqueRows;
+  }
+
+  public int getNumUniqueFamilies() {
+    return numUniqueFamilies;
+  }
+
+  public void setNumUniqueFamilies(int numUniqueFamilies) {
+    this.numUniqueFamilies = numUniqueFamilies;
+  }
+
+  public int getNumUniqueQualifiers() {
+    return numUniqueQualifiers;
+  }
+
+  public void setNumUniqueQualifiers(int numUniqueQualifiers) {
+    this.numUniqueQualifiers = numUniqueQualifiers;
+  }
+
+  public int getNumQualifierBytes() {
+    return numQualifierBytes;
+  }
+
+  public void setNumQualifierBytes(int numQualifierBytes) {
+    this.numQualifierBytes = numQualifierBytes;
+  }
+
+  public int getQualifierOffsetWidth() {
+    return qualifierOffsetWidth;
+  }
+
+  public void setQualifierOffsetWidth(int qualifierOffsetWidth) {
+    this.qualifierOffsetWidth = qualifierOffsetWidth;
+  }
+
+  public int getMaxQualifierLength() {
+    return maxQualifierLength;
+  }
+
+  public void setMaxQualifierLength(int maxQualifierLength) {
+    this.maxQualifierLength = maxQualifierLength;
+  }
+
+  public int getTimestampIndexWidth() {
+    return timestampIndexWidth;
+  }
+
+  public void setTimestampIndexWidth(int timestampIndexWidth) {
+    this.timestampIndexWidth = timestampIndexWidth;
+  }
+
+  public int getMemstoreTimestampIndexWidth() {
+    return memstoreTimestampIndexWidth;
+  }
+
+  public void setMemstoreTimestampIndexWidth(int memstoreTimestampIndexWidth) {
+    this.memstoreTimestampIndexWidth = memstoreTimestampIndexWidth;
+  }
+
+  public int getVersion() {
+    return version;
+  }
+
+  public void setVersion(int version) {
+    this.version = version;
+  }
+
+  public boolean isIncludesMemstoreTimestamp() {
+    return includesMemstoreTimestamp;
+  }
+
+  public void setIncludesMemstoreTimestamp(boolean includesMemstoreTimestamp) {
+    this.includesMemstoreTimestamp = includesMemstoreTimestamp;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeCodec.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeCodec.java
new file mode 100644
index 0000000..86bf4a1
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeCodec.java
@@ -0,0 +1,205 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.KeyComparator;
+import org.apache.hadoop.hbase.KeyValue.MetaKeyComparator;
+import org.apache.hadoop.hbase.KeyValue.RootKeyComparator;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoder;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext;
+import org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultDecodingContext;
+import org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultEncodingContext;
+import org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext;
+import org.apache.hadoop.hbase.io.hfile.BlockType;
+import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
+import org.apache.hadoop.hbase.util.ByteBufferUtils;
+import org.apache.hadoop.io.RawComparator;
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.apache.hbase.codec.prefixtree.pool.DecoderFactory;
+import org.apache.hbase.codec.prefixtree.pool.EncoderFactory;
+
+/**
+ * This class created via reflection in DataBlockEncoding enum.  Update the enum if class name or
+ * package changes.
+ *
+ * PrefixTreeDataBlockEncoder implementation of DataBlockEncoder.  This is the primary entry point
+ * for PrefixTree encoding and decoding.  Encoding is delegated to instances of {@link PrefixTreeEncoder},
+ * and decoding is delegated to instances of {@ PtCellSearcher}.  Encoder and decoder instances are
+ * created and recycled by static PtEncoderFactory and PtDecoderFactory.
+ */
+public class PrefixTreeCodec implements DataBlockEncoder{
+
+  /**
+   * no-arg constructor for reflection
+   */
+  public PrefixTreeCodec() {
+  }
+
+  /**
+   * Copied from BufferedDataBlockEncoder. Almost definitely can be improved, but i'm not familiar
+   * enough with the concept of the HFileBlockEncodingContext.
+   */
+  @Override
+  public void encodeKeyValues(ByteBuffer in, boolean includesMemstoreTS,
+      HFileBlockEncodingContext blkEncodingCtx) throws IOException {
+    if (blkEncodingCtx.getClass() != HFileBlockDefaultEncodingContext.class) {
+      throw new IOException(this.getClass().getName() + " only accepts "
+          + HFileBlockDefaultEncodingContext.class.getName() + " as the " + "encoding context.");
+    }
+
+    HFileBlockDefaultEncodingContext encodingCtx = (HFileBlockDefaultEncodingContext) blkEncodingCtx;
+    encodingCtx.prepareEncoding();
+    DataOutputStream dataOut = encodingCtx.getOutputStreamForEncoder();
+    internalEncodeKeyValues(dataOut, in, includesMemstoreTS);
+
+    //do i need to check this, or will it always be DataBlockEncoding.PREFIX_TREE?
+    if (encodingCtx.getDataBlockEncoding() != DataBlockEncoding.NONE) {
+      encodingCtx.postEncoding(BlockType.ENCODED_DATA);
+    } else {
+      encodingCtx.postEncoding(BlockType.DATA);
+    }
+  }
+
+  protected void internalEncodeKeyValues(DataOutputStream encodedOutputStream,
+      ByteBuffer rawKeyValues, boolean includesMemstoreTS) throws IOException {
+    rawKeyValues.rewind();
+    PrefixTreeEncoder builder = EncoderFactory.checkOut(encodedOutputStream, includesMemstoreTS);
+
+    KeyValue kv;
+    while ((kv = KeyValueUtils.nextShallowCopy(rawKeyValues, includesMemstoreTS)) != null) {
+      builder.write(kv);
+    }
+
+    builder.flush();
+    EncoderFactory.checkIn(builder);
+  }
+
+
+  @Override
+  public ByteBuffer decodeKeyValues(DataInputStream source, boolean includesMemstoreTS)
+      throws IOException {
+    return decodeKeyValues(source, 0, 0, includesMemstoreTS);
+  }
+
+
+  /**
+   * I don't think this method is called during normal HBase operation, so efficiency is not
+   * important.
+   */
+  @Override
+  public ByteBuffer decodeKeyValues(DataInputStream source, int allocateHeaderLength,
+      int skipLastBytes, boolean includesMemstoreTS) throws IOException {
+    ByteBuffer sourceAsBuffer = ByteBufferUtils.drainInputStreamToBuffer(source);// waste
+    sourceAsBuffer.mark();
+    PrefixTreeBlockMeta blockMeta = new PrefixTreeBlockMeta(sourceAsBuffer);
+    sourceAsBuffer.rewind();
+    int numV1BytesWithHeader = allocateHeaderLength + blockMeta.getNumKeyValueBytes();
+    byte[] keyValueBytesWithHeader = new byte[numV1BytesWithHeader];
+    ByteBuffer result = ByteBuffer.wrap(keyValueBytesWithHeader);
+    result.rewind();
+    CellSearcher searcher = null;
+    try {
+      searcher = DecoderFactory.checkOut(sourceAsBuffer, includesMemstoreTS);
+      while (searcher.nextCell()) {
+        KeyValue currentCell = KeyValueUtils.copyToNewKeyValue(searcher.getCurrentCell());
+        // needs to be modified for DirectByteBuffers. no existing methods to
+        // write VLongs to byte[]
+        int offset = result.arrayOffset() + result.position();
+        KeyValueUtils.appendToByteArray(currentCell, result.array(), offset);
+        int keyValueLength = KeyValueUtils.length(currentCell);
+        ByteBufferUtils.skip(result, keyValueLength);
+        offset += keyValueLength;
+        if (includesMemstoreTS) {
+          ByteBufferUtils.writeVLong(result, currentCell.getMemstoreTS());
+        }
+      }
+      result.position(result.limit());//make it appear as if we were appending
+      return result;
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+
+  @Override
+  public ByteBuffer getFirstKeyInBlock(ByteBuffer block) {
+    block.rewind();
+    PrefixTreeArraySearcher searcher = null;
+    try {
+      //should i includeMemstoreTS (second argument)?  i think PrefixKeyDeltaEncoder is, so i will
+      searcher = DecoderFactory.checkOut(block, true);
+      if (!searcher.positionAtFirstCell()) {
+        return null;
+      }
+      return KeyValueUtils.copyKeyToNewByteBuffer(searcher.getCurrentCell());
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+  @Override
+  public HFileBlockEncodingContext newDataBlockEncodingContext(Algorithm compressionAlgorithm,
+      DataBlockEncoding encoding, byte[] header) {
+    if(DataBlockEncoding.PREFIX_TREE != encoding){
+      //i'm not sure why encoding is in the interface.  Each encoder implementation should probably
+      //know it's encoding type
+      throw new IllegalArgumentException("only DataBlockEncoding.PREFIX_TREE supported");
+    }
+    return new HFileBlockDefaultEncodingContext(compressionAlgorithm, encoding, header);
+  }
+
+  @Override
+  public HFileBlockDecodingContext newDataBlockDecodingContext(Algorithm compressionAlgorithm) {
+    return new HFileBlockDefaultDecodingContext(compressionAlgorithm);
+  }
+
+  /**
+   * Is this the correct handling of an illegal comparator?  How to prevent that from getting all
+   * the way to this point.
+   */
+  @Override
+  public EncodedSeeker createSeeker(RawComparator<byte[]> comparator, boolean includesMemstoreTS) {
+    if(! (comparator instanceof KeyComparator)){
+      throw new IllegalArgumentException("comparator must be KeyValue.KeyComparator");
+    }
+    if(comparator instanceof MetaKeyComparator){
+      throw new IllegalArgumentException("DataBlockEncoding.PREFIX_TREE not compatible with META "
+          +"table");
+    }
+    if(comparator instanceof RootKeyComparator){
+      throw new IllegalArgumentException("DataBlockEncoding.PREFIX_TREE not compatible with ROOT "
+          +"table");
+    }
+
+    //TODO what should i do with includeMemstoreTS?
+
+    return new PrefixTreeSeeker(includesMemstoreTS);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeSeeker.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeSeeker.java
new file mode 100644
index 0000000..6f76e52
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/PrefixTreeSeeker.java
@@ -0,0 +1,206 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree;
+
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.CellUtils;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoder.EncodedSeeker;
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher;
+import org.apache.hbase.codec.prefixtree.pool.DecoderFactory;
+
+/**
+ * These methods have the same definition as any implementation of the EncodedSeeker.
+ *
+ * In the future, the EncodedSeeker could be modified to work with the Cell interface directly.  It
+ * currently returns a new KeyValue object each time getKeyValue is called.  This is not horrible,
+ * but in order to create a new KeyValue object, we must first allocate a new byte[] and copy in
+ * the data from the PrefixTreeCell.  It is somewhat heavyweight right now.
+ */
+public class PrefixTreeSeeker implements EncodedSeeker {
+
+  protected ByteBuffer block;
+  protected boolean includeMemstoreTS;
+  protected PrefixTreeArraySearcher ptSearcher;
+
+  public PrefixTreeSeeker(boolean includeMemstoreTS) {
+    this.includeMemstoreTS = includeMemstoreTS;
+  }
+
+  @Override
+  public void setCurrentBuffer(ByteBuffer fullBlockBuffer) {
+    block = fullBlockBuffer;
+    ptSearcher = DecoderFactory.checkOut(block, includeMemstoreTS);
+    rewind();
+  }
+
+  /**
+   * Currently unused.
+   * <p/>
+   * TODO performance leak. should reuse the searchers. hbase does not currently have a hook where
+   * this can be called
+   */
+  public void releaseCurrentBuffer(){
+    DecoderFactory.checkIn(ptSearcher);
+  }
+
+
+  @Override
+  public ByteBuffer getKeyDeepCopy() {
+    return KeyValueUtils.copyKeyToNewByteBuffer(ptSearcher.getCurrentCell());
+  }
+
+
+  @Override
+  public ByteBuffer getValueShallowCopy() {
+    return CellUtils.getValueBufferShallowCopy(ptSearcher.getCurrentCell());
+  }
+
+  /**
+   * currently must do deep copy into new array
+   */
+  @Override
+  public ByteBuffer getKeyValueBuffer() {
+    return KeyValueUtils.copyToNewByteBuffer(ptSearcher.getCurrentCell());
+  }
+
+  /**
+   * currently must do deep copy into new array
+   */
+  @Override
+  public KeyValue getKeyValue() {
+    return KeyValueUtils.copyToNewKeyValue(ptSearcher.getCurrentCell());
+  }
+
+  /**
+   * Currently unused.
+   * <p/>
+   * A nice, lightweight reference, though the underlying cell is transient.  This method may return
+   * the same reference to the backing PrefixTreeCell repeatedly, while other implementations may
+   * return a different reference for each Cell.
+   * <p/>
+   * The goal will be to transition the upper layers of HBase, like Filters and KeyValueHeap, to use
+   * this method instead of the getKeyValue() methods above.
+   */
+  public Cell getCurrentCell() {
+    return ptSearcher.getCurrentCell();
+  }
+
+  @Override
+  public void rewind() {
+    ptSearcher.positionAtFirstCell();
+  }
+
+  @Override
+  public boolean next() {
+    return ptSearcher.nextCell();
+  }
+
+
+  private static final boolean USE_POSITION_BEFORE_VS_AFTER = false;
+
+  /**
+   * Seek forward only (should be called reseekToKeyInBlock?).
+   * <p/>
+   * If the exact key is found look at the seekBefore variable and:<br/>
+   * - if true: go to the previous key if it's true<br/>
+   * - if false: stay on the exact key
+   * <p/>
+   * If the exact key is not found, then go to the previous key *if possible*, but remember to leave
+   * the scanner in a valid state if possible.
+   * <p/>
+   * @param keyOnlyBytes KeyValue format of a Cell's key at which to position the seeker
+   * @param offset offset into the keyOnlyBytes array
+   * @param length number of bytes of the keyOnlyBytes array to use
+   * @param forceBeforeOnExactMatch if an exact match is found and seekBefore=true, back up one Cell
+   * @return 0 if the seeker is on the exact key<br/>
+   *         1 if the seeker is not on the key for any reason, including seekBefore being true
+   */
+  @Override
+  public int seekToKeyInBlock(byte[] keyOnlyBytes, int offset, int length,
+      boolean forceBeforeOnExactMatch) {
+    if (USE_POSITION_BEFORE_VS_AFTER) {
+      return seekToOrBeforeUsingPositionAtOrBefore(keyOnlyBytes, offset, length,
+        forceBeforeOnExactMatch);
+    }else{
+      return seekToOrBeforeUsingPositionAtOrAfter(keyOnlyBytes, offset, length,
+        forceBeforeOnExactMatch);
+    }
+  }
+
+
+
+  /*
+   * Include both of these options since the underlying PrefixTree supports both.  Possibly
+   * expand the EncodedSeeker to utilize them both.
+   */
+
+  protected int seekToOrBeforeUsingPositionAtOrBefore(byte[] keyOnlyBytes, int offset, int length,
+      boolean forceBeforeOnExactMatch){
+    // this does a deep copy of the key byte[] because the CellSearcher interface wants a Cell
+    KeyValue kv = KeyValue.createKeyValueFromKey(keyOnlyBytes, offset, length);
+
+    //should probably switch this to use the seekForwardToOrBefore method
+    CellScannerPosition position = ptSearcher.seekForwardToOrBefore(kv);
+
+    if(CellScannerPosition.AT == position){
+      if (forceBeforeOnExactMatch) {
+        ptSearcher.previousCell();
+        return 1;
+      }
+      return 0;
+    }
+
+    return 1;
+  }
+
+
+  protected int seekToOrBeforeUsingPositionAtOrAfter(byte[] keyOnlyBytes, int offset, int length,
+      boolean forceBeforeOnExactMatch){
+    // this does a deep copy of the key byte[] because the CellSearcher interface wants a Cell
+    KeyValue kv = KeyValue.createKeyValueFromKey(keyOnlyBytes, offset, length);
+
+    //should probably switch this to use the seekForwardToOrBefore method
+    CellScannerPosition position = ptSearcher.seekForwardToOrAfter(kv);
+
+    if(CellScannerPosition.AT == position){
+      if (forceBeforeOnExactMatch) {
+        ptSearcher.previousCell();
+        return 1;
+      }
+      return 0;
+
+    }else if(CellScannerPosition.AFTER == position){
+      if(!ptSearcher.isBeforeFirst()){
+        ptSearcher.previousCell();
+      }
+      return 1;
+
+    }else if(position == CellScannerPosition.AFTER_LAST){
+      return 1;
+    }
+
+    throw new RuntimeException("unexpected CellScannerPosition:"+position);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayReversibleScanner.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayReversibleScanner.java
new file mode 100644
index 0000000..e5165e0
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayReversibleScanner.java
@@ -0,0 +1,129 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode;
+
+import org.apache.hadoop.hbase.cell.scanner.ReversibleCellScanner;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+
+public class PrefixTreeArrayReversibleScanner extends PrefixTreeArrayScanner implements
+    ReversibleCellScanner {
+
+  /***************** construct ******************************/
+
+  public PrefixTreeArrayReversibleScanner(PrefixTreeBlockMeta blockMeta, int rowTreeDepth,
+      int rowBufferLength, int qualifierBufferLength) {
+    super(blockMeta, rowTreeDepth, rowBufferLength, qualifierBufferLength);
+  }
+
+
+  /***************** methods **********************************/
+
+  @Override
+  public boolean previousCell() {
+    if (afterLast) {
+      afterLast = false;
+      positionAtLastCell();
+      return true;
+    }
+    if (beforeFirst) {
+      return false;
+    }
+    if (isFirstCellInRow()) {
+      previousRowInternal();
+      if (beforeFirst) {
+        return false;
+      }
+      populateLastNonRowFields();
+      return true;
+    }
+    populatePreviousNonRowFields();
+    return true;
+  }
+
+  @Override
+  public boolean previousRow(boolean endOfRow) {
+    previousRowInternal();
+    if(beforeFirst){
+      return false;
+    }
+    if(endOfRow){
+      populateLastNonRowFields();
+    }else{
+      populateFirstNonRowFields();
+    }
+    return true;
+  }
+
+  public boolean previousRowInternal() {
+    if (beforeFirst) {
+      return false;
+    }
+    if (afterLast) {
+      positionAtLastRow();
+      return true;
+    }
+    if (currentRowNode.hasOccurrences()) {
+      discardCurrentRowNode(false);
+      if(currentRowNode==null){
+        return false;
+      }
+    }
+    while (!beforeFirst) {
+      if (isDirectlyAfterNub()) {//we are about to back up to the nub
+        currentRowNode.resetFanIndex();//sets it to -1, which is before the first leaf
+        nubCellsRemain = true;//this positions us on the nub
+        return true;
+      }
+      if (currentRowNode.hasPreviousFanNodes()) {
+        followPreviousFan();
+        descendToLastRowFromCurrentPosition();
+      } else {// keep going up the stack until we find previous fan positions
+        discardCurrentRowNode(false);
+        if(currentRowNode==null){
+          return false;
+        }
+      }
+      if (currentRowNode.hasOccurrences()) {// escape clause
+        return true;// found some values
+      }
+    }
+    return false;// went past the beginning
+  }
+  
+  protected boolean isDirectlyAfterNub() {
+    return currentRowNode.isNub() && currentRowNode.getFanIndex()==0;
+  }
+
+  protected void positionAtLastRow() {
+    reInitFirstNode();
+    descendToLastRowFromCurrentPosition();
+  }
+
+  protected void descendToLastRowFromCurrentPosition() {
+    while (currentRowNode.hasChildren()) {
+      followLastFan();
+    }
+  }
+
+  protected void positionAtLastCell() {
+    positionAtLastRow();
+    populateLastNonRowFields();
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java
new file mode 100644
index 0000000..5aa6410
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArrayScanner.java
@@ -0,0 +1,499 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode;
+
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.cell.scanner.CellScanner;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.decode.column.ColumnReader;
+import org.apache.hbase.codec.prefixtree.decode.row.RowNodeReader;
+import org.apache.hbase.codec.prefixtree.decode.timestamp.MemstoreTimestampDecoder;
+import org.apache.hbase.codec.prefixtree.decode.timestamp.TimestampDecoder;
+
+/**
+ * Extends PtCell and manipulates its protected fields.  Could alternatively contain a PtCell and
+ * call get/set methods.
+ *
+ * This is an "Array" scanner to distinguish from a future "ByteBuffer" scanner.  This
+ * implementation requires that the bytes be in a normal java byte[] for performance.  The
+ * alternative ByteBuffer implementation would allow for accessing data in an off-heap ByteBuffer
+ * without copying the whole buffer on-heap.
+ */
+public class PrefixTreeArrayScanner extends PrefixTreeCell implements CellScanner {
+
+  /***************** fields ********************************/
+
+  protected PrefixTreeBlockMeta blockMeta;
+
+  protected boolean beforeFirst;
+  protected boolean afterLast;
+
+  protected RowNodeReader[] rowNodes;
+  protected int rowNodeStackIndex;
+
+  protected RowNodeReader currentRowNode;
+  protected ColumnReader familyReader;
+  protected ColumnReader qualifierReader;
+  protected TimestampDecoder timestampReader;
+  protected MemstoreTimestampDecoder memstoreTimestampReader;
+
+  protected boolean nubCellsRemain;
+  protected int currentCellIndex;
+
+
+  /*********************** construct ******************************/
+
+  // pass in blockMeta so we can initialize buffers big enough for all cells in the block
+  public PrefixTreeArrayScanner(PrefixTreeBlockMeta blockMeta, int rowTreeDepth, int rowBufferLength,
+      int qualifierBufferLength) {
+    this.rowNodes = new RowNodeReader[rowTreeDepth];
+    for (int i = 0; i < rowNodes.length; ++i) {
+      rowNodes[i] = new RowNodeReader();
+    }
+    this.rowBuffer = new byte[rowBufferLength];
+    this.familyBuffer = new byte[PrefixTreeBlockMeta.MAX_FAMILY_LENGTH];
+    this.familyReader = new ColumnReader(familyBuffer, true);
+    this.qualifierBuffer = new byte[qualifierBufferLength];
+    this.qualifierReader = new ColumnReader(qualifierBuffer, false);
+    this.timestampReader = new TimestampDecoder();
+    this.memstoreTimestampReader = new MemstoreTimestampDecoder();
+  }
+
+
+  /**************** init helpers ***************************************/
+
+  // call when first accessing a block. create entirely new scanner if false
+  public boolean areBuffersBigEnough() {
+    if (rowNodes.length < blockMeta.getRowTreeDepth()) {
+      return false;
+    }
+    if (rowBuffer.length < blockMeta.getMaxRowLength()) {
+      return false;
+    }
+    if (qualifierBuffer.length < blockMeta.getMaxQualifierLength()) {
+      return false;
+    }
+    return true;
+  }
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block, boolean includeMemstoreTS) {
+    this.block = block;
+    this.blockMeta = blockMeta;
+    this.familyOffset = familyBuffer.length;
+    this.familyReader.initOnBlock(blockMeta, block);
+    this.qualifierOffset = qualifierBuffer.length;
+    this.qualifierReader.initOnBlock(blockMeta, block);
+    this.timestampReader.initOnBlock(blockMeta, block);
+    this.memstoreTimestampReader.initOnBlock(blockMeta, block);
+    this.includeMemstoreTS = includeMemstoreTS;
+    resetToBeforeFirstEntry();
+  }
+
+  @Override
+  public void resetToBeforeFirstEntry() {
+    beforeFirst = true;
+    afterLast = false;
+    rowNodeStackIndex = -1;
+    currentRowNode = null;
+    rowLength = 0;
+    familyOffset = familyBuffer.length;
+    familyLength = 0;
+    qualifierOffset = blockMeta.getMaxQualifierLength();
+    qualifierLength = 0;
+    nubCellsRemain = false;
+    currentCellIndex = -1;
+    timestamp = -1L;
+    type = DEFAULT_TYPE;
+    absoluteValueOffset = 0;//use 0 vs -1 so the cell is valid when value hasn't been initialized
+    valueLength = 0;// had it at -1, but that causes null Cell to add up to the wrong length
+  }
+
+  /**
+   * Call this before putting the scanner back into a pool so it doesn't hold the last used block
+   * in memory.
+   */
+  public void releaseBlockReference(){
+    block = null;
+  }
+
+
+  /********************** CellScanner **********************/
+
+  @Override
+  public PrefixTreeCell getCurrentCell() {
+    if(isOutOfBounds()){
+      return null;
+    }
+    return this;
+  }
+  
+  
+  /******************* Object methods ************************/
+  
+  /**
+   * Override PrefixTreeCell.toString() with a check to see if the current cell is valid.
+   */
+  @Override
+  public String toString() {
+    PrefixTreeCell currentCell = getCurrentCell();
+    if(currentCell==null){
+      return "null";
+    }
+    return currentCell.getKeyValueString();
+  }
+
+
+  /******************* advance ***************************/
+  
+  public boolean positionAtFirstCell() {
+    reInitFirstNode();
+    return nextCell();
+  }
+
+  @Override
+  public boolean nextCell() {
+    if (afterLast) {
+      return false;
+    }
+    if (!hasOccurrences()) {
+      resetToBeforeFirstEntry();
+    }
+    if (beforeFirst || isLastCellInRow()) {
+      nextRow();
+      if (afterLast) {
+        return false;
+      }
+    } else {
+      ++currentCellIndex;
+    }
+
+    populateNonRowFields(currentCellIndex);
+    return true;
+  }
+
+
+  public boolean nextRow() {
+    nextRowInternal();
+    if (afterLast) {
+      return false;
+    }
+    populateNonRowFields(currentCellIndex);
+    return true;
+  }
+
+
+  /**
+   * This method is safe to call when the scanner is not on a fully valid row node, as in the case
+   * of a row token miss in the Searcher
+   * @return
+   */
+  protected boolean nextRowInternal() {
+    if (afterLast) {
+      return false;
+    }
+    if (beforeFirst) {
+      initFirstNode();
+      if (currentRowNode.hasOccurrences()) {
+        if (currentRowNode.isNub()) {
+          nubCellsRemain = true;
+        }
+        currentCellIndex = 0;
+        return true;
+      }
+    }
+    if (currentRowNode.isLeaf()) {
+      discardCurrentRowNode(true);
+    }
+    while (!afterLast) {
+      if (nubCellsRemain) {
+        nubCellsRemain = false;
+      }
+      if (currentRowNode.hasMoreFanNodes()) {
+        followNextFan();
+        if (currentRowNode.hasOccurrences()) {
+          currentCellIndex = 0;
+          return true;
+        }// found some values
+      } else {
+        discardCurrentRowNode(true);
+      }
+    }
+    return false;// went past the end
+  }
+
+
+  /**************** secondary traversal methods ******************************/
+
+  protected void reInitFirstNode() {
+    resetToBeforeFirstEntry();
+    initFirstNode();
+  }
+
+  protected void initFirstNode() {
+    int offsetIntoUnderlyingStructure = blockMeta.getAbsoluteRowOffset();
+    rowNodeStackIndex = 0;
+    currentRowNode = rowNodes[0];
+    currentRowNode.initOnBlock(blockMeta, block, offsetIntoUnderlyingStructure);
+    appendToRowBuffer(currentRowNode.getToken());
+    beforeFirst = false;
+  }
+
+  protected void followFirstFan() {
+    followFan(0);
+  }
+
+  protected void followPreviousFan() {
+    int nextFanPosition = currentRowNode.getFanIndex() - 1;
+    followFan(nextFanPosition);
+  }
+
+  protected void followCurrentFan() {
+    int currentFanPosition = currentRowNode.getFanIndex();
+    followFan(currentFanPosition);
+  }
+
+  protected void followNextFan() {
+    int nextFanPosition = currentRowNode.getFanIndex() + 1;
+    followFan(nextFanPosition);
+  }
+
+  protected void followLastFan() {
+    followFan(currentRowNode.getLastFanIndex());
+  }
+
+  protected void followFan(int fanIndex) {
+    currentRowNode.setFanIndex(fanIndex);
+    appendToRowBuffer(currentRowNode.getFanByte(fanIndex));
+
+    int nextOffsetIntoUnderlyingStructure = currentRowNode.getOffset()
+        + currentRowNode.getNextNodeOffset(fanIndex, blockMeta);
+    ++rowNodeStackIndex;
+
+    currentRowNode = rowNodes[rowNodeStackIndex];
+    currentRowNode.initOnBlock(blockMeta, block, nextOffsetIntoUnderlyingStructure);
+
+    //TODO getToken is spewing garbage
+    appendToRowBuffer(currentRowNode.getToken());
+    if (currentRowNode.isNub()) {
+      nubCellsRemain = true;
+    }
+    currentCellIndex = 0;
+  }
+
+  /**
+   * @param forwards: which marker to set if we overflow
+   */
+  protected void discardCurrentRowNode(boolean forwards) {
+    RowNodeReader rowNodeBeingPopped = currentRowNode;
+    --rowNodeStackIndex;// pop it off the stack
+    if (rowNodeStackIndex < 0) {
+      currentRowNode = null;
+      if (forwards) {
+        markAfterLast();
+      } else {
+        markBeforeFirst();
+      }
+      return;
+    }
+    popFromRowBuffer(rowNodeBeingPopped);
+    currentRowNode = rowNodes[rowNodeStackIndex];
+  }
+
+  protected void markBeforeFirst() {
+    beforeFirst = true;
+    afterLast = false;
+    currentRowNode = null;
+  }
+
+  protected void markAfterLast() {
+    beforeFirst = false;
+    afterLast = true;
+    currentRowNode = null;
+  }
+
+
+  /***************** helper methods **************************/
+
+  protected void appendToRowBuffer(byte[] bytes) {
+    System.arraycopy(bytes, 0, rowBuffer, rowLength, bytes.length);
+    rowLength += bytes.length;
+  }
+
+  protected void appendToRowBuffer(byte b) {
+    rowBuffer[rowLength] = b;
+    ++rowLength;
+  }
+
+  protected void popFromRowBuffer(RowNodeReader rowNodeBeingPopped) {
+    rowLength -= rowNodeBeingPopped.getTokenLength();
+    --rowLength; // pop the parent's fan byte
+  }
+
+  protected boolean hasOccurrences() {
+    return currentRowNode != null && currentRowNode.hasOccurrences();
+  }
+  
+  protected boolean isBranch() {
+    return currentRowNode != null && !currentRowNode.hasOccurrences()
+        && currentRowNode.hasChildren();
+  }
+  
+  protected boolean isNub() {
+    return currentRowNode != null && currentRowNode.hasOccurrences()
+        && currentRowNode.hasChildren();
+  }
+  
+  protected boolean isLeaf() {
+    return currentRowNode != null && currentRowNode.hasOccurrences()
+        && !currentRowNode.hasChildren();
+  }
+  
+  //TODO expose this in a PrefixTreeScanner interface
+  public boolean isBeforeFirst(){
+    return beforeFirst;
+  }
+  
+  public boolean isAfterLast(){
+    return afterLast;
+  }
+  
+  protected boolean isOutOfBounds(){
+    return beforeFirst || afterLast;
+  }
+
+  protected boolean isFirstCellInRow() {
+    return currentCellIndex == 0;
+  }
+
+  protected boolean isLastCellInRow() {
+    return currentCellIndex == currentRowNode.getLastCellIndex();
+  }
+
+  protected boolean isOnLastFanOfEveryPreviousRowNode() {
+    // rowNodeStackIndex should skip the top stack element
+    for (int i = 0; i < rowNodeStackIndex; ++i) {
+      if (!rowNodes[i].isOnLastFanNode()) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+
+  /********************* fill in family/qualifier/ts/type/value ************/
+
+  protected int populateNonRowFieldsAndCompareTo(int cellNum, Cell key) {
+    populateNonRowFields(cellNum);
+    return CellComparator.compareStatic(this, key);
+  }
+
+  protected void populateFirstNonRowFields() {
+    populateNonRowFields(0);
+  }
+
+  protected void populatePreviousNonRowFields() {
+    populateNonRowFields(currentCellIndex - 1);
+  }
+
+  protected void populateLastNonRowFields() {
+    populateNonRowFields(currentRowNode.getLastCellIndex());
+  }
+
+  protected void populateNonRowFields(int cellIndex) {
+    currentCellIndex = cellIndex;
+    populateFamily();
+    populateQualifier();
+    populateTimestamp();
+    populateMemstoreTimestamp();
+    populateType();
+    populateValueOffsets();
+  }
+
+  protected void populateFamily() {
+    int familyTreeIndex = currentRowNode.getFamilyOffset(currentCellIndex, blockMeta);
+    familyOffset = familyReader.populateBuffer(familyTreeIndex).getColumnOffset();
+    familyLength = familyReader.getColumnLength();
+  }
+
+  protected void populateQualifier() {
+    int qualifierTreeIndex = currentRowNode.getColumnOffset(currentCellIndex, blockMeta);
+    qualifierOffset = qualifierReader.populateBuffer(qualifierTreeIndex).getColumnOffset();
+    qualifierLength = qualifierReader.getColumnLength();
+  }
+
+  protected void populateTimestamp() {
+    if (blockMeta.isAllSameTimestamp()) {
+      timestamp = blockMeta.getMinTimestamp();
+    } else {
+      int timestampIndex = currentRowNode.getTimestampIndex(currentCellIndex, blockMeta);
+      timestamp = timestampReader.getTimestamp(timestampIndex);
+    }
+  }
+
+  protected void populateMemstoreTimestamp() {
+    if (blockMeta.isAllSameMemstoreTimestamp()) {
+      memstoreTimestamp = blockMeta.getMinMemstoreTimestamp();
+    } else {
+      int memstoreTimestampIndex = currentRowNode.getMemstoreTimestampIndex(currentCellIndex,
+        blockMeta);
+      memstoreTimestamp = memstoreTimestampReader.getMemstoreTimestamp(memstoreTimestampIndex);
+    }
+  }
+
+  protected void populateType() {
+    int typeInt;
+    if (blockMeta.isAllSameType()) {
+      typeInt = blockMeta.getAllTypes();
+    } else {
+      typeInt = currentRowNode.getType(currentCellIndex, blockMeta);
+    }
+    type = PrefixTreeCell.TYPES[typeInt];
+  }
+
+  protected void populateValueOffsets() {
+    int offsetIntoValueSection = currentRowNode.getValueOffset(currentCellIndex, blockMeta);
+    absoluteValueOffset = blockMeta.getAbsoluteDataOffset() + offsetIntoValueSection;
+    valueLength = currentRowNode.getValueLength(currentCellIndex, blockMeta);
+  }
+
+
+  /**************** get/set ***************************/
+
+  public byte[] getTreeBytes() {
+    return block;
+  }
+
+  public PrefixTreeBlockMeta getBlockMeta() {
+    return blockMeta;
+  }
+
+  public int getMaxRowTreeStackNodes() {
+    return rowNodes.length;
+  }
+
+  public int getRowBufferLength() {
+    return rowBuffer.length;
+  }
+
+  public int getQualifierBufferLength() {
+    return qualifierBuffer.length;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArraySearcher.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArraySearcher.java
new file mode 100644
index 0000000..a2743f6
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeArraySearcher.java
@@ -0,0 +1,383 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode;
+
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.CellUtils;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+
+import com.google.common.primitives.UnsignedBytes;
+
+public class PrefixTreeArraySearcher extends PrefixTreeArrayReversibleScanner implements
+    CellSearcher {
+
+  /*************** construct ******************************/
+
+  public PrefixTreeArraySearcher(PrefixTreeBlockMeta blockMeta, int rowTreeDepth,
+      int rowBufferLength, int qualifierBufferLength) {
+    super(blockMeta, rowTreeDepth, rowBufferLength, qualifierBufferLength);
+  }
+
+
+  /********************* CellSearcher methods *******************/
+
+  @Override
+  public boolean positionAt(Cell key) {
+    return CellScannerPosition.AT == positionAtOrAfter(key);
+  }
+
+  @Override
+  public CellScannerPosition positionAtOrBefore(Cell key) {
+    reInitFirstNode();
+    int fanIndex = -1;
+
+    while(true){
+      //detect row mismatch.  break loop if mismatch
+      int currentNodeDepth = rowLength;
+      int rowTokenComparison = compareToCurrentToken(key);
+      if(rowTokenComparison != 0){
+        return fixRowTokenMissReverse(rowTokenComparison);
+      }
+
+      //exact row found, move on to qualifier & ts
+      if(matchesEndOfRow(key)){
+        return positionAtQualifierTimestamp(key, true);
+      }
+
+      //detect dead end (no fan to descend into)
+      if(!currentRowNode.hasFan()){
+        if(hasOccurrences()){//must be leaf or nub
+          populateLastNonRowFields();
+          return CellScannerPosition.BEFORE;
+        }else{
+          //TODO i don't think this case is exercised by any tests
+          return fixRowFanMissReverse(0);
+        }
+      }
+
+      //keep hunting for the rest of the row
+      byte searchForByte = CellUtils.getRowByte(key, currentNodeDepth);
+      fanIndex = currentRowNode.whichFanNode(searchForByte);
+      if(fanIndex < 0){//no matching row.  return early
+        int insertionPoint = -fanIndex;
+        return fixRowFanMissReverse(insertionPoint);
+      }
+      //found a match, so dig deeper into the tree
+      followFan(fanIndex);
+    }
+  }
+
+  /**
+   * Identical workflow as positionAtOrBefore, but split them to avoid having ~10 extra
+   * if-statements. Priority on readability and debugability.
+   */
+  @Override
+  public CellScannerPosition positionAtOrAfter(Cell key) {
+    reInitFirstNode();
+    int fanIndex = -1;
+
+    while(true){
+      //detect row mismatch.  break loop if mismatch
+      int currentNodeDepth = rowLength;
+      int rowTokenComparison = compareToCurrentToken(key);
+      if(rowTokenComparison != 0){
+        return fixRowTokenMissForward(rowTokenComparison);
+      }
+
+      //exact row found, move on to qualifier & ts
+      if(matchesEndOfRow(key)){
+        return positionAtQualifierTimestamp(key, false);
+      }
+
+      //detect dead end (no fan to descend into)
+      if(!currentRowNode.hasFan()){
+        if(hasOccurrences()){
+          populateFirstNonRowFields();
+          return CellScannerPosition.AFTER;
+        }else{
+          //TODO i don't think this case is exercised by any tests
+          return fixRowFanMissForward(0);
+        }
+      }
+
+      //keep hunting for the rest of the row
+      byte searchForByte = CellUtils.getRowByte(key, currentNodeDepth);
+      fanIndex = currentRowNode.whichFanNode(searchForByte);
+      if(fanIndex < 0){//no matching row.  return early
+        int insertionPoint = -fanIndex;
+        return fixRowFanMissForward(insertionPoint);
+      }
+      //found a match, so dig deeper into the tree
+      followFan(fanIndex);
+    }
+  }
+
+  @Override
+  public boolean seekForwardTo(Cell key) {
+    if(currentPositionIsAfter(key)){
+      //our position is after the requested key, so can't do anything
+      return false;
+    }
+    return positionAt(key);
+  }
+
+  @Override
+  public CellScannerPosition seekForwardToOrBefore(Cell key) {
+    //Do we even need this check or should upper layers avoid this situation.  It's relatively
+    //expensive compared to the rest of the seek operation.
+    if(currentPositionIsAfter(key)){
+      //our position is after the requested key, so can't do anything
+      return CellScannerPosition.AFTER;
+    }
+
+    return positionAtOrBefore(key);
+  }
+
+  @Override
+  public CellScannerPosition seekForwardToOrAfter(Cell key) {
+    //Do we even need this check or should upper layers avoid this situation.  It's relatively
+    //expensive compared to the rest of the seek operation.
+    if(currentPositionIsAfter(key)){
+      //our position is after the requested key, so can't do anything
+      return CellScannerPosition.AFTER;
+    }
+
+    return positionAtOrAfter(key);
+  }
+
+  /**
+   * The content of the buffers doesn't matter here, only that afterLast=true and beforeFirst=false
+   */
+  @Override
+  public void positionAfterLastCell() {
+    resetToBeforeFirstEntry();
+    beforeFirst = false;
+    afterLast = true;
+  }
+
+
+  /****************** internal methods ************************/
+
+  protected boolean currentPositionIsAfter(Cell cell){
+    return compareTo(cell) > 0;
+  }
+
+  protected CellScannerPosition positionAtQualifierTimestamp(Cell key, boolean beforeOnMiss) {
+    int minIndex = 0;
+    int maxIndex = currentRowNode.getLastCellIndex();
+    int diff;
+    while (true) {
+      int midIndex = (maxIndex + minIndex) / 2;//don't worry about overflow
+      diff = populateNonRowFieldsAndCompareTo(midIndex, key);
+
+      if (diff == 0) {// found exact match
+        return CellScannerPosition.AT;
+      } else if (minIndex == maxIndex) {// even termination case
+        break;
+      } else if ((minIndex + 1) == maxIndex) {// odd termination case
+        diff = populateNonRowFieldsAndCompareTo(maxIndex, key);
+        if(diff > 0){
+          diff = populateNonRowFieldsAndCompareTo(minIndex, key);
+        }
+        break;
+      } else if (diff < 0) {// keep going forward
+        minIndex = currentCellIndex;
+      } else {// went past it, back up
+        maxIndex = currentCellIndex;
+      }
+    }
+
+    if (diff == 0) {
+      return CellScannerPosition.AT;
+
+    } else if (diff < 0) {// we are before key
+      if (beforeOnMiss) {
+        return CellScannerPosition.BEFORE;
+      }
+      if (nextCell()) {
+        return CellScannerPosition.AFTER;
+      }
+      return CellScannerPosition.AFTER_LAST;
+
+    } else {// we are after key
+      if (!beforeOnMiss) {
+        return CellScannerPosition.AFTER;
+      }
+      if (previousCell()) {
+        return CellScannerPosition.BEFORE;
+      }
+      return CellScannerPosition.BEFORE_FIRST;
+    }
+  }
+
+  /**
+   * compare this.row to key.row but starting at the current rowLength
+   * @param key Cell being searched for
+   * @return true if row buffer contents match key.row
+   */
+  protected boolean matchesEndOfRow(Cell key) {
+    if (!currentRowNode.hasOccurrences()) {
+      return false;
+    }
+    int thisRowLength = rowLength;
+    int thatRowLength = key.getRowLength();
+    if (thisRowLength != thatRowLength) {
+      return false;
+    }
+    // TODO oops - is this missing the comparison of tail bytes?
+    return true;
+  }
+
+	//TODO move part of this to Cell comparator?
+	/**
+	 * compare only the bytes within the window of the current token
+	 *
+	 * @param key
+	 * @return return -1 if key is lessThan (before) this, 0 if equal, and 1 if key is after
+	 */
+  protected int compareToCurrentToken(Cell key) {
+    int startIndex = rowLength - currentRowNode.getTokenLength();
+    int endIndexExclusive = startIndex + currentRowNode.getTokenLength();
+    for (int i = startIndex; i < endIndexExclusive; ++i) {
+      if (i >= key.getRowLength()) {// key was shorter, so it's first
+        return -1;
+      }
+      byte keyByte = CellUtils.getRowByte(key, i);
+      byte thisByte = rowBuffer[i];
+      if (keyByte == thisByte) {
+        continue;
+      }
+      return UnsignedBytes.compare(keyByte, thisByte);
+    }
+    return 0;
+  }
+
+  protected void followLastFansUntilExhausted(){
+    while(currentRowNode.hasFan()){
+      followLastFan();
+    }
+  }
+
+
+	/****************** complete seek when token mismatch ******************/
+
+  /**
+   * @param comparison <0: input key is before the searcher's position<br/>
+   *                   >0: input key is after the searcher's position
+   */
+  protected CellScannerPosition fixRowTokenMissReverse(int comparison) {
+    if (comparison < 0) {//searcher position is after the input key, so back up
+      boolean foundPreviousRow = previousRow(true);
+      if(foundPreviousRow){
+        populateLastNonRowFields();
+        return CellScannerPosition.BEFORE;
+      }else{
+        return CellScannerPosition.BEFORE_FIRST;
+      }
+
+    }else{//searcher position is before the input key
+      if(currentRowNode.hasOccurrences()){
+        populateFirstNonRowFields();
+        return CellScannerPosition.BEFORE;
+      }
+      boolean foundNextRow = nextRow();
+      if(foundNextRow){
+        return CellScannerPosition.AFTER;
+      }else{
+        return CellScannerPosition.AFTER_LAST;
+      }
+    }
+  }
+
+  /**
+   * @param comparison <0: input key is before the searcher's position<br/>
+   *                   >0: input key is after the searcher's position
+   */
+  protected CellScannerPosition fixRowTokenMissForward(int comparison) {
+    if (comparison < 0) {//searcher position is after the input key
+      if(currentRowNode.hasOccurrences()){
+        populateFirstNonRowFields();
+        return CellScannerPosition.AFTER;
+      }
+      boolean foundNextRow = nextRow();
+      if(foundNextRow){
+        return CellScannerPosition.AFTER;
+      }else{
+        return CellScannerPosition.AFTER_LAST;
+      }
+
+    }else{//searcher position is before the input key, so go forward
+      discardCurrentRowNode(true);
+      boolean foundNextRow = nextRow();
+      if(foundNextRow){
+        return CellScannerPosition.AFTER;
+      }else{
+        return CellScannerPosition.AFTER_LAST;
+      }
+    }
+  }
+
+
+  /****************** complete seek when fan mismatch ******************/
+
+  protected CellScannerPosition fixRowFanMissReverse(int fanInsertionPoint){
+    if(fanInsertionPoint == 0){//we need to back up a row
+      boolean foundPreviousRow = previousRow(true);//true -> position on last cell in row
+      if(foundPreviousRow){
+        populateLastNonRowFields();
+        return CellScannerPosition.BEFORE;
+      }
+      return CellScannerPosition.BEFORE_FIRST;
+    }
+
+    //follow the previous fan, but then descend recursively forward
+    followFan(fanInsertionPoint - 1);
+    followLastFansUntilExhausted();
+    populateLastNonRowFields();
+    return CellScannerPosition.BEFORE;
+  }
+
+  protected CellScannerPosition fixRowFanMissForward(int fanInsertionPoint){
+    if(fanInsertionPoint >= currentRowNode.getFanOut()){
+      discardCurrentRowNode(true);
+      if (!nextRow()) {
+        return CellScannerPosition.AFTER_LAST;
+      } else {
+        return CellScannerPosition.AFTER;
+      }
+    }
+
+    followFan(fanInsertionPoint);
+    if(hasOccurrences()){
+      populateFirstNonRowFields();
+      return CellScannerPosition.AFTER;
+    }
+
+    if(nextRowInternal()){
+      populateFirstNonRowFields();
+      return CellScannerPosition.AFTER;
+
+    }else{
+      return CellScannerPosition.AFTER_LAST;
+    }
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeCell.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeCell.java
new file mode 100644
index 0000000..4aef52a
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/PrefixTreeCell.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+
+/**
+ * As the PtRowIterator moves through the tree bytes, it changes the values in the fields of
+ * this class so that KeyValue logic can be applied, but without allocating new memory for
+ * every KeyValue iterated through.
+ */
+public class PrefixTreeCell implements Cell, Comparable<Cell> {
+
+  /********************** static **********************/
+
+  public static final KeyValue.Type[] TYPES = new KeyValue.Type[256];
+  static {
+    for (KeyValue.Type type : KeyValue.Type.values()) {
+      TYPES[type.getCode() & 0xff] = type;
+    }
+  }
+
+  //Same as KeyValue constructor.  Only used to avoid NPE's when full cell hasn't been initialized.
+  public static final KeyValue.Type DEFAULT_TYPE = KeyValue.Type.Put;
+
+  /******************** fields ************************/
+
+  protected byte[] block;
+  //we could also avoid setting the memstoreTS in the scanner/searcher, but this is simpler
+  protected boolean includeMemstoreTS;
+
+  protected byte[] rowBuffer;
+  protected int rowLength;
+
+  protected byte[] familyBuffer;
+  protected int familyOffset;
+  protected int familyLength;
+
+  protected byte[] qualifierBuffer;// aligned to the end of the array
+  protected int qualifierOffset;
+  protected int qualifierLength;
+
+  protected Long timestamp;
+  protected Long memstoreTimestamp;
+
+  protected KeyValue.Type type;
+
+  protected int absoluteValueOffset;
+  protected int valueLength;
+
+
+  /********************** Cell methods ******************/
+
+  /**
+   * For debugging.  Currently creates new KeyValue to utilize its toString() method.
+   */
+  @Override
+  public String toString() {
+    return getKeyValueString();
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (!(obj instanceof Cell)) {
+      return false;
+    }
+    //Temporary hack to maintain backwards compatibility with KeyValue.equals
+    return CellComparator.equalsIgnoreMemstoreTS(this, (Cell)obj);
+  }
+  
+  @Override
+  public int compareTo(Cell other) {
+    return CellComparator.compareStatic(this, other);
+  }
+
+  @Override
+  public long getTimestamp() {
+    return timestamp;
+  }
+
+  @Override
+  public long getMemstoreTS() {
+    if (!includeMemstoreTS) {
+      return 0L;
+    }
+    return memstoreTimestamp;
+  }
+
+  @Override
+  public int getValueLength() {
+    return valueLength;
+  }
+
+  @Override
+  public byte[] getRowArray() {
+    return rowBuffer;
+  }
+
+  @Override
+  public int getRowOffset() {
+    return 0;
+  }
+
+  @Override
+  public short getRowLength() {
+    return (short) rowLength;
+  }
+
+  @Override
+  public byte[] getFamilyArray() {
+    return familyBuffer;
+  }
+
+  @Override
+  public int getFamilyOffset() {
+    return familyOffset;
+  }
+
+  @Override
+  public byte getFamilyLength() {
+    return (byte) familyLength;
+  }
+
+  @Override
+  public byte[] getQualifierArray() {
+    return qualifierBuffer;
+  }
+
+  @Override
+  public int getQualifierOffset() {
+    return qualifierOffset;
+  }
+
+  @Override
+  public int getQualifierLength() {
+    return qualifierLength;
+  }
+
+  @Override
+  public byte[] getValueArray() {
+    return block;
+  }
+
+  @Override
+  public int getValueOffset() {
+    return absoluteValueOffset;
+  }
+
+  @Override
+  public byte getTypeByte() {
+    return type.getCode();
+  }
+  
+  
+  /************************* helper methods *************************/
+  
+  /**
+   * Need this separate method so we can call it from subclasses' toString() methods
+   */
+  protected String getKeyValueString(){
+    KeyValue kv = KeyValueUtils.copyToNewKeyValue(this);
+    if (kv == null) {
+      return "null";
+    }
+    return kv.toString();
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnNodeReader.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnNodeReader.java
new file mode 100644
index 0000000..a758354
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnNodeReader.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode.column;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.util.vint.UFIntTool;
+import org.apache.hbase.util.vint.UVIntTool;
+
+public class ColumnNodeReader {
+
+  /**************** fields ************************/
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected byte[] block;
+
+  protected byte[] columnBuffer;
+  protected boolean familyVsQualifier;
+
+  protected int offsetIntoBlock;
+
+  protected int tokenOffsetIntoBlock;
+  protected int tokenLength;
+  protected int parentStartPosition;
+
+
+  /************** construct *************************/
+
+  public ColumnNodeReader(byte[] columnBuffer, boolean familyVsQualifier) {
+    this.columnBuffer = columnBuffer;
+    this.familyVsQualifier = familyVsQualifier;
+  }
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block) {
+    this.blockMeta = blockMeta;
+    this.block = block;
+  }
+
+
+  /************* methods *****************************/
+
+  public void positionAt(int offsetIntoBlock) {
+    this.offsetIntoBlock = offsetIntoBlock;
+    tokenLength = UVIntTool.getInt(block, offsetIntoBlock);
+    tokenOffsetIntoBlock = offsetIntoBlock + UVIntTool.numBytes(tokenLength);
+    int parentStartPositionIndex = tokenOffsetIntoBlock + tokenLength;
+    int offsetWidth;
+    if (familyVsQualifier) {
+      offsetWidth = blockMeta.getFamilyOffsetWidth();
+    } else {
+      offsetWidth = blockMeta.getQualifierOffsetWidth();
+    }
+    parentStartPosition = (int) UFIntTool.fromBytes(block, parentStartPositionIndex, offsetWidth);
+  }
+
+  public void prependTokenToBuffer(int bufferStartIndex) {
+    System.arraycopy(block, tokenOffsetIntoBlock, columnBuffer, bufferStartIndex, tokenLength);
+  }
+
+  public boolean isRoot() {
+    if (familyVsQualifier) {
+      return offsetIntoBlock == blockMeta.getAbsoluteFamilyOffset();
+    } else {
+      return offsetIntoBlock == blockMeta.getAbsoluteQualifierOffset();
+    }
+  }
+
+
+  /************** standard methods *********************/
+
+  @Override
+  public String toString() {
+    return super.toString() + "[" + offsetIntoBlock + "]";
+  }
+
+
+  /****************** get/set ****************************/
+
+  public int getTokenLength() {
+    return tokenLength;
+  }
+
+  public int getParentStartPosition() {
+    return parentStartPosition;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnReader.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnReader.java
new file mode 100644
index 0000000..aa19f7f
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/column/ColumnReader.java
@@ -0,0 +1,100 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode.column;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+
+public class ColumnReader {
+
+  /****************** fields *************************/
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected byte[] block;
+
+  protected byte[] columnBuffer;
+  protected int columnOffset;
+  protected int columnLength;
+  protected boolean familyVsQualifier;
+
+  protected ColumnNodeReader columnNodeReader;
+
+
+  /******************** construct *******************/
+
+  public ColumnReader(byte[] columnBuffer, boolean familyVsQualifier) {
+    this.columnBuffer = columnBuffer;
+    this.familyVsQualifier = familyVsQualifier;
+    this.columnNodeReader = new ColumnNodeReader(columnBuffer, familyVsQualifier);
+  }
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block) {
+    this.blockMeta = blockMeta;
+    this.block = block;
+    clearColumnBuffer();
+    columnNodeReader.initOnBlock(blockMeta, block);
+  }
+
+
+  /********************* methods *******************/
+
+  public ColumnReader populateBuffer(int offsetIntoColumnData) {
+    clearColumnBuffer();
+    int nextRelativeOffset = offsetIntoColumnData;
+    while (true) {
+      int absoluteOffset;
+      if (familyVsQualifier) {
+        absoluteOffset = blockMeta.getAbsoluteFamilyOffset() + nextRelativeOffset;
+      } else {
+        absoluteOffset = blockMeta.getAbsoluteQualifierOffset() + nextRelativeOffset;
+      }
+      columnNodeReader.positionAt(absoluteOffset);
+      columnOffset -= columnNodeReader.getTokenLength();
+      columnLength += columnNodeReader.getTokenLength();
+      columnNodeReader.prependTokenToBuffer(columnOffset);
+      if (columnNodeReader.isRoot()) {
+        return this;
+      }
+      nextRelativeOffset = columnNodeReader.getParentStartPosition();
+    }
+  }
+
+  public byte[] copyBufferToNewArray() {// for testing
+    byte[] out = new byte[columnLength];
+    System.arraycopy(columnBuffer, columnOffset, out, 0, out.length);
+    return out;
+  }
+
+  public int getColumnLength() {
+    return columnLength;
+  }
+
+  public void clearColumnBuffer() {
+    columnOffset = columnBuffer.length;
+    columnLength = 0;
+  }
+
+
+  /****************************** get/set *************************************/
+
+  public int getColumnOffset() {
+    return columnOffset;
+  }
+
+}
+
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/row/RowNodeReader.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/row/RowNodeReader.java
new file mode 100644
index 0000000..86b7fb6
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/row/RowNodeReader.java
@@ -0,0 +1,257 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode.row;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.util.vint.UFIntTool;
+import org.apache.hbase.util.vint.UVIntTool;
+
+public class RowNodeReader {
+
+  /************* fields ***********************************/
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected byte[] block;
+  protected int offset;
+  protected int fanIndex;
+
+  protected int numCells;
+
+  protected int tokenOffset;
+  protected int tokenWidth;
+  protected int fanOffset;
+  protected int fanOut;
+
+  protected int familyOffsetsOffset;
+  protected int columnOffsetsOffset;
+  protected int timestampIndexesOffset;
+  protected int memstoreTimestampIndexesOffset;
+  protected int operationTypesOffset;
+  protected int dataOffsetsOffset;
+  protected int dataLengthsOffset;
+  protected int nextNodeOffsetsOffset;
+
+
+  /******************* construct **************************/
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block, int offset) {
+    this.blockMeta = blockMeta;
+    this.block = block;
+
+    this.offset = offset;
+    resetFanIndex();
+
+    this.tokenWidth = UVIntTool.getInt(block, offset);
+    this.tokenOffset = offset + UVIntTool.numBytes(tokenWidth);
+
+    this.fanOut = UVIntTool.getInt(block, tokenOffset + tokenWidth);
+    this.fanOffset = tokenOffset + tokenWidth + UVIntTool.numBytes(fanOut);
+
+    this.numCells = UVIntTool.getInt(block, fanOffset + fanOut);
+
+    this.familyOffsetsOffset = fanOffset + fanOut + UVIntTool.numBytes(numCells);
+    this.columnOffsetsOffset = familyOffsetsOffset + numCells * blockMeta.getFamilyOffsetWidth();
+    this.timestampIndexesOffset = columnOffsetsOffset + numCells
+        * blockMeta.getQualifierOffsetWidth();
+    this.memstoreTimestampIndexesOffset = timestampIndexesOffset + numCells
+        * blockMeta.getMemstoreTimestampIndexWidth();
+    this.operationTypesOffset = timestampIndexesOffset + numCells
+        * blockMeta.getTimestampIndexWidth();
+    this.dataOffsetsOffset = operationTypesOffset + numCells * blockMeta.getKeyValueTypeWidth();
+    this.dataLengthsOffset = dataOffsetsOffset + numCells * blockMeta.getDataOffsetWidth();
+    this.nextNodeOffsetsOffset = dataLengthsOffset + numCells * blockMeta.getDataLengthWidth();
+  }
+
+
+  /******************** methods ****************************/
+
+  public boolean isLeaf() {
+    return fanOut == 0;
+  }
+
+  public boolean isNub() {
+    return fanOut > 0 && numCells > 0;
+  }
+
+  public boolean isBranch() {
+    return fanOut > 0 && numCells == 0;
+  }
+
+  public boolean hasOccurrences() {
+    return numCells > 0;
+  }
+
+  public int getTokenLength() {
+    return tokenWidth;
+  }
+
+  public byte getFanByte(int i) {
+    return block[fanOffset + i];
+  }
+  
+  /**
+   * for debugging
+   */
+  protected String getFanByteReadable(int i){
+    return Bytes.toStringBinary(block, fanOffset + i, 1);
+  }
+
+  public int getFamilyOffset(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getFamilyOffsetWidth();
+    int startIndex = familyOffsetsOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getColumnOffset(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getQualifierOffsetWidth();
+    int startIndex = columnOffsetsOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getTimestampIndex(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getTimestampIndexWidth();
+    int startIndex = timestampIndexesOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getMemstoreTimestampIndex(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getMemstoreTimestampIndexWidth();
+    int startIndex = memstoreTimestampIndexesOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getType(int index, PrefixTreeBlockMeta blockMeta) {
+    if (blockMeta.isAllSameType()) {
+      return blockMeta.getAllTypes();
+    }
+    return block[operationTypesOffset + index];
+  }
+
+  public int getValueOffset(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getDataOffsetWidth();
+    int startIndex = dataOffsetsOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getValueLength(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getDataLengthWidth();
+    int startIndex = dataLengthsOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public int getNextNodeOffset(int index, PrefixTreeBlockMeta blockMeta) {
+    int fIntWidth = blockMeta.getNextNodeOffsetWidth();
+    int startIndex = nextNodeOffsetsOffset + fIntWidth * index;
+    return (int) UFIntTool.fromBytes(block, startIndex, fIntWidth);
+  }
+
+  public String getBranchNubLeafIndicator() {
+    if (isNub()) {
+      return "N";
+    }
+    return isBranch() ? "B" : "L";
+  }
+
+  public boolean hasChildren() {
+    return fanOut > 0;
+  }
+
+  public int getLastFanIndex() {
+    return fanOut - 1;
+  }
+
+  public int getLastCellIndex() {
+    return numCells - 1;
+  }
+
+  public int getNumCells() {
+    return numCells;
+  }
+
+  public int getFanOut() {
+    return fanOut;
+  }
+
+  public byte[] getToken() {
+    // TODO pass in reusable ByteRange
+    return new ByteRange(block, tokenOffset, tokenWidth).deepCopyToNewArray();
+  }
+
+  public int getOffset() {
+    return offset;
+  }
+
+  public int whichFanNode(byte searchForByte) {
+    if( ! hasFan()){
+      throw new IllegalStateException("This row node has no fan, so can't search it");
+    }
+    int fanIndexInBlock = Bytes.unsignedBinarySearch(block, fanOffset, fanOffset + fanOut,
+      searchForByte);
+    if (fanIndexInBlock >= 0) {// found it, but need to adjust for position of fan in overall block
+      return fanIndexInBlock - fanOffset;
+    }
+    return fanIndexInBlock + fanOffset + 1;// didn't find it, so compensate in reverse
+  }
+  
+  public void resetFanIndex() {
+    fanIndex = -1;// just the way the logic currently works
+  }
+
+  public int getFanIndex() {
+    return fanIndex;
+  }
+
+  public void setFanIndex(int fanIndex) {
+    this.fanIndex = fanIndex;
+  }
+
+  public boolean hasFan(){
+    return fanOut > 0;
+  }
+
+  public boolean hasPreviousFanNodes() {
+    return fanOut > 0 && fanIndex > 0;
+  }
+
+  public boolean hasMoreFanNodes() {
+    return fanIndex < getLastFanIndex();
+  }
+
+  public boolean isOnLastFanNode() {
+    return !hasMoreFanNodes();
+  }
+
+
+  /*************** standard methods **************************/
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append("fan:" + Bytes.toStringBinary(block, fanOffset, fanOut));
+    sb.append(",token:" + Bytes.toStringBinary(block, tokenOffset, tokenWidth));
+    sb.append(",numCells:" + numCells);
+    sb.append("fanIndex:"+fanIndex);
+    if(fanIndex>=0){
+      sb.append("("+getFanByteReadable(fanIndex)+")");
+    }
+    return sb.toString();
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/MemstoreTimestampDecoder.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/MemstoreTimestampDecoder.java
new file mode 100644
index 0000000..47111be
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/MemstoreTimestampDecoder.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode.timestamp;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.util.vint.UFIntTool;
+
+public class MemstoreTimestampDecoder {
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected byte[] block;
+
+
+  /************** construct ***********************/
+
+  public MemstoreTimestampDecoder() {
+  }
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block) {
+    this.block = block;
+    this.blockMeta = blockMeta;
+  }
+
+
+  /************** methods *************************/
+
+  public long getMemstoreTimestamp(int index) {
+    if (blockMeta.getMemstoreTimestampIndexWidth() == 0) {
+      return blockMeta.getMinMemstoreTimestamp();
+    }
+    int startIndex = blockMeta.getAbsoluteMemstoreTimestampOffset()
+        + blockMeta.getMemstoreTimestampDeltaWidth() * index;
+    long delta = UFIntTool.fromBytes(block, startIndex, blockMeta.getMemstoreTimestampDeltaWidth());
+    return blockMeta.getMinMemstoreTimestamp() + delta;
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/TimestampDecoder.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/TimestampDecoder.java
new file mode 100644
index 0000000..59ed0ae
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/decode/timestamp/TimestampDecoder.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.decode.timestamp;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.util.vint.UFIntTool;
+
+public class TimestampDecoder {
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected byte[] block;
+
+
+  /************** construct ***********************/
+
+  public TimestampDecoder() {
+  }
+
+  public void initOnBlock(PrefixTreeBlockMeta blockMeta, byte[] block) {
+    this.block = block;
+    this.blockMeta = blockMeta;
+  }
+
+
+  /************** methods *************************/
+
+  public long getTimestamp(int index) {
+    if (blockMeta.getTimestampIndexWidth() == 0) {
+      return blockMeta.getMinTimestamp();
+    }
+    int startIndex = blockMeta.getAbsoluteTimestampOffset() + blockMeta.getTimestampDeltaWidth()
+        * index;
+    long delta = UFIntTool.fromBytes(block, startIndex, blockMeta.getTimestampDeltaWidth());
+    return blockMeta.getMinTimestamp() + delta;
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/PrefixTreeEncoder.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/PrefixTreeEncoder.java
new file mode 100644
index 0000000..3c4a52d
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/PrefixTreeEncoder.java
@@ -0,0 +1,464 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode;
+
+import java.io.OutputStream;
+
+import org.apache.commons.lang.NotImplementedException;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.CellUtils;
+import org.apache.hadoop.hbase.cell.stream.CellOutputStream;
+import org.apache.hadoop.hbase.util.ArrayUtils;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.encode.celltype.CellTypeEncoder;
+import org.apache.hbase.codec.prefixtree.encode.column.ColumnWriter;
+import org.apache.hbase.codec.prefixtree.encode.row.RowWriter;
+import org.apache.hbase.codec.prefixtree.encode.timestamp.TimestampEncoder;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.apache.hbase.util.byterange.ByteRangeSet;
+import org.apache.hbase.util.byterange.impl.ByteRangeHashSet;
+import org.apache.hbase.util.byterange.impl.ByteRangeTreeSet;
+import org.apache.hbase.util.io.OutputStreamUtils;
+import org.apache.hbase.util.vint.UFIntTool;
+
+/**
+ * This is the primary class for converting a CellOutputStream into an encoded byte[]. As Cells are
+ * added they are completely copied into the various encoding structures. This is important because
+ * usually the cells being fed in during compactions will be transient.<br/>
+ * <br/>
+ * Usage:<br/>
+ * 1) constructor<br/>
+ * 2) kvBuilder.setOs(os);<br/>
+ * 3) kvBuilder.setIncludeMemstoreTimestamp(false);<br/>
+ * 4) append cells in sorted order: append(Cell cell)<br/>
+ * 5) flush()<br/>
+ */
+public class PrefixTreeEncoder implements CellOutputStream {
+
+  /**************** static ************************/
+
+  protected static final Log LOG = LogFactory.getLog(PrefixTreeEncoder.class);
+
+  //future-proof where HBase supports multiple families in a data block.
+  public static final boolean MULITPLE_FAMILIES_POSSIBLE = true;
+
+  public static final boolean USE_HASH_COLUMN_SORTER = true;
+  public static final int EXPECTED_CELLS_PLUS_EXTRA = 2000;
+
+  public static final int FAMILY_BYTES_INIT_SIZE = 1 << 14;
+  public static final int QUALIFIER_BYTES_INIT_SIZE = 1 << 14;
+  public static final int VALUE_BUFFER_INIT_SIZE = 1 << 16;
+
+
+  /**************** fields *************************/
+
+  protected long numResets = 0L;
+
+  protected OutputStream outputStream;
+
+  protected boolean includeMemstoreTimestamp;
+
+  protected ByteRange rowRange;
+  protected ByteRange familyRange;
+  protected ByteRange qualifierRange;
+
+  protected long[] timestamps;
+  protected long[] memstoreTimestamps;
+  protected byte[] typeBytes;
+  protected int[] dataOffsets;
+
+  protected byte[] values;
+
+  protected PrefixTreeBlockMeta blockMeta;
+
+  protected ByteRangeSet familySorter;
+  protected ByteRangeSet qualifierSorter;
+  protected TimestampEncoder timestampEncoder;
+  protected TimestampEncoder memstoreTimestampEncoder;
+  protected CellTypeEncoder keyValueTypeBuilder;
+
+  protected Tokenizer rowTokenizer;
+  protected Tokenizer familyTokenizer;
+  protected Tokenizer qualifierTokenizer;
+  protected RowWriter rowWriter;
+  protected ColumnWriter familyWriter;
+  protected ColumnWriter qualifierWriter;
+
+  protected int totalCells = 0;
+  protected int totalUncompressedBytes = 0;
+  protected int totalValueBytes = 0;
+  protected int maxValueLength = 0;
+  protected int timestampBytes = 0;
+  protected int memstoreTimestampBytes = 0;
+  protected int totalBytes = 0;
+
+
+  /***************** construct ***********************/
+  /*
+   * after calling the constructor, you must call these two setters:<br/>
+   * kvBuilder.setOutputStream(outputStream);<br/>
+   * kvBuilder.setIncludeMemstoreTimestamp(true/false);
+   */
+  public PrefixTreeEncoder() {
+    // used during cell accumulation
+    this.blockMeta = new PrefixTreeBlockMeta();
+    this.rowRange = new ByteRange();
+    this.familyRange = new ByteRange();
+    this.qualifierRange = new ByteRange();
+    this.timestamps = new long[EXPECTED_CELLS_PLUS_EXTRA];
+    this.memstoreTimestamps = new long[EXPECTED_CELLS_PLUS_EXTRA];
+    this.typeBytes = new byte[EXPECTED_CELLS_PLUS_EXTRA];
+    this.dataOffsets = new int[EXPECTED_CELLS_PLUS_EXTRA];
+    this.values = new byte[VALUE_BUFFER_INIT_SIZE];
+
+    // used during compilation
+    this.familySorter = USE_HASH_COLUMN_SORTER ? new ByteRangeHashSet() : new ByteRangeTreeSet();
+    this.qualifierSorter = USE_HASH_COLUMN_SORTER ? new ByteRangeHashSet() : new ByteRangeTreeSet();
+    this.timestampEncoder = new TimestampEncoder(blockMeta, false);
+    this.memstoreTimestampEncoder = new TimestampEncoder(blockMeta, true);
+    this.keyValueTypeBuilder = new CellTypeEncoder();
+    this.rowTokenizer = new Tokenizer();
+    this.familyTokenizer = new Tokenizer();
+    this.qualifierTokenizer = new Tokenizer();
+    this.rowWriter = new RowWriter();
+    this.familyWriter = new ColumnWriter();
+    this.qualifierWriter = new ColumnWriter();
+
+    reset();
+  }
+
+  public void reset() {
+    ++numResets;
+    dataOffsets[0] = 0;
+
+    familySorter.reset();
+    qualifierSorter.reset();
+    rowTokenizer.reset();
+    timestampEncoder.reset();
+    memstoreTimestampEncoder.reset();
+    keyValueTypeBuilder.reset();
+    familyTokenizer.reset();
+    qualifierTokenizer.reset();
+    rowWriter.reset();
+    familyWriter.reset();
+    qualifierWriter.reset();
+
+    totalCells = 0;
+    totalUncompressedBytes = 0;
+    totalValueBytes = 0;
+    maxValueLength = 0;
+    timestampBytes = 0;
+    memstoreTimestampBytes = 0;
+    totalBytes = 0;
+  }
+
+  protected void ensurePerCellCapacities() {
+    int currentCapacity = dataOffsets.length;
+    int neededCapacity = totalCells + 2;// some things write one index ahead. +2 to be safe
+    if (neededCapacity < currentCapacity) {
+      return;
+    }
+
+    int padding = neededCapacity;
+    timestamps = ArrayUtils.ensureCapacity(timestamps, neededCapacity, padding);
+    memstoreTimestamps = ArrayUtils.ensureCapacity(memstoreTimestamps, neededCapacity, padding);
+    typeBytes = ArrayUtils.ensureCapacity(typeBytes, neededCapacity, padding);
+    dataOffsets = ArrayUtils.ensureCapacity(dataOffsets, neededCapacity, padding);
+  }
+
+  /******************** CellOutputStream methods *************************/
+
+  @Override
+  public void writeWithRepeatRowFamilyQualifier(Cell cell) {
+    throw new NotImplementedException();
+  }
+
+
+  @Override
+  public void writeWithRepeatRowFamily(Cell cell) {
+    throw new NotImplementedException();
+  }
+
+
+  /*
+   * WARNING: not used or tested yet.  optimization on the write(Cell cell) case
+   */
+  @Override
+  public void writeWithRepeatRow(Cell cell) {
+    ensurePerCellCapacities();//can we optimize away some of this?
+
+    //increment the previous row count
+    if(rowTokenizer.getNumAdded() == 0){
+      write(cell);//couldn't optimize because we don't know the previous cell
+      return;
+    }
+
+    rowTokenizer.incrementNumOccurrencesOfLatestValue();
+    addFamilyPart(cell);
+    addQualifierPart(cell);
+    addAfterRowFamilyQualifier(cell);
+  }
+
+
+  /**
+   * The commented out System.nanoTime() calls add ~100% overhead to the encoder. They are still
+   * needed for continuing performance testing.
+   */
+  @Override
+	public void write(Cell cell) {
+    ensurePerCellCapacities();
+    
+    // rows
+    rowTokenizer.addSorted(CellUtils.fillRowRange(cell, rowRange));
+    // family
+    addFamilyPart(cell);
+    // qualifier
+    addQualifierPart(cell);
+
+    addAfterRowFamilyQualifier(cell);
+	}
+
+
+  /***************** internal add methods ************************/
+
+  protected void addAfterRowFamilyQualifier(Cell cell){
+    // timestamps
+    timestamps[totalCells] = cell.getTimestamp();
+    timestampEncoder.add(cell.getTimestamp());
+
+    // memstore timestamps
+    if (includeMemstoreTimestamp) {
+      memstoreTimestamps[totalCells] = cell.getMemstoreTS();
+      memstoreTimestampEncoder.add(cell.getMemstoreTS());
+      totalUncompressedBytes += WritableUtils.getVIntSize(cell.getMemstoreTS());
+    }
+
+    // types
+    typeBytes[totalCells] = cell.getTypeByte();
+    keyValueTypeBuilder.add(cell.getTypeByte());
+
+    // values
+    totalValueBytes += cell.getValueLength();
+    // double the array each time we run out of space
+    values = ArrayUtils.ensureCapacity(values, totalValueBytes, 2 * totalValueBytes);
+    CellUtils.copyValueTo(cell, values, dataOffsets[totalCells]);
+    if (cell.getValueLength() > maxValueLength) {
+      maxValueLength = cell.getValueLength();
+    }
+    dataOffsets[totalCells + 1] = totalValueBytes;
+
+    // general
+    totalUncompressedBytes += KeyValueUtils.length(cell);
+    ++totalCells;
+  }
+
+  protected void addFamilyPart(Cell cell) {
+    if (MULITPLE_FAMILIES_POSSIBLE || totalCells == 0) {
+      CellUtils.fillFamilyRange(cell, familyRange);
+      familySorter.add(familyRange);
+    }
+  }
+
+  protected void addQualifierPart(Cell cell) {
+    CellUtils.fillQualifierRange(cell, qualifierRange);
+    qualifierSorter.add(qualifierRange);
+  }
+
+
+  /****************** compiling/flushing ********************/
+
+  /**
+   * Expensive method.  The second half of the encoding work happens here.
+   * 
+   * Take all the separate accumulated data structures and turn them into a single stream of bytes
+   * which is written to the outputStream.
+   */
+  @Override
+  public void flush() {
+    blockMeta.setNumKeyValueBytes(totalUncompressedBytes);
+    int lastValueOffset = dataOffsets[totalCells];
+    blockMeta.setDataOffsetWidth(UFIntTool.numBytes(lastValueOffset));
+    blockMeta.setDataLengthWidth(UFIntTool.numBytes(maxValueLength));
+    blockMeta.setNumDataBytes(totalValueBytes);
+    totalBytes += totalValueBytes;
+
+    compileTypes();
+    byte[] timestampBytes = compileTimestamps();
+    byte[] memstoreTimestampBytes = compileMemstoreTimestamps();
+    compileQualifiers();
+    compileFamilies();
+    compileRows();
+
+    int numMetaBytes = blockMeta.calculateNumMetaBytes();
+    blockMeta.setNumMetaBytes(numMetaBytes);
+    totalBytes += numMetaBytes;
+
+    // do the actual flushing to the output stream (order: meta, row, col, ts, data)
+    blockMeta.writeVariableBytesToOutputStream(outputStream);
+    rowWriter.writeBytes(outputStream);
+    familyWriter.writeBytes(outputStream);
+    qualifierWriter.writeBytes(outputStream);
+    OutputStreamUtils.write(outputStream, timestampBytes, 0, timestampBytes.length);
+    if (includeMemstoreTimestamp) {
+      OutputStreamUtils.write(outputStream, memstoreTimestampBytes, 0,
+        memstoreTimestampBytes.length);
+    }
+    OutputStreamUtils.write(outputStream, values, 0, totalValueBytes);
+  }
+
+
+  protected void compileTypes() {
+    if (keyValueTypeBuilder.areAllSameType()) {
+      blockMeta.setAllSameType(true);
+      blockMeta.setAllTypes(keyValueTypeBuilder.getOnlyType());
+    } else {
+      blockMeta.setAllSameType(false);
+    }
+  }
+
+  protected byte[] compileMemstoreTimestamps() {
+    byte[] memstoreTimestampBytes = null;
+    if (includeMemstoreTimestamp) {
+      memstoreTimestampEncoder.compile();
+      memstoreTimestampBytes = memstoreTimestampEncoder.getOutputArray();
+      int numMemstoreTimestampBytes = memstoreTimestampEncoder.getOutputArrayLength();
+      totalBytes += numMemstoreTimestampBytes;
+    }
+    return memstoreTimestampBytes;
+  }
+
+  protected byte[] compileTimestamps() {
+    timestampEncoder.compile();
+    byte[] timestampBytes = timestampEncoder.getOutputArray();
+    int numTimestampBytes = timestampEncoder.getOutputArrayLength();
+    totalBytes += numTimestampBytes;
+    return timestampBytes;
+  }
+
+  protected void compileQualifiers() {
+    blockMeta.setNumUniqueQualifiers(qualifierSorter.size());
+    qualifierSorter.compile();
+    qualifierTokenizer.addAll(qualifierSorter.getSortedRanges());
+    qualifierWriter.reconstruct(blockMeta, qualifierTokenizer, false);
+    qualifierWriter.compile();
+    int numQualifierBytes = qualifierWriter.getNumBytes();
+    blockMeta.setNumQualifierBytes(numQualifierBytes);
+    totalBytes += numQualifierBytes;
+  }
+
+  protected void compileFamilies() {
+    blockMeta.setNumUniqueFamilies(familySorter.size());
+    familySorter.compile();
+    familyTokenizer.addAll(familySorter.getSortedRanges());
+    familyWriter.reconstruct(blockMeta, familyTokenizer, true);
+    familyWriter.compile();
+    int numFamilyBytes = familyWriter.getNumBytes();
+    blockMeta.setNumFamilyBytes(numFamilyBytes);
+    totalBytes += numFamilyBytes;
+  }
+
+  protected void compileRows() {
+    rowWriter.reconstruct(this);
+    rowWriter.compile();
+    int numRowBytes = rowWriter.getNumBytes();
+    blockMeta.setNumRowBytes(numRowBytes);
+    blockMeta.setRowTreeDepth(rowTokenizer.getTreeDepth());
+    totalBytes += numRowBytes;
+  }
+
+  public long getValueOffset(int index) {
+    return dataOffsets[index];
+  }
+
+  public int getValueLength(int index) {
+    return (int) (dataOffsets[index + 1] - dataOffsets[index]);
+  }
+
+  public void setOutputStream(OutputStream outputStream) {
+    this.outputStream = outputStream;
+  }
+
+  /************************* get/set *************************************/
+
+  public PrefixTreeBlockMeta getBlockMeta() {
+    return blockMeta;
+  }
+
+  public Tokenizer getRowTokenizer() {
+    return rowTokenizer;
+  }
+
+  public TimestampEncoder getTimestampEncoder() {
+    return timestampEncoder;
+  }
+
+  public int getTotalBytes() {
+    return totalBytes;
+  }
+
+  public long[] getTimestamps() {
+    return timestamps;
+  }
+
+  public long[] getMemstoreTimestamps() {
+    return memstoreTimestamps;
+  }
+
+  public byte[] getTypeBytes() {
+    return typeBytes;
+  }
+
+  public TimestampEncoder getMemstoreTimestampCompressor() {
+    return memstoreTimestampEncoder;
+  }
+
+  public ByteRangeSet getFamilySorter() {
+    return familySorter;
+  }
+
+  public ByteRangeSet getQualifierSorter() {
+    return qualifierSorter;
+  }
+
+  public void setIncludeMemstoreTimestamp(boolean includeMemstoreTimestamp) {
+    this.includeMemstoreTimestamp = includeMemstoreTimestamp;
+  }
+
+  public ColumnWriter getFamilyWriter() {
+    return familyWriter;
+  }
+
+  public ColumnWriter getQualifierWriter() {
+    return qualifierWriter;
+  }
+
+  public RowWriter getRowWriter() {
+    return rowWriter;
+  }
+
+  public ByteRange getValueByteRange() {
+    return new ByteRange(values, 0, totalValueBytes);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/celltype/CellTypeEncoder.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/celltype/CellTypeEncoder.java
new file mode 100644
index 0000000..b4d0313
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/celltype/CellTypeEncoder.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.celltype;
+
+/**
+ * Detect if every KV has the same KeyValue.Type, in which case we don't need to store it for each
+ * KV.  If(allSameType) during conversion to byte[], then we can store the "onlyType" in blockMeta,
+ * therefore not repeating it for each cell and saving 1 byte per cell.
+ */
+public class CellTypeEncoder {
+
+  /************* fields *********************/
+
+  protected boolean pendingFirstType = true;
+  protected boolean allSameType = true;
+  protected byte onlyType;
+
+
+  /************* construct *********************/
+
+  public void reset() {
+    pendingFirstType = true;
+    allSameType = true;
+  }
+
+
+  /************* methods *************************/
+
+  public void add(byte type) {
+    if (pendingFirstType) {
+      onlyType = type;
+      pendingFirstType = false;
+    } else if (onlyType != type) {
+      allSameType = false;
+    }
+  }
+
+
+  /**************** get/set **************************/
+
+  public boolean areAllSameType() {
+    return allSameType;
+  }
+
+  public byte getOnlyType() {
+    return onlyType;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnNodeWriter.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnNodeWriter.java
new file mode 100644
index 0000000..1effba0
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnNodeWriter.java
@@ -0,0 +1,138 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.column;
+
+import java.io.OutputStream;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.StringUtils;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.util.io.OutputStreamUtils;
+import org.apache.hbase.util.vint.UFIntTool;
+import org.apache.hbase.util.vint.UVIntTool;
+
+public class ColumnNodeWriter{
+
+  /************* fields ****************************/
+
+  protected TokenizerNode builderNode;
+  protected PrefixTreeBlockMeta blockMeta;
+
+  protected boolean familyVsQualifier;
+
+  protected int tokenLength;
+  protected byte[] token;
+  protected int parentStartPosition;
+
+
+  /*************** construct **************************/
+
+  public ColumnNodeWriter(PrefixTreeBlockMeta blockMeta, TokenizerNode builderNode,
+      boolean familyVsQualifier) {
+    this.blockMeta = blockMeta;
+    this.builderNode = builderNode;
+    this.familyVsQualifier = familyVsQualifier;
+    calculateOffsetsAndLengths();
+  }
+
+
+  /************* methods *******************************/
+
+  public boolean isRoot() {
+    return parentStartPosition == 0;
+  }
+
+  protected void calculateOffsetsAndLengths() {
+    tokenLength = builderNode.getTokenLength();
+    token = new byte[tokenLength];
+  }
+
+  /**
+   * This method is called before blockMeta.qualifierOffsetWidth is known, so we pass in a
+   * placeholder.
+   * @param offsetWidthPlaceholder the placeholder
+   * @return
+   */
+  public int getWidthUsingPlaceholderForOffsetWidth(int offsetWidthPlaceholder) {
+    int width = 0;
+    width += UVIntTool.numBytes(tokenLength);
+    width += token.length;
+    width += offsetWidthPlaceholder;
+    return width;
+  }
+
+  public byte[] getBytes() {
+    int parentOffsetWidth;
+    if (familyVsQualifier) {
+      parentOffsetWidth = blockMeta.getFamilyOffsetWidth();
+    } else {
+      parentOffsetWidth = blockMeta.getQualifierOffsetWidth();
+    }
+    int length = UVIntTool.numBytes(tokenLength) + tokenLength + parentOffsetWidth;
+    byte[] bytes = new byte[length];
+    byte[] tokenLengthBytes = UVIntTool.getBytes(tokenLength);
+    System.arraycopy(tokenLengthBytes, 0, bytes, 0, tokenLengthBytes.length);
+    System.arraycopy(token, 0, bytes, tokenLengthBytes.length, token.length);
+    byte[] parentStartPositionBytes = UFIntTool.getBytes(parentOffsetWidth, parentStartPosition);
+    int bytesStartIndex = bytes.length - parentStartPositionBytes.length;
+    System.arraycopy(parentStartPositionBytes, 0, bytes, bytesStartIndex,
+      parentStartPositionBytes.length);
+    return bytes;
+  }
+
+  public void writeBytes(OutputStream os) {
+    int parentOffsetWidth;
+    if (familyVsQualifier) {
+      parentOffsetWidth = blockMeta.getFamilyOffsetWidth();
+    } else {
+      parentOffsetWidth = blockMeta.getQualifierOffsetWidth();
+    }
+    UVIntTool.writeBytes(tokenLength, os);
+    OutputStreamUtils.write(os, token);
+    UFIntTool.writeBytes(parentOffsetWidth, parentStartPosition, os);
+  }
+
+  public void setTokenBytes(ByteRange source) {
+    source.deepCopySubRangeTo(0, tokenLength, token, 0);
+  }
+
+
+  /****************** standard methods ************************/
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append(StringUtils.padFront(builderNode.getOutputArrayOffset() + "", ' ', 3) + ",");
+    sb.append("[");
+    sb.append(Bytes.toString(token));
+    sb.append("]->");
+    sb.append(parentStartPosition);
+    return sb.toString();
+  }
+
+
+  /************************** get/set ***********************/
+
+  public void setParentStartPosition(int parentStartPosition) {
+    this.parentStartPosition = parentStartPosition;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnWriter.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnWriter.java
new file mode 100644
index 0000000..421bcbb
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/column/ColumnWriter.java
@@ -0,0 +1,198 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.column;
+
+import java.io.OutputStream;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.util.vint.UFIntTool;
+
+import com.google.common.collect.Lists;
+
+public class ColumnWriter {
+
+  public static final int EXPECTED_NUBS_PLUS_LEAVES = 100;
+
+  /****************** fields ****************************/
+
+  protected Tokenizer builder;
+  protected PrefixTreeBlockMeta blockMeta;
+  protected boolean familyVsQualifier;
+  protected int numBytes = 0;
+  protected int numAppended = 0;// for tests
+  protected ArrayList<TokenizerNode> nonLeaves;
+  protected ArrayList<TokenizerNode> leaves;
+  protected int numNonLeaves = 0;
+  protected int numLeaves = 0;
+  protected ArrayList<TokenizerNode> allNodes;
+  protected ArrayList<ColumnNodeWriter> columnNodeWriters;
+  protected List<Integer> outputArrayOffsets;
+
+
+	/*********************** construct *********************/
+
+  public ColumnWriter() {
+    this.nonLeaves = Lists.newArrayList();
+    this.leaves = Lists.newArrayList();
+    this.outputArrayOffsets = Lists.newArrayList();
+  }
+
+  public ColumnWriter(PrefixTreeBlockMeta blockMeta, Tokenizer builder, boolean familyVsQualifier) {
+    this();// init collections
+    reconstruct(blockMeta, builder, familyVsQualifier);
+  }
+
+  public void reconstruct(PrefixTreeBlockMeta blockMeta, Tokenizer builder, boolean familyVsQualifier) {
+    this.blockMeta = blockMeta;
+    this.builder = builder;
+    this.familyVsQualifier = familyVsQualifier;
+  }
+
+  public void reset() {
+    numBytes = 0;
+    numAppended = 0;
+    nonLeaves.clear();
+    leaves.clear();
+    outputArrayOffsets.clear();
+    numNonLeaves = 0;
+    numLeaves = 0;
+  }
+
+
+	/****************** methods *******************************/
+
+  public ColumnWriter compile() {
+    if (familyVsQualifier) {
+      // do nothing. max family length fixed at Byte.MAX_VALUE
+    } else {
+      blockMeta.setMaxQualifierLength(builder.getMaxElementLength());
+    }
+
+    builder.setNodeFirstInsertionIndexes();
+
+    builder.appendNodes(nonLeaves, true, false);
+    numNonLeaves = nonLeaves.size();
+    Tokenizer.setNodeSortedIndexes(nonLeaves, 0);
+
+    builder.appendNodes(leaves, false, true);
+    numLeaves = leaves.size();
+    Tokenizer.setNodeSortedIndexes(nonLeaves, nonLeaves.size());
+
+    allNodes = Lists.newArrayListWithCapacity(nonLeaves.size() + leaves.size());
+    allNodes.addAll(nonLeaves);
+    allNodes.addAll(leaves);
+
+    columnNodeWriters = Lists.newArrayListWithCapacity(CollectionUtils.size(allNodes));
+    for (int i = 0; i < allNodes.size(); ++i) {
+      TokenizerNode node = allNodes.get(i);
+      columnNodeWriters.add(new ColumnNodeWriter(blockMeta, node, familyVsQualifier));
+    }
+
+    // leaf widths are known at this point, so add them up
+    int totalBytesWithoutOffsets = 0;
+    for (int i = allNodes.size() - 1; i >= 0; --i) {
+      ColumnNodeWriter columnNodeWriter = columnNodeWriters.get(i);
+      // leaves store all but their first token byte
+      totalBytesWithoutOffsets += columnNodeWriter.getWidthUsingPlaceholderForOffsetWidth(0);
+    }
+
+    // figure out how wide our offset FInts are
+    int parentOffsetWidth = 0;
+    while (true) {
+      ++parentOffsetWidth;
+      int numBytesFinder = totalBytesWithoutOffsets + parentOffsetWidth * allNodes.size();
+      if (numBytesFinder < UFIntTool.maxValueForNumBytes(parentOffsetWidth)) {
+        numBytes = numBytesFinder;
+        break;
+      }// it fits
+    }
+    if (familyVsQualifier) {
+      blockMeta.setFamilyOffsetWidth(parentOffsetWidth);
+    } else {
+      blockMeta.setQualifierOffsetWidth(parentOffsetWidth);
+    }
+
+    int forwardIndex = 0;
+    for (int i = 0; i < allNodes.size(); ++i) {
+      TokenizerNode node = allNodes.get(i);
+      ColumnNodeWriter columnNodeWriter = columnNodeWriters.get(i);
+      int fullNodeWidth = columnNodeWriter
+          .getWidthUsingPlaceholderForOffsetWidth(parentOffsetWidth);
+      node.setOutputArrayOffset(forwardIndex);
+      columnNodeWriter.setTokenBytes(node.getToken());
+      if (node.isRoot()) {
+        columnNodeWriter.setParentStartPosition(0);
+      } else {
+        columnNodeWriter.setParentStartPosition(node.getParent().getOutputArrayOffset());
+      }
+      forwardIndex += fullNodeWidth;
+    }
+
+    builder.appendOutputArrayOffsets(outputArrayOffsets);
+
+    return this;
+  }
+
+
+  public byte[] writeBytes() {
+    byte[] bytes = new byte[numBytes];
+    int destinationArrayIndex = 0;
+    for (ColumnNodeWriter columnNodeWriter : columnNodeWriters) {
+      byte[] nodeBytes = columnNodeWriter.getBytes();
+      System.arraycopy(nodeBytes, 0, bytes, destinationArrayIndex, nodeBytes.length);
+      destinationArrayIndex += nodeBytes.length;
+    }
+    return bytes;
+  }
+
+  public void writeBytes(OutputStream os) {
+    for (ColumnNodeWriter columnNodeWriter : columnNodeWriters) {
+      columnNodeWriter.writeBytes(os);
+    }
+  }
+
+
+  /************* get/set **************************/
+
+  public ArrayList<ColumnNodeWriter> getColumnNodeWriters() {
+    return columnNodeWriters;
+  }
+
+  public int getNumBytes() {
+    return numBytes;
+  }
+
+  public int getOutputArrayOffset(int sortedIndex) {
+    return outputArrayOffsets.get(sortedIndex);
+  }
+
+  public ArrayList<TokenizerNode> getNonLeaves() {
+    return nonLeaves;
+  }
+
+  public ArrayList<TokenizerNode> getLeaves() {
+    return leaves;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowNodeWriter.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowNodeWriter.java
new file mode 100644
index 0000000..ad15a40
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowNodeWriter.java
@@ -0,0 +1,216 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.row;
+
+import java.io.ByteArrayOutputStream;
+import java.io.OutputStream;
+import java.util.ArrayList;
+
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerRowSearchResult;
+import org.apache.hbase.util.io.OutputStreamUtils;
+import org.apache.hbase.util.vint.UFIntTool;
+import org.apache.hbase.util.vint.UVIntTool;
+
+public class RowNodeWriter{
+
+  /********************* fields ******************************/
+
+  protected PrefixTreeEncoder keyValueBuilder;
+  protected PrefixTreeBlockMeta blockMeta;
+  protected TokenizerRowSearchResult reusablePtBuilderRowSearchResult;
+  protected TokenizerNode node;
+
+  protected int tokenWidth;
+  protected int fanOut;
+  protected int numCells;
+
+  protected int width;
+
+
+  /*********************** construct *************************/
+
+  public RowNodeWriter(PrefixTreeEncoder keyValueBuilder, TokenizerNode node) {
+    reconstruct(keyValueBuilder, node);
+  }
+
+  public void reconstruct(PrefixTreeEncoder keyValueBuilder, TokenizerNode node) {
+    this.keyValueBuilder = keyValueBuilder;
+    this.reusablePtBuilderRowSearchResult = new TokenizerRowSearchResult();
+    reset(node);
+  }
+
+  public void reset(TokenizerNode node) {
+    this.blockMeta = keyValueBuilder.getBlockMeta();// changes between blocks
+    this.node = node;
+    this.tokenWidth = 0;
+    this.fanOut = 0;
+    this.numCells = 0;
+    this.width = 0;
+    calculateOffsetsAndLengths();
+  }
+
+
+  /********************* methods ****************************/
+
+  protected void calculateOffsetsAndLengths(){
+    tokenWidth = node.getTokenLength();
+    if(!node.isRoot()){
+      --tokenWidth;//root has no parent
+    }
+    fanOut = CollectionUtils.size(node.getChildren());
+    numCells = node.getNumOccurrences();
+  }
+
+  public int calculateWidth(){
+    calculateWidthOverrideOffsetWidth(blockMeta.getNextNodeOffsetWidth());
+    return width;
+  }
+
+  public int calculateWidthOverrideOffsetWidth(int offsetWidth){
+    width = 0;
+    width += UVIntTool.numBytes(tokenWidth);
+    width += tokenWidth;
+
+    width += UVIntTool.numBytes(fanOut);
+    width += fanOut;
+
+    width += UVIntTool.numBytes(numCells);
+
+    if(node.hasOccurrences()){
+      int fixedBytesPerCell = blockMeta.getFamilyOffsetWidth()
+        + blockMeta.getQualifierOffsetWidth()
+        + blockMeta.getTimestampIndexWidth()
+        + blockMeta.getMemstoreTimestampIndexWidth()
+        + blockMeta.getKeyValueTypeWidth()
+        + blockMeta.getDataOffsetWidth()
+        + blockMeta.getDataLengthWidth();
+      width += numCells * fixedBytesPerCell;
+    }
+
+    if( ! node.isLeaf()){
+      width += fanOut * offsetWidth;
+    }
+
+    return width;
+  }
+
+
+  public byte[] getNodeBytes(){
+    ByteArrayOutputStream baos = new ByteArrayOutputStream(width);
+    write(baos);
+    return baos.toByteArray();//copies to new array
+  }
+
+  public void write(OutputStream os){
+    // lengths
+    UVIntTool.writeBytes(tokenWidth, os);
+    writeToken(os);
+    UVIntTool.writeBytes(fanOut, os);
+    if (fanOut > 0) {
+      writeFan(os);
+    }
+    UVIntTool.writeBytes(numCells, os);
+
+    if (blockMeta.getFamilyOffsetWidth() > 0) {
+      for (int i = 0; i < node.getNumOccurrences(); ++i) {
+        int keyValueInsertionIndex = PrefixTreeEncoder.MULITPLE_FAMILIES_POSSIBLE ? node
+            .getFirstInsertionIndex() + i : 0;
+        int sortedIndex = keyValueBuilder.getFamilySorter().getSortedIndexForInsertionId(
+          keyValueInsertionIndex);
+        int indexedFamilyOffset = keyValueBuilder.getFamilyWriter().getOutputArrayOffset(
+          sortedIndex);
+        UFIntTool.writeBytes(blockMeta.getFamilyOffsetWidth(), indexedFamilyOffset, os);
+      }
+    }
+
+    if (blockMeta.getQualifierOffsetWidth() > 0) {
+      for (int i = 0; i < node.getNumOccurrences(); ++i) {
+        int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+        int sortedIndex = keyValueBuilder.getQualifierSorter().getSortedIndexForInsertionId(
+          keyValueInsertionIndex);
+        int indexedQualifierOffset = keyValueBuilder.getQualifierWriter().getOutputArrayOffset(
+          sortedIndex);
+        UFIntTool.writeBytes(blockMeta.getQualifierOffsetWidth(), indexedQualifierOffset, os);
+      }
+    }
+
+    if (blockMeta.getTimestampIndexWidth() > 0) {
+      for (int i = 0; i < node.getNumOccurrences(); ++i) {
+        int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+        long timestamp = keyValueBuilder.getTimestamps()[keyValueInsertionIndex];
+        int timestampIndex = keyValueBuilder.getTimestampEncoder().getIndex(timestamp);
+        UFIntTool.writeBytes(blockMeta.getTimestampIndexWidth(), timestampIndex, os);
+      }
+    }
+
+    if (blockMeta.getMemstoreTimestampIndexWidth() > 0) {
+      for (int i = 0; i < node.getNumOccurrences(); ++i) {
+        int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+        long memstoreTimestamp = keyValueBuilder.getMemstoreTimestamps()[keyValueInsertionIndex];
+        int memstoreTimestampIndex = keyValueBuilder.getMemstoreTimestampCompressor().getIndex(
+          memstoreTimestamp);
+        UFIntTool.writeBytes(blockMeta.getMemstoreTimestampIndexWidth(), memstoreTimestampIndex, os);
+      }
+    }
+
+    if (!blockMeta.isAllSameType()) {
+      for (int i = 0; i < node.getNumOccurrences(); ++i) {
+        int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+        OutputStreamUtils.write(os, keyValueBuilder.getTypeBytes()[keyValueInsertionIndex]);
+      }
+    }
+
+    for (int i = 0; i < node.getNumOccurrences(); ++i) {
+      int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+      long dataStartIndex = keyValueBuilder.getValueOffset(keyValueInsertionIndex);
+      UFIntTool.writeBytes(blockMeta.getDataOffsetWidth(), dataStartIndex, os);
+    }
+
+    for (int i = 0; i < node.getNumOccurrences(); ++i) {
+      int keyValueInsertionIndex = node.getFirstInsertionIndex() + i;
+      int dataLength = keyValueBuilder.getValueLength(keyValueInsertionIndex);
+      UFIntTool.writeBytes(blockMeta.getDataLengthWidth(), dataLength, os);
+    }
+
+    ArrayList<TokenizerNode> children = node.getChildren();
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      int distanceToChild = node.getNegativeIndex() - child.getNegativeIndex();
+      UFIntTool.writeBytes(blockMeta.getNextNodeOffsetWidth(), distanceToChild, os);
+    }
+  }
+
+
+  protected void writeToken(OutputStream os) {
+    int tokenStartIndex = node.isRoot() ? 0 : 1;
+    OutputStreamUtils.write(os, node.getToken(), tokenStartIndex);
+  }
+
+  public void writeFan(OutputStream os) {
+    ArrayList<TokenizerNode> children = node.getChildren();
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      OutputStreamUtils.write(os, child.getToken().get(0));// first byte of each child's token
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowWriter.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowWriter.java
new file mode 100644
index 0000000..e348d6e
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/row/RowWriter.java
@@ -0,0 +1,241 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.row;
+
+import java.io.OutputStream;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.util.vint.UFIntTool;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.Lists;
+
+public class RowWriter {
+
+  /***************** fields **************************/
+
+  protected PrefixTreeEncoder keyValueBuilder;
+
+  protected PrefixTreeBlockMeta blockMeta;
+
+  protected int numBytes;
+  protected int numAppended = 0;// for tests
+
+  protected ArrayList<TokenizerNode> nonLeaves;
+  protected ArrayList<TokenizerNode> leaves;
+
+  protected int numNonLeaves = 0;
+  protected int numLeaves = 0;
+
+  protected ArrayList<RowNodeWriter> leafWriters;
+  protected ArrayList<RowNodeWriter> nonLeafWriters;
+
+  protected int numLeafWriters;
+  protected int numNonLeafWriters;
+
+
+  /********************* construct **********************/
+
+  public RowWriter() {
+    this.nonLeaves = Lists.newArrayList();
+    this.leaves = Lists.newArrayList();
+    this.leafWriters = Lists.newArrayList();
+    this.nonLeafWriters = Lists.newArrayList();
+  }
+
+  public RowWriter(PrefixTreeEncoder keyValueBuilder) {
+    reconstruct(keyValueBuilder);
+  }
+
+  public void reconstruct(PrefixTreeEncoder keyValueBuilder) {
+    this.keyValueBuilder = keyValueBuilder;
+    this.blockMeta = keyValueBuilder.getBlockMeta();
+    reset();
+  }
+
+  public void reset() {
+    numBytes = 0;
+    numAppended = 0;
+    nonLeaves.clear();
+    leaves.clear();
+    numNonLeaves = 0;
+    numLeaves = 0;
+    numLeafWriters = 0;
+    numNonLeafWriters = 0;
+  }
+
+
+  /****************** methods *******************************/
+
+  public RowWriter compile() {
+    blockMeta.setMaxRowLength(keyValueBuilder.getRowTokenizer().getMaxElementLength());
+    keyValueBuilder.getRowTokenizer().setNodeFirstInsertionIndexes();
+
+    keyValueBuilder.getRowTokenizer().appendNodes(nonLeaves, true, false);
+    numNonLeaves = nonLeaves.size();
+    Tokenizer.setNodeSortedIndexes(nonLeaves, 0);
+    keyValueBuilder.getRowTokenizer().appendNodes(leaves, false, true);
+    numLeaves = leaves.size();
+    Tokenizer.setNodeSortedIndexes(leaves, nonLeaves.size());
+
+    // track the starting position of each node in final output
+    int negativeIndex = 0;
+
+    // create leaf writer nodes
+    // leaf widths are known at this point, so add them up
+    int totalLeafBytes = 0;
+    for (int i = leaves.size() - 1; i >= 0; --i) {
+      TokenizerNode leaf = leaves.get(i);
+      RowNodeWriter leafWriter = initializeWriter(leafWriters, numLeafWriters, leaf);
+      ++numLeafWriters;
+      // leaves store all but their first token byte
+      int leafNodeWidth = leafWriter.calculateWidthOverrideOffsetWidth(0);
+      totalLeafBytes += leafNodeWidth;
+      negativeIndex += leafNodeWidth;
+      leaf.setNegativeIndex(negativeIndex);
+    }
+
+    int totalNonLeafBytesWithoutOffsets = 0;
+    int totalChildPointers = 0;
+    for (int i = nonLeaves.size() - 1; i >= 0; --i) {
+      TokenizerNode nonLeaf = nonLeaves.get(i);
+      RowNodeWriter nonLeafWriter = initializeWriter(nonLeafWriters, numNonLeafWriters, nonLeaf);
+      ++numNonLeafWriters;
+      totalNonLeafBytesWithoutOffsets += nonLeafWriter.calculateWidthOverrideOffsetWidth(0);
+      totalChildPointers += nonLeaf.getNumChildren();
+    }
+
+    // figure out how wide our offset FInts are
+    int offsetWidth = 0;
+    while (true) {
+      ++offsetWidth;
+      int offsetBytes = totalChildPointers * offsetWidth;
+      int totalRowBytes = totalNonLeafBytesWithoutOffsets + offsetBytes + totalLeafBytes;
+      if (totalRowBytes < UFIntTool.maxValueForNumBytes(offsetWidth)) {
+        // it fits
+        numBytes = totalRowBytes;
+        break;
+      }
+    }
+    blockMeta.setNextNodeOffsetWidth(offsetWidth);
+
+    // populate negativeIndexes
+    for (int i = nonLeaves.size() - 1; i >= 0; --i) {
+      TokenizerNode nonLeaf = nonLeaves.get(i);
+      int writerIndex = numNonLeaves - i - 1;
+      RowNodeWriter nonLeafWriter = nonLeafWriters.get(writerIndex);
+      int nodeWidth = nonLeafWriter.calculateWidth();
+      negativeIndex += nodeWidth;
+      nonLeaf.setNegativeIndex(negativeIndex);
+    }
+
+    return this;
+  }
+
+  protected RowNodeWriter initializeWriter(List<RowNodeWriter> list, int index,
+      TokenizerNode builderNode) {
+    RowNodeWriter rowNodeWriter = null;
+    //check if there is an existing node we can recycle
+    if (index >= list.size()) {
+      //there are not enough existing nodes, so add a new one which will be retrieved below
+      list.add(new RowNodeWriter(keyValueBuilder, builderNode));
+    }
+    rowNodeWriter = list.get(index);
+    rowNodeWriter.reset(builderNode);// TODO debug why reset doesn't work instead of reconstruct
+    return rowNodeWriter;
+  }
+
+
+  /******************** write to byte[], OutputStream, or ByteBuffer **************/
+
+  public byte[] writeBytes() {
+    byte[] bytes = new byte[numBytes];
+    for (int i = numNonLeafWriters - 1; i >= 0; --i) {
+      RowNodeWriter nonLeafWriter = nonLeafWriters.get(i);
+      byte[] nodeBytes = nonLeafWriter.getNodeBytes();
+      System.arraycopy(nodeBytes, 0, bytes, numAppended, nodeBytes.length);
+      numAppended += nodeBytes.length;
+    }
+    // duplicates above... written more for clarity right now
+    for (int i = numLeafWriters - 1; i >= 0; --i) {
+      RowNodeWriter leafWriter = leafWriters.get(i);
+      byte[] nodeBytes = leafWriter.getNodeBytes();
+      System.arraycopy(nodeBytes, 0, bytes, numAppended, nodeBytes.length);
+      numAppended += nodeBytes.length;
+    }
+    Preconditions.checkArgument(numBytes == numAppended);
+    return bytes;
+  }
+
+  public void writeBytes(OutputStream os) {
+    for (int i = numNonLeafWriters - 1; i >= 0; --i) {
+      RowNodeWriter nonLeafWriter = nonLeafWriters.get(i);
+      nonLeafWriter.write(os);
+    }
+    // duplicates above... written more for clarity right now
+    for (int i = numLeafWriters - 1; i >= 0; --i) {
+      RowNodeWriter leafWriter = leafWriters.get(i);
+      leafWriter.write(os);
+    }
+  }
+
+
+  /***************** static ******************************/
+
+  protected static ArrayList<TokenizerNode> filterByLeafAndReverse(
+      ArrayList<TokenizerNode> ins, boolean leaves) {
+    ArrayList<TokenizerNode> outs = Lists.newArrayList();
+    for (int i = ins.size() - 1; i >= 0; --i) {
+      TokenizerNode n = ins.get(i);
+      if (n.isLeaf() && leaves || (!n.isLeaf() && !leaves)) {
+        outs.add(ins.get(i));
+      }
+    }
+    return outs;
+  }
+
+
+  /************* get/set **************************/
+
+  public int getNumBytes() {
+    return numBytes;
+  }
+
+  public ArrayList<TokenizerNode> getNonLeaves() {
+    return nonLeaves;
+  }
+
+  public ArrayList<TokenizerNode> getLeaves() {
+    return leaves;
+  }
+
+  public ArrayList<RowNodeWriter> getNonLeafWriters() {
+    return nonLeafWriters;
+  }
+
+  public ArrayList<RowNodeWriter> getLeafWriters() {
+    return leafWriters;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/timestamp/TimestampEncoder.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/timestamp/TimestampEncoder.java
new file mode 100644
index 0000000..825bc66
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/timestamp/TimestampEncoder.java
@@ -0,0 +1,171 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.timestamp;
+
+import java.util.Arrays;
+import java.util.HashSet;
+
+import org.apache.hadoop.hbase.util.ArrayUtils;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.util.vint.UFIntTool;
+
+/*
+ * 1. add timestamps to a HashSet for fast de-duplication
+ * 2. keep track of the min and max
+ * 3. copy all timestamps to a new long[]
+ * 4. Collections.sort the long[]
+ * 5. calculate maxDelta = max - min
+ * 6. determine FInt width based on maxDelta
+ * 7. KeyValueBuilder binary searches to find timestamp index
+ */
+public class TimestampEncoder {
+
+  /****************** fields ****************************/
+
+  protected PrefixTreeBlockMeta blockMeta;
+  protected boolean memstoreTimestamp;
+
+  protected HashSet<Long> uniqueTimestamps;
+  protected long[] sortedUniqueTimestamps;
+  protected long min, max, maxDelta;
+
+  protected int bytesPerDelta;
+  protected int bytesPerIndex;
+  protected int totalCompressedBytes;
+
+
+  /****************** construct ****************************/
+
+  public TimestampEncoder(PrefixTreeBlockMeta blockMeta, boolean memstoreTimestamp) {
+    this.blockMeta = blockMeta;
+    this.uniqueTimestamps = new HashSet<Long>();
+    this.memstoreTimestamp = memstoreTimestamp;
+  }
+
+  public void reset() {
+    uniqueTimestamps.clear();
+    sortedUniqueTimestamps = null;
+    min = Long.MAX_VALUE;
+    max = Long.MIN_VALUE;
+    maxDelta = Long.MIN_VALUE;
+    bytesPerIndex = 0;
+    bytesPerDelta = 0;
+    totalCompressedBytes = 0;
+  }
+
+
+	/************* methods ***************************/
+
+  public void add(long timestamp) {
+    uniqueTimestamps.add(timestamp);
+  }
+
+  public TimestampEncoder compile() {
+    int numUnique = uniqueTimestamps.size();
+    if (numUnique == 1) {
+      min = CollectionUtils.getFirst(uniqueTimestamps);
+      sortedUniqueTimestamps = new long[] { min };
+      setBlockMetaValues();
+      return this;
+    }
+    sortedUniqueTimestamps = new long[numUnique];
+    int lastIndex = -1;
+    for (long timestamp : uniqueTimestamps) {
+      sortedUniqueTimestamps[++lastIndex] = timestamp;
+    }
+    Arrays.sort(sortedUniqueTimestamps);
+    min = ArrayUtils.getFirst(sortedUniqueTimestamps);
+    max = ArrayUtils.getLast(sortedUniqueTimestamps);
+    maxDelta = max - min;
+    if (maxDelta > 0) {
+      bytesPerDelta = UFIntTool.numBytes(maxDelta);
+    } else {
+      bytesPerDelta = 0;
+    }
+
+    int maxIndex = numUnique - 1;
+    bytesPerIndex = UFIntTool.numBytes(maxIndex);
+
+    totalCompressedBytes = numUnique * bytesPerDelta;
+
+    setBlockMetaValues();
+    return this;
+  }
+
+  protected void setBlockMetaValues() {
+    if (memstoreTimestamp) {
+      blockMeta.setMinMemstoreTimestamp(min);
+      blockMeta.setMemstoreTimestampIndexWidth(bytesPerIndex);
+      blockMeta.setMemstoreTimestampDeltaWidth(bytesPerDelta);
+      blockMeta.setNumMemstoreTimestampBytes(totalCompressedBytes);
+    } else {
+      blockMeta.setMinTimestamp(min);
+      blockMeta.setTimestampIndexWidth(bytesPerIndex);
+      blockMeta.setTimestampDeltaWidth(bytesPerDelta);
+      blockMeta.setNumTimestampBytes(totalCompressedBytes);
+    }
+  }
+
+  public long getDelta(int index) {
+    if (sortedUniqueTimestamps.length == 0) {
+      return 0;
+    }
+    return sortedUniqueTimestamps[index] - min;
+  }
+
+  public int getIndex(long timestamp) {
+    // should always find an exact match
+    return Arrays.binarySearch(sortedUniqueTimestamps, timestamp);
+  }
+
+  public byte[] getOutputArray() {
+    // TODO write directly to the block array
+    // TODO the first one is already saved in blockMeta. decide whether to duplicate
+    byte[] bytes = new byte[getOutputArrayLength()];
+    if (bytes.length == 0) {
+      return bytes;
+    }
+    for (int i = 0; i < sortedUniqueTimestamps.length; ++i) {
+      long delta = sortedUniqueTimestamps[i] - min;
+      UFIntTool.writeBytes(bytesPerDelta, delta, bytes, i * bytesPerDelta);
+    }
+    return bytes;
+  }
+
+  public int getOutputArrayLength() {
+    return sortedUniqueTimestamps.length * bytesPerDelta;
+  }
+
+  public int getNumUniqueTimestamps() {
+    return sortedUniqueTimestamps.length;
+  }
+
+
+	/******************** get/set **************************/
+
+  public long getMin() {
+    return min;
+  }
+
+  public long[] getSortedUniqueTimestamps() {
+    return sortedUniqueTimestamps;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenDepthComparator.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenDepthComparator.java
new file mode 100644
index 0000000..d803605
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenDepthComparator.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.tokenize;
+
+import java.util.Comparator;
+
+/**
+ * Determines order of nodes in the output array.  Maybe possible to optimize further.
+ */
+public class TokenDepthComparator implements Comparator<TokenizerNode> {
+
+  @Override
+  public int compare(TokenizerNode a, TokenizerNode b) {
+    // just throw npe if a or b is null
+
+    // put leaves at the end
+    if (!a.isLeaf() && b.isLeaf()) {
+      return -1;
+    }
+    if (a.isLeaf() && !b.isLeaf()) {
+      return 1;
+    }
+
+    if (a.isLeaf() && b.isLeaf()) {// keep leaves in sorted order (for debugability)
+      return a.getId() < b.getId() ? -1 : 1;
+    }
+
+    // compare depth
+    if (a.getTokenOffset() < b.getTokenOffset()) {
+      return -1;
+    }
+    if (a.getTokenOffset() > b.getTokenOffset()) {
+      return 1;
+    }
+
+    // if same depth, return lower id first. ids are unique
+    return a.getId() < b.getId() ? -1 : 1;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java
new file mode 100644
index 0000000..e255652
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/Tokenizer.java
@@ -0,0 +1,238 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.tokenize;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.ArrayUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+
+import com.google.common.collect.Lists;
+
+/**
+ * Holds the root node and context for a prefix tree tokenizer
+ */
+public class Tokenizer{
+
+  /***************** fields **************************/
+
+  protected int numArraysAdded = 0;
+  protected long lastNodeId = -1;
+  protected ArrayList<TokenizerNode> nodes;
+  protected int numNodes;
+  protected TokenizerNode root;
+  protected byte[] tokens;
+  protected int tokensLength;
+
+  protected int maxElementLength = 0;
+  // number of levels in the tree assuming root level is 0
+  protected int treeDepth = 0;
+
+
+  /******************* construct *******************/
+
+  public Tokenizer() {
+    this.nodes = Lists.newArrayList();
+    this.tokens = new byte[0];
+  }
+
+  public void reset() {
+    numArraysAdded = 0;
+    lastNodeId = -1;
+    numNodes = 0;
+    tokensLength = 0;
+    root = null;
+    maxElementLength = 0;
+    treeDepth = 0;
+  }
+
+
+  /***************** building *************************/
+
+  public void addAll(ArrayList<ByteRange> sortedByteRanges) {
+    for (int i = 0; i < sortedByteRanges.size(); ++i) {
+      ByteRange byteRange = sortedByteRanges.get(i);
+      addSorted(byteRange);
+    }
+  }
+
+  public void addSorted(final ByteRange bytes) {
+    ++numArraysAdded;
+    if (bytes.getLength() > maxElementLength) {
+      maxElementLength = bytes.getLength();
+    }
+    if (root == null) {
+      // nodeDepth of firstNode (non-root) is 1
+      root = addNode(null, 1, 0, bytes, 0);
+    } else {
+      root.addSorted(bytes);
+    }
+  }
+
+  public void incrementNumOccurrencesOfLatestValue(){
+    CollectionUtils.getLast(nodes).incrementNumOccurrences(1);
+  }
+
+  protected long nextNodeId() {
+    return ++lastNodeId;
+  }
+
+  protected TokenizerNode addNode(TokenizerNode parent, int nodeDepth, int tokenStartOffset,
+      final ByteRange token, int inputTokenOffset) {
+    int inputTokenLength = token.getLength() - inputTokenOffset;
+    int tokenOffset = appendTokenAndRepointByteRange(token, inputTokenOffset);
+    TokenizerNode node = null;
+    if (nodes.size() <= numNodes) {
+      node = new TokenizerNode(this, parent, nodeDepth, tokenStartOffset, tokenOffset,
+          inputTokenLength);
+      nodes.add(node);
+    } else {
+      node = nodes.get(numNodes);
+      node.reset();
+      node.reconstruct(this, parent, nodeDepth, tokenStartOffset, tokenOffset, inputTokenLength);
+    }
+    ++numNodes;
+    return node;
+  }
+
+  protected int appendTokenAndRepointByteRange(final ByteRange token, int inputTokenOffset) {
+    int newOffset = tokensLength;
+    int inputTokenLength = token.getLength() - inputTokenOffset;
+    int newMinimum = tokensLength + inputTokenLength;
+    tokens = ArrayUtils.ensureCapacity(tokens, newMinimum, 2 * newMinimum);
+    token.deepCopySubRangeTo(inputTokenOffset, inputTokenLength, tokens, tokensLength);
+    tokensLength += inputTokenLength;
+    return newOffset;
+  }
+
+  protected void submitMaxNodeDepthCandidate(int nodeDepth) {
+    if (nodeDepth > treeDepth) {
+      treeDepth = nodeDepth;
+    }
+  }
+
+
+  /********************* read ********************/
+  
+  public int getNumAdded(){
+    return numArraysAdded;
+  }
+
+  // for debugging
+  public ArrayList<TokenizerNode> getNodes(boolean includeNonLeaves, boolean includeLeaves) {
+    ArrayList<TokenizerNode> nodes = Lists.newArrayList();
+    root.appendNodesToExternalList(nodes, includeNonLeaves, includeLeaves);
+    return nodes;
+  }
+
+  public void appendNodes(List<TokenizerNode> appendTo, boolean includeNonLeaves,
+      boolean includeLeaves) {
+    root.appendNodesToExternalList(appendTo, includeNonLeaves, includeLeaves);
+  }
+
+  public List<byte[]> getArrays() {
+    List<TokenizerNode> nodes = new ArrayList<TokenizerNode>();
+    root.appendNodesToExternalList(nodes, true, true);
+    List<byte[]> byteArrays = Lists.newArrayListWithCapacity(CollectionUtils.size(nodes));
+    for (int i = 0; i < nodes.size(); ++i) {
+      TokenizerNode node = nodes.get(i);
+      for (int j = 0; j < node.getNumOccurrences(); ++j) {
+        byte[] byteArray = node.getNewByteArray();
+        byteArrays.add(byteArray);
+      }
+    }
+    return byteArrays;
+  }
+
+  public void getNode(TokenizerRowSearchResult resultHolder, byte[] key, int keyOffset,
+      int keyLength) {
+    root.getNode(resultHolder, key, keyOffset, keyLength);
+  }
+
+
+	/********************** write ***************************/
+
+  public Tokenizer setNodeFirstInsertionIndexes() {
+    root.setInsertionIndexes(0);
+    return this;
+  }
+
+  public static void setNodeSortedIndexes(List<TokenizerNode> nodes, int startIndex) {
+    int sortedIndex = startIndex;
+    for (int i = 0; i < nodes.size(); ++i) {
+      TokenizerNode node = nodes.get(i);
+      node.setNodeSortedIndex(sortedIndex);
+      ++sortedIndex;
+    }
+  }
+
+  public Tokenizer appendOutputArrayOffsets(List<Integer> offsets) {
+    root.appendOutputArrayOffsets(offsets);
+    return this;
+  }
+
+
+  /********************* print/debug ********************/
+
+  protected static final Boolean INCLUDE_FULL_TREE_IN_TO_STRING = false;
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append(getStructuralString());
+    if (INCLUDE_FULL_TREE_IN_TO_STRING) {
+      for (byte[] bytes : getArrays()) {
+        if (sb.length() > 0) {
+          sb.append("\n");
+        }
+        sb.append(Bytes.toString(bytes));
+      }
+    }
+    return sb.toString();
+  }
+
+  public String getStructuralString() {
+    List<TokenizerNode> nodes = getNodes(true, true);
+    StringBuilder sb = new StringBuilder();
+    for (TokenizerNode node : nodes) {
+      String line = node.getPaddedTokenAndOccurrenceString();
+      sb.append(line + "\n");
+    }
+    return sb.toString();
+  }
+
+
+  /****************** get/set ************************/
+
+  public TokenizerNode getRoot() {
+    return root;
+  }
+
+  public int getMaxElementLength() {
+    return maxElementLength;
+  }
+
+  public int getTreeDepth() {
+    return treeDepth;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerNode.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerNode.java
new file mode 100644
index 0000000..6606940
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerNode.java
@@ -0,0 +1,529 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.tokenize;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.StringUtils;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+
+import com.google.common.collect.Lists;
+
+/*
+ * tree builder node.  used for building and writing.  arrays must be added in sorted order.
+ */
+public class TokenizerNode{
+
+//  protected static long NUM = 0;
+
+  /*********************** fields *********************************/
+
+  // ref to parent
+  protected Tokenizer builder;
+
+  // tree content/structure. used during building
+  protected TokenizerNode parent;
+  protected int nodeDepth;
+  protected int tokenStartOffset;
+  protected ByteRange token;
+  protected int numOccurrences;
+  protected ArrayList<TokenizerNode> children;// branches & nubs
+
+  // ids/offsets. used during writing to byte[]
+  protected long id;
+  protected int firstInsertionIndex = -1;// set >=0 for nubs and leaves
+  protected int nodeSortedIndex = -1;
+  protected int negativeIndex = 0;
+  protected int outputArrayOffset = -1;
+
+
+  /*********************** construct *****************************/
+
+  public TokenizerNode(Tokenizer builder, TokenizerNode parent, int nodeDepth,
+      int tokenStartOffset, int tokenOffset, int tokenLength) {
+    this.token = new ByteRange();
+    reconstruct(builder, parent, nodeDepth, tokenStartOffset, tokenOffset, tokenLength);
+    this.children = Lists.newArrayList();
+  }
+
+  public void reconstruct(Tokenizer builder, TokenizerNode parent, int nodeDepth,
+      int tokenStartOffset, int tokenOffset, int tokenLength) {
+    this.builder = builder;
+    this.id = builder.nextNodeId();
+    this.parent = parent;
+    this.nodeDepth = nodeDepth;
+    builder.submitMaxNodeDepthCandidate(nodeDepth);
+    this.tokenStartOffset = tokenStartOffset;
+    this.token.set(builder.tokens, tokenOffset, tokenLength);
+    this.numOccurrences = 1;
+  }
+
+  public void reset() {
+    builder = null;
+    parent = null;
+    nodeDepth = 0;
+    tokenStartOffset = 0;
+    token.clear();
+    numOccurrences = 0;
+    children.clear();// branches & nubs
+
+    // ids/offsets. used during writing to byte[]
+    id = 0;
+    firstInsertionIndex = -1;// set >=0 for nubs and leaves
+    nodeSortedIndex = -1;
+    negativeIndex = 0;
+    outputArrayOffset = -1;
+  }
+
+
+  /************************* building *********************************/
+
+  // requires that byte arrays be provided in sorted order
+  public void addSorted(final ByteRange bytes) {// recursively build the tree
+
+    if (matchesToken(bytes)) {
+      if (CollectionUtils.notEmpty(children)) {
+        // recurse as deeply as possible
+        TokenizerNode lastChild = CollectionUtils.getLast(children);
+        if (lastChild.partiallyMatchesToken(bytes)) {
+          lastChild.addSorted(bytes);
+          return;
+        }
+      }
+    }
+
+    // add it as a child of this node
+    int numIdenticalTokenBytes = numIdenticalBytes(bytes);// should be <= token.length
+    int tailOffset = tokenStartOffset + numIdenticalTokenBytes;
+    int tailLength = bytes.getLength() - tailOffset;
+
+    if (numIdenticalTokenBytes == token.getLength()) {
+      if (tailLength == 0) {// identical to this node
+        incrementNumOccurrences(1);
+      } else {// identical to this node, but with a few extra tailing bytes (nub)
+        int childNodeDepth = nodeDepth + 1;
+        int childTokenStartOffset = tokenStartOffset + numIdenticalTokenBytes;
+        TokenizerNode newChildNode = builder.addNode(this, childNodeDepth, childTokenStartOffset,
+          bytes, tailOffset);
+        addChild(newChildNode);
+      }
+    } else/* if(numIdenticalBytes > 0) */{
+      split(numIdenticalTokenBytes, bytes);
+    }
+  }
+
+
+  protected void addChild(TokenizerNode node) {
+    node.setParent(this);
+    children.add(node);
+  }
+
+
+  protected void split(int numTokenBytesToRetain, final ByteRange bytes) {
+    int childNodeDepth = nodeDepth;
+    int childTokenStartOffset = tokenStartOffset + numTokenBytesToRetain;
+
+    TokenizerNode firstChild = builder.addNode(this, childNodeDepth, childTokenStartOffset,
+      token, numTokenBytesToRetain);
+    firstChild.setNumOccurrences(numOccurrences);// do before clearing this node's numOccurrences
+    token.setLength(numTokenBytesToRetain);
+    numOccurrences = 0;
+
+    // need to re-point the previous children to the new node
+    moveChildrenToDifferentParent(firstChild);
+    addChild(firstChild);// do after re-pointing existing children
+
+    // this is a leaf
+    TokenizerNode secondChild = builder.addNode(this, childNodeDepth, childTokenStartOffset,
+      bytes, tokenStartOffset + numTokenBytesToRetain);
+    addChild(secondChild);
+
+    // we added "this" node as a new level above the two children, so increment nodeDepths below us
+    firstChild.incrementNodeDepthRecursive();
+    secondChild.incrementNodeDepthRecursive();
+  }
+
+
+  protected void incrementNodeDepthRecursive() {
+    ++nodeDepth;
+    builder.submitMaxNodeDepthCandidate(nodeDepth);
+    for (int i = 0; i < children.size(); ++i) {
+      children.get(i).incrementNodeDepthRecursive();
+    }
+  }
+
+
+  protected void moveChildrenToDifferentParent(TokenizerNode newParent) {
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      child.setParent(newParent);
+      newParent.children.add(child);
+    }
+    children.clear();
+  }
+
+
+	/************************ byte[] utils *************************/
+
+  protected boolean partiallyMatchesToken(ByteRange bytes) {
+    return numIdenticalBytes(bytes) > 0;
+  }
+
+  protected boolean matchesToken(ByteRange bytes) {
+    return numIdenticalBytes(bytes) == getTokenLength();
+  }
+
+  protected int numIdenticalBytes(ByteRange bytes) {
+    return token.numEqualPrefixBytes(bytes, tokenStartOffset);
+  }
+
+
+	/***************** moving nodes around ************************/
+
+  public void appendNodesToExternalList(List<TokenizerNode> appendTo, boolean includeNonLeaves,
+      boolean includeLeaves) {
+    if (includeNonLeaves && !isLeaf() || includeLeaves && isLeaf()) {
+      appendTo.add(this);
+    }
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      child.appendNodesToExternalList(appendTo, includeNonLeaves, includeLeaves);
+    }
+  }
+
+  public int setInsertionIndexes(int nextIndex) {
+    int newNextIndex = nextIndex;
+    if (hasOccurrences()) {
+      setFirstInsertionIndex(nextIndex);
+      newNextIndex += numOccurrences;
+    }
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      newNextIndex = child.setInsertionIndexes(newNextIndex);
+    }
+    return newNextIndex;
+  }
+
+  public void appendOutputArrayOffsets(List<Integer> offsets) {
+    if (hasOccurrences()) {
+      offsets.add(outputArrayOffset);
+    }
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      child.appendOutputArrayOffsets(offsets);
+    }
+  }
+
+
+  /***************** searching *********************************/
+
+  public void getNode(TokenizerRowSearchResult resultHolder, byte[] key, int keyOffset,
+      int keyLength) {
+    int thisNodeDepthPlusLength = tokenStartOffset + token.getLength();
+
+    // quick check if the key is shorter than this node (may not work for binary search)
+    if (CollectionUtils.isEmpty(children)) {
+      if (thisNodeDepthPlusLength < keyLength) {// ran out of bytes
+        resultHolder.set(TokenizerRowSearchPosition.NO_MATCH, null);
+        return;
+      }
+    }
+
+    // all token bytes must match
+    for (int i = 0; i < token.getLength(); ++i) {
+      if (key[tokenStartOffset + keyOffset + i] != token.get(i)) {
+        // TODO return whether it's before or after so we can binary search
+        resultHolder.set(TokenizerRowSearchPosition.NO_MATCH, null);
+        return;
+      }
+    }
+
+    if (thisNodeDepthPlusLength == keyLength && numOccurrences > 0) {
+      resultHolder.set(TokenizerRowSearchPosition.MATCH, this);// MATCH
+      return;
+    }
+
+    if (CollectionUtils.notEmpty(children)) {
+      // TODO binary search the children
+      for (int i = 0; i < children.size(); ++i) {
+        TokenizerNode child = children.get(i);
+        child.getNode(resultHolder, key, keyOffset, keyLength);
+        if (resultHolder.isMatch()) {
+          return;
+        } else if (resultHolder.getDifference() == TokenizerRowSearchPosition.BEFORE) {
+          // passed it, so it doesn't exist
+          resultHolder.set(TokenizerRowSearchPosition.NO_MATCH, null);
+          return;
+        }
+        // key is still AFTER the current node, so continue searching
+      }
+    }
+
+    // checked all children (or there were no children), and didn't find it
+    resultHolder.set(TokenizerRowSearchPosition.NO_MATCH, null);
+    return;
+  }
+
+
+  /****************** writing back to byte[]'s *************************/
+
+  public byte[] getNewByteArray() {
+    byte[] arrayToFill = new byte[tokenStartOffset + token.getLength()];
+    fillInBytes(arrayToFill);
+    return arrayToFill;
+  }
+
+  public void fillInBytes(byte[] arrayToFill) {
+    for (int i = 0; i < token.getLength(); ++i) {
+      arrayToFill[tokenStartOffset + i] = token.get(i);
+    }
+    if (parent != null) {
+      parent.fillInBytes(arrayToFill);
+    }
+  }
+
+
+  /************************** printing ***********************/
+
+  @Override
+  public String toString() {
+    String s = "";
+    if (parent == null) {
+      s += "R:";
+    } else {
+      s += (getBnlIndicator(false)) + Bytes.toString(parent.getNewByteArray());
+    }
+    s += "[" + Bytes.toString(token.deepCopyToNewArray()) + "]";
+    if (numOccurrences > 0) {
+      s += "x" + numOccurrences;
+    }
+    return s;
+  }
+
+  public String getPaddedTokenAndOccurrenceString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append(getBnlIndicator(true));
+    sb.append(StringUtils.padFront(numOccurrences + "", ' ', 3));
+    sb.append(StringUtils.padFront(nodeDepth + "", ' ', 3));
+    if (outputArrayOffset >= 0) {
+      sb.append(StringUtils.padFront(outputArrayOffset + "", ' ', 3));
+    }
+    sb.append("  ");
+    for (int i = 0; i < tokenStartOffset; ++i) {
+      sb.append(" ");
+    }
+    sb.append(Bytes.toString(token.deepCopyToNewArray()).replaceAll(" ", "_"));
+    return sb.toString();
+  }
+
+  public String getBnlIndicator(boolean indent) {
+    if (indent) {
+      if (isNub()) {
+        return " N ";
+      }
+      return isBranch() ? "B  " : "  L";
+    }
+    if (isNub()) {
+      return "N";
+    }
+    return isBranch() ? "B" : "L";
+  }
+
+
+	/********************** count different node types ********************/
+
+  public int getNumBranchNodesIncludingThisNode() {
+    if (isLeaf()) {
+      return 0;
+    }
+    int totalFromThisPlusChildren = isBranch() ? 1 : 0;
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      totalFromThisPlusChildren += child.getNumBranchNodesIncludingThisNode();
+    }
+    return totalFromThisPlusChildren;
+  }
+
+  public int getNumNubNodesIncludingThisNode() {
+    if (isLeaf()) {
+      return 0;
+    }
+    int totalFromThisPlusChildren = isNub() ? 1 : 0;
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      totalFromThisPlusChildren += child.getNumNubNodesIncludingThisNode();
+    }
+    return totalFromThisPlusChildren;
+  }
+
+  public int getNumLeafNodesIncludingThisNode() {
+    if (isLeaf()) {
+      return 1;
+    }
+    int totalFromChildren = 0;
+    for (int i = 0; i < children.size(); ++i) {
+      TokenizerNode child = children.get(i);
+      totalFromChildren += child.getNumLeafNodesIncludingThisNode();
+    }
+    return totalFromChildren;
+  }
+
+
+  /*********************** simple read methods *******************************/
+
+  public int getNodeDepth() {
+    return nodeDepth;
+  }
+
+  public int getTokenLength() {
+    return token.getLength();
+  }
+
+  public boolean hasOccurrences() {
+    return numOccurrences > 0;
+  }
+
+  public boolean isRoot() {
+    return this.parent == null;
+  }
+
+  public int getNumChildren() {
+    return CollectionUtils.size(children);
+  }
+
+  public TokenizerNode getLastChild() {
+    if (CollectionUtils.isEmpty(children)) {
+      return null;
+    }
+    return CollectionUtils.getLast(children);
+  }
+
+  public boolean isLeaf() {
+    return CollectionUtils.isEmpty(children) && hasOccurrences();
+  }
+
+  public boolean isBranch() {
+    return CollectionUtils.notEmpty(children) && !hasOccurrences();
+  }
+
+  public boolean isNub() {
+    return CollectionUtils.notEmpty(children) && hasOccurrences();
+  }
+
+
+  /********************** simple write methods *************************/
+
+  /**
+   * Each occurrence > 1 indicates a repeat of the previous entry.  This can be called directly by
+   * an external class without going through the process of detecting a repeat if it is a known
+   * repeat by some external mechanism.  PtEncoder uses this when adding cells to a row if it knows
+   * the new cells are part of the current row.
+   * @param d increment by this amount
+   */
+  public void incrementNumOccurrences(int d) {
+    numOccurrences += d;
+  }
+
+
+  /************************* autogenerated get/set ******************/
+
+  public int getTokenOffset() {
+    return tokenStartOffset;
+  }
+
+  public TokenizerNode getParent() {
+    return parent;
+  }
+
+  public ByteRange getToken() {
+    return token;
+  }
+
+  public int getNumOccurrences() {
+    return numOccurrences;
+  }
+
+  public void setParent(TokenizerNode parent) {
+    this.parent = parent;
+  }
+
+  public void setNumOccurrences(int numOccurrences) {
+    this.numOccurrences = numOccurrences;
+  }
+
+  public ArrayList<TokenizerNode> getChildren() {
+    return children;
+  }
+
+  public long getId() {
+    return id;
+  }
+
+  public int getFirstInsertionIndex() {
+    return firstInsertionIndex;
+  }
+
+  public void setFirstInsertionIndex(int firstInsertionIndex) {
+    this.firstInsertionIndex = firstInsertionIndex;
+  }
+
+  public int getNodeSortedIndex() {
+    return nodeSortedIndex;
+  }
+
+  public void setNodeSortedIndex(int nodeSortedIndex) {
+    this.nodeSortedIndex = nodeSortedIndex;
+  }
+
+  public int getNegativeIndex() {
+    return negativeIndex;
+  }
+
+  public void setNegativeIndex(int negativeIndex) {
+    this.negativeIndex = negativeIndex;
+  }
+
+  public int getOutputArrayOffset() {
+    return outputArrayOffset;
+  }
+
+  public void setOutputArrayOffset(int outputArrayOffset) {
+    this.outputArrayOffset = outputArrayOffset;
+  }
+
+  public void setId(long id) {
+    this.id = id;
+  }
+
+  public void setBuilder(Tokenizer builder) {
+    this.builder = builder;
+  }
+
+  public void setTokenOffset(int tokenOffset) {
+    this.tokenStartOffset = tokenOffset;
+  }
+
+  public void setToken(ByteRange token) {
+    this.token = token;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchPosition.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchPosition.java
new file mode 100644
index 0000000..a6c8a13
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchPosition.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.tokenize;
+
+public enum TokenizerRowSearchPosition {
+
+	/*
+	 * where is the key relative to our current position in the tree
+	 *
+	 * for example, the current tree node is "BEFORE" the key we are seeking
+	 */
+
+	AFTER,//the key is after this tree node, so keep searching
+	BEFORE,//in a binary search, this tells us to back up
+	MATCH,//the current node is a full match
+	NO_MATCH,//might as well return a value more informative than null
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchResult.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchResult.java
new file mode 100644
index 0000000..1396da1
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/encode/tokenize/TokenizerRowSearchResult.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.encode.tokenize;
+
+
+/**
+ * for recursively searching a PtBuilder
+ */
+public class TokenizerRowSearchResult{
+
+  /************ fields ************************/
+
+  protected TokenizerRowSearchPosition difference;
+  protected TokenizerNode matchingNode;
+
+
+  /*************** construct *****************/
+
+  public TokenizerRowSearchResult() {
+  }
+
+  public TokenizerRowSearchResult(TokenizerRowSearchPosition difference) {
+    this.difference = difference;
+  }
+
+  public TokenizerRowSearchResult(TokenizerNode matchingNode) {
+    this.difference = TokenizerRowSearchPosition.MATCH;
+    this.matchingNode = matchingNode;
+  }
+
+
+  /*************** methods **********************/
+
+  public boolean isMatch() {
+    return TokenizerRowSearchPosition.MATCH == difference;
+  }
+
+
+  /************* get/set ***************************/
+
+  public TokenizerRowSearchPosition getDifference() {
+    return difference;
+  }
+
+  public TokenizerNode getMatchingNode() {
+    return matchingNode;
+  }
+
+  public void set(TokenizerRowSearchPosition difference, TokenizerNode matchingNode) {
+    this.difference = difference;
+    this.matchingNode = matchingNode;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ArrayBlockSearcherPool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ArrayBlockSearcherPool.java
new file mode 100644
index 0000000..e8c6cb2
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ArrayBlockSearcherPool.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.pool;
+
+import java.nio.ByteBuffer;
+import java.util.Queue;
+import java.util.concurrent.LinkedBlockingQueue;
+
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher;
+
+public class ArrayBlockSearcherPool {
+
+  protected Queue<PrefixTreeArraySearcher> pool = new LinkedBlockingQueue<PrefixTreeArraySearcher>();
+
+  public PrefixTreeArraySearcher checkOut(ByteBuffer buffer, boolean incldueMemstoreTS) {
+    PrefixTreeArraySearcher searcher = pool.poll();
+    searcher = DecoderFactory.ensureArraySearcherValid(buffer, searcher, incldueMemstoreTS);
+    return searcher;
+  }
+
+  public void checkIn(PrefixTreeArraySearcher searcher) {
+    searcher.releaseBlockReference();
+    //TODO limit the pool size
+    pool.offer(searcher);
+  }
+
+  @Override
+  public String toString() {
+    return ("poolSize:" + pool.size());
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/DecoderFactory.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/DecoderFactory.java
new file mode 100644
index 0000000..5a99c85
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/DecoderFactory.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.pool;
+
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher;
+
+public class DecoderFactory {
+
+  protected static ArrayBlockSearcherPool ARRAY_BLOCK_SEARCHER_POOL = new ArrayBlockSearcherPool();
+
+  //TODO will need a PrefixTreeSearcher on top of CellSearcher
+  public static PrefixTreeArraySearcher checkOut(final ByteBuffer buffer, boolean includeMemstoreTS) {
+    if (buffer.isDirect()) {
+      throw new IllegalArgumentException("DirectByteBuffers not supported yet");
+      // TODO implement PtByteBufferBlockScanner
+    } else {
+      PrefixTreeArraySearcher searcher = ARRAY_BLOCK_SEARCHER_POOL.checkOut(buffer, includeMemstoreTS);
+      return searcher;
+    }
+  }
+
+  public static void checkIn(CellSearcher pSearcher) {
+    if (pSearcher == null) {
+      return;
+    }
+    if (pSearcher instanceof PrefixTreeArraySearcher) {
+      PrefixTreeArraySearcher searcher = (PrefixTreeArraySearcher) pSearcher;
+      ARRAY_BLOCK_SEARCHER_POOL.checkIn(searcher);
+    }
+  }
+
+
+  /**************************** helper ******************************/
+
+  protected static PrefixTreeArraySearcher ensureArraySearcherValid(ByteBuffer buffer,
+      PrefixTreeArraySearcher searcher, boolean includeMemstoreTS) {
+    if (searcher == null) {
+      PrefixTreeBlockMeta blockMeta = new PrefixTreeBlockMeta(buffer);
+      searcher = new PrefixTreeArraySearcher(blockMeta, blockMeta.getRowTreeDepth(),
+          blockMeta.getMaxRowLength(), blockMeta.getMaxQualifierLength());
+      searcher.initOnBlock(blockMeta, buffer.array(), includeMemstoreTS);
+      return searcher;
+    }
+
+    PrefixTreeBlockMeta blockMeta = searcher.getBlockMeta();
+    blockMeta.initOnBlock(buffer);
+    if (!searcher.areBuffersBigEnough()) {
+      int maxRowTreeStackNodes = Math.max(blockMeta.getRowTreeDepth(),
+        searcher.getMaxRowTreeStackNodes());
+      int rowBufferLength = Math.max(blockMeta.getMaxRowLength(), searcher.getRowBufferLength());
+      int qualifierBufferLength = Math.max(blockMeta.getMaxQualifierLength(),
+        searcher.getQualifierBufferLength());
+      searcher = new PrefixTreeArraySearcher(blockMeta, maxRowTreeStackNodes, rowBufferLength,
+          qualifierBufferLength);
+    }
+    searcher.initOnBlock(blockMeta, buffer.array(), includeMemstoreTS);
+    return searcher;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderFactory.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderFactory.java
new file mode 100644
index 0000000..ddee708
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderFactory.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.pool;
+
+import java.io.OutputStream;
+
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+
+public class EncoderFactory {
+
+  protected static EncoderPool POOL = new ThreadLocalEncoderPool();
+
+
+  public static PrefixTreeEncoder checkOut(OutputStream os, boolean includeMemstoreTimestamp) {
+    return POOL.checkOut(os, includeMemstoreTimestamp);
+  }
+
+  public static void checkIn(PrefixTreeEncoder builder) {
+    POOL.checkIn(builder);
+  }
+
+
+  /**************************** helper ******************************/
+
+  protected static PrefixTreeEncoder prepareBuilder(PrefixTreeEncoder builder, OutputStream os,
+      boolean includeMemstoreTimestamp) {
+    PrefixTreeEncoder ret = builder;
+    if (builder == null) {
+      ret = new PrefixTreeEncoder();
+    }
+    ret.setOutputStream(os);
+    ret.setIncludeMemstoreTimestamp(includeMemstoreTimestamp);
+    ret.reset();
+    return ret;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderPool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderPool.java
new file mode 100644
index 0000000..1f58add
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/EncoderPool.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.pool;
+
+import java.io.OutputStream;
+
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+
+public interface EncoderPool {
+
+  PrefixTreeEncoder checkOut(OutputStream os, boolean includeMemstoreTimestamp);
+  void checkIn(PrefixTreeEncoder encoder);
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ThreadLocalEncoderPool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ThreadLocalEncoderPool.java
new file mode 100644
index 0000000..7b1d6bd
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/codec/prefixtree/pool/ThreadLocalEncoderPool.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.pool;
+
+import java.io.OutputStream;
+
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+
+/**
+ * Assumes compactions are handled by a single thread.
+ */
+public class ThreadLocalEncoderPool implements EncoderPool{
+
+  protected static ThreadLocal<PrefixTreeEncoder> BUILDER_HOLDER = new ThreadLocal<PrefixTreeEncoder>();
+
+  @Override
+  public PrefixTreeEncoder checkOut(OutputStream os, boolean includeMemstoreTimestamp) {
+    PrefixTreeEncoder builder = BUILDER_HOLDER.get();
+    builder = EncoderFactory.prepareBuilder(builder, os, includeMemstoreTimestamp);
+    BUILDER_HOLDER.set(builder);
+    return builder;
+  }
+
+  @Override
+  public void checkIn(PrefixTreeEncoder encoder) {
+    // attached to thread on checkOut, so shouldn't need to do anything here
+
+    // do we need to worry about detaching encoders from compaction threads or are the same threads
+    // used over and over
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/ByteUtils.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/ByteUtils.java
new file mode 100644
index 0000000..9c58f66
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/ByteUtils.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+
+public class ByteUtils {
+
+  public static boolean equals(List<byte[]> a, List<byte[]> b) {
+    if (a == null && b == null) {
+      return true;
+    }
+    if (a == null && b != null) {
+      return false;
+    }
+    if (a != null && b == null) {
+      return false;
+    }
+    if (a.size() != b.size()) {
+      return false;
+    }
+    for (int i = 0; i < a.size(); ++i) {
+      if (!Bytes.equals(a.get(i), b.get(i))) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  public static boolean isSorted(Collection<byte[]> arrays) {
+    byte[] previous = new byte[0];
+    for (byte[] array : IterableUtils.nullSafe(arrays)) {
+      if (Bytes.compareTo(previous, array) > 0) {
+        return false;
+      }
+      previous = array;
+    }
+    return true;
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/ByteRangeSet.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/ByteRangeSet.java
new file mode 100644
index 0000000..dd186b9
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/ByteRangeSet.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.byterange;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.util.ArrayUtils;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+
+import com.google.common.collect.Lists;
+
+/**
+ * Performance oriented class for de-duping and storing arbitrary byte[]'s arriving in non-sorted
+ * order.  Appends individual byte[]'s to a single big byte[] to avoid overhead and garbage.
+ *
+ */
+public abstract class ByteRangeSet {
+
+  /******************** fields **********************/
+
+  protected byte[] byteAppender;
+  protected int numBytes;
+
+  protected Map<ByteRange, Integer> uniqueIndexByUniqueRange;
+
+  protected ArrayList<ByteRange> uniqueRanges;
+  protected int numUniqueRanges = 0;
+
+  protected int[] uniqueRangeIndexByInsertionId;
+  protected int numInputs;
+
+  protected List<Integer> sortedIndexByUniqueIndex;
+  protected int[] sortedIndexByInsertionId;
+  protected ArrayList<ByteRange> sortedRanges;
+
+
+  /****************** construct **********************/
+
+  protected ByteRangeSet() {
+    this.byteAppender = new byte[0];
+    this.uniqueRanges = Lists.newArrayList();
+    this.uniqueRangeIndexByInsertionId = new int[0];
+    this.sortedIndexByUniqueIndex = Lists.newArrayList();
+    this.sortedIndexByInsertionId = new int[0];
+    this.sortedRanges = Lists.newArrayList();
+  }
+
+  public void reset() {
+    numBytes = 0;
+    uniqueIndexByUniqueRange.clear();
+    numUniqueRanges = 0;
+    numInputs = 0;
+    sortedIndexByUniqueIndex.clear();
+    sortedRanges.clear();
+  }
+
+
+  /*************** abstract *************************/
+
+  public abstract void addToSortedRanges();
+
+
+  /**************** methods *************************/
+
+  public void add(ByteRange bytes) {
+    Integer index = uniqueIndexByUniqueRange.get(bytes);
+    if (index == null) {
+      index = store(bytes);
+    }
+    int minLength = numInputs + 1;
+    uniqueRangeIndexByInsertionId = ArrayUtils.ensureCapacity(uniqueRangeIndexByInsertionId,
+        minLength, 2 * minLength);
+    uniqueRangeIndexByInsertionId[numInputs] = index;
+    ++numInputs;
+  }
+
+  protected int store(ByteRange bytes) {
+    int indexOfNewElement = numUniqueRanges;
+    if (uniqueRanges.size() <= numUniqueRanges) {
+      uniqueRanges.add(new ByteRange());
+    }
+    ByteRange storedRange = uniqueRanges.get(numUniqueRanges);
+    int neededBytes = numBytes + bytes.getLength();
+    byteAppender = ArrayUtils.ensureCapacity(byteAppender, neededBytes, 2 * neededBytes);
+    bytes.deepCopyTo(byteAppender, numBytes);
+    storedRange.set(byteAppender, numBytes, bytes.getLength());// this isn't valid yet
+    numBytes += bytes.getLength();
+    uniqueIndexByUniqueRange.put(storedRange, indexOfNewElement);
+    int newestUniqueIndex = numUniqueRanges;
+    ++numUniqueRanges;
+    return newestUniqueIndex;
+  }
+
+  public ByteRangeSet compile() {
+    addToSortedRanges();
+    for (int i = 0; i < sortedRanges.size(); ++i) {
+      sortedIndexByUniqueIndex.add(null);// need to grow the size
+    }
+    // TODO move this to an invert(int[]) util method
+    for (int i = 0; i < sortedIndexByUniqueIndex.size(); ++i) {
+      int uniqueIndex = uniqueIndexByUniqueRange.get(sortedRanges.get(i));
+      sortedIndexByUniqueIndex.set(uniqueIndex, i);
+    }
+    sortedIndexByInsertionId = ArrayUtils.ensureCapacity(sortedIndexByInsertionId, numInputs,
+        numInputs);
+    for (int i = 0; i < numInputs; ++i) {
+      int uniqueRangeIndex = uniqueRangeIndexByInsertionId[i];
+      int sortedIndex = sortedIndexByUniqueIndex.get(uniqueRangeIndex);
+      sortedIndexByInsertionId[i] = sortedIndex;
+    }
+    return this;
+  }
+
+  public ByteRange getForInputIndex(int insertionId) {
+    return uniqueRanges.get(uniqueRangeIndexByInsertionId[insertionId]);
+  }
+
+  public int getSortedIndexForInsertionId(int insertionId) {
+    return sortedIndexByInsertionId[insertionId];
+  }
+
+  public int size() {
+    return uniqueIndexByUniqueRange.size();
+  }
+
+  public long getTotalSize() {
+    return numBytes;
+  }
+
+
+  /***************** standard methods ************************/
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    int i = 0;
+    for (ByteRange r : sortedRanges) {
+      if (i > 0) {
+        sb.append("\n");
+      }
+      sb.append(i + " " + Bytes.toStringBinary(r.deepCopyToNewArray()));
+      ++i;
+    }
+    sb.append("\ntotalSize:" + getTotalSize());
+    sb.append("\navgSize:" + getAvgSize());
+    return sb.toString();
+  }
+
+
+  /**************** get/set *****************************/
+
+  public ArrayList<ByteRange> getSortedRanges() {
+    return sortedRanges;
+  }
+
+  public long getAvgSize() {
+    return numBytes / numUniqueRanges;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeHashSet.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeHashSet.java
new file mode 100644
index 0000000..a8af065
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeHashSet.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.byterange.impl;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+import org.apache.hbase.util.byterange.ByteRangeSet;
+
+/**
+ * This is probably the best implementation at the moment, though a HashMap produces garbage when
+ * adding a new element to it.  We can probably create a tighter implementation without pointers
+ * or garbage.
+ */
+public class ByteRangeHashSet extends ByteRangeSet {
+
+  /************************ constructors *****************************/
+
+  public ByteRangeHashSet() {
+    this.uniqueIndexByUniqueRange = new HashMap<ByteRange, Integer>();
+  }
+
+  public ByteRangeHashSet(List<ByteRange> rawByteArrays) {
+    for (ByteRange in : IterableUtils.nullSafe(rawByteArrays)) {
+      add(in);
+    }
+  }
+
+  @Override
+  public void addToSortedRanges() {
+    sortedRanges.addAll(CollectionUtils.nullSafe(uniqueIndexByUniqueRange.keySet()));
+    Collections.sort(sortedRanges);
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeTreeSet.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeTreeSet.java
new file mode 100644
index 0000000..4df6f0c
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/byterange/impl/ByteRangeTreeSet.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.byterange.impl;
+
+import java.util.List;
+import java.util.TreeMap;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+import org.apache.hbase.util.byterange.ByteRangeSet;
+
+/**
+ * Not currently used in production, but here as a benchmark comparison against ByteRangeHashSet.
+ */
+public class ByteRangeTreeSet extends ByteRangeSet {
+
+  /************************ constructors *****************************/
+
+  public ByteRangeTreeSet() {
+    this.uniqueIndexByUniqueRange = new TreeMap<ByteRange,Integer>();
+  }
+
+  public ByteRangeTreeSet(List<ByteRange> rawByteArrays) {
+    this();//needed to initialize the TreeSet
+    for(ByteRange in : IterableUtils.nullSafe(rawByteArrays)){
+      add(in);
+    }
+  }
+
+  @Override
+  public void addToSortedRanges() {
+    sortedRanges.addAll(CollectionUtils.nullSafe(uniqueIndexByUniqueRange.keySet()));
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/io/OutputStreamUtils.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/io/OutputStreamUtils.java
new file mode 100644
index 0000000..f55f70c
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/io/OutputStreamUtils.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.io;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+
+/**
+ * Convenience methods for writing to an OutputStream.  They're currently unchecking IOExceptions
+ * because prefix-tree only writes to ByteArrayOutputStreams which should not throw IOExceptions,
+ * but maybe there is a better way to rethrow them.  RuntimeIOException?
+ */
+public class OutputStreamUtils {
+
+  public static void write(OutputStream os, byte b) {
+    try {
+      os.write(b);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public static void write(OutputStream os, byte[] bytes) {
+    write(os, bytes, 0, bytes.length);
+  }
+
+  public static void write(OutputStream os, byte[] bytes, int offset, int length) {
+    try {
+      os.write(bytes, offset, length);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public static void write(OutputStream os, ByteRange byteRange) {
+    write(os, byteRange.getBytes(), byteRange.getOffset(), byteRange.getLength());
+  }
+
+  public static void write(OutputStream os, ByteRange byteRange, int byteRangeInnerOffset) {
+    write(os, byteRange.getBytes(), byteRange.getOffset() + byteRangeInnerOffset,
+        byteRange.getLength() - byteRangeInnerOffset);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UFIntTool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UFIntTool.java
new file mode 100644
index 0000000..6da6281
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UFIntTool.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.OutputStream;
+
+import org.apache.hbase.util.io.OutputStreamUtils;
+
+/**
+ * UFInt is an abbreviation for Unsigned Fixed-width Integer.
+ *
+ * This class converts between positive ints and 1-4 bytes that represent the int.  All input ints
+ * must be positive.  Max values stored in N bytes are:
+ *
+ * N=1: 2^8  =>           256
+ * N=2: 2^16 =>        65,536
+ * N=3: 2^24 =>    16,777,216
+ * N=4: 2^31 => 2,147,483,648 (Integer.MAX_VALUE)
+ *
+ * Currently does not support 2^32, but could be supported by modifying the interface to accept
+ * long inputs and outputs.
+ *
+ * This was created to get most of the memory savings of a variable length integer when encoding
+ * an array of input integers, but to fix the number of bytes for each integer to the number needed
+ * to store the maximum integer in the array.  This enables a binary search to be performed on the
+ * array of encoded integers.
+ *
+ * PrefixTree nodes often store offsets into a block that can fit into 1 or 2 bytes.  Note that if
+ * the maximum value of an array of numbers needs 2 bytes, then it's likely that a majority of the
+ * numbers will also require 2 bytes.
+ *
+ * warnings:
+ *  * no input validation for max performance
+ *  * no negatives
+ */
+public class UFIntTool {
+
+  public static long maxValueForNumBytes(int numBytes) {
+    return (1L << (numBytes * 8)) - 1;
+  }
+
+  public static int numBytes(final long value) {
+    if (value == 0) {
+      return 1;
+    }// 0 doesn't work with the formula below
+    return (64 + 7 - Long.numberOfLeadingZeros(value)) / 8;
+  }
+
+  public static byte[] getBytes(int outputWidth, final long value) {
+    byte[] bytes = new byte[outputWidth];
+    writeBytes(outputWidth, value, bytes, 0);
+    return bytes;
+  }
+
+  public static void writeBytes(int outputWidth, final long value, byte[] bytes, int offset) {
+    bytes[offset + outputWidth - 1] = (byte) value;
+    for (int i = outputWidth - 2; i >= 0; --i) {
+      bytes[offset + i] = (byte) (value >>> (outputWidth - i - 1) * 8);
+    }
+  }
+
+  static final long[] MASKS = new long[]{
+    (long)255,
+	  (long)255 << 8,
+	  (long)255 << 16,
+	  (long)255 << 24,
+	  (long)255 << 32,
+	  (long)255 << 40,
+	  (long)255 << 48,
+	  (long)255 << 56
+  };
+
+  public static void writeBytes(int outputWidth, final long value, OutputStream os) {
+    for (int i = outputWidth - 1; i >= 0; --i) {
+      OutputStreamUtils.write(os, (byte) ((value & MASKS[i]) >>> (8 * i)));
+    }
+  }
+
+  public static long fromBytes(final byte[] bytes) {
+    long value = 0;
+    value |= bytes[0] & 0xff;// these seem to do ok without casting the byte to int
+    for (int i = 1; i < bytes.length; ++i) {
+      value <<= 8;
+      value |= bytes[i] & 0xff;
+    }
+    return value;
+  }
+
+  public static long fromBytes(final byte[] bytes, final int offset, final int width) {
+    long value = 0;
+    value |= bytes[0 + offset] & 0xff;// these seem to do ok without casting the byte to int
+    for (int i = 1; i < width; ++i) {
+      value <<= 8;
+      value |= bytes[i + offset] & 0xff;
+    }
+    return value;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVIntTool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVIntTool.java
new file mode 100644
index 0000000..201e2d1
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVIntTool.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+
+import org.apache.hbase.util.io.OutputStreamUtils;
+
+/**
+ * Simple Variable Length Integer encoding.  Left bit of 0 means we are on the last bit.
+ *
+ * Need more doc.
+ */
+public class UVIntTool {
+
+  public static final byte
+    BYTE_7_RIGHT_BITS_SET = 127,
+    BYTE_LEFT_BIT_SET = -128;
+
+  public static final long
+    INT_3_RIGHT_BITS_SET = 127,
+    INT_8TH_BIT_SET = 128;
+
+  public static final byte[]
+    MAX_VALUE_BYTES = new byte[] { -1, -1, -1, -1, 7 };
+
+  /********************* int -> bytes **************************/
+
+  public static int numBytes(int in) {
+    if (in == 0) {
+      // doesn't work with the formula below
+      return 1;
+    }
+    return (38 - Integer.numberOfLeadingZeros(in)) / 7;// 38 comes from 32+(7-1)
+  }
+
+  public static byte[] getBytes(int value) {
+    int numBytes = numBytes(value);
+    byte[] bytes = new byte[numBytes];
+    int remainder = value;
+    for (int i = 0; i < numBytes - 1; ++i) {
+      // set the left bit
+      bytes[i] = (byte) ((remainder & INT_3_RIGHT_BITS_SET) | INT_8TH_BIT_SET);
+      remainder >>= 7;
+    }
+    // do not set the left bit
+    bytes[numBytes - 1] = (byte) (remainder & INT_3_RIGHT_BITS_SET);
+    return bytes;
+  }
+
+  public static int writeBytes(int value, OutputStream os) {
+    int numBytes = numBytes(value);
+    int remainder = value;
+    for (int i = 0; i < numBytes - 1; ++i) {
+      // set the left bit
+      OutputStreamUtils.write(os, (byte) ((remainder & INT_3_RIGHT_BITS_SET) | INT_8TH_BIT_SET));
+      remainder >>= 7;
+    }
+    // do not set the left bit
+    OutputStreamUtils.write(os, (byte) (remainder & INT_3_RIGHT_BITS_SET));
+    return numBytes;
+  }
+
+  /******************** bytes -> int **************************/
+
+  public static int getInt(byte[] bytes) {
+    return getInt(bytes, 0);
+  }
+
+  public static int getInt(byte[] bytes, int offset) {
+    int value = 0;
+    for (int i = 0;; ++i) {
+      byte b = bytes[offset + i];
+      int shifted = BYTE_7_RIGHT_BITS_SET & b;// kill leftmost bit
+      shifted <<= 7 * i;
+      value |= shifted;
+      if (b >= 0) {
+        break;
+      }
+    }
+    return value;
+  }
+
+  public static int getInt(InputStream is) throws IOException {
+    int value = 0;
+    int i = 0;
+    int b;
+    do{
+      b = is.read();
+      int shifted = BYTE_7_RIGHT_BITS_SET & b;// kill leftmost bit
+      shifted <<= 7 * i;
+      value |= shifted;
+      ++i;
+    }while(b > Byte.MAX_VALUE);
+    return value;
+  }
+}
diff --git a/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVLongTool.java b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVLongTool.java
new file mode 100644
index 0000000..e1aca35
--- /dev/null
+++ b/hbase-prefix-tree/src/main/java/org/apache/hbase/util/vint/UVLongTool.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.OutputStream;
+
+import org.apache.hbase.util.io.OutputStreamUtils;
+
+public class UVLongTool{
+
+  public static final byte
+    BYTE_7_RIGHT_BITS_SET = 127,
+    BYTE_LEFT_BIT_SET = -128;
+
+  public static final long
+    LONG_7_RIGHT_BITS_SET = 127,
+    LONG_8TH_BIT_SET = 128;
+
+  public static final byte[]
+    MAX_VALUE_BYTES = new byte[] { -1, -1, -1, -1, -1, -1, -1, -1, 127 };
+
+
+  /********************* int -> bytes **************************/
+
+  public static int numBytes(long in) {// do a check for illegal arguments if not protected
+    if (in == 0) {
+      return 1;
+    }// doesn't work with the formula below
+    return (70 - Long.numberOfLeadingZeros(in)) / 7;// 70 comes from 64+(7-1)
+  }
+
+  public static byte[] getBytes(long value) {
+    int numBytes = numBytes(value);
+    byte[] bytes = new byte[numBytes];
+    long remainder = value;
+    for (int i = 0; i < numBytes - 1; ++i) {
+      bytes[i] = (byte) ((remainder & LONG_7_RIGHT_BITS_SET) | LONG_8TH_BIT_SET);// set the left bit
+      remainder >>= 7;
+    }
+    bytes[numBytes - 1] = (byte) (remainder & LONG_7_RIGHT_BITS_SET);// do not set the left bit
+    return bytes;
+  }
+
+  public static int writeBytes(long value, OutputStream os) {
+    int numBytes = numBytes(value);
+    long remainder = value;
+    for (int i = 0; i < numBytes - 1; ++i) {
+      // set the left bit
+      OutputStreamUtils.write(os, (byte) ((remainder & LONG_7_RIGHT_BITS_SET) | LONG_8TH_BIT_SET));
+      remainder >>= 7;
+    }
+    // do not set the left bit
+    OutputStreamUtils.write(os, (byte) (remainder & LONG_7_RIGHT_BITS_SET));
+    return numBytes;
+  }
+
+  /******************** bytes -> int **************************/
+
+  public static long getLong(byte[] bytes) {
+    return getLong(bytes, 0);
+  }
+
+  public static long getLong(byte[] bytes, int offset) {
+    long value = 0;
+    for (int i = 0;; ++i) {
+      byte b = bytes[offset + i];
+      long shifted = BYTE_7_RIGHT_BITS_SET & b;// kill leftmost bit
+      shifted <<= 7 * i;
+      value |= shifted;
+      if (b >= 0) {
+        break;
+      }// first bit was 0, so that's the last byte in the VarLong
+    }
+    return value;
+  }
+
+  public static long getLong(InputStream is) throws IOException {
+    long value = 0;
+    int i = 0;
+    int b;
+    do {
+      b = is.read();
+      long shifted = BYTE_7_RIGHT_BITS_SET & b;// kill leftmost bit
+      shifted <<= 7 * i;
+      value |= shifted;
+      ++i;
+    } while (b > Byte.MAX_VALUE);
+    return value;
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/keyvalue/KeyValueToolTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/keyvalue/KeyValueToolTests.java
new file mode 100644
index 0000000..219fc10
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/keyvalue/KeyValueToolTests.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.keyvalue;
+
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueTestUtil;
+import org.apache.hbase.codec.prefixtree.row.TestRows;
+import org.junit.Assert;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+@RunWith(Parameterized.class)
+public class KeyValueToolTests {
+
+  @Parameters
+  public static Collection<Object[]> parameters() {
+    return new TestRows.InMemory().getAllAsObjectArray();
+  }
+
+  private TestRows rows;
+
+  public KeyValueToolTests(TestRows testRows) {
+    this.rows = testRows;
+  }
+
+  @Test
+  public void testRoundTripToBytes() {
+    List<KeyValue> kvs = rows.getInputs();
+    ByteBuffer bb = KeyValueTestUtil.toByteBufferAndRewind(kvs, false);
+    List<KeyValue> roundTrippedKvs = KeyValueTestUtil.rewindThenToList(bb, false);
+    Assert.assertArrayEquals(kvs.toArray(), roundTrippedKvs.toArray());
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/PrefixTreeTestConstants.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/PrefixTreeTestConstants.java
new file mode 100644
index 0000000..675934f
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/PrefixTreeTestConstants.java
@@ -0,0 +1,28 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class PrefixTreeTestConstants {
+
+  public static final byte[]
+  	TEST_CF = Bytes.toBytes("cfDefault");
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/blockmeta/PrefixTreeBlockMetaTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/blockmeta/PrefixTreeBlockMetaTests.java
new file mode 100644
index 0000000..45c6ece
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/blockmeta/PrefixTreeBlockMetaTests.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.blockmeta;
+
+import java.io.ByteArrayOutputStream;
+import java.nio.ByteBuffer;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.junit.Assert;
+import org.junit.Test;
+
+public class PrefixTreeBlockMetaTests {
+
+  static int BLOCK_START = 123;
+
+  static PrefixTreeBlockMeta createSample() {
+    PrefixTreeBlockMeta m = new PrefixTreeBlockMeta();
+    m.setNumMetaBytes(0);
+    m.setNumKeyValueBytes(3195);
+
+    m.setNumRowBytes(0);
+    m.setNumFamilyBytes(3);
+    m.setNumQualifierBytes(12345);
+    m.setNumTimestampBytes(23456);
+    m.setNumMemstoreTimestampBytes(5);
+    m.setNumDataBytes(34567);
+
+    m.setNextNodeOffsetWidth(3);
+    m.setFamilyOffsetWidth(1);
+    m.setQualifierOffsetWidth(2);
+    m.setTimestampIndexWidth(1);
+    m.setMemstoreTimestampIndexWidth(2);
+    m.setDataOffsetWidth(8);
+    m.setDataLengthWidth(3);
+
+    m.setRowTreeDepth(11);
+    m.setMaxRowLength(200);
+    m.setMaxQualifierLength(50);
+
+    m.setMinTimestamp(1318966363481L);
+    m.setTimestampDeltaWidth(3);
+    m.setMinMemstoreTimestamp(100L);
+    m.setMemstoreTimestampDeltaWidth(4);
+
+    m.setAllSameType(false);
+    m.setAllTypes(KeyValue.Type.Delete.getCode());
+
+    m.setNumUniqueRows(88);
+    m.setNumUniqueFamilies(1);
+    m.setNumUniqueQualifiers(56);
+    return m;
+  }
+
+	@Test public void testStreamSerialization(){
+		PrefixTreeBlockMeta original = createSample();
+		ByteArrayOutputStream os = new ByteArrayOutputStream(10000);
+		original.writeVariableBytesToOutputStream(os);
+		ByteBuffer buffer = ByteBuffer.wrap(os.toByteArray());
+		PrefixTreeBlockMeta roundTripped = new PrefixTreeBlockMeta(buffer);
+		Assert.assertTrue(original.equals(roundTripped));
+	}
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestSortedByteArrays.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestSortedByteArrays.java
new file mode 100644
index 0000000..e4a0946
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestSortedByteArrays.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.builder;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.builder.data.TestSortedByteArrays1;
+import org.apache.hbase.codec.prefixtree.builder.data.TestSortedByteArrays2;
+
+import com.google.common.collect.Lists;
+
+public interface TestSortedByteArrays {
+
+  List<byte[]> getInputs();
+  List<byte[]> getOutputs();
+
+  public static class InMemory {
+    public Collection<Object[]> getAllAsObjectArray() {
+      List<Object[]> all = Lists.newArrayList();
+      all.add(new Object[] { new TestSortedByteArrays1() });
+      all.add(new Object[] { new TestSortedByteArrays2() });
+      return all;
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestTreeDepth.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestTreeDepth.java
new file mode 100644
index 0000000..c2cfc53
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TestTreeDepth.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.builder;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.junit.Test;
+import org.mortbay.log.Log;
+
+import com.google.common.collect.Lists;
+
+public class TestTreeDepth {
+
+  @Test
+  public void testSingleNode() {
+    List<String> inputs = Lists.newArrayList("a");
+    testInternal(inputs, 1);
+  }
+
+  @Test
+  public void testSimpleBranch() {
+    List<String> inputs = Lists.newArrayList("a", "aa", "ab");
+    testInternal(inputs, 2);
+  }
+
+  @Test
+  public void testEmptyRoot() {
+    List<String> inputs = Lists.newArrayList("a", "b");
+    testInternal(inputs, 2);
+  }
+
+  @Test
+  public void testRootAsNub() {
+    List<String> inputs = Lists.newArrayList("a", "aa");
+    testInternal(inputs, 2);
+  }
+
+  @Test
+  public void testRootAsNubPlusNub() {
+    List<String> inputs = Lists.newArrayList("a", "aa", "aaa");
+    testInternal(inputs, 3);
+  }
+
+  @Test
+  public void testEmptyRootPlusNub() {
+    List<String> inputs = Lists.newArrayList("a", "aa", "b");
+    testInternal(inputs, 3);
+  }
+
+  @Test
+  public void testSplitDistantAncestor() {
+    List<String> inputs = Lists.newArrayList("a", "ac", "acd", "b");
+    testInternal(inputs, 4);
+  }
+
+  protected void testInternal(List<String> inputs, int expectedTreeDepth) {
+    Log.warn("init logger");
+    Tokenizer builder = new Tokenizer();
+    for (String s : inputs) {
+      System.out.println(s);
+      ByteRange b = new ByteRange(Bytes.toBytes(s));
+      builder.addSorted(b);
+      System.out.println(builder);
+    }
+    Assert.assertEquals(1, builder.getRoot().getNodeDepth());
+    Assert.assertEquals(expectedTreeDepth, builder.getTreeDepth());
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TokenizerTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TokenizerTests.java
new file mode 100644
index 0000000..094cde5
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/TokenizerTests.java
@@ -0,0 +1,76 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.builder;
+
+import java.util.Collection;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerRowSearchResult;
+import org.apache.hbase.util.ByteUtils;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+@RunWith(Parameterized.class)
+public class TokenizerTests {
+
+  @Parameters
+  public static Collection<Object[]> parameters() {
+    return new TestSortedByteArrays.InMemory().getAllAsObjectArray();
+  }
+
+  private List<byte[]> inputs;
+  private Tokenizer builder;
+  private List<byte[]> roundTripped;
+
+  public TokenizerTests(TestSortedByteArrays sortedByteArrays) {
+    this.inputs = sortedByteArrays.getInputs();
+    this.builder = new Tokenizer();
+    for (byte[] array : inputs) {
+      builder.addSorted(new ByteRange(array));
+    }
+    this.roundTripped = builder.getArrays();
+  }
+
+  @Test
+  public void testReaderRoundTrip() {
+    Assert.assertEquals(inputs.size(), roundTripped.size());
+    Assert.assertTrue(ByteUtils.isSorted(roundTripped));
+    Assert.assertTrue(ByteUtils.equals(inputs, roundTripped));
+  }
+
+  @Test
+  public void testSearching() {
+    for (byte[] input : inputs) {
+      TokenizerRowSearchResult resultHolder = new TokenizerRowSearchResult();
+      builder.getNode(resultHolder, input, 0, input.length);
+      TokenizerNode n = resultHolder.getMatchingNode();
+      byte[] output = n.getNewByteArray();
+      Assert.assertTrue(Bytes.equals(input, output));
+    }
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays1.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays1.java
new file mode 100644
index 0000000..ebe3bea
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays1.java
@@ -0,0 +1,51 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.builder.data;
+
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.builder.TestSortedByteArrays;
+import org.apache.hbase.util.bytes.StringByteUtils;
+
+import com.google.common.collect.Lists;
+
+public class TestSortedByteArrays1 implements TestSortedByteArrays {
+
+  static List<byte[]> d = Lists.newArrayList();
+  static {
+    List<String> s = Lists.newArrayList();
+    s.add("abc");// nub
+    s.add("abcde");// leaf
+    s.add("bbc");// causes root to split and have empty token
+    s.add("bbc");// makes numOccurrences=2 on the bbc node
+    s.add("cd");// just to get another node after the numOccurrences=2
+    d = StringByteUtils.getUtf8ByteArrays(s);
+  }
+
+  @Override
+  public List<byte[]> getInputs() {
+    return d;
+  }
+
+  @Override
+  public List<byte[]> getOutputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays2.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays2.java
new file mode 100644
index 0000000..75fff2a
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/builder/data/TestSortedByteArrays2.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.builder.data;
+
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.builder.TestSortedByteArrays;
+import org.apache.hbase.util.bytes.StringByteUtils;
+
+import com.google.common.collect.Lists;
+
+public class TestSortedByteArrays2 implements TestSortedByteArrays {
+
+  static List<byte[]> d = Lists.newArrayList();
+  static {
+    /*
+     * tricky little combination because the acegi token will partially match abdfi, but when you
+     * descend into abdfi, it will not fully match
+     */
+    List<String> s = Lists.newArrayList();
+    s.add("abdfh");
+    s.add("abdfi");
+    s.add("acegi");
+    d = StringByteUtils.getUtf8ByteArrays(s);
+  }
+
+  @Override
+  public List<byte[]> getInputs() {
+    return d;
+  }
+
+  @Override
+  public List<byte[]> getOutputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/ColumnBuilderTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/ColumnBuilderTests.java
new file mode 100644
index 0000000..9291243
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/ColumnBuilderTests.java
@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.column;
+
+import java.util.Collection;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.bytes.ByteRangeUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.decode.column.ColumnReader;
+import org.apache.hbase.codec.prefixtree.encode.column.ColumnWriter;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.util.ByteUtils;
+import org.apache.hbase.util.byterange.impl.ByteRangeTreeSet;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+import com.google.common.collect.Lists;
+
+@RunWith(Parameterized.class)
+public class ColumnBuilderTests {
+
+  @Parameters
+  public static Collection<Object[]> parameters() {
+    return new TestColumns.InMemory().getAllAsObjectArray();
+  }
+
+  /*********** fields **********************************/
+
+  protected TestColumns columns;
+  protected ByteRangeTreeSet columnSorter;
+  protected List<ByteRange> sortedUniqueColumns;
+  protected PrefixTreeBlockMeta blockMeta;
+  protected Tokenizer builder;
+  protected ColumnWriter writer;
+  protected byte[] bytes;
+  protected byte[] buffer;
+  protected ColumnReader reader;
+
+  /*************** construct ****************************/
+
+  public ColumnBuilderTests(TestColumns columns) {
+    this.columns = columns;
+    List<ByteRange> inputs = columns.getInputs();
+    this.columnSorter = new ByteRangeTreeSet(inputs);
+    this.sortedUniqueColumns = columnSorter.compile().getSortedRanges();
+    List<byte[]> copies = ByteRangeUtils.copyToNewArrays(sortedUniqueColumns);
+    Assert.assertTrue(ByteUtils.isSorted(copies));
+    this.blockMeta = new PrefixTreeBlockMeta();
+    this.blockMeta.setNumMetaBytes(0);
+    this.blockMeta.setNumRowBytes(0);
+    this.builder = new Tokenizer();
+  }
+
+  /************* methods ********************************/
+
+  @Test
+  public void testReaderRoundTrip() {
+    for (int i = 0; i < sortedUniqueColumns.size(); ++i) {
+      ByteRange column = sortedUniqueColumns.get(i);
+      builder.addSorted(column);
+    }
+    List<byte[]> builderOutputArrays = builder.getArrays();
+    for (int i = 0; i < builderOutputArrays.size(); ++i) {
+      byte[] inputArray = sortedUniqueColumns.get(i).deepCopyToNewArray();
+      byte[] outputArray = builderOutputArrays.get(i);
+      boolean same = Bytes.equals(inputArray, outputArray);
+      Assert.assertTrue(same);
+    }
+    Assert.assertEquals(sortedUniqueColumns.size(), builderOutputArrays.size());
+
+    writer = new ColumnWriter(blockMeta, builder, false);
+    bytes = writer.compile().writeBytes();
+    buffer = new byte[blockMeta.getMaxQualifierLength()];
+    reader = new ColumnReader(buffer, false);
+    reader.initOnBlock(blockMeta, bytes);
+
+    List<TokenizerNode> builderNodes = Lists.newArrayList();
+    builder.appendNodes(builderNodes, true, true);
+    int i = 0;
+    for (TokenizerNode builderNode : builderNodes) {
+      if (!builderNode.hasOccurrences()) {
+        continue;
+      }
+      Assert.assertEquals(1, builderNode.getNumOccurrences());// we de-duped before adding to
+                                                              // builder
+      int position = builderNode.getOutputArrayOffset();
+      byte[] output = reader.populateBuffer(position).copyBufferToNewArray();
+      boolean same = Bytes.equals(sortedUniqueColumns.get(i).deepCopyToNewArray(), output);
+      Assert.assertTrue(same);
+      ++i;
+    }
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/TestColumns.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/TestColumns.java
new file mode 100644
index 0000000..dff7204
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/TestColumns.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.column;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.column.data.TestColumnsRandom;
+import org.apache.hbase.codec.prefixtree.column.data.TestColumnsSimple;
+
+import com.google.common.collect.Lists;
+
+public interface TestColumns {
+
+  List<ByteRange> getInputs();
+  List<ByteRange> getOutputs();
+
+  public static class InMemory {
+    public Collection<Object[]> getAllAsObjectArray() {
+      List<Object[]> all = Lists.newArrayList();
+      all.add(new Object[] { new TestColumnsSimple() });
+      for (int powerOf2 = 0; powerOf2 < 16; ++powerOf2) {
+        all.add(new Object[] { new TestColumnsRandom(1 << powerOf2) });
+      }
+      return all;
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsRandom.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsRandom.java
new file mode 100644
index 0000000..1cc2331
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsRandom.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.column.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.test.RedundantKVGenerator;
+import org.apache.hbase.codec.prefixtree.column.TestColumns;
+import org.apache.hbase.util.byterange.ByteRangeSet;
+import org.apache.hbase.util.byterange.impl.ByteRangeTreeSet;
+
+import com.google.common.collect.Lists;
+
+public class TestColumnsRandom implements TestColumns {
+
+  private List<ByteRange> inputs = Lists.newArrayList();
+  private List<ByteRange> outputs = Lists.newArrayList();
+
+  public TestColumnsRandom(int numColumns) {
+    RedundantKVGenerator generator = new RedundantKVGenerator();
+    ByteRangeSet sortedColumns = new ByteRangeTreeSet();
+    List<KeyValue> d = generator.generateTestKeyValues(numColumns);
+    for (KeyValue col : d) {
+      ByteRange colRange = new ByteRange(col.getQualifier());
+      inputs.add(colRange);
+      sortedColumns.add(colRange);
+    }
+    for (ByteRange col : sortedColumns.compile().getSortedRanges()) {
+      outputs.add(col);
+    }
+  }
+
+  @Override
+  public List<ByteRange> getInputs() {
+    return inputs;
+  }
+
+  @Override
+  public List<ByteRange> getOutputs() {
+    return outputs;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsSimple.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsSimple.java
new file mode 100644
index 0000000..4f4c038
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/column/data/TestColumnsSimple.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.column.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hadoop.hbase.util.bytes.ByteRangeUtils;
+import org.apache.hbase.codec.prefixtree.column.TestColumns;
+import org.apache.hbase.util.bytes.StringByteUtils;
+
+import com.google.common.collect.Lists;
+
+public class TestColumnsSimple implements TestColumns {
+
+  @Override
+  public List<ByteRange> getInputs() {
+    List<String> d = Lists.newArrayList();
+    d.add("abc");
+    d.add("abcde");
+    d.add("abc");
+    d.add("bbc");
+    d.add("abc");
+    return ByteRangeUtils.fromArrays(StringByteUtils.getUtf8ByteArrays(d));
+  }
+
+  @Override
+  public List<ByteRange> getOutputs() {
+    List<String> d = Lists.newArrayList();
+    d.add("abc");
+    d.add("abcde");
+    d.add("bbc");
+    return ByteRangeUtils.fromArrays(StringByteUtils.getUtf8ByteArrays(d));
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/BaseTestRows.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/BaseTestRows.java
new file mode 100644
index 0000000..53beddb
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/BaseTestRows.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+
+import com.google.common.collect.Lists;
+
+public abstract class BaseTestRows implements TestRows {
+
+  @Override
+  public List<Integer> getRowStartIndexes() {
+    List<Integer> rowStartIndexes = Lists.newArrayList();
+    rowStartIndexes.add(0);
+    List<KeyValue> inputs = getInputs();
+    for (int i = 1; i < inputs.size(); ++i) {
+      KeyValue lastKv = inputs.get(i - 1);
+      KeyValue kv = inputs.get(i);
+      if (!CellComparator.equalsRow(lastKv, kv)) {
+        rowStartIndexes.add(i);
+      }
+    }
+    return rowStartIndexes;
+  }
+
+  @Override
+  public void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta) {
+  }
+
+  @Override
+  public void individualSearcherAssertions(CellSearcher searcher) {
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/RowBuilderTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/RowBuilderTests.java
new file mode 100644
index 0000000..6a8b7f8
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/RowBuilderTests.java
@@ -0,0 +1,194 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.StringUtils;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+import com.google.common.collect.Lists;
+
+@RunWith(Parameterized.class)
+public class RowBuilderTests {
+
+	protected static int BLOCK_START = 7;
+
+	@Parameters
+	public static Collection<Object[]> parameters(){
+	  List<Object[]> parameters = Lists.newArrayList();
+	  for(TestRows testRows : TestRows.InMemory.getAll()){
+      parameters.add(new Object[]{testRows});
+	  }
+		return parameters;
+	}
+
+	protected TestRows rows;
+	protected boolean includeMemstoreTS = false;
+	protected ByteArrayOutputStream os;
+	protected PrefixTreeEncoder encoder;
+	protected int totalBytes;
+	protected PrefixTreeBlockMeta blockMetaWriter;
+	protected byte[] outputBytes;
+	protected ByteBuffer buffer;
+	protected ByteArrayInputStream is;
+	protected PrefixTreeBlockMeta blockMetaReader;
+	protected byte[] inputBytes;
+	protected PrefixTreeArraySearcher searcher;
+
+	public RowBuilderTests(TestRows testRows){
+		this.rows = testRows;
+	}
+
+	@Before 
+	public void compile(){
+    os = new ByteArrayOutputStream(1<<20);
+    encoder = new PrefixTreeEncoder();
+    encoder.setOutputStream(os);
+    encoder.setIncludeMemstoreTimestamp(includeMemstoreTS);
+
+    int counter = 0;
+    for(KeyValue kv : rows.getInputs()){
+      encoder.write(kv);
+//      System.out.println(counter+" "+kv);
+//      System.out.println(encoder.getRowTokenizer());
+      ++counter;
+    }
+    encoder.flush();
+    totalBytes = encoder.getTotalBytes();
+    blockMetaWriter = encoder.getBlockMeta();
+    outputBytes = os.toByteArray();
+
+    //start reading, but save the assertions for @Test methods
+    buffer = ByteBuffer.wrap(outputBytes);
+    blockMetaReader = new PrefixTreeBlockMeta(buffer);
+
+    searcher = new PrefixTreeArraySearcher(blockMetaReader, blockMetaReader.getRowTreeDepth(),
+        blockMetaReader.getMaxRowLength(), blockMetaReader.getMaxQualifierLength());
+    searcher.initOnBlock(blockMetaReader, outputBytes, includeMemstoreTS);
+	}
+
+  @Test
+  public void testEncoderOutput() throws IOException {
+    Assert.assertEquals(totalBytes, outputBytes.length);
+    Assert.assertEquals(blockMetaWriter, blockMetaReader);
+  }
+
+  @Test
+  public void testForwardScanner() {
+    int counter = -1;
+    while (searcher.nextCell()) {
+      ++counter;
+      KeyValue inputKv = rows.getInputs().get(counter);
+      KeyValue outputKv = KeyValueUtils.copyToNewKeyValue(searcher.getCurrentCell());
+//      System.out.println(StringUtils.padFront(counter + "", ' ', 5) + " out:" + outputKv);
+      assertKeyAndValueEqual(inputKv, outputKv);
+    }
+    // assert same number of cells
+    Assert.assertEquals(rows.getInputs().size(), counter + 1);
+  }
+
+
+  /**
+   * probably not needed since testReverseScannerWithJitter() below is more thorough
+   */
+  @Test
+  public void testReverseScanner() {
+    searcher.positionAfterLastCell();
+    int counter = -1;
+    while (searcher.previousCell()) {
+      ++counter;
+      System.out.println(counter+" "+searcher);
+      int oppositeIndex = rows.getInputs().size() - counter - 1;
+      KeyValue inputKv = rows.getInputs().get(oppositeIndex);
+      KeyValue outputKv = KeyValueUtils.copyToNewKeyValue(searcher.getCurrentCell());
+      assertKeyAndValueEqual(inputKv, outputKv);
+    }
+    Assert.assertEquals(rows.getInputs().size(), counter + 1);
+  }
+
+
+  /**
+   * Exercise the nubCellsRemain variable by calling next+previous.  NubCellsRemain is basically
+   * a special fan index.
+   */
+  @Test
+  public void testReverseScannerWithJitter() {
+    searcher.positionAfterLastCell();
+    int counter = -1;
+    while (true) {
+      boolean foundCell = searcher.previousCell();
+      if(!foundCell){
+        break;
+      }
+      ++counter;
+      System.out.println(counter+" "+searcher);
+
+      //a next+previous should cancel out
+      if(!searcher.isAfterLast()){
+        searcher.nextCell();
+        System.out.println(counter+" "+searcher);
+        searcher.previousCell();
+      }
+
+      System.out.println(counter+" "+searcher);
+      int oppositeIndex = rows.getInputs().size() - counter - 1;
+      KeyValue inputKv = rows.getInputs().get(oppositeIndex);
+      KeyValue outputKv = KeyValueUtils.copyToNewKeyValue(searcher.getCurrentCell());
+      assertKeyAndValueEqual(inputKv, outputKv);
+    }
+    Assert.assertEquals(rows.getInputs().size(), counter + 1);
+  }
+
+  @Test
+  public void testIndividualBlockMetaAssertions() {
+    rows.individualBlockMetaAssertions(blockMetaReader);
+  }
+
+
+	/**************** helper **************************/
+
+  protected void assertKeyAndValueEqual(Cell expected, Cell actual) {
+    // assert keys are equal (doesn't compare values)
+    Assert.assertEquals(expected, actual);
+    // assert values equal
+    Assert.assertTrue(Bytes.equals(expected.getValueArray(), expected.getValueOffset(),
+      expected.getValueLength(), actual.getValueArray(), actual.getValueOffset(),
+      actual.getValueLength()));
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestPtSearcher.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestPtSearcher.java
new file mode 100644
index 0000000..9479632
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestPtSearcher.java
@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.Collection;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.apache.hbase.codec.prefixtree.pool.DecoderFactory;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+@RunWith(Parameterized.class)
+public class TestPtSearcher {
+
+	protected static int BLOCK_START = 7;
+
+	@Parameters
+	public static Collection<Object[]> parameters(){
+		return new TestRows.InMemory().getAllAsObjectArray();
+	}
+
+	protected TestRows rows;
+	protected ByteBuffer block;
+
+	public TestPtSearcher(TestRows testRows){
+		this.rows = testRows;
+		ByteArrayOutputStream os = new ByteArrayOutputStream(1<<20);
+		PrefixTreeEncoder kvBuilder = new PrefixTreeEncoder();
+		kvBuilder.setOutputStream(os);
+		kvBuilder.setIncludeMemstoreTimestamp(false);
+		for(KeyValue kv : rows.getInputs()){
+			kvBuilder.write(kv);
+		}
+		kvBuilder.flush();
+		byte[] outputBytes = os.toByteArray();
+		this.block = ByteBuffer.wrap(outputBytes);
+	}
+
+	
+  @Test
+  public void testScanForwards() throws IOException {
+    CellSearcher searcher = null;
+    try {
+      searcher = DecoderFactory.checkOut(block, true);
+
+      int i = -1;
+      while (searcher.nextCell()) {
+        ++i;
+        KeyValue inputCell = rows.getInputs().get(i);
+        Cell outputCell = searcher.getCurrentCell();
+
+        // check all 3 permutations of equals()
+        Assert.assertEquals(inputCell, outputCell);
+        Assert.assertEquals(outputCell, inputCell);
+        Assert.assertTrue(CellComparator.equals(inputCell, outputCell));
+      }
+      Assert.assertEquals(rows.getInputs().size(), i + 1);
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+
+  @Test
+  public void testScanBackwards() throws IOException {
+    CellSearcher searcher = null;
+    try {
+      searcher = DecoderFactory.checkOut(block, true);
+      searcher.positionAfterLastCell();
+      int i = -1;
+      while (searcher.previousCell()) {
+        ++i;
+        int oppositeIndex = rows.getInputs().size() - i - 1;
+        KeyValue inputKv = rows.getInputs().get(oppositeIndex);
+        KeyValue outputKv = KeyValueUtils.copyToNewKeyValue(searcher.getCurrentCell());
+        Assert.assertEquals(inputKv, outputKv);
+      }
+      Assert.assertEquals(rows.getInputs().size(), i + 1);
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+
+  @Test
+  public void testRandomSeekHits() throws IOException {
+    CellSearcher searcher = null;
+    try {
+      searcher = DecoderFactory.checkOut(block, true);
+      for (KeyValue kv : rows.getInputs()) {
+        boolean hit = searcher.positionAt(kv);
+        Assert.assertTrue(hit);
+        Cell foundKv = searcher.getCurrentCell();
+        Assert.assertTrue(CellComparator.equals(kv, foundKv));
+      }
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+  /**
+   * very hard to test nubs with this thing since the a nextRowKey function will usually skip them
+   */
+  @Test
+  public void testRandomSeekMisses() throws IOException {
+    CellSearcher searcher = null;
+    List<Integer> rowStartIndexes = rows.getRowStartIndexes();
+    try {
+      searcher = DecoderFactory.checkOut(block, true);
+      for (int i=0; i < rows.getInputs().size(); ++i) {
+        KeyValue kv = rows.getInputs().get(i);
+        System.out.println(i);
+        System.out.println(kv);
+
+        //nextRow
+        KeyValue inputNextRow = KeyValueUtils.createFirstKeyInNextRow(kv);
+        System.out.println(inputNextRow);
+        //expect a hit if we are not on the last row
+//        boolean expectedHit = i < CollectionUtils.getLast(rowStartIndexes);
+//        boolean hit = searcher.positionAt(inputNextRow);
+//        Assert.assertEquals(expectedHit, hit);
+
+        CellScannerPosition position = searcher.positionAtOrBefore(inputNextRow);
+        System.out.println(searcher.getCurrentCell());
+        boolean isFirstInRow = rowStartIndexes.contains(i);
+        if(isFirstInRow){
+          int rowIndex = rowStartIndexes.indexOf(i);
+          if(rowIndex < rowStartIndexes.size() - 1){
+            int lastKvInRowI = rowStartIndexes.get(rowIndex + 1) - 1;
+            Assert.assertEquals(CellScannerPosition.BEFORE, position);
+            /*
+             * can't get this to work between nubs like rowB\x00 <-> rowBB
+             */
+//            Assert.assertEquals(rows.getInputs().get(lastKvInRowI), searcher.getCurrentCell());
+          }
+        }
+
+        //previous KV
+        KeyValue inputPreviousKv = KeyValueUtils.previousKey(kv);
+        boolean hit = searcher.positionAt(inputPreviousKv);
+        Assert.assertFalse(hit);
+        position = searcher.positionAtOrAfter(inputPreviousKv);
+        if(CollectionUtils.isLastIndex(rows.getInputs(), i)){
+          Assert.assertTrue(CellScannerPosition.AFTER_LAST == position);
+        }else{
+          Assert.assertTrue(CellScannerPosition.AFTER == position);
+          /*
+           * arghh - why i+1 instead of i?
+           */
+          Assert.assertEquals(rows.getInputs().get(i+1), searcher.getCurrentCell());
+        }
+      }
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+
+
+  @Test
+  public void testRandomSeekIndividualAssertions() throws IOException {
+    CellSearcher searcher = null;
+    try {
+      searcher = DecoderFactory.checkOut(block, true);
+      rows.individualSearcherAssertions(searcher);
+    } finally {
+      DecoderFactory.checkIn(searcher);
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestRows.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestRows.java
new file mode 100644
index 0000000..e4e36ac
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/TestRows.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.row.data.TestRandomKeyValues;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsComplexQualifiers;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsDeeper;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsDifferentTimestamps;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsEmpty;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsExerciseFInts;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsMultiFamilies;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsNub;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsNumberStrings;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsQualifierByteOrdering;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsSearcherRowMiss;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsSimple;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsSingleQualifier;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsTrivial;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsUrls;
+import org.apache.hbase.codec.prefixtree.row.data.TestRowsUrlsExample;
+
+import com.google.common.collect.Lists;
+
+public interface TestRows {
+
+  List<KeyValue> getInputs();
+  List<Integer> getRowStartIndexes();
+
+  void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta);
+
+  void individualSearcherAssertions(CellSearcher searcher);
+
+  public static class InMemory {
+
+    public static Collection<TestRows> getAll() {
+      List<TestRows> all = Lists.newArrayList();
+      //simple
+      all.add(new TestRowsEmpty());
+      all.add(new TestRowsTrivial());
+      all.add(new TestRowsSimple());
+      all.add(new TestRowsDeeper());
+      
+      //more specific
+      all.add(new TestRowsSingleQualifier());
+      all.add(new TestRowsMultiFamilies());
+      all.add(new TestRowsNub());
+      all.add(new TestRowsSearcherRowMiss());
+      all.add(new TestRowsQualifierByteOrdering());
+      all.add(new TestRowsComplexQualifiers());
+      all.add(new TestRowsDifferentTimestamps());
+
+      //larger data volumes (hard to debug)
+      all.add(new TestRowsNumberStrings());
+      all.add(new TestRowsUrls());
+      all.add(new TestRowsUrlsExample());
+      all.add(new TestRowsExerciseFInts());
+      all.add(new TestRandomKeyValues());
+      return all;
+    }
+
+    public static Collection<Object[]> getAllAsObjectArray() {
+      List<Object[]> all = Lists.newArrayList();
+      for (TestRows testRows : getAll()) {
+        all.add(new Object[] { testRows });
+      }
+      return all;
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRandomKeyValues.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRandomKeyValues.java
new file mode 100644
index 0000000..537dceb
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRandomKeyValues.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.test.RedundantKVGenerator;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRandomKeyValues extends BaseTestRows {
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static RedundantKVGenerator generator = new RedundantKVGenerator();
+  static {
+    d = generator.generateTestKeyValues(1 << 10);
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsComplexQualifiers.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsComplexQualifiers.java
new file mode 100644
index 0000000..79ba466
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsComplexQualifiers.java
@@ -0,0 +1,68 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsComplexQualifiers extends BaseTestRows{
+
+	static byte[] 
+        Arow = Bytes.toBytes("Arow"),
+        cf = PrefixTreeTestConstants.TEST_CF,
+        v0 = Bytes.toBytes("v0");
+
+	static List<byte[]> qualifiers = Lists.newArrayList();
+	static{
+		List<String> qualifierStrings = Lists.newArrayList();
+		qualifierStrings.add("cq");
+		qualifierStrings.add("cq0");
+		qualifierStrings.add("cq1");
+		qualifierStrings.add("cq2");
+		qualifierStrings.add("dq0");//second root level fan
+		qualifierStrings.add("dq1");//nub
+		qualifierStrings.add("dq111");//leaf on nub
+		qualifierStrings.add("dq11111a");//leaf on leaf
+		for(String s : qualifierStrings){
+			qualifiers.add(Bytes.toBytes(s));
+		}
+	}
+
+	static long
+		ts = 55L;
+
+	static List<KeyValue> d = Lists.newArrayList();
+	static{
+		for(byte[] qualifier : qualifiers){
+			d.add(new KeyValue(Arow, cf, qualifier, ts, v0));
+		}
+	}
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return d;
+	}
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDeeper.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDeeper.java
new file mode 100644
index 0000000..6f89704
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDeeper.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsDeeper extends BaseTestRows{
+
+	static byte[] 
+        cdc = Bytes.toBytes("cdc"),
+        cf6 = Bytes.toBytes("cf6"),
+        cfc = Bytes.toBytes("cfc"),
+        f = Bytes.toBytes("f"),
+        q = Bytes.toBytes("q"),
+        v = Bytes.toBytes("v");
+
+	static long
+		ts = 55L;
+
+	static List<KeyValue> d = Lists.newArrayList();
+	static{
+		d.add(new KeyValue(cdc, f, q, ts, v));
+    d.add(new KeyValue(cf6, f, q, ts, v));
+    d.add(new KeyValue(cfc, f, q, ts, v));
+	}
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return d;
+	}
+
+	@Override
+	public void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta) {
+	  //0: token:c; fan:d,f
+	  //1: token:f; fan:6,c
+	  //2: leaves
+		Assert.assertEquals(3, blockMeta.getRowTreeDepth());
+	}
+
+  @Override
+  public void individualSearcherAssertions(CellSearcher searcher) {
+    /**
+     * The searcher should get a token mismatch on the "r" branch.  Assert that it skips not only
+     * rA, but rB as well.
+     */
+    KeyValue cfcRow = KeyValue.createFirstOnRow(Bytes.toBytes("cfc"));
+    CellScannerPosition position = searcher.positionAtOrAfter(cfcRow);
+    Assert.assertEquals(CellScannerPosition.AFTER, position);
+    Assert.assertEquals(d.get(2), searcher.getCurrentCell());
+    searcher.previousCell();
+    Assert.assertEquals(d.get(1), searcher.getCurrentCell());
+  }
+}
+
+
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDifferentTimestamps.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDifferentTimestamps.java
new file mode 100644
index 0000000..33826c5
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsDifferentTimestamps.java
@@ -0,0 +1,70 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+/*
+ * test different timestamps
+ */
+public class TestRowsDifferentTimestamps extends BaseTestRows{
+
+	static byte[] 
+        Arow = Bytes.toBytes("Arow"),
+        Brow = Bytes.toBytes("Brow"),
+        cf = Bytes.toBytes("fammy"),
+        cq0 = Bytes.toBytes("cq0"),
+        cq1 = Bytes.toBytes("cq1"),
+        v0 = Bytes.toBytes("v0");
+
+	static List<KeyValue> d = Lists.newArrayList();
+	static{
+		d.add(new KeyValue(Arow, cf, cq0, 0L, v0));
+		d.add(new KeyValue(Arow, cf, cq1, 1L, v0));
+		d.add(new KeyValue(Brow, cf, cq0, 12345678L, v0));
+		//watch out... Long.MAX_VALUE comes back as 1332221664203, even with other encoders
+//		d.add(new KeyValue(Brow, cf, cq1, Long.MAX_VALUE, v0));
+    d.add(new KeyValue(Brow, cf, cq1, Long.MAX_VALUE-1, v0));
+    d.add(new KeyValue(Brow, cf, cq1, 999999999, v0));
+    d.add(new KeyValue(Brow, cf, cq1, 12345, v0));
+	}
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return d;
+	}
+
+	@Override
+	public void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta) {
+		Assert.assertFalse(blockMeta.isAllSameTimestamp());
+		Assert.assertNotNull(blockMeta.getMinTimestamp());
+		Assert.assertTrue(blockMeta.getTimestampIndexWidth() > 0);
+		Assert.assertTrue(blockMeta.getTimestampDeltaWidth() > 0);
+	}
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsEmpty.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsEmpty.java
new file mode 100644
index 0000000..3eac3c4
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsEmpty.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.Type;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsEmpty extends BaseTestRows{
+  
+  private static byte[] b = new byte[0];
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(b, b, b, 0L, Type.Put, b));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+  
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsExerciseFInts.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsExerciseFInts.java
new file mode 100644
index 0000000..e72a858
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsExerciseFInts.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+import org.apache.hbase.util.byterange.impl.ByteRangeTreeSet;
+
+import com.google.common.collect.Lists;
+
+/*
+ * test different timestamps
+ *
+ * http://pastebin.com/7ks8kzJ2
+ * http://pastebin.com/MPn03nsK
+ */
+public class TestRowsExerciseFInts extends BaseTestRows{
+
+	static List<ByteRange> rows;
+	static{
+		List<String> rowStrings = new ArrayList<String>();
+        rowStrings.add("com.edsBlog/directoryAa/pageAaa");
+        rowStrings.add("com.edsBlog/directoryAa/pageBbb");
+        rowStrings.add("com.edsBlog/directoryAa/pageCcc");
+        rowStrings.add("com.edsBlog/directoryAa/pageDdd");
+        rowStrings.add("com.edsBlog/directoryBb/pageEee");
+        rowStrings.add("com.edsBlog/directoryBb/pageFff");
+        rowStrings.add("com.edsBlog/directoryBb/pageGgg");
+        rowStrings.add("com.edsBlog/directoryBb/pageHhh");
+        rowStrings.add("com.isabellasBlog/directoryAa/pageAaa");
+        rowStrings.add("com.isabellasBlog/directoryAa/pageBbb");
+        rowStrings.add("com.isabellasBlog/directoryAa/pageCcc");
+        rowStrings.add("com.isabellasBlog/directoryAa/pageDdd");
+        rowStrings.add("com.isabellasBlog/directoryBb/pageEee");
+        rowStrings.add("com.isabellasBlog/directoryBb/pageFff");
+        rowStrings.add("com.isabellasBlog/directoryBb/pageGgg");
+        rowStrings.add("com.isabellasBlog/directoryBb/pageHhh");
+        ByteRangeTreeSet ba = new ByteRangeTreeSet();
+        for(String row : rowStrings){
+        	ba.add(new ByteRange(Bytes.toBytes(row)));
+        }
+        rows = ba.compile().getSortedRanges();
+	}
+
+	static List<String> cols = Lists.newArrayList();
+	static{
+		cols.add("Chrome");
+		cols.add("Chromeb");
+		cols.add("Firefox");
+		cols.add("InternetExplorer");
+		cols.add("Opera");
+		cols.add("Safari");
+		cols.add("Z1stBrowserWithHuuuuuuuuuuuugeQualifier");
+		cols.add("Z2ndBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z3rdBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z4thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z5thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z6thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z7thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z8thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+		cols.add("Z9thBrowserWithEvenBiggerQualifierMoreMoreMoreMoreMore");
+	}
+
+  static long ts = 1234567890;
+
+  static int MAX_VALUE = 50;
+
+  static List<KeyValue> kvs = Lists.newArrayList();
+  static {
+    for (ByteRange row : rows) {
+      for (String col : cols) {
+        KeyValue kv = new KeyValue(row.deepCopyToNewArray(), PrefixTreeTestConstants.TEST_CF,
+            Bytes.toBytes(col), ts, KeyValue.Type.Put, Bytes.toBytes("VALUE"));
+        kvs.add(kv);
+      }
+    }
+  }
+
+  public static void main(String... args) {
+    for (KeyValue kv : kvs) {
+      System.out.println(kv);
+    }
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return kvs;
+  }
+
+  @Override
+  public void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta) {
+    Assert.assertTrue(blockMeta.getNextNodeOffsetWidth() > 1);
+    Assert.assertTrue(blockMeta.getQualifierOffsetWidth() > 1);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsMultiFamilies.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsMultiFamilies.java
new file mode 100644
index 0000000..b01ae85
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsMultiFamilies.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsMultiFamilies extends BaseTestRows{
+
+  static byte[] 
+        rowA = Bytes.toBytes("rowA"),
+        rowB = Bytes.toBytes("rowB"),
+        famA = Bytes.toBytes("famA"),
+        famB = Bytes.toBytes("famB"),
+        famBB = Bytes.toBytes("famBB"),
+        q0 = Bytes.toBytes("q0"),
+        q1 = Bytes.toBytes("q1"),//start with a different character
+        vvv = Bytes.toBytes("vvv");
+
+  static long ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(rowA, famA, q0, ts, vvv));
+    d.add(new KeyValue(rowA, famB, q1, ts, vvv));
+    d.add(new KeyValue(rowA, famBB, q0, ts, vvv));
+    d.add(new KeyValue(rowB, famA, q0, ts, vvv));
+    d.add(new KeyValue(rowB, famA, q1, ts, vvv));
+    d.add(new KeyValue(rowB, famB, q0, ts, vvv));
+    d.add(new KeyValue(rowB, famBB, q0, ts, vvv));
+    d.add(new KeyValue(rowB, famBB, q1, ts, vvv));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNub.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNub.java
new file mode 100644
index 0000000..c54a4a8
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNub.java
@@ -0,0 +1,59 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsNub extends BaseTestRows{
+
+	static byte[]
+        rowA = Bytes.toBytes("rowA"),
+        rowB = Bytes.toBytes("rowB"),//nub
+        rowBB = Bytes.toBytes("rowBB"),
+        cf = PrefixTreeTestConstants.TEST_CF,
+        cq0 = Bytes.toBytes("cq0"),
+        cq1 = Bytes.toBytes("cq1"),
+        v0 = Bytes.toBytes("v0");
+
+	static long
+		ts = 55L;
+
+	static List<KeyValue> d = Lists.newArrayList();
+	static{
+		d.add(new KeyValue(rowA, cf, cq0, ts, v0));
+		d.add(new KeyValue(rowA, cf, cq1, ts, v0));
+		d.add(new KeyValue(rowB, cf, cq0, ts, v0));
+		d.add(new KeyValue(rowB, cf, cq1, ts, v0));
+		d.add(new KeyValue(rowBB, cf, cq0, ts, v0));
+		d.add(new KeyValue(rowBB, cf, cq1, ts, v0));
+	}
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return d;
+	}
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNumberStrings.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNumberStrings.java
new file mode 100644
index 0000000..2f9c6b8
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsNumberStrings.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValue.Type;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsNumberStrings extends BaseTestRows{
+  
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+
+  /**
+   * Test a string-encoded list of numbers.  0, 1, 10, 11 will sort as 0, 1, 10, 11 if strings
+   * 
+   * This helped catch a bug with reverse scanning where it was jumping from the last leaf cell to
+   * the previous nub.  It should do 11->10, but it was incorrectly doing 11->1
+   */
+    List<Integer> problematicSeries = Lists.newArrayList(0, 1, 10, 11);//sort this at the end
+    for(Integer i : problematicSeries){
+//    for(int i=0; i < 13; ++i){
+      byte[] row = Bytes.toBytes(""+i);
+      byte[] family = Bytes.toBytes("F");
+      byte[] column = Bytes.toBytes("C");
+      byte[] value = Bytes.toBytes("V");
+      
+      d.add(new KeyValue(row, family, column, 0L, Type.Put, value));
+    }
+    Collections.sort(d, new CellComparator());
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+  
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsQualifierByteOrdering.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsQualifierByteOrdering.java
new file mode 100644
index 0000000..abe0679
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsQualifierByteOrdering.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsQualifierByteOrdering extends BaseTestRows{
+
+	static byte[]
+        Arow = Bytes.toBytes("Arow"),
+        Brow = Bytes.toBytes("Brow"),
+        Brow2 = Bytes.toBytes("Brow2"),
+        fam = Bytes.toBytes("HappyFam"),
+        cq0 = Bytes.toBytes("cq0"),
+        cq1 = Bytes.toBytes("cq1tail"),//make sure tail does not come back as liat
+        cq2 = Bytes.toBytes("cq2"),
+        v0 = Bytes.toBytes("v0");
+
+  static long ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(Arow, fam, cq0, ts, v0));
+    d.add(new KeyValue(Arow, fam, cq1, ts, v0));
+    d.add(new KeyValue(Brow, fam, cq0, ts, v0));
+    d.add(new KeyValue(Brow, fam, cq2, ts, v0));
+    d.add(new KeyValue(Brow2, fam, cq1, ts, v0));
+    d.add(new KeyValue(Brow2, fam, cq2, ts, v0));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSearcherRowMiss.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSearcherRowMiss.java
new file mode 100644
index 0000000..4c76a63
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSearcherRowMiss.java
@@ -0,0 +1,124 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsSearcherRowMiss extends BaseTestRows{
+
+  static byte[]
+      //don't let the rows share any common prefix bytes
+      A = Bytes.toBytes("A"),
+      AA = Bytes.toBytes("AA"),
+      AAA = Bytes.toBytes("AAA"),
+      B = Bytes.toBytes("B"),
+      cf = Bytes.toBytes("fam"),
+      cq = Bytes.toBytes("cq0"),
+      v = Bytes.toBytes("v0");
+
+  static long
+    ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static{
+    d.add(new KeyValue(A, cf, cq, ts, v));
+    d.add(new KeyValue(AA, cf, cq, ts, v));
+    d.add(new KeyValue(AAA, cf, cq, ts, v));
+    d.add(new KeyValue(B, cf, cq, ts, v));
+  }
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return d;
+	}
+
+	@Override
+	public void individualSearcherAssertions(CellSearcher searcher) {
+	  assertRowOffsetsCorrect();
+
+    searcher.resetToBeforeFirstEntry();
+
+    //test first cell
+    searcher.nextCell();
+    Cell first = searcher.getCurrentCell();
+    Assert.assertTrue(CellComparator.equals(d.get(0), first));
+
+    //test first cell in second row
+    Assert.assertTrue(searcher.positionAt(d.get(1)));
+    Assert.assertTrue(CellComparator.equals(d.get(1), searcher.getCurrentCell()));
+
+    testBetween1and2(searcher);
+    testBetween2and3(searcher);
+  }
+
+	/************ private methods, call from above *******************/
+
+	private void assertRowOffsetsCorrect(){
+	  Assert.assertEquals(4, getRowStartIndexes().size());
+	}
+
+	private void testBetween1and2(CellSearcher searcher){
+    CellScannerPosition p;//reuse
+    Cell betweenAAndAAA = new KeyValue(AA, cf, cq, ts-2, v);
+
+    //test exact
+    Assert.assertFalse(searcher.positionAt(betweenAAndAAA));
+
+    //test atOrBefore
+    p = searcher.positionAtOrBefore(betweenAAndAAA);
+    Assert.assertEquals(CellScannerPosition.BEFORE, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(1)));
+
+    //test atOrAfter
+    p = searcher.positionAtOrAfter(betweenAAndAAA);
+    Assert.assertEquals(CellScannerPosition.AFTER, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(2)));
+	}
+
+  private void testBetween2and3(CellSearcher searcher){
+    CellScannerPosition p;//reuse
+    Cell betweenAAAndB = new KeyValue(AAA, cf, cq, ts-2, v);
+
+    //test exact
+    Assert.assertFalse(searcher.positionAt(betweenAAAndB));
+
+    //test atOrBefore
+    p = searcher.positionAtOrBefore(betweenAAAndB);
+    Assert.assertEquals(CellScannerPosition.BEFORE, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(2)));
+
+    //test atOrAfter
+    p = searcher.positionAtOrAfter(betweenAAAndB);
+    Assert.assertEquals(CellScannerPosition.AFTER, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(3)));
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSimple.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSimple.java
new file mode 100644
index 0000000..7bf63fb
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSimple.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.Cell;
+import org.apache.hadoop.hbase.cell.comparator.CellComparator;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsSimple extends BaseTestRows {
+
+  static byte[]
+  // don't let the rows share any common prefix bytes
+      rowA = Bytes.toBytes("Arow"),
+      rowB = Bytes.toBytes("Brow"), cf = Bytes.toBytes("fam"),
+      cq0 = Bytes.toBytes("cq0"),
+      cq1 = Bytes.toBytes("cq1tail"),// make sure tail does not come back as liat
+      cq2 = Bytes.toBytes("dcq2"),// start with a different character
+      v0 = Bytes.toBytes("v0");
+
+  static long ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(rowA, cf, cq0, ts, v0));
+    d.add(new KeyValue(rowA, cf, cq1, ts, v0));
+    d.add(new KeyValue(rowA, cf, cq2, ts, v0));
+    d.add(new KeyValue(rowB, cf, cq0, ts, v0));
+    d.add(new KeyValue(rowB, cf, cq1, ts, v0));
+    d.add(new KeyValue(rowB, cf, cq2, ts, v0));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+  @Override
+  public void individualSearcherAssertions(CellSearcher searcher) {
+    CellScannerPosition p;// reuse
+    searcher.resetToBeforeFirstEntry();
+
+    // test first cell
+    searcher.nextCell();
+    Cell first = searcher.getCurrentCell();
+    Assert.assertTrue(CellComparator.equals(d.get(0), first));
+
+    // test first cell in second row
+    Assert.assertTrue(searcher.positionAt(d.get(3)));
+    Assert.assertTrue(CellComparator.equals(d.get(3), searcher.getCurrentCell()));
+
+    Cell between4And5 = new KeyValue(rowB, cf, cq1, ts - 2, v0);
+
+    // test exact
+    Assert.assertFalse(searcher.positionAt(between4And5));
+
+    // test atOrBefore
+    p = searcher.positionAtOrBefore(between4And5);
+    Assert.assertEquals(CellScannerPosition.BEFORE, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(4)));
+
+    // test atOrAfter
+    p = searcher.positionAtOrAfter(between4And5);
+    Assert.assertEquals(CellScannerPosition.AFTER, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(5)));
+
+    // test when key falls before first key in block
+    Cell beforeFirst = new KeyValue(Bytes.toBytes("A"), cf, cq0, ts, v0);
+    Assert.assertFalse(searcher.positionAt(beforeFirst));
+    p = searcher.positionAtOrBefore(beforeFirst);
+    Assert.assertEquals(CellScannerPosition.BEFORE_FIRST, p);
+    p = searcher.positionAtOrAfter(beforeFirst);
+    Assert.assertEquals(CellScannerPosition.AFTER, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), d.get(0)));
+    Assert.assertEquals(d.get(0), searcher.getCurrentCell());
+
+    // test when key falls after last key in block
+    Cell afterLast = new KeyValue(Bytes.toBytes("z"), cf, cq0, ts, v0);// must be lower case z
+    Assert.assertFalse(searcher.positionAt(afterLast));
+    p = searcher.positionAtOrAfter(afterLast);
+    Assert.assertEquals(CellScannerPosition.AFTER_LAST, p);
+    p = searcher.positionAtOrBefore(afterLast);
+    System.out.println(searcher.getCurrentCell());
+    Assert.assertEquals(CellScannerPosition.BEFORE, p);
+    Assert.assertTrue(CellComparator.equals(searcher.getCurrentCell(), CollectionUtils.getLast(d)));
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSingleQualifier.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSingleQualifier.java
new file mode 100644
index 0000000..e58654d
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsSingleQualifier.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsSingleQualifier extends BaseTestRows{
+
+	static byte[] 
+        rowA = Bytes.toBytes("rowA"),
+        rowB = Bytes.toBytes("rowB"),
+        cf = PrefixTreeTestConstants.TEST_CF,
+        cq0 = Bytes.toBytes("cq0"),
+        v0 = Bytes.toBytes("v0");
+
+  static long ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(rowA, cf, cq0, ts, v0));
+    d.add(new KeyValue(rowB, cf, cq0, ts, v0));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsTrivial.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsTrivial.java
new file mode 100644
index 0000000..10029d2
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsTrivial.java
@@ -0,0 +1,74 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.List;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.cell.scanner.CellScannerPosition;
+import org.apache.hadoop.hbase.cell.scanner.CellSearcher;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+public class TestRowsTrivial extends BaseTestRows{
+
+	static byte[] 
+        rA = Bytes.toBytes("rA"),
+        rB = Bytes.toBytes("rB"),//turn "r" into a branch for the Searcher tests
+        cf = Bytes.toBytes("fam"),
+        cq0 = Bytes.toBytes("q0"),
+        v0 = Bytes.toBytes("v0");
+
+  static long ts = 55L;
+
+  static List<KeyValue> d = Lists.newArrayList();
+  static {
+    d.add(new KeyValue(rA, cf, cq0, ts, v0));
+    d.add(new KeyValue(rB, cf, cq0, ts, v0));
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return d;
+  }
+
+  @Override
+  public void individualBlockMetaAssertions(PrefixTreeBlockMeta blockMeta) {
+    // node[0] -> root[r]
+    // node[1] -> leaf[A], etc
+    Assert.assertEquals(2, blockMeta.getRowTreeDepth());
+  }
+
+  @Override
+  public void individualSearcherAssertions(CellSearcher searcher) {
+    /**
+     * The searcher should get a token mismatch on the "r" branch. Assert that it skips not only rA,
+     * but rB as well.
+     */
+    KeyValue afterLast = KeyValue.createFirstOnRow(Bytes.toBytes("zzz"));
+    CellScannerPosition position = searcher.positionAtOrAfter(afterLast);
+    Assert.assertEquals(CellScannerPosition.AFTER_LAST, position);
+    Assert.assertNull(searcher.getCurrentCell());
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrls.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrls.java
new file mode 100644
index 0000000..eeeb15f
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrls.java
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+import org.apache.hbase.util.byterange.impl.ByteRangeTreeSet;
+
+import com.google.common.collect.Lists;
+
+/*
+ * test different timestamps
+ * 
+ * http://pastebin.com/7ks8kzJ2
+ * http://pastebin.com/MPn03nsK
+ */
+public class TestRowsUrls extends BaseTestRows{
+
+	static List<ByteRange> rows;
+	static{
+    List<String> rowStrings = new ArrayList<String>();
+    rowStrings.add("com.edsBlog/directoryAa/pageAaa");
+    rowStrings.add("com.edsBlog/directoryAa/pageBbb");
+    rowStrings.add("com.edsBlog/directoryAa/pageCcc");
+    rowStrings.add("com.edsBlog/directoryAa/pageDdd");
+    rowStrings.add("com.edsBlog/directoryBb/pageEee");
+    rowStrings.add("com.edsBlog/directoryBb/pageFff");
+    rowStrings.add("com.edsBlog/directoryBb/pageGgg");
+    rowStrings.add("com.edsBlog/directoryBb/pageHhh");
+    rowStrings.add("com.isabellasBlog/directoryAa/pageAaa");
+    rowStrings.add("com.isabellasBlog/directoryAa/pageBbb");
+    rowStrings.add("com.isabellasBlog/directoryAa/pageCcc");
+    rowStrings.add("com.isabellasBlog/directoryAa/pageDdd");
+    rowStrings.add("com.isabellasBlog/directoryBb/pageEee");
+    rowStrings.add("com.isabellasBlog/directoryBb/pageFff");
+    rowStrings.add("com.isabellasBlog/directoryBb/pageGgg");
+    rowStrings.add("com.isabellasBlog/directoryBb/pageHhh");
+    ByteRangeTreeSet ba = new ByteRangeTreeSet();
+    for (String row : rowStrings) {
+      ba.add(new ByteRange(Bytes.toBytes(row)));
+    }
+    rows = ba.compile().getSortedRanges();
+  }
+
+  static List<String> cols = Lists.newArrayList();
+  static {
+    cols.add("Chrome");
+    cols.add("Chromeb");
+    cols.add("Firefox");
+    cols.add("InternetExplorer");
+    cols.add("Opera");
+    cols.add("Safari");
+  }
+
+  static long ts = 1234567890;
+
+  static int MAX_VALUE = 50;
+
+  static List<KeyValue> kvs = Lists.newArrayList();
+  static {
+    for (ByteRange row : rows) {
+      for (String col : cols) {
+        KeyValue kv = new KeyValue(row.deepCopyToNewArray(), PrefixTreeTestConstants.TEST_CF,
+            Bytes.toBytes(col), ts, KeyValue.Type.Put, Bytes.toBytes("VALUE"));
+        kvs.add(kv);
+        // System.out.println("TestRows5:"+kv);
+      }
+    }
+  }
+
+  public static void main(String... args) {
+    for (KeyValue kv : kvs) {
+      System.out.println(kv);
+    }
+  }
+
+  @Override
+  public List<KeyValue> getInputs() {
+    return kvs;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrlsExample.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrlsExample.java
new file mode 100644
index 0000000..a031909
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/row/data/TestRowsUrlsExample.java
@@ -0,0 +1,125 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.row.data;
+
+import java.io.ByteArrayOutputStream;
+import java.util.List;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueTestUtil;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder;
+import org.apache.hbase.codec.prefixtree.encode.column.ColumnNodeWriter;
+import org.apache.hbase.codec.prefixtree.encode.row.RowNodeWriter;
+import org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode;
+import org.apache.hbase.codec.prefixtree.row.BaseTestRows;
+
+import com.google.common.collect.Lists;
+
+/*
+ * test different timestamps
+ * 
+ * http://pastebin.com/7ks8kzJ2
+ * http://pastebin.com/MPn03nsK
+ */
+public class TestRowsUrlsExample extends BaseTestRows{
+
+  static String TENANT_ID = Integer.toString(95322);
+  static String APP_ID = Integer.toString(12);
+  static List<String> URLS = Lists.newArrayList(
+      "com.dablog/2011/10/04/boating", 
+      "com.dablog/2011/10/09/lasers", 
+      "com.jamiesrecipes", //this nub helped find a bug
+      "com.jamiesrecipes/eggs");
+  static String FAMILY = "hits";
+  static List<String> BROWSERS = Lists.newArrayList(
+      "Chrome", "IE8", "IE9beta");//, "Opera", "Safari");
+	static long TIMESTAMP = 1234567890;
+
+	static int MAX_VALUE = 50;
+
+	static List<KeyValue> kvs = Lists.newArrayList();
+	static{
+		for(String rowKey : URLS){
+			for(String qualifier : BROWSERS){
+			  KeyValue kv = new KeyValue(
+						Bytes.toBytes(rowKey), 
+						Bytes.toBytes(FAMILY), 
+						Bytes.toBytes(qualifier), 
+						TIMESTAMP, 
+						KeyValue.Type.Put, 
+						Bytes.toBytes("VvvV"));
+				kvs.add(kv);
+			}
+		}
+	}
+
+	/**
+	 * Used for generating docs.
+	 */
+	public static void main(String... args){
+    System.out.println("-- inputs --");
+    System.out.println(KeyValueTestUtil.toStringWithPadding(kvs, true));
+		ByteArrayOutputStream os = new ByteArrayOutputStream(1<<20);
+    PrefixTreeEncoder kvBuilder = new PrefixTreeEncoder();
+    kvBuilder.setOutputStream(os);
+    kvBuilder.setIncludeMemstoreTimestamp(false);
+
+    for(KeyValue kv : kvs){
+      kvBuilder.write(kv);
+    }
+    kvBuilder.flush();
+
+    System.out.println("-- qualifier SortedPtBuilderNodes --");
+    for(TokenizerNode builder : kvBuilder.getQualifierWriter().getNonLeaves()){
+      System.out.println(builder);
+    }
+    for(TokenizerNode builder : kvBuilder.getQualifierWriter().getLeaves()){
+      System.out.println(builder);
+    }
+    System.out.println("-- qualifier PtColumnNodeWriters --");
+    for(ColumnNodeWriter writer : kvBuilder.getQualifierWriter().getColumnNodeWriters()){
+      System.out.println(writer);
+    }
+
+    System.out.println("-- rowKey SortedPtBuilderNodes --");
+    for(TokenizerNode builder : kvBuilder.getRowWriter().getNonLeaves()){
+      System.out.println(builder);
+    }
+    for(TokenizerNode builder : kvBuilder.getRowWriter().getLeaves()){
+      System.out.println(builder);
+    }
+    System.out.println("-- row PtRowNodeWriterLights --");
+    for(RowNodeWriter writer : kvBuilder.getRowWriter().getNonLeafWriters()){
+      System.out.println(writer);
+    }
+    for(RowNodeWriter writer : kvBuilder.getRowWriter().getLeafWriters()){
+      System.out.println(writer);
+    }
+
+    System.out.println("-- concatenated values --");
+    System.out.println(Bytes.toStringBinary(kvBuilder.getValueByteRange().deepCopyToNewArray()));
+	}
+
+	@Override
+	public List<KeyValue> getInputs() {
+		return kvs;
+	}
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TestTimestamps.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TestTimestamps.java
new file mode 100644
index 0000000..bdd3212
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TestTimestamps.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.timestamp;
+
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestamps1;
+import org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestamps2;
+import org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestamps3;
+
+import com.google.common.collect.Lists;
+
+public interface TestTimestamps {
+
+  List<Long> getInputs();
+  long getMinimum();
+  List<Long> getOutputs();
+
+  public static class InMemory {
+    public Collection<Object[]> getAllAsObjectArray() {
+      List<Object[]> all = Lists.newArrayList();
+      all.add(new Object[] { new TestTimestamps1() });
+      all.add(new Object[] { new TestTimestamps2() });
+      all.add(new Object[] { new TestTimestamps3() });
+      return all;
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TimestampCompressorTests.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TimestampCompressorTests.java
new file mode 100644
index 0000000..02fd90a
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/TimestampCompressorTests.java
@@ -0,0 +1,91 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.timestamp;
+
+import java.util.Collection;
+
+import junit.framework.Assert;
+
+import org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta;
+import org.apache.hbase.codec.prefixtree.decode.timestamp.TimestampDecoder;
+import org.apache.hbase.codec.prefixtree.encode.timestamp.TimestampEncoder;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameters;
+
+@RunWith(Parameterized.class)
+public class TimestampCompressorTests {
+
+  @Parameters
+  public static Collection<Object[]> parameters() {
+    return new TestTimestamps.InMemory().getAllAsObjectArray();
+  }
+
+  private TestTimestamps timestamps;
+  private PrefixTreeBlockMeta blockMeta;
+  private TimestampEncoder compressor;
+  private byte[] bytes;
+  private TimestampDecoder reader;
+
+  public TimestampCompressorTests(TestTimestamps testTimestamps) {
+    this.timestamps = testTimestamps;
+    this.blockMeta = new PrefixTreeBlockMeta();
+    this.blockMeta.setNumMetaBytes(0);
+    this.blockMeta.setNumRowBytes(0);
+    this.blockMeta.setNumQualifierBytes(0);
+    this.compressor = new TimestampEncoder(blockMeta, false);
+    for (Long ts : testTimestamps.getInputs()) {
+      compressor.add(ts);
+    }
+    compressor.compile();
+    bytes = compressor.getOutputArray();
+    reader = new TimestampDecoder();
+    reader.initOnBlock(blockMeta, bytes);
+  }
+
+  @Test
+  public void testCompressorMinimum() {
+    Assert.assertEquals(timestamps.getMinimum(), compressor.getMin());
+  }
+
+  @Test
+  public void testCompressorRoundTrip() {
+    long[] outputs = compressor.getSortedUniqueTimestamps();
+    for (int i = 0; i < timestamps.getOutputs().size(); ++i) {
+      long input = timestamps.getOutputs().get(i);
+      long output = outputs[i];
+      Assert.assertEquals(input, output);
+    }
+  }
+
+  @Test
+  public void testReaderMinimum() {
+    Assert.assertEquals(timestamps.getMinimum(), reader.getTimestamp(0));
+  }
+
+  @Test
+  public void testReaderRoundTrip() {
+    for (int i = 0; i < timestamps.getOutputs().size(); ++i) {
+      long input = timestamps.getOutputs().get(i);
+      long output = reader.getTimestamp(i);
+      Assert.assertEquals(input, output);
+    }
+  }
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps1.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps1.java
new file mode 100644
index 0000000..caa79cd
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps1.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.timestamp.data;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.timestamp.TestTimestamps;
+
+public class TestTimestamps1 implements TestTimestamps {
+
+  @Override
+  public List<Long> getInputs() {
+    List<Long> d = new ArrayList<Long>();
+    d.add(5L);
+    d.add(3L);
+    d.add(0L);
+    d.add(1L);
+    d.add(3L);
+    return d;
+  }
+
+  @Override
+  public long getMinimum() {
+    return 0L;
+  }
+
+  @Override
+  public List<Long> getOutputs() {
+    List<Long> d = new ArrayList<Long>();
+    d.add(0L);
+    d.add(1L);
+    d.add(3L);
+    d.add(5L);
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps2.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps2.java
new file mode 100644
index 0000000..d7a3f90
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps2.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.timestamp.data;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.timestamp.TestTimestamps;
+
+public class TestTimestamps2 implements TestTimestamps {
+
+  private int shift = 8;
+
+  @Override
+  public List<Long> getInputs() {
+    List<Long> d = new ArrayList<Long>();
+    d.add(5L << shift);
+    d.add(3L << shift);
+    d.add(7L << shift);
+    d.add(1L << shift);
+    d.add(3L << shift);
+    return d;
+  }
+
+  @Override
+  public long getMinimum() {
+    return 1L << shift;
+  }
+
+  @Override
+  public List<Long> getOutputs() {
+    List<Long> d = new ArrayList<Long>();
+    d.add(1L << shift);
+    d.add(3L << shift);
+    d.add(5L << shift);
+    d.add(7L << shift);
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps3.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps3.java
new file mode 100644
index 0000000..1dd2839
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/codec/prefixtree/timestamp/data/TestTimestamps3.java
@@ -0,0 +1,52 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.codec.prefixtree.timestamp.data;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hbase.codec.prefixtree.timestamp.TestTimestamps;
+
+public class TestTimestamps3 implements TestTimestamps {
+
+  private static long t = 1234567890L;
+
+  @Override
+  public List<Long> getInputs() {
+    List<Long> d = new ArrayList<Long>();
+    d.add(t);
+    d.add(t);
+    d.add(t);
+    d.add(t);
+    d.add(t);
+    return d;
+  }
+
+  @Override
+  public long getMinimum() {
+    return t;
+  }
+
+  @Override
+  public List<Long> getOutputs() {
+    List<Long> d = new ArrayList<Long>();
+    return d;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/StringByteUtils.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/StringByteUtils.java
new file mode 100644
index 0000000..d72e305
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/StringByteUtils.java
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.bytes;
+
+import java.util.List;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hadoop.hbase.util.collections.IterableUtils;
+
+import com.google.common.collect.Lists;
+
+public class StringByteUtils {
+
+  public static List<byte[]> getUtf8ByteArrays(List<String> strings) {
+    List<byte[]> byteArrays = Lists.newArrayListWithCapacity(CollectionUtils.size(strings));
+    for (String s : IterableUtils.nullSafe(strings)) {
+      byteArrays.add(Bytes.toBytes(s));
+    }
+    return byteArrays;
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/TestByteRange.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/TestByteRange.java
new file mode 100644
index 0000000..4b04ce5
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/bytes/TestByteRange.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.bytes;
+
+import junit.framework.Assert;
+
+import org.apache.hadoop.hbase.util.bytes.ByteRange;
+import org.junit.Test;
+
+public class TestByteRange {
+
+  @Test
+  public void testConstructor() {
+    ByteRange b = new ByteRange(new byte[] { 0, 1, 2 });
+    Assert.assertEquals(3, b.getLength());
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/comparator/ByteArrayComparator.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/comparator/ByteArrayComparator.java
new file mode 100644
index 0000000..8cf7bd9
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/comparator/ByteArrayComparator.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.comparator;
+
+import java.util.Comparator;
+
+import org.apache.hadoop.hbase.util.Bytes;
+
+public class ByteArrayComparator implements Comparator<byte[]> {
+
+  @Override
+  public int compare(byte[] a, byte[] b) {
+    return Bytes.compareTo(a, b);
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/NumberFormatter.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/NumberFormatter.java
new file mode 100644
index 0000000..05f9c02
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/NumberFormatter.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.number;
+
+import java.text.DecimalFormat;
+
+public class NumberFormatter {
+
+  public static String addCommas(final Number pValue) {
+    if (pValue == null) {
+      return null;
+    }
+    String format = "###,###,###,###,###,###,###,###.#####################";
+    return new DecimalFormat(format).format(pValue);// biggest is 19 digits
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/RandomNumberUtils.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/RandomNumberUtils.java
new file mode 100644
index 0000000..57fd8f5
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/number/RandomNumberUtils.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.number;
+
+import java.util.Random;
+
+public class RandomNumberUtils {
+
+  public static long nextPositiveLong(Random random) {
+    while (true) {
+      long value = random.nextLong();
+      if (value > 0) {
+        return value;
+      }
+    }
+  }
+
+}
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestFIntTool.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestFIntTool.java
new file mode 100644
index 0000000..c9abc8e
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestFIntTool.java
@@ -0,0 +1,121 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.ByteArrayOutputStream;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+/********************** tests *************************/
+
+public class TestFIntTool {
+  @Test
+  public void testLeadingZeros() {
+    Assert.assertEquals(64, Long.numberOfLeadingZeros(0));
+    Assert.assertEquals(63, Long.numberOfLeadingZeros(1));
+    Assert.assertEquals(0, Long.numberOfLeadingZeros(Long.MIN_VALUE));
+    Assert.assertEquals(0, Long.numberOfLeadingZeros(-1));
+    Assert.assertEquals(1, Long.numberOfLeadingZeros(Long.MAX_VALUE));
+    Assert.assertEquals(1, Long.numberOfLeadingZeros(Long.MAX_VALUE - 1));
+  }
+
+  @Test
+  public void testMaxValueForNumBytes() {
+    Assert.assertEquals(255, UFIntTool.maxValueForNumBytes(1));
+    Assert.assertEquals(65535, UFIntTool.maxValueForNumBytes(2));
+    Assert.assertEquals(0xffffff, UFIntTool.maxValueForNumBytes(3));
+    Assert.assertEquals(0xffffffffffffffL, UFIntTool.maxValueForNumBytes(7));
+  }
+
+  @Test
+  public void testNumBytes() {
+    Assert.assertEquals(1, UFIntTool.numBytes(0));
+    Assert.assertEquals(1, UFIntTool.numBytes(1));
+    Assert.assertEquals(1, UFIntTool.numBytes(255));
+    Assert.assertEquals(2, UFIntTool.numBytes(256));
+    Assert.assertEquals(2, UFIntTool.numBytes(65535));
+    Assert.assertEquals(3, UFIntTool.numBytes(65536));
+    Assert.assertEquals(4, UFIntTool.numBytes(0xffffffffL));
+    Assert.assertEquals(5, UFIntTool.numBytes(0x100000000L));
+    Assert.assertEquals(4, UFIntTool.numBytes(Integer.MAX_VALUE));
+    Assert.assertEquals(8, UFIntTool.numBytes(Long.MAX_VALUE));
+    Assert.assertEquals(8, UFIntTool.numBytes(Long.MAX_VALUE - 1));
+  }
+
+  @Test
+  public void testGetBytes() {
+    Assert.assertArrayEquals(new byte[] { 0 }, UFIntTool.getBytes(1, 0));
+    Assert.assertArrayEquals(new byte[] { 1 }, UFIntTool.getBytes(1, 1));
+    Assert.assertArrayEquals(new byte[] { -1 }, UFIntTool.getBytes(1, 255));
+    Assert.assertArrayEquals(new byte[] { 1, 0 }, UFIntTool.getBytes(2, 256));
+    Assert.assertArrayEquals(new byte[] { 1, 3 }, UFIntTool.getBytes(2, 256 + 3));
+    Assert.assertArrayEquals(new byte[] { 1, -128 }, UFIntTool.getBytes(2, 256 + 128));
+    Assert.assertArrayEquals(new byte[] { 1, -1 }, UFIntTool.getBytes(2, 256 + 255));
+    Assert.assertArrayEquals(new byte[] { 127, -1, -1, -1 },
+      UFIntTool.getBytes(4, Integer.MAX_VALUE));
+    Assert.assertArrayEquals(new byte[] { 127, -1, -1, -1, -1, -1, -1, -1 },
+      UFIntTool.getBytes(8, Long.MAX_VALUE));
+  }
+
+  @Test
+  public void testFromBytes() {
+    Assert.assertEquals(0, UFIntTool.fromBytes(new byte[] { 0 }));
+    Assert.assertEquals(1, UFIntTool.fromBytes(new byte[] { 1 }));
+    Assert.assertEquals(255, UFIntTool.fromBytes(new byte[] { -1 }));
+    Assert.assertEquals(256, UFIntTool.fromBytes(new byte[] { 1, 0 }));
+    Assert.assertEquals(256 + 3, UFIntTool.fromBytes(new byte[] { 1, 3 }));
+    Assert.assertEquals(256 + 128, UFIntTool.fromBytes(new byte[] { 1, -128 }));
+    Assert.assertEquals(256 + 255, UFIntTool.fromBytes(new byte[] { 1, -1 }));
+    Assert.assertEquals(Integer.MAX_VALUE, UFIntTool.fromBytes(new byte[] { 127, -1, -1, -1 }));
+    Assert.assertEquals(Long.MAX_VALUE,
+      UFIntTool.fromBytes(new byte[] { 127, -1, -1, -1, -1, -1, -1, -1 }));
+  }
+
+  @Test
+  public void testRoundTrips() {
+    long[] values = new long[] { 0, 1, 2, 255, 256, 31123, 65535, 65536, 65537, 0xfffffeL,
+        0xffffffL, 0x1000000L, 0x1000001L, Integer.MAX_VALUE - 1, Integer.MAX_VALUE,
+        (long) Integer.MAX_VALUE + 1, Long.MAX_VALUE - 1, Long.MAX_VALUE };
+    for (int i = 0; i < values.length; ++i) {
+      Assert.assertEquals(values[i], UFIntTool.fromBytes(UFIntTool.getBytes(8, values[i])));
+    }
+  }
+
+  @Test
+  public void testWriteBytes() {// copied from testGetBytes
+    Assert.assertArrayEquals(new byte[] { 0 }, bytesViaOutputStream(1, 0));
+    Assert.assertArrayEquals(new byte[] { 1 }, bytesViaOutputStream(1, 1));
+    Assert.assertArrayEquals(new byte[] { -1 }, bytesViaOutputStream(1, 255));
+    Assert.assertArrayEquals(new byte[] { 1, 0 }, bytesViaOutputStream(2, 256));
+    Assert.assertArrayEquals(new byte[] { 1, 3 }, bytesViaOutputStream(2, 256 + 3));
+    Assert.assertArrayEquals(new byte[] { 1, -128 }, bytesViaOutputStream(2, 256 + 128));
+    Assert.assertArrayEquals(new byte[] { 1, -1 }, bytesViaOutputStream(2, 256 + 255));
+    Assert.assertArrayEquals(new byte[] { 127, -1, -1, -1 },
+      bytesViaOutputStream(4, Integer.MAX_VALUE));
+    Assert.assertArrayEquals(new byte[] { 127, -1, -1, -1, -1, -1, -1, -1 },
+      bytesViaOutputStream(8, Long.MAX_VALUE));
+  }
+
+  private byte[] bytesViaOutputStream(int outputWidth, long value) {
+    ByteArrayOutputStream os = new ByteArrayOutputStream();
+    UFIntTool.writeBytes(outputWidth, value, os);
+    return os.toByteArray();
+  }
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVIntTool.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVIntTool.java
new file mode 100644
index 0000000..204c558
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVIntTool.java
@@ -0,0 +1,100 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.Random;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+/************************** Tests ********************************/
+
+public class TestVIntTool {
+
+  @Test
+  public void testNumBytes() {
+    Assert.assertEquals(1, UVIntTool.numBytes(0));
+    Assert.assertEquals(1, UVIntTool.numBytes(1));
+    Assert.assertEquals(1, UVIntTool.numBytes(100));
+    Assert.assertEquals(1, UVIntTool.numBytes(126));
+    Assert.assertEquals(1, UVIntTool.numBytes(127));
+    Assert.assertEquals(2, UVIntTool.numBytes(128));
+    Assert.assertEquals(2, UVIntTool.numBytes(129));
+    Assert.assertEquals(5, UVIntTool.numBytes(Integer.MAX_VALUE));
+  }
+
+  @Test
+  public void testWriteBytes() {
+    Assert.assertArrayEquals(new byte[] { 0 }, bytesViaOutputStream(0));
+    Assert.assertArrayEquals(new byte[] { 1 }, bytesViaOutputStream(1));
+    Assert.assertArrayEquals(new byte[] { 63 }, bytesViaOutputStream(63));
+    Assert.assertArrayEquals(new byte[] { 127 }, bytesViaOutputStream(127));
+    Assert.assertArrayEquals(new byte[] { -128, 1 }, bytesViaOutputStream(128));
+    Assert.assertArrayEquals(new byte[] { -128 + 27, 1 }, bytesViaOutputStream(155));
+    Assert.assertArrayEquals(UVIntTool.MAX_VALUE_BYTES, bytesViaOutputStream(Integer.MAX_VALUE));
+  }
+
+  private byte[] bytesViaOutputStream(int value) {
+    ByteArrayOutputStream os = new ByteArrayOutputStream();
+    UVIntTool.writeBytes(value, os);
+    return os.toByteArray();
+  }
+
+  @Test
+  public void testToBytes() {
+    Assert.assertArrayEquals(new byte[] { 0 }, UVIntTool.getBytes(0));
+    Assert.assertArrayEquals(new byte[] { 1 }, UVIntTool.getBytes(1));
+    Assert.assertArrayEquals(new byte[] { 63 }, UVIntTool.getBytes(63));
+    Assert.assertArrayEquals(new byte[] { 127 }, UVIntTool.getBytes(127));
+    Assert.assertArrayEquals(new byte[] { -128, 1 }, UVIntTool.getBytes(128));
+    Assert.assertArrayEquals(new byte[] { -128 + 27, 1 }, UVIntTool.getBytes(155));
+    Assert.assertArrayEquals(UVIntTool.MAX_VALUE_BYTES, UVIntTool.getBytes(Integer.MAX_VALUE));
+  }
+
+  @Test
+  public void testFromBytes() {
+    Assert.assertEquals(Integer.MAX_VALUE, UVIntTool.getInt(UVIntTool.MAX_VALUE_BYTES));
+  }
+
+  @Test
+  public void testRoundTrips() {
+    Random random = new Random();
+    for (int i = 0; i < 10000; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      byte[] bytes = UVIntTool.getBytes(value);
+      int roundTripped = UVIntTool.getInt(bytes);
+      Assert.assertEquals(value, roundTripped);
+    }
+  }
+
+  @Test
+  public void testInputStreams() throws IOException {
+    ByteArrayInputStream is;
+    is = new ByteArrayInputStream(new byte[] { 0 });
+    Assert.assertEquals(0, UVIntTool.getInt(is));
+    is = new ByteArrayInputStream(new byte[] { 5 });
+    Assert.assertEquals(5, UVIntTool.getInt(is));
+    is = new ByteArrayInputStream(new byte[] { -128 + 27, 1 });
+    Assert.assertEquals(155, UVIntTool.getInt(is));
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVLongTool.java b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVLongTool.java
new file mode 100644
index 0000000..606a048
--- /dev/null
+++ b/hbase-prefix-tree/src/test/java/org/apache/hbase/util/vint/TestVLongTool.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hbase.util.vint;
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Random;
+
+import org.apache.hbase.util.number.RandomNumberUtils;
+import org.junit.Assert;
+import org.junit.Test;
+
+public class TestVLongTool {
+
+  @Test
+  public void testNumBytes() {
+    Assert.assertEquals(1, UVLongTool.numBytes(0));
+    Assert.assertEquals(1, UVLongTool.numBytes(1));
+    Assert.assertEquals(1, UVLongTool.numBytes(100));
+    Assert.assertEquals(1, UVLongTool.numBytes(126));
+    Assert.assertEquals(1, UVLongTool.numBytes(127));
+    Assert.assertEquals(2, UVLongTool.numBytes(128));
+    Assert.assertEquals(2, UVLongTool.numBytes(129));
+    Assert.assertEquals(9, UVLongTool.numBytes(Long.MAX_VALUE));
+  }
+
+  @Test
+  public void testToBytes() {
+    Assert.assertArrayEquals(new byte[] { 0 }, UVLongTool.getBytes(0));
+    Assert.assertArrayEquals(new byte[] { 1 }, UVLongTool.getBytes(1));
+    Assert.assertArrayEquals(new byte[] { 63 }, UVLongTool.getBytes(63));
+    Assert.assertArrayEquals(new byte[] { 127 }, UVLongTool.getBytes(127));
+    Assert.assertArrayEquals(new byte[] { -128, 1 }, UVLongTool.getBytes(128));
+    Assert.assertArrayEquals(new byte[] { -128 + 27, 1 }, UVLongTool.getBytes(155));
+    Assert.assertArrayEquals(UVLongTool.MAX_VALUE_BYTES, UVLongTool.getBytes(Long.MAX_VALUE));
+  }
+
+  @Test
+  public void testFromBytes() {
+    Assert.assertEquals(Long.MAX_VALUE, UVLongTool.getLong(UVLongTool.MAX_VALUE_BYTES));
+  }
+
+  @Test
+  public void testFromBytesOffset() {
+    Assert.assertEquals(Long.MAX_VALUE, UVLongTool.getLong(UVLongTool.MAX_VALUE_BYTES, 0));
+
+    long ms = 1318966363481L;
+    System.out.println(ms);
+    byte[] bytes = UVLongTool.getBytes(ms);
+    System.out.println(Arrays.toString(bytes));
+    long roundTripped = UVLongTool.getLong(bytes, 0);
+    Assert.assertEquals(ms, roundTripped);
+
+    int calculatedNumBytes = UVLongTool.numBytes(ms);
+    int actualNumBytes = bytes.length;
+    Assert.assertEquals(actualNumBytes, calculatedNumBytes);
+
+    byte[] shiftedBytes = new byte[1000];
+    int shift = 33;
+    System.arraycopy(bytes, 0, shiftedBytes, shift, bytes.length);
+    long shiftedRoundTrip = UVLongTool.getLong(shiftedBytes, shift);
+    Assert.assertEquals(ms, shiftedRoundTrip);
+  }
+
+  @Test
+  public void testRoundTrips() {
+    Random random = new Random();
+    for (int i = 0; i < 10000; ++i) {
+      long value = RandomNumberUtils.nextPositiveLong(random);
+      byte[] bytes = UVLongTool.getBytes(value);
+      long roundTripped = UVLongTool.getLong(bytes);
+      Assert.assertEquals(value, roundTripped);
+      int calculatedNumBytes = UVLongTool.numBytes(value);
+      int actualNumBytes = bytes.length;
+      Assert.assertEquals(actualNumBytes, calculatedNumBytes);
+    }
+  }
+
+  @Test
+  public void testInputStreams() throws IOException {
+    ByteArrayInputStream is;
+    is = new ByteArrayInputStream(new byte[] { 0 });
+    Assert.assertEquals(0, UVLongTool.getLong(is));
+    is = new ByteArrayInputStream(new byte[] { 5 });
+    Assert.assertEquals(5, UVLongTool.getLong(is));
+    is = new ByteArrayInputStream(new byte[] { -128 + 27, 1 });
+    Assert.assertEquals(155, UVLongTool.getLong(is));
+  }
+}
\ No newline at end of file
diff --git a/hbase-server/pom.xml b/hbase-server/pom.xml
index 0efa8f4..5a0a75b 100644
--- a/hbase-server/pom.xml
+++ b/hbase-server/pom.xml
@@ -272,6 +272,12 @@
     </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-prefix-tree</artifactId>
+      <!-- unfortunately, runtime scope causes eclipse to put it in the compile time classpath -->
+      <scope>runtime</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-common</artifactId>
       <type>test-jar</type>
     </dependency>
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index 68b4354..5112fcc 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -352,4 +352,21 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     }
   }
 
+  /**
+   * Asserts that there is at least the given amount of unfilled space
+   * remaining in the given buffer.
+   * @param out typically, the buffer we are writing to
+   * @param length the required space in the buffer
+   * @throws EncoderBufferTooSmallException If there are no enough bytes.
+   */
+  public static void ensureSpace(ByteBuffer out, int length)
+      throws EncoderBufferTooSmallException {
+    if (out.position() + length > out.limit()) {
+      throw new EncoderBufferTooSmallException(
+          "Buffer position=" + out.position() +
+          ", buffer limit=" + out.limit() +
+          ", length to be written=" + length);
+    }
+  }
+
 }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
index eca0554..039187d 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/DiffKeyDeltaEncoder.java
@@ -233,8 +233,7 @@ public class DiffKeyDeltaEncoder extends BufferedDataBlockEncoder {
 
     // create KeyValue buffer and fill it prefix
     int keyOffset = buffer.position();
-    ByteBufferUtils.ensureSpace(buffer, keyLength + valueLength
-        + KeyValue.ROW_OFFSET);
+    ensureSpace(buffer, keyLength + valueLength + KeyValue.ROW_OFFSET);
     buffer.putInt(keyLength);
     buffer.putInt(valueLength);
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
index e4e7594..cb2447b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/EncodedDataBlock.java
@@ -30,7 +30,7 @@ import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.io.hfile.Compression;
 import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
-import org.apache.hadoop.hbase.io.hfile.HFileBlock;
+import org.apache.hadoop.hbase.io.hfile.HFileBlockConstants;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.io.compress.Compressor;
 
@@ -65,7 +65,7 @@ public class EncodedDataBlock {
     this.dataBlockEncoder = dataBlockEncoder;
     encodingCtx =
         dataBlockEncoder.newDataBlockEncodingContext(Compression.Algorithm.NONE,
-            encoding, HFileBlock.DUMMY_HEADER);
+            encoding, HFileBlockConstants.DUMMY_HEADER);
     this.rawKVs = rawKVs;
   }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
index 6d1c5fb..7c7c4b7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java
@@ -230,8 +230,7 @@ public class FastDiffDeltaEncoder extends BufferedDataBlockEncoder {
     }
     int commonLength = ByteBufferUtils.readCompressedInt(source);
 
-    ByteBufferUtils.ensureSpace(out, state.keyLength + state.valueLength +
-        KeyValue.ROW_OFFSET);
+    ensureSpace(out, state.keyLength + state.valueLength + KeyValue.ROW_OFFSET);
 
     int kvPos = out.position();
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java
deleted file mode 100644
index dee40aa..0000000
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultDecodingContext.java
+++ /dev/null
@@ -1,62 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.hadoop.hbase.io.encoding;
-
-import java.io.ByteArrayInputStream;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.ByteBuffer;
-
-import org.apache.hadoop.hbase.io.hfile.Compression;
-import org.apache.hadoop.hbase.io.hfile.HFileBlock;
-import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
-
-/**
- * A default implementation of {@link HFileBlockDecodingContext}. It assumes the
- * block data section is compressed as a whole.
- *
- * @see HFileBlockDefaultEncodingContext for the default compression context
- *
- */
-public class HFileBlockDefaultDecodingContext implements
-    HFileBlockDecodingContext {
-
-  private final Compression.Algorithm compressAlgo;
-
-  public HFileBlockDefaultDecodingContext(
-      Compression.Algorithm compressAlgo) {
-    this.compressAlgo = compressAlgo;
-  }
-
-  @Override
-  public void prepareDecoding(int onDiskSizeWithoutHeader, int uncompressedSizeWithoutHeader,
-      ByteBuffer blockBufferWithoutHeader, byte[] onDiskBlock, int offset) throws IOException {
-    DataInputStream dis = new DataInputStream(new ByteArrayInputStream(onDiskBlock, offset,
-        onDiskSizeWithoutHeader));
-
-    Compression.decompress(blockBufferWithoutHeader.array(),
-      blockBufferWithoutHeader.arrayOffset(), (InputStream) dis, onDiskSizeWithoutHeader,
-      uncompressedSizeWithoutHeader, compressAlgo);
-  }
-
-  @Override
-  public Algorithm getCompression() {
-    return compressAlgo;
-  }
-
-}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java
deleted file mode 100644
index 965d5cf..0000000
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/HFileBlockDefaultEncodingContext.java
+++ /dev/null
@@ -1,208 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.hadoop.hbase.io.encoding;
-
-import static org.apache.hadoop.hbase.io.hfile.Compression.Algorithm.NONE;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.hbase.io.hfile.BlockType;
-import org.apache.hadoop.hbase.io.hfile.Compression;
-import org.apache.hadoop.hbase.io.hfile.HFileBlock;
-import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
-import org.apache.hadoop.io.compress.CompressionOutputStream;
-import org.apache.hadoop.io.compress.Compressor;
-
-/**
- * A default implementation of {@link HFileBlockEncodingContext}. It will
- * compress the data section as one continuous buffer.
- *
- * @see HFileBlockDefaultDecodingContext for the decompression part
- *
- */
-public class HFileBlockDefaultEncodingContext implements
-    HFileBlockEncodingContext {
-
-  private byte[] onDiskBytesWithHeader;
-  private byte[] uncompressedBytesWithHeader;
-  private BlockType blockType;
-  private final DataBlockEncoding encodingAlgo;
-
-  /** Compressor, which is also reused between consecutive blocks. */
-  private Compressor compressor;
-
-  /** Compression output stream */
-  private CompressionOutputStream compressionStream;
-
-  /** Underlying stream to write compressed bytes to */
-  private ByteArrayOutputStream compressedByteStream;
-
-  /** Compression algorithm for all blocks this instance writes. */
-  private final Compression.Algorithm compressionAlgorithm;
-
-  private ByteArrayOutputStream encodedStream = new ByteArrayOutputStream();
-  private DataOutputStream dataOut = new DataOutputStream(encodedStream);
-
-  private final byte[] dummyHeader;
-
-  /**
-   * @param compressionAlgorithm compression algorithm used
-   * @param encoding encoding used
-   * @param headerBytes dummy header bytes
-   */
-  public HFileBlockDefaultEncodingContext(
-      Compression.Algorithm compressionAlgorithm,
-      DataBlockEncoding encoding, byte[] headerBytes) {
-    this.encodingAlgo = encoding;
-    this.compressionAlgorithm =
-        compressionAlgorithm == null ? NONE : compressionAlgorithm;
-    if (this.compressionAlgorithm != NONE) {
-      compressor = compressionAlgorithm.getCompressor();
-      compressedByteStream = new ByteArrayOutputStream();
-      try {
-        compressionStream =
-            compressionAlgorithm.createPlainCompressionStream(
-                compressedByteStream, compressor);
-      } catch (IOException e) {
-        throw new RuntimeException(
-            "Could not create compression stream for algorithm "
-                + compressionAlgorithm, e);
-      }
-    }
-    if (headerBytes == null) {
-      dummyHeader = HFileBlock.DUMMY_HEADER;
-    } else {
-      dummyHeader = headerBytes;
-    }
-  }
-
-  /**
-   * @param compressionAlgorithm compression algorithm
-   * @param encoding encoding
-   */
-  public HFileBlockDefaultEncodingContext(
-      Compression.Algorithm compressionAlgorithm,
-      DataBlockEncoding encoding) {
-    this(compressionAlgorithm, encoding, null);
-  }
-
-  /**
-   * prepare to start a new encoding.
-   * @throws IOException
-   */
-  void prepareEncoding() throws IOException {
-    encodedStream.reset();
-    dataOut.write(dummyHeader);
-    if (encodingAlgo != null
-        && encodingAlgo != DataBlockEncoding.NONE) {
-      encodingAlgo.writeIdInBytes(dataOut);
-    }
-  }
-
-  @Override
-  public void postEncoding(BlockType blockType)
-      throws IOException {
-    dataOut.flush();
-    compressAfterEncoding(encodedStream.toByteArray(), blockType);
-    this.blockType = blockType;
-  }
-
-  /**
-   * @param uncompressedBytesWithHeader
-   * @param blockType
-   * @throws IOException
-   */
-  public void compressAfterEncoding(byte[] uncompressedBytesWithHeader,
-      BlockType blockType) throws IOException {
-    compressAfterEncoding(uncompressedBytesWithHeader, blockType, dummyHeader);
-  }
-
-  /**
-   * @param uncompressedBytesWithHeader
-   * @param blockType
-   * @param headerBytes
-   * @throws IOException
-   */
-  protected void compressAfterEncoding(byte[] uncompressedBytesWithHeader,
-      BlockType blockType, byte[] headerBytes) throws IOException {
-    this.uncompressedBytesWithHeader = uncompressedBytesWithHeader;
-    if (compressionAlgorithm != NONE) {
-      compressedByteStream.reset();
-      compressedByteStream.write(headerBytes);
-      compressionStream.resetState();
-      compressionStream.write(uncompressedBytesWithHeader,
-          headerBytes.length, uncompressedBytesWithHeader.length
-              - headerBytes.length);
-
-      compressionStream.flush();
-      compressionStream.finish();
-      onDiskBytesWithHeader = compressedByteStream.toByteArray();
-    } else {
-      onDiskBytesWithHeader = uncompressedBytesWithHeader;
-    }
-    this.blockType = blockType;
-  }
-
-  @Override
-  public byte[] getOnDiskBytesWithHeader() {
-    return onDiskBytesWithHeader;
-  }
-
-  @Override
-  public byte[] getUncompressedBytesWithHeader() {
-    return uncompressedBytesWithHeader;
-  }
-
-  @Override
-  public BlockType getBlockType() {
-    return blockType;
-  }
-
-  /**
-   * Releases the compressor this writer uses to compress blocks into the
-   * compressor pool.
-   */
-  @Override
-  public void close() {
-    if (compressor != null) {
-      compressionAlgorithm.returnCompressor(compressor);
-      compressor = null;
-    }
-  }
-
-  @Override
-  public Algorithm getCompression() {
-    return this.compressionAlgorithm;
-  }
-
-  public DataOutputStream getOutputStreamForEncoder() {
-    return this.dataOut;
-  }
-
-  @Override
-  public DataBlockEncoding getDataBlockEncoding() {
-    return this.encodingAlgo;
-  }
-
-  @Override
-  public int getHeaderSize() {
-    return this.dummyHeader.length;
-  }
-
-}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
index 3a2de8e..925801a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java
@@ -126,8 +126,7 @@ public class PrefixKeyDeltaEncoder extends BufferedDataBlockEncoder {
     int keyOffset;
     keyLength += commonLength;
 
-    ByteBufferUtils.ensureSpace(buffer, keyLength + valueLength
-        + KeyValue.ROW_OFFSET);
+    ensureSpace(buffer, keyLength + valueLength + KeyValue.ROW_OFFSET);
 
     buffer.putInt(keyLength);
     buffer.putInt(valueLength);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
index c93c8d5..222bfa1 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java
@@ -17,8 +17,6 @@
  */
 package org.apache.hadoop.hbase.io.hfile;
 
-import static org.apache.hadoop.hbase.io.hfile.BlockType.MAGIC_LENGTH;
-
 import java.io.BufferedInputStream;
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
@@ -40,6 +38,7 @@ import org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultDecodingContext;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultEncodingContext;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext;
 import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
+import org.apache.hadoop.hbase.io.hfile.HFileBlockConstants;
 import org.apache.hadoop.hbase.regionserver.MemStore;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -99,32 +98,18 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
    */
   static final int CHECKSUM_VERIFICATION_NUM_IO_THRESHOLD = 3;
 
-  /** The size data structures with minor version is 0 */
-  static final int HEADER_SIZE_NO_CHECKSUM = MAGIC_LENGTH + 2 * Bytes.SIZEOF_INT
-      + Bytes.SIZEOF_LONG;
-
   public static final boolean FILL_HEADER = true;
   public static final boolean DONT_FILL_HEADER = false;
 
-  /** The size of a version 2 {@link HFile} block header, minor version 1.
-   * There is a 1 byte checksum type, followed by a 4 byte bytesPerChecksum
-   * followed by another 4 byte value to store sizeofDataOnDisk.
-   */
-  public static final int HEADER_SIZE = HEADER_SIZE_NO_CHECKSUM +
-    Bytes.SIZEOF_BYTE + 2 * Bytes.SIZEOF_INT;
-
   /**
    * The size of block header when blockType is {@link BlockType#ENCODED_DATA}.
    * This extends normal header by adding the id of encoder.
    */
-  public static final int ENCODED_HEADER_SIZE = HEADER_SIZE
+  public static final int ENCODED_HEADER_SIZE = HFileBlockConstants.HEADER_SIZE
       + DataBlockEncoding.ID_SIZE;
 
-  /** Just an array of bytes of the right size. */
-  public static final byte[] DUMMY_HEADER = new byte[HEADER_SIZE];
-
   static final byte[] DUMMY_HEADER_NO_CHECKSUM = 
-     new byte[HEADER_SIZE_NO_CHECKSUM];
+     new byte[HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM];
 
   public static final int BYTE_BUFFER_HEAP_SIZE = (int) ClassSize.estimateBase(
       ByteBuffer.wrap(new byte[0], 0, 0).getClass(), false);
@@ -211,9 +196,9 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
    *          compression is disabled.
    * @param prevBlockOffset the offset of the previous block in the
    *          {@link HFile}
-   * @param buf block header ({@link #HEADER_SIZE} bytes) followed by
+   * @param buf block header ({@link HFileBlockConstants#HEADER_SIZE} bytes) followed by
    *          uncompressed data. This
-   * @param fillHeader true to fill in the first {@link #HEADER_SIZE} bytes of
+   * @param fillHeader true to fill in the first {@link HFileBlockConstants#HEADER_SIZE} bytes of
    *          the buffer based on the header fields provided
    * @param offset the file offset the block was read from
    * @param minorVersion the minor version of this block
@@ -265,7 +250,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
       this.checksumType = ChecksumType.NULL.getCode();
       this.bytesPerChecksum = 0;
       this.onDiskDataSizeWithHeader = onDiskSizeWithoutHeader +
-                                       HEADER_SIZE_NO_CHECKSUM;
+                                       HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM;
     }
     buf = b;
     buf.rewind();
@@ -394,7 +379,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
 
   /**
    * Checks if the block is internally consistent, i.e. the first
-   * {@link #HEADER_SIZE} bytes of the buffer contain a valid header consistent
+   * {@link HFileBlockConstants#HEADER_SIZE} bytes of the buffer contain a valid header consistent
    * with the fields. This function is primary for testing and debugging, and
    * is not thread-safe, because it alters the internal buffer pointer.
    */
@@ -649,7 +634,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
     /**
      * The stream we use to accumulate data in uncompressed format for each
      * block. We reset this stream at the end of each block and reuse it. The
-     * header is written as the first {@link #HEADER_SIZE} bytes into this
+     * header is written as the first {@link HFileBlockConstants#HEADER_SIZE} bytes into this
      * stream.
      */
     private ByteArrayOutputStream baosInMemory;
@@ -685,7 +670,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
     /**
      * Valid in the READY state. Contains the header and the uncompressed (but
      * potentially encoded, if this is a data block) bytes, so the length is
-     * {@link #uncompressedSizeWithoutHeader} + {@link HFileBlock#HEADER_SIZE}.
+     * {@link #uncompressedSizeWithoutHeader} + {@link HFileBlockConstants#HEADER_SIZE}.
      * Does not store checksums.
      */
     private byte[] uncompressedBytesWithHeader;
@@ -729,11 +714,11 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
         new HFileBlockDefaultEncodingContext(compressionAlgorithm, null);
       dataBlockEncodingCtx =
         this.dataBlockEncoder.newOnDiskDataBlockEncodingContext(
-            compressionAlgorithm, DUMMY_HEADER);
+            compressionAlgorithm, HFileBlockConstants.DUMMY_HEADER);
 
-      if (bytesPerChecksum < HEADER_SIZE) {
+      if (bytesPerChecksum < HFileBlockConstants.HEADER_SIZE) {
         throw new RuntimeException("Unsupported value of bytesPerChecksum. " +
-            " Minimum is " + HEADER_SIZE + " but the configured value is " +
+            " Minimum is " + HFileBlockConstants.HEADER_SIZE + " but the configured value is " +
             bytesPerChecksum);
       }
 
@@ -766,7 +751,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
       blockType = newBlockType;
 
       baosInMemory.reset();
-      baosInMemory.write(DUMMY_HEADER);
+      baosInMemory.write(HFileBlockConstants.DUMMY_HEADER);
 
       state = State.WRITING;
 
@@ -853,8 +838,8 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
     private void encodeDataBlockForDisk() throws IOException {
       // do data block encoding, if data block encoder is set
       ByteBuffer rawKeyValues =
-          ByteBuffer.wrap(uncompressedBytesWithHeader, HEADER_SIZE,
-              uncompressedBytesWithHeader.length - HEADER_SIZE).slice();
+          ByteBuffer.wrap(uncompressedBytesWithHeader, HFileBlockConstants.HEADER_SIZE,
+              uncompressedBytesWithHeader.length - HFileBlockConstants.HEADER_SIZE).slice();
 
       //do the encoding
       dataBlockEncoder.beforeWriteToDisk(rawKeyValues,
@@ -878,8 +863,8 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
     private void putHeader(byte[] dest, int offset, int onDiskSize,
         int uncompressedSize, int onDiskDataSize) {
       offset = blockType.put(dest, offset);
-      offset = Bytes.putInt(dest, offset, onDiskSize - HEADER_SIZE);
-      offset = Bytes.putInt(dest, offset, uncompressedSize - HEADER_SIZE);
+      offset = Bytes.putInt(dest, offset, onDiskSize - HFileBlockConstants.HEADER_SIZE);
+      offset = Bytes.putInt(dest, offset, uncompressedSize - HFileBlockConstants.HEADER_SIZE);
       offset = Bytes.putLong(dest, offset, prevOffset);
       offset = Bytes.putByte(dest, offset, checksumType.getCode());
       offset = Bytes.putInt(dest, offset, bytesPerChecksum);
@@ -970,7 +955,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
      */
     int getOnDiskSizeWithoutHeader() {
       expectState(State.BLOCK_READY);
-      return onDiskBytesWithHeader.length + onDiskChecksum.length - HEADER_SIZE;
+      return onDiskBytesWithHeader.length + onDiskChecksum.length - HFileBlockConstants.HEADER_SIZE;
     }
 
     /**
@@ -990,7 +975,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
      */
     int getUncompressedSizeWithoutHeader() {
       expectState(State.BLOCK_READY);
-      return uncompressedBytesWithHeader.length - HEADER_SIZE;
+      return uncompressedBytesWithHeader.length - HFileBlockConstants.HEADER_SIZE;
     }
 
     /**
@@ -1277,7 +1262,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
       }
 
       assert peekIntoNextBlock;
-      return Bytes.toInt(dest, destOffset + size + BlockType.MAGIC_LENGTH) +
+      return Bytes.toInt(dest, destOffset + size + HFileBlockConstants.MAGIC_LENGTH) +
           hdrSize;
     }
 
@@ -1315,8 +1300,8 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
   static class FSReaderV1 extends AbstractFSReader {
 
     /** Header size difference between version 1 and 2 */
-    private static final int HEADER_DELTA = HEADER_SIZE_NO_CHECKSUM - 
-                                            MAGIC_LENGTH;
+    private static final int HEADER_DELTA = HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM - 
+                                            HFileBlockConstants.MAGIC_LENGTH;
 
     public FSReaderV1(FSDataInputStream istream, Algorithm compressAlgo,
         long fileSize) throws IOException {
@@ -1332,7 +1317,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
      * coming to end of the compressed section.
      *
      * The block returned is still a version 2 block, and in particular, its
-     * first {@link #HEADER_SIZE} bytes contain a valid version 2 header.
+     * first {@link HFileBlockConstants#HEADER_SIZE} bytes contain a valid version 2 header.
      *
      * @param offset the offset of the block to read in the file
      * @param onDiskSizeWithMagic the on-disk size of the version 1 block,
@@ -1357,10 +1342,10 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
 
       int onDiskSize = (int) onDiskSizeWithMagic;
 
-      if (uncompressedSizeWithMagic < MAGIC_LENGTH) {
+      if (uncompressedSizeWithMagic < HFileBlockConstants.MAGIC_LENGTH) {
         throw new IOException("Uncompressed size for a version 1 block is "
             + uncompressedSizeWithMagic + " but must be at least "
-            + MAGIC_LENGTH);
+            + HFileBlockConstants.MAGIC_LENGTH);
       }
 
       // The existing size already includes magic size, and we are inserting
@@ -1382,7 +1367,7 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
         readAtOffset(istream, buf.array(), buf.arrayOffset() + HEADER_DELTA,
             onDiskSize, false, offset, pread);
 
-        onDiskSizeWithoutHeader = uncompressedSizeWithMagic - MAGIC_LENGTH;
+        onDiskSizeWithoutHeader = uncompressedSizeWithMagic - HFileBlockConstants.MAGIC_LENGTH;
       } else {
         InputStream bufferedBoundedStream = createBufferedBoundedStream(
             offset, onDiskSize, pread);
@@ -1396,15 +1381,15 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
       }
 
       BlockType newBlockType = BlockType.parse(buf.array(), buf.arrayOffset()
-          + HEADER_DELTA, MAGIC_LENGTH);
+          + HEADER_DELTA, HFileBlockConstants.MAGIC_LENGTH);
 
       // We set the uncompressed size of the new HFile block we are creating
       // to the size of the data portion of the block without the magic record,
       // since the magic record gets moved to the header.
       HFileBlock b = new HFileBlock(newBlockType, onDiskSizeWithoutHeader,
-          uncompressedSizeWithMagic - MAGIC_LENGTH, -1L, buf, FILL_HEADER,
+          uncompressedSizeWithMagic - HFileBlockConstants.MAGIC_LENGTH, -1L, buf, FILL_HEADER,
           offset, MemStore.NO_PERSISTENT_TS, 0, 0, ChecksumType.NULL.getCode(),
-          onDiskSizeWithoutHeader + HEADER_SIZE_NO_CHECKSUM);
+          onDiskSizeWithoutHeader + HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM);
       return b;
     }
   }
@@ -1415,8 +1400,8 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
    */
   private static class PrefetchedHeader {
     long offset = -1;
-    byte[] header = new byte[HEADER_SIZE];
-    ByteBuffer buf = ByteBuffer.wrap(header, 0, HEADER_SIZE);
+    byte[] header = new byte[HFileBlockConstants.HEADER_SIZE];
+    ByteBuffer buf = ByteBuffer.wrap(header, 0, HFileBlockConstants.HEADER_SIZE);
   }
 
   /** Reads version 2 blocks from the filesystem. */
@@ -1908,9 +1893,9 @@ public class HFileBlock extends SchemaConfigured implements Cacheable {
    */
   static private int headerSize(int minorVersion) {
     if (minorVersion < MINOR_VERSION_WITH_CHECKSUM) {
-      return HEADER_SIZE_NO_CHECKSUM;
+      return HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM;
     }
-    return HEADER_SIZE;
+    return HFileBlockConstants.HEADER_SIZE;
   }
 
   /**
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
index 013218a..3e7c7ae 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.java
@@ -70,7 +70,7 @@ public class HFileDataBlockEncoderImpl implements HFileDataBlockEncoder {
    */
   public HFileDataBlockEncoderImpl(DataBlockEncoding onDisk,
       DataBlockEncoding inCache, byte[] dummyHeader) {
-    dummyHeader = dummyHeader == null ? HFileBlock.DUMMY_HEADER : dummyHeader;
+    dummyHeader = dummyHeader == null ? HFileBlockConstants.DUMMY_HEADER : dummyHeader;
     this.onDisk = onDisk != null ?
         onDisk : DataBlockEncoding.NONE;
     this.inCache = inCache != null ?
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java
index 3861c00..19dc818 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java
@@ -154,7 +154,7 @@ public class HFileWriterV1 extends AbstractHFileWriter {
           0,                                         // bytesPerChecksum
           ChecksumType.NULL.getCode(),               // checksum type
           (int) (outputStream.getPos() - blockBegin) +
-          HFileBlock.HEADER_SIZE_NO_CHECKSUM);       // onDiskDataSizeWithHeader
+          HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM);       // onDiskDataSizeWithHeader
 
       block = blockEncoder.diskToCacheFormat(block, false);
       passSchemaMetricsTo(block);
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
index ed501dc..ca442f5 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java
@@ -73,20 +73,26 @@ public class KeyValueHeap extends NonLazyKeyValueScanner
           this.comparator);
       for (KeyValueScanner scanner : scanners) {
         if (scanner.peek() != null) {
+//          System.out.println(" adding scanner: "+scanner.peek());
           this.heap.add(scanner);
         } else {
+//          System.out.println("closing scanner: "+scanner);
           scanner.close();
         }
       }
       this.current = pollRealKV();
+//      System.out.println("polled "+current.peek());
     }
   }
 
   public KeyValue peek() {
     if (this.current == null) {
+//      System.out.println("peek:null");
       return null;
     }
-    return this.current.peek();
+    KeyValue p = this.current.peek();
+//    System.out.println("peek:"+p);
+    return p;
   }
 
   public KeyValue next()  throws IOException {
@@ -302,6 +308,7 @@ public class KeyValueHeap extends NonLazyKeyValueScanner
     KeyValueScanner scanner;
     while ((scanner = heap.poll()) != null) {
       KeyValue topKey = scanner.peek();
+//      System.out.println("top:"+topKey);
       if (comparator.getComparator().compare(seekKey, topKey) <= 0) {
         // Top KeyValue is at-or-after Seek KeyValue. We only know that all
         // scanners are at or after seekKey (because fake keys of
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
index b595c06..a7f1880 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
@@ -133,6 +133,8 @@ public class StoreScanner extends NonLazyKeyValueScanner
     } else {
       for (KeyValueScanner scanner : scanners) {
         scanner.seek(matcher.getStartKey());
+//        System.out.println("try seek to:"+matcher.getStartKey());
+//        System.out.println("  seeked to:"+scanner.peek());
       }
     }
 
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java
deleted file mode 100644
index 21481bf..0000000
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/ByteBufferUtils.java
+++ /dev/null
@@ -1,443 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.hadoop.hbase.util;
-
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.nio.ByteBuffer;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.hbase.io.encoding.
-    EncoderBufferTooSmallException;
-import org.apache.hadoop.io.WritableUtils;
-
-/**
- * Utility functions for working with byte buffers, such as reading/writing
- * variable-length long numbers.
- */
-@InterfaceAudience.Public
-@InterfaceStability.Evolving
-public final class ByteBufferUtils {
-
-  // "Compressed integer" serialization helper constants.
-  private final static int VALUE_MASK = 0x7f;
-  private final static int NEXT_BIT_SHIFT = 7;
-  private final static int NEXT_BIT_MASK = 1 << 7;
-
-  private ByteBufferUtils() {
-  }
-
-  /**
-   * Similar to {@link WritableUtils#writeVLong(java.io.DataOutput, long)},
-   * but writes to a {@link ByteBuffer}.
-   */
-  public static void writeVLong(ByteBuffer out, long i) {
-    if (i >= -112 && i <= 127) {
-      out.put((byte) i);
-      return;
-    }
-
-    int len = -112;
-    if (i < 0) {
-      i ^= -1L; // take one's complement
-      len = -120;
-    }
-
-    long tmp = i;
-    while (tmp != 0) {
-      tmp = tmp >> 8;
-      len--;
-    }
-
-    out.put((byte) len);
-
-    len = (len < -120) ? -(len + 120) : -(len + 112);
-
-    for (int idx = len; idx != 0; idx--) {
-      int shiftbits = (idx - 1) * 8;
-      long mask = 0xFFL << shiftbits;
-      out.put((byte) ((i & mask) >> shiftbits));
-    }
-  }
-
-  /**
-   * Similar to {@link WritableUtils#readVLong(DataInput)} but reads from a
-   * {@link ByteBuffer}.
-   */
-  public static long readVLong(ByteBuffer in) {
-    byte firstByte = in.get();
-    int len = WritableUtils.decodeVIntSize(firstByte);
-    if (len == 1) {
-      return firstByte;
-    }
-    long i = 0;
-    for (int idx = 0; idx < len-1; idx++) {
-      byte b = in.get();
-      i = i << 8;
-      i = i | (b & 0xFF);
-    }
-    return (WritableUtils.isNegativeVInt(firstByte) ? (i ^ -1L) : i);
-  }
-
-
-  /**
-   * Put in buffer integer using 7 bit encoding. For each written byte:
-   * 7 bits are used to store value
-   * 1 bit is used to indicate whether there is next bit.
-   * @param value Int to be compressed.
-   * @param out Where to put compressed data
-   * @return Number of bytes written.
-   * @throws IOException on stream error
-   */
-   public static int putCompressedInt(OutputStream out, final int value)
-      throws IOException {
-    int i = 0;
-    int tmpvalue = value;
-    do {
-      byte b = (byte) (tmpvalue & VALUE_MASK);
-      tmpvalue >>>= NEXT_BIT_SHIFT;
-      if (tmpvalue != 0) {
-        b |= (byte) NEXT_BIT_MASK;
-      }
-      out.write(b);
-      i++;
-    } while (tmpvalue != 0);
-    return i;
-  }
-
-   /**
-    * Put in output stream 32 bit integer (Big Endian byte order).
-    * @param out Where to put integer.
-    * @param value Value of integer.
-    * @throws IOException On stream error.
-    */
-   public static void putInt(OutputStream out, final int value)
-       throws IOException {
-     for (int i = Bytes.SIZEOF_INT - 1; i >= 0; --i) {
-       out.write((byte) (value >>> (i * 8)));
-     }
-   }
-
-  /**
-   * Copy the data to the output stream and update position in buffer.
-   * @param out the stream to write bytes to
-   * @param in the buffer to read bytes from
-   * @param length the number of bytes to copy
-   */
-  public static void moveBufferToStream(OutputStream out, ByteBuffer in,
-      int length) throws IOException {
-    copyBufferToStream(out, in, in.position(), length);
-    skip(in, length);
-  }
-
-  /**
-   * Copy data from a buffer to an output stream. Does not update the position
-   * in the buffer.
-   * @param out the stream to write bytes to
-   * @param in the buffer to read bytes from
-   * @param offset the offset in the buffer (from the buffer's array offset)
-   *      to start copying bytes from
-   * @param length the number of bytes to copy
-   */
-  public static void copyBufferToStream(OutputStream out, ByteBuffer in,
-      int offset, int length) throws IOException {
-    if (in.hasArray()) {
-      out.write(in.array(), in.arrayOffset() + offset,
-          length);
-    } else {
-      for (int i = 0; i < length; ++i) {
-        out.write(in.get(offset + i));
-      }
-    }
-  }
-
-  public static int putLong(OutputStream out, final long value,
-      final int fitInBytes) throws IOException {
-    long tmpValue = value;
-    for (int i = 0; i < fitInBytes; ++i) {
-      out.write((byte) (tmpValue & 0xff));
-      tmpValue >>>= 8;
-    }
-    return fitInBytes;
-  }
-
-  /**
-   * Check how many bytes are required to store value.
-   * @param value Value which size will be tested.
-   * @return How many bytes are required to store value.
-   */
-  public static int longFitsIn(final long value) {
-    if (value < 0) {
-      return 8;
-    }
-
-    if (value < (1l << 4 * 8)) {
-      // no more than 4 bytes
-      if (value < (1l << 2 * 8)) {
-        if (value < (1l << 1 * 8)) {
-          return 1;
-        }
-        return 2;
-      }
-      if (value < (1l << 3 * 8)) {
-        return 3;
-      }
-      return 4;
-    }
-    // more than 4 bytes
-    if (value < (1l << 6 * 8)) {
-      if (value < (1l << 5 * 8)) {
-        return 5;
-      }
-      return 6;
-    }
-    if (value < (1l << 7 * 8)) {
-      return 7;
-    }
-    return 8;
-  }
-
-  /**
-   * Check how many bytes is required to store value.
-   * @param value Value which size will be tested.
-   * @return How many bytes are required to store value.
-   */
-  public static int intFitsIn(final int value) {
-    if (value < 0) {
-      return 4;
-    }
-
-    if (value < (1 << 2 * 8)) {
-      if (value < (1 << 1 * 8)) {
-        return 1;
-      }
-      return 2;
-    }
-    if (value <= (1 << 3 * 8)) {
-      return 3;
-    }
-    return 4;
-  }
-
-  /**
-   * Read integer from stream coded in 7 bits and increment position.
-   * @return the integer that has been read
-   * @throws IOException
-   */
-  public static int readCompressedInt(InputStream input)
-      throws IOException {
-    int result = 0;
-    int i = 0;
-    byte b;
-    do {
-      b = (byte) input.read();
-      result += (b & VALUE_MASK) << (NEXT_BIT_SHIFT * i);
-      i++;
-      if (i > Bytes.SIZEOF_INT + 1) {
-        throw new IllegalStateException(
-            "Corrupted compressed int (too long: " + (i + 1) + " bytes)");
-      }
-    } while (0 != (b & NEXT_BIT_MASK));
-    return result;
-  }
-
-  /**
-   * Read integer from buffer coded in 7 bits and increment position.
-   * @return Read integer.
-   */
-  public static int readCompressedInt(ByteBuffer buffer) {
-    byte b = buffer.get();
-    if ((b & NEXT_BIT_MASK) != 0) {
-      return (b & VALUE_MASK) + (readCompressedInt(buffer) << NEXT_BIT_SHIFT);
-    }
-    return b & VALUE_MASK;
-  }
-
-  /**
-   * Read long which was written to fitInBytes bytes and increment position.
-   * @param fitInBytes In how many bytes given long is stored.
-   * @return The value of parsed long.
-   * @throws IOException
-   */
-  public static long readLong(InputStream in, final int fitInBytes)
-      throws IOException {
-    long tmpLong = 0;
-    for (int i = 0; i < fitInBytes; ++i) {
-      tmpLong |= (in.read() & 0xffl) << (8 * i);
-    }
-    return tmpLong;
-  }
-
-  /**
-   * Read long which was written to fitInBytes bytes and increment position.
-   * @param fitInBytes In how many bytes given long is stored.
-   * @return The value of parsed long.
-   */
-  public static long readLong(ByteBuffer in, final int fitInBytes) {
-    long tmpLength = 0;
-    for (int i = 0; i < fitInBytes; ++i) {
-      tmpLength |= (in.get() & 0xffl) << (8l * i);
-    }
-    return tmpLength;
-  }
-
-  /**
-   * Asserts that there is at least the given amount of unfilled space
-   * remaining in the given buffer.
-   * @param out typically, the buffer we are writing to
-   * @param length the required space in the buffer
-   * @throws EncoderBufferTooSmallException If there are no enough bytes.
-   */
-  public static void ensureSpace(ByteBuffer out, int length)
-      throws EncoderBufferTooSmallException {
-    if (out.position() + length > out.limit()) {
-      throw new EncoderBufferTooSmallException(
-          "Buffer position=" + out.position() +
-          ", buffer limit=" + out.limit() +
-          ", length to be written=" + length);
-    }
-  }
-
-  /**
-   * Copy the given number of bytes from the given stream and put it at the
-   * current position of the given buffer, updating the position in the buffer.
-   * @param out the buffer to write data to
-   * @param in the stream to read data from
-   * @param length the number of bytes to read/write
-   */
-  public static void copyFromStreamToBuffer(ByteBuffer out,
-      DataInputStream in, int length) throws IOException {
-    if (out.hasArray()) {
-      in.readFully(out.array(), out.position() + out.arrayOffset(),
-          length);
-      skip(out, length);
-    } else {
-      for (int i = 0; i < length; ++i) {
-        out.put(in.readByte());
-      }
-    }
-  }
-
-  /**
-   * Copy from one buffer to another from given offset
-   * @param out destination buffer
-   * @param in source buffer
-   * @param sourceOffset offset in the source buffer
-   * @param length how many bytes to copy
-   */
-  public static void copyFromBufferToBuffer(ByteBuffer out,
-      ByteBuffer in, int sourceOffset, int length) {
-    if (in.hasArray() && out.hasArray()) {
-      System.arraycopy(in.array(), sourceOffset + in.arrayOffset(),
-          out.array(), out.position() +
-          out.arrayOffset(), length);
-      skip(out, length);
-    } else {
-      for (int i = 0; i < length; ++i) {
-        out.put(in.get(sourceOffset + i));
-      }
-    }
-  }
-
-  /**
-   * Find length of common prefix of two parts in the buffer
-   * @param buffer Where parts are located.
-   * @param offsetLeft Offset of the first part.
-   * @param offsetRight Offset of the second part.
-   * @param limit Maximal length of common prefix.
-   * @return Length of prefix.
-   */
-  public static int findCommonPrefix(ByteBuffer buffer, int offsetLeft,
-      int offsetRight, int limit) {
-    int prefix = 0;
-
-    for (; prefix < limit; ++prefix) {
-      if (buffer.get(offsetLeft + prefix) != buffer.get(offsetRight + prefix)) {
-        break;
-      }
-    }
-
-    return prefix;
-  }
-
-  /**
-   * Find length of common prefix in two arrays.
-   * @param left Array to be compared.
-   * @param leftOffset Offset in left array.
-   * @param leftLength Length of left array.
-   * @param right Array to be compared.
-   * @param rightArray Offset in right array.
-   * @param rightLength Length of right array.
-   */
-  public static int findCommonPrefix(
-      byte[] left, int leftOffset, int leftLength,
-      byte[] right, int rightOffset, int rightLength) {
-    int length = Math.min(leftLength, rightLength);
-    int result = 0;
-
-    while (result < length &&
-        left[leftOffset + result] == right[rightOffset + result]) {
-      result++;
-    }
-
-    return result;
-  }
-
-  /**
-   * Check whether two parts in the same buffer are equal.
-   * @param buffer In which buffer there are parts
-   * @param offsetLeft Beginning of first part.
-   * @param lengthLeft Length of the first part.
-   * @param offsetRight Beginning of the second part.
-   * @param lengthRight Length of the second part.
-   * @return
-   */
-  public static boolean arePartsEqual(ByteBuffer buffer,
-      int offsetLeft, int lengthLeft,
-      int offsetRight, int lengthRight) {
-    if (lengthLeft != lengthRight) {
-      return false;
-    }
-
-    if (buffer.hasArray()) {
-      return 0 == Bytes.compareTo(
-          buffer.array(), buffer.arrayOffset() + offsetLeft, lengthLeft,
-          buffer.array(), buffer.arrayOffset() + offsetRight, lengthRight);
-    }
-
-    for (int i = 0; i < lengthRight; ++i) {
-      if (buffer.get(offsetLeft + i) != buffer.get(offsetRight + i)) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  /**
-   * Increment position in buffer.
-   * @param buffer In this buffer.
-   * @param length By that many bytes.
-   */
-  public static void skip(ByteBuffer buffer, int length) {
-    buffer.position(buffer.position() + length);
-  }
-
-}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/KeyValueTestUtil.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
deleted file mode 100644
index af9032b..0000000
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/KeyValueTestUtil.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- *
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hbase;
-
-import org.apache.hadoop.hbase.util.Bytes;
-
-public class KeyValueTestUtil {
-
-  public static KeyValue create(
-      String row,
-      String family,
-      String qualifier,
-      long timestamp,
-      String value)
-  {
-    return create(row, family, qualifier, timestamp, KeyValue.Type.Put, value);
-  }
-
-  public static KeyValue create(
-      String row,
-      String family,
-      String qualifier,
-      long timestamp,
-      KeyValue.Type type,
-      String value)
-  {
-      return new KeyValue(
-          Bytes.toBytes(row),
-          Bytes.toBytes(family),
-          Bytes.toBytes(qualifier),
-          timestamp,
-          type,
-          Bytes.toBytes(value)
-      );
-  }
-}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java
deleted file mode 100644
index 07dcb63..0000000
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/RedundantKVGenerator.java
+++ /dev/null
@@ -1,290 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations
- * under the License.
- */
-package org.apache.hadoop.hbase.io.encoding;
-
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.util.ByteBufferUtils;
-import org.apache.hadoop.io.WritableUtils;
-
-/**
- * Generate list of key values which are very useful to test data block encoding
- * and compression.
- */
-public class RedundantKVGenerator {
-  // row settings
-  static int DEFAULT_NUMBER_OF_ROW_PREFIXES = 10;
-  static int DEFAULT_AVERAGE_PREFIX_LENGTH = 6;
-  static int DEFAULT_PREFIX_LENGTH_VARIANCE = 3;
-  static int DEFAULT_AVERAGE_SUFFIX_LENGTH = 3;
-  static int DEFAULT_SUFFIX_LENGTH_VARIANCE = 3;
-  static int DEFAULT_NUMBER_OF_ROW = 500;
-
-  // qualifier
-  static float DEFAULT_CHANCE_FOR_SAME_QUALIFIER = 0.5f;
-  static float DEFAULT_CHANCE_FOR_SIMILIAR_QUALIFIER = 0.4f;
-  static int DEFAULT_AVERAGE_QUALIFIER_LENGTH = 9;
-  static int DEFAULT_QUALIFIER_LENGTH_VARIANCE = 3;
-
-  static int DEFAULT_COLUMN_FAMILY_LENGTH = 9;
-  static int DEFAULT_VALUE_LENGTH = 8;
-  static float DEFAULT_CHANCE_FOR_ZERO_VALUE = 0.5f;
-
-  static int DEFAULT_BASE_TIMESTAMP_DIVIDE = 1000000;
-  static int DEFAULT_TIMESTAMP_DIFF_SIZE = 100000000;
-
-  /**
-   * Default constructor, assumes all parameters from class constants.
-   */
-  public RedundantKVGenerator() {
-    this(new Random(42L),
-        DEFAULT_NUMBER_OF_ROW_PREFIXES,
-        DEFAULT_AVERAGE_PREFIX_LENGTH,
-        DEFAULT_PREFIX_LENGTH_VARIANCE,
-        DEFAULT_AVERAGE_SUFFIX_LENGTH,
-        DEFAULT_SUFFIX_LENGTH_VARIANCE,
-        DEFAULT_NUMBER_OF_ROW,
-
-        DEFAULT_CHANCE_FOR_SAME_QUALIFIER,
-        DEFAULT_CHANCE_FOR_SIMILIAR_QUALIFIER,
-        DEFAULT_AVERAGE_QUALIFIER_LENGTH,
-        DEFAULT_QUALIFIER_LENGTH_VARIANCE,
-
-        DEFAULT_COLUMN_FAMILY_LENGTH,
-        DEFAULT_VALUE_LENGTH,
-        DEFAULT_CHANCE_FOR_ZERO_VALUE,
-
-        DEFAULT_BASE_TIMESTAMP_DIVIDE,
-        DEFAULT_TIMESTAMP_DIFF_SIZE
-    );
-  }
-
-
-  /**
-   * Various configuration options for generating key values
-   * @param randomizer pick things by random
-   */
-  public RedundantKVGenerator(Random randomizer,
-      int numberOfRowPrefixes,
-      int averagePrefixLength,
-      int prefixLengthVariance,
-      int averageSuffixLength,
-      int suffixLengthVariance,
-      int numberOfRows,
-
-      float chanceForSameQualifier,
-      float chanceForSimiliarQualifier,
-      int averageQualifierLength,
-      int qualifierLengthVariance,
-
-      int columnFamilyLength,
-      int valueLength,
-      float chanceForZeroValue,
-
-      int baseTimestampDivide,
-      int timestampDiffSize
-      ) {
-    this.randomizer = randomizer;
-
-    this.numberOfRowPrefixes = numberOfRowPrefixes;
-    this.averagePrefixLength = averagePrefixLength;
-    this.prefixLengthVariance = prefixLengthVariance;
-    this.averageSuffixLength = averageSuffixLength;
-    this.suffixLengthVariance = suffixLengthVariance;
-    this.numberOfRows = numberOfRows;
-
-    this.chanceForSameQualifier = chanceForSameQualifier;
-    this.chanceForSimiliarQualifier = chanceForSimiliarQualifier;
-    this.averageQualifierLength = averageQualifierLength;
-    this.qualifierLengthVariance = qualifierLengthVariance;
-
-    this.columnFamilyLength = columnFamilyLength;
-    this.valueLength = valueLength;
-    this.chanceForZeroValue = chanceForZeroValue;
-
-    this.baseTimestampDivide = baseTimestampDivide;
-    this.timestampDiffSize = timestampDiffSize;
-  }
-
-  /** Used to generate dataset */
-  private Random randomizer;
-
-  // row settings
-  private int numberOfRowPrefixes;
-  private int averagePrefixLength = 6;
-  private int prefixLengthVariance = 3;
-  private int averageSuffixLength = 3;
-  private int suffixLengthVariance = 3;
-  private int numberOfRows = 500;
-
-  // qualifier
-  private float chanceForSameQualifier = 0.5f;
-  private float chanceForSimiliarQualifier = 0.4f;
-  private int averageQualifierLength = 9;
-  private int qualifierLengthVariance = 3;
-
-  private int columnFamilyLength = 9;
-  private int valueLength = 8;
-  private float chanceForZeroValue = 0.5f;
-
-  private int baseTimestampDivide = 1000000;
-  private int timestampDiffSize = 100000000;
-
-  private List<byte[]> generateRows() {
-    // generate prefixes
-    List<byte[]> prefixes = new ArrayList<byte[]>();
-    prefixes.add(new byte[0]);
-    for (int i = 1; i < numberOfRowPrefixes; ++i) {
-      int prefixLength = averagePrefixLength;
-      prefixLength += randomizer.nextInt(2 * prefixLengthVariance + 1) -
-          prefixLengthVariance;
-      byte[] newPrefix = new byte[prefixLength];
-      randomizer.nextBytes(newPrefix);
-      prefixes.add(newPrefix);
-    }
-
-    // generate rest of the row
-    List<byte[]> rows = new ArrayList<byte[]>();
-    for (int i = 0; i < numberOfRows; ++i) {
-      int suffixLength = averageSuffixLength;
-      suffixLength += randomizer.nextInt(2 * suffixLengthVariance + 1) -
-          suffixLengthVariance;
-      int randomPrefix = randomizer.nextInt(prefixes.size());
-      byte[] row = new byte[prefixes.get(randomPrefix).length +
-                            suffixLength];
-      rows.add(row);
-    }
-
-    return rows;
-  }
-
-  /**
-   * Generate test data useful to test encoders.
-   * @param howMany How many Key values should be generated.
-   * @return sorted list of key values
-   */
-  public List<KeyValue> generateTestKeyValues(int howMany) {
-    List<KeyValue> result = new ArrayList<KeyValue>();
-
-    List<byte[]> rows = generateRows();
-    Map<Integer, List<byte[]>> rowsToQualifier =
-        new HashMap<Integer, List<byte[]>>();
-
-    byte[] family = new byte[columnFamilyLength];
-    randomizer.nextBytes(family);
-
-    long baseTimestamp = Math.abs(randomizer.nextLong()) /
-        baseTimestampDivide;
-
-    byte[] value = new byte[valueLength];
-
-    for (int i = 0; i < howMany; ++i) {
-      long timestamp = baseTimestamp + randomizer.nextInt(
-          timestampDiffSize);
-      Integer rowId = randomizer.nextInt(rows.size());
-      byte[] row = rows.get(rowId);
-
-      // generate qualifier, sometimes it is same, sometimes similar,
-      // occasionally completely different
-      byte[] qualifier;
-      float qualifierChance = randomizer.nextFloat();
-      if (!rowsToQualifier.containsKey(rowId) ||
-          qualifierChance > chanceForSameQualifier +
-          chanceForSimiliarQualifier) {
-        int qualifierLength = averageQualifierLength;
-        qualifierLength +=
-            randomizer.nextInt(2 * qualifierLengthVariance + 1) -
-            qualifierLengthVariance;
-        qualifier = new byte[qualifierLength];
-        randomizer.nextBytes(qualifier);
-
-        // add it to map
-        if (!rowsToQualifier.containsKey(rowId)) {
-          rowsToQualifier.put(rowId, new ArrayList<byte[]>());
-        }
-        rowsToQualifier.get(rowId).add(qualifier);
-      } else if (qualifierChance > chanceForSameQualifier) {
-        // similar qualifier
-        List<byte[]> previousQualifiers = rowsToQualifier.get(rowId);
-        byte[] originalQualifier = previousQualifiers.get(
-            randomizer.nextInt(previousQualifiers.size()));
-
-        qualifier = new byte[originalQualifier.length];
-        int commonPrefix = randomizer.nextInt(qualifier.length);
-        System.arraycopy(originalQualifier, 0, qualifier, 0, commonPrefix);
-        for (int j = commonPrefix; j < qualifier.length; ++j) {
-          qualifier[j] = (byte) (randomizer.nextInt() & 0xff);
-        }
-
-        rowsToQualifier.get(rowId).add(qualifier);
-      } else {
-        // same qualifier
-        List<byte[]> previousQualifiers = rowsToQualifier.get(rowId);
-        qualifier = previousQualifiers.get(
-            randomizer.nextInt(previousQualifiers.size()));
-      }
-
-      if (randomizer.nextFloat() < chanceForZeroValue) {
-        for (int j = 0; j < value.length; ++j) {
-          value[j] = (byte) 0;
-        }
-      } else {
-        randomizer.nextBytes(value);
-      }
-
-      result.add(new KeyValue(row, family, qualifier, timestamp, value));
-    }
-
-    Collections.sort(result, KeyValue.COMPARATOR);
-
-    return result;
-  }
-
-  /**
-   * Convert list of KeyValues to byte buffer.
-   * @param keyValues list of KeyValues to be converted.
-   * @return buffer with content from key values
-   */
-  public static ByteBuffer convertKvToByteBuffer(List<KeyValue> keyValues,
-      boolean includesMemstoreTS) {
-    int totalSize = 0;
-    for (KeyValue kv : keyValues) {
-      totalSize += kv.getLength();
-      if (includesMemstoreTS) {
-        totalSize += WritableUtils.getVIntSize(kv.getMemstoreTS());
-      }
-    }
-
-    ByteBuffer result = ByteBuffer.allocate(totalSize);
-    for (KeyValue kv : keyValues) {
-      result.put(kv.getBuffer(), kv.getOffset(), kv.getLength());
-      if (includesMemstoreTS) {
-        ByteBufferUtils.writeVLong(result, kv.getMemstoreTS());
-      }
-    }
-
-    return result;
-  }
-
-}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
index c79c7d6..6211512 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestDataBlockEncoders.java
@@ -33,8 +33,9 @@ import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.LargeTests;
 import org.apache.hadoop.hbase.io.hfile.Compression;
-import org.apache.hadoop.hbase.io.hfile.HFileBlock;
+import org.apache.hadoop.hbase.io.hfile.HFileBlockConstants;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.test.RedundantKVGenerator;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.runner.RunWith;
@@ -52,7 +53,7 @@ public class TestDataBlockEncoders {
   static int NUM_RANDOM_SEEKS = 10000;
 
   private static int ENCODED_DATA_OFFSET =
-      HFileBlock.HEADER_SIZE + DataBlockEncoding.ID_SIZE;
+      HFileBlockConstants.HEADER_SIZE + DataBlockEncoding.ID_SIZE;
 
   private RedundantKVGenerator generator = new RedundantKVGenerator();
   private Random randomizer = new Random(42l);
@@ -73,7 +74,7 @@ public class TestDataBlockEncoders {
     DataBlockEncoder encoder = encoding.getEncoder();
     if (encoder != null) {
       return encoder.newDataBlockEncodingContext(algo, encoding,
-          HFileBlock.DUMMY_HEADER);
+          HFileBlockConstants.DUMMY_HEADER);
     } else {
       return new HFileBlockDefaultEncodingContext(algo, encoding);
     }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
index 64bdf17..bbb4137 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/TestEncodedSeekers.java
@@ -27,19 +27,17 @@ import java.util.Map;
 
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.MediumTests;
-import org.apache.hadoop.hbase.SmallTests;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
-import org.apache.hadoop.hbase.io.hfile.Compression.Algorithm;
-import org.apache.hadoop.hbase.io.hfile.HFile;
 import org.apache.hadoop.hbase.io.hfile.LruBlockCache;
 import org.apache.hadoop.hbase.regionserver.HRegion;
-import org.apache.hadoop.hbase.regionserver.StoreFile.BloomType;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.MultiThreadedWriter;
+import org.apache.hadoop.hbase.util.StringUtils;
 import org.apache.hadoop.hbase.util.test.LoadTestKVGenerator;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
@@ -59,9 +57,10 @@ public class TestEncodedSeekers {
   private static final byte[] CF_BYTES = Bytes.toBytes(CF_NAME);
   private static final int MAX_VERSIONS = 5;
 
+  private static final int BLOCK_SIZE = 1024 * 64;
   private static final int MIN_VALUE_SIZE = 30;
   private static final int MAX_VALUE_SIZE = 60;
-  private static final int NUM_ROWS = 1000;
+  private static final int NUM_ROWS = 1003;
   private static final int NUM_COLS_PER_ROW = 20;
   private static final int NUM_HFILES = 4;
   private static final int NUM_ROWS_PER_FLUSH = NUM_ROWS / NUM_HFILES;
@@ -101,61 +100,68 @@ public class TestEncodedSeekers {
             .setMaxVersions(MAX_VERSIONS)
             .setDataBlockEncoding(encoding)
             .setEncodeOnDisk(encodeOnDisk)
+            .setBlocksize(BLOCK_SIZE)
     );
-    LoadTestKVGenerator dataGenerator = new LoadTestKVGenerator(
-        MIN_VALUE_SIZE, MAX_VALUE_SIZE);
 
-    // Write
-    for (int i = 0; i < NUM_ROWS; ++i) {
+    doPuts(region);
+
+    doGets(region);
+    region.compactStores();
+    doGets(region);
+
+    Map<DataBlockEncoding, Integer> encodingCounts = cache.getEncodingCountsForTest();
+
+    // Ensure that compactions don't pollute the cache with unencoded blocks
+    // in case of in-cache-only encoding.
+    System.err.println("encodingCounts=" + encodingCounts);
+    assertEquals(1, encodingCounts.size());
+    DataBlockEncoding encodingInCache = encodingCounts.keySet().iterator().next();
+    assertEquals(encoding, encodingInCache);
+    assertTrue(encodingCounts.get(encodingInCache) > 0);
+  }
+  
+  
+  private void doPuts(HRegion region) throws IOException{
+    LoadTestKVGenerator dataGenerator = new LoadTestKVGenerator(MIN_VALUE_SIZE, MAX_VALUE_SIZE);
+     for (int i = 0; i < NUM_ROWS; ++i) {
       byte[] key = MultiThreadedWriter.longToByteArrayKey(i);
       for (int j = 0; j < NUM_COLS_PER_ROW; ++j) {
         Put put = new Put(key);
         String colAsStr = String.valueOf(j);
+        byte[] col = Bytes.toBytes(colAsStr);
         byte[] value = dataGenerator.generateRandomSizeValue(i, colAsStr);
         put.add(CF_BYTES, Bytes.toBytes(colAsStr), value);
+        if(VERBOSE){
+          KeyValue kvPut = new KeyValue(key, CF_BYTES, col, value);
+          System.out.println(StringUtils.padFront(i+"", ' ', 4)+" "+kvPut);
+        }
         region.put(put);
       }
       if (i % NUM_ROWS_PER_FLUSH == 0) {
         region.flushcache();
       }
     }
-
-    for (int doneCompaction = 0; doneCompaction <= 1; ++doneCompaction) {
-      // Read
-      for (int i = 0; i < NUM_ROWS; ++i) {
-        final byte[] rowKey = MultiThreadedWriter.longToByteArrayKey(i);
-        for (int j = 0; j < NUM_COLS_PER_ROW; ++j) {
-          if (VERBOSE) {
-            System.err.println("Reading row " + i + ", column " +  j);
-          }
-          final String qualStr = String.valueOf(j);
-          final byte[] qualBytes = Bytes.toBytes(qualStr);
-          Get get = new Get(rowKey);
-          get.addColumn(CF_BYTES, qualBytes);
-          Result result = region.get(get, null);
-          assertEquals(1, result.size());
-          assertTrue(LoadTestKVGenerator.verify(Bytes.toString(rowKey), qualStr,
-              result.getValue(CF_BYTES, qualBytes)));
+  }
+  
+  
+  private void doGets(HRegion region) throws IOException{
+    for (int i = 0; i < NUM_ROWS; ++i) {
+      final byte[] rowKey = MultiThreadedWriter.longToByteArrayKey(i);
+      for (int j = 0; j < NUM_COLS_PER_ROW; ++j) {
+        final String qualStr = String.valueOf(j);
+        if (VERBOSE) {
+          System.out.println("Reading row " + i + ", column " + j + " " + Bytes.toString(rowKey)+"/"
+        +qualStr);
         }
-      }
-
-      if (doneCompaction == 0) {
-        // Compact, then read again at the next loop iteration.
-        region.compactStores();
+        final byte[] qualBytes = Bytes.toBytes(qualStr);
+        Get get = new Get(rowKey);
+        get.addColumn(CF_BYTES, qualBytes);
+        Result result = region.get(get, null);
+        assertEquals(1, result.size());
+        assertTrue(LoadTestKVGenerator.verify(Bytes.toString(rowKey), qualStr,
+            result.getValue(CF_BYTES, qualBytes)));
       }
     }
-
-    Map<DataBlockEncoding, Integer> encodingCounts =
-        cache.getEncodingCountsForTest();
-
-    // Ensure that compactions don't pollute the cache with unencoded blocks
-    // in case of in-cache-only encoding.
-    System.err.println("encodingCounts=" + encodingCounts);
-    assertEquals(1, encodingCounts.size());
-    DataBlockEncoding encodingInCache =
-        encodingCounts.keySet().iterator().next();
-    assertEquals(encoding, encodingInCache);
-    assertTrue(encodingCounts.get(encodingInCache) > 0);
   }
 
 }
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/roundtrip/TestHFileRoundTrip.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/roundtrip/TestHFileRoundTrip.java
new file mode 100644
index 0000000..780bf4c
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/roundtrip/TestHFileRoundTrip.java
@@ -0,0 +1,204 @@
+package org.apache.hadoop.hbase.io.encoding.roundtrip;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.KeyValueTestUtil;
+import org.apache.hadoop.hbase.KeyValueUtils;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder;
+import org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreFile.BloomType;
+import org.apache.hadoop.hbase.regionserver.StoreFileScanner;
+import org.apache.hadoop.hbase.util.FileUtils;
+import org.apache.hadoop.hbase.util.test.RedundantKVGenerator;
+import org.apache.hadoop.util.StringUtils;
+import org.junit.Assert;
+
+import com.google.common.collect.Lists;
+
+public class TestHFileRoundTrip {
+
+	HBaseTestingUtility testingUtility = new HBaseTestingUtility();
+	Configuration configuration = testingUtility.getConfiguration();
+	CacheConfig cacheConfig = new CacheConfig(configuration);
+
+	public TestHFileRoundTrip() {
+	}
+
+	/**
+	 * Command line interface:
+	 * 
+	 * @param args
+	 *            Takes one argument - file size.
+	 * @throws IOException
+	 *             if there is a bug while reading from disk
+	 */
+	public static void main(final String[] args) throws IOException {
+
+    String rootDir = "/home/mcorgan/junk/prefix-tree-test-hfiles";
+		List<String> tables = Lists.newArrayList("Table1");
+		List<Integer> blockSizes = Lists.newArrayList(
+				1<<16);
+    List<DataBlockEncoding> encodings = Lists.newArrayList(
+        DataBlockEncoding.NONE, DataBlockEncoding.PREFIX//DataBlockEncoding.TREE
+        );
+
+    int numCells = 100;
+
+		ArrayList<KeyValue> cells = Lists
+				.newArrayList(new RedundantKVGenerator()
+						.generateTestKeyValues(numCells));
+    System.out.println(KeyValueTestUtil.toStringWithPadding(cells, true));
+		System.out.println("generated "+cells.size()+" cells, total bytes="
+        + StringUtils.humanReadableInt(KeyValueUtils.totalLengthWithMemstoreTSs(
+            cells, true)));
+
+		List<TestResult> results = Lists.newArrayList();
+		TestHFileRoundTrip tester = new TestHFileRoundTrip();
+
+		for (String table : tables) {
+			FileUtils.createFileAndParents(new File(rootDir + "/" + table));
+      for (DataBlockEncoding algorithm : encodings) {
+				for (Integer blockSize : blockSizes) {
+					for (boolean record : new boolean[] { false, true }) {
+						tester.runTestsOnFile(rootDir, table, blockSize,
+								algorithm, cells, results, record);
+					}
+				}
+			}
+		}
+
+		System.out.println(TestResult.HEADER);
+		for (TestResult result : results) {
+			System.out.println(result);
+		}
+
+		System.exit(0);
+	}
+
+	public void runTestsOnFile(String rootDir, String table, int blockSize,
+      DataBlockEncoding blockEncoder, ArrayList<KeyValue> cells,
+			List<TestResult> results, boolean record) throws IOException {
+
+		TestResult result = new TestResult();
+		if (record) {
+			results.add(result);
+		}
+		result.encoding = blockEncoder;
+    result.blockSize = blockSize;
+
+		String hfilePath = rootDir + "/" + table + "/" + table + "_"
+				+ blockSize;
+		Path path = new Path(hfilePath);
+		HFileDataBlockEncoder encoder = new HFileDataBlockEncoderImpl(blockEncoder,
+        blockEncoder);
+
+		StoreFile.Writer writer = new StoreFile.WriterBuilder(configuration, cacheConfig, 
+				testingUtility.getTestFileSystem(), blockSize)
+		    .withFilePath(path)
+		    .withDataBlockEncoder(encoder)
+		    .build();
+
+		System.gc();
+		long writeStartNs = System.nanoTime();
+		for (KeyValue cell : cells) {
+			writer.append(cell);
+			++result.numCellsWritten;
+			result.numKeyBytes += cell.getKeyLength();
+			result.numValueBytes += cell.getValueLength();
+			result.numKeyValueBytes += cell.getLength();
+		}
+		result.writeNs = System.nanoTime() - writeStartNs;
+
+		Path writerPath = writer.getPath();
+		writer.close();
+
+		StoreFile storeFile = new StoreFile(testingUtility.getTestFileSystem(),
+        path, testingUtility.getConfiguration(), cacheConfig, BloomType.NONE,
+        encoder);
+		StoreFile.Reader reader = storeFile.createReader();
+		// Assert.assertEquals(uncompressedBytesWritten, uncompressedBytes);
+		StoreFileScanner scanner = reader.getStoreFileScanner(true, false);
+		scanner.seek(KeyValue.LOWESTKEY);
+		int i = -1;
+		System.gc();
+		long readStartNs = System.nanoTime();
+		while (true) {
+			++i;
+			KeyValue kv = scanner.next();
+			if (kv == null) {
+				break;
+			}
+			++result.numCellsRead;
+			boolean same = kv.equals(cells.get(i));
+			if (!same) {
+				System.out.println("mismatch on " + i);
+				System.out.println(" in:" + cells.get(i));
+				System.out.println("out:" + kv);
+			}
+			Assert.assertTrue(same);
+		}
+		result.readNs = System.nanoTime() - readStartNs;
+		reader.close(true);
+
+		Assert.assertEquals(result.numCellsWritten, result.numCellsRead);
+		System.out.println(result);
+	}
+
+	protected static class TestResult {
+		String table;
+		int blockSize;
+    DataBlockEncoding encoding;
+		long numCellsWritten = 0, numCellsRead = 0;;
+		long numKeyBytes = 0, numValueBytes = 0, numKeyValueBytes = 0;
+		long writeNs, readNs;
+
+		long ASSUME_CYCLES_PER_NS = 2, BYTES_IN_MB = 1 << 20,
+				NANOSEC_IN_SEC = 1000 * 1000 * 1000;
+
+		static String HEADER = "table,encoding,blockSize,numCells,avgKeyBytes,avgValueBytes,"
+				+ "writeMB/s,readMB/s";
+
+		@Override
+		public String toString() {
+			StringBuilder sb = new StringBuilder();
+			sb.append(table);
+			sb.append("," + encoding.name());
+			sb.append("," + blockSize);
+			sb.append("," + numCellsWritten);
+			sb.append("," + numCellsRead);
+			sb.append("," + getAvgKeyBytes());
+			sb.append("," + getAvgValueBytes());
+			sb.append("," + getWriteMegabytesPerSecond().longValue());
+			sb.append("," + getReadMegabytesPerSecond().longValue());
+			return sb.toString();
+		}
+
+		Long getAvgKeyBytes() {
+			return numKeyBytes / numCellsWritten;
+		}
+
+		Long getAvgValueBytes() {
+			return numValueBytes / numCellsWritten;
+		}
+
+		Double getWriteMegabytesPerSecond() {
+			double seconds = (double) writeNs / (double) NANOSEC_IN_SEC;
+			return (double) (numKeyValueBytes / BYTES_IN_MB) / seconds;
+		}
+
+		Double getReadMegabytesPerSecond() {
+			double seconds = (double) readNs / (double) NANOSEC_IN_SEC;
+			return (double) (numKeyValueBytes / BYTES_IN_MB) / seconds;
+		}
+	}
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkMain.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkMain.java
new file mode 100644
index 0000000..1cd4433
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkMain.java
@@ -0,0 +1,112 @@
+package org.apache.hadoop.hbase.io.encoding.seek;
+
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+import org.apache.hbase.codec.prefixtree.decode.PrefixTreeArrayScanner;
+
+import com.google.common.collect.Lists;
+
+/**
+ * - set rootDir variable below to something on your dev machine
+ * - note (or set and compact) the block size of an HFile in your cluster
+ * - copy HFiles from your cluster to (example): [rootDir]/MonthlyListingSummary/256KB/a
+ * - rename the file "a"
+ * - add jvm options like: -Xms4g -Xmx4g -XX:+UseConcMarkSweepGC  -XX:NewSize=512M -XX:+PrintGCDetails
+ * - comment in/out the options below and run
+ * - avoid debug mode for performance
+ * - copy paste csv output with headers into spreadsheet
+ * - (in LibreOffice), go to Data -> Text To Columns
+ */
+public class SeekBenchmarkMain {
+  private static final Log LOG = LogFactory.getLog(SeekBenchmarkMain.class);
+
+  public static void main(final String[] args) throws Exception {
+    String rootDir = "/home/mcorgan/prefix-tree-test";
+
+    List<String> tables = Lists.newArrayList();
+    // tables.add("Count5s");
+    // tables.add("Event");
+    tables.add("MonthlyListingSummary");
+    // tables.add("RandomValue");
+    // tables.add("Trace");
+
+    // filename must be only hex characters
+    String hFileName = "a";// StringTool.repeat('0', 32);
+
+    List<String> blockSizes = Lists.newArrayList();
+    // blockSizes.add("1KB");
+    // blockSizes.add("4KB");
+//    blockSizes.add("16KB");
+    blockSizes.add("64KB");
+//    blockSizes.add("256KB");
+    // blockSizes.add("1MB");
+
+    List<DataBlockEncoding> encodings = Lists.newArrayList();
+//    encodings.add(null);//is this valid anymore?
+//    encodings.add(DataBlockEncoding.NONE);
+//    encodings.add(DataBlockEncoding.PREFIX);
+//    encodings.add(DataBlockEncoding.DIFF);
+//    encodings.add(DataBlockEncoding.FAST_DIFF);
+    encodings.add(DataBlockEncoding.PREFIX_TREE);
+
+    List<Integer> readerThreadCounts = Lists.newArrayList();
+    readerThreadCounts.add(1);
+    // readerThreadCounts.add(2);
+    // readerThreadCounts.add(4);
+    // readerThreadCounts.add(8);
+    // readerThreadCounts.add(16);
+    // readerThreadCounts.add(32);
+
+    boolean doWarmupRun = true;
+
+    int numberOfSeeks = 100000;
+
+    // boolean[] preads = new boolean[]{false, true};
+    // boolean[] preads = new boolean[]{true};
+    boolean[] preads = new boolean[] { false };
+    // if(numReaderThreads <= 1){ preads = new boolean[]{false, true}; }
+
+    List<SeekBenchmarkResult> results = Lists.newArrayList();
+
+    for (String table : tables) {
+      for (DataBlockEncoding encoding : encodings) {
+        for (String blockSize : blockSizes) {
+          for (Integer numReaderThreads : readerThreadCounts) {
+            for (Boolean pread : preads) {
+              SingleSeekBenchmark benchmark = null;
+              try {
+                benchmark = new SingleSeekBenchmark();
+                benchmark.rootDir = rootDir;
+                benchmark.table = table;
+                benchmark.blockSize = blockSize;
+                benchmark.blockCacheHFileName = hFileName;
+                benchmark.encodingInMemory = encoding;
+                benchmark.numReaderThreads = numReaderThreads;
+                benchmark.numberOfSeeks = numberOfSeeks;
+                benchmark.pread = pread;
+                if (doWarmupRun) {// usually 10% slower than second run
+                  benchmark.runTestsOnFile(false, results);
+                }
+                benchmark.runTestsOnFile(true, results);
+              } catch (Exception e) {
+                e.printStackTrace();
+              } finally {
+                benchmark.testingUtility.shutdownMiniCluster();
+              }
+            }
+          }
+        }
+      }
+    }
+
+    System.out.println(CollectionUtils.getFirst(results).getHeader());
+    for (SeekBenchmarkResult result : results) {
+      System.out.println(result.toString());
+    }
+    System.exit(0);
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkResult.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkResult.java
new file mode 100644
index 0000000..34158b6
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeekBenchmarkResult.java
@@ -0,0 +1,202 @@
+package org.apache.hadoop.hbase.io.encoding.seek;
+
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.util.StringUtils;
+
+class SeekBenchmarkResult {
+
+  String table, blockSize;
+  long numHardwareThreads, numReaderThreads;
+  boolean pread;
+  DataBlockEncoding encodingOnDisk, encodingInMemory;
+  boolean inMemory;
+  long numCells = 0;
+  long numKeyBytes = 0, numValueBytes = 0, numKeyValueBytes = 0;
+  long numBlocks, numNonLeafRowNodes = 0, numLeafRowNodes = 0,
+      numNonLeafQualifierNodes = 0, numLeafQualifierNodes = 0,
+      numRawBytes = 0, numEncodedBytes = 0;
+  long nsForEncoding, nsAllocateBytes, nsAddTotal, 
+      nsCompileTimestamps, nsCompileQualifiers, nsCompileRows;
+  long nsAddRow=0, nsAddFamily=0, nsAddQualifier=0, nsAddTimestamp=0, nsAddMemstoreTimestamp=0, nsAddType=0, nsAddValue=0;
+  long nsWriteRows=0, nsWriteFamilies=0, nsWriteQualifiers=0, nsWriteTimestamps=0, nsWriteMemstoreTimestamps=0, nsWriteValues=0;
+  long nsForScanning, numIterationGarbageBytes;
+  long nsForSeeking, numSeeks = 0, numSeekGarbageBytes;
+  boolean includePerformance, includeSettings, includeDataStats,
+      includePtDetails;
+
+  long ASSUME_CYCLES_PER_NS = 2;
+
+  public SeekBenchmarkResult(boolean includePerformance,
+      boolean includeSettings, boolean includeDataStats,
+      boolean includePtDetails) {
+    this.includePerformance = includePerformance;
+    this.includeSettings = includeSettings;
+    this.includeDataStats = includeDataStats;
+    this.includePtDetails = includePtDetails;
+  }
+
+  public String getHeader() {
+    String header = "table,encoding,rawBlockSize";
+    if (includePerformance) {
+      header += ",encodingMB/s,scanningMB/s,seeks/s,cycles/seek";
+    }
+    if (includeSettings) {
+      header += ",numHardwareThreads,numReaderThreads,pread";
+    }
+    if (includeDataStats) {
+      header += ",numBlocks,numNonLeafRowNodes,numLeafRowNodes,numNonLeafQualifierNodes,numLeafQualifierNodes"
+      		+ ",avgRawBlockBytes,avgEncodedBlockBytes,avgCellsPerBlock"
+          + ",numCells,avgRawKeyBytes,avgEncodedKeyBytes,keyCompressionRatio"
+          + ",avgValueBytes,compressionRatio";
+    }
+    if (includePtDetails) {
+      header += ",totalMs,allocateBytesMs,addTotalMs,addRowMs,addFamilyMs,addQualifierMs,addTimestampMs,addMemstoreTimestampMs"
+    	  +",addTypeMs,addValueMs";
+      header += ",compileTimestampMs,compileColumnMs,compileRowMs";
+      header += ",writeRowsMs,writeFamiliesMs,writeQualifiersMs,writeTimestampsMs,writeMemstoreTimestampsMs,writeValuesMs";
+    }
+    return header;
+  }
+
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append(table);
+    sb.append("," + (encodingInMemory==null?"NULL":encodingInMemory.name()));
+    sb.append("," + blockSize);
+    if (includePerformance) {
+      sb.append("," + StringUtils.limitDecimalTo2(getEncodingMBPerSec()));
+      sb.append("," + StringUtils.limitDecimalTo2(getScanningMBPerSec()));
+      sb.append("," + getSeeksPerSec());
+      sb.append("," + getCyclesPerSeek());
+    }
+    if (includeSettings) {
+      sb.append("," + numHardwareThreads);
+      sb.append("," + numReaderThreads);
+      sb.append("," + pread);
+    }
+    if (includeDataStats) {
+      sb.append("," + (numBlocks == 0 ? "?" : numBlocks));
+      sb.append("," + numNonLeafRowNodes);
+      sb.append("," + numLeafRowNodes);
+      sb.append("," + numNonLeafQualifierNodes);
+      sb.append("," + numLeafQualifierNodes);
+      sb.append(","
+          + (getAvgRawBlockBytes() == 0 ? "?" : getAvgRawBlockBytes()));
+      sb.append(","
+          + (getAvgEncodedBlockBytes() == 0 ? "?" : getAvgEncodedBlockBytes()));
+      sb.append(","
+          + (getAvgCellsPerBlock() == 0 ? "?" : getAvgCellsPerBlock()));
+      sb.append("," + numCells);
+      sb.append("," + getAvgRawKeyBytes());
+      sb.append("," + getAvgEncodedKeyBytes());
+      sb.append("," + StringUtils.limitDecimalTo2(getKeyCompressionRatio()));
+      sb.append("," + getAvgValueBytes());
+      // includes values which we can't compress
+      sb.append("," + StringUtils.limitDecimalTo2(getCompressionRatio()));
+    }
+    if (includePtDetails) {
+      sb.append("," + nsToMs(nsForEncoding));
+      sb.append("," + nsToMs(nsAllocateBytes));
+      sb.append("," + nsToMs(nsAddTotal));
+      sb.append("," + nsToMs(nsAddRow));
+      sb.append("," + nsToMs(nsAddFamily));
+      sb.append("," + nsToMs(nsAddQualifier));
+      sb.append("," + nsToMs(nsAddTimestamp));
+      sb.append("," + nsToMs(nsAddMemstoreTimestamp));
+      sb.append("," + nsToMs(nsAddType));
+      sb.append("," + nsToMs(nsAddValue));
+      sb.append("," + nsToMs(nsCompileTimestamps));
+      sb.append("," + nsToMs(nsCompileQualifiers));
+      sb.append("," + nsToMs(nsCompileRows));
+      sb.append("," + nsToMs(nsWriteRows));
+      sb.append("," + nsToMs(nsWriteFamilies));
+      sb.append("," + nsToMs(nsWriteQualifiers));
+      sb.append("," + nsToMs(nsWriteTimestamps));
+      sb.append("," + nsToMs(nsWriteMemstoreTimestamps));
+      sb.append("," + nsToMs(nsWriteValues));
+    }
+    return sb.toString();
+  }
+
+  long getAvgRawKeyBytes() {
+    return numKeyBytes / numCells;
+  }
+
+  long getAvgEncodedKeyBytes() {
+    if (numEncodedBytes == 0) {
+      return getAvgRawKeyBytes();
+    }
+    long numEncodedKeyBytes = numEncodedBytes - numValueBytes;
+    return numEncodedKeyBytes / numCells;
+  }
+
+  long getAvgValueBytes() {
+    return numValueBytes / numCells;
+  }
+
+  double getEncodingMBPerSec() {
+    double mb = (double) numKeyValueBytes
+        / (double) SingleSeekBenchmark.BYTES_IN_MEGABYTES;
+    double sec = (double) nsForEncoding
+        / (double) SingleSeekBenchmark.NANOSEC_IN_SEC;
+    return mb / sec;
+  }
+
+  double getScanningMBPerSec() {
+    double mb = (double) numKeyValueBytes
+        / (double) SingleSeekBenchmark.BYTES_IN_MEGABYTES;
+    double sec = (double) nsForScanning
+        / (double) SingleSeekBenchmark.NANOSEC_IN_SEC;
+    return mb / sec;
+  }
+
+  long getSeeksPerSec() {
+    return (long) (numSeeks * SingleSeekBenchmark.NANOSEC_IN_SEC)
+        / nsForSeeking;
+  }
+
+  long getCyclesPerSeek() {
+    return ASSUME_CYCLES_PER_NS * nsForSeeking / numSeeks;
+  }
+
+  double getCompressionRatio() {
+    if (numRawBytes == 0 || numEncodedBytes == 0) {
+      return 1.0;
+    }
+    return (double) numRawBytes / (double) numEncodedBytes;
+  }
+
+  double getKeyCompressionRatio() {
+    if (numRawBytes == 0 || numEncodedBytes == 0) {
+      return 1.0;
+    }
+    return (double) getAvgRawKeyBytes() / (double) getAvgEncodedKeyBytes();
+  }
+
+  long getAvgRawBlockBytes() {
+    if (numRawBytes == 0 || numBlocks == 0) {
+      return 0;
+    }
+    return numRawBytes / numBlocks;
+  }
+
+  long getAvgEncodedBlockBytes() {
+    if (numEncodedBytes == 0 || numBlocks == 0) {
+      return 0;
+    }
+    return numEncodedBytes / numBlocks;
+  }
+
+  long getAvgCellsPerBlock() {
+    if (numCells == 0 || numBlocks == 0) {
+      return 0;
+    }
+    return numCells / numBlocks;
+  }
+
+  
+	public static Long nsToMs(Long ns) {
+		double ms = (double) ns / (double) 1000000;
+		return (long) ms;
+	}
+}
\ No newline at end of file
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeeksCallable.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeeksCallable.java
new file mode 100644
index 0000000..36f026e
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SeeksCallable.java
@@ -0,0 +1,73 @@
+package org.apache.hadoop.hbase.io.encoding.seek;
+
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CountDownLatch;
+import java.util.logging.Logger;
+
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreFileScanner;
+
+/**
+ * NOTICE: multi-threaded behavior not working since addition of
+ * PrefetchedHeader ThreadLocal variable in HFileBlock. Haven't figured out a
+ * work-around.
+ * 
+ * Class for testing how many seeks we can get on a machine. Does performance
+ * degrade before numThreads=numCores or after?
+ */
+class SeeksCallable implements Callable<Long> {
+  static Logger logger = Logger.getLogger(SeeksCallable.class.getSimpleName());
+
+  StoreFile storeFile;
+  boolean pread;
+  List<KeyValue> seeks;
+  CountDownLatch countDownLatch;
+
+  public SeeksCallable(StoreFile storeFile, boolean pread, List<KeyValue> seeks,
+      CountDownLatch countDownLatch) {
+    this.storeFile = storeFile;
+    this.pread = pread;
+    this.seeks = seeks;
+    this.countDownLatch = countDownLatch;
+  }
+
+  @Override
+  public Long call() throws Exception {
+    StoreFile.Reader threadReader = storeFile.createReader();
+    StoreFileScanner threadScanner = threadReader.getStoreFileScanner(true, pread);
+    KeyValue seek = null;
+    KeyValue foundKv = null;
+    int numCorrect = 0;
+    Long timeNs = null;
+    long startSeeksTime = System.nanoTime();
+    try {
+      for (int i = 0; i < seeks.size(); ++i) {
+        seek = seeks.get(i);
+        try {
+          threadScanner.seek(seek);
+          foundKv = threadScanner.peek();
+          if (!seek.equals(foundKv)) { throw new RuntimeException("orig!=ret"); }
+          ++numCorrect;
+        } catch (RuntimeException re) {
+          System.out.println(Thread.currentThread().getId() + " "
+              + SingleSeekBenchmark.getKeyValueMismatchMessage(i, seek, foundKv));
+        }
+      }
+    } catch (Exception e) {
+      logger.warning("numCorrect:" + numCorrect + ", errored key=" + seek);
+      e.printStackTrace();
+      throw new RuntimeException(e);
+    } finally {
+      countDownLatch.countDown();
+    }
+    if (numCorrect < seeks.size()) {
+      System.out.println(" numCorrect " + numCorrect + "/" + seeks.size());
+    }
+    timeNs = System.nanoTime() - startSeeksTime;
+    threadScanner.close();
+    return timeNs;
+  }
+
+}
\ No newline at end of file
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SingleSeekBenchmark.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SingleSeekBenchmark.java
new file mode 100644
index 0000000..18b34e6
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/encoding/seek/SingleSeekBenchmark.java
@@ -0,0 +1,349 @@
+package org.apache.hadoop.hbase.io.encoding.seek;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+
+import junit.framework.Assert;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
+import org.apache.hadoop.hbase.io.hfile.BlockCache;
+import org.apache.hadoop.hbase.io.hfile.CacheConfig;
+import org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder;
+import org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl;
+import org.apache.hadoop.hbase.regionserver.StoreFile;
+import org.apache.hadoop.hbase.regionserver.StoreFile.BloomType;
+import org.apache.hadoop.hbase.regionserver.StoreFileScanner;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.collections.CollectionUtils;
+
+import com.google.common.collect.Lists;
+
+public class SingleSeekBenchmark {
+  private static final Log LOG = LogFactory.getLog(SingleSeekBenchmark.class);
+
+	static final Long NANOSEC_IN_SEC = 1000 * 1000 * 1000L;
+	static final Long BYTES_IN_MEGABYTES = 1024 * 1024L;
+
+	static boolean RANDOM_RANDOM = true;
+	static long RANDOM_SEED = 1L;//if RANDOM_RANDOM==false
+
+	HBaseTestingUtility testingUtility = new HBaseTestingUtility();
+	Configuration configuration = testingUtility.getConfiguration();
+	CacheConfig cacheConfig = new CacheConfig(configuration);
+
+	ArrayList<KeyValue> allKeyValues = Lists.newArrayList();
+	ArrayList<KeyValue> seeks = Lists.newArrayList();
+	String rootDir;
+	String table;
+	String blockSize;
+	String blockCacheHFileName;
+	DataBlockEncoding encodingInMemory;
+	Integer numReaderThreads;
+	boolean pread;
+	Integer numberOfSeeks;
+	Integer maxSeekIndex = null;// set to null for full coverage
+	SeekBenchmarkResult result;
+
+	public SingleSeekBenchmark() {
+		configuration.setFloat(HConstants.HFILE_BLOCK_CACHE_SIZE_KEY, 0.5f);
+	}
+
+	public void runTestsOnFile(boolean record, List<SeekBenchmarkResult> results)
+			throws IOException {
+		assertBlockCacheIsEmpty();
+
+    String hfilePath = rootDir + "/" + table + "/" + blockSize + "/" + blockCacheHFileName;
+		Path path = new Path(hfilePath);
+		allKeyValues.clear();
+		loadAllKeyValues(path);
+		prepareListOfTestSeeks();
+//		DataBlockEncodingStaticVars.reset();
+
+    Pair<DataBlockEncoding, DataBlockEncoding> encoderPair
+        = new Pair<DataBlockEncoding, DataBlockEncoding>(DataBlockEncoding.NONE, encodingInMemory);
+    result = new SeekBenchmarkResult(true, false, true, true);
+    if (record) {
+      results.add(result);
+    }
+		result.table = table;
+		result.blockSize = blockSize;
+		result.numHardwareThreads = Runtime.getRuntime().availableProcessors();
+		result.numReaderThreads = numReaderThreads;
+		result.pread = pread;
+		result.encodingOnDisk = encoderPair.getFirst();
+		result.encodingInMemory = encoderPair.getSecond();
+    HFileDataBlockEncoder encoder = new HFileDataBlockEncoderImpl(encoderPair.getFirst(),
+        encoderPair.getSecond());
+		runTest(path, encoder, numReaderThreads, pread, seeks, false);
+		System.out.println(result.getHeader());
+		System.out.println(result);
+	}
+
+	void loadAllKeyValues(Path path) throws IOException {
+		// read all of the key values
+    HFileDataBlockEncoder encoder = new HFileDataBlockEncoderImpl(DataBlockEncoding.NONE,
+        DataBlockEncoding.NONE);
+    StoreFile storeFile = new StoreFile(testingUtility.getTestFileSystem(), path,
+        testingUtility.getConfiguration(), cacheConfig, BloomType.NONE, encoder);
+
+		StoreFile.Reader reader = storeFile.createReader();
+		StoreFileScanner scanner = reader.getStoreFileScanner(true, false);
+		KeyValue previous = null, current = null;
+
+		System.out.println("--start building random seek list--");
+		scanner.seek(KeyValue.LOWESTKEY);
+//		long lastTimestamp = 0L;
+//		int uniqueTimestamps = 0;
+		int numToPrint = 0;//set this to a positive number of you want to print the next n rows
+		while (true) {
+			previous = current;
+			current = scanner.next();
+			if (current == null) {
+				break;
+			}
+//			33366, \x00\x00\x07\xDC\x00\x00\x00\x02RealPage\x001002353\x00
+//			33366, \x00\x00\x07\xDC\x00\x00\x00\x02Real
+//			34838, \x00\x00\x07\xDC\x00\x00\x00\x02RealtySoft\x0020113711\x00
+      byte[] debugRow = Bytes.toBytesBinary("\\x00\\x00\\x07\\xDC\\x00\\x00\\x00\\x02RealEstateShows\\x0092898\\x00");
+      if(Bytes.equals(debugRow, current.getRow())){
+        numToPrint = 200;
+      }
+			if(numToPrint > 0){
+//        LOG.warn(Bytes.toStringBinary(current.getRow()));
+//			  LOG.warn(current);
+        --numToPrint;
+      }
+//			long timestamp = current.getTimestamp();
+//			if(timestamp != lastTimestamp){
+//				++uniqueTimestamps;
+//				System.out.println("new timestamp "+timestamp+", unique:"+uniqueTimestamps);
+//				lastTimestamp = timestamp;
+//			}
+			// if(FLATTEN_TIMESTAMPS){
+			// KeyValueTool.overwriteTimestamp(current, FLAT_TIMESTAMP);
+			// }
+			if (previous != null) {
+				int comp = KeyValue.COMPARATOR.compare(previous, current);
+				Assert.assertTrue(comp < 0);
+			}
+			allKeyValues.add(current);
+			if (maxSeekIndex != null && allKeyValues.size() >= maxSeekIndex) {
+				break;
+			}
+		}
+		System.out.println("loaded " + allKeyValues.size() + " KeyValues");
+		System.out.println("first:"+CollectionUtils.getFirst(allKeyValues));
+		System.out.println(" last:"+CollectionUtils.getLast(allKeyValues));
+		System.out.println("--finished building random seek list--");
+
+		storeFile.closeReader(true);
+		assertBlockCacheIsEmpty();
+	}
+
+	void prepareListOfTestSeeks() throws IOException {
+		seeks.clear();
+		Random random;
+		if(RANDOM_RANDOM){
+			random = new Random(); 
+		}else{
+			random = new Random(RANDOM_SEED);
+		}
+		if (maxSeekIndex == null) {
+			maxSeekIndex = allKeyValues.size();
+		}
+		List<Integer> seekIndexes = Lists.newArrayList();
+		for (int i = 0; i < numberOfSeeks; ++i) {
+			int r = random.nextInt(maxSeekIndex);
+			seekIndexes.add(r);
+		}
+		for (Integer seekIndex : seekIndexes) {
+			KeyValue keyValue = allKeyValues.get(seekIndex);
+			seeks.add(keyValue);
+		}
+	}
+
+	private void runTest(Path path, HFileDataBlockEncoder dataBlockEncoder,
+			Integer numReaderThreads, boolean pread, List<KeyValue> seeks, boolean clearCacheBeforeSeeks)
+			throws IOException{
+
+		// read all of the key values
+		StoreFile storeFile = new StoreFile(testingUtility.getTestFileSystem(),
+				path, testingUtility.getConfiguration(), cacheConfig,
+				BloomType.NONE, dataBlockEncoder);
+
+		encodeWhileReadingFromDiskToBlockCache(storeFile);
+		runSequentialScan(storeFile);
+		if(clearCacheBeforeSeeks){
+			clearBlockCache(false);
+		}
+		runRandomSeeks(storeFile);
+	}
+
+	protected void encodeWhileReadingFromDiskToBlockCache(StoreFile storeFile) throws IOException {
+    assertBlockCacheIsEmpty();
+
+    StoreFile.Reader reader = storeFile.createReader();
+    StoreFileScanner scanner = reader.getStoreFileScanner(true, pread);
+
+		System.gc();
+		System.out.println("--starting encoding --");
+		long startEncodingNs = System.nanoTime();
+		KeyValue current;
+		scanner.seek(KeyValue.LOWESTKEY);
+		//this call to scanner.next is creating garbage
+		while (null != (current = scanner.next())) {
+			++result.numCells;
+			result.numKeyBytes += current.getKeyLength();
+			result.numValueBytes += current.getValueLength();
+			// does this include memstoreTS?
+			result.numKeyValueBytes += current.getLength();
+		}
+		result.nsForEncoding = System.nanoTime() - startEncodingNs;
+		System.gc();
+
+//		result.numBlocks = DataBlockEncodingStaticVars.NUM_BLOCKS_ENCODED;
+//		result.numNonLeafRowNodes = DataBlockEncodingStaticVars.NUM_NON_LEAF_ROW_NODES;
+//    result.numLeafRowNodes = DataBlockEncodingStaticVars.NUM_LEAF_ROW_NODES;
+//    result.numNonLeafQualifierNodes = DataBlockEncodingStaticVars.NUM_NON_LEAF_QUALIFIER_NODES;
+//    result.numLeafQualifierNodes = DataBlockEncodingStaticVars.NUM_LEAF_QUALIFIER_NODES;
+//		result.numRawBytes = DataBlockEncodingStaticVars.NUM_UNENCODED_BYTES;
+//		result.numEncodedBytes = DataBlockEncodingStaticVars.NUM_ENCODED_BYTES;
+//
+//		result.nsAllocateBytes = DataBlockEncodingStaticVars.PT_WRITER_ALLOCATE_BYTES_NS;
+//
+//		result.nsAddTotal = DataBlockEncodingStaticVars.PT_WRITER_ADD_NS;
+//		result.nsAddRow = DataBlockEncodingStaticVars.PT_WRITER_ADD_ROW_NS;
+//		result.nsAddFamily = DataBlockEncodingStaticVars.PT_WRITER_ADD_FAMILY_NS;
+//		result.nsAddQualifier = DataBlockEncodingStaticVars.PT_WRITER_ADD_QUALIFIER_NS;
+//		result.nsAddTimestamp = DataBlockEncodingStaticVars.PT_WRITER_ADD_TIMESTAMP_NS;
+//		result.nsAddMemstoreTimestamp = DataBlockEncodingStaticVars.PT_WRITER_ADD_MEMSTORE_TIMESTAMP_NS;
+//		result.nsAddType = DataBlockEncodingStaticVars.PT_WRITER_ADD_TYPE_NS;
+//		result.nsAddValue = DataBlockEncodingStaticVars.PT_WRITER_ADD_VALUE_NS;
+//
+//		result.nsCompileTimestamps = DataBlockEncodingStaticVars.PT_COMPILE_TIMESTAMPS_NS;
+//		result.nsCompileQualifiers = DataBlockEncodingStaticVars.PT_COMPILE_QUALIFIERS_NS;
+//		result.nsCompileRows = DataBlockEncodingStaticVars.PT_COMPILE_ROWS_NS;
+//
+//		result.nsWriteRows = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_ROWS_NS;
+//		result.nsWriteFamilies = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_FAMILIES_NS;
+//		result.nsWriteQualifiers = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_QUALIFIERS_NS;
+//		result.nsWriteTimestamps = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_TIMESTAMPS_NS;
+//		result.nsWriteMemstoreTimestamps = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_MEMSTORE_TIMESTAMPS_NS;
+//		result.nsWriteValues = DataBlockEncodingStaticVars.PT_WRITER_FLUSH_VALUES_NS;
+//
+//		DataBlockEncodingStaticVars.reset();
+		System.out.println("--finished block cache load--");
+		scanner.close();
+		assertBlockCacheNotEmpty();
+	}
+
+	protected void runSequentialScan(StoreFile storeFile) throws IOException{
+    StoreFile.Reader reader = storeFile.createReader();
+    StoreFileScanner scanner = reader.getStoreFileScanner(true, pread);
+
+		System.gc();
+		System.out.println("--starting scan--");
+		int numScanCells = 0;
+		long startScanningNs = System.nanoTime();
+		scanner.seek(KeyValue.LOWESTKEY);
+		while (scanner.next() != null) {// cuts a new byte[] with cell contents
+			// while(scanner.nextCell()){//just positions the Cell. 2x faster,
+			// not counting GC
+			// sequential scan
+			++numScanCells;
+		}
+		result.nsForScanning = System.nanoTime() - startScanningNs;
+		System.out.println("--finished scan--");
+		// Assert.assertEquals(result.numCells, numScanCells);
+		assertBlockCacheNotEmpty();
+		scanner.close();
+	}
+
+	protected void runRandomSeeks(StoreFile storeFile) throws IOException{
+    ExecutorService executorService = Executors.newFixedThreadPool(numReaderThreads);
+		List<List<KeyValue>> perThreadSeeks = Lists.newArrayList();
+		int seeksPerThread = seeks.size() / numReaderThreads;
+		for (int i = 0; i < numReaderThreads; ++i) {
+			int startIndex = i * seeksPerThread;
+			int endIndex = Math.min(startIndex + seeksPerThread, seeks.size());
+			perThreadSeeks.add(seeks.subList(startIndex, endIndex));
+		}
+		List<Future<Long>> futures = Lists.newArrayList();
+		CountDownLatch countDownLatch = new CountDownLatch(numReaderThreads);
+		System.gc();
+		System.out.println("--starting random reads--");
+		long startSeeksTime = System.nanoTime();
+		try {
+			if(numReaderThreads > 1){
+				for (int i = 0; i < numReaderThreads; ++i) {
+          futures.add(executorService.submit(new SeeksCallable(storeFile, pread, perThreadSeeks
+              .get(i), countDownLatch)));
+				}
+				countDownLatch.await();
+				executorService.shutdown();
+				executorService.awaitTermination(1, TimeUnit.SECONDS);
+			}else{
+        new SeeksCallable(storeFile, pread, perThreadSeeks.get(0), countDownLatch).call();
+			}
+		} catch (Exception e) {
+			throw new RuntimeException(e);
+		}
+		result.nsForSeeking = System.nanoTime() - startSeeksTime;
+		result.numSeeks = seeks.size();
+		System.gc();
+		System.out.println("--finished random reads--");
+		assertBlockCacheNotEmpty();
+
+		storeFile.closeReader(true);
+		assertBlockCacheIsEmpty();
+	}
+
+
+	/********************** helpers ********************************/
+
+	protected static String getKeyValueMismatchMessage(int i,
+			KeyValue expectedKv, KeyValue wrongKv) {
+		String expectedString = expectedKv == null ? "null" : expectedKv.toString();
+		String wrongString = wrongKv == null ? "null" : wrongKv.toString();
+		return String.format(i + ":KeyValue doesn't match:\n"
+				+ "Orig key: %s\n" + "Ret key:  %s", expectedString,
+				wrongString);
+		// String right = Bytes.toStringBinary(expectedKv.getRow()) +"   "
+		// + Bytes.toStringBinary(expectedKv.getQualifier());
+		// String wrong = Bytes.toStringBinary(wrongKv.getRow()) +"   "
+		// + Bytes.toStringBinary(wrongKv.getQualifier());
+		// return right+"\n"+wrong;
+	}
+
+	private void clearBlockCache(boolean assertNotEmpty) {
+		BlockCache blockCache = cacheConfig.getBlockCache();
+		int numEvicted = blockCache.evictBlocksByHfileName(blockCacheHFileName);
+		if (assertNotEmpty) {
+			Assert.assertTrue(numEvicted > 0);
+		}
+		assertBlockCacheIsEmpty();
+	}
+
+	private void assertBlockCacheNotEmpty() {
+		Assert.assertTrue(cacheConfig.getBlockCache().getBlockCount() > 0);
+	}
+
+	private void assertBlockCacheIsEmpty() {
+		Assert.assertTrue(cacheConfig.getBlockCache().getBlockCount() == 0);
+	}
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
index c73c491..a6661dd 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/CacheTestUtils.java
@@ -325,7 +325,7 @@ public class CacheTestUtils {
           prevBlockOffset, cachedBuffer, HFileBlock.DONT_FILL_HEADER,
           blockSize, includesMemstoreTS, HFileBlock.MINOR_VERSION_NO_CHECKSUM,
           0, ChecksumType.NULL.getCode(),
-          onDiskSizeWithoutHeader + HFileBlock.HEADER_SIZE);
+          onDiskSizeWithoutHeader + HFileBlockConstants.HEADER_SIZE);
 
       String strKey;
       /* No conflicting keys */
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestChecksum.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestChecksum.java
index ca90b11..0a34633 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestChecksum.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestChecksum.java
@@ -206,7 +206,7 @@ public class TestChecksum {
         os.close();
 
         long expectedChunks = ChecksumUtil.numChunks(
-                               dataSize + HFileBlock.HEADER_SIZE,
+                               dataSize + HFileBlockConstants.HEADER_SIZE,
                                bytesPerChecksum);
         LOG.info("testChecksumChunks: pread=" + pread +
                    ", bytesPerChecksum=" + bytesPerChecksum +
@@ -228,7 +228,7 @@ public class TestChecksum {
         assertEquals(dataSize, b.getUncompressedSizeWithoutHeader());
 
         // verify that we have the expected number of checksum chunks
-        assertEquals(totalSize, HFileBlock.HEADER_SIZE + dataSize + 
+        assertEquals(totalSize, HFileBlockConstants.HEADER_SIZE + dataSize + 
                      expectedChunks * HFileBlock.CHECKSUM_SIZE);
 
         // assert that we did not encounter hbase checksum verification failures
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
index 71d79d8..93b65b6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlock.java
@@ -213,7 +213,7 @@ public class TestHFileBlock {
       int correctLength) throws IOException {
     HFileBlock.Writer hbw = createTestV2Block(algo, includesMemstoreTS);
     byte[] testV2Block = hbw.getHeaderAndDataForTest();
-    int osOffset = HFileBlock.HEADER_SIZE + 9;
+    int osOffset = HFileBlockConstants.HEADER_SIZE + 9;
     if (testV2Block.length == correctLength) {
       // Force-set the "OS" field of the gzip header to 3 (Unix) to avoid
       // variations across operating systems.
@@ -328,13 +328,13 @@ public class TestHFileBlock {
         if (algo == GZ) {
           is = fs.open(path);
           hbr = new HFileBlock.FSReaderV2(is, algo, totalSize);
-          b = hbr.readBlockData(0, 2173 + HFileBlock.HEADER_SIZE +
+          b = hbr.readBlockData(0, 2173 + HFileBlockConstants.HEADER_SIZE +
                                 b.totalChecksumBytes(), -1, pread);
           assertEquals(blockStr, b.toString());
           int wrongCompressedSize = 2172;
           try {
             b = hbr.readBlockData(0, wrongCompressedSize
-                + HFileBlock.HEADER_SIZE, -1, pread);
+                + HFileBlockConstants.HEADER_SIZE, -1, pread);
             fail("Exception expected");
           } catch (IOException ex) {
             String expectedPrefix = "On-disk size without header provided is "
@@ -374,7 +374,7 @@ public class TestHFileBlock {
           for (int blockId = 0; blockId < numBlocks; ++blockId) {
             DataOutputStream dos = hbw.startWriting(BlockType.DATA);
             writeEncodedBlock(algo, encoding, dos, encodedSizes, encodedBlocks,
-                blockId, includesMemstoreTS, HFileBlock.DUMMY_HEADER);
+                blockId, includesMemstoreTS, HFileBlockConstants.DUMMY_HEADER);
             hbw.writeHeaderAndData(os);
             totalSize += hbw.getOnDiskSizeWithHeader();
           }
@@ -523,7 +523,7 @@ public class TestHFileBlock {
           for (int i = 0; i < NUM_TEST_BLOCKS; ++i) {
             if (!pread) {
               assertEquals(is.getPos(), curOffset + (i == 0 ? 0 :
-                  HFileBlock.HEADER_SIZE));
+                  HFileBlockConstants.HEADER_SIZE));
             }
 
             assertEquals(expectedOffsets.get(i).longValue(), curOffset);
@@ -781,7 +781,7 @@ public class TestHFileBlock {
     }
 
     for (int size : new int[] { 100, 256, 12345 }) {
-      byte[] byteArr = new byte[HFileBlock.HEADER_SIZE + size];
+      byte[] byteArr = new byte[HFileBlockConstants.HEADER_SIZE + size];
       ByteBuffer buf = ByteBuffer.wrap(byteArr, 0, size);
       HFileBlock block = new HFileBlock(BlockType.DATA, size, size, -1, buf,
           HFileBlock.FILL_HEADER, -1, includesMemstoreTS, 
@@ -789,7 +789,7 @@ public class TestHFileBlock {
           0);
       long byteBufferExpectedSize =
           ClassSize.align(ClassSize.estimateBase(buf.getClass(), true)
-              + HFileBlock.HEADER_SIZE + size);
+              + HFileBlockConstants.HEADER_SIZE + size);
       long hfileBlockExpectedSize =
           ClassSize.align(ClassSize.estimateBase(HFileBlock.class, true));
       long expected = hfileBlockExpectedSize + byteBufferExpectedSize;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
index 3314d35..7b7589a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileBlockCompatibility.java
@@ -128,7 +128,7 @@ public class TestHFileBlockCompatibility {
       int correctLength) throws IOException {
     Writer hbw = createTestV2Block(algo);
     byte[] testV2Block = hbw.getHeaderAndData();
-    int osOffset = HFileBlock.HEADER_SIZE_NO_CHECKSUM + 9;
+    int osOffset = HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM + 9;
     if (testV2Block.length == correctLength) {
       // Force-set the "OS" field of the gzip header to 3 (Unix) to avoid
       // variations across operating systems.
@@ -239,13 +239,13 @@ public class TestHFileBlockCompatibility {
           is = fs.open(path);
           hbr = new HFileBlock.FSReaderV2(is, is, algo, totalSize, MINOR_VERSION,
                                           fs, path);
-          b = hbr.readBlockData(0, 2173 + HFileBlock.HEADER_SIZE_NO_CHECKSUM +
+          b = hbr.readBlockData(0, 2173 + HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM +
                                 b.totalChecksumBytes(), -1, pread);
           assertEquals(blockStr, b.toString());
           int wrongCompressedSize = 2172;
           try {
             b = hbr.readBlockData(0, wrongCompressedSize
-                + HFileBlock.HEADER_SIZE_NO_CHECKSUM, -1, pread);
+                + HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM, -1, pread);
             fail("Exception expected");
           } catch (IOException ex) {
             String expectedPrefix = "On-disk size without header provided is "
@@ -348,7 +348,7 @@ public class TestHFileBlockCompatibility {
   public static final class Writer {
 
     // These constants are as they were in minorVersion 0.
-    private static final int HEADER_SIZE = HFileBlock.HEADER_SIZE_NO_CHECKSUM;
+    private static final int HEADER_SIZE = HFileBlockConstants.HEADER_SIZE_NO_CHECKSUM;
     private static final boolean DONT_FILL_HEADER = HFileBlock.DONT_FILL_HEADER;
     private static final byte[] DUMMY_HEADER = 
       HFileBlock.DUMMY_HEADER_NO_CHECKSUM;
@@ -405,7 +405,7 @@ public class TestHFileBlockCompatibility {
     /**
      * Valid in the READY state. Contains the header and the uncompressed (but
      * potentially encoded, if this is a data block) bytes, so the length is
-     * {@link #uncompressedSizeWithoutHeader} + {@link HFileBlock#HEADER_SIZE}.
+     * {@link #uncompressedSizeWithoutHeader} + {@link HFileBlockConstants#HEADER_SIZE}.
      */
     private byte[] uncompressedBytesWithHeader;
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
index ff97fe6..8480b47 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/io/hfile/TestHFileDataBlockEncoder.java
@@ -32,10 +32,10 @@ import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoding;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultEncodingContext;
 import org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext;
-import org.apache.hadoop.hbase.io.encoding.RedundantKVGenerator;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured;
 import org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics;
 import org.apache.hadoop.hbase.util.ChecksumType;
+import org.apache.hadoop.hbase.util.test.RedundantKVGenerator;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -123,7 +123,7 @@ public class TestHFileDataBlockEncoder {
             includesMemstoreTS, context, block.getBlockType());
 
     byte[] encodedBytes = context.getUncompressedBytesWithHeader();
-    int size = encodedBytes.length - HFileBlock.HEADER_SIZE;
+    int size = encodedBytes.length - HFileBlockConstants.HEADER_SIZE;
     HFileBlock blockOnDisk =
         new HFileBlock(context.getBlockType(), size, size, -1,
             ByteBuffer.wrap(encodedBytes), HFileBlock.FILL_HEADER, 0,
@@ -154,8 +154,8 @@ public class TestHFileDataBlockEncoder {
     ByteBuffer keyValues = RedundantKVGenerator.convertKvToByteBuffer(
         generator.generateTestKeyValues(60), includesMemstoreTS);
     int size = keyValues.limit();
-    ByteBuffer buf = ByteBuffer.allocate(size + HFileBlock.HEADER_SIZE);
-    buf.position(HFileBlock.HEADER_SIZE);
+    ByteBuffer buf = ByteBuffer.allocate(size + HFileBlockConstants.HEADER_SIZE);
+    buf.position(HFileBlockConstants.HEADER_SIZE);
     keyValues.rewind();
     buf.put(keyValues);
     HFileBlock b = new HFileBlock(BlockType.DATA, size, size, -1, buf,
diff --git a/pom.xml b/pom.xml
index cd7c940..fd7c567 100644
--- a/pom.xml
+++ b/pom.xml
@@ -55,6 +55,7 @@
     <module>hbase-hadoop-compat</module>
     <module>hbase-common</module>
     <module>hbase-it</module>
+    <module>hbase-prefix-tree</module>
   </modules>
   <scm>
     <connection>scm:svn:http://svn.apache.org/repos/asf/hbase/trunk</connection>
@@ -941,6 +942,11 @@
         <type>test-jar</type>
         <scope>test</scope>
       </dependency>
+      <dependency>
+        <groupId>org.apache.hbase</groupId>
+        <artifactId>hbase-prefix-tree</artifactId>
+        <version>${project.version}</version>
+      </dependency>
       <!-- General dependencies -->
       <dependency>
         <groupId>io.netty</groupId>
