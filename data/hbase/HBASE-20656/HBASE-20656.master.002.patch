From 4baa21f96b344e2853bfdf8e447e4a15e82b839f Mon Sep 17 00:00:00 2001
From: Balazs Meszaros <balazs.meszaros@cloudera.com>
Date: Wed, 23 May 2018 13:49:19 +0200
Subject: [PATCH] HBASE-20656 Validate pre-2.0 coprocessors against HBase 2.0+

---
 .../hbase/coprocessor/BulkLoadObserver.java   |    8 +-
 .../Branch1CoprocessorMethods.java            | 1137 +++++++++++++++++
 .../tool/coprocessor/CoprocessorMethod.java   |   54 +
 .../tool/coprocessor/CoprocessorMethods.java  |   66 +
 .../coprocessor/CoprocessorValidator.java     |  251 ++++
 .../coprocessor/CoprocessorViolation.java     |   55 +
 .../CurrentCoprocessorMethods.java            |   47 +
 .../coprocessor/CoprocessorValidatorTest.java |  125 ++
 .../src/test/resources/coprocessor-1.x.jar    |  Bin 0 -> 5152 bytes
 src/main/asciidoc/_chapters/ops_mgt.adoc      |   28 +
 10 files changed, 1765 insertions(+), 6 deletions(-)
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/Branch1CoprocessorMethods.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethods.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation.java
 create mode 100644 hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CurrentCoprocessorMethods.java
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidatorTest.java
 create mode 100644 hbase-server/src/test/resources/coprocessor-1.x.jar

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.java
index 25e6522018..b69a727037 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.java
@@ -21,13 +21,9 @@ package org.apache.hadoop.hbase.coprocessor;
 
 import java.io.IOException;
 
-import org.apache.hadoop.hbase.Coprocessor;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.yetus.audience.InterfaceAudience;
 import org.apache.yetus.audience.InterfaceStability;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.PrepareBulkLoadRequest;
-import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.CleanupBulkLoadRequest;
 
 /**
  * Coprocessors implement this interface to observe and mediate bulk load operations.
@@ -55,7 +51,7 @@ public interface BulkLoadObserver {
       * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
       * If you need to get the region or table name, get it from the
       * <code>ctx</code> as follows: <code>code>ctx.getEnvironment().getRegion()</code>. Use
-      * getRegionInfo to fetch the encodedName and use getTabldDescriptor() to get the tableName.
+      * getRegionInfo to fetch the encodedName and use getTableDescriptor() to get the tableName.
       * @param ctx the environment to interact with the framework and master
       */
     default void prePrepareBulkLoad(ObserverContext<RegionCoprocessorEnvironment> ctx)
@@ -66,7 +62,7 @@ public interface BulkLoadObserver {
       * It can't bypass the default action, e.g., ctx.bypass() won't have effect.
       * If you need to get the region or table name, get it from the
       * <code>ctx</code> as follows: <code>code>ctx.getEnvironment().getRegion()</code>. Use
-      * getRegionInfo to fetch the encodedName and use getTabldDescriptor() to get the tableName.
+      * getRegionInfo to fetch the encodedName and use getTableDescriptor() to get the tableName.
       * @param ctx the environment to interact with the framework and master
       */
     default void preCleanupBulkLoad(ObserverContext<RegionCoprocessorEnvironment> ctx)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/Branch1CoprocessorMethods.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/Branch1CoprocessorMethods.java
new file mode 100644
index 0000000000..0f5d829de6
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/Branch1CoprocessorMethods.java
@@ -0,0 +1,1137 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import org.apache.yetus.audience.InterfaceAudience;
+
+@InterfaceAudience.Private
+public class Branch1CoprocessorMethods extends CoprocessorMethods {
+  public Branch1CoprocessorMethods() {
+    addMethods();
+  }
+
+  /*
+   * This list of methods was generated from HBase 1.4.4.
+   */
+  private void addMethods() {
+    /* BulkLoadObserver */
+
+    addMethod("prePrepareBulkLoad",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.PrepareBulkLoadRequest");
+
+    addMethod("preCleanupBulkLoad",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.CleanupBulkLoadRequest");
+
+    /* EndpointObserver */
+
+    addMethod("postEndpointInvocation",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "com.google.protobuf.Service",
+        "java.lang.String",
+        "com.google.protobuf.Message",
+        "com.google.protobuf.Message.Builder");
+
+    addMethod("preEndpointInvocation",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "com.google.protobuf.Service",
+        "java.lang.String",
+        "com.google.protobuf.Message");
+
+    /* MasterObserver */
+
+    addMethod("preCreateTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HTableDescriptor",
+        "org.apache.hadoop.hbase.HRegionInfo[]");
+
+    addMethod("postCreateTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HTableDescriptor",
+        "org.apache.hadoop.hbase.HRegionInfo[]");
+
+    addMethod("preDeleteTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postDeleteTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preDeleteTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preMove",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.ServerName",
+        "org.apache.hadoop.hbase.ServerName");
+
+    addMethod("preCreateTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HTableDescriptor",
+        "org.apache.hadoop.hbase.HRegionInfo[]");
+
+    addMethod("postCreateTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HTableDescriptor",
+        "org.apache.hadoop.hbase.HRegionInfo[]");
+
+    addMethod("postMove",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.ServerName",
+        "org.apache.hadoop.hbase.ServerName");
+
+    addMethod("postDeleteTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preTruncateTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postTruncateTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preTruncateTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postTruncateTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preModifyTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("postModifyTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("preModifyTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("postModifyTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("preAddColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("postAddColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("preAddColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("postAddColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("preModifyColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("postModifyColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("preModifyColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("postModifyColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.HColumnDescriptor");
+
+    addMethod("preDeleteColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "byte[]");
+
+    addMethod("postDeleteColumn",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "byte[]");
+
+    addMethod("preDeleteColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "byte[]");
+
+    addMethod("postDeleteColumnHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "byte[]");
+
+    addMethod("preEnableTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postEnableTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preEnableTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postEnableTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preDisableTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postDisableTable",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preDisableTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postDisableTableHandler",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preAbortProcedure",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.procedure2.ProcedureExecutor",
+        "long");
+
+    addMethod("postAbortProcedure",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preListProcedures",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postListProcedures",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("preAssign",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("postAssign",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("preUnassign",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "boolean");
+
+    addMethod("postUnassign",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "boolean");
+
+    addMethod("preRegionOffline",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("postRegionOffline",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("preBalance",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postBalance",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("preSetSplitOrMergeEnabled",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean",
+        "org.apache.hadoop.hbase.client.Admin.MasterSwitchType");
+
+    addMethod("postSetSplitOrMergeEnabled",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean",
+        "org.apache.hadoop.hbase.client.Admin.MasterSwitchType");
+
+    addMethod("preBalanceSwitch",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean");
+
+    addMethod("postBalanceSwitch",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean",
+        "boolean");
+
+    addMethod("preShutdown",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preStopMaster",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postStartMaster",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preMasterInitialization",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("postSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("preListSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription");
+
+    addMethod("postListSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription");
+
+    addMethod("preCloneSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("postCloneSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("preRestoreSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("postRestoreSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription",
+        "org.apache.hadoop.hbase.HTableDescriptor");
+
+    addMethod("preDeleteSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription");
+
+    addMethod("postDeleteSnapshot",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.SnapshotDescription");
+
+    addMethod("preGetTableDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.util.List");
+
+    addMethod("preGetTableDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.util.List",
+        "java.lang.String");
+
+    addMethod("postGetTableDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.util.List",
+        "java.lang.String");
+
+    addMethod("postGetTableDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("preGetTableNames",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.lang.String");
+
+    addMethod("postGetTableNames",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.lang.String");
+
+    addMethod("preCreateNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.NamespaceDescriptor");
+
+    addMethod("postCreateNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.NamespaceDescriptor");
+
+    addMethod("preDeleteNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("postDeleteNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("preModifyNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.NamespaceDescriptor");
+
+    addMethod("postModifyNamespace",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.NamespaceDescriptor");
+
+    addMethod("preGetNamespaceDescriptor",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("postGetNamespaceDescriptor",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.NamespaceDescriptor");
+
+    addMethod("preListNamespaceDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("postListNamespaceDescriptors",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("preTableFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("postTableFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName");
+
+    addMethod("preSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("preSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("preSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("postSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("postSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("postSetUserQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("preSetTableQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("postSetTableQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.TableName",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("preSetNamespaceQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("postSetNamespaceQuota",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.Quotas");
+
+    addMethod("preDispatchMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("postDispatchMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.HRegionInfo");
+
+    addMethod("preGetClusterStatus",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postGetClusterStatus",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.ClusterStatus");
+
+    addMethod("preClearDeadServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postClearDeadServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "java.util.List");
+
+    addMethod("preMoveServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("postMoveServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("preMoveTables",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("postMoveTables",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("preMoveServersAndTables",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("postMoveServersAndTables",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set",
+        "java.util.Set",
+        "java.lang.String");
+
+    addMethod("preAddRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("postAddRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("preRemoveRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("postRemoveRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("preRemoveServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set");
+
+    addMethod("postRemoveServers",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.Set");
+
+    addMethod("preBalanceRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String");
+
+    addMethod("postBalanceRSGroup",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.lang.String",
+        "boolean");
+
+    /* RegionObserver */
+
+    addMethod("preOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postLogReplay",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preFlushScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.KeyValueScanner",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "long");
+
+    addMethod("preFlushScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.KeyValueScanner",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner");
+
+    addMethod("preFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner");
+
+    addMethod("preFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.StoreFile");
+
+    addMethod("postFlush",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preCompactSelection",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "java.util.List");
+
+    addMethod("preCompactSelection",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "java.util.List",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest");
+
+    addMethod("postCompactSelection",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "com.google.common.collect.ImmutableList");
+
+    addMethod("postCompactSelection",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "com.google.common.collect.ImmutableList",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest");
+
+    addMethod("preCompact",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "org.apache.hadoop.hbase.regionserver.ScanType");
+
+    addMethod("preCompact",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "org.apache.hadoop.hbase.regionserver.ScanType",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest");
+
+    addMethod("preClose",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean");
+
+    addMethod("preCompactScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "java.util.List",
+        "org.apache.hadoop.hbase.regionserver.ScanType",
+        "long",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner");
+
+    addMethod("preCompactScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "java.util.List",
+        "org.apache.hadoop.hbase.regionserver.ScanType",
+        "long",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest",
+        "long");
+
+    addMethod("preCompactScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "java.util.List",
+        "org.apache.hadoop.hbase.regionserver.ScanType",
+        "long",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest");
+
+    addMethod("postCompact",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.StoreFile");
+
+    addMethod("postCompact",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.regionserver.StoreFile",
+        "org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest");
+
+    addMethod("preSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]");
+
+    addMethod("preSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("preSplitBeforePONR",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "java.util.List");
+
+    addMethod("preSplitAfterPONR",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("preRollBackSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postRollBackSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postCompleteSplit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postClose",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "boolean");
+
+    addMethod("preGetClosestRowBefore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.client.Result");
+
+    addMethod("postGetClosestRowBefore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.client.Result");
+
+    addMethod("preGetOp",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Get",
+        "java.util.List");
+
+    addMethod("postGetOp",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Get",
+        "java.util.List");
+
+    addMethod("preExists",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Get",
+        "boolean");
+
+    addMethod("postExists",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Get",
+        "boolean");
+
+    addMethod("prePut",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Put",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit",
+        "org.apache.hadoop.hbase.client.Durability");
+
+    addMethod("postPut",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Put",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit",
+        "org.apache.hadoop.hbase.client.Durability");
+
+    addMethod("preDelete",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Delete",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit",
+        "org.apache.hadoop.hbase.client.Durability");
+
+    addMethod("prePrepareTimeStampForDeleteVersion",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Mutation",
+        "org.apache.hadoop.hbase.Cell",
+        "byte[]",
+        "org.apache.hadoop.hbase.client.Get");
+
+    addMethod("postDelete",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Delete",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit",
+        "org.apache.hadoop.hbase.client.Durability");
+
+    addMethod("preBatchMutate",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress");
+
+    addMethod("postBatchMutate",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress");
+
+    addMethod("postStartRegionOperation",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region.Operation");
+
+    addMethod("postCloseRegionOperation",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region.Operation");
+
+    addMethod("postBatchMutateIndispensably",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress",
+        "boolean");
+
+    addMethod("preCheckAndPut",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Put",
+        "boolean");
+
+    addMethod("preCheckAndPutAfterRowLock",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Put",
+        "boolean");
+
+    addMethod("postCheckAndPut",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Put",
+        "boolean");
+
+    addMethod("preCheckAndDelete",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Delete",
+        "boolean");
+
+    addMethod("preCheckAndDeleteAfterRowLock",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Delete",
+        "boolean");
+
+    addMethod("postCheckAndDelete",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "org.apache.hadoop.hbase.filter.CompareFilter.CompareOp",
+        "org.apache.hadoop.hbase.filter.ByteArrayComparable",
+        "org.apache.hadoop.hbase.client.Delete",
+        "boolean");
+
+    addMethod("preIncrementColumnValue",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "long",
+        "boolean");
+
+    addMethod("postIncrementColumnValue",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "byte[]",
+        "byte[]",
+        "long",
+        "boolean",
+        "long");
+
+    addMethod("preAppend",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Append");
+
+    addMethod("preAppendAfterRowLock",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Append");
+
+    addMethod("postAppend",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Append",
+        "org.apache.hadoop.hbase.client.Result");
+
+    addMethod("preIncrement",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Increment");
+
+    addMethod("preIncrementAfterRowLock",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Increment");
+
+    addMethod("postIncrement",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Increment",
+        "org.apache.hadoop.hbase.client.Result");
+
+    addMethod("preScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Scan",
+        "org.apache.hadoop.hbase.regionserver.RegionScanner");
+
+    addMethod("preStoreScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Store",
+        "org.apache.hadoop.hbase.client.Scan",
+        "java.util.NavigableSet",
+        "org.apache.hadoop.hbase.regionserver.KeyValueScanner");
+
+    addMethod("postScannerOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.client.Scan",
+        "org.apache.hadoop.hbase.regionserver.RegionScanner");
+
+    addMethod("preScannerNext",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "java.util.List",
+        "int",
+        "boolean");
+
+    addMethod("postScannerNext",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "java.util.List",
+        "int",
+        "boolean");
+
+    addMethod("postScannerFilterRow",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner",
+        "byte[]",
+        "int",
+        "short",
+        "boolean");
+
+    addMethod("preScannerClose",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner");
+
+    addMethod("postScannerClose",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.InternalScanner");
+
+    addMethod("preWALRestore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.regionserver.wal.HLogKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("preWALRestore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.wal.WALKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("postWALRestore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.regionserver.wal.HLogKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("postWALRestore",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.wal.WALKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("preBulkLoadHFile",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List");
+
+    addMethod("preCommitStoreFile",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "java.util.List");
+
+    addMethod("postCommitStoreFile",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "byte[]",
+        "org.apache.hadoop.fs.Path",
+        "org.apache.hadoop.fs.Path");
+
+    addMethod("postBulkLoadHFile",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "boolean");
+
+    addMethod("preStoreFileReaderOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.fs.FileSystem",
+        "org.apache.hadoop.fs.Path",
+        "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper",
+        "long",
+        "org.apache.hadoop.hbase.io.hfile.CacheConfig",
+        "org.apache.hadoop.hbase.io.Reference",
+        "org.apache.hadoop.hbase.regionserver.StoreFile.Reader");
+
+    addMethod("postStoreFileReaderOpen",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.fs.FileSystem",
+        "org.apache.hadoop.fs.Path",
+        "org.apache.hadoop.hbase.io.FSDataInputStreamWrapper",
+        "long",
+        "org.apache.hadoop.hbase.io.hfile.CacheConfig",
+        "org.apache.hadoop.hbase.io.Reference",
+        "org.apache.hadoop.hbase.regionserver.StoreFile.Reader");
+
+    addMethod("postMutationBeforeWAL",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.coprocessor.RegionObserver.MutationType",
+        "org.apache.hadoop.hbase.client.Mutation",
+        "org.apache.hadoop.hbase.Cell",
+        "org.apache.hadoop.hbase.Cell");
+
+    addMethod("postInstantiateDeleteTracker",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.DeleteTracker");
+
+    /* RegionServerObserver */
+
+    addMethod("preMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("preStopRegionServer",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("preMergeCommit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "java.util.List");
+
+    addMethod("postMergeCommit",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("preRollBackMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("postRollBackMerge",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.regionserver.Region",
+        "org.apache.hadoop.hbase.regionserver.Region");
+
+    addMethod("preRollWALWriterRequest",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postRollWALWriterRequest",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext");
+
+    addMethod("postCreateReplicationEndPoint",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.replication.ReplicationEndpoint");
+
+    addMethod("preReplicateLogEntries",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "org.apache.hadoop.hbase.CellScanner");
+
+    addMethod("postReplicateLogEntries",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "java.util.List",
+        "org.apache.hadoop.hbase.CellScanner");
+
+    /* WALObserver */
+
+    addMethod("preWALWrite",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.wal.WALKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("preWALWrite",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.regionserver.wal.HLogKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("postWALWrite",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.regionserver.wal.HLogKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("postWALWrite",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.hbase.HRegionInfo",
+        "org.apache.hadoop.hbase.wal.WALKey",
+        "org.apache.hadoop.hbase.regionserver.wal.WALEdit");
+
+    addMethod("preWALRoll",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.fs.Path",
+        "org.apache.hadoop.fs.Path");
+
+    addMethod("postWALRoll",
+        "org.apache.hadoop.hbase.coprocessor.ObserverContext",
+        "org.apache.hadoop.fs.Path",
+        "org.apache.hadoop.fs.Path");
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java
new file mode 100644
index 0000000000..8bcba0423e
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.java
@@ -0,0 +1,54 @@
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Objects;
+
+import org.apache.yetus.audience.InterfaceAudience;
+
+@InterfaceAudience.Private
+public class CoprocessorMethod {
+  private final String name;
+  private final List<String> parameters;
+
+  public CoprocessorMethod(String name) {
+    this.name = name;
+
+    parameters = new ArrayList<>();
+  }
+
+  public CoprocessorMethod withParameters(String ... parameters) {
+    for (String parameter : parameters) {
+      this.parameters.add(parameter);
+    }
+
+    return this;
+  }
+
+  public CoprocessorMethod withParameters(Class<?> ... parameters) {
+    for (Class<?> parameter : parameters) {
+      this.parameters.add(parameter.getCanonicalName());
+    }
+
+    return this;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == this) {
+      return true;
+    } else if (!(obj instanceof CoprocessorMethod)) {
+      return false;
+    }
+
+    CoprocessorMethod other = (CoprocessorMethod)obj;
+
+    return Objects.equals(name, other.name) &&
+        Objects.equals(parameters, other.parameters);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(name, parameters);
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethods.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethods.java
new file mode 100644
index 0000000000..2e0c801b8a
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethods.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import java.lang.reflect.Method;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.yetus.audience.InterfaceAudience;
+
+@InterfaceAudience.Private
+public class CoprocessorMethods {
+  private final Set<CoprocessorMethod> methods;
+
+  public CoprocessorMethods() {
+    methods = new HashSet<>();
+  }
+
+  public void addMethod(String name, String ... parameters) {
+    CoprocessorMethod cpMethod = new CoprocessorMethod(name).withParameters(parameters);
+    methods.add(cpMethod);
+  }
+
+  public void addMethod(String name, Class<?> ... parameters) {
+    CoprocessorMethod cpMethod = new CoprocessorMethod(name).withParameters(parameters);
+    methods.add(cpMethod);
+  }
+
+  public void addMethod(Method method) {
+    CoprocessorMethod cpMethod = new CoprocessorMethod(method.getName())
+        .withParameters(method.getParameterTypes());
+    methods.add(cpMethod);
+  }
+
+  public boolean hasMethod(String name, String ... parameters) {
+    CoprocessorMethod method = new CoprocessorMethod(name).withParameters(parameters);
+    return methods.contains(method);
+  }
+
+  public boolean hasMethod(String name, Class<?> ... parameters) {
+    CoprocessorMethod method = new CoprocessorMethod(name).withParameters(parameters);
+    return methods.contains(method);
+  }
+
+  public boolean hasMethod(Method method) {
+    CoprocessorMethod cpMethod = new CoprocessorMethod(method.getName())
+        .withParameters(method.getParameterTypes());
+    return methods.contains(cpMethod);
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java
new file mode 100644
index 0000000000..b9dc515c3d
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.java
@@ -0,0 +1,251 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import java.io.IOException;
+import java.lang.reflect.Method;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.security.AccessController;
+import java.security.PrivilegedAction;
+import java.util.ArrayList;
+import java.util.Enumeration;
+import java.util.List;
+import java.util.jar.JarEntry;
+import java.util.jar.JarFile;
+
+import org.apache.hadoop.hbase.Coprocessor;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation.Severity;
+import org.apache.hadoop.hbase.util.AbstractHBaseTool;
+import org.apache.hadoop.util.ToolRunner;
+import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+import org.apache.hbase.thirdparty.org.apache.commons.cli.CommandLine;
+import org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException;
+import org.apache.yetus.audience.InterfaceAudience;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@InterfaceAudience.Private
+public class CoprocessorValidator extends AbstractHBaseTool {
+  private static final Logger LOG = LoggerFactory
+      .getLogger(CoprocessorValidator.class);
+
+  private CoprocessorMethods branch1;
+  private CoprocessorMethods current;
+
+  private boolean dieOnWarnings;
+  private boolean scan;
+  private List<String> args;
+
+  public CoprocessorValidator() {
+    branch1 = new Branch1CoprocessorMethods();
+    current = new CurrentCoprocessorMethods();
+  }
+
+  private static class ResolverUrlClassLoader extends URLClassLoader {
+    public ResolverUrlClassLoader(URL[] urls) {
+      super(urls, ResolverUrlClassLoader.class.getClassLoader());
+    }
+
+    @Override
+    public Class<?> loadClass(String name) throws ClassNotFoundException {
+      return loadClass(name, true);
+    }
+  }
+
+  private ResolverUrlClassLoader createClassLoader(URL[] urls) {
+    return AccessController.doPrivileged(new PrivilegedAction<ResolverUrlClassLoader>() {
+      @Override
+      public ResolverUrlClassLoader run() {
+        return new ResolverUrlClassLoader(urls);
+      }
+    });
+  }
+
+  private void validate(ClassLoader classLoader, String className,
+      List<CoprocessorViolation> violations) {
+    LOG.debug("Validating class '{}'.", className);
+
+    try {
+      Class<?> clazz = classLoader.loadClass(className);
+
+      for (Method method : clazz.getDeclaredMethods()) {
+        LOG.trace("Validating method '{}'.", method);
+
+        if (branch1.hasMethod(method) && !current.hasMethod(method)) {
+          CoprocessorViolation violation = new CoprocessorViolation(Severity.WARNING,
+              "Method '" + method + "' was removed from new coprocessor API, "
+                  + "so it won't be called by HBase.");
+          violations.add(violation);
+        }
+      }
+    } catch (ClassNotFoundException e) {
+      CoprocessorViolation violation = new CoprocessorViolation(Severity.ERROR,
+          "No such class '" + className + "'.", e);
+      violations.add(violation);
+    } catch (RuntimeException | Error e) {
+      CoprocessorViolation violation = new CoprocessorViolation(Severity.ERROR,
+          "Could not validate class '" + className + "'.", e);
+      violations.add(violation);
+    }
+  }
+
+  public List<CoprocessorViolation> validate(ClassLoader classLoader, List<String> classNames) {
+    List<CoprocessorViolation> violations = new ArrayList<>();
+
+    for (String className : classNames) {
+      validate(classLoader, className, violations);
+    }
+
+    return violations;
+  }
+
+  public List<CoprocessorViolation> validate(List<URL> urls, List<String> classNames)
+      throws IOException {
+    URL[] urlArray = new URL[urls.size()];
+    urls.toArray(urlArray);
+
+    try (ResolverUrlClassLoader classLoader = createClassLoader(urlArray)) {
+      return validate(classLoader, classNames);
+    }
+  }
+
+  @VisibleForTesting
+  protected List<String> getJarClasses(Path path) throws IOException {
+    List<String> classes = new ArrayList<>();
+
+    try (JarFile jarFile = new JarFile(path.toFile())) {
+      Enumeration<JarEntry> entries = jarFile.entries();
+
+      while (entries.hasMoreElements()) {
+        JarEntry entry = entries.nextElement();
+        String name = entry.getName();
+
+        if (name.endsWith(".class")) {
+          name = name.substring(0, name.length() - 6).replace('/', '.');
+          classes.add(name);
+        }
+      }
+    }
+
+    return classes;
+  }
+
+  @VisibleForTesting
+  protected List<String> filterObservers(ClassLoader classLoader, List<String> classNames)
+      throws ClassNotFoundException {
+    List<String> filteredClassNames = new ArrayList<>();
+
+    for (String className : classNames) {
+      LOG.debug("Scanning class '{}'.", className);
+
+      Class<?> clazz = classLoader.loadClass(className);
+
+      if (Coprocessor.class.isAssignableFrom(clazz)) {
+        LOG.debug("Found coprocessor class '{}'.", className);
+        filteredClassNames.add(className);
+      }
+    }
+
+    return filteredClassNames;
+  }
+
+  @Override
+  protected void printUsage() {
+    printUsage("hbase " + getClass().getName() + " <jar> -scan|<classes>", "Options:", "");
+  }
+
+  @Override
+  protected void addOptions() {
+    addOptNoArg("e", "Threat warnings as errors.");
+    addOptNoArg("scan", "Scan jar for observers.");
+  }
+
+  @Override
+  protected void processOptions(CommandLine cmd) {
+    scan = cmd.hasOption("scan");
+    dieOnWarnings = cmd.hasOption("e");
+    args = cmd.getArgList();
+  }
+
+  @Override
+  protected int doWork() throws Exception {
+    if (args.size() < 1) {
+      System.err.println("Missing jar file.");
+      printUsage();
+      return 1;
+    }
+
+    String jar = args.get(0);
+
+    if (args.size() == 1 && !scan) {
+      throw new ParseException("Missing classes or -scan option.");
+    } else if (args.size() > 1 && scan) {
+      throw new ParseException("Can't use classes with -scan option.");
+    }
+
+    Path jarPath = Paths.get(jar);
+    URL[] urls = new URL[] { jarPath.toUri().toURL() };
+
+    List<CoprocessorViolation> violations;
+
+    try (ResolverUrlClassLoader classLoader = createClassLoader(urls)) {
+      List<String> classNames;
+
+      if (scan) {
+        List<String> jarClassNames = getJarClasses(jarPath);
+        classNames = filterObservers(classLoader, jarClassNames);
+      } else {
+        classNames = args.subList(1, args.size());
+      }
+
+      violations = validate(classLoader, classNames);
+    }
+
+    boolean error = false;
+
+    for (CoprocessorViolation violation : violations) {
+      switch (violation.getSeverity()) {
+        case WARNING:
+          System.err.println("[WARNING] " + violation.getMessage());
+
+          if (dieOnWarnings) {
+            error = true;
+          }
+
+          break;
+        case ERROR:
+          System.err.println("[ERROR] " + violation.getMessage());
+          error = true;
+
+          break;
+      }
+    }
+
+    return (error) ? 1 : 0;
+  }
+
+  public static void main(String[] args) throws Exception {
+    System.exit(ToolRunner.run(HBaseConfiguration.create(), new CoprocessorValidator(), args));
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation.java
new file mode 100644
index 0000000000..78e9920c4e
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation.java
@@ -0,0 +1,55 @@
+/**
+ *
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import org.apache.hbase.thirdparty.com.google.common.base.Throwables;
+import org.apache.yetus.audience.InterfaceAudience;
+
+@InterfaceAudience.Private
+public class CoprocessorViolation {
+  public enum Severity {
+    WARNING, ERROR
+  }
+
+  private final Severity severity;
+  private final String message;
+
+  public CoprocessorViolation(Severity severity, String message) {
+    this(severity, message, null);
+  }
+
+  public CoprocessorViolation(Severity severity, String message, Throwable t) {
+    this.severity = severity;
+
+    if (t == null) {
+      this.message = message;
+    } else {
+      this.message = message + "\n" + Throwables.getStackTraceAsString(t);
+    }
+  }
+
+  public Severity getSeverity() {
+    return severity;
+  }
+
+  public String getMessage() {
+    return message;
+  }
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CurrentCoprocessorMethods.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CurrentCoprocessorMethods.java
new file mode 100644
index 0000000000..265cf5158e
--- /dev/null
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/tool/coprocessor/CurrentCoprocessorMethods.java
@@ -0,0 +1,47 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import java.lang.reflect.Method;
+
+import org.apache.hadoop.hbase.coprocessor.BulkLoadObserver;
+import org.apache.hadoop.hbase.coprocessor.EndpointObserver;
+import org.apache.hadoop.hbase.coprocessor.MasterObserver;
+import org.apache.hadoop.hbase.coprocessor.RegionObserver;
+import org.apache.hadoop.hbase.coprocessor.RegionServerObserver;
+import org.apache.hadoop.hbase.coprocessor.WALObserver;
+import org.apache.yetus.audience.InterfaceAudience;
+
+@InterfaceAudience.Private
+public class CurrentCoprocessorMethods extends CoprocessorMethods {
+  public CurrentCoprocessorMethods() {
+    addMethods(BulkLoadObserver.class);
+    addMethods(EndpointObserver.class);
+    addMethods(MasterObserver.class);
+    addMethods(RegionObserver.class);
+    addMethods(RegionServerObserver.class);
+    addMethods(WALObserver.class);
+  }
+
+  private void addMethods(Class<?> clazz) {
+    for (Method method : clazz.getDeclaredMethods()) {
+      addMethod(method);
+    }
+  }
+}
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidatorTest.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidatorTest.java
new file mode 100644
index 0000000000..66c0d3becd
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidatorTest.java
@@ -0,0 +1,125 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hbase.tool.coprocessor;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.net.URL;
+import java.net.URLClassLoader;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.List;
+
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation.Severity;
+import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({ SmallTests.class })
+public class CoprocessorValidatorTest {
+  private static final String PACKAGE = "org.apache.hadoop.hbase.coprocessor";
+
+  private CoprocessorValidator validator;
+
+  public CoprocessorValidatorTest() {
+    validator = new CoprocessorValidator();
+  }
+
+  private URLClassLoader createClassLoader(URL url) {
+    URL[] urls = new URL[] { url };
+    ClassLoader parentClassLoader = getClass().getClassLoader();
+
+    return new URLClassLoader(urls, parentClassLoader);
+  }
+
+  @Test
+  public void testScan() throws Exception {
+    Path path = Paths.get("src/test/resources/coprocessor-1.x.jar");
+
+    List<String> classNames = validator.getJarClasses(path);
+    List<String> filteredClassNames;
+
+    try (URLClassLoader urlClassLoader = createClassLoader(path.toUri().toURL())) {
+      filteredClassNames = validator.filterObservers(urlClassLoader, classNames);
+    }
+
+    assertEquals(1, filteredClassNames.size());
+    String scanMe = filteredClassNames.get(0);
+    assertEquals(PACKAGE + ".ScanMe", scanMe);
+  }
+
+  private List<CoprocessorViolation> validate(String className) throws IOException {
+    ClassLoader classLoader = getClass().getClassLoader();
+    URL jar = classLoader.getResource("coprocessor-1.x.jar");
+    List<String> classNames = Lists.newArrayList(PACKAGE + "." + className);
+
+    try (URLClassLoader urlClassLoader = createClassLoader(jar)) {
+      return validator.validate(urlClassLoader, classNames);
+    }
+  }
+
+  /**
+   * In this test case, we are try to load a not-existent class.
+   */
+  @Test
+  public void testNoSuchClass() throws IOException {
+    List<CoprocessorViolation> violations = validate("NoSuchClass");
+    assertEquals(1, violations.size());
+
+    CoprocessorViolation violation = violations.get(0);
+    assertEquals(Severity.ERROR, violation.getSeverity());
+    assertTrue(violation.getMessage().contains(
+        "java.lang.ClassNotFoundException: org.apache.hadoop.hbase.coprocessor.NoSuchClass"));
+  }
+
+  /**
+   * In this test case, we are validating MissingClass coprocessor, which
+   * references org.apache.hadoop.hbase.regionserver.wal.WALEdit. Since
+   * this class has been moved to org.apache.hadoop.hbase.wal, class loading
+   * must fail.
+   */
+  @Test
+  public void testMissingClass() throws IOException {
+    List<CoprocessorViolation> violations = validate("MissingClass");
+    assertEquals(1, violations.size());
+
+    CoprocessorViolation violation = violations.get(0);
+    assertEquals(Severity.ERROR, violation.getSeverity());
+    assertTrue(violation.getMessage().contains(
+        "java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/regionserver/wal/WALEdit"));
+  }
+
+  /**
+   * ObsolateMethod coprocessor implements preCreateTable method which has
+   * HRegionInfo parameters. In our current implementation, we pass only
+   * RegionInfo parameters, so this method won't be called by HBase at all.
+   */
+  @Test
+  public void testObsolateMethod() throws IOException {
+    List<CoprocessorViolation> violations = validate("ObsolateMethod");
+    assertEquals(1, violations.size());
+
+    CoprocessorViolation violation = violations.get(0);
+    assertEquals(Severity.WARNING, violation.getSeverity());
+    assertTrue(violation.getMessage().contains("was removed from new coprocessor API"));
+  }
+}
diff --git a/hbase-server/src/test/resources/coprocessor-1.x.jar b/hbase-server/src/test/resources/coprocessor-1.x.jar
new file mode 100644
index 0000000000000000000000000000000000000000..eda5afd54b9894e0fa39589f6d6c185a328b3c37
GIT binary patch
literal 5152
zcmbVQ2{@Gd7an6x1|e$_Q8A1yB3TDv?8d&1rHpN?T_Ia`*$rjMHkRz!BC?e%p>WBb
zEqjEKQ2a-my4Sr=_y3z`zHjE4bI$u)-Z|%ck2(yWfD~{rG@WXczkK+1IflEd$iby~
z6jkMU)xVjM05ouB0fSOwf8c(Oz`bdHHdB#SRg{<0f<si~H&i-bD8qOlJs=nlS9j;j
za!tNJ1t;D*b1QYXal?3INQh@JI`OpNMt1j_6eU3yCD&B0BAQq#7pj7=O=qp}0;=%!
z@S8O$;hNMiO|8b*NW#!|h6kkep;ixWl0Nsy@pw*I#QE5SbLh+nH<oM)1d4F4ajyY<
zpC|ypX#eYa2yhccIbZ*=l)qaX|7L-3L|9l`{fi;huZGqLOBBlSU+n3Rw6``#xcrP#
z?(pk`+5g4M0_Et8vaoV-K{@}x{dzX3k9C^c@c@8q+<HZRGlH7A3(5}RYNcZ3YK^jl
zSlA(4Tq@xzC~AJnTPrz3!tqrVUbLi(gy>Ux54duRJdVl1l^)1y@QRCeN4uei!6nq$
z`@RiNkA>n(0@ji#EiTDo)j4HLdm~#>pRSEZzWcnlv2+Pg>pmdm-^zI(Gf0!Sip=_4
zFIDz=3Z6btW=y4rw1ut`(J?!-SmwTLGZ^6-l0>TpZ_^iU%jg1Y?Uy|6ac6T~09(Eu
z764x^%Tylf)tA$@GQKhTz|Pd;GMnk31EkdGvW=*@b;?TLhl<cPP<v(J+Qs$l2+Sk?
zRonddzU)U3$=aI>o{LCZ@~*4N!4m8pQ3NJknBd*sw@JXR1@titglpo4M4D<&<L4z$
zXZ<~gnYY}Jw{qB++dGIm2t@gy?+R<qOmEzx?H8fv<hu1So<@z3R?YRLf}~^NQmZo4
zC$)gjQP5tIhxobk5#-{g4c_&p_g`cf$$&P1Vm?0N1UgXvLSB_9=p5!sVn}q=(BvAX
zeX#k|o4NO$(g7ZJOIKdF(_HDk)F9;*IT>#+?$4xWR_j^`2)BNJ!7)6BJ@)0tB0_G;
zILVOYn87;I$k3+Rh`01}ro=im@b@cb5j9J3GKhZTsx5P8iF$WRD{;$x4Xm&?-uHjR
zS9(x(5BrYc=pjJ}00{pkJyeh`E=Y&#&;we0B}dmcayaw`1Oe^+)-^7`r_`mI^SE;1
zXmt`Q&GeAW56feEOuB}iR-Vv}au1nzek_eXK>iy_7lNf2oy;hvhaRTQJz*FzZAkOR
zD-EqA;;J@nwPB{9w=q-#H*~4%`_#3n+utj;tRB!Xs0UL}BPGMwbK5jqF>G(BbprRx
z&HA_EEe7Z5lbtj|D$d_jbB*A1=RKji{wK`<rdQNr+wg5luRfTrTZq#M@piXYFr`q9
z;EiHR#Vw?vyywlC<Z0=e*kPAp&rghCK`X2ybWd@{Yu_q&Oxx$prZ!S8)A=oNmpUXt
zpzdy%#K}66MHZFJ+gv=MV`*ICIwzo6Vr*USu|QIW1fSKN^p8W^O&2pCodezqQ8H*?
zvmetPn1`Cb9wqk}zWRXME84IX8dPJ9Moz+Db@eK4clVAx+@F#XzM+ED+BOJ0$8*2I
z`6aomX=hfLe$X7<lM=Gyc-<};f?VT%$_soZQLI)UlT$UWmIg6+m^5F)r!9*q>S4)^
z=@HqT5gc?%DUK^@nIMJ4q_RL7PcdE;Fv?Nj@VB~SK~k84Uo1Z{>$pxvUk9#coLk^{
z3d|-QXG>rcWQ$~awmf^mO0Plr1MRjpBWhZe-97OWdVXy83t@0sorDAsjmt<>vO=dr
zfB?W4E}sbfKZLPC+(1OYdmNIbD81&uI^Ifbbop0=wK*0Cxok+$PK<BYd<Bheug;Gl
z%(b_-x>H%J477R5?9ztrrQS0>Wo%41W8!*V2?8o<<0=E?R$FjLI9FEU_oe9MC!Q>q
zQG6V-(a$m2CdAp!Y=9i*grk)Vc*4e~f<Lq=6b_`UR#%*3?-1R>BC8Z>VZ&hq(_loT
z2nl_Fz4}=)kdkB%Exa&4z_!bPib`_1j*|G#x@(WA%H)?;(&fhVG;hxgU(AFyM3l#t
zoa3KWxKbJT`u5^g@b%(WzF90twtL;}V**5dLaKaiWqJL1)*s7Kn)-_%$%oWFkek<8
z>6&jCgYR;Sd$?{5<1?~crP9_5@|Z1(9=@M=qI^1iv|n%JNq8oLUS{@Wu+r<E8UDx6
z3S_+(e*f8dld4gA%w`rHBLf|=IWrtx=sS=Aa`5+7_r^;%f2CkF;36<h^hqK|%Q{Y0
zPm)*4PPv!6o3s5omcKjBYenI7Y*+5GH<V({gE=?Z@}5>zj$Fd{xny>jlYvHxMDHB?
zlT_-fCi73z#NM5f%5ijB=W;dbyR1SrT2@`Y;Lcf<86c#;RX7VT5P#jvqtv>oN`FZQ
z-w&OIY*E!~(3({<E!LnRXxnZ-!MebS4boO}Lum2A5!MFiS9w?!N-0clLVdaT@j%iz
zyM--0W``V49}a&p<kJz8a+!+e(|dGlm5=thTQ;VQoFgta`&CXty&gS#O4x2}YQ~yp
z$Kbp-MHRc*%#sOa*;!-FsmyH)JA14wBMq-7WE`w<LY|O^POBa0L;7i`UJ^)`o08zn
zTM--i0UzxGB)LLsI8~y-=v3FW5KpXKV#>>Zp@;~29nVvgw+Z(C_(3lGO6?XLwGA<9
zxqdimm*S{>={KqUmr{8^?m(?y$u04X+`w4Yh|GKn;=)PB>e_MU=Q?Pg)o}(!u(o!9
zB}->_cPFH$UmLIzWN4DXr*|7|W*DINFwEPzru=Ll36p3@l0Jmc=-Kux)qDseMoO1Q
z2pg-8rqS7OF2$yc@dIrynnyJ!QJ%Sq#Ugds3xt;4=ay_8RNdIkW~w;IF7YXSCJ@Us
zck42?9l)c*rf<m@WjqgBND2=!D9D;sv6YSlRw8RMy#vc*QP=oqWv@KqD7h-CtkZ3z
zhTxtApAdbq+Wg2-8lYQocN_vKx#jjU@(d=hY2vupM23u&BIMYTcI&-7ug#eiHHzMk
zA~9{f0?mRb!kLv*M4hU)opo)CJXqwWMS?oP<k=3HLl&;Eo}BF+(1ZJ7D{{fl)eR(x
zi#ey)Nt9#q!_b&Yi>%~ymUvTuCcll6(M1?vA5q=uKA3g6Y?(KC`rut(xfEzD?}S#$
z`R(4AE9@<E%k{p&?zItpZ2EK2_6T(LCB+y^&QxK_+^F{)il_wNc)HD|+r8a*?K2S7
zLTIZ{NgsC;Pq-LU;K?^@VTGaB%s?_{xAFoFX#yY5J&)**ZXE7$t(zNIjo%(gw51`$
zd;4i7<idg|HlbfLzm$g|!`j+F2cxmLbmmNqrs2n?58N{X%4L;rh!-8j8+Pe>=6Hky
zVAehRQ9Ppn)GK%rcuA}v15+c@&mRT`M=alM5vwRFWPQB8kXkQaZSg$w9EnXwkaBYg
zz%2g_%mm-6zE@$hzN>}IxJNgVcz05^)PujC*0VU<5zCTMMx6Ah!Q@3i(jH{8bK%kI
ziq;MMYRq!wDNu^lY@j&t%O=zzt3|Erl-)Y~n6z*e9)n{x-*2*7%L3t`V)eDQqcyrz
zf&7$vq{qX3Gbi@D+@;M6Lp_?^S?}IWKgBm%n~i{_k1a!<y<WGRY9fB0ai{#aMiy%j
zbk3(9m9w$8wnH>Q6$}v|12K`;kOW{)ZnZQH`|jQkp-B`bDS~TQLdWBpN5ORDPiGZ6
z30RH`&KRRGmdk7}9qY)lHzS&(8wTWCFOd!HD5f<bhy1wM!ZjHnq-5ww;YGydHTcI1
zbq~$+ZWPi?*ORzS@%pPSFNI82l;6Rw&Z`6&(S-9|tIp$nx65xg(&=j5C3~;>UZ;Vb
zq~ghE!%1x|!cd;iME2XpTVSXsr=W$1DcV;j%$$i5_EHu4z`?ET8d73Mzd7J6<>e=*
zQXxH9Q})+w(-?*YqiMpH>jDB7;yQDifCGvNbA*FZffvUp4dL1xBdjF}%|_X&@t;8&
zN~|~L+6iUN->GvW#tjJz*C__Q4Se1oJ3Jt6|2k!YPw*#d-_j26e><Rqbd##3g2L6(
zCmUdS3DJb`g44+`q<KLer$Q4lewy)lDQMa54?60C4xiO!M#ciu{0_o*0;^q~sGxrH
z6HPz|%g_#U!pKG;l2BTX2GZeLY5D-rW<Hf_tp80-JBV+TwJ(jwMTYs10wvAvHsjTg
znd1EZc_8!Pjv@MVDKe~ny-LYIX`+3jZts0ZZnNm(Nk$~%%m#^P4wW{?E!!S{xK|G$
z>uKv!+ML+@*?}Ev%Q2C3>5PXFe~zM-ifED*?}(pJvmaWD+3D%Hx5R&k!|NIwYFp(K
z<E+tybotuS@{~Zjm=l2<xkotfvacG4O{=rMUalc0oZ(egEsMC`^iu^HvGdg?1YoJ%
z4FSr?p8JiG5iUFLY=p?5-2PK{m7gSCDRvMUF4Su<#}!i5N^73n&C0|e!Et73YaQ({
zW?TD~iSzVCCux1P$%3eLH>sAZ@g$=bI4Ts`3?+Fd1_S){HKhzXCvItCT6*I~b4Mt)
z$7omLMn>32a+ZMWhSeSAORmYY3~9pAmK5*#yNVNiQ|Pvu&fC}FCve_M(QCPHYmlqx
zZ%XS^#R#iO<S^87<FyG$BDan=rHROi8;I&JYwYN&D2szE$A9imL{)^bb#M1jJN0VK
zN92XLjxH1EuDv2d7shm2(L7WTAg>R(9m3IqtS8@XWHyo53td=DS$Jdndhra1b3fOK
zVr<pDPQA)s3#^nOflye%o^NI#ua^U?I!Cs)g|(8M#Y&X{mjawN-u!_F4?eEi@ycwn
z6LH2Xa<4;aXI~*LYgHYF2c*IO=N15tptwN^cqe^u|EVGH-R<a>z=83>!1$fVU4OX!
z)FAj7_$aOZ#r+HLE8NGg%Kba$(c1mcWBr@Q4<-N4jt5%)>o`~=6E1NcKz(!kq3r+J
z|3KRx`ooU&|E}~8qa3C652N_~OO#)g|M!STYya;v!}vY@&4_<_y!p}zI6SDMCFCK>
z6#h}W_jUC@&-?F!@(ad6FX<mt#~}ta(f@&QkT-vZIO^%|5MqVo_aOeVS$_pNsu&-F
wJi~RH{?4?&@72$x`R64ZRY?w`TY>&l^siGV#BD7A0K&bF;hu=`C=b5<51C;gEC2ui

literal 0
HcmV?d00001

diff --git a/src/main/asciidoc/_chapters/ops_mgt.adoc b/src/main/asciidoc/_chapters/ops_mgt.adoc
index e15b73f865..626cddfdba 100644
--- a/src/main/asciidoc/_chapters/ops_mgt.adoc
+++ b/src/main/asciidoc/_chapters/ops_mgt.adoc
@@ -790,6 +790,34 @@ $ bin/hbase cellcounter <tablename> <outputDir> [reportSeparator] [regex or pref
 
 Note: just like RowCounter, caching for the input Scan is configured via `hbase.client.scanner.caching` in the job configuration.
 
+[[coprocessorValidator]]
+=== CoprocessorValidator
+
+HBase supports co-processors for a long time, but the co-processor API can be changed between major releases. Co-processor validator tries to determine
+whether the old co-processors are still compatible with the actual HBase version.
+
+----
+$ bin/hbase org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator <jar> -scan|<classes>
+Options:
+ -e      Threat warnings as errors.
+ -scan   Scan jar for observers.
+----
+
+The first parameter of the tool is the `jar` file which holds the co-processor implementation. Further parameters can be `-scan` when the tool will
+search the jar file for `Coprocessor` implementations or the `classes` can be explicitly given.
+
+The tool can report errors and warnings. Errors mean that HBase won't be able to load the coprocessor, because it is incompatible with the current version
+of HBase. Warnings mean that the co-processors can be loaded, but they won't work as expected. If `-e` option is given, then the tool will also fail
+for warnings.
+
+Please note that this tool cannot validate every aspects of jar files, it just does some static checks.
+
+For example:
+
+----
+$ bin/hbase org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator my-coprocessor.jar MyMasterObserver MyRegionObserver
+----
+
 === mlockall
 
 It is possible to optionally pin your servers in physical memory making them less likely to be swapped out in oversubscribed environments by having the servers call link:http://linux.die.net/man/2/mlockall[mlockall] on startup.
-- 
2.17.0

