diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
index 7615d6e..9bbe8a9 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
@@ -376,7 +376,7 @@ public class ClientScanner extends AbstractClientScanner {
           if (values != null && values.length > 0) {
             for (Result rs : values) {
               cache.add(rs);
-              for (Cell kv : rs.raw()) {
+              for (Cell kv : rs.rawCells()) {
                 // TODO make method in Cell or CellUtil
                 remainingResultSize -= KeyValueUtil.ensureKeyValue(kv).heapSize();
               }
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
index 9039b01..0d7406f 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
@@ -61,7 +61,7 @@ import org.apache.hadoop.hbase.util.Bytes;
  * A Result is backed by an array of {@link KeyValue} objects, each representing
  * an HBase cell defined by the row, family, qualifier, timestamp, and value.<p>
  *
- * The underlying {@link KeyValue} objects can be accessed through the method {@link #list()}.
+ * The underlying {@link KeyValue} objects can be accessed through the method {@link #listCells()}.
  * Each KeyValue can then be accessed through
  * {@link KeyValue#getRow()}, {@link KeyValue#getFamily()}, {@link KeyValue#getQualifier()},
  * {@link KeyValue#getTimestamp()}, and {@link KeyValue#getValue()}.<p>
@@ -85,7 +85,7 @@ public class Result implements CellScannable {
   public static final Result EMPTY_RESULT = new Result();
 
   /**
-   * Creates an empty Result w/ no KeyValue payload; returns null if you call {@link #raw()}.
+   * Creates an empty Result w/ no KeyValue payload; returns null if you call {@link #rawCells()}.
    * Use this to represent no results if <code>null</code> won't do or in old 'mapred' as oppposed to 'mapreduce' package
    * MapReduce where you need to overwrite a Result
    * instance with a {@link #copyFrom(Result)} call.
@@ -147,20 +147,55 @@ public class Result implements CellScannable {
    *
    * @return array of Cells; can be null if nothing in the result
    */
-  public Cell[] raw() {
+  public Cell[] rawCells() {
     return cells;
   }
 
   /**
+   * Return an cells of a Result as an array of KeyValues 
+   * 
+   * WARNING do not use, expensive.  This does an arraycopy of the cell[]'s value.
+   *
+   * Added to ease transition from  0.94 -> 0.96.
+   * 
+   * @deprecated as of 0.96, use {@link #rawCells()}  
+   * @return
+   */
+  @Deprecated
+  public KeyValue[] raw() {
+    KeyValue[] kvs = new KeyValue[cells.length];
+    for (int i = 0 ; i < kvs.length; i++) {
+      kvs[i] = KeyValueUtil.ensureKeyValue(cells[i]);
+    }
+    return kvs;
+  }
+
+  /**
    * Create a sorted list of the Cell's in this result.
    *
    * Since HBase 0.20.5 this is equivalent to raw().
    *
    * @return The sorted list of Cell's.
    */
-  public List<Cell> list() {
-    return isEmpty()? null: Arrays.asList(raw());
+  public List<Cell> listCells() {
+    return isEmpty()? null: Arrays.asList(rawCells());
   }
+  
+  /**
+   * Return an cells of a Result as an array of KeyValues 
+   * 
+   * WARNING do not use, expensive.  This does  an arraycopy of the cell[]'s value.
+   *
+   * Added to ease transition from  0.94 -> 0.96.
+   * 
+   * @deprecated as of 0.96, use {@link #listCells()}  
+   * @return
+   */
+  @Deprecated
+  public List<KeyValue> list() {
+    return isEmpty() ? null : Arrays.asList(raw());
+  }
+
 
   /**
    * Return the Cells for the specific column.  The Cells are sorted in
@@ -180,7 +215,7 @@ public class Result implements CellScannable {
   public List<Cell> getColumn(byte [] family, byte [] qualifier) {
     List<Cell> result = new ArrayList<Cell>();
 
-    Cell [] kvs = raw();
+    Cell [] kvs = rawCells();
 
     if (kvs == null || kvs.length == 0) {
       return result;
@@ -275,7 +310,7 @@ public class Result implements CellScannable {
    * selected in the query (Get/Scan)
    */
   public Cell getColumnLatest(byte [] family, byte [] qualifier) {
-    Cell [] kvs = raw(); // side effect possibly.
+    Cell [] kvs = rawCells(); // side effect possibly.
     if (kvs == null || kvs.length == 0) {
       return null;
     }
@@ -306,7 +341,7 @@ public class Result implements CellScannable {
   public Cell getColumnLatest(byte [] family, int foffset, int flength,
       byte [] qualifier, int qoffset, int qlength) {
 
-    Cell [] kvs = raw(); // side effect possibly.
+    Cell [] kvs = rawCells(); // side effect possibly.
     if (kvs == null || kvs.length == 0) {
       return null;
     }
@@ -692,8 +727,8 @@ public class Result implements CellScannable {
       throw new Exception("This row doesn't have the same number of KVs: "
           + res1.toString() + " compared to " + res2.toString());
     }
-    Cell[] ourKVs = res1.raw();
-    Cell[] replicatedKVs = res2.raw();
+    Cell[] ourKVs = res1.rawCells();
+    Cell[] replicatedKVs = res2.rawCells();
     for (int i = 0; i < res1.size(); i++) {
       if (!ourKVs[i].equals(replicatedKVs[i]) ||
           !Bytes.equals(CellUtil.getValueArray(ourKVs[i]), CellUtil.getValueArray(replicatedKVs[i]))) {
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
index a4747ec..0eb3cfd 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java
@@ -253,7 +253,7 @@ public class ScannerCallable extends RegionServerCallable<Result[]> {
     }
     long resultSize = 0;
     for (Result rr : rrs) {
-      for (Cell kv : rr.raw()) {
+      for (Cell kv : rr.rawCells()) {
         // TODO add getLength to Cell/use CellUtil#estimatedSizeOf
         resultSize += KeyValueUtil.ensureKeyValue(kv).getLength();
       }
diff --git hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
index a976b44..16c0a8e 100644
--- hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
+++ hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java
@@ -1026,7 +1026,7 @@ public final class ProtobufUtil {
    */
   public static ClientProtos.Result toResult(final Result result) {
     ClientProtos.Result.Builder builder = ClientProtos.Result.newBuilder();
-    Cell [] cells = result.raw();
+    Cell [] cells = result.rawCells();
     if (cells != null) {
       for (Cell c : cells) {
         builder.addCell(toCell(c));
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
index 6be129d..485891b 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/mapreduce/IntegrationTestImportTsv.java
@@ -154,7 +154,7 @@ public class IntegrationTestImportTsv implements Configurable, Tool {
       Iterator<KeyValue> expectedIt = simple_expected.iterator();
       while (resultsIt.hasNext() && expectedIt.hasNext()) {
         Result r = resultsIt.next();
-        for (Cell actual : r.raw()) {
+        for (Cell actual : r.rawCells()) {
           assertTrue(
             "Ran out of expected values prematurely!",
             expectedIt.hasNext());
diff --git hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
index a2c416a..ba4eba4 100644
--- hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
+++ hbase-it/src/test/java/org/apache/hadoop/hbase/test/IntegrationTestLoadAndVerify.java
@@ -248,7 +248,7 @@ public class IntegrationTestLoadAndVerify  extends IntegrationTestBase  {
         throws IOException, InterruptedException {
       BytesWritable bwKey = new BytesWritable(key.get());
       BytesWritable bwVal = new BytesWritable();
-      for (Cell kv : value.list()) {
+      for (Cell kv : value.listCells()) {
         if (Bytes.compareTo(TEST_QUALIFIER, 0, TEST_QUALIFIER.length,
                             kv.getQualifierArray(), kv.getQualifierOffset(), kv.getQualifierLength()) == 0) {
           context.write(bwKey, EMPTY);
diff --git hbase-server/pom.xml.hadoop2 hbase-server/pom.xml.hadoop2
new file mode 100644
index 0000000..a453fec
--- /dev/null
+++ hbase-server/pom.xml.hadoop2
@@ -0,0 +1,721 @@
+<?xml version="1.0"?>
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <!--
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+-->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <artifactId>hbase</artifactId>
+    <groupId>org.apache.hbase</groupId>
+    <version>0.97.0-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+  <artifactId>hbase-server</artifactId>
+  <name>HBase - Server</name>
+  <description>Main functionality for HBase</description>
+  <build>
+    <!-- Makes sure the resources get added before they are processed
+      by placing this first -->
+    <resources>
+      <!-- Add the build webabpps to the classpth -->
+      <resource>
+        <directory>${project.build.directory}</directory>
+        <includes>
+          <include>hbase-webapps/**</include>
+        </includes>
+      </resource>
+    </resources>
+    <testResources>
+      <testResource>
+        <directory>src/test/resources</directory>
+        <includes>
+          <include>**/**</include>
+        </includes>
+      </testResource>
+    </testResources>
+    <plugins>
+        <plugin>
+          <groupId>org.apache.maven.plugins</groupId>
+          <artifactId>maven-site-plugin</artifactId>
+          <configuration>
+            <skip>true</skip>
+          </configuration>
+        </plugin>
+      <!-- Run with -Dmaven.test.skip.exec=true to build -tests.jar without running
+        tests (this is needed for upstream projects whose tests need this jar simply for
+        compilation) -->
+      <plugin>
+        <!--Make it so assembly:single does nothing in here-->
+        <artifactId>maven-assembly-plugin</artifactId>
+        <version>${maven.assembly.version}</version>
+        <configuration>
+          <skipAssembly>true</skipAssembly>
+        </configuration>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-jar-plugin</artifactId>
+        <configuration>
+          <archive>
+            <manifest>
+              <mainClass>org/apache/hadoop/hbase/mapreduce/Driver</mainClass>
+            </manifest>
+          </archive>
+          <!-- Exclude these 2 packages, because their dependency _binary_ files
+            include the sources, and Maven 2.2 appears to add them to the sources to compile,
+            weird -->
+          <excludes>
+            <exclude>org/apache/jute/**</exclude>
+            <exclude>org/apache/zookeeper/**</exclude>
+            <exclude>**/*.jsp</exclude>
+            <exclude>hbase-site.xml</exclude>
+            <exclude>log4j.properties</exclude>
+            <exclude>mapred-queues.xml</exclude>
+            <exclude>hdfs-site.xml</exclude>
+          </excludes>
+        </configuration>
+      </plugin>
+      <!-- Make a jar and put the sources in the jar -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-source-plugin</artifactId>
+      </plugin>
+      <!-- General ant tasks, bound to different build phases -->
+      <plugin>
+        <artifactId>maven-antrun-plugin</artifactId>
+        <executions>
+          <!-- Generate web app sources -->
+          <execution>
+            <id>generate</id>
+            <phase>generate-sources</phase>
+            <configuration>
+              <target>
+                <property name="build.webapps" location="${project.build.directory}/hbase-webapps"/>
+                <property name="src.webapps" location="${basedir}/src/main/resources/hbase-webapps"/>
+                <property name="generated.sources" location="${project.build.directory}/generated-sources"/>
+                <mkdir dir="${build.webapps}"/>
+                <copy todir="${build.webapps}">
+                  <fileset dir="${src.webapps}">
+                    <exclude name="**/*.jsp"/>
+                    <exclude name="**/.*"/>
+                    <exclude name="**/*~"/>
+                  </fileset>
+                </copy>
+                <!--The compile.classpath is passed in by maven -->
+                <taskdef classname="org.apache.jasper.JspC" name="jspcompiler" classpathref="maven.compile.classpath"/>
+                <mkdir dir="${build.webapps}/master/WEB-INF"/>
+                <jspcompiler uriroot="${src.webapps}/master" outputdir="${generated.sources}/java" package="org.apache.hadoop.hbase.generated.master" webxml="${build.webapps}/master/WEB-INF/web.xml"/>
+                <mkdir dir="${build.webapps}/regionserver/WEB-INF"/>
+                <jspcompiler uriroot="${src.webapps}/regionserver" outputdir="${generated.sources}/java" package="org.apache.hadoop.hbase.generated.regionserver" webxml="${build.webapps}/regionserver/WEB-INF/web.xml"/>
+                <mkdir dir="${build.webapps}/rest/WEB-INF"/>
+                <jspcompiler uriroot="${src.webapps}/rest" outputdir="${generated.sources}/java" package="org.apache.hadoop.hbase.generated.rest" webxml="${build.webapps}/rest/WEB-INF/web.xml"/>
+                <mkdir dir="${build.webapps}/thrift/WEB-INF"/>
+                <jspcompiler uriroot="${src.webapps}/thrift" outputdir="${generated.sources}/java" package="org.apache.hadoop.hbase.generated.thrift" webxml="${build.webapps}/thrift/WEB-INF/web.xml"/>
+              </target>
+            </configuration>
+            <goals>
+              <goal>run</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>build-helper-maven-plugin</artifactId>
+        <executions>
+          <!-- Add the generated sources -->
+          <execution>
+            <id>jspcSource-packageInfo-Avro-source</id>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>add-source</goal>
+            </goals>
+            <configuration>
+              <sources>
+                <source>${project.build.directory}/generated-jamon</source>
+                <source>${project.build.directory}/generated-sources/java</source>
+              </sources>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.jamon</groupId>
+        <artifactId>jamon-maven-plugin</artifactId>
+        <executions>
+          <execution>
+            <phase>generate-sources</phase>
+            <goals>
+              <goal>translate</goal>
+            </goals>
+            <configuration>
+              <templateSourceDir>src/main/jamon</templateSourceDir>
+              <templateOutputDir>target/generated-jamon</templateOutputDir>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <!-- General plugins -->
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-eclipse-plugin</artifactId>
+        <configuration>
+          <additionalProjectnatures>
+            <projectnature>org.jamon.project.jamonnature</projectnature>
+          </additionalProjectnatures>
+          <buildcommands>
+            <buildcommand>org.jamon.project.templateBuilder</buildcommand>
+            <buildcommand>org.eclipse.jdt.core.javabuilder</buildcommand>
+            <buildcommand>org.jamon.project.markerUpdater</buildcommand>
+          </buildcommands>
+          <additionalConfig>
+            <file>
+              <name>.settings/org.jamon.prefs</name>
+              <content># now
+                eclipse.preferences.version=1
+                templateSourceDir=src/main/jamon
+                templateOutputDir=target/generated-jamon
+              </content>
+            </file>
+          </additionalConfig>
+        </configuration>
+      </plugin>
+      <!-- Run findbugs -->
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>findbugs-maven-plugin</artifactId>
+      </plugin>
+      <!-- Testing plugins -->
+      <plugin>
+        <artifactId>maven-surefire-plugin</artifactId>
+        <configuration>
+          <properties>
+            <property>
+              <name>listener</name>
+              <value>org.apache.hadoop.hbase.ServerResourceCheckerJUnitListener</value>
+            </property>
+          </properties>
+        </configuration>
+      </plugin>
+    </plugins>
+    <!-- General Resources -->
+    <pluginManagement>
+       <plugins>
+          <!--This plugin's configuration is used to store Eclipse m2e settings only. It has no
+          influence on the Maven build itself and needs to be kept in plugin management, not in
+          the actual plugins. -->
+        <plugin>
+          <groupId>org.eclipse.m2e</groupId>
+          <artifactId>lifecycle-mapping</artifactId>
+          <version>1.0.0</version>
+          <configuration>
+            <lifecycleMappingMetadata>
+              <pluginExecutions>
+                <pluginExecution>
+                  <pluginExecutionFilter>
+                    <groupId>org.jamon</groupId>
+                    <artifactId>jamon-maven-plugin</artifactId>
+                    <versionRange>[2.3.4,)</versionRange>
+                    <goals>
+                      <goal>translate</goal>
+                    </goals>
+                  </pluginExecutionFilter>
+                  <action>
+                      <execute>
+                        <runOnIncremental>false</runOnIncremental>
+                        <runOnConfiguration>true</runOnConfiguration>
+                      </execute>
+                  </action>
+                </pluginExecution>
+                <pluginExecution>
+                  <pluginExecutionFilter>
+                    <groupId>org.apache.maven.plugins</groupId>
+                    <artifactId>maven-antrun-plugin</artifactId>
+                    <versionRange>[1.6,)</versionRange>
+                    <goals>
+                      <goal>run</goal>
+                    </goals>
+                  </pluginExecutionFilter>
+                  <action>
+                    <execute>
+                      <runOnIncremental>false</runOnIncremental>
+                      <runOnConfiguration>true</runOnConfiguration>
+                    </execute>
+                  </action>
+                </pluginExecution>
+              </pluginExecutions>
+            </lifecycleMappingMetadata>
+          </configuration>
+        </plugin>
+        <plugin>
+          <artifactId>maven-surefire-plugin</artifactId>
+          <version>${surefire.version}</version>
+          <configuration>
+            <!-- Have to set the groups here because we only do
+            split tests in this package, so groups on live in this module -->
+            <groups>${surefire.firstPartGroups}</groups>
+          </configuration>
+        </plugin>
+      </plugins>
+    </pluginManagement>
+  </build>
+  <dependencies>
+    <!-- Intra-project dependencies -->
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-common</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-protocol</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-client</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-prefix-tree</artifactId>
+      <scope>runtime</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-common</artifactId>
+      <type>test-jar</type>
+    </dependency>
+    <dependency>
+      <groupId>commons-httpclient</groupId>
+      <artifactId>commons-httpclient</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-collections</groupId>
+      <artifactId>commons-collections</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-hadoop-compat</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-hadoop-compat</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-hadoop2-compat</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-hadoop2-compat</artifactId>
+      <version>${project.version}</version>
+      <type>test-jar</type>
+      <scope>test</scope>
+    </dependency>
+    <!-- General dependencies -->
+    <dependency>
+      <groupId>io.netty</groupId>
+      <artifactId>netty</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.yammer.metrics</groupId>
+      <artifactId>metrics-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.google.guava</groupId>
+      <artifactId>guava</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-cli</groupId>
+      <artifactId>commons-cli</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.github.stephenc.high-scale-lib</groupId>
+      <artifactId>high-scale-lib</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-codec</groupId>
+      <artifactId>commons-codec</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-io</groupId>
+      <artifactId>commons-io</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-lang</groupId>
+      <artifactId>commons-lang</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>commons-logging</groupId>
+      <artifactId>commons-logging</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.commons</groupId>
+      <artifactId>commons-math</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>log4j</groupId>
+      <artifactId>log4j</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.zookeeper</groupId>
+      <artifactId>zookeeper</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-api</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.thrift</groupId>
+      <artifactId>libthrift</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.jruby</groupId>
+      <artifactId>jruby-complete</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>jetty</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>jetty-util</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>jetty-sslengine</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>jsp-2.1</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>jsp-api-2.1</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.mortbay.jetty</groupId>
+      <artifactId>servlet-api-2.5</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.codehaus.jackson</groupId>
+      <artifactId>jackson-core-asl</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.codehaus.jackson</groupId>
+      <artifactId>jackson-mapper-asl</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.codehaus.jackson</groupId>
+      <artifactId>jackson-jaxrs</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.codehaus.jackson</groupId>
+      <artifactId>jackson-xc</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>tomcat</groupId>
+      <artifactId>jasper-compiler</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>tomcat</groupId>
+      <artifactId>jasper-runtime</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.jamon</groupId>
+      <artifactId>jamon-runtime</artifactId>
+    </dependency>
+    <!-- REST dependencies -->
+    <dependency>
+      <groupId>com.google.protobuf</groupId>
+      <artifactId>protobuf-java</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.sun.jersey</groupId>
+      <artifactId>jersey-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.sun.jersey</groupId>
+      <artifactId>jersey-json</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.sun.jersey</groupId>
+      <artifactId>jersey-server</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>javax.xml.bind</groupId>
+      <artifactId>jaxb-api</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>stax</groupId>
+      <artifactId>stax-api</artifactId>
+    </dependency>
+    <!-- Test Dependencies -->
+    <dependency>
+      <groupId>org.codehaus.jettison</groupId>
+      <artifactId>jettison</artifactId>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.cloudera.htrace</groupId>
+      <artifactId>htrace-core</artifactId>
+   </dependency>
+    <dependency>
+      <groupId>org.cloudera.htrace</groupId>
+      <artifactId>htrace-zipkin</artifactId>
+    </dependency>
+  </dependencies>
+  <profiles>
+    <!-- Skip the tests in this module -->
+    <profile>
+      <id>skipServerTests</id>
+      <activation>
+        <property>
+          <name>skipServerTests</name>
+        </property>
+      </activation>
+      <properties>
+        <surefire.skipFirstPart>true</surefire.skipFirstPart>
+        <surefire.skipSecondPart>true</surefire.skipSecondPart>
+      </properties>
+    </profile>
+    <!-- Special builds -->
+    <profile>
+      <id>hadoop-snappy</id>
+      <activation>
+        <activeByDefault>false</activeByDefault>
+        <property>
+          <name>snappy</name>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-snappy</artifactId>
+          <version>${hadoop-snappy.version}</version>
+        </dependency>
+      </dependencies>
+    </profile>
+    <profile>
+      <id>native</id>
+      <activation>
+        <activeByDefault>false</activeByDefault>
+      </activation>
+      <build>
+        <plugins>
+          <plugin>
+            <groupId>org.apache.maven.plugins</groupId>
+            <artifactId>maven-antrun-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>make</id>
+                <phase>compile</phase>
+                <goals><goal>run</goal></goals>
+                <configuration>
+                  <target>
+                    <mkdir dir="${project.build.directory}/native"/>
+                    <exec executable="cmake" dir="${project.build.directory}/native" failonerror="true">
+                      <arg line="${basedir}/src/main/native -DJVM_ARCH_DATA_MODEL=${sun.arch.data.model}"/>
+                    </exec>
+                    <exec executable="make" dir="${project.build.directory}/native" failonerror="true">
+                      <arg line="VERBOSE=1"/>
+                    </exec>
+                  </target>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+    <!-- Profiles for building against different hadoop versions -->
+    <!-- There are a lot of common dependencies used here, should investigate
+    if we can combine these profiles somehow -->
+    <!-- profile against Hadoop 1.1.x: This is the default. It has to have the same
+    activation property as the parent Hadoop 1.1.x profile to make sure it gets run at
+    the same time. -->
+    <profile>
+      <id>hadoop-1.1</id>
+      <activation>
+        <property>
+            <name>hadoop.profile</name><value>1.1</value>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-core</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-test</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+    <profile>
+      <id>hadoop-1.0</id>
+      <activation>
+        <property>
+          <name>hadoop.profile</name>
+          <value>1.0</value>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-core</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-test</artifactId>
+        </dependency>
+      </dependencies>
+    </profile>
+    <!--
+      profile for building against Hadoop 2.0.0-alpha. Activate using:
+       mvn -Dhadoop.profile=2.0
+    -->
+    <profile>
+      <id>hadoop-2.0</id>
+      <activation>
+        <property>
+            <name>!hadoop.profile</name>
+        </property>
+      </activation>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-auth</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-core</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+          <type>test-jar</type>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-hdfs</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-hdfs</artifactId>
+          <type>test-jar</type>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-client</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-annotations</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-minicluster</artifactId>
+        </dependency>
+      </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <artifactId>maven-dependency-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>create-mrapp-generated-classpath</id>
+                <phase>generate-test-resources</phase>
+                <goals>
+                  <goal>build-classpath</goal>
+                </goals>
+                <configuration>
+                  <!-- needed to run the unit test for DS to generate
+                  the required classpath that is required in the env
+                  of the launch container in the mini mr/yarn cluster
+                  -->
+                  <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+    <!--
+      profile for building against Hadoop 3.0.x. Activate using:
+       mvn -Dhadoop.profile=3.0
+    -->
+    <profile>
+      <id>hadoop-3.0</id>
+      <activation>
+        <property>
+          <name>hadoop.profile</name>
+          <value>3.0</value>
+        </property>
+      </activation>
+      <properties>
+        <hadoop.version>3.0-SNAPSHOT</hadoop.version>
+      </properties>
+      <dependencies>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-common</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-annotations</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-minicluster</artifactId>
+        </dependency>
+      </dependencies>
+      <build>
+        <plugins>
+          <plugin>
+            <artifactId>maven-dependency-plugin</artifactId>
+            <executions>
+              <execution>
+                <id>create-mrapp-generated-classpath</id>
+                <phase>generate-test-resources</phase>
+                <goals>
+                  <goal>build-classpath</goal>
+                </goals>
+                <configuration>
+                  <!-- needed to run the unit test for DS to generate
+                  the required classpath that is required in the env
+                  of the launch container in the mini mr/yarn cluster
+                  -->
+                  <outputFile>${project.build.directory}/test-classes/mrapp-generated-classpath</outputFile>
+                </configuration>
+              </execution>
+            </executions>
+          </plugin>
+        </plugins>
+      </build>
+    </profile>
+  </profiles>
+</project>
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
index b40de51..ce35d99 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapred/GroupingTableMap.java
@@ -116,7 +116,7 @@ implements TableMap<ImmutableBytesWritable,Result> {
     ArrayList<byte[]> foundList = new ArrayList<byte[]>();
     int numCols = columns.length;
     if (numCols > 0) {
-      for (Cell value: r.list()) {
+      for (Cell value: r.listCells()) {
         byte [] column = KeyValue.makeColumn(CellUtil.getFamilyArray(value),
             CellUtil.getQualifierArray(value));
         for (int i = 0; i < numCols; i++) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java
index 7df2533..f5561ef 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CellCounter.java
@@ -116,7 +116,7 @@ public class CellCounter {
         context.getCounter(Counters.ROWS).increment(1);
         context.write(new Text("Total ROWS"), new IntWritable(1));
 
-        for (Cell value : values.list()) {
+        for (Cell value : values.listCells()) {
           currentRowKey = Bytes.toStringBinary(CellUtil.getRowArray(value));
           String thisRowFamilyName = Bytes.toStringBinary(CellUtil.getFamilyArray(value));
           if (!thisRowFamilyName.equals(currentFamilyName)) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
index 4e46663..3a75b25 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/GroupingTableMapper.java
@@ -109,7 +109,7 @@ extends TableMapper<ImmutableBytesWritable,Result> implements Configurable {
     ArrayList<byte[]> foundList = new ArrayList<byte[]>();
     int numCols = columns.length;
     if (numCols > 0) {
-      for (Cell value: r.list()) {
+      for (Cell value: r.listCells()) {
         byte [] column = KeyValue.makeColumn(CellUtil.getFamilyArray(value),
             CellUtil.getQualifierArray(value));
         for (int i = 0; i < numCols; i++) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
index 6099a8b..9fae177 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java
@@ -93,7 +93,7 @@ public class Import {
       Context context)
     throws IOException {
       try {
-        for (Cell kv : value.raw()) {
+        for (Cell kv : value.rawCells()) {
           kv = filterKv(kv);
           // skip if we filtered it out
           if (kv == null) continue;
@@ -143,7 +143,7 @@ public class Import {
     throws IOException, InterruptedException {
       Put put = null;
       Delete delete = null;
-      for (Cell kv : result.raw()) {
+      for (Cell kv : result.rawCells()) {
         kv = filterKv(kv);
         // skip if we filter it out
         if (kv == null) continue;
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index 5c8ccd2..4d24821 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -3070,7 +3070,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
               if (!results.isEmpty()) {
                 for (Result r : results) {
                   if (maxScannerResultSize < Long.MAX_VALUE){
-                    for (Cell kv : r.raw()) {
+                    for (Cell kv : r.rawCells()) {
                       // TODO
                       currentScanResultSize += KeyValueUtil.ensureKeyValue(kv).heapSize();
                     }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java
index fbd16e8..ed79607 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/rest/RowResultGenerator.java
@@ -64,7 +64,7 @@ public class RowResultGenerator extends ResultGenerator {
       }
       Result result = table.get(get);
       if (result != null && !result.isEmpty()) {
-        valuesI = result.list().iterator();
+        valuesI = result.listCells().iterator();
       }
     } catch (DoNotRetryIOException e) {
       // Warn here because Stargate will return 404 in the case if multiple
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java hbase-server/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java
index 75a5666..ebeae0d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/rest/ScannerResultGenerator.java
@@ -149,7 +149,7 @@ public class ScannerResultGenerator extends ResultGenerator {
         }
       }
       if (cached != null) {
-        rowI = cached.list().iterator();
+        rowI = cached.listCells().iterator();
         loop = true;
         cached = null;
       } else {
@@ -162,7 +162,7 @@ public class ScannerResultGenerator extends ResultGenerator {
           LOG.error(StringUtils.stringifyException(e));
         }
         if (result != null && !result.isEmpty()) {
-          rowI = result.list().iterator();
+          rowI = result.listCells().iterator();
           loop = true;
         }
       }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
index 26073f3..74390c9 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java
@@ -513,7 +513,7 @@ public class AccessControlLists {
       byte[] entryName, Result result) {
     ListMultimap<String, TablePermission> perms = ArrayListMultimap.create();
     if (result != null && result.size() > 0) {
-      for (Cell kv : result.raw()) {
+      for (Cell kv : result.rawCells()) {
 
         Pair<String,TablePermission> permissionsOfUserOnTable =
             parsePermissionRecord(entryName, kv);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
index 0b36568..eecc9e2 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java
@@ -672,7 +672,7 @@ public class ThriftServerRunner implements Runnable {
           get.addColumn(family, qualifier);
         }
         Result result = table.get(get);
-        return ThriftUtilities.cellFromHBase(result.raw());
+        return ThriftUtilities.cellFromHBase(result.rawCells());
       } catch (IOException e) {
         LOG.warn(e.getMessage(), e);
         throw new IOError(e.getMessage());
@@ -704,7 +704,7 @@ public class ThriftServerRunner implements Runnable {
         get.addColumn(family, qualifier);
         get.setMaxVersions(numVersions);
         Result result = table.get(get);
-        return ThriftUtilities.cellFromHBase(result.raw());
+        return ThriftUtilities.cellFromHBase(result.rawCells());
       } catch (IOException e) {
         LOG.warn(e.getMessage(), e);
         throw new IOError(e.getMessage());
@@ -740,7 +740,7 @@ public class ThriftServerRunner implements Runnable {
         get.setTimeRange(0, timestamp);
         get.setMaxVersions(numVersions);
         Result result = table.get(get);
-        return ThriftUtilities.cellFromHBase(result.raw());
+        return ThriftUtilities.cellFromHBase(result.rawCells());
       } catch (IOException e) {
         LOG.warn(e.getMessage(), e);
         throw new IOError(e.getMessage());
@@ -1371,7 +1371,7 @@ public class ThriftServerRunner implements Runnable {
       try {
         HTable table = getTable(getBytes(tableName));
         Result result = table.getRowOrBefore(getBytes(row), getBytes(family));
-        return ThriftUtilities.cellFromHBase(result.raw());
+        return ThriftUtilities.cellFromHBase(result.rawCells());
       } catch (IOException e) {
         LOG.warn(e.getMessage(), e);
         throw new IOError(e.getMessage());
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
index 59ec052..3d9c8de 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/thrift/ThriftUtilities.java
@@ -152,7 +152,7 @@ public class ThriftUtilities {
         result.row = ByteBuffer.wrap(result_.getRow());
         if (sortColumns) {
           result.sortedColumns = new ArrayList<TColumn>();
-          for (Cell kv : result_.raw()) {
+          for (Cell kv : result_.rawCells()) {
             result.sortedColumns.add(new TColumn(
                 ByteBuffer.wrap(KeyValue.makeColumn(CellUtil.getFamilyArray(kv),
                     CellUtil.getQualifierArray(kv))),
@@ -160,7 +160,7 @@ public class ThriftUtilities {
           }
         } else {
           result.columns = new TreeMap<ByteBuffer, TCell>();
-          for (Cell kv : result_.raw()) {
+          for (Cell kv : result_.rawCells()) {
             result.columns.put(
                 ByteBuffer.wrap(KeyValue.makeColumn(CellUtil.getFamilyArray(kv),
                     CellUtil.getQualifierArray(kv))),
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
index 0ee8e84..1a61e1b 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/thrift2/ThriftUtilities.java
@@ -140,7 +140,7 @@ public class ThriftUtilities {
    * @return converted result, returns an empty result if the input is <code>null</code>
    */
   public static TResult resultFromHBase(Result in) {
-    Cell[] raw = in.raw();
+    Cell[] raw = in.rawCells();
     TResult out = new TResult();
     byte[] row = in.getRow();
     if (row != null) {
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
index efd1390..c706654 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java
@@ -2570,8 +2570,8 @@ public class HBaseFsck extends Configured implements Tool {
       public boolean processRow(Result result) throws IOException {
         try {
 
-          // record the latest modification of this hbase:meta record
-          long ts =  Collections.max(result.list(), comp).getTimestamp();
+          // record the latest modification of this META record
+          long ts =  Collections.max(result.listCells(), comp).getTimestamp();
           Pair<HRegionInfo, ServerName> pair = HRegionInfo.getHRegionInfoAndServerName(result);
           if (pair == null || pair.getFirst() == null) {
             emptyRegionInfoQualifiers.add(result);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
index 3e59649..a73becf 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/HBaseTestCase.java
@@ -535,7 +535,7 @@ public abstract class HBaseTestCase extends TestCase {
         return false;
       }
       values.clear();
-      values.addAll(results.list());
+      values.addAll(results.listCells());
       return true;
     }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
index ebe8376..94c950b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestAcidGuarantees.java
@@ -173,7 +173,7 @@ public class TestAcidGuarantees implements Tool {
       msg.append("Failed after ").append(numVerified).append("!");
       msg.append("Expected=").append(Bytes.toStringBinary(expected));
       msg.append("Got:\n");
-      for (Cell kv : res.list()) {
+      for (Cell kv : res.listCells()) {
         msg.append(kv.toString());
         msg.append(" val= ");
         msg.append(Bytes.toStringBinary(CellUtil.getValueArray(kv)));
@@ -230,7 +230,7 @@ public class TestAcidGuarantees implements Tool {
       msg.append("Failed after ").append(numRowsScanned).append("!");
       msg.append("Expected=").append(Bytes.toStringBinary(expected));
       msg.append("Got:\n");
-      for (Cell kv : res.list()) {
+      for (Cell kv : res.listCells()) {
         msg.append(kv.toString());
         msg.append(" val= ");
         msg.append(Bytes.toStringBinary(CellUtil.getValueArray(kv)));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
index 13ca615..5f45be3 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TestMultiVersions.java
@@ -236,7 +236,7 @@ public class TestMultiVersions {
         get.setTimeStamp(timestamp[j]);
         Result result = table.get(get);
         int cellCount = 0;
-        for(@SuppressWarnings("unused")Cell kv : result.list()) {
+        for(@SuppressWarnings("unused")Cell kv : result.listCells()) {
           cellCount++;
         }
         assertTrue(cellCount == 1);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
index a392ae4..f1bd566 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/TimestampTestBase.java
@@ -107,7 +107,7 @@ public class TimestampTestBase extends HBaseTestCase {
     get.setMaxVersions(3);
     Result result = incommon.get(get);
     assertEquals(1, result.size());
-    long time = Bytes.toLong(CellUtil.getValueArray(result.raw()[0]));
+    long time = Bytes.toLong(CellUtil.getValueArray(result.rawCells()[0]));
     assertEquals(time, currentTime);
   }
 
@@ -136,7 +136,7 @@ public class TimestampTestBase extends HBaseTestCase {
     get.addColumn(FAMILY_NAME, QUALIFIER_NAME);
     get.setMaxVersions(tss.length);
     Result result = incommon.get(get);
-    Cell [] kvs = result.raw();
+    Cell [] kvs = result.rawCells();
     assertEquals(kvs.length, tss.length);
     for(int i=0;i<kvs.length;i++) {
       t = Bytes.toLong(CellUtil.getValueArray(kvs[i]));
@@ -152,7 +152,7 @@ public class TimestampTestBase extends HBaseTestCase {
     get.setTimeRange(0, maxStamp);
     get.setMaxVersions(kvs.length - 1);
     result = incommon.get(get);
-    kvs = result.raw();
+    kvs = result.rawCells();
     assertEquals(kvs.length, tss.length - 1);
     for(int i=1;i<kvs.length;i++) {
       t = Bytes.toLong(CellUtil.getValueArray(kvs[i-1]));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
index e7c8414..b872a66 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSide.java
@@ -207,7 +207,7 @@ public class TestFromClientSide {
      s.setTimeRange(0, ts+3);
      s.setMaxVersions();
      ResultScanner scanner = h.getScanner(s);
-     Cell[] kvs = scanner.next().raw();
+     Cell[] kvs = scanner.next().rawCells();
      assertArrayEquals(T2, CellUtil.getValueArray(kvs[0]));
      assertArrayEquals(T1, CellUtil.getValueArray(kvs[1]));
      scanner.close();
@@ -216,7 +216,7 @@ public class TestFromClientSide {
      s.setRaw(true);
      s.setMaxVersions();
      scanner = h.getScanner(s);
-     kvs = scanner.next().raw();
+     kvs = scanner.next().rawCells();
      assertTrue(CellUtil.isDeleteFamily(kvs[0]));
      assertArrayEquals(T3, CellUtil.getValueArray(kvs[1]));
      assertTrue(CellUtil.isDelete(kvs[2]));
@@ -477,7 +477,7 @@ public class TestFromClientSide {
     while (scanner.hasNext()) {
       Result result = scanner.next();
       System.out.println("Got back key: " + Bytes.toString(result.getRow()));
-      for (Cell kv : result.raw()) {
+      for (Cell kv : result.rawCells()) {
         System.out.println("kv=" + kv.toString() + ", "
             + Bytes.toString(CellUtil.getValueArray(kv)));
       }
@@ -748,8 +748,8 @@ public class TestFromClientSide {
     int expectedIndex = 1;
     for(Result result : ht.getScanner(scan)) {
       assertEquals(result.size(), 1);
-      assertTrue(Bytes.equals(CellUtil.getRowArray(result.raw()[0]), ROWS[expectedIndex]));
-      assertTrue(Bytes.equals(CellUtil.getQualifierArray(result.raw()[0]),
+      assertTrue(Bytes.equals(CellUtil.getRowArray(result.rawCells()[0]), ROWS[expectedIndex]));
+      assertTrue(Bytes.equals(CellUtil.getQualifierArray(result.rawCells()[0]),
           QUALIFIERS[expectedIndex]));
       expectedIndex++;
     }
@@ -783,8 +783,8 @@ public class TestFromClientSide {
     int count = 0;
     for(Result result : ht.getScanner(scan)) {
       assertEquals(result.size(), 1);
-      assertEquals(result.raw()[0].getValueLength(), Bytes.SIZEOF_INT);
-      assertEquals(Bytes.toInt(CellUtil.getValueArray(result.raw()[0])), VALUE.length);
+      assertEquals(result.rawCells()[0].getValueLength(), Bytes.SIZEOF_INT);
+      assertEquals(Bytes.toInt(CellUtil.getValueArray(result.rawCells()[0])), VALUE.length);
       count++;
     }
     assertEquals(count, 10);
@@ -2135,15 +2135,15 @@ public class TestFromClientSide {
     result = scanner.next();
     assertTrue("Expected 1 key but received " + result.size(),
         result.size() == 1);
-    assertTrue(Bytes.equals(CellUtil.getRowArray(result.raw()[0]), ROWS[3]));
-    assertTrue(Bytes.equals(CellUtil.getValueArray(result.raw()[0]), VALUES[0]));
+    assertTrue(Bytes.equals(CellUtil.getRowArray(result.rawCells()[0]), ROWS[3]));
+    assertTrue(Bytes.equals(CellUtil.getValueArray(result.rawCells()[0]), VALUES[0]));
     result = scanner.next();
     assertTrue("Expected 2 keys but received " + result.size(),
         result.size() == 2);
-    assertTrue(Bytes.equals(CellUtil.getRowArray(result.raw()[0]), ROWS[4]));
-    assertTrue(Bytes.equals(CellUtil.getRowArray(result.raw()[1]), ROWS[4]));
-    assertTrue(Bytes.equals(CellUtil.getValueArray(result.raw()[0]), VALUES[1]));
-    assertTrue(Bytes.equals(CellUtil.getValueArray(result.raw()[1]), VALUES[2]));
+    assertTrue(Bytes.equals(CellUtil.getRowArray(result.rawCells()[0]), ROWS[4]));
+    assertTrue(Bytes.equals(CellUtil.getRowArray(result.rawCells()[1]), ROWS[4]));
+    assertTrue(Bytes.equals(CellUtil.getValueArray(result.rawCells()[0]), VALUES[1]));
+    assertTrue(Bytes.equals(CellUtil.getValueArray(result.rawCells()[1]), VALUES[2]));
     scanner.close();
 
     // Add test of bulk deleting.
@@ -2271,7 +2271,7 @@ public class TestFromClientSide {
     Get get = new Get(ROWS[numRows-1]);
     Result result = ht.get(get);
     assertNumKeys(result, numColsPerRow);
-    Cell [] keys = result.raw();
+    Cell [] keys = result.rawCells();
     for(int i=0;i<result.size();i++) {
       assertKey(keys[i], ROWS[numRows-1], FAMILY, QUALIFIERS[i], QUALIFIERS[i]);
     }
@@ -2282,7 +2282,7 @@ public class TestFromClientSide {
     int rowCount = 0;
     while((result = scanner.next()) != null) {
       assertNumKeys(result, numColsPerRow);
-      Cell [] kvs = result.raw();
+      Cell [] kvs = result.rawCells();
       for(int i=0;i<numColsPerRow;i++) {
         assertKey(kvs[i], ROWS[rowCount], FAMILY, QUALIFIERS[i], QUALIFIERS[i]);
       }
@@ -2300,7 +2300,7 @@ public class TestFromClientSide {
     get = new Get(ROWS[numRows-1]);
     result = ht.get(get);
     assertNumKeys(result, numColsPerRow);
-    keys = result.raw();
+    keys = result.rawCells();
     for(int i=0;i<result.size();i++) {
       assertKey(keys[i], ROWS[numRows-1], FAMILY, QUALIFIERS[i], QUALIFIERS[i]);
     }
@@ -2311,7 +2311,7 @@ public class TestFromClientSide {
     rowCount = 0;
     while((result = scanner.next()) != null) {
       assertNumKeys(result, numColsPerRow);
-      Cell [] kvs = result.raw();
+      Cell [] kvs = result.rawCells();
       for(int i=0;i<numColsPerRow;i++) {
         assertKey(kvs[i], ROWS[rowCount], FAMILY, QUALIFIERS[i], QUALIFIERS[i]);
       }
@@ -3133,7 +3133,7 @@ public class TestFromClientSide {
     assertTrue("Expected " + idxs.length + " keys but result contains "
         + result.size(), result.size() == idxs.length);
 
-    Cell [] keys = result.raw();
+    Cell [] keys = result.rawCells();
 
     for(int i=0;i<keys.length;i++) {
       byte [] family = families[idxs[i][0]];
@@ -3166,7 +3166,7 @@ public class TestFromClientSide {
     int expectedResults = end - start + 1;
     assertEquals(expectedResults, result.size());
 
-    Cell[] keys = result.raw();
+    Cell[] keys = result.rawCells();
 
     for (int i=0; i<keys.length; i++) {
       byte [] value = values[end-i];
@@ -3200,7 +3200,7 @@ public class TestFromClientSide {
         equals(row, result.getRow()));
     assertTrue("Expected two keys but result contains " + result.size(),
         result.size() == 2);
-    Cell [] kv = result.raw();
+    Cell [] kv = result.rawCells();
     Cell kvA = kv[0];
     assertTrue("(A) Expected family [" + Bytes.toString(familyA) + "] " +
         "Got family [" + Bytes.toString(CellUtil.getFamilyArray(kvA)) + "]",
@@ -3231,7 +3231,7 @@ public class TestFromClientSide {
         equals(row, result.getRow()));
     assertTrue("Expected a single key but result contains " + result.size(),
         result.size() == 1);
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     assertTrue("Expected family [" + Bytes.toString(family) + "] " +
         "Got family [" + Bytes.toString(CellUtil.getFamilyArray(kv)) + "]",
         equals(family, CellUtil.getFamilyArray(kv)));
@@ -3251,7 +3251,7 @@ public class TestFromClientSide {
         equals(row, result.getRow()));
     assertTrue("Expected a single key but result contains " + result.size(),
         result.size() == 1);
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     assertTrue("Expected family [" + Bytes.toString(family) + "] " +
         "Got family [" + Bytes.toString(CellUtil.getFamilyArray(kv)) + "]",
         equals(family, CellUtil.getFamilyArray(kv)));
@@ -3814,7 +3814,7 @@ public class TestFromClientSide {
     scan.addColumn(CONTENTS_FAMILY, null);
     ResultScanner scanner = table.getScanner(scan);
     for (Result r : scanner) {
-      for(Cell key : r.raw()) {
+      for(Cell key : r.rawCells()) {
         System.out.println(Bytes.toString(r.getRow()) + ": " + key.toString());
       }
     }
@@ -4016,7 +4016,7 @@ public class TestFromClientSide {
       int index = 0;
       Result r = null;
       while ((r = s.next()) != null) {
-        for(Cell key : r.raw()) {
+        for(Cell key : r.rawCells()) {
           times[index++] = key.getTimestamp();
         }
       }
@@ -4050,7 +4050,7 @@ public class TestFromClientSide {
       int index = 0;
       Result r = null;
       while ((r = s.next()) != null) {
-        for(Cell key : r.raw()) {
+        for(Cell key : r.rawCells()) {
           times[index++] = key.getTimestamp();
         }
       }
@@ -4177,7 +4177,7 @@ public class TestFromClientSide {
       for (Result r : s) {
         put = new Put(r.getRow());
         put.setDurability(Durability.SKIP_WAL);
-        for (Cell kv : r.raw()) {
+        for (Cell kv : r.rawCells()) {
           put.add(kv);
         }
         b.put(put);
@@ -4526,7 +4526,7 @@ public class TestFromClientSide {
 
     // Verify expected results
     Result r = ht.get(new Get(ROW));
-    Cell [] kvs = r.raw();
+    Cell [] kvs = r.rawCells();
     assertEquals(5, kvs.length);
     assertIncrementKey(kvs[0], ROW, FAMILY, QUALIFIERS[0], 1);
     assertIncrementKey(kvs[1], ROW, FAMILY, QUALIFIERS[1], 3);
@@ -4542,7 +4542,7 @@ public class TestFromClientSide {
     ht.increment(inc);
     // Verify
     r = ht.get(new Get(ROWS[0]));
-    kvs = r.raw();
+    kvs = r.rawCells();
     assertEquals(QUALIFIERS.length, kvs.length);
     for (int i=0;i<QUALIFIERS.length;i++) {
       assertIncrementKey(kvs[i], ROWS[0], FAMILY, QUALIFIERS[i], i+1);
@@ -4556,7 +4556,7 @@ public class TestFromClientSide {
     ht.increment(inc);
     // Verify
     r = ht.get(new Get(ROWS[0]));
-    kvs = r.raw();
+    kvs = r.rawCells();
     assertEquals(QUALIFIERS.length, kvs.length);
     for (int i=0;i<QUALIFIERS.length;i++) {
       assertIncrementKey(kvs[i], ROWS[0], FAMILY, QUALIFIERS[i], 2*(i+1));
@@ -5199,7 +5199,7 @@ public class TestFromClientSide {
     ResultScanner scanner = table.getScanner(s);
     int count = 0;
     for (Result r : scanner) {
-      assertEquals("Found an unexpected number of results for the row!", versions, r.list().size());
+      assertEquals("Found an unexpected number of results for the row!", versions, r.listCells().size());
       count++;
     }
     assertEquals("Found more than a single row when raw scanning the table with a single row!", 1,
@@ -5213,7 +5213,7 @@ public class TestFromClientSide {
     scanner = table.getScanner(s);
     count = 0;
     for (Result r : scanner) {
-      assertEquals("Found an unexpected number of results for the row!", versions, r.list().size());
+      assertEquals("Found an unexpected number of results for the row!", versions, r.listCells().size());
       count++;
     }
     assertEquals("Found more than a single row when raw scanning the table with a single row!", 1,
@@ -5227,7 +5227,7 @@ public class TestFromClientSide {
     scanner = table.getScanner(s);
     count = 0;
     for (Result r : scanner) {
-      assertEquals("Found an unexpected number of results for the row!", versions, r.list().size());
+      assertEquals("Found an unexpected number of results for the row!", versions, r.listCells().size());
       count++;
     }
     assertEquals("Found more than a single row when raw scanning the table with a single row!", 1,
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideNoCodec.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideNoCodec.java
index d90736a..1a2017a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideNoCodec.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFromClientSideNoCodec.java
@@ -86,7 +86,7 @@ public class TestFromClientSideNoCodec {
     ResultScanner scanner = ht.getScanner(new Scan());
     int count = 0;
     while ((r = scanner.next()) != null) {
-      assertTrue(r.list().size() == 3);
+      assertTrue(r.listCells().size() == 3);
       count++;
     }
     assertTrue(count == 1);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
index 382ab18..78f3563 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultiParallel.java
@@ -184,8 +184,8 @@ public class TestMultiParallel {
     Assert.assertEquals(singleRes.size(), multiRes.length);
     for (int i = 0; i < singleRes.size(); i++) {
       Assert.assertTrue(singleRes.get(i).containsColumn(BYTES_FAMILY, QUALIFIER));
-      Cell[] singleKvs = singleRes.get(i).raw();
-      Cell[] multiKvs = multiRes[i].raw();
+      Cell[] singleKvs = singleRes.get(i).rawCells();
+      Cell[] multiKvs = multiRes[i].rawCells();
       for (int j = 0; j < singleKvs.length; j++) {
         Assert.assertEquals(singleKvs[j], multiKvs[j]);
         Assert.assertEquals(0, Bytes.compareTo(CellUtil.getValueArray(singleKvs[j]), 
@@ -587,7 +587,7 @@ public class TestMultiParallel {
     Result result = (Result)r1;
     Assert.assertTrue(result != null);
     Assert.assertTrue(result.getRow() == null);
-    Assert.assertEquals(0, result.raw().length);
+    Assert.assertEquals(0, result.rawCells().length);
   }
 
   private void validateSizeAndEmpty(Object[] results, int expectedSize) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java
index eebe087..2e45ca5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestMultipleTimestamps.java
@@ -107,11 +107,11 @@ public class TestMultipleTimestamps {
 
     Cell [] kvs;
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(2, kvs.length);
     checkOneCell(kvs[0], FAMILY, 3, 3, 4);
     checkOneCell(kvs[1], FAMILY, 3, 3, 3);
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(2, kvs.length);
     checkOneCell(kvs[0], FAMILY, 5, 3, 4);
     checkOneCell(kvs[1], FAMILY, 5, 3, 3);
@@ -149,10 +149,10 @@ public class TestMultipleTimestamps {
 
     Cell[] kvs;
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(1, kvs.length);
     checkOneCell(kvs[0], FAMILY, 3, 3, 3);
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(1, kvs.length);
     checkOneCell(kvs[0], FAMILY, 5, 3, 3);
 
@@ -198,13 +198,13 @@ public class TestMultipleTimestamps {
 
     // This looks like wrong answer.  Should be 2.  Even then we are returning wrong result,
     // timestamps that are 3 whereas should be 2 since min is inclusive.
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(4, kvs.length);
     checkOneCell(kvs[0], FAMILY, 5, 3, 3);
     checkOneCell(kvs[1], FAMILY, 5, 3, 2);
     checkOneCell(kvs[2], FAMILY, 5, 5, 3);
     checkOneCell(kvs[3], FAMILY, 5, 5, 2);
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(4, kvs.length);
     checkOneCell(kvs[0], FAMILY, 7, 3, 3);
     checkOneCell(kvs[1], FAMILY, 7, 3, 2);
@@ -254,20 +254,20 @@ public class TestMultipleTimestamps {
 
     Cell[] kvs;
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(2, kvs.length);
     checkOneCell(kvs[0], FAMILY, 3, 3, 4);
     checkOneCell(kvs[1], FAMILY, 3, 5, 2);
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(1, kvs.length);
     checkOneCell(kvs[0], FAMILY, 5, 3, 4);
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(1, kvs.length);
     checkOneCell(kvs[0], FAMILY, 6, 3, 4);
 
-    kvs = scanner.next().raw();
+    kvs = scanner.next().rawCells();
     assertEquals(1, kvs.length);
     checkOneCell(kvs[0], FAMILY, 7, 3, 4);
 
@@ -439,7 +439,7 @@ public class TestMultipleTimestamps {
     get.setTimeRange(Collections.min(versions), Collections.max(versions)+1);
     Result result = ht.get(get);
 
-    return result.raw();
+    return result.rawCells();
   }
 
   private  ResultScanner scan(HTable ht, byte[] cf,
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
index f9302ea..1289b4e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestScannersFromClientSide.java
@@ -320,7 +320,7 @@ public class TestScannersFromClientSide {
     ResultScanner scanner = ht.getScanner(scan);
     kvListScan = new ArrayList<Cell>();
     while ((result = scanner.next()) != null) {
-      for (Cell kv : result.list()) {
+      for (Cell kv : result.listCells()) {
         kvListScan.add(kv);
       }
     }
@@ -434,7 +434,7 @@ public class TestScannersFromClientSide {
       return;
 
     int i = 0;
-    for (Cell kv : result.raw()) {
+    for (Cell kv : result.rawCells()) {
       if (i >= expKvList.size()) {
         break;  // we will check the size later
       }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
index aa1da9b..8ffef18 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestTimestampsFilter.java
@@ -149,7 +149,7 @@ public class TestTimestampsFilter {
                                      Arrays.asList(6L, 106L, 306L));
     assertEquals("# of rows returned from scan", 5, results.length);
     for (int rowIdx = 0; rowIdx < 5; rowIdx++) {
-      kvs = results[rowIdx].raw();
+      kvs = results[rowIdx].rawCells();
       // each row should have 5 columns.
       // And we have requested 3 versions for each.
       assertEquals("Number of KeyValues in result for row:" + rowIdx,
@@ -196,15 +196,15 @@ public class TestTimestampsFilter {
     g.addColumn(FAMILY, Bytes.toBytes("column4"));
 
     Result result = ht.get(g);
-    for (Cell kv : result.list()) {
+    for (Cell kv : result.listCells()) {
       System.out.println("found row " + Bytes.toString(CellUtil.getRowArray(kv)) +
           ", column " + Bytes.toString(CellUtil.getQualifierArray(kv)) + ", value "
           + Bytes.toString(CellUtil.getValueArray(kv)));
     }
 
-    assertEquals(result.list().size(), 2);
-    assertTrue(CellUtil.matchingValue(result.list().get(0), Bytes.toBytes("value2-3")));
-    assertTrue(CellUtil.matchingValue(result.list().get(1), Bytes.toBytes("value4-3")));
+    assertEquals(result.listCells().size(), 2);
+    assertTrue(CellUtil.matchingValue(result.listCells().get(0), Bytes.toBytes("value2-3")));
+    assertTrue(CellUtil.matchingValue(result.listCells().get(1), Bytes.toBytes("value4-3")));
 
     ht.close();
   }
@@ -325,7 +325,7 @@ public class TestTimestampsFilter {
     get.setMaxVersions();
     Result result = ht.get(get);
 
-    return result.raw();
+    return result.rawCells();
   }
 
   /**
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
index dbea844..c0ad091 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestOpenTableInCoprocessor.java
@@ -123,7 +123,7 @@ public class TestOpenTableInCoprocessor {
     ResultScanner results = table.getScanner(scan);
     int count = 0;
     for (Result res : results) {
-      count += res.list().size();
+      count += res.listCells().size();
       System.out.println(count + ") " + res);
     }
     results.close();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
index 7e9cc76..22b03cc 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRegionObserverScannerOpenHook.java
@@ -176,7 +176,7 @@ public class TestRegionObserverScannerOpenHook {
     Result r = region.get(get);
     assertNull(
       "Got an unexpected number of rows - no data should be returned with the NoDataFromScan coprocessor. Found: "
-          + r, r.list());
+          + r, r.listCells());
   }
 
   @Test
@@ -201,7 +201,7 @@ public class TestRegionObserverScannerOpenHook {
     Result r = region.get(get);
     assertNull(
       "Got an unexpected number of rows - no data should be returned with the NoDataFromScan coprocessor. Found: "
-          + r, r.list());
+          + r, r.listCells());
   }
 
   /**
@@ -262,13 +262,13 @@ public class TestRegionObserverScannerOpenHook {
     Result r = table.get(get);
     assertNull(
       "Got an unexpected number of rows - no data should be returned with the NoDataFromScan coprocessor. Found: "
-          + r, r.list());
+          + r, r.listCells());
 
     get = new Get(Bytes.toBytes("anotherrow"));
     r = table.get(get);
     assertNull(
       "Got an unexpected number of rows - no data should be returned with the NoDataFromScan coprocessor Found: "
-          + r, r.list());
+          + r, r.listCells());
 
     table.close();
     UTIL.shutdownMiniCluster();
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
index d5af485..fee298a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/TestRowProcessorEndpoint.java
@@ -166,7 +166,7 @@ public class TestRowProcessorEndpoint {
     Set<String> expected =
       new HashSet<String>(Arrays.asList(new String[]{"d", "e", "f", "g"}));
     Get get = new Get(ROW);
-    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).list()));
+    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).listCells()));
     assertEquals(expected, result);
   }
 
@@ -177,7 +177,7 @@ public class TestRowProcessorEndpoint {
     int numThreads = 1000;
     concurrentExec(new IncrementRunner(), numThreads);
     Get get = new Get(ROW);
-    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).list()));
+    LOG.debug("row keyvalues:" + stringifyKvs(table.get(get).listCells()));
     int finalCounter = incrementCounter(table);
     assertEquals(numThreads + 1, finalCounter);
     assertEquals(0, failures.get());
@@ -238,11 +238,11 @@ public class TestRowProcessorEndpoint {
     int numThreads = 1000;
     concurrentExec(new SwapRowsRunner(), numThreads);
     LOG.debug("row keyvalues:" +
-              stringifyKvs(table.get(new Get(ROW)).list()));
+              stringifyKvs(table.get(new Get(ROW)).listCells()));
     LOG.debug("row2 keyvalues:" +
-              stringifyKvs(table.get(new Get(ROW2)).list()));
-    assertEquals(rowSize, table.get(new Get(ROW)).list().size());
-    assertEquals(row2Size, table.get(new Get(ROW2)).list().size());
+              stringifyKvs(table.get(new Get(ROW2)).listCells()));
+    assertEquals(rowSize, table.get(new Get(ROW)).listCells().size());
+    assertEquals(row2Size, table.get(new Get(ROW2)).listCells().size());
     assertEquals(0, failures.get());
   }
 
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
index 6ec9469..20135dd 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestColumnRangeFilter.java
@@ -219,7 +219,7 @@ public class TestColumnRangeFilter {
 
       Result result;
       while ((result = scanner.next()) != null) {
-        for (Cell kv : result.list()) {
+        for (Cell kv : result.listCells()) {
           results.add(kv);
         }
       }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
index d65a2f5..a35d5c5 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWithScanLimits.java
@@ -93,7 +93,7 @@ public class TestFilterWithScanLimits {
       // row2 => <f1:c5, 2_c5>
 
       for (Result result : scanner) {
-        for (Cell kv : result.list()) {
+        for (Cell kv : result.listCells()) {
           kv_number++;
           LOG.debug(kv_number + ". kv: " + kv);
         }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
index 0d38593..097244e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFilterWrapper.java
@@ -89,7 +89,7 @@ public class TestFilterWrapper {
       // row2 (c1-c4) and row3(c1-c4) are returned
       for (Result result : scanner) {
         row_number++;
-        for (Cell kv : result.list()) {
+        for (Cell kv : result.listCells()) {
           LOG.debug(kv_number + ". kv: " + kv);
           kv_number++;
           assertEquals("Returned row is not correct", new String(CellUtil.getRowArray(kv)),
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
index f4aac94..a38e2bf 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/filter/TestFuzzyRowAndColumnRangeFilter.java
@@ -163,7 +163,7 @@ public class TestFuzzyRowAndColumnRangeFilter {
     Result result;
     long timeBeforeScan = System.currentTimeMillis();
     while ((result = scanner.next()) != null) {
-      for (Cell kv : result.list()) {
+      for (Cell kv : result.listCells()) {
         LOG.info("Got rk: " + Bytes.toStringBinary(CellUtil.getRowArray(kv)) + " cq: "
                 + Bytes.toStringBinary(CellUtil.getQualifierArray(kv)));
         results.add(kv);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
index bfb8453..31ce15a 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapred/TestTableMapReduce.java
@@ -213,7 +213,7 @@ public class TestTableMapReduce {
         byte[] firstValue = null;
         byte[] secondValue = null;
         int count = 0;
-         for(Cell kv : r.list()) {
+         for(Cell kv : r.listCells()) {
           if (count == 0) {
             firstValue = CellUtil.getValueArray(kv);
           }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java
index 8183c4e..6163bb9 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestCopyTable.java
@@ -106,7 +106,7 @@ public class TestCopyTable {
       Get g = new Get(Bytes.toBytes("row" + i));
       Result r = t2.get(g);
       assertEquals(1, r.size());
-      assertTrue(CellUtil.matchingQualifier(r.raw()[0], COLUMN1));
+      assertTrue(CellUtil.matchingQualifier(r.rawCells()[0], COLUMN1));
     }
     
     t1.close();
@@ -150,7 +150,7 @@ public class TestCopyTable {
     Get g = new Get(ROW1);
     Result r = t2.get(g);
     assertEquals(1, r.size());
-    assertTrue(CellUtil.matchingQualifier(r.raw()[0], COLUMN1));
+    assertTrue(CellUtil.matchingQualifier(r.rawCells()[0], COLUMN1));
 
     g = new Get(ROW0);
     r = t2.get(g);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
index e0aac68..c359137 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestGroupingTableMapper.java
@@ -56,7 +56,7 @@ public class TestGroupingTableMapper {
         .toBytes("value1")));
     keyValue.add(new KeyValue(row, Bytes.toBytes("family1"), Bytes.toBytes("clm"), Bytes
         .toBytes("value2")));
-    when(result.list()).thenReturn(keyValue);
+    when(result.listCells()).thenReturn(keyValue);
     mapper.map(null, result, context);
     // template data
     byte[][] data = { Bytes.toBytes("value1"), Bytes.toBytes("value2") };
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
index 06e5793..0e01a5e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestHFileOutputFormat.java
@@ -427,9 +427,9 @@ public class TestHFileOutputFormat  {
       Scan scan = new Scan();
       ResultScanner results = table.getScanner(scan);
       for (Result res : results) {
-        assertEquals(FAMILIES.length, res.raw().length);
-        Cell first = res.raw()[0];
-        for (Cell kv : res.raw()) {
+        assertEquals(FAMILIES.length, res.rawCells().length);
+        Cell first = res.rawCells()[0];
+        for (Cell kv : res.rawCells()) {
           assertTrue(CellUtil.matchingRow(first, kv));
           assertTrue(Bytes.equals(CellUtil.getValueArray(first), CellUtil.getValueArray(kv)));
         }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
index 9bab8b9..21b6655 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportExport.java
@@ -286,7 +286,7 @@ public class TestImportExport {
     s.setRaw(true);
     ResultScanner scanner = t.getScanner(s);
     Result r = scanner.next();
-    Cell[] res = r.raw();
+    Cell[] res = r.rawCells();
     assertTrue(CellUtil.isDeleteFamily(res[0]));
     assertEquals(now+4, res[1].getTimestamp());
     assertEquals(now+3, res[2].getTimestamp());
@@ -467,7 +467,7 @@ public class TestImportExport {
             Bytes.toBytes("value")),
         new KeyValue(Bytes.toBytes("row"), Bytes.toBytes("family"), Bytes.toBytes("qualifier"),
             Bytes.toBytes("value1")) };
-    when(value.raw()).thenReturn(keys);
+    when(value.rawCells()).thenReturn(keys);
     importer.map(new ImmutableBytesWritable(Bytes.toBytes("Key")), value, ctx);
 
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
index 15cdbb3..e8d2ac9 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestImportTsv.java
@@ -316,7 +316,7 @@ public class TestImportTsv implements Configurable {
         ResultScanner resScanner = table.getScanner(scan);
         for (Result res : resScanner) {
           assertTrue(res.size() == 2);
-          List<Cell> kvs = res.list();
+          List<Cell> kvs = res.listCells();
           assertTrue(CellUtil.matchingRow(kvs.get(0), Bytes.toBytes("KEY")));
           assertTrue(CellUtil.matchingRow(kvs.get(1), Bytes.toBytes("KEY")));
           assertTrue(CellUtil.matchingValue(kvs.get(0), Bytes.toBytes("VALUE" + valueMultiplier)));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java
index 003dd87..72e5d7e 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultithreadedTableMapper.java
@@ -213,7 +213,7 @@ public class TestMultithreadedTableMapper {
         byte[] firstValue = null;
         byte[] secondValue = null;
         int count = 0;
-        for(Cell kv : r.list()) {
+        for(Cell kv : r.listCells()) {
           if (count == 0) {
             firstValue = CellUtil.getValueArray(kv);
           }else if (count == 1) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
index 1be08b6..b47844b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java
@@ -226,7 +226,7 @@ public class TestTableMapReduce {
         byte[] firstValue = null;
         byte[] secondValue = null;
         int count = 0;
-        for(Cell kv : r.list()) {
+        for(Cell kv : r.listCells()) {
           if (count == 0) {
             firstValue = CellUtil.getValueArray(kv);
           }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
index f3b5ba4..ee5b746 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestTimeRangeMapRed.java
@@ -110,7 +110,7 @@ public class TestTimeRangeMapRed {
         Context context)
     throws IOException {
       List<Long> tsList = new ArrayList<Long>();
-      for (Cell kv : result.list()) {
+      for (Cell kv : result.listCells()) {
         tsList.add(kv.getTimestamp());
       }
 
@@ -196,7 +196,7 @@ public class TestTimeRangeMapRed {
     scan.setMaxVersions(1);
     ResultScanner scanner = table.getScanner(scan);
     for (Result r: scanner) {
-      for (Cell kv : r.list()) {
+      for (Cell kv : r.listCells()) {
         log.debug(Bytes.toString(r.getRow()) + "\t" + Bytes.toString(CellUtil.getFamilyArray(kv))
             + "\t" + Bytes.toString(CellUtil.getQualifierArray(kv))
             + "\t" + kv.getTimestamp() + "\t" + Bytes.toBoolean(CellUtil.getValueArray(kv)));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
index 5960df0..64f9874 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestWALPlayer.java
@@ -124,7 +124,7 @@ public class TestWALPlayer {
     Get g = new Get(ROW);
     Result r = t2.get(g);
     assertEquals(1, r.size());
-    assertTrue(CellUtil.matchingQualifier(r.raw()[0], COLUMN2));
+    assertTrue(CellUtil.matchingQualifier(r.rawCells()[0], COLUMN2));
   }
 
   /**
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
index 815268f..8098096 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
@@ -174,7 +174,7 @@ public class TestAtomicOperation {
     Result result = region.get(get);
     assertEquals(1, result.size());
 
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     long r = Bytes.toLong(CellUtil.getValueArray(kv));
     assertEquals(amount, r);
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
index 0aa28dd..58c4ba6 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestBlocksRead.java
@@ -160,7 +160,7 @@ public class TestBlocksRead extends HBaseTestCase {
         get.addColumn(cf, Bytes.toBytes(column));
       }
 
-      kvs = region.get(get).raw();
+      kvs = region.get(get).rawCells();
       long blocksEnd = getBlkAccessCount(cf);
       if (expBlocks[i] != -1) {
         assertEquals("Blocks Read Check for Bloom: " + bloomType, expBlocks[i],
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 22c2dcf..05840a2 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -1066,7 +1066,7 @@ public class TestHRegion extends HBaseTestCase {
 
       Get get = new Get(row1);
       get.addColumn(fam2, qf1);
-      Cell [] actual = region.get(get).raw();
+      Cell [] actual = region.get(get).rawCells();
 
       Cell [] expected = {kv};
 
@@ -1386,7 +1386,7 @@ public class TestHRegion extends HBaseTestCase {
       Get get = new Get(row).addColumn(fam, qual);
       Result result = region.get(get);
       assertEquals(1, result.size());
-      Cell kv = result.raw()[0];
+      Cell kv = result.rawCells()[0];
       LOG.info("Got: " + kv);
       assertTrue("LATEST_TIMESTAMP was not replaced with real timestamp",
           kv.getTimestamp() != HConstants.LATEST_TIMESTAMP);
@@ -1402,7 +1402,7 @@ public class TestHRegion extends HBaseTestCase {
       get = new Get(row).addColumn(fam, qual);
       result = region.get(get);
       assertEquals(1, result.size());
-      kv = result.raw()[0];
+      kv = result.rawCells()[0];
       LOG.info("Got: " + kv);
       assertTrue("LATEST_TIMESTAMP was not replaced with real timestamp",
           kv.getTimestamp() != HConstants.LATEST_TIMESTAMP);
@@ -1656,9 +1656,9 @@ public class TestHRegion extends HBaseTestCase {
       Result res = region.get(get);
       assertEquals(expected.length, res.size());
       for(int i=0; i<res.size(); i++){
-        assertTrue(CellUtil.matchingRow(expected[i], res.raw()[i]));
-        assertTrue(CellUtil.matchingFamily(expected[i], res.raw()[i]));
-        assertTrue(CellUtil.matchingQualifier(expected[i], res.raw()[i]));
+        assertTrue(CellUtil.matchingRow(expected[i], res.rawCells()[i]));
+        assertTrue(CellUtil.matchingFamily(expected[i], res.rawCells()[i]));
+        assertTrue(CellUtil.matchingQualifier(expected[i], res.rawCells()[i]));
       }
 
       // Test using a filter on a Get
@@ -2366,7 +2366,7 @@ public class TestHRegion extends HBaseTestCase {
     Result result = region.get(get);
     assertEquals(1, result.size());
 
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     long r = Bytes.toLong(CellUtil.getValueArray(kv));
     assertEquals(amount, r);
   }
@@ -2381,7 +2381,7 @@ public class TestHRegion extends HBaseTestCase {
     Result result = region.get(get);
     assertEquals(1, result.size());
 
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     int r = Bytes.toInt(CellUtil.getValueArray(kv));
     assertEquals(amount, r);
   }
@@ -3117,7 +3117,7 @@ public class TestHRegion extends HBaseTestCase {
           // TODO this was removed, now what dangit?!
           // search looking for the qualifier in question?
           long timestamp = 0;
-          for (Cell kv : result.raw()) {
+          for (Cell kv : result.rawCells()) {
             if (CellUtil.matchingFamily(kv, families[0])
                 && CellUtil.matchingQualifier(kv, qualifiers[0])) {
               timestamp = kv.getTimestamp();
@@ -3127,7 +3127,7 @@ public class TestHRegion extends HBaseTestCase {
           prevTimestamp = timestamp;
           Cell previousKV = null;
 
-          for (Cell kv : result.raw()) {
+          for (Cell kv : result.rawCells()) {
             byte[] thisValue = CellUtil.getValueArray(kv);
             if (previousKV != null) {
               if (Bytes.compareTo(CellUtil.getValueArray(previousKV), thisValue) != 0) {
@@ -3324,7 +3324,7 @@ public class TestHRegion extends HBaseTestCase {
       //Get rows
       Get get = new Get(row);
       get.setMaxVersions();
-      Cell[] kvs = region.get(get).raw();
+      Cell[] kvs = region.get(get).rawCells();
 
       //Check if rows are correct
       assertEquals(4, kvs.length);
@@ -3375,7 +3375,7 @@ public class TestHRegion extends HBaseTestCase {
       Get get = new Get(row);
       get.addColumn(familyName, col);
 
-      Cell[] keyValues = region.get(get).raw();
+      Cell[] keyValues = region.get(get).rawCells();
       assertTrue(keyValues.length == 0);
     } finally {
       HRegion.closeHRegion(this.region);
@@ -3899,7 +3899,7 @@ public class TestHRegion extends HBaseTestCase {
         get.addColumn(family, qf);
       }
       Result result = newReg.get(get);
-      Cell [] raw = result.raw();
+      Cell [] raw = result.rawCells();
       assertEquals(families.length, result.size());
       for(int j=0; j<families.length; j++) {
         assertTrue(CellUtil.matchingRow(raw[j], row));
@@ -3913,7 +3913,7 @@ public class TestHRegion extends HBaseTestCase {
   throws IOException {
     // Now I have k, get values out and assert they are as expected.
     Get get = new Get(k).addFamily(family).setMaxVersions();
-    Cell [] results = r.get(get).raw();
+    Cell [] results = r.get(get).rawCells();
     for (int j = 0; j < results.length; j++) {
       byte [] tmp = CellUtil.getValueArray(results[j]);
       // Row should be equal to value every time.
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
index 7551bb6..7f0c145 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestParallelPut.java
@@ -140,7 +140,7 @@ public class TestParallelPut extends HBaseTestCase {
     Result result = region.get(get);
     assertEquals(1, result.size());
 
-    Cell kv = result.raw()[0];
+    Cell kv = result.rawCells()[0];
     byte[] r = CellUtil.getValueArray(kv);
     assertTrue(Bytes.compareTo(r, value) == 0);
   }
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
index 2da1cb3..98e165b 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestResettingCounters.java
@@ -91,7 +91,7 @@ public class TestResettingCounters {
       // increment all qualifiers, should have value=6 for all
       Result result = region.increment(all);
       assertEquals(numQualifiers, result.size());
-      Cell [] kvs = result.raw();
+      Cell [] kvs = result.rawCells();
       for (int i=0;i<kvs.length;i++) {
         System.out.println(kvs[i].toString());
         assertTrue(CellUtil.matchingQualifier(kvs[i], qualifiers[i]));
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
index f410f5b..f84b526 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/replication/TestReplicationSmallTests.java
@@ -140,9 +140,9 @@ public class TestReplicationSmallTests extends TestReplicationBase {
         LOG.info("Rows not available");
         Thread.sleep(SLEEP_TIME);
       } else {
-        assertArrayEquals(CellUtil.getValueArray(res.raw()[0]), v3);
-        assertArrayEquals(CellUtil.getValueArray(res.raw()[1]), v2);
-        assertArrayEquals(CellUtil.getValueArray(res.raw()[2]), v1);
+        assertArrayEquals(CellUtil.getValueArray(res.rawCells()[0]), v3);
+        assertArrayEquals(CellUtil.getValueArray(res.rawCells()[1]), v2);
+        assertArrayEquals(CellUtil.getValueArray(res.rawCells()[2]), v1);
         break;
       }
     }
@@ -162,8 +162,8 @@ public class TestReplicationSmallTests extends TestReplicationBase {
         LOG.info("Version not deleted");
         Thread.sleep(SLEEP_TIME);
       } else {
-        assertArrayEquals(CellUtil.getValueArray(res.raw()[0]), v3);
-        assertArrayEquals(CellUtil.getValueArray(res.raw()[1]), v2);
+        assertArrayEquals(CellUtil.getValueArray(res.rawCells()[0]), v3);
+        assertArrayEquals(CellUtil.getValueArray(res.rawCells()[1]), v2);
         break;
       }
     }
@@ -462,7 +462,7 @@ public class TestReplicationSmallTests extends TestReplicationBase {
     Put put = null;
     for (Result result : rs) {
       put = new Put(result.getRow());
-      Cell firstVal = result.raw()[0];
+      Cell firstVal = result.rawCells()[0];
       put.add(CellUtil.getFamilyArray(firstVal),
           CellUtil.getQualifierArray(firstVal), Bytes.toBytes("diff data"));
       htable2.put(put);
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java hbase-server/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
index b5972c0..a4cb847 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/rest/client/TestRemoteTable.java
@@ -209,7 +209,7 @@ public class TestRemoteTable {
     get.setMaxVersions(2);
     result = remoteTable.get(get);
     int count = 0;
-    for (Cell kv: result.list()) {
+    for (Cell kv: result.listCells()) {
       if (CellUtil.matchingFamily(kv, COLUMN_1) && TS_1 == kv.getTimestamp()) {
         assertTrue(CellUtil.matchingValue(kv, VALUE_1)); // @TS_1
         count++;
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
index 2df127e..fab0f2d 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/util/TestMergeTool.java
@@ -243,7 +243,7 @@ public class TestMergeTool extends HBaseTestCase {
         get.addFamily(FAMILY);
         Result result = merged.get(get);
         assertEquals(1, result.size());
-        byte [] bytes = CellUtil.getValueArray(result.raw()[0]);
+        byte [] bytes = CellUtil.getValueArray(result.rawCells()[0]);
         assertNotNull(Bytes.toStringBinary(rows[i][j]), bytes);
         assertTrue(Bytes.equals(bytes, rows[i][j]));
       }
@@ -262,7 +262,7 @@ public class TestMergeTool extends HBaseTestCase {
         Get get = new Get(rows[i][j]);
         get.addFamily(FAMILY);
         Result result = regions[i].get(get);
-        byte [] bytes =  CellUtil.getValueArray(result.raw()[0]);
+        byte [] bytes =  CellUtil.getValueArray(result.rawCells()[0]);
         assertNotNull(bytes);
         assertTrue(Bytes.equals(bytes, rows[i][j]));
       }
