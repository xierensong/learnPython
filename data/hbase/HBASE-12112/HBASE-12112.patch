diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
index 09af2b7..8fc3d81 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java
@@ -27,11 +27,11 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.DoNotRetryIOException;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.UnknownScannerException;
@@ -452,9 +452,8 @@ public class ClientScanner extends AbstractClientScanner {
           if (values != null && values.length > 0) {
             for (Result rs : values) {
               cache.add(rs);
-              for (Cell kv : rs.rawCells()) {
-                // TODO make method in Cell or CellUtil
-                remainingResultSize -= KeyValueUtil.ensureKeyValue(kv).heapSize();
+              for (Cell cell : rs.rawCells()) {
+                remainingResultSize -= CellUtil.estimatedHeapSizeOf(cell);
               }
               countdown--;
               this.lastResult = rs;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
index a1df4a4..ca89a3f 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallReversedScanner.java
@@ -23,11 +23,10 @@ package org.apache.hadoop.hbase.client;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
-import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.ipc.RpcControllerFactory;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -160,8 +159,8 @@ public class ClientSmallReversedScanner extends ReversedClientScanner {
               continue;
             }
             cache.add(rs);
-            for (Cell kv : rs.rawCells()) {
-              remainingResultSize -= KeyValueUtil.ensureKeyValue(kv).heapSize();
+            for (Cell cell : rs.rawCells()) {
+              remainingResultSize -= CellUtil.estimatedHeapSizeOf(cell);
             }
             countdown--;
             this.lastResult = rs;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
index 3e4ce69..9de809e 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/ClientSmallScanner.java
@@ -27,8 +27,8 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.metrics.ScanMetrics;
 import org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController;
@@ -225,8 +225,8 @@ public class ClientSmallScanner extends ClientScanner {
               continue;
             }
             cache.add(rs);
-            for (Cell kv : rs.rawCells()) {
-              remainingResultSize -= KeyValueUtil.ensureKeyValue(kv).heapSize();
+            for (Cell cell : rs.rawCells()) {
+              remainingResultSize -= CellUtil.estimatedHeapSizeOf(cell);
             }
             countdown--;
             this.lastResult = rs;
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
index 54e366b..3abe6a8 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Increment.java
@@ -30,7 +30,6 @@ import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.io.TimeRange;
 import org.apache.hadoop.hbase.security.access.Permission;
 import org.apache.hadoop.hbase.security.visibility.CellVisibility;
@@ -240,9 +239,8 @@ public class Increment extends Mutation implements Comparable<Row> {
           } else {
             moreThanOneB = true;
           }
-          KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
-          sb.append(Bytes.toStringBinary(kv.getKey()) + "+=" +
-              Bytes.toLong(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength()));
+          sb.append(Bytes.toStringBinary(CellUtil.getFlatKey(cell)) + "+=" +
+              Bytes.toLong(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
         }
         sb.append("}");
       }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
index 7cdc1d1..1ebff93 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Mutation.java
@@ -192,13 +192,8 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
         if (--maxCols <= 0 ) {
           continue;
         }
-        // KeyValue v1 expectation.  Cast for now until we go all Cell all the time.
-        KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
-        Map<String, Object> kvMap = kv.toStringMap();
-        // row and family information are already available in the bigger map
-        kvMap.remove("row");
-        kvMap.remove("family");
-        qualifierDetails.add(kvMap);
+        Map<String, Object> cellMap = cellToStringMap(cell);
+        qualifierDetails.add(cellMap);
       }
     }
     map.put("totalColumns", colCount);
@@ -209,6 +204,23 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
     return map;
   }
 
+  private static Map<String, Object> cellToStringMap(Cell c) {
+    Map<String, Object> stringMap = new HashMap<String, Object>();
+    stringMap.put("qualifier", Bytes.toStringBinary(c.getQualifierArray(), c.getQualifierOffset(),
+                c.getQualifierLength()));
+    stringMap.put("timestamp", c.getTimestamp());
+    stringMap.put("vlen", c.getValueLength());
+    List<Tag> tags = Tag.asList(c.getTagsArray(), c.getTagsOffset(), c.getTagsLength());
+    if (tags != null) {
+      List<String> tagsString = new ArrayList<String>();
+      for (Tag t : tags) {
+        tagsString.add((t.getType()) + ":" + Bytes.toStringBinary(t.getValue()));
+      }
+      stringMap.put("tag", tagsString);
+    }
+    return stringMap;
+  }
+
   /**
    * @deprecated Use {@link #getDurability()} instead.
    * @return true if edits should be applied to WAL, false if not
@@ -423,8 +435,7 @@ public abstract class Mutation extends OperationWithAttributes implements Row, C
           size * ClassSize.REFERENCE);
 
       for(Cell cell : entry.getValue()) {
-        KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
-        heapsize += kv.heapSize();
+        heapsize += CellUtil.estimatedHeapSizeOf(cell);
       }
     }
     heapsize += getAttributeSize();
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
index a041b91..09c58cf 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Result.java
@@ -743,7 +743,7 @@ public class Result implements CellScannable, CellScanner {
   public static long getTotalSizeOfCells(Result result) {
     long size = 0;
     for (Cell c : result.rawCells()) {
-      size += KeyValueUtil.ensureKeyValue(c).heapSize();
+      size += CellUtil.estimatedHeapSizeOf(c);
     }
     return size;
   }
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
index e945d48..752042c 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java
@@ -25,8 +25,7 @@ import java.util.ArrayList;
 import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.Cell;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.exceptions.DeserializationException;
 import org.apache.hadoop.hbase.protobuf.generated.FilterProtos;
 
@@ -49,15 +48,8 @@ public class KeyOnlyFilter extends FilterBase {
   public KeyOnlyFilter(boolean lenAsVal) { this.lenAsVal = lenAsVal; }
 
   @Override
-  public Cell transformCell(Cell kv) {
-    // TODO Move to KeyValueUtil
-
-    // TODO make matching Column a cell method or CellUtil method.
-    // Even if we want to make use of KeyValue.KeyOnlyKeyValue we need to convert
-    // the cell to KV so that we can make use of kv.getKey() to form the key part
-    KeyValue v = KeyValueUtil.ensureKeyValue(kv);
-
-    return v.createKeyOnly(this.lenAsVal);
+  public Cell transformCell(Cell cell) {
+    return CellUtil.createKeyOnlyCell(cell, this.lenAsVal);
   }
 
   @Override
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
index 03cfe4d..ec2b144 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/CellUtil.java
@@ -604,6 +604,16 @@ public final class CellUtil {
   }
 
   /**
+   * This involves bytes copy. Do not use unless you have to.
+   * @return Copy of cell's key portion as it would have serialized in a KeyValue.
+   */
+  public static byte[] getFlatKey(Cell cell) {
+    byte[] key = new byte[KeyValueUtil.keyLength(cell)];
+    KeyValueUtil.appendKeyTo(cell, key, 0);
+    return key;
+  }
+
+  /**
    * Write rowkey excluding the common part.
    * @param cell
    * @param rLen
@@ -704,4 +714,25 @@ public final class CellUtil {
     }
     return commonPrefix;
   }
+
+  /**
+   * @param c
+   * @param lenAsVal
+   * @return
+   */
+  public static Cell createKeyOnlyCell(Cell c, boolean lenAsVal) {
+    // KV format: <keylen:4><valuelen:4><key:keylen><value:valuelen>
+    // Rebuild as: <keylen:4><0:4><key:keylen>
+    int dataLen = lenAsVal ? Bytes.SIZEOF_INT : 0;
+    int keyOffset = (2 * Bytes.SIZEOF_INT);
+    int keyLen = KeyValueUtil.keyLength(c);
+    byte[] newBuffer = new byte[keyLen + keyOffset + dataLen];
+    Bytes.putInt(newBuffer, 0, keyLen);
+    Bytes.putInt(newBuffer, Bytes.SIZEOF_INT, dataLen);
+    KeyValueUtil.appendKeyTo(c, newBuffer, keyOffset);
+    if (lenAsVal) {
+      Bytes.putInt(newBuffer, newBuffer.length - dataLen, c.getValueLength());
+    }
+    return new KeyValue(newBuffer);
+  }
 }
\ No newline at end of file
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
index 58ebcad..a4b3857 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.java
@@ -32,12 +32,14 @@ import org.apache.hadoop.hbase.KeyValue.KVComparator;
 import org.apache.hadoop.hbase.KeyValue.SamePrefixComparator;
 import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.KeyValueUtil;
+import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.io.TagCompressionContext;
 import org.apache.hadoop.hbase.io.hfile.BlockType;
 import org.apache.hadoop.hbase.io.hfile.HFileContext;
 import org.apache.hadoop.hbase.io.util.LRUDictionary;
 import org.apache.hadoop.hbase.util.ByteBufferUtils;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.hadoop.io.WritableUtils;
 
 /**
@@ -326,7 +328,10 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
   // there. So this has to be an instance of SettableSequenceId. SeekerState need not be
   // SettableSequenceId as we never return that to top layers. When we have to, we make
   // ClonedSeekerState from it.
-  protected static class ClonedSeekerState implements Cell, SettableSequenceId {
+  protected static class ClonedSeekerState implements Cell, HeapSize, SettableSequenceId {
+    private static final long FIXED_OVERHEAD = ClassSize.align(ClassSize.OBJECT
+        + (4 * ClassSize.REFERENCE) + (2 * Bytes.SIZEOF_LONG) + (7 * Bytes.SIZEOF_INT)
+        + (Bytes.SIZEOF_SHORT) + (2 * Bytes.SIZEOF_BYTE));
     private byte[] keyOnlyBuffer;
     private ByteBuffer currentBuffer;
     private short rowLength;
@@ -507,6 +512,12 @@ abstract class BufferedDataBlockEncoder implements DataBlockEncoder {
     public void setSequenceId(long seqId) {
       this.seqId = seqId;
     }
+
+    @Override
+    public long heapSize() {
+      return FIXED_OVERHEAD + rowLength + familyLength + qualifierLength + valueLength + tagsLength
+          + KeyValue.TIMESTAMP_TYPE_SIZE;
+    }
   }
 
   protected abstract static class
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
index 148b9e3..8e806d3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java
@@ -45,13 +45,14 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValueUtil;
-import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.Tag;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.hfile.HFile.FileInfo;
@@ -307,11 +308,12 @@ public class HFilePrettyPrinter extends Configured implements Tool {
 
   private void scanKeysValues(Path file, KeyValueStatsCollector fileStats,
       HFileScanner scanner,  byte[] row) throws IOException {
-    KeyValue pkv = null;
+    Cell pCell = null;
     do {
-      KeyValue kv = KeyValueUtil.ensureKeyValue(scanner.getKeyValue());
+      Cell cell = scanner.getKeyValue();
       if (row != null && row.length != 0) {
-        int result = Bytes.compareTo(kv.getRow(), row);
+        int result = Bytes.compareTo(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength(),
+            row, 0, row.length);
         if (result > 0) {
           break;
         } else if (result < 0) {
@@ -320,48 +322,55 @@ public class HFilePrettyPrinter extends Configured implements Tool {
       }
       // collect stats
       if (printStats) {
-        fileStats.collect(kv);
+        fileStats.collect(cell);
       }
       // dump key value
       if (printKey) {
-        System.out.print("K: " + kv);
+        System.out.print("K: " + cell);
         if (printValue) {
-          System.out.print(" V: " + Bytes.toStringBinary(kv.getValue()));
+          System.out.print(" V: "
+              + Bytes.toStringBinary(cell.getValueArray(), cell.getValueOffset(),
+                  cell.getValueLength()));
           int i = 0;
-          List<Tag> tags = kv.getTags();
+          List<Tag> tags = Tag.asList(cell.getTagsArray(), cell.getTagsOffset(),
+              cell.getTagsLength());
           for (Tag tag : tags) {
-            System.out
-                .print(String.format(" T[%d]: %s", i++, Bytes.toStringBinary(tag.getValue())));
+            System.out.print(String.format(" T[%d]: %s", i++,
+                Bytes.toStringBinary(tag.getBuffer(), tag.getTagOffset(), tag.getTagLength())));
           }
         }
         System.out.println();
       }
       // check if rows are in order
-      if (checkRow && pkv != null) {
-        if (Bytes.compareTo(pkv.getRow(), kv.getRow()) > 0) {
+      if (checkRow && pCell != null) {
+        if (Bytes.compareTo(pCell.getRowArray(), pCell.getRowOffset(), pCell.getRowLength(),
+            cell.getRowArray(), cell.getRowOffset(), cell.getRowLength()) > 0) {
           System.err.println("WARNING, previous row is greater then"
               + " current row\n\tfilename -> " + file + "\n\tprevious -> "
-              + Bytes.toStringBinary(pkv.getKey()) + "\n\tcurrent  -> "
-              + Bytes.toStringBinary(kv.getKey()));
+              + Bytes.toStringBinary(CellUtil.getFlatKey(pCell)) + "\n\tcurrent  -> "
+              + Bytes.toStringBinary(CellUtil.getFlatKey(cell)));
         }
       }
       // check if families are consistent
       if (checkFamily) {
-        String fam = Bytes.toString(kv.getFamily());
+        String fam = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(),
+            cell.getFamilyLength());
         if (!file.toString().contains(fam)) {
           System.err.println("WARNING, filename does not match kv family,"
               + "\n\tfilename -> " + file + "\n\tkeyvalue -> "
-              + Bytes.toStringBinary(kv.getKey()));
+              + Bytes.toStringBinary(CellUtil.getFlatKey(cell)));
         }
-        if (pkv != null
-            && !Bytes.equals(pkv.getFamily(), kv.getFamily())) {
+        if (pCell != null
+            && !Bytes.equals(pCell.getFamilyArray(), pCell.getFamilyOffset(),
+                pCell.getFamilyLength(), cell.getFamilyArray(), cell.getFamilyOffset(),
+                cell.getFamilyLength())) {
           System.err.println("WARNING, previous kv has different family"
               + " compared to current key\n\tfilename -> " + file
-              + "\n\tprevious -> " + Bytes.toStringBinary(pkv.getKey())
-              + "\n\tcurrent  -> " + Bytes.toStringBinary(kv.getKey()));
+              + "\n\tprevious -> " + Bytes.toStringBinary(CellUtil.getFlatKey(pCell))
+              + "\n\tcurrent  -> " + Bytes.toStringBinary(CellUtil.getFlatKey(cell)));
         }
       }
-      pkv = kv;
+      pCell = cell;
       ++count;
     } while (scanner.next());
   }
@@ -451,21 +460,21 @@ public class HFilePrettyPrinter extends Configured implements Tool {
 
     byte[] biggestRow = null;
 
-    private KeyValue prevKV = null;
+    private Cell prevCell = null;
     private long maxRowBytes = 0;
     private long curRowKeyLength;
 
-    public void collect(KeyValue kv) {
-      valLen.update(kv.getValueLength());
-      if (prevKV != null &&
-          KeyValue.COMPARATOR.compareRows(prevKV, kv) != 0) {
+    public void collect(Cell cell) {
+      valLen.update(cell.getValueLength());
+      if (prevCell != null &&
+          KeyValue.COMPARATOR.compareRows(prevCell, cell) != 0) {
         // new row
         collectRow();
       }
-      curRowBytes += kv.getLength();
-      curRowKeyLength = kv.getKeyLength();
+      curRowBytes += KeyValueUtil.length(cell);
+      curRowKeyLength = KeyValueUtil.keyLength(cell);
       curRowCols++;
-      prevKV = kv;
+      prevCell = cell;
     }
 
     private void collectRow() {
@@ -473,8 +482,8 @@ public class HFilePrettyPrinter extends Configured implements Tool {
       rowSizeCols.update(curRowCols);
       keyLen.update(curRowKeyLength);
 
-      if (curRowBytes > maxRowBytes && prevKV != null) {
-        biggestRow = prevKV.getRow();
+      if (curRowBytes > maxRowBytes && prevCell != null) {
+        biggestRow = prevCell.getRow();
         maxRowBytes = curRowBytes;
       }
 
@@ -490,7 +499,7 @@ public class HFilePrettyPrinter extends Configured implements Tool {
 
     @Override
     public String toString() {
-      if (prevKV == null)
+      if (prevCell == null)
         return "no data available for statistics";
 
       // Dump the metrics to the output stream
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
index c243b1d..0045ffb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java
@@ -29,10 +29,10 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.KVComparator;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.encoding.DataBlockEncoder;
@@ -1021,10 +1021,9 @@ public class HFileReaderV2 extends AbstractHFileReader {
         if (comp == 0) {
           if (seekBefore) {
             if (lastKeyValueSize < 0) {
-              KeyValue kv = KeyValueUtil.ensureKeyValue(key);
               throw new IllegalStateException("blockSeek with seekBefore "
                   + "at the first key of the block: key="
-                  + Bytes.toStringBinary(kv.getKey(), kv.getKeyOffset(), kv.getKeyLength())
+                  + Bytes.toStringBinary(CellUtil.getFlatKey(key))
                   + ", blockOffset=" + block.getOffset() + ", onDiskSize="
                   + block.getOnDiskSizeWithHeader());
             }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
index 0d5ced9..6273ce9 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV3.java
@@ -27,9 +27,9 @@ import org.apache.hadoop.hbase.classification.InterfaceAudience;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.FSDataInputStreamWrapper;
 import org.apache.hadoop.hbase.io.crypto.Cipher;
@@ -295,10 +295,9 @@ public class HFileReaderV3 extends HFileReaderV2 {
         if (comp == 0) {
           if (seekBefore) {
             if (lastKeyValueSize < 0) {
-              KeyValue kv = KeyValueUtil.ensureKeyValue(key);
               throw new IllegalStateException("blockSeek with seekBefore "
                   + "at the first key of the block: key="
-                  + Bytes.toStringBinary(kv.getKey(), kv.getKeyOffset(), kv.getKeyLength())
+                  + Bytes.toStringBinary(CellUtil.getFlatKey(key))
                   + ", blockOffset=" + block.getOffset() + ", onDiskSize="
                   + block.getOnDiskSizeWithHeader());
             }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
index 4813e10..a09873a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HStore.java
@@ -709,30 +709,34 @@ public class HStore implements Store {
       if (verifyBulkLoads) {
         long verificationStartTime = EnvironmentEdgeManager.currentTime();
         LOG.info("Full verification started for bulk load hfile: " + srcPath.toString());
-        Cell prevKV = null;
+        Cell prevCell = null;
         HFileScanner scanner = reader.getScanner(false, false, false);
         scanner.seekTo();
         do {
-          Cell kv = scanner.getKeyValue();
-          if (prevKV != null) {
-            if (Bytes.compareTo(prevKV.getRowArray(), prevKV.getRowOffset(),
-                prevKV.getRowLength(), kv.getRowArray(), kv.getRowOffset(),
-                kv.getRowLength()) > 0) {
+          Cell cell = scanner.getKeyValue();
+          if (prevCell != null) {
+            if (Bytes.compareTo(prevCell.getRowArray(), prevCell.getRowOffset(),
+                prevCell.getRowLength(), cell.getRowArray(), cell.getRowOffset(),
+                cell.getRowLength()) > 0) {
               throw new InvalidHFileException("Previous row is greater than"
                   + " current row: path=" + srcPath + " previous="
-                  + Bytes.toStringBinary(KeyValueUtil.ensureKeyValue(prevKV).getKey()) + " current="
-                  + Bytes.toStringBinary(KeyValueUtil.ensureKeyValue(kv).getKey()));
+                  + Bytes.toStringBinary(CellUtil.getFlatKey(prevCell)) + " current="
+                  + Bytes.toStringBinary(CellUtil.getFlatKey(cell)));
             }
-            if (Bytes.compareTo(prevKV.getFamilyArray(), prevKV.getFamilyOffset(),
-                prevKV.getFamilyLength(), kv.getFamilyArray(), kv.getFamilyOffset(),
-                kv.getFamilyLength()) != 0) {
+            if (Bytes.compareTo(prevCell.getFamilyArray(), prevCell.getFamilyOffset(),
+                prevCell.getFamilyLength(), cell.getFamilyArray(), cell.getFamilyOffset(),
+                cell.getFamilyLength()) != 0) {
               throw new InvalidHFileException("Previous key had different"
                   + " family compared to current key: path=" + srcPath
-                  + " previous=" + Bytes.toStringBinary(prevKV.getFamily())
-                  + " current=" + Bytes.toStringBinary(kv.getFamily()));
+                  + " previous="
+                  + Bytes.toStringBinary(prevCell.getFamilyArray(), prevCell.getFamilyOffset(),
+                      prevCell.getFamilyLength())
+                  + " current="
+                  + Bytes.toStringBinary(cell.getFamilyArray(), cell.getFamilyOffset(),
+                      cell.getFamilyLength()));
             }
           }
-          prevKV = kv;
+          prevCell = cell;
         } while (scanner.next());
       LOG.info("Full verification complete for bulk load hfile: " + srcPath.toString()
          + " took " + (EnvironmentEdgeManager.currentTime() - verificationStartTime)
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
index 3404448..aa12030 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -37,8 +37,6 @@ import org.apache.hadoop.hbase.HBaseIOException;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HTableDescriptor;
-import org.apache.hadoop.hbase.KeyValue;
-import org.apache.hadoop.hbase.KeyValueUtil;
 import org.apache.hadoop.hbase.NotServingRegionException;
 import org.apache.hadoop.hbase.ServerName;
 import org.apache.hadoop.hbase.TableName;
@@ -133,7 +131,6 @@ import org.apache.hadoop.hbase.protobuf.generated.RPCProtos.RequestHeader;
 import org.apache.hadoop.hbase.protobuf.generated.WALProtos.CompactionDescriptor;
 import org.apache.hadoop.hbase.quotas.OperationQuota;
 import org.apache.hadoop.hbase.quotas.RegionServerQuotaManager;
-import org.apache.hadoop.hbase.quotas.ThrottlingException;
 import org.apache.hadoop.hbase.regionserver.HRegion.Operation;
 import org.apache.hadoop.hbase.regionserver.Leases.LeaseStillHeldException;
 import org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler;
@@ -2047,7 +2044,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
           lease = regionServer.leases.removeLease(scannerName);
           List<Result> results = new ArrayList<Result>(rows);
           long currentScanResultSize = 0;
-          long totalKvSize = 0;
+          long totalCellSize = 0;
 
           boolean done = false;
           // Call coprocessor. Get region info from scanner.
@@ -2057,9 +2054,8 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
             if (!results.isEmpty()) {
               for (Result r : results) {
                 for (Cell cell : r.rawCells()) {
-                  KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
-                  currentScanResultSize += kv.heapSize();
-                  totalKvSize += kv.getLength();
+                  currentScanResultSize += CellUtil.estimatedHeapSizeOf(cell);
+                  totalCellSize += CellUtil.estimatedLengthOf(cell);
                 }
               }
             }
@@ -2089,9 +2085,8 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
                   boolean moreRows = scanner.nextRaw(values);
                   if (!values.isEmpty()) {
                     for (Cell cell : values) {
-                      KeyValue kv = KeyValueUtil.ensureKeyValue(cell);
-                      currentScanResultSize += kv.heapSize();
-                      totalKvSize += kv.getLength();
+                      currentScanResultSize += CellUtil.estimatedHeapSizeOf(cell);
+                      totalCellSize += CellUtil.estimatedLengthOf(cell);
                     }
                     results.add(Result.create(values, null, stale));
                     i++;
@@ -2103,7 +2098,7 @@ public class RSRpcServices implements HBaseRPCErrorHandler,
                 }
               }
               region.readRequestsCount.add(i);
-              region.getMetrics().updateScanNext(totalKvSize);
+              region.getMetrics().updateScanNext(totalCellSize);
             } finally {
               region.closeRegionOperation();
             }
