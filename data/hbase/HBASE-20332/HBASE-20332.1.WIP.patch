From 282d1c8a742af2ba99e6731d141f0cb8dfd63d29 Mon Sep 17 00:00:00 2001
From: Sean Busbey <busbey@apache.org>
Date: Tue, 17 Apr 2018 14:40:25 -0500
Subject: [PATCH 1/4] HBASE-20439 Clean up incorrect use of commons-logging in
 hbase-server

---
 .../org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java   | 7 ++++---
 .../org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java  | 7 ++++---
 .../org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java  | 7 ++++---
 .../java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java  | 7 ++++---
 .../hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java | 8 +++++---
 .../java/org/apache/hadoop/hbase/TestClusterPortAssignment.java   | 7 ++++---
 .../java/org/apache/hadoop/hbase/client/TestFlushFromClient.java  | 7 ++++---
 .../apache/hadoop/hbase/client/TestSeparateClientZKCluster.java   | 7 ++++---
 .../org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java  | 7 ++++---
 .../test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java    | 7 ++++---
 10 files changed, 41 insertions(+), 30 deletions(-)

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java
index 8f735bde4d..550aea7e06 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java
@@ -25,8 +25,6 @@ import java.util.Map;
 import java.util.concurrent.ArrayBlockingQueue;
 import java.util.concurrent.BlockingQueue;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.Server;
 import org.apache.hadoop.hbase.util.Threads;
@@ -37,6 +35,9 @@ import org.apache.yetus.audience.InterfaceAudience;
 import org.apache.zookeeper.CreateMode;
 import org.apache.zookeeper.KeeperException;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  * Tracks the target znode(s) on server ZK cluster and synchronize them to client ZK cluster if
  * changed
@@ -45,7 +46,7 @@ import org.apache.zookeeper.KeeperException;
  */
 @InterfaceAudience.Private
 public abstract class ClientZKSyncer extends ZKListener {
-  private static final Log LOG = LogFactory.getLog(ClientZKSyncer.class);
+  private static final Logger LOG = LoggerFactory.getLogger(ClientZKSyncer.class);
   private final Server server;
   private final ZKWatcher clientZkWatcher;
   // We use queues and daemon threads to synchronize the data to client ZK cluster
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
index 8cde9c1cff..58434f7bde 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
@@ -35,8 +35,6 @@ import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
 import org.apache.commons.lang.builder.HashCodeBuilder;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -54,6 +52,9 @@ import org.apache.hadoop.hbase.util.HFileArchiveUtil;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.yetus.audience.InterfaceAudience;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.hbase.thirdparty.com.google.common.collect.HashMultimap;
 import org.apache.hbase.thirdparty.com.google.common.collect.Multimap;
 import org.apache.hbase.thirdparty.com.google.protobuf.InvalidProtocolBufferException;
@@ -69,7 +70,7 @@ import org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos.Snapshot
  */
 @InterfaceAudience.Private
 public class FileArchiverNotifierImpl implements FileArchiverNotifier {
-  private static final Log LOG = LogFactory.getLog(FileArchiverNotifierImpl.class);
+  private static final Logger LOG = LoggerFactory.getLogger(FileArchiverNotifierImpl.class);
   private final Connection conn;
   private final Configuration conf;
   private final FileSystem fs;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java
index bf525e55e6..56798fd937 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java
@@ -23,8 +23,6 @@ import java.util.Map.Entry;
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.ScheduledChore;
 import org.apache.hadoop.hbase.client.RegionInfo;
@@ -33,12 +31,15 @@ import org.apache.hadoop.hbase.regionserver.Region;
 import org.apache.hadoop.hbase.regionserver.RegionServerServices;
 import org.apache.yetus.audience.InterfaceAudience;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  * A Chore which sends the region size reports on this RegionServer to the Master.
  */
 @InterfaceAudience.Private
 public class RegionSizeReportingChore extends ScheduledChore {
-  private static final Log LOG = LogFactory.getLog(RegionSizeReportingChore.class);
+  private static final Logger LOG = LoggerFactory.getLogger(RegionSizeReportingChore.class);
 
   static final String REGION_SIZE_REPORTING_CHORE_PERIOD_KEY =
       "hbase.regionserver.quotas.region.size.reporting.chore.period";
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java
index 4b48869f6c..b7f59a3018 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java
@@ -20,13 +20,14 @@ import java.util.Iterator;
 import java.util.Map.Entry;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.client.RegionInfo;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.yetus.audience.InterfaceAudience;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  * A {@link RegionSizeStore} implementation backed by a ConcurrentHashMap. We expected similar
  * amounts of reads and writes to the "store", so using a RWLock is not going to provide any
@@ -34,7 +35,7 @@ import org.apache.yetus.audience.InterfaceAudience;
  */
 @InterfaceAudience.Private
 public class RegionSizeStoreImpl implements RegionSizeStore {
-  private static final Log LOG = LogFactory.getLog(RegionSizeStoreImpl.class);
+  private static final Logger LOG = LoggerFactory.getLogger(RegionSizeStoreImpl.class);
   private static final long sizeOfEntry = ClassSize.align(
       ClassSize.CONCURRENT_HASHMAP_ENTRY
       + ClassSize.OBJECT + Bytes.SIZEOF_LONG
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java
index a237a52dc6..a0dc629ad7 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java
@@ -21,8 +21,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ConcurrentSkipListMap;
 import java.util.concurrent.atomic.AtomicInteger;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.RegionTooBusyException;
@@ -32,6 +30,10 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.ClassSize;
 import org.apache.yetus.audience.InterfaceAudience;
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 /**
  * StoreHotnessProtector is designed to help limit the concurrency of puts with dense columns, it
  * does best-effort to avoid exhausting all RS's handlers. When a lot of clients write requests with
@@ -60,7 +62,7 @@ import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesti
  */
 @InterfaceAudience.Private
 public class StoreHotnessProtector {
-  private static final Log LOG = LogFactory.getLog(StoreHotnessProtector.class);
+  private static final Logger LOG = LoggerFactory.getLogger(StoreHotnessProtector.class);
   private volatile int parallelPutToStoreThreadLimit;
 
   private volatile int parallelPreparePutToStoreThreadLimit;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java
index 0c8247f362..69541010a4 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java
@@ -22,8 +22,6 @@ import static org.junit.Assert.assertTrue;
 
 import java.net.BindException;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 
@@ -31,6 +29,9 @@ import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 @Category(MediumTests.class)
 public class TestClusterPortAssignment {
   @ClassRule
@@ -38,7 +39,7 @@ public class TestClusterPortAssignment {
       HBaseClassTestRule.forClass(TestClusterPortAssignment.class);
 
   private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
-  private static final Log LOG = LogFactory.getLog(TestClusterPortAssignment.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestClusterPortAssignment.class);
 
   /**
    * Check that we can start an HBase cluster specifying a custom set of
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java
index 207e1fc781..7afd36b77b 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java
@@ -25,8 +25,6 @@ import java.util.Arrays;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 import java.util.stream.Collectors;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.TableName;
@@ -47,6 +45,9 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 @Category({MediumTests.class, ClientTests.class})
 public class TestFlushFromClient {
 
@@ -54,7 +55,7 @@ public class TestFlushFromClient {
   public static final HBaseClassTestRule CLASS_RULE =
       HBaseClassTestRule.forClass(TestFlushFromClient.class);
 
-  private static final Log LOG = LogFactory.getLog(TestFlushFromClient.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestFlushFromClient.class);
   private final static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private static AsyncConnection asyncConn;
   private static final byte[][] SPLITS = new byte[][]{Bytes.toBytes("3"), Bytes.toBytes("7")};
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java
index 769ac64be8..533af935b5 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java
@@ -20,8 +20,6 @@ package org.apache.hadoop.hbase.client;
 import java.io.File;
 
 import org.apache.commons.io.FileUtils;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HConstants;
@@ -44,9 +42,12 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 @Category(MediumTests.class)
 public class TestSeparateClientZKCluster {
-  private static final Log LOG = LogFactory.getLog(TestSeparateClientZKCluster.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestSeparateClientZKCluster.class);
   private static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private static final File clientZkDir = new File("/tmp/TestSeparateClientZKCluster");
   private static final int ZK_SESSION_TIMEOUT = 5000;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java
index bf72d331cb..3e219517d6 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java
@@ -23,8 +23,6 @@ import static org.junit.Assert.fail;
 import java.io.IOException;
 import java.util.List;
 import java.util.Optional;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
@@ -45,6 +43,9 @@ import org.junit.ClassRule;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ProcedureProtos;
 
 /**
@@ -57,7 +58,7 @@ public class TestFailedProcCleanup {
   public static final HBaseClassTestRule CLASS_RULE =
       HBaseClassTestRule.forClass(TestFailedProcCleanup.class);
 
-  private static final Log LOG = LogFactory.getLog(TestFailedProcCleanup.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestFailedProcCleanup.class);
 
   protected static HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private static Configuration conf;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java
index bd59ce9410..bee76c6d8a 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java
@@ -22,8 +22,6 @@ import static org.junit.Assert.fail;
 
 import java.io.IOException;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
@@ -45,6 +43,9 @@ import org.junit.Test;
 import org.junit.experimental.categories.Category;
 import org.junit.rules.TestName;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 @Category({ RegionServerTests.class, MediumTests.class })
 public class TestDisabledWAL {
 
@@ -55,7 +56,7 @@ public class TestDisabledWAL {
   @Rule
   public TestName name = new TestName();
 
-  private static final Log LOG = LogFactory.getLog(TestDisabledWAL.class);
+  private static final Logger LOG = LoggerFactory.getLogger(TestDisabledWAL.class);
   static final HBaseTestingUtility TEST_UTIL = new HBaseTestingUtility();
   private Table table;
   private TableName tableName;
-- 
2.16.1


From c86ba6f16ac4283e5489b0b0bdb4b4944f41eb1f Mon Sep 17 00:00:00 2001
From: Sean Busbey <busbey@apache.org>
Date: Tue, 17 Apr 2018 15:21:49 -0500
Subject: [PATCH 2/4] HBASE-20440 Clean up incorrect use of commons-lang 2.y

---
 hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java   | 2 +-
 .../apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.java   | 4 ++--
 .../java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java | 2 +-
 .../org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java | 2 +-
 .../hadoop/hbase/util/compaction/TestMajorCompactionRequest.java      | 4 ++--
 5 files changed, 7 insertions(+), 7 deletions(-)

diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java
index 9d7f65c7b3..ab7fa3bcd4 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/net/Address.java
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hbase.net;
 
-import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.yetus.audience.InterfaceAudience;
 
 import org.apache.hbase.thirdparty.com.google.common.net.HostAndPort;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.java
index 3d21518957..5b6d8c14bf 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.java
@@ -19,7 +19,7 @@ package org.apache.hadoop.hbase.quotas;
 import java.util.Objects;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.commons.lang.builder.HashCodeBuilder;
+import org.apache.commons.lang3.builder.HashCodeBuilder;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hbase.TableName;
@@ -111,4 +111,4 @@ public final class FileArchiverNotifierFactoryImpl implements FileArchiverNotifi
       return "CacheKey[TableName=" + tn + "]";
     }
   }
-}
\ No newline at end of file
+}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
index 58434f7bde..aa916963c3 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java
@@ -34,7 +34,7 @@ import java.util.concurrent.locks.ReentrantReadWriteLock.WriteLock;
 import java.util.function.Predicate;
 import java.util.stream.Collectors;
 
-import org.apache.commons.lang.builder.HashCodeBuilder;
+import org.apache.commons.lang3.builder.HashCodeBuilder;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java
index feea0865a0..6c20b5ba75 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHdfsSnapshotHRegion.java
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hbase.regionserver;
 
 import java.io.IOException;
-import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseClassTestRule;
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/compaction/TestMajorCompactionRequest.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/compaction/TestMajorCompactionRequest.java
index b62648189a..adecd5c6b0 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/util/compaction/TestMajorCompactionRequest.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/util/compaction/TestMajorCompactionRequest.java
@@ -22,7 +22,7 @@ import java.util.List;
 import java.util.Optional;
 import java.util.Set;
 import java.util.stream.Collectors;
-import org.apache.commons.lang.RandomStringUtils;
+import org.apache.commons.lang3.RandomStringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -167,4 +167,4 @@ public class TestMajorCompactionRequest {
     doReturn(mock(Connection.class)).when(spy).getConnection(eq(configuration));
     return spy;
   }
-}
\ No newline at end of file
+}
-- 
2.16.1


From 814e3f54fb377a706c55269bc86345bff28e8da8 Mon Sep 17 00:00:00 2001
From: Sean Busbey <busbey@apache.org>
Date: Tue, 17 Apr 2018 16:15:11 -0500
Subject: [PATCH 3/4] HBASE-20442 clean up incorrect use of commons-collections
 3

---
 .../java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java  | 3 ++-
 .../src/main/java/org/apache/hadoop/hbase/client/RowMutations.java    | 3 ++-
 hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java    | 2 +-
 .../apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java    | 2 +-
 .../src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java   | 2 +-
 .../main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java | 2 +-
 .../main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java  | 2 +-
 .../java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java     | 2 +-
 .../src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java        | 4 ++--
 9 files changed, 12 insertions(+), 10 deletions(-)

diff --git a/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java b/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java
index 093ef763af..5ce11d19c0 100644
--- a/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java
+++ b/hbase-backup/src/main/java/org/apache/hadoop/hbase/backup/master/BackupLogCleaner.java
@@ -24,7 +24,6 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.commons.collections.MapUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.hbase.HBaseInterfaceAudience;
@@ -42,6 +41,8 @@ import org.apache.yetus.audience.InterfaceAudience;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.MapUtils;
+
 /**
  * Implementation of a log cleaner that checks if a log is still scheduled for incremental backup
  * before deleting it when its TTL is over.
diff --git a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RowMutations.java b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RowMutations.java
index 4b426cf10e..345e26aa02 100644
--- a/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RowMutations.java
+++ b/hbase-client/src/main/java/org/apache/hadoop/hbase/client/RowMutations.java
@@ -23,10 +23,11 @@ import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.yetus.audience.InterfaceAudience;
 
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
+
 /**
  * Performs multiple mutations atomically on a single row.
  * Currently {@link Put} and {@link Delete} are supported.
diff --git a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
index a315fd2b39..6eb09c11df 100644
--- a/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
+++ b/hbase-common/src/main/java/org/apache/hadoop/hbase/util/Bytes.java
@@ -38,7 +38,6 @@ import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
 import org.apache.hadoop.hbase.KeyValue;
@@ -50,6 +49,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 import com.google.protobuf.ByteString;
 
diff --git a/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java b/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java
index 6d721281f2..b9ebfb9051 100644
--- a/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java
+++ b/hbase-replication/src/main/java/org/apache/hadoop/hbase/replication/ZKReplicationQueueStorage.java
@@ -29,7 +29,6 @@ import java.util.Set;
 import java.util.SortedSet;
 import java.util.TreeSet;
 import java.util.stream.Collectors;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HConstants;
@@ -53,6 +52,7 @@ import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 /**
  * ZK based replication queue storage.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index eccb67ec8c..1fb6afe7ec 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -70,7 +70,6 @@ import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReadWriteLock;
 import java.util.concurrent.locks.ReentrantReadWriteLock;
 import java.util.function.Function;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -188,6 +187,7 @@ import org.apache.hbase.thirdparty.com.google.common.io.Closeables;
 import org.apache.hbase.thirdparty.com.google.protobuf.Service;
 import org.apache.hbase.thirdparty.com.google.protobuf.TextFormat;
 import org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
index 922fa863c8..8828a22610 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RSRpcServices.java
@@ -42,7 +42,6 @@ import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.LongAdder;
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.commons.lang3.mutable.MutableObject;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
@@ -150,6 +149,7 @@ import org.apache.hbase.thirdparty.com.google.protobuf.RpcController;
 import org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;
 import org.apache.hbase.thirdparty.com.google.protobuf.TextFormat;
 import org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;
 import org.apache.hadoop.hbase.shaded.protobuf.RequestConverter;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
index 2973e57819..f493a15ea8 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java
@@ -27,7 +27,6 @@ import java.util.OptionalInt;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.locks.ReentrantLock;
 
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
 import org.apache.hadoop.hbase.CellUtil;
@@ -54,6 +53,7 @@ import org.slf4j.LoggerFactory;
 
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
 import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 /**
  * Scanner scans both the memstore and the Store. Coalesce KeyValue stream into List&lt;KeyValue&gt;
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java
index 4db69737b8..ac5d3ed37b 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.java
@@ -25,7 +25,6 @@ import java.util.List;
 import java.util.Set;
 import java.util.TreeSet;
 
-import org.apache.commons.collections.CollectionUtils;
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellComparator;
 import org.apache.hadoop.hbase.CellUtil;
@@ -39,6 +38,7 @@ import org.apache.hadoop.hbase.wal.WALKeyImpl;
 import org.apache.yetus.audience.InterfaceAudience;
 
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
 
 /**
  * A WAL Entry for {@link AbstractFSWAL} implementation.  Immutable.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
index 6c77c4ce95..dee5fb045f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/wal/WALSplitter.java
@@ -46,8 +46,6 @@ import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
-import org.apache.commons.collections.CollectionUtils;
-import org.apache.commons.collections.MapUtils;
 import org.apache.commons.lang3.ArrayUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileAlreadyExistsException;
@@ -95,6 +93,8 @@ import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesti
 import org.apache.hbase.thirdparty.com.google.common.base.Preconditions;
 import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
 import org.apache.hbase.thirdparty.com.google.protobuf.TextFormat;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.CollectionUtils;
+import org.apache.hbase.thirdparty.org.apache.commons.collections4.MapUtils;
 
 import org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.WALEntry;
 import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.MutationProto.MutationType;
-- 
2.16.1


From 39f6f74c7c1472b66faff5224f34f2ace7420548 Mon Sep 17 00:00:00 2001
From: Sean Busbey <busbey@apache.org>
Date: Mon, 9 Apr 2018 13:37:44 -0500
Subject: [PATCH 4/4] HBASE-20332 shaded mapreduce module shouldn't include
 hadoop

* modify the jar checking script to take args; make hadoop stuff optional
* separate out checking the artifacts that have hadoop vs those that don't.
* * Unfortunately means we need two modules for checking things
* * put in a safety check that the support script for checking jar contents is maintained in both modules
* * have to carve out an exception for o.a.hadoop.metrics2. :(
* fix duplicated class warning
* clean up dependencies in hbase-server and some modules that depend on it.
---
 hbase-mapreduce/pom.xml                            |  18 ++
 hbase-rest/pom.xml                                 |  13 ++
 hbase-server/pom.xml                               | 109 ++++++-----
 hbase-shaded/hbase-shaded-check-invariants/pom.xml |  54 ++++--
 .../resources/ensure-jars-have-correct-contents.sh |  92 +++++++--
 hbase-shaded/hbase-shaded-mapreduce/pom.xml        | 198 ++++++++++++++++++-
 .../pom.xml                                        | 215 +++++++++++++++++++++
 .../resources/ensure-jars-have-correct-contents.sh | 129 +++++++++++++
 hbase-shaded/pom.xml                               |  13 ++
 pom.xml                                            |  36 +++-
 10 files changed, 782 insertions(+), 95 deletions(-)
 create mode 100644 hbase-shaded/hbase-shaded-with-hadoop-check-invariants/pom.xml
 create mode 100644 hbase-shaded/hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh

diff --git a/hbase-mapreduce/pom.xml b/hbase-mapreduce/pom.xml
index af80737644..5a6a634f40 100644
--- a/hbase-mapreduce/pom.xml
+++ b/hbase-mapreduce/pom.xml
@@ -196,6 +196,15 @@
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-server</artifactId>
+      <exclusions>
+        <!-- commons-logging is only used by hbase-http's HttpRequestLog and hbase-server's
+             HBaseTestingUtil. We don't need either of those here, so execlude it.
+          -->
+        <exclusion>
+          <groupId>commons-logging</groupId>
+          <artifactId>commons-logging</artifactId>
+        </exclusion>
+      </exclusions>
     </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
@@ -246,10 +255,19 @@
       <artifactId>junit</artifactId>
       <scope>test</scope>
     </dependency>
+    <!-- jackson(s) used by PerformanceEvaluation and it looks like TableMapReduceUtil -->
     <dependency>
       <groupId>com.fasterxml.jackson.core</groupId>
       <artifactId>jackson-databind</artifactId>
     </dependency>
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-annotations</artifactId>
+    </dependency>
   </dependencies>
   <profiles>
     <!-- Skip the tests in this module -->
diff --git a/hbase-rest/pom.xml b/hbase-rest/pom.xml
index 617f254a2f..1419a35820 100644
--- a/hbase-rest/pom.xml
+++ b/hbase-rest/pom.xml
@@ -299,6 +299,19 @@
       <groupId>com.fasterxml.jackson.jaxrs</groupId>
       <artifactId>jackson-jaxrs-json-provider</artifactId>
     </dependency>
+    <dependency>
+      <!-- We *might* need this for XMLStreamReader use in RemoteAdmin
+           TODO figure out if we can remove it.
+        -->
+      <groupId>org.codehaus.jettison</groupId>
+      <artifactId>jettison</artifactId>
+        <exclusions>
+          <exclusion>
+            <groupId>stax</groupId>
+            <artifactId>stax-api</artifactId>
+          </exclusion>
+        </exclusions>
+    </dependency>
     <dependency>
       <!--For JspC used in ant task-->
       <groupId>org.glassfish.web</groupId>
diff --git a/hbase-server/pom.xml b/hbase-server/pom.xml
index e35fc8336d..aefb4c8891 100644
--- a/hbase-server/pom.xml
+++ b/hbase-server/pom.xml
@@ -70,6 +70,14 @@
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-remote-resources-plugin</artifactId>
         <version>1.5</version>
+        <dependencies>
+          <!-- resource bundle only needed at build time -->
+          <dependency>
+            <groupId>org.apache.hbase</groupId>
+            <artifactId>hbase-resource-bundle</artifactId>
+            <version>${project.version}</version>
+          </dependency>
+        </dependencies>
         <executions>
           <execution>
             <id>default</id>
@@ -391,12 +399,6 @@
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-metrics</artifactId>
     </dependency>
-    <!-- resource bundle only needed at build time -->
-    <dependency>
-      <groupId>org.apache.hbase</groupId>
-      <artifactId>hbase-resource-bundle</artifactId>
-      <optional>true</optional>
-    </dependency>
     <dependency>
       <groupId>commons-codec</groupId>
       <artifactId>commons-codec</artifactId>
@@ -436,19 +438,16 @@
       <artifactId>jetty-webapp</artifactId>
     </dependency>
     <dependency>
-      <!--For JspC used in ant task-->
+      <!-- For JspC used in ant task, then needed at compile /runtime
+           because the source code made from the JSP refers to its runtime
+        -->
       <groupId>org.glassfish.web</groupId>
       <artifactId>javax.servlet.jsp</artifactId>
     </dependency>
+      <!-- Also used by generated sources from our JSP -->
     <dependency>
-      <groupId>org.codehaus.jettison</groupId>
-      <artifactId>jettison</artifactId>
-        <exclusions>
-          <exclusion>
-            <groupId>stax</groupId>
-            <artifactId>stax-api</artifactId>
-          </exclusion>
-        </exclusions>
+      <groupId>javax.servlet.jsp</groupId>
+      <artifactId>javax.servlet.jsp-api</artifactId>
     </dependency>
     <!-- General dependencies -->
     <dependency>
@@ -500,9 +499,20 @@
       <groupId>javax.servlet</groupId>
       <artifactId>javax.servlet-api</artifactId>
     </dependency>
+    <!-- Jackson only used in compile/runtime scope by BlockCacheUtil class
+         also used by some tests
+      -->
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-databind</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-core</artifactId>
+    </dependency>
     <dependency>
-      <groupId>javax.ws.rs</groupId>
-      <artifactId>javax.ws.rs-api</artifactId>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-annotations</artifactId>
     </dependency>
 
     <!-- tracing Dependencies -->
@@ -510,11 +520,6 @@
       <groupId>org.apache.htrace</groupId>
       <artifactId>htrace-core4</artifactId>
     </dependency>
-    <dependency>
-      <groupId>org.apache.htrace</groupId>
-      <artifactId>htrace-core</artifactId>
-      <version>${htrace-hadoop.version}</version>
-    </dependency>
     <dependency>
       <groupId>com.lmax</groupId>
       <artifactId>disruptor</artifactId>
@@ -555,6 +560,15 @@
       <artifactId>httpcore</artifactId>
       <scope>test</scope>
     </dependency>
+    <!-- commons-logging is used by HBTU to monkey with log levels
+         have to put it at compile scope because Hadoop's IOUtils uses it
+         both for hadoop 2.7 and 3.0, so we'll fail at compile if it's at test scope.
+      -->
+    <dependency>
+      <groupId>commons-logging</groupId>
+      <artifactId>commons-logging</artifactId>
+      <scope>compile</scope>
+    </dependency>
     <dependency>
       <groupId>org.apache.commons</groupId>
       <artifactId>commons-crypto</artifactId>
@@ -673,34 +687,10 @@
         </property>
       </activation>
       <dependencies>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-distcp</artifactId>
-          <version>${hadoop-two.version}</version>
-        </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-common</artifactId>
         </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-auth</artifactId>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-annotations</artifactId>
-          <version>${hadoop-two.version}</version>
-        </dependency>
-        <dependency>
-          <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-client</artifactId>
-          <exclusions>
-            <exclusion>
-              <groupId>com.google.guava</groupId>
-              <artifactId>guava</artifactId>
-            </exclusion>
-          </exclusions>
-        </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-mapreduce-client-core</artifactId>
@@ -755,6 +745,12 @@
           <version>${netty.hadoop.version}</version>
           <scope>test</scope>
         </dependency>
+        <dependency>
+          <groupId>org.apache.htrace</groupId>
+          <artifactId>htrace-core</artifactId>
+          <version>${htrace-hadoop.version}</version>
+          <scope>test</scope>
+        </dependency>
       </dependencies>
       <build>
         <plugins>
@@ -795,21 +791,32 @@
       <dependencies>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-distcp</artifactId>
-          <version>${hadoop-three.version}</version>
+          <artifactId>hadoop-common</artifactId>
         </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-common</artifactId>
+          <artifactId>hadoop-hdfs</artifactId>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-hdfs-client</artifactId>
+          <version>${hadoop.version}</version>
+        </dependency>
+        <dependency>
+          <groupId>org.apache.hadoop</groupId>
+          <artifactId>hadoop-mapreduce-client-core</artifactId>
         </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
           <artifactId>hadoop-hdfs</artifactId>
+          <type>test-jar</type>
+          <scope>test</scope>
         </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
-          <artifactId>hadoop-annotations</artifactId>
-          <version>${hadoop-three.version}</version>
+          <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
+          <type>test-jar</type>
+          <scope>test</scope>
         </dependency>
         <dependency>
           <groupId>org.apache.hadoop</groupId>
diff --git a/hbase-shaded/hbase-shaded-check-invariants/pom.xml b/hbase-shaded/hbase-shaded-check-invariants/pom.xml
index 7322769f0b..88dfc73947 100644
--- a/hbase-shaded/hbase-shaded-check-invariants/pom.xml
+++ b/hbase-shaded/hbase-shaded-check-invariants/pom.xml
@@ -26,7 +26,7 @@
   Enforces our invariants for our shaded artifacts. e.g. shaded clients have
   a specific set of transitive dependencies and shaded clients only contain
   classes that are in particular packages. Does the enforcement through
-  the maven-enforcer-plugin and and integration test.
+  the maven-enforcer-plugin and integration test.
   </description>
   <name>Apache HBase Shaded Packaging Invariants</name>
 
@@ -34,11 +34,15 @@
   </properties>
 
   <dependencies>
-    <dependency>
-      <groupId>org.apache.hbase</groupId>
-      <artifactId>hbase-shaded-client</artifactId>
-      <version>${project.version}</version>
-    </dependency>
+    <!-- Include here any client facing artifacts that presume 
+         the runtime environment will have hadoop.
+
+         If our checks fail for the shaded mapreduce artifact,
+         then probably a dependency from hadoop has shown up
+         in the hbase-mapreduce module without being flagged
+         as 'provided' scope. See the note by the relevant
+         hadoop profile in that module.
+      -->
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-shaded-mapreduce</artifactId>
@@ -113,6 +117,8 @@
                     <exclude>com.github.stephenc.findbugs:*</exclude>
                     <!-- We leave HTrace as an unshaded dependnecy on purpose so that tracing within a JVM will work -->
                     <exclude>org.apache.htrace:*</exclude>
+                    <!-- Our public API requires Hadoop at runtime to work -->
+                    <exclude>org.apache.hadoop:*</exclude>
                   </excludes>
                 </banTransitiveDependencies>
                 <banDuplicateClasses>
@@ -158,18 +164,37 @@
           </execution>
         </executions>
       </plugin>
-      <!--
-        Check that we actually relocated everything we included.
-        It's critical that we don't ship third party dependencies that haven't
-        been relocated under our pacakge space, since this will lead to
-        difficult to debug classpath errors for downstream. Unfortunately, that
-        means inspecting all the jars.
-        -->
       <plugin>
         <groupId>org.codehaus.mojo</groupId>
         <artifactId>exec-maven-plugin</artifactId>
         <version>1.6.0</version>
         <executions>
+          <!-- It's easier to have two copies of our validation
+               script than to copy it via remote-resources-plugin, but
+               we need to make sure they stay the same.
+            -->
+          <execution>
+            <id>make-sure-validation-files-are-in-sync</id>
+            <phase>validate</phase>
+            <goals>
+              <goal>exec</goal>
+            </goals>
+            <configuration>
+              <executable>diff</executable>
+              <requiresOnline>false</requiresOnline>
+              <arguments>
+                <argument>../hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh</argument>
+                <argument>../hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh</argument>
+              </arguments>
+            </configuration>
+          </execution>
+          <!--
+            Check that we actually relocated everything we included.
+            It's critical that we don't ship third party dependencies that haven't
+            been relocated under our package space, since this will lead to
+            difficult to debug classpath errors for downstream. Unfortunately, that
+            means inspecting all the jars.
+            -->
           <execution>
             <id>check-jar-contents</id>
             <phase>integration-test</phase>
@@ -180,6 +205,9 @@
               <executable>${shell-executable}</executable>
               <workingDirectory>${project.build.testOutputDirectory}</workingDirectory>
               <requiresOnline>false</requiresOnline>
+              <!-- Important that we don't pass the 'allow-hadoop' flag here, because
+                   we allowed it as a provided dependency above.
+                -->
               <arguments>
                 <argument>ensure-jars-have-correct-contents.sh</argument>
                 <argument>${hbase-client-artifacts}</argument>
diff --git a/hbase-shaded/hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh b/hbase-shaded/hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh
index 8bda8ce953..a2bb332e3c 100644
--- a/hbase-shaded/hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh
+++ b/hbase-shaded/hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh
@@ -15,33 +15,67 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-# Usage: $0 [/path/to/some/example.jar:/path/to/another/example/created.jar]
-#
-# accepts a single command line argument with a colon separated list of
-# paths to jars to check. Iterates through each such passed jar and checks
-# all the contained paths to make sure they follow the below constructed
-# safe list.
+set -e
+function usage {
+  echo "Usage: ${0} [options] [/path/to/some/example.jar:/path/to/another/example/created.jar]"
+  echo ""
+  echo "  accepts a single command line argument with a colon separated list of"
+  echo "  paths to jars to check. Iterates through each such passed jar and checks"
+  echo "  all the contained paths to make sure they follow the below constructed"
+  echo "  safe list."
+  echo ""
+  echo "    --allow-hadoop     Include stuff from the Apache Hadoop project in the list"
+  echo "                       of allowed jar contents. default: false"
+  echo "    --debug            print more info to stderr"
+  exit 1
+}
+# if no args specified, show usage
+if [ $# -lt 1 ]; then
+  usage
+fi
+
+# Get arguments
+declare allow_hadoop
+declare debug
+while [ $# -gt 0 ]
+do
+  case "$1" in
+    --allow-hadoop) shift; allow_hadoop="true";;
+    --debug) shift; debug="true";;
+    --) shift; break;;
+    -*) usage ;;
+    *)  break;;  # terminate while loop
+  esac
+done
+
+# should still have jars to check.
+if [ $# -lt 1 ]; then
+  usage
+fi
+if [ -n "${debug}" ]; then
+  echo "[DEBUG] Checking on jars: $@" >&2
+  echo "jar command is: $(which jar)" >&2
+  echo "grep command is: $(which grep)" >&2
+  grep -V >&2 || true
+fi
+
+IFS=: read -r -d '' -a artifact_list < <(printf '%s\0' "$1")
 
-# we have to allow the directories that lead to the org/apache/hadoop dir
-allowed_expr="(^org/$|^org/apache/$"
+# we have to allow the directories that lead to the hbase dirs
+allowed_expr="(^org/$|^org/apache/$|^org/apache/hadoop/$"
 # We allow the following things to exist in our client artifacts:
-#   * classes in packages that start with org.apache.hadoop, which by
-#     convention should be in a path that looks like org/apache/hadoop
-allowed_expr+="|^org/apache/hadoop/"
+#   * classes in packages that start with org.apache.hadoop.hbase, which by
+#     convention should be in a path that looks like org/apache/hadoop/hbase
+allowed_expr+="|^org/apache/hadoop/hbase"
 #   * classes in packages that start with org.apache.hbase
 allowed_expr+="|^org/apache/hbase/"
 #   * whatever in the "META-INF" directory
 allowed_expr+="|^META-INF/"
 #   * the folding tables from jcodings
 allowed_expr+="|^tables/"
-#   * Hadoop's and HBase's default configuration files, which have the form
+#   * HBase's default configuration files, which have the form
 #     "_module_-default.xml"
-allowed_expr+="|^[^-]*-default.xml$"
-#   * Hadoop's versioning properties files, which have the form
-#     "_module_-version-info.properties"
-allowed_expr+="|^[^-]*-version-info.properties$"
-#   * Hadoop's application classloader properties file.
-allowed_expr+="|^org.apache.hadoop.application-classloader.properties$"
+allowed_expr+="|^hbase-default.xml$"
 # public suffix list used by httpcomponents
 allowed_expr+="|^mozilla/$"
 allowed_expr+="|^mozilla/public-suffix-list.txt$"
@@ -51,12 +85,30 @@ allowed_expr+="|^properties.dtd$"
 allowed_expr+="|^PropertyList-1.0.dtd$"
 
 
+if [ -n "${allow_hadoop}" ]; then
+  #   * classes in packages that start with org.apache.hadoop, which by
+  #     convention should be in a path that looks like org/apache/hadoop
+  allowed_expr+="|^org/apache/hadoop/"
+  #   * Hadoop's default configuration files, which have the form
+  #     "_module_-default.xml"
+  allowed_expr+="|^[^-]*-default.xml$"
+  #   * Hadoop's versioning properties files, which have the form
+  #     "_module_-version-info.properties"
+  allowed_expr+="|^[^-]*-version-info.properties$"
+  #   * Hadoop's application classloader properties file.
+  allowed_expr+="|^org.apache.hadoop.application-classloader.properties$"
+else
+  # We have some classes for integrating with the Hadoop Metrics2 system
+  # that have to be in a particular package space due to access rules.
+  allowed_expr+="|^org/apache/hadoop/metrics2"
+fi
+
+
 allowed_expr+=")"
 declare -i bad_artifacts=0
 declare -a bad_contents
-IFS=: read -r -d '' -a artifact_list < <(printf '%s\0' "$1")
 for artifact in "${artifact_list[@]}"; do
-  bad_contents=($(jar tf "${artifact}" | grep -v -E "${allowed_expr}"))
+  bad_contents=($(jar tf "${artifact}" | grep -v -E "${allowed_expr}" || true))
   if [ ${#bad_contents[@]} -gt 0 ]; then
     echo "[ERROR] Found artifact with unexpected contents: '${artifact}'"
     echo "    Please check the following and either correct the build or update"
diff --git a/hbase-shaded/hbase-shaded-mapreduce/pom.xml b/hbase-shaded/hbase-shaded-mapreduce/pom.xml
index cfcc357877..6c76167e57 100644
--- a/hbase-shaded/hbase-shaded-mapreduce/pom.xml
+++ b/hbase-shaded/hbase-shaded-mapreduce/pom.xml
@@ -62,6 +62,10 @@
         </plugins>
     </build>
     <dependencies>
+        <!--
+             We want to ensure needed hadoop bits are at provided scope for our shaded
+             artifact, so we list them below in hadoop specific profiles.
+          -->
         <dependency>
             <groupId>org.apache.hbase</groupId>
             <artifactId>hbase-mapreduce</artifactId>
@@ -137,10 +141,6 @@
                 <groupId>org.eclipse.jetty</groupId>
                 <artifactId>jetty-webapp</artifactId>
               </exclusion>
-              <exclusion>
-                <groupId>org.glassfish.web</groupId>
-                <artifactId>javax.servlet.jsp</artifactId>
-              </exclusion>
               <exclusion>
                 <groupId>org.glassfish.jersey.core</groupId>
                 <artifactId>jersey-server</artifactId>
@@ -149,6 +149,17 @@
                 <groupId>org.glassfish.jersey.containers</groupId>
                 <artifactId>jersey-container-servlet-core</artifactId>
               </exclusion>
+              <!-- We excluded the server-side generated classes for JSP, so exclude
+                   their runtime support libraries too
+                -->
+              <exclusion>
+                <groupId>org.glassfish.web</groupId>
+                <artifactId>javax.servlet.jsp</artifactId>
+              </exclusion>
+              <exclusion>
+                <groupId>javax.servlet.jsp</groupId>
+                <artifactId>javax.servlet.jsp-api</artifactId>
+              </exclusion>
             </exclusions>
         </dependency>
     </dependencies>
@@ -158,12 +169,183 @@
             <id>release</id>
             <build>
                 <plugins>
-                    <plugin>
-                        <groupId>org.apache.maven.plugins</groupId>
-                        <artifactId>maven-shade-plugin</artifactId>
-                    </plugin>
+                <!-- Tell the shade plugin we want to leave Hadoop as a dependency -->
+                <plugin>
+                    <groupId>org.apache.maven.plugins</groupId>
+                    <artifactId>maven-shade-plugin</artifactId>
+                    <executions>
+                        <execution>
+                            <id>aggregate-into-a-jar-with-relocated-third-parties</id>
+                            <configuration>
+                                <artifactSet>
+                                    <excludes>
+                                        <exclude>org.apache.hadoop:*</exclude>
+                                        <!-- The rest of these should be kept in sync with the parent pom -->
+                                        <exclude>org.apache.hbase:hbase-resource-bundle</exclude>
+                                        <exclude>org.slf4j:*</exclude>
+                                        <exclude>com.google.code.findbugs:*</exclude>
+                                        <exclude>com.github.stephenc.findbugs:*</exclude>
+                                        <exclude>org.apache.htrace:*</exclude>
+                                        <exclude>org.apache.yetus:*</exclude>
+                                        <exclude>log4j:*</exclude>
+                                        <exclude>commons-logging:*</exclude>
+                                    </excludes>
+                                </artifactSet>
+                            </configuration>
+                        </execution>
+                    </executions>
+                </plugin>
                 </plugins>
             </build>
         </profile>
+        <!-- These hadoop profiles should be derived from those in the hbase-mapreduce
+             module. Essentially, you must list the same hadoop-* dependencies
+             since provided dependencies are not transitively included.
+        -->
+        <!-- profile against Hadoop 2.x: This is the default. -->
+        <profile>
+          <id>hadoop-2.0</id>
+          <activation>
+            <property>
+                <!--Below formatting for dev-support/generate-hadoopX-poms.sh-->
+                <!--h2--><name>!hadoop.profile</name>
+            </property>
+          </activation>
+          <dependencies>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-common</artifactId>
+              <scope>provided</scope>
+              <exclusions>
+                <exclusion>
+                  <groupId>org.apache.htrace</groupId>
+                  <artifactId>htrace-core</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>net.java.dev.jets3t</groupId>
+                  <artifactId>jets3t</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>javax.servlet.jsp</groupId>
+                  <artifactId>jsp-api</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>org.mortbay.jetty</groupId>
+                  <artifactId>jetty</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>com.sun.jersey</groupId>
+                  <artifactId>jersey-server</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>com.sun.jersey</groupId>
+                  <artifactId>jersey-core</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>com.sun.jersey</groupId>
+                  <artifactId>jersey-json</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>javax.servlet</groupId>
+                  <artifactId>servlet-api</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>tomcat</groupId>
+                  <artifactId>jasper-compiler</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>tomcat</groupId>
+                  <artifactId>jasper-runtime</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>com.google.code.findbugs</groupId>
+                  <artifactId>jsr305</artifactId>
+                </exclusion>
+              </exclusions>
+            </dependency>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-hdfs</artifactId>
+              <scope>provided</scope>
+              <exclusions>
+                <exclusion>
+                  <groupId>org.apache.htrace</groupId>
+                  <artifactId>htrace-core</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>javax.servlet.jsp</groupId>
+                  <artifactId>jsp-api</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>javax.servlet</groupId>
+                  <artifactId>servlet-api</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>io.netty</groupId>
+                  <artifactId>netty</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>stax</groupId>
+                  <artifactId>stax-api</artifactId>
+                </exclusion>
+                <exclusion>
+                  <groupId>xerces</groupId>
+                  <artifactId>xercesImpl</artifactId>
+                </exclusion>
+              </exclusions>
+              <version>${hadoop-two.version}</version>
+            </dependency>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-mapreduce-client-core</artifactId>
+              <scope>provided</scope>
+              <exclusions>
+                <exclusion>
+                  <groupId>com.google.guava</groupId>
+                  <artifactId>guava</artifactId>
+                </exclusion>
+              </exclusions>
+            </dependency>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-auth</artifactId>
+              <scope>provided</scope>
+            </dependency>
+          </dependencies>
+        </profile>
+
+        <!--
+          profile for building against Hadoop 3.0.x. Activate using:
+           mvn -Dhadoop.profile=3.0
+        -->
+        <profile>
+          <id>hadoop-3.0</id>
+          <activation>
+            <property>
+              <name>hadoop.profile</name>
+              <value>3.0</value>
+            </property>
+          </activation>
+          <properties>
+            <hadoop.version>${hadoop-three.version}</hadoop.version>
+          </properties>
+          <dependencies>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-common</artifactId>
+              <scope>provided</scope>
+            </dependency>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-hdfs</artifactId>
+              <scope>provided</scope>
+            </dependency>
+            <dependency>
+              <groupId>org.apache.hadoop</groupId>
+              <artifactId>hadoop-auth</artifactId>
+              <scope>provided</scope>
+            </dependency>
+          </dependencies>
+        </profile>
     </profiles>
 </project>
diff --git a/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/pom.xml b/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/pom.xml
new file mode 100644
index 0000000000..07789f4712
--- /dev/null
+++ b/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/pom.xml
@@ -0,0 +1,215 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+ Licensed under the Apache License, Version 2.0 (the "License");
+ you may not use this file except in compliance with the License.
+ You may obtain a copy of the License at
+   http://www.apache.org/licenses/LICENSE-2.0
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License. See accompanying LICENSE file.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <artifactId>hbase</artifactId>
+    <groupId>org.apache.hbase</groupId>
+    <version>3.0.0-SNAPSHOT</version>
+    <relativePath>../..</relativePath>
+  </parent>
+  <artifactId>hbase-shaded-with-hadoop-check-invariants</artifactId>
+  <packaging>pom</packaging>
+
+  <description>
+  Enforces our invariants for our shaded artifacts. e.g. shaded clients have
+  a specific set of transitive dependencies and shaded clients only contain
+  classes that are in particular packages. Does the enforcement through
+  the maven-enforcer-plugin and integration test.
+  </description>
+  <name>Apache HBase Shaded Packaging Invariants (with Hadoop bundled)</name>
+
+  <properties>
+  </properties>
+
+  <dependencies>
+    <!-- This should only be client facing artifacts that bundle
+         Apache Hadoop related artifacts.
+      -->
+    <dependency>
+      <groupId>org.apache.hbase</groupId>
+      <artifactId>hbase-shaded-client</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <!-- parent pom defines these for children. :( :( :( -->
+    <dependency>
+      <groupId>com.github.stephenc.findbugs</groupId>
+      <artifactId>findbugs-annotations</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>log4j</groupId>
+      <artifactId>log4j</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <!-- Test dependencies -->
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+      <scope>provided</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.mockito</groupId>
+      <artifactId>mockito-core</artifactId>
+      <scope>provided</scope>
+    </dependency>
+  </dependencies>
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-site-plugin</artifactId>
+        <configuration>
+          <skip>true</skip>
+        </configuration>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-enforcer-plugin</artifactId>
+        <dependencies>
+          <dependency>
+            <groupId>org.codehaus.mojo</groupId>
+            <artifactId>extra-enforcer-rules</artifactId>
+            <version>1.0-beta-6</version>
+          </dependency>
+        </dependencies>
+        <executions>
+          <execution>
+            <id>enforce-banned-dependencies</id>
+            <goals>
+              <goal>enforce</goal>
+            </goals>
+            <configuration>
+              <skip>true</skip>
+              <rules>
+                <banTransitiveDependencies>
+<!--
+                  <message>
+    Our client-facing artifacts are not supposed to have additional dependencies
+    and one or more of them do. The output from the enforcer plugin should give
+    specifics.
+                  </message>
+-->
+                  <excludes>
+                    <!-- We leave logging stuff alone -->
+                    <exclude>org.slf4j:*</exclude>
+                    <exclude>log4j:*</exclude>
+                    <exclude>commons-logging:*</exclude>
+                    <!-- annotations that never change -->
+                    <exclude>com.google.code.findbugs:*</exclude>
+                    <exclude>com.github.stephenc.findbugs:*</exclude>
+                    <!-- We leave HTrace as an unshaded dependnecy on purpose so that tracing within a JVM will work -->
+                    <exclude>org.apache.htrace:*</exclude>
+                    <!-- NB we don't exclude Hadoop from this check here, because the assumption is any needed classes
+                         are contained in our artifacts.
+                      -->
+                  </excludes>
+                </banTransitiveDependencies>
+                <banDuplicateClasses>
+                  <findAllDuplicates>true</findAllDuplicates>
+                </banDuplicateClasses>
+              </rules>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-resources-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>test-resources</id>
+            <phase>pre-integration-test</phase>
+            <goals>
+              <goal>testResources</goal>
+            </goals>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <!-- create a maven pom property that has all of our dependencies.
+             below in the integration-test phase we'll pass this list
+             of paths to our jar checker script.
+          -->
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-dependency-plugin</artifactId>
+        <executions>
+          <execution>
+            <id>put-client-artifacts-in-a-property</id>
+            <phase>pre-integration-test</phase>
+            <goals>
+              <goal>build-classpath</goal>
+            </goals>
+            <configuration>
+              <excludeScope>provided</excludeScope>
+              <excludeTransitive>true</excludeTransitive>
+              <outputProperty>hbase-client-artifacts</outputProperty>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+      <plugin>
+        <groupId>org.codehaus.mojo</groupId>
+        <artifactId>exec-maven-plugin</artifactId>
+        <version>1.6.0</version>
+        <executions>
+          <!-- It's easier to have two copies of our validation
+               script than to copy it via remote-resources-plugin, but
+               we need to make sure they stay the same.
+            -->
+          <execution>
+            <id>make-sure-validation-files-are-in-sync</id>
+            <phase>validate</phase>
+            <goals>
+              <goal>exec</goal>
+            </goals>
+            <configuration>
+              <executable>diff</executable>
+              <requiresOnline>false</requiresOnline>
+              <arguments>
+                <argument>../hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh</argument>
+                <argument>../hbase-shaded-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh</argument>
+              </arguments>
+            </configuration>
+          </execution>
+          <!--
+            Check that we actually relocated everything we included.
+            It's critical that we don't ship third party dependencies that haven't
+            been relocated under our package space, since this will lead to
+            difficult to debug classpath errors for downstream. Unfortunately, that
+            means inspecting all the jars.
+            -->
+          <execution>
+            <id>check-jar-contents-for-stuff-with-hadoop</id>
+            <phase>integration-test</phase>
+            <goals>
+              <goal>exec</goal>
+            </goals>
+            <configuration>
+              <executable>${shell-executable}</executable>
+              <workingDirectory>${project.build.testOutputDirectory}</workingDirectory>
+              <requiresOnline>false</requiresOnline>
+              <arguments>
+                <argument>ensure-jars-have-correct-contents.sh</argument>
+                <argument>--allow-hadoop</argument>
+                <argument>${hbase-client-artifacts}</argument>
+              </arguments>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
+    </plugins>
+  </build>
+
+</project>
diff --git a/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh b/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh
new file mode 100644
index 0000000000..a2bb332e3c
--- /dev/null
+++ b/hbase-shaded/hbase-shaded-with-hadoop-check-invariants/src/test/resources/ensure-jars-have-correct-contents.sh
@@ -0,0 +1,129 @@
+#!/usr/bin/env bash
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+set -e
+function usage {
+  echo "Usage: ${0} [options] [/path/to/some/example.jar:/path/to/another/example/created.jar]"
+  echo ""
+  echo "  accepts a single command line argument with a colon separated list of"
+  echo "  paths to jars to check. Iterates through each such passed jar and checks"
+  echo "  all the contained paths to make sure they follow the below constructed"
+  echo "  safe list."
+  echo ""
+  echo "    --allow-hadoop     Include stuff from the Apache Hadoop project in the list"
+  echo "                       of allowed jar contents. default: false"
+  echo "    --debug            print more info to stderr"
+  exit 1
+}
+# if no args specified, show usage
+if [ $# -lt 1 ]; then
+  usage
+fi
+
+# Get arguments
+declare allow_hadoop
+declare debug
+while [ $# -gt 0 ]
+do
+  case "$1" in
+    --allow-hadoop) shift; allow_hadoop="true";;
+    --debug) shift; debug="true";;
+    --) shift; break;;
+    -*) usage ;;
+    *)  break;;  # terminate while loop
+  esac
+done
+
+# should still have jars to check.
+if [ $# -lt 1 ]; then
+  usage
+fi
+if [ -n "${debug}" ]; then
+  echo "[DEBUG] Checking on jars: $@" >&2
+  echo "jar command is: $(which jar)" >&2
+  echo "grep command is: $(which grep)" >&2
+  grep -V >&2 || true
+fi
+
+IFS=: read -r -d '' -a artifact_list < <(printf '%s\0' "$1")
+
+# we have to allow the directories that lead to the hbase dirs
+allowed_expr="(^org/$|^org/apache/$|^org/apache/hadoop/$"
+# We allow the following things to exist in our client artifacts:
+#   * classes in packages that start with org.apache.hadoop.hbase, which by
+#     convention should be in a path that looks like org/apache/hadoop/hbase
+allowed_expr+="|^org/apache/hadoop/hbase"
+#   * classes in packages that start with org.apache.hbase
+allowed_expr+="|^org/apache/hbase/"
+#   * whatever in the "META-INF" directory
+allowed_expr+="|^META-INF/"
+#   * the folding tables from jcodings
+allowed_expr+="|^tables/"
+#   * HBase's default configuration files, which have the form
+#     "_module_-default.xml"
+allowed_expr+="|^hbase-default.xml$"
+# public suffix list used by httpcomponents
+allowed_expr+="|^mozilla/$"
+allowed_expr+="|^mozilla/public-suffix-list.txt$"
+# Comes from commons-configuration, not sure if relocatable.
+allowed_expr+="|^digesterRules.xml$"
+allowed_expr+="|^properties.dtd$"
+allowed_expr+="|^PropertyList-1.0.dtd$"
+
+
+if [ -n "${allow_hadoop}" ]; then
+  #   * classes in packages that start with org.apache.hadoop, which by
+  #     convention should be in a path that looks like org/apache/hadoop
+  allowed_expr+="|^org/apache/hadoop/"
+  #   * Hadoop's default configuration files, which have the form
+  #     "_module_-default.xml"
+  allowed_expr+="|^[^-]*-default.xml$"
+  #   * Hadoop's versioning properties files, which have the form
+  #     "_module_-version-info.properties"
+  allowed_expr+="|^[^-]*-version-info.properties$"
+  #   * Hadoop's application classloader properties file.
+  allowed_expr+="|^org.apache.hadoop.application-classloader.properties$"
+else
+  # We have some classes for integrating with the Hadoop Metrics2 system
+  # that have to be in a particular package space due to access rules.
+  allowed_expr+="|^org/apache/hadoop/metrics2"
+fi
+
+
+allowed_expr+=")"
+declare -i bad_artifacts=0
+declare -a bad_contents
+for artifact in "${artifact_list[@]}"; do
+  bad_contents=($(jar tf "${artifact}" | grep -v -E "${allowed_expr}" || true))
+  if [ ${#bad_contents[@]} -gt 0 ]; then
+    echo "[ERROR] Found artifact with unexpected contents: '${artifact}'"
+    echo "    Please check the following and either correct the build or update"
+    echo "    the allowed list with reasoning."
+    echo ""
+    for bad_line in "${bad_contents[@]}"; do
+      echo "    ${bad_line}"
+    done
+    bad_artifacts=${bad_artifacts}+1
+  else
+    echo "[INFO] Artifact looks correct: '$(basename "${artifact}")'"
+  fi
+done
+
+# if there was atleast one bad artifact, exit with failure
+if [ "${bad_artifacts}" -gt 0 ]; then
+  exit 1
+fi
diff --git a/hbase-shaded/pom.xml b/hbase-shaded/pom.xml
index 24c515844e..93b122fe08 100644
--- a/hbase-shaded/pom.xml
+++ b/hbase-shaded/pom.xml
@@ -42,6 +42,7 @@
         <module>hbase-shaded-client</module>
         <module>hbase-shaded-mapreduce</module>
         <module>hbase-shaded-check-invariants</module>
+        <module>hbase-shaded-with-hadoop-check-invariants</module>
     </modules>
     <dependencies>
       <dependency>
@@ -118,6 +119,7 @@
                     <artifactId>maven-shade-plugin</artifactId>
                     <executions>
                         <execution>
+                            <id>aggregate-into-a-jar-with-relocated-third-parties</id>
                             <phase>package</phase>
                             <goals>
                                 <goal>shade</goal>
@@ -449,12 +451,23 @@
                                       <exclude>META-INF/ECLIPSEF.RSA</exclude>
                                     </excludes>
                                   </filter>
+                                  <filter>
+                                    <!-- Duplication of classes that ship in commons-collections 2.x and 3.x
+                                         If we stop bundling a relevant commons-collections artifact we'll
+                                         need to revisit. See: https://s.apache.org/e09o
+                                    -->
+                                    <artifact>commons-beanutils:commons-beanutils-core</artifact>
+                                    <excludes>
+                                      <exclude>org/apache/commons/collections/*.class</exclude>
+                                    </excludes>
+                                  </filter>
                                   <filter>
                                     <!-- server side webapps that we don't need -->
                                     <artifact>org.apache.hbase:hbase-server</artifact>
                                     <excludes>
                                       <exclude>hbase-webapps/*</exclude>
                                       <exclude>hbase-webapps/**/*</exclude>
+                                      <exclude>**/*_jsp.class</exclude>
                                     </excludes>
                                   </filter>
                                   <filter>
diff --git a/pom.xml b/pom.xml
index f8f1150f29..eb091c0139 100755
--- a/pom.xml
+++ b/pom.xml
@@ -1437,8 +1437,12 @@
     <hadoop.guava.version>11.0.2</hadoop.guava.version>
     <compat.module>hbase-hadoop2-compat</compat.module>
     <assembly.file>src/main/assembly/hadoop-two-compat.xml</assembly.file>
-    <audience-annotations.version>0.5.0</audience-annotations.version>
+    <htrace-hadoop.version>3.1.0-incubating</htrace-hadoop.version>
+    <!--This property is for hadoops netty. HBase netty
+         comes in via hbase-thirdparty hbase-shaded-netty-->
+    <netty.hadoop.version>3.6.2.Final</netty.hadoop.version>
     <!-- end HBASE-15925 default hadoop compatibility values -->
+    <audience-annotations.version>0.5.0</audience-annotations.version>
     <avro.version>1.7.7</avro.version>
     <commons-codec.version>1.10</commons-codec.version>
     <!-- pretty outdated -->
@@ -1466,7 +1470,6 @@
     <junit.version>4.12</junit.version>
     <hamcrest.version>1.3</hamcrest.version>
     <htrace.version>4.2.0-incubating</htrace.version>
-    <htrace-hadoop.version>3.2.0-incubating</htrace-hadoop.version>
     <log4j.version>1.2.17</log4j.version>
     <mockito-core.version>2.1.0</mockito-core.version>
     <!--Internally we use a different version of protobuf. See hbase-protocol-shaded-->
@@ -1594,7 +1597,8 @@
           org.mortbay.jetty:servlet-api, javax.servlet:servlet-api: These are excluded because they are
           the same implementations. I chose org.mortbay.jetty:servlet-api-2.5 instead, which is a third
           implementation of the same, because Hadoop also uses this version
-          javax.servlet:jsp-api in favour of org.mortbay.jetty:jsp-api-2.1
+          javax.servlet:jsp-api in favour of javax.servlet.jsp:javax.servlet.jsp-api:2.3.1 since it
+          is what glassfish's jspC jar uses and that's where we get our own need for a jsp-api.
         -->
       <!-- Intra-module dependencies -->
       <dependency>
@@ -1909,6 +1913,14 @@
         <artifactId>commons-math3</artifactId>
         <version>${commons-math.version}</version>
       </dependency>
+      <dependency>
+        <!-- commons-logging is only used by hbase-http's HttpRequestLog and hbase-server's
+             HBaseTestingUtil.
+          -->
+        <groupId>commons-logging</groupId>
+        <artifactId>commons-logging</artifactId>
+        <version>1.2</version>
+      </dependency>
       <dependency>
         <groupId>org.apache.zookeeper</groupId>
         <artifactId>zookeeper</artifactId>
@@ -1972,6 +1984,16 @@
         <artifactId>jackson-jaxrs-json-provider</artifactId>
         <version>${jackson.version}</version>
       </dependency>
+      <dependency>
+        <groupId>com.fasterxml.jackson.core</groupId>
+        <artifactId>jackson-annotations</artifactId>
+        <version>${jackson.version}</version>
+      </dependency>
+      <dependency>
+        <groupId>com.fasterxml.jackson.core</groupId>
+        <artifactId>jackson-core</artifactId>
+        <version>${jackson.version}</version>
+      </dependency>
       <dependency>
         <groupId>com.fasterxml.jackson.core</groupId>
         <artifactId>jackson-databind</artifactId>
@@ -2075,6 +2097,12 @@
         <artifactId>javax.servlet.jsp</artifactId>
         <version>${glassfish.jsp.version}</version>
       </dependency>
+      <dependency>
+        <!-- this lib is used by the compiled Jsp from the above JspC -->
+        <groupId>javax.servlet.jsp</groupId>
+        <artifactId>javax.servlet.jsp-api</artifactId>
+        <version>2.3.1</version>
+      </dependency>
       <dependency>
         <groupId>org.glassfish</groupId>
         <artifactId>javax.el</artifactId>
@@ -2472,6 +2500,7 @@
         <!--This property is for hadoops netty. HBase netty
              comes in via hbase-thirdparty hbase-shaded-netty-->
         <netty.hadoop.version>3.6.2.Final</netty.hadoop.version>
+        <htrace-hadoop.version>3.1.0-incubating</htrace-hadoop.version>
       </properties>
       <dependencyManagement>
         <dependencies>
@@ -2749,6 +2778,7 @@
         <!--This property is for hadoops netty. HBase netty
              comes in via hbase-thirdparty hbase-shaded-netty-->
         <netty.hadoop.version>3.10.5.Final</netty.hadoop.version>
+        <htrace-hadoop.version>4.1.0-incubating</htrace-hadoop.version>
       </properties>
      <dependencyManagement>
        <dependencies>
-- 
2.16.1

