diff --git hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
index e36ee43..b678f55 100644
--- hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
+++ hbase-examples/src/main/java/org/apache/hadoop/hbase/coprocessor/example/BulkDeleteEndpoint.java
@@ -49,7 +49,6 @@ import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.regionserver.OperationStatus;
 import org.apache.hadoop.hbase.regionserver.RegionScanner;
 import org.apache.hadoop.hbase.util.Bytes;
-import org.apache.hadoop.hbase.util.Pair;
 
 import com.google.protobuf.RpcCallback;
 import com.google.protobuf.RpcController;
@@ -146,20 +145,19 @@ public class BulkDeleteEndpoint extends BulkDeleteService implements Coprocessor
           }
         }
         if (deleteRows.size() > 0) {
-          Pair<Mutation, Integer>[] deleteWithLockArr = new Pair[deleteRows.size()];
+          Mutation[] deleteArr = new Mutation[deleteRows.size()];
           int i = 0;
           for (List<KeyValue> deleteRow : deleteRows) {
-            Delete delete = createDeleteMutation(deleteRow, deleteType, timestamp);
-            deleteWithLockArr[i++] = new Pair<Mutation, Integer>(delete, null);
+            deleteArr[i++] = createDeleteMutation(deleteRow, deleteType, timestamp);
           }
-          OperationStatus[] opStatus = region.batchMutate(deleteWithLockArr);
+          OperationStatus[] opStatus = region.batchMutate(deleteArr);
           for (i = 0; i < opStatus.length; i++) {
             if (opStatus[i].getOperationStatusCode() != OperationStatusCode.SUCCESS) {
               break;
             }
             totalRowsDeleted++;
             if (deleteType == DeleteType.VERSION) {
-              byte[] versionsDeleted = deleteWithLockArr[i].getFirst().getAttribute(
+              byte[] versionsDeleted = deleteArr[i].getAttribute(
                   NO_OF_VERSIONS_TO_DELETE);
               if (versionsDeleted != null) {
                 totalVersionsDeleted += Bytes.toInt(versionsDeleted);
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
index 9ea9ffd..6839aef 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java
@@ -253,12 +253,12 @@ public abstract class BaseRegionObserver implements RegionObserver {
   
   @Override
   public void preBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> c,
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
   }
 
   @Override
   public void postBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> c,
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
   }
 
   @Override
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
index 91b6eae..9f3cb8d 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java
@@ -554,7 +554,7 @@ public interface RegionObserver extends Coprocessor {
    * @throws IOException if an error occurred on the coprocessor
    */
   void preBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> c,
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException;
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException;
 
   /**
    * This will be called after applying a batch of Mutations on a region. The Mutations are added to
@@ -564,7 +564,7 @@ public interface RegionObserver extends Coprocessor {
    * @throws IOException if an error occurred on the coprocessor
    */
   void postBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> c,
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException;
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException;
 
   /**
    * Called before checkAndPut
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index 7f21e3a..2c7aeea 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -34,7 +34,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.NavigableMap;
 import java.util.NavigableSet;
-import java.util.Random;
 import java.util.Set;
 import java.util.TreeMap;
 import java.util.UUID;
@@ -139,7 +138,6 @@ import org.apache.hadoop.util.StringUtils;
 import org.cliffc.high_scale_lib.Counter;
 
 import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.io.Closeables;
 import com.google.protobuf.Descriptors;
@@ -217,12 +215,13 @@ public class HRegion implements HeapSize { // , Writable{
   // Members
   //////////////////////////////////////////////////////////////////////////////
 
-  private final ConcurrentHashMap<HashedBytes, CountDownLatch> lockedRows =
-    new ConcurrentHashMap<HashedBytes, CountDownLatch>();
-  private final ConcurrentHashMap<Integer, HashedBytes> lockIds =
-    new ConcurrentHashMap<Integer, HashedBytes>();
-  private final AtomicInteger lockIdGenerator = new AtomicInteger(1);
-  static private Random rand = new Random();
+  private final ConcurrentHashMap<HashedBytes, RowLockContext> lockedRows =
+      new ConcurrentHashMap<HashedBytes, RowLockContext>();
+  private final ThreadLocal<List<HashedBytes>> locksHeldByThread =
+      new ThreadLocal<List<HashedBytes>>() {
+        protected List<HashedBytes> initialValue() {
+          return new ArrayList<HashedBytes>();
+        };};
 
   protected final Map<byte[], Store> stores = new ConcurrentSkipListMap<byte[], Store>(
       Bytes.BYTES_RAWCOMPARATOR);
@@ -1751,7 +1750,7 @@ public class HRegion implements HeapSize { // , Writable{
     try {
       delete.getRow();
       // All edits for the given row (across all column families) must happen atomically.
-      doBatchMutate(delete, null);
+      doBatchMutate(delete);
     } finally {
       closeRegionOperation();
     }
@@ -1774,7 +1773,7 @@ public class HRegion implements HeapSize { // , Writable{
     delete.setFamilyMap(familyMap);
     delete.setClusterId(clusterId);
     delete.setDurability(durability);
-    doBatchMutate(delete, null);
+    doBatchMutate(delete);
   }
 
   /**
@@ -1849,7 +1848,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     try {
       // All edits for the given row (across all column families) must happen atomically.
-      doBatchMutate(put, null);
+      doBatchMutate(put);
     } finally {
       closeRegionOperation();
     }
@@ -1879,46 +1878,37 @@ public class HRegion implements HeapSize { // , Writable{
   }
 
   /**
-   * Perform a batch put with no pre-specified locks
-   * @see HRegion#batchMutate(Pair[])
+   * Perform a batch put
+   * @see HRegion#batchMutate(Mutation[])
    */
   public OperationStatus[] put(Put[] puts) throws IOException {
-    @SuppressWarnings("unchecked")
-    Pair<Mutation, Integer> putsAndLocks[] = new Pair[puts.length];
-
-    for (int i = 0; i < puts.length; i++) {
-      putsAndLocks[i] = new Pair<Mutation, Integer>(puts[i], null);
-    }
-    return batchMutate(putsAndLocks);
+    return batchMutate(puts);
   }
 
   /**
    * Perform a batch of mutations.
    * It supports only Put and Delete mutations and will ignore other types passed.
-   * @param mutationsAndLocks
-   *          the list of mutations paired with their requested lock IDs.
+   * @param mutations the list of mutations
    * @return an array of OperationStatus which internally contains the
    *         OperationStatusCode and the exceptionMessage if any.
    * @throws IOException
    */
-  public OperationStatus[] batchMutate(
-      Pair<Mutation, Integer>[] mutationsAndLocks) throws IOException {
-    return batchMutate(mutationsAndLocks, false);
+  public OperationStatus[] batchMutate(Mutation[] mutations) throws IOException {
+    return batchMutate(mutations, false);
   }
  
   /**
    * Perform a batch of mutations.
    * It supports only Put and Delete mutations and will ignore other types passed.
-   * @param mutationsAndLocks
-   *          the list of mutations paired with their requested lock IDs.
+   * @param mutations the list of mutations
    * @return an array of OperationStatus which internally contains the
    *         OperationStatusCode and the exceptionMessage if any.
    * @throws IOException
    */
-  OperationStatus[] batchMutate(Pair<Mutation, Integer>[] mutationsAndLocks, boolean isReplay)
+  OperationStatus[] batchMutate(Mutation[] mutations, boolean isReplay)
       throws IOException {
-    BatchOperationInProgress<Pair<Mutation, Integer>> batchOp =
-      new BatchOperationInProgress<Pair<Mutation,Integer>>(mutationsAndLocks);
+    BatchOperationInProgress<Mutation> batchOp =
+      new BatchOperationInProgress<Mutation>(mutations);
 
     boolean initialized = false;
 
@@ -1956,14 +1946,13 @@ public class HRegion implements HeapSize { // , Writable{
   }
   
 
-  private void doPreMutationHook(BatchOperationInProgress<Pair<Mutation, Integer>> batchOp)
+  private void doPreMutationHook(BatchOperationInProgress<Mutation> batchOp)
       throws IOException {
     /* Run coprocessor pre hook outside of locks to avoid deadlock */
     WALEdit walEdit = new WALEdit();
     if (coprocessorHost != null) {
       for (int i = 0 ; i < batchOp.operations.length; i++) {
-        Pair<Mutation, Integer> nextPair = batchOp.operations[i];
-        Mutation m = nextPair.getFirst();
+        Mutation m = batchOp.operations[i];
         if (m instanceof Put) {
           if (coprocessorHost.prePut((Put) m, walEdit, m.getDurability())) {
             // pre hook says skip this Put
@@ -1992,7 +1981,7 @@ public class HRegion implements HeapSize { // , Writable{
   }
 
   @SuppressWarnings("unchecked")
-  private long doMiniBatchMutation(BatchOperationInProgress<Pair<Mutation, Integer>> batchOp,
+  private long doMiniBatchMutation(BatchOperationInProgress<Mutation> batchOp,
       boolean isInReplay) throws IOException {
 
     // variable to note if all Put items are for the same CF -- metrics related
@@ -2010,8 +1999,6 @@ public class HRegion implements HeapSize { // , Writable{
     boolean walSyncSuccessful = false;
     boolean locked = false;
 
-    /** Keep track of the locks we hold so we can release them in finally clause */
-    List<Integer> acquiredLocks = Lists.newArrayListWithCapacity(batchOp.operations.length);
     // reference family maps directly so coprocessors can mutate them if desired
     Map<byte[], List<? extends Cell>>[] familyMaps = new Map[batchOp.operations.length];
     // We try to set up a batch in the range [firstIndex,lastIndexExclusive)
@@ -2027,10 +2014,8 @@ public class HRegion implements HeapSize { // , Writable{
       int numReadyToWrite = 0;
       long now = EnvironmentEdgeManager.currentTimeMillis();
       while (lastIndexExclusive < batchOp.operations.length) {
-        Pair<Mutation, Integer> nextPair = batchOp.operations[lastIndexExclusive];
-        Mutation mutation = nextPair.getFirst();
+        Mutation mutation = batchOp.operations[lastIndexExclusive];
         boolean isPutMutation = mutation instanceof Put;
-        Integer providedLockId = nextPair.getSecond();
 
         Map<byte[], List<? extends Cell>> familyMap = mutation.getFamilyMap();
         // store the family map reference to allow for mutations
@@ -2068,25 +2053,27 @@ public class HRegion implements HeapSize { // , Writable{
           lastIndexExclusive++;
           continue;
         }
+        
         // If we haven't got any rows in our batch, we should block to
         // get the next one.
         boolean shouldBlock = numReadyToWrite == 0;
-        Integer acquiredLockId = null;
-        try {
-          acquiredLockId = getLock(providedLockId, mutation.getRow(),
-              shouldBlock);
-        } catch (IOException ioe) {
-          LOG.warn("Failed getting lock in batch put, row="
-                  + Bytes.toStringBinary(mutation.getRow()), ioe);
+        boolean acquiredLock = false;
+        if (shouldBlock) {
+          try {
+            getRowLock(mutation.getRow());
+            acquiredLock = true;
+          } catch (IOException ioe) {
+            LOG.warn("Failed getting lock in batch put, row=" + Bytes.toStringBinary(mutation.getRow()), ioe);
+          }
+        } else {
+          acquiredLock = tryRowLock(mutation.getRow());
         }
-        if (acquiredLockId == null) {
+        if (!acquiredLock) {
           // We failed to grab another lock
           assert !shouldBlock : "Should never fail to get lock when blocking";
           break; // stop acquiring more rows for this batch
         }
-        if (providedLockId == null) {
-          acquiredLocks.add(acquiredLockId);
-        }
+
         lastIndexExclusive++;
         numReadyToWrite++;
 
@@ -2128,7 +2115,7 @@ public class HRegion implements HeapSize { // , Writable{
         if (batchOp.retCodeDetails[i].getOperationStatusCode()
             != OperationStatusCode.NOT_RUN) continue;
 
-        Mutation mutation = batchOp.operations[i].getFirst();
+        Mutation mutation = batchOp.operations[i];
         if (mutation instanceof Put) {
           updateKVTimestamps(familyMaps[i].values(), byteNow);
           noOfPuts++;
@@ -2149,8 +2136,8 @@ public class HRegion implements HeapSize { // , Writable{
 
       // calling the pre CP hook for batch mutation
       if (!isInReplay && coprocessorHost != null) {
-        MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp = 
-          new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(batchOp.operations, 
+        MiniBatchOperationInProgress<Mutation> miniBatchOp = 
+          new MiniBatchOperationInProgress<Mutation>(batchOp.operations, 
           batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
         if (coprocessorHost.preBatchMutate(miniBatchOp)) return 0L;
       }
@@ -2185,7 +2172,7 @@ public class HRegion implements HeapSize { // , Writable{
         }
         batchOp.retCodeDetails[i] = OperationStatus.SUCCESS;
 
-        Mutation m = batchOp.operations[i].getFirst();
+        Mutation m = batchOp.operations[i];
         Durability tmpDur = m.getDurability();
         if (tmpDur.ordinal() > durability.ordinal()) {
           durability = tmpDur;
@@ -2208,9 +2195,9 @@ public class HRegion implements HeapSize { // , Writable{
       // -------------------------
       // STEP 5. Append the edit to WAL. Do not sync wal.
       // -------------------------
-      Mutation first = batchOp.operations[firstIndex].getFirst();
+      Mutation mutation = batchOp.operations[firstIndex];
       txid = this.log.appendNoSync(this.getRegionInfo(), this.htableDescriptor.getName(),
-               walEdit, first.getClusterId(), now, this.htableDescriptor);
+               walEdit, mutation.getClusterId(), now, this.htableDescriptor);
 
       // -------------------------------
       // STEP 6. Release row locks, etc.
@@ -2219,12 +2206,8 @@ public class HRegion implements HeapSize { // , Writable{
         this.updatesLock.readLock().unlock();
         locked = false;
       }
-      if (acquiredLocks != null) {
-        for (Integer toRelease : acquiredLocks) {
-          releaseRowLock(toRelease);
-        }
-        acquiredLocks = null;
-      }
+      releaseMyRowLocks();
+      
       // -------------------------
       // STEP 7. Sync wal.
       // -------------------------
@@ -2234,8 +2217,8 @@ public class HRegion implements HeapSize { // , Writable{
       walSyncSuccessful = true;
       // calling the post CP hook for batch mutation
       if (!isInReplay && coprocessorHost != null) {
-        MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp = 
-          new MiniBatchOperationInProgress<Pair<Mutation, Integer>>(batchOp.operations, 
+        MiniBatchOperationInProgress<Mutation> miniBatchOp = 
+          new MiniBatchOperationInProgress<Mutation>(batchOp.operations, 
           batchOp.retCodeDetails, batchOp.walEditsFromCoprocessors, firstIndex, lastIndexExclusive);
         coprocessorHost.postBatchMutate(miniBatchOp);
       }
@@ -2259,7 +2242,7 @@ public class HRegion implements HeapSize { // , Writable{
               != OperationStatusCode.SUCCESS) {
             continue;
           }
-          Mutation m = batchOp.operations[i].getFirst();
+          Mutation m = batchOp.operations[i];
           if (m instanceof Put) {
             coprocessorHost.postPut((Put) m, walEdit, m.getDurability());
           } else {
@@ -2282,11 +2265,7 @@ public class HRegion implements HeapSize { // , Writable{
         this.updatesLock.readLock().unlock();
       }
 
-      if (acquiredLocks != null) {
-        for (Integer toRelease : acquiredLocks) {
-          releaseRowLock(toRelease);
-        }
-      }
+      releaseMyRowLocks();
 
       // See if the column families were consistent through the whole thing.
       // if they were then keep them. If they were not then pass a null.
@@ -2356,7 +2335,7 @@ public class HRegion implements HeapSize { // , Writable{
       get.addColumn(family, qualifier);
 
       // Lock row
-      Integer lid = getLock(null, get.getRow(), true);
+      getRowLock(get.getRow());
       // wait for all previous transactions to complete (with lock held)
       mvcc.completeMemstoreInsert(mvcc.beginMemstoreInsert());
       List<KeyValue> result = null;
@@ -2402,27 +2381,23 @@ public class HRegion implements HeapSize { // , Writable{
         if (matches) {
           // All edits for the given row (across all column families) must
           // happen atomically.
-          doBatchMutate((Mutation)w, lid);
+          doBatchMutate((Mutation)w);
           this.checkAndMutateChecksPassed.increment();
           return true;
         }
         this.checkAndMutateChecksFailed.increment();
         return false;
       } finally {
-        releaseRowLock(lid);
+        releaseMyRowLocks();
       }
     } finally {
       closeRegionOperation();
     }
   }
 
-  @SuppressWarnings("unchecked")
-  private void doBatchMutate(Mutation mutation, Integer lid) throws IOException,
+  private void doBatchMutate(Mutation mutation) throws IOException,
       org.apache.hadoop.hbase.exceptions.DoNotRetryIOException {
-    Pair<Mutation, Integer>[] mutateWithLocks = new Pair[] {
-      new Pair<Mutation, Integer>(mutation, lid)
-    };
-    OperationStatus[] batchMutate = this.batchMutate(mutateWithLocks);
+    OperationStatus[] batchMutate = this.batchMutate(new Mutation[] { mutation });
     if (batchMutate[0].getOperationStatusCode().equals(OperationStatusCode.SANITY_CHECK_FAILURE)) {
       throw new FailedSanityCheckException(batchMutate[0].getExceptionMsg());
     } else if (batchMutate[0].getOperationStatusCode().equals(OperationStatusCode.BAD_FAMILY)) {
@@ -2598,7 +2573,7 @@ public class HRegion implements HeapSize { // , Writable{
     Put p = new Put(row);
     p.setFamilyMap(familyMap);
     p.setClusterId(HConstants.DEFAULT_CLUSTER_ID);
-    doBatchMutate(p, null);
+    doBatchMutate(p);
   }
 
   /**
@@ -2649,7 +2624,7 @@ public class HRegion implements HeapSize { // , Writable{
    * called when a Put/Delete has updated memstore but subequently fails to update
    * the wal. This method is then invoked to rollback the memstore.
    */
-  private void rollbackMemstore(BatchOperationInProgress<Pair<Mutation, Integer>> batchOp,
+  private void rollbackMemstore(BatchOperationInProgress<Mutation> batchOp,
                                 Map<byte[], List<? extends Cell>>[] familyMaps,
                                 int start, int end) {
     int kvsRolledback = 0;
@@ -3159,59 +3134,35 @@ public class HRegion implements HeapSize { // , Writable{
   }
 
   /**
-   * Obtain a lock on the given row.  Blocks until success.
-   *
-   * I know it's strange to have two mappings:
-   * <pre>
-   *   ROWS  ==> LOCKS
-   * </pre>
-   * as well as
-   * <pre>
-   *   LOCKS ==> ROWS
-   * </pre>
-   * <p>It would be more memory-efficient to just have one mapping;
-   * maybe we'll do that in the future.
-   *
-   * @param row Name of row to lock.
-   * @throws IOException
-   * @return The id of the held lock.
-   */
-  public Integer obtainRowLock(final byte [] row) throws IOException {
-    startRegionOperation();
-    this.writeRequestsCount.increment();
-    try {
-      return internalObtainRowLock(row, true);
-    } finally {
-      closeRegionOperation();
-    }
-  }
-
-  /**
    * Obtains or tries to obtain the given row lock.
    * @param waitForLock if true, will block until the lock is available.
    *        Otherwise, just tries to obtain the lock and returns
-   *        null if unavailable.
+   *        false if unavailable.
+   * @return true if the lock was obtained, false if waitForLock was false and the lock was not obtained
+   * @throws IOException if waitForLock was true and the lock could not be obtained after waiting
    */
-  private Integer internalObtainRowLock(final byte[] row, boolean waitForLock)
+  private boolean internalObtainRowLock(final byte[] row, boolean waitForLock)
   throws IOException {
     checkRow(row, "row lock");
     startRegionOperation();
     try {
       HashedBytes rowKey = new HashedBytes(row);
       CountDownLatch rowLatch = new CountDownLatch(1);
+      RowLockContext rowLockContext = new RowLockContext(rowLatch, Thread.currentThread());
 
       // loop until we acquire the row lock (unless !waitForLock)
       while (true) {
-        CountDownLatch existingLatch = lockedRows.putIfAbsent(rowKey, rowLatch);
-        if (existingLatch == null) {
+          RowLockContext existingContext = lockedRows.putIfAbsent(rowKey, rowLockContext);
+          // If the row is already locked by this same thread we acquired it
+          if (existingContext == null || existingContext.thread == Thread.currentThread()) {
           break;
         } else {
-          // row already locked
+          // row already locked by some other thread
           if (!waitForLock) {
-            return null;
+            return false;
           }
           try {
-            if (!existingLatch.await(this.rowLockWaitDuration,
+            if (!existingContext.latch.await(this.rowLockWaitDuration,
                             TimeUnit.MILLISECONDS)) {
               throw new IOException("Timed out on getting lock for row="
                   + Bytes.toStringBinary(row));
@@ -3225,72 +3176,48 @@ public class HRegion implements HeapSize { // , Writable{
         }
       }
 
-      // loop until we generate an unused lock id
-      while (true) {
-        Integer lockId = lockIdGenerator.incrementAndGet();
-        HashedBytes existingRowKey = lockIds.putIfAbsent(lockId, rowKey);
-        if (existingRowKey == null) {
-          return lockId;
-        } else {
-          // lockId already in use, jump generator to a new spot
-          lockIdGenerator.set(rand.nextInt());
-        }
-      }
+      locksHeldByThread.get().add(rowKey);
+      return true;
     } finally {
       closeRegionOperation();
     }
   }
 
   /**
-   * Release the row lock!
-   * @param lockId  The lock ID to release.
+   * Releases all row locks held by the current thread.
    */
-  public void releaseRowLock(final Integer lockId) {
-    if (lockId == null) return; // null lock id, do nothing
-    HashedBytes rowKey = lockIds.remove(lockId);
-    if (rowKey == null) {
-      LOG.warn("Release unknown lockId: " + lockId);
-      return;
-    }
-    CountDownLatch rowLatch = lockedRows.remove(rowKey);
-    if (rowLatch == null) {
-      LOG.error("Releases row not locked, lockId: " + lockId + " row: "
-          + rowKey);
-      return;
+  public void releaseMyRowLocks() {
+    List<HashedBytes> locksHeld = locksHeldByThread.get();
+    for (HashedBytes rowKey : locksHeld) {
+      RowLockContext rowLockContext = lockedRows.remove(rowKey);
+      if (rowLockContext == null) {
+        LOG.error("Internal row lcok state inconsistent, should not happen, row: " + rowKey);
+        continue;
+      }
+      rowLockContext.latch.countDown();
     }
-    rowLatch.countDown();
+    locksHeld.clear();
   }
 
   /**
-   * See if row is currently locked.
-   * @param lockId
-   * @return boolean
+   * Acqures a lock on the given row.
+   * @throws IOException if the lock could not be obtained after waiting
    */
-  boolean isRowLocked(final Integer lockId) {
-    return lockIds.containsKey(lockId);
+  public void getRowLock(byte[] row) throws IOException {
+    internalObtainRowLock(row, true);
   }
 
   /**
-   * Returns existing row lock if found, otherwise
-   * obtains a new row lock and returns it.
-   * @param lockid requested by the user, or null if the user didn't already hold lock
-   * @param row the row to lock
-   * @param waitForLock if true, will block until the lock is available, otherwise will
-   * simply return null if it could not acquire the lock.
-   * @return lockid or null if waitForLock is false and the lock was unavailable.
+   * public void getRowLock(byte [] row) throws IOException {
+   * internalObtainRowLock(row, true);
    */
-  public Integer getLock(Integer lockid, byte [] row, boolean waitForLock)
-  throws IOException {
-    Integer lid = null;
-    if (lockid == null) {
-      lid = internalObtainRowLock(row, waitForLock);
-    } else {
-      if (!isRowLocked(lockid)) {
-        throw new IOException("Invalid row lock");
-      }
-      lid = lockid;
+  public boolean tryRowLock(byte[] row) {
+    try {
+      return internalObtainRowLock(row, false);
+    } catch (IOException e) {
+      LOG.error("Unexpected exception trying lock without wait", e);
+      return false;
     }
-    return lid;
   }
 
   /**
@@ -4558,24 +4485,17 @@ public class HRegion implements HeapSize { // , Writable{
     MultiVersionConsistencyControl.WriteEntry writeEntry = null;
     boolean locked = false;
     boolean walSyncSuccessful = false;
-    List<Integer> acquiredLocks = null;
     long addedSize = 0;
     List<KeyValue> mutations = new ArrayList<KeyValue>();
     Collection<byte[]> rowsToLock = processor.getRowsToLock();
     try {
       // 2. Acquire the row lock(s)
-      acquiredLocks = new ArrayList<Integer>(rowsToLock.size());
       for (byte[] row : rowsToLock) {
         // Attempt to lock all involved rows, fail if one lock times out
-        Integer lid = getLock(null, row, true);
-        if (lid == null) {
-          throw new IOException("Failed to acquire lock on "
-              + Bytes.toStringBinary(row));
-        }
-        acquiredLocks.add(lid);
+        getRowLock(row);
       }
       // 3. Region lock
-      lock(this.updatesLock.readLock(), acquiredLocks.size());
+      lock(this.updatesLock.readLock(), rowsToLock.size());
       locked = true;
 
       long now = EnvironmentEdgeManager.currentTimeMillis();
@@ -4610,12 +4530,7 @@ public class HRegion implements HeapSize { // , Writable{
           }
 
           // 9. Release row lock(s)
-          if (acquiredLocks != null) {
-            for (Integer lid : acquiredLocks) {
-              releaseRowLock(lid);
-            }
-            acquiredLocks = null;
-          }
+          releaseMyRowLocks();
           // 10. Sync edit log
           if (txid != 0) {
             syncOrDefer(txid, processor.useDurability());
@@ -4640,12 +4555,8 @@ public class HRegion implements HeapSize { // , Writable{
           this.updatesLock.readLock().unlock();
           locked = false;
         }
-        if (acquiredLocks != null) {
-          for (Integer lid : acquiredLocks) {
-            releaseRowLock(lid);
-          }
-        }
-
+        // release locks if some were acquired but another timed out
+        releaseMyRowLocks();
       }
 
       // 12. Run post-process hook
@@ -4740,7 +4651,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(null, row, true);
+      getRowLock(row);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
@@ -4857,7 +4768,7 @@ public class HRegion implements HeapSize { // , Writable{
         flush = isFlushSize(size);
       } finally {
         this.updatesLock.readLock().unlock();
-        releaseRowLock(lid);
+        releaseMyRowLocks();
       }
       if (writeToWAL) {
         // sync the transaction log outside the rowlock
@@ -4909,7 +4820,7 @@ public class HRegion implements HeapSize { // , Writable{
     this.writeRequestsCount.increment();
     WriteEntry w = null;
     try {
-      Integer lid = getLock(null, row, true);
+      getRowLock(row);
       lock(this.updatesLock.readLock());
       // wait for all prior MVCC transactions to finish - while we hold the row lock
       // (so that we are guaranteed to see the latest state)
@@ -5002,7 +4913,7 @@ public class HRegion implements HeapSize { // , Writable{
         flush = isFlushSize(size);
       } finally {
         this.updatesLock.readLock().unlock();
-        releaseRowLock(lid);
+        releaseMyRowLocks();
       }
       if (writeToWAL) {
         // sync the transaction log outside the rowlock
@@ -5629,4 +5540,14 @@ public class HRegion implements HeapSize { // , Writable{
      */
     void failedBulkLoad(byte[] family, String srcPath) throws IOException;
   }
+  
+  static class RowLockContext {
+    private CountDownLatch latch;
+    private Thread thread;
+
+    public RowLockContext(CountDownLatch latch, Thread thread) {
+      this.latch = latch;
+      this.thread = thread;
+    }
+  }
 }
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
index cd80bab..75c2aa6 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java
@@ -108,16 +108,15 @@ import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
 import org.apache.hadoop.hbase.fs.HFileSystem;
 import org.apache.hadoop.hbase.io.hfile.CacheConfig;
 import org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler;
+import org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController;
 import org.apache.hadoop.hbase.ipc.RpcClient;
 import org.apache.hadoop.hbase.ipc.RpcServer;
-import org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController;
-import org.apache.hadoop.hbase.ipc.RpcServerInterface;
 import org.apache.hadoop.hbase.ipc.RpcServer.BlockingServiceAndInterface;
+import org.apache.hadoop.hbase.ipc.RpcServerInterface;
 import org.apache.hadoop.hbase.ipc.ServerRpcController;
 import org.apache.hadoop.hbase.master.SplitLogManager;
 import org.apache.hadoop.hbase.master.TableLockManager;
 import org.apache.hadoop.hbase.protobuf.ProtobufUtil;
-import org.apache.hadoop.hbase.protobuf.ReplicationProtbufUtil;
 import org.apache.hadoop.hbase.protobuf.RequestConverter;
 import org.apache.hadoop.hbase.protobuf.ResponseConverter;
 import org.apache.hadoop.hbase.protobuf.generated.AdminProtos;
@@ -171,9 +170,9 @@ import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ResultCellMeta;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanRequest;
 import org.apache.hadoop.hbase.protobuf.generated.ClientProtos.ScanResponse;
 import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos;
+import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.Coprocessor;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.NameStringPair;
-import org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.RegionLoad;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier;
 import org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.RegionSpecifier.RegionSpecifierType;
 import org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.GetLastFlushedSequenceIdRequest;
@@ -3923,8 +3922,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
    */
   protected void doBatchOp(final MultiResponse.Builder builder, final HRegion region,
       final List<MutationProto> mutations, final CellScanner cells, boolean isReplay) {
-    @SuppressWarnings("unchecked")
-    Pair<Mutation, Integer>[] mutationsWithLocks = new Pair[mutations.size()];
+    Mutation[] mArray = new Mutation[mutations.size()];
     long before = EnvironmentEdgeManager.currentTimeMillis();
     boolean batchContainsPuts = false, batchContainsDelete = false;
     try {
@@ -3941,7 +3939,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
           mutation = ProtobufUtil.toDelete(m, cells);
           batchContainsDelete = true;
         }
-        mutationsWithLocks[i++] = new Pair<Mutation, Integer>(mutation, null);
+        mArray[i++] = mutation;
         builder.addResult(result);
       }
 
@@ -3950,7 +3948,7 @@ public class HRegionServer implements ClientProtos.ClientService.BlockingInterfa
         cacheFlusher.reclaimMemStoreMemory();
       }
 
-      OperationStatus codes[] = region.batchMutate(mutationsWithLocks, isReplay);
+      OperationStatus codes[] = region.batchMutate(mArray);
       for (i = 0; i < codes.length; i++) {
         switch (codes[i].getOperationStatusCode()) {
           case BAD_FAMILY:
diff --git hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
index 9102aaa..4fe11f4 100644
--- hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
+++ hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java
@@ -993,7 +993,7 @@ public class RegionCoprocessorHost
    * @throws IOException
    */
   public boolean preBatchMutate(
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
     boolean bypass = false;
     ObserverContext<RegionCoprocessorEnvironment> ctx = null;
     for (RegionEnvironment env : coprocessors) {
@@ -1018,7 +1018,7 @@ public class RegionCoprocessorHost
    * @throws IOException
    */
   public void postBatchMutate(
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
     ObserverContext<RegionCoprocessorEnvironment> ctx = null;
     for (RegionEnvironment env : coprocessors) {
       if (env.getInstance() instanceof RegionObserver) {
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
index 0a588af..d6c0c97 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/coprocessor/SimpleRegionObserver.java
@@ -407,7 +407,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
   
   @Override
   public void preBatchMutate(ObserverContext<RegionCoprocessorEnvironment> c,
-      MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
     RegionCoprocessorEnvironment e = c.getEnvironment();
     assertNotNull(e);
     assertNotNull(e.getRegion());
@@ -417,7 +417,7 @@ public class SimpleRegionObserver extends BaseRegionObserver {
 
   @Override
   public void postBatchMutate(final ObserverContext<RegionCoprocessorEnvironment> c,
-      final MiniBatchOperationInProgress<Pair<Mutation, Integer>> miniBatchOp) throws IOException {
+      final MiniBatchOperationInProgress<Mutation> miniBatchOp) throws IOException {
     RegionCoprocessorEnvironment e = c.getEnvironment();
     assertNotNull(e);
     assertNotNull(e.getRegion());
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
index 6d4cbe6..67f03e8 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestAtomicOperation.java
@@ -59,11 +59,8 @@ import org.apache.hadoop.hbase.io.HeapSize;
 import org.apache.hadoop.hbase.regionserver.wal.HLog;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManagerTestHelper;
-import org.apache.hadoop.hbase.util.Pair;
 import org.junit.experimental.categories.Category;
 
-import com.google.common.collect.Lists;
-
 
 /**
  * Testing of HRegion.incrementColumnValue, HRegion.increment,
@@ -528,16 +525,12 @@ public class TestAtomicOperation extends HBaseTestCase {
     final MockHRegion region = (MockHRegion) TestHRegion.initHRegion(
         Bytes.toBytes(tableName), tableName, conf, Bytes.toBytes(family));
 
-    List<Pair<Mutation, Integer>> putsAndLocks = Lists.newArrayList();
     Put[] puts = new Put[1];
     Put put = new Put(Bytes.toBytes("r1"));
     put.add(Bytes.toBytes(family), Bytes.toBytes("q1"), Bytes.toBytes("10"));
     puts[0] = put;
-    Pair<Mutation, Integer> pair = new Pair<Mutation, Integer>(puts[0], null);
-
-    putsAndLocks.add(pair);
-
-    region.batchMutate(putsAndLocks.toArray(new Pair[0]));
+    
+    region.batchMutate(puts);
     MultithreadedTestUtil.TestContext ctx =
       new MultithreadedTestUtil.TestContext(conf);
     ctx.addThread(new PutThread(ctx, region));
@@ -565,15 +558,12 @@ public class TestAtomicOperation extends HBaseTestCase {
     }
 
     public void doWork() throws Exception {
-      List<Pair<Mutation, Integer>> putsAndLocks = Lists.newArrayList();
       Put[] puts = new Put[1];
       Put put = new Put(Bytes.toBytes("r1"));
       put.add(Bytes.toBytes(family), Bytes.toBytes("q1"), Bytes.toBytes("50"));
       puts[0] = put;
-      Pair<Mutation, Integer> pair = new Pair<Mutation, Integer>(puts[0], null);
-      putsAndLocks.add(pair);
       testStep = TestStep.PUT_STARTED;
-      region.batchMutate(putsAndLocks.toArray(new Pair[0]));
+      region.batchMutate(puts);
     }
   }
 
@@ -607,16 +597,16 @@ public class TestAtomicOperation extends HBaseTestCase {
     }
 
     @Override
-    public void releaseRowLock(Integer lockId) {
+    public void releaseMyRowLocks() {
       if (testStep == TestStep.INIT) {
-        super.releaseRowLock(lockId);
+        super.releaseMyRowLocks();
         return;
       }
 
       if (testStep == TestStep.PUT_STARTED) {
         try {
           testStep = TestStep.PUT_COMPLETED;
-          super.releaseRowLock(lockId);
+          super.releaseMyRowLocks();
           // put has been written to the memstore and the row lock has been released, but the
           // MVCC has not been advanced.  Prior to fixing HBASE-7051, the following order of
           // operations would cause the non-atomicity to show up:
@@ -634,16 +624,16 @@ public class TestAtomicOperation extends HBaseTestCase {
         }
       }
       else if (testStep == TestStep.CHECKANDPUT_STARTED) {
-        super.releaseRowLock(lockId);
+        super.releaseMyRowLocks();
       }
     }
 
     @Override
-    public Integer getLock(Integer lockid, byte[] row, boolean waitForLock) throws IOException {
+    public void getRowLock(byte[] row) throws IOException {
       if (testStep == TestStep.CHECKANDPUT_STARTED) {
         latch.countDown();
       }
-      return super.getLock(lockid, row, waitForLock);
+      super.getRowLock(row);
     }
   }
 }
\ No newline at end of file
diff --git hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
index 796c9c4..d0f41c4 100644
--- hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
+++ hbase-server/src/test/java/org/apache/hadoop/hbase/regionserver/TestHRegion.java
@@ -62,7 +62,6 @@ import org.apache.hadoop.hbase.client.Durability;
 import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Increment;
-import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
@@ -97,7 +96,6 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManagerTestHelper;
 import org.apache.hadoop.hbase.util.FSUtils;
 import org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge;
-import org.apache.hadoop.hbase.util.Pair;
 import org.apache.hadoop.hbase.util.PairOfSameType;
 import org.apache.hadoop.hbase.util.Threads;
 import org.junit.Assert;
@@ -794,7 +792,7 @@ public class TestHRegion extends HBaseTestCase {
       metricsAssertHelper.assertCounter("syncTimeNumOps", syncs + 2, source);
 
       LOG.info("Next a batch put that has to break into two batches to avoid a lock");
-      Integer lockedRow = region.obtainRowLock(Bytes.toBytes("row_2"));
+      region.getRowLock(Bytes.toBytes("row_2"));
 
       MultithreadedTestUtil.TestContext ctx =
         new MultithreadedTestUtil.TestContext(conf);
@@ -819,7 +817,7 @@ public class TestHRegion extends HBaseTestCase {
         }
       }
       LOG.info("...releasing row lock, which should let put thread continue");
-      region.releaseRowLock(lockedRow);
+      region.releaseMyRowLocks();
       LOG.info("...joining on thread");
       ctx.stop();
       LOG.info("...checking that next batch was synced");
@@ -830,29 +828,6 @@ public class TestHRegion extends HBaseTestCase {
           OperationStatusCode.SUCCESS, codes[i].getOperationStatusCode());
       }
 
-      LOG.info("Nexta, a batch put which uses an already-held lock");
-      lockedRow = region.obtainRowLock(Bytes.toBytes("row_2"));
-      LOG.info("...obtained row lock");
-      List<Pair<Mutation, Integer>> putsAndLocks = Lists.newArrayList();
-      for (int i = 0; i < 10; i++) {
-        Pair<Mutation, Integer> pair = new Pair<Mutation, Integer>(puts[i], null);
-        if (i == 2) pair.setSecond(lockedRow);
-        putsAndLocks.add(pair);
-      }
-
-      codes = region.batchMutate(putsAndLocks.toArray(new Pair[0]));
-      LOG.info("...performed put");
-      for (int i = 0; i < 10; i++) {
-        assertEquals((i == 5) ? OperationStatusCode.BAD_FAMILY :
-          OperationStatusCode.SUCCESS, codes[i].getOperationStatusCode());
-      }
-      // Make sure we didn't do an extra batch
-      metricsAssertHelper.assertCounter("syncTimeNumOps", syncs + 5, source);
-
-      // Make sure we still hold lock
-      assertTrue(region.isRowLocked(lockedRow));
-      LOG.info("...releasing lock");
-      region.releaseRowLock(lockedRow);
     } finally {
       HRegion.closeHRegion(this.region);
        this.region = null;
